[
    {
        "title": "[bitcoin-dev] Multipayment Channels - A scalability solution for Layer 1",
        "thread_messages": [
            {
                "author": "Ali Sherief",
                "date": "2022-09-04T22:31:38",
                "message_text_only": "Over the past few days I've figured out a novel way to batch transactions together into blocks, thereby compacting the transaction size and increasing the transactions-per-second. This is all on layer 1, without any hardforks - only a single softfork is required to add MuSig1 support for individual invoice addresses.\n\nThe nucleus of the idea was born after a discussion with Greg Maxwell about a different BIP (Implementing multisig using Taproot, to be specific)[1]. He suggested to me that I should add MuSig1 signatures into the Taproot script paths.\n\nAfter some thinking, I realized a use case for MuSig1 signatures as a kind of on-chain Lightning Network. Allow me to explain:\n\nLN is very attractive to users because it keeps intermediate transaction states off-chain, and only broadcasts the final state. But without mitigations in the protocol, it suffers from two disadvantages:\n\n- You have to trust the other channel partner not to broadcast a previous state\n- You also have to trust all the middlemen in intermediate channels not to do the above.\n\nMost of us probably know that many mitigations have been created for this problem, e.g. penalty transactions. But what if it were possible to create a scheme where so-called technical fraud is not possible? That is what I'm going to demonstrate here.\n\nMy scheme makes use of MuSig1, OP_CHECKLOCKTIMEVERIFY (OP_CLTV) timelock type, and negligible OP_RETURN data. It revolves around constructs I call \"multipayment channels\", called so because they allow multiple people to pay in one transaction - something that is already possible BTW, but with much larger tx size (for large number of cosigners) than when using MuSig1. These have the advantage over LN channels that the intermediate state is also on the blockchain, but it's very compact.\n\nA channel consists of a fixed amount of people N. These people open a channel by creating a (optionally Taproot) address with the following script:\n<blockheightofoutput+desiredwaitingblocks>* OP_CTLV OP_DROP <N-of-N MuSig1> OP_CHECKMUSIG**\n\nSimultaneously, each of the N participants receives the N signatures and constructs the N-of-N MuSig. Each participant will use this MuSig to generate his own independent \"commitment transaction\" with the following properties:\n\n- It has a single input, the MuSig output. It has an nSequence of desiredwaitingblocks. <This prevents the output from being spent immediately.>\n- It has outputs corresponding to the addresses and balances of each of the participants in the agreed-upon distribution.\nDisadvantage: Because the N-of-N signature is given to all participants, it might be leaked into the public and consequentially anybody can spend this transaction after the timelock, to commit the balance.*** On the other hand, removing the timelocks means that if one of the participants goes missing, all funds are locked forever.****\n\nA second output with a script OP_RETURN <32-byte connection ID> can be added to the transaction to enable L1 channel discovery.\n\nFull nodes parsing the blockchain can maintain a list of connection IDs to connect to (but without a non-malleable refund transaction, nobody is going to use this). SPVs can simply export a list of them from Full Nodes.\n\nA connection only lasts for one transaction. Spending the output to another MuSig of the above format will create a new connection if it spends to a similarly-constructed MuSig output with different signature. In all cases, the current connection is destroyed.\n\n*This introduces a variable grace period, in blocks, after which anybody can broadcast this transaction to commit the channel funds distribution to each of the participants' addresses. blockheightofoutput is the block height of the musig output, and desiredwaitingblocks is the maximum number of blocks the connection can stay alive for.\n**This implies that a hypothetical OP_CHECKMUSIG would take a single aggregated signature, a single aggregated public key, and an integer N that denotes how many public keys were combined together. I elected not to overload OP_CHECKSIG since MuSig and regular signatures are both valid for the same address types. This part is a rought draft and requires lots of work on making an OP_CHECKMUSIG opcode that satisfies the requirements of multipayment channels.\n***This is quite a bad flaw of this scheme because it means that all the participants must be trustworthy - you can't use this in trustless environments. I appreciate any ways on how to implement non-malleable refund transactions with single (non-aggregated) signatures!\n****Perhaps the best solution is to offer both alternatives: <N-of-N MuSig1> OP_CHECKMUSIG in a public scenario where none of the participants want to face the prospect of losing their money, and <blockheightofoutput+desiredwaitingblocks>* OP_CTLV OP_DROP <N-of-N MuSig1> OP_CHECKMUSIG with signature sharing in private scenarios.\n\nThis draft is very crude and parts have not been fully developed. Tips for fleshing it out is much appreciated. Not that there's anything wrong with LN for that matter, I'm just concerned about the security reprocussions of not broadcasting intermediate transactions, and its enabling of crime.\n\n- Ali\n\n[1]: https://bitcointalk.org/index.php?topic=5410553.0"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-05T03:17:27",
                "message_text_only": "Good morning Ali,\n\n> Over the past few days I've figured out a novel way to batch transactions together into blocks, thereby compacting the transaction size and increasing the transactions-per-second. This is all on layer 1, without any hardforks - only a single softfork is required to add MuSig1 support for individual invoice addresses.\n> \n> The nucleus of the idea was born after a discussion with Greg Maxwell about a different BIP (Implementing multisig using Taproot, to be specific)[1]. He suggested to me that I should add MuSig1 signatures into the Taproot script paths.\n> \n> After some thinking, I realized a use case for MuSig1 signatures as a kind of on-chain Lightning Network. Allow me to explain:\n> \n> LN is very attractive to users because it keeps intermediate transaction states off-chain, and only broadcasts the final state. But without mitigations in the protocol, it suffers from two disadvantages:\n> \n> - You have to trust the other channel partner not to broadcast a previous state\n> - You also have to trust all the middlemen in intermediate channels not to do the above.\n> \n> Most of us probably know that many mitigations have been created for this problem, e.g. penalty transactions. But what if it were possible to create a scheme where so-called technical fraud is not possible? That is what I'm going to demonstrate here.\n\nThe fact that you need to invoke trust later on (\"Because the N-of-N signature is given to all participants, it might be leaked into the public\") kinda breaks the point of \"technical fraud is not possible\".\n\nAt least with the penalty transactions of Poon-Dryja and the update transactions of Decker-Russell-Osuntokun you never have to worry about other parties leaking information and possibly changing the balance of the channel.\nYou only need to worry about ensuring you have an up-to-date view of the blockchain, which can be mitigated further by e.g. running a \"spare\" fullnode on a Torv3 address that secretly connects to your main fullnode (making eclipse attacks that target your known IP harder), connecting to Blockstream Satellite, etc.\nYou can always get more data yourself, you cannot stop data being acquired by others.\n\n> My scheme makes use of MuSig1, OP_CHECKLOCKTIMEVERIFY (OP_CLTV) timelock type, and negligible OP_RETURN data. It revolves around constructs I call \"multipayment channels\", called so because they allow multiple people to pay in one transaction - something that is already possible BTW, but with much larger tx size (for large number of cosigners) than when using MuSig1. These have the advantage over LN channels that the intermediate state is also on the blockchain, but it's very compact.\n\nHow is this more advantageous than e.g. CoinPools / multiparticipant channels / Statechains ?\n\n> A channel consists of a fixed amount of people N. These people open a channel by creating a (optionally Taproot) address with the following script:\n> <blockheightofoutput+desiredwaitingblocks>* OP_CTLV OP_DROP <N-of-N MuSig1> OP_CHECKMUSIG**\n\nIf it is Taproot, then `OP_CHECKSIG` is already `OP_CHECKMUSIG`, since MuSig1 (and MuSig2, for that matter....) is just an ordinary Schnorr signature.\nIn a Tapscript, `OP_CHECKSIG` validates Schnorr signatures (as specified in the relevant BIP), not the ECDSA signatures.\n\n> Simultaneously, each of the N participants receives the N signatures and constructs the N-of-N MuSig. Each participant will use this MuSig to generate his own independent \"commitment transaction\" with the following properties:\n> \n> - It has a single input, the MuSig output. It has an nSequence of desiredwaitingblocks. <This prevents the output from being spent immediately.>\n> \n> - It has outputs corresponding to the addresses and balances of each of the participants in the agreed-upon distribution.\n> Disadvantage: Because the N-of-N signature is given to all participants, it might be leaked into the public and consequentially anybody can spend this transaction after the timelock, to commit the balance.*** On the other hand, removing the timelocks means that if one of the participants goes missing, all funds are locked forever.****\n\nAs I understand it, in your mechanism:\n\n* Onchain, there is an output with the above SCRIPT: `<blockheightofoutput+desiredwaitingblocks>* OP_CTLV OP_DROP <N-of-N MuSig1> OP_CHECKMUSIG`\n  * Let me call this the \"channel UTXO\".\n* Offchain, you have a \"default transaction\" which spends the above output, and redistributes it back to the original owners of the funds, with a timelock requirement (as needed by `OP_CLTV`).\n\nIs that correct?\n\nThen I can improve it in the following ways:\n\n* Since everyone has to sign off the \"default transaction\" anyway, everyone can ensure that the `nLockTime` field is correct, without having `OP_CLTV` in the channel UTXO SCRIPT.\n  * So, the channel UTXO does not need a SCRIPT --- it can just use a Taproot-address Schnorr MuSig point directly.\n  * This has the massive advantage that the \"default transaction\" does not have any special SCRIPTs, improving privacy (modulo the fact that you are cooperating with others who could leak their data).\n* If the participants can agree on a new distribution of the funds, then they can sign off with the current blockheight without waiting for the later blockheight.\n  * This improves security, since the new distribution can appear on the mempool first and be confirmed first before the \"default transaction\" can.\n\n\nNow I want you to look at the literature on channel constructions, particularly \"Spilman channels\".\nFor example, see this:\n\n* https://old.reddit.com/r/Bitcoin/comments/cc9psl/technical_a_brief_history_of_payment_channels/\n\nMy modifications to your scheme are just a modernization of the Spilman channels.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Multipayment Channels - A scalability solution for Layer 1",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Ali Sherief"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 11004
        }
    },
    {
        "title": "[bitcoin-dev] Transcript: Online Socratic on MuSig2",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2022-09-08T11:19:47",
                "message_text_only": "Hi\n\nWe had an online Socratic on August 11th with Tim Ruffing (co-author of MuSig2 draft BIP) and Elizabeth Crites (co-author of research papers on MuSig(2), FROST). It was previously announced here [0] but ended up being rescheduled.\n\nThe transcript is here [1], the video is here [2] and a reading list collecting together a number of resources on MuSig(2), FROST, ROAST etc is here [3].\n\nWe discussed a retrospective look at BIP340, handling x-only public keys in MuSig2 and the proposed TLUV covenant opcode, the history from initially broken MuSig1 through MuSig-DN to MuSig2, how MuSig2 and FROST compare for multisig schemes (i.e. n-of-n), why MuSig2 doesn't use proofs of possession and the current state of the draft MuSig2 BIP.\n\nWe covered a lot of topics and it was rather long (~2.5 hours) so check out the transcript or video if you are interested in any of the above topics. Many thanks to those who participated.\n\nJonas Nick recently tweeted [4] that the MuSig2 BIP [5] is approaching a stable version 1.0 which should be helpful to those who are interested (or already!) using it in the wild.\n\n[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-July/020772.html\n[1]: https://btctranscripts.com/london-bitcoin-devs/2022-08-11-tim-ruffing-musig2/\n[2]: https://www.youtube.com/watch?v=TpyK_ayKlj0\n[3]: https://gist.github.com/michaelfolkson/5bfffa71a93426b57d518b09ebd0998c\n[4}: https://twitter.com/n1ckler/status/1567168267025874944\n[5]: https://github.com/jonasnick/bips/blob/musig2/bip-musig2.mediawiki\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220908/f6918882/attachment.html>"
            },
            {
                "author": "Ali Sherief",
                "date": "2022-09-11T07:43:11",
                "message_text_only": "Hi Michael.\n\nI read the transcript of the Socratic and I have to say that it is quite detailed and touches a lot of problems including the well-known theft/offline problems which also has forms elsewhere such as for passwords.\n\nMy question is, do you or anyone else in the Socratic know of any research to this that's don't involve a trade-off of theft or online connectivity?\n\nROAST and Liquid is perhaps the farthest I know of that addresses this problem, but it's using centralized nodes right now. I was thinking, maybe these federated nodes can be decentralized into a few of these \"lite nodes\" managed by each service wanting a payment, that make a threshold signature out of many subscribers paying at the same time.\n\n- Ali\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220911/5966df87/attachment.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-09-12T16:00:52",
                "message_text_only": "Hi Ali\n\n> do you or anyone else in the Socratic know of any research to this that's don't involve a trade-off of theft or online connectivity?\n\nAny generation of a signature(s), whether that be single key (e.g. OP_CHECKSIG), multisig with multiple signatures going onchain (e.g. OP_CHECKMULTISIG, OP_CHECKSIGADD) or key aggregation multisig with only a single signature going onchain (e.g. OP_CHECKSIG), requires private key(s) and hence has concerns with regards to security and theft of those private keys. Clearly funds locked behind any kind multisig arrangement is better than no multisig arrangement as otherwise theft of a single private key can result in loss of funds.\n\nWith regards to connectivity or interactivity key aggregation multisig does increase the interactivity requirements so if you wanted to minimize interactivity requirements you'd probably stick to OP_CHECKMULTISIG, OP_CHECKSIGADD that only requires you to generate a signature and then pass it onto the next signer.\n\n> ROAST and Liquid is perhaps the farthest I know of that addresses this problem, but it's using centralized nodes right now. I was thinking, maybe these federated nodes can be decentralized into a few of these \"lite nodes\" managed by each service wanting a payment, that make a threshold signature out of many subscribers paying at the same time.\n\nI'm not sure what you mean here. In the realm of generating signatures there isn't really a concept of a \"lite node\". That makes more sense in the realm of verification where you may or may not be doing full verification. In the generating signatures realm you are either contributing to the aggregated signature or generating a standalone signature yourself. If you are not doing either of those and aren't doing some kind of coordination then you are entirely irrelevant to the scheme. In the case of Liquid there is a 11-of-15 threshold signature arrangement where currently 11 signatures go onchain when funds are moved but if Liquid used a key aggregation scheme like FROST only a single signature would need to go onchain. With regards to centralization/decentralization you could increase the 11-of-15 to say a 22-of-30. Or you could have a nested MuSig/FROST scheme behind one of the 11 signers of the 11-of-15. But you can't get around the fact that you are either generating a signature that ultimately contributes to the moving of the funds or you aren't. If you aren't generating a signature then you are just verifying the signature(s) that go onchain like all other full nodes on the network.\n\nThanks\nMichael\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Sunday, September 11th, 2022 at 08:43, Ali Sherief <ali at notatether.com> wrote:\n\n> Hi Michael.\n>\n> I read the transcript of the Socratic and I have to say that it is quite detailed and touches a lot of problems including the well-known theft/offline problems which also has forms elsewhere such as for passwords.\n>\n> My question is, do you or anyone else in the Socratic know of any research to this that's don't involve a trade-off of theft or online connectivity?\n>\n> ROAST and Liquid is perhaps the farthest I know of that addresses this problem, but it's using centralized nodes right now. I was thinking, maybe these federated nodes can be decentralized into a few of these \"lite nodes\" managed by each service wanting a payment, that make a threshold signature out of many subscribers paying at the same time.\n>\n> - Ali\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220912/f261ff41/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Transcript: Online Socratic on MuSig2",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Ali Sherief",
                "Michael Folkson"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 6573
        }
    },
    {
        "title": "[bitcoin-dev] joinstr: coinjoin implementation using nostr",
        "thread_messages": [
            {
                "author": "woltx",
                "date": "2022-09-09T21:05:07",
                "message_text_only": "Hi /dev/fd0,\n\nI've been reviewing joinstr, and if I understand the code correctly, the cryptographic scheme mentioned as an alternative to blind signatures isn't implemented yet, is it? Currently, it seems that anyone can submit unrelated inputs and outputs.\n\nPerhaps PR #24058 (https://github.com/bitcoin/bitcoin/pull/24058) (basic support BIP-322) can improve this scheme as it implements proof of ownership. \n\nInstead of clients sending descriptors to the relay and then verifying them using `scantxoutset`, it can send `txid:out` with a message signed with the address, verify using `verifymessage` and then use `gettxout` to retrieve the value. That way, only the owner can send the UTXO.\n\nI've done some tests connected to a node with BIP322 enabled:\n\n# to send\ninput_txt: str = json.dumps(input)\nresult = core.signmessage(wallet, input['address'], input_txt)\ninput['signature'] = result['result']\nnostr_interface.publish_input(input)\n\n# to receive\ndef validate_input(input: dict[str, int, str, str]) -> bool:\n    # ...\n    result = core.verifymessage(address=input['address'], message=json.dumps(message), signature=input['signature'])\n    return result['error'] == None and result['result'] == True\n\n\n\n\n\n------- Original Message -------\nOn Saturday, August 20th, 2022 at 1:52 PM, alicexbt via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> Hi Max,\n> \n> There a few DoS vectors that need to be fixed. Its just a proof of concept that I wanted to share with everyone to get feedback which could be improved over time. There is also a warning at the bottom of README to not use this on mainnet as it might have bugs.\n> \n> I will continue the development with coinjoin transactions on signet for a few weeks until there is a stable release with no bugs.\n> \n> I have a few ideas in mind for various relay types that might be used concurrently to prevent numerous problems. Custom relays are supported by Nostr. Examples include paying a fee to register for a round, subscribing with a time limit, or using invite-only relays. I will run a free and open nostr relay for this project and try to fix the Dos issues before a mainnet version is released for python script(for nerds) and android app (for all users).\n> \n> Related links:\n> \n> https://github.com/fiatjaf/relayer\n> https://github.com/fiatjaf/expensive-relay\n> https://github.com/fiatjaf/relayer/tree/master/whitelisted\n> \n> /dev/fd0\n> \n> \n> Sent with Proton Mail secure email.\n> \n> \n> ------- Original Message -------\n> On Saturday, August 20th, 2022 at 10:04 AM, Max Hillebrand max at towardsliberty.com wrote:\n> \n> \n> \n> > Great to see an implementation of the idea.\n> > \n> > Maybe I misunderstand, but isn't there a vulnerability of denial of service here?\n> > \n> > A user who registers one input will receive the round secret identifier, and this is all the information required for output registration. However, that malicious user can now register multiple outputs, providing the same secret, and nobody can link the malicious outputs to any specific input. Therefor there cannot be a blame round where the malicious input is removed, and thus there can be a ongoing free denial of service attack without attribution or defense.\n> > \n> > Skol\n> > Max\n> > \n> > On August 20, 2022 10:20:00 AM GMT+02:00, alicexbt via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> > \n> > > Hi Bitcoin Developers,\n> > > \n> > > I have written a python script as proof of concept for the coinjoin implementation using nostr. I used a lot of Python scripts created by others in school, so it feels nice to offer something that could be useful to others.\n> > > \n> > > The implementation uses Bitcoin Core wallet and RPCs: `listunspent`, `getnewaddress`, `scantxoutset`, `createpsbt`, `combinepsbt`, `finalizepsbt` and `sendrawtransaction`. It requires python-nostr library because nostr is used for coordination between peers. Nostr is a decentralized network based on cryptographic keypairs. It is not peer-to-peer however simple and scalable.\n> > > \n> > > Every step is published as an event using a nostr relay and 5 peers coordinate to create, sign and broadcast a coinjoin transaction. I need to write a NIP that would be an alternative to blind signatures. Relay will share a random secret with clients for one round which should be present in output registration request although never gets published. If someone tries to register an output without registering any inputs, request would not have the number initially shared with inputs so request would get rejected or published as unverified. Relay would not be able to link inputs and outputs as the number is same for all inputs in a round and they get registered at different times with new keys and IP address. Clients can use multiple relays at the same time to avoid trusting one relay. This would result in different shared secret number but same process. If a relay tries to cheat, users will not sign the transaction and avoid using it in future.\n> > > \n> > > Usage:\n> > > \n> > > 1)Run `python coinjoin.py` and enter descriptor for one of the inputs.\n> > > 2)Script will check inputs for this round in every 30 seconds and register a new adddress for output once 5 inputs are registered.\n> > > 3)Similar check happens every 30 seconds for outputs. Last peer should create a PSBT.\n> > > 4)Unsigned PSBT will be printed and signed by wallet with `walletprocesspsbt` RPC.\n> > > 5)Script will check signed PSBTs and last peer to sign should finalize coinjoin transaction once 5 signed PSBTs are received.\n> > > 6)Coinjoin transaction will be broadcasted and txid will printed.\n> > > \n> > > Example:\n> > > \n> > > ```\n> > > List of utxos in wallet:\n> > > \n> > > wpkh([53830dca/84'/1'/0'/0/0]02449be5fb74725255eeeb50eba930fa87705f21e99d13cd710cf2c1f21153c808)#x2hyyeg5\n> > > \n> > > Enter descriptor for the input registration: wpkh([53830dca/84'/1'/0'/0/0]02449be5fb74725255eeeb50eba930fa87705f21e99d13cd710cf2c1f21153c808)#x2hyyeg5\n> > > \n> > > event id: bcbbe62d75d99fed73f1e50ac58a38d1840b658951893e63c0322b378d7d56f0\n> > > \n> > > \n> > > tb1qhxrp4zl54ul0twtyz0gury5399q7z0kvqqrl6m registered for output\n> > > \n> > > event id: 9449c9065bef356d21507a98f88b028b17fc1c49eb195c8d4420604fcaaef041\n> > > \n> > > Unsigned PSBT: cHNidP8BAP1yAQIAAAAFtMaoJYcXvOG5L3Yaz3YyS7gIt4h5/zzOrRRS3hrVvwoAAAAAAP////+o83geaSm4L76KToIUl5MiZqLAUbIDJLq6DWrjP/3b8AEAAAAA/////zEF3CXIvVHpIa7No1s1yg+KtyOfXTRSyWnOdXMfzcDwAQAAAAD/////wMa4XAgnU+39Ien+KG9rYtv8bLMNYakmZyY/QFfwLRcAAAAAAP/////5M42ID6uLmQTb2tnFHnN7UMpnDD25uN8ZX7A+GNSM3QEAAAAA/////wV4xwEAAAAAABYAFLmGGov0rz71uWQT0cGSkSlB4T7MeMcBAAAAAAAWABSc0/FM6Hdbdxh10IJkYOklVFWqjnjHAQAAAAAAFgAUPSZKe/w6PT6qIF+WhL4wHaFymjd4xwEAAAAAABYAFMx0rxYlpPWB3NFry4Ctk2eVi/UNeMcBAAAAAAAWABSzc4xK0VTfvjK0MHXrAUFLYgYnOgAAAAAAAAAAAAAAAAAAAA==\n> > > \n> > > event id: 976744b38fa9343fb79e1b5215512ead6ee08e5890d79a201fc5b872f6de4eba\n> > > \n> > > Signed PSBT: cHNidP8BAP1yAQIAAAAFtMaoJYcXvOG5L3Yaz3YyS7gIt4h5/zzOrRRS3hrVvwoAAAAAAP////+o83geaSm4L76KToIUl5MiZqLAUbIDJLq6DWrjP/3b8AEAAAAA/////zEF3CXIvVHpIa7No1s1yg+KtyOfXTRSyWnOdXMfzcDwAQAAAAD/////wMa4XAgnU+39Ien+KG9rYtv8bLMNYakmZyY/QFfwLRcAAAAAAP/////5M42ID6uLmQTb2tnFHnN7UMpnDD25uN8ZX7A+GNSM3QEAAAAA/////wV4xwEAAAAAABYAFLmGGov0rz71uWQT0cGSkSlB4T7MeMcBAAAAAAAWABSc0/FM6Hdbdxh10IJkYOklVFWqjnjHAQAAAAAAFgAUPSZKe/w6PT6qIF+WhL4wHaFymjd4xwEAAAAAABYAFMx0rxYlpPWB3NFry4Ctk2eVi/UNeMcBAAAAAAAWABSzc4xK0VTfvjK0MHXrAUFLYgYnOgAAAAAAAQBxAgAAAAG+qpMXZCy6tBuUlgo8JD0GVXKp60FkhwDeg2sF1fkFkwMAAAAA/f///wLo9wEAAAAAABYAFFfLA5xarC/w/SxeMDQ5tuXrYJLUWwMAAAAAAAAWABRfPf//hwMjHB4OKj87cU19XOSh7yOWAQABAR/o9wEAAAAAABYAFFfLA5xarC/w/SxeMDQ5tuXrYJLUAQhrAkcwRAIgOIhLoC5348U8YkEr4GU1K4yWskIOEXgW4Wsk/W2cR7ICIEJXqtOuDJ5CkwrSuwJLWtzab4dslbN3KuL/pyooMnOCASECRJvl+3RyUlXu61DrqTD6h3BfIemdE81xDPLB8hFTyAgAAAAAACICA77Cnd6o3kr0yc+91eabpOn5igs/MUMbudNYSS6oyMWMGFODDcpUAACAAQAAgAAAAIAAAAAAFAAAAAAAAAAA\n> > > \n> > > event id: 5846b6e6902f3c5a43496d7d9785ed62444aa74963f03c33d637d8b09ee7a139\n> > > \n> > > Coinjoin tx: 75e490b10b15a6a0422f25ff66ad98ef70390c8fecaac02712705dce8cc3564b\n> > > \n> > > event id: 9b5d4bf279b59e2b6e539e683fba83da72dce2b640360aa95db1b1400be93190\n> > > ```\n> > > \n> > > There are lot of things that could be improved and a few suggestions are in the gist that described the idea. I would love read to any opinions about this experiment and will start working on creating an Android app for joinstr next week.\n> > > \n> > > Credits:\n> > > \n> > > - fiatjaf (Nostr)\n> > > - Andrew Chow (PSBT)\n> > > - Jeff Thibault (python-nostr)\n> > > - Existing coinjoin implmentations\n> > > \n> > > /dev/fd0\n> > > \n> > > Sent with Proton Mail secure email.\n> > > \n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "alicexbt",
                "date": "2022-09-10T10:17:37",
                "message_text_only": "Hi woltx,\n\n> I've been reviewing joinstr, and if I understand the code correctly, the cryptographic scheme mentioned as an alternative to blind signatures isn't implemented yet, is it? Currently, it seems that anyone can submit unrelated inputs and outputs.\n\nThanks for reviewing joinstr. Yes, its not implemented right now as it requires a NIP and at least one relay using it.\n\n> Instead of clients sending descriptors to the relay and then verifying them using `scantxoutset`, it can send `txid:out` with a message signed with the address, verify using `verifymessage` and then use `gettxout` to retrieve the value. That way, only the owner can send the UTXO.\n\n`scantxoutset` is only used to get UTXO details (txid, vout and amount) as I thought its easier for users to just share a descriptor for coinjoin. \n\nIf a user sends `txid:out` with a message signed with the address, this would be publicly available to everyone connected to same relay and links an input with output. Responding with a secret shared by relay for the round confirms user owns one of the input but does not reveal exact input.\n\n\n/dev/fd0\n\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Friday, September 9th, 2022 at 9:05 PM, woltx <woltx at protonmail.com> wrote:\n\n\n> Hi /dev/fd0,\n> \n> I've been reviewing joinstr, and if I understand the code correctly, the cryptographic scheme mentioned as an alternative to blind signatures isn't implemented yet, is it? Currently, it seems that anyone can submit unrelated inputs and outputs.\n> \n> Perhaps PR #24058 (https://github.com/bitcoin/bitcoin/pull/24058) (basic support BIP-322) can improve this scheme as it implements proof of ownership.\n> \n> Instead of clients sending descriptors to the relay and then verifying them using `scantxoutset`, it can send `txid:out` with a message signed with the address, verify using `verifymessage` and then use `gettxout` to retrieve the value. That way, only the owner can send the UTXO.\n> \n> I've done some tests connected to a node with BIP322 enabled:\n> \n> # to send\n> input_txt: str = json.dumps(input)\n> result = core.signmessage(wallet, input['address'], input_txt)\n> input['signature'] = result['result']\n> nostr_interface.publish_input(input)\n> \n> # to receive\n> def validate_input(input: dict[str, int, str, str]) -> bool:\n> \n> # ...\n> result = core.verifymessage(address=input['address'], message=json.dumps(message), signature=input['signature'])\n> return result['error'] == None and result['result'] == True\n> \n> \n> \n> \n> \n> ------- Original Message -------\n> On Saturday, August 20th, 2022 at 1:52 PM, alicexbt via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n> \n> \n> > Hi Max,\n> > \n> > There a few DoS vectors that need to be fixed. Its just a proof of concept that I wanted to share with everyone to get feedback which could be improved over time. There is also a warning at the bottom of README to not use this on mainnet as it might have bugs.\n> > \n> > I will continue the development with coinjoin transactions on signet for a few weeks until there is a stable release with no bugs.\n> > \n> > I have a few ideas in mind for various relay types that might be used concurrently to prevent numerous problems. Custom relays are supported by Nostr. Examples include paying a fee to register for a round, subscribing with a time limit, or using invite-only relays. I will run a free and open nostr relay for this project and try to fix the Dos issues before a mainnet version is released for python script(for nerds) and android app (for all users).\n> > \n> > Related links:\n> > \n> > https://github.com/fiatjaf/relayer\n> > https://github.com/fiatjaf/expensive-relay\n> > https://github.com/fiatjaf/relayer/tree/master/whitelisted\n> > \n> > /dev/fd0\n> > \n> > Sent with Proton Mail secure email.\n> > \n> > ------- Original Message -------\n> > On Saturday, August 20th, 2022 at 10:04 AM, Max Hillebrand max at towardsliberty.com wrote:\n> > \n> > > Great to see an implementation of the idea.\n> > > \n> > > Maybe I misunderstand, but isn't there a vulnerability of denial of service here?\n> > > \n> > > A user who registers one input will receive the round secret identifier, and this is all the information required for output registration. However, that malicious user can now register multiple outputs, providing the same secret, and nobody can link the malicious outputs to any specific input. Therefor there cannot be a blame round where the malicious input is removed, and thus there can be a ongoing free denial of service attack without attribution or defense.\n> > > \n> > > Skol\n> > > Max\n> > > \n> > > On August 20, 2022 10:20:00 AM GMT+02:00, alicexbt via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> > > \n> > > > Hi Bitcoin Developers,\n> > > > \n> > > > I have written a python script as proof of concept for the coinjoin implementation using nostr. I used a lot of Python scripts created by others in school, so it feels nice to offer something that could be useful to others.\n> > > > \n> > > > The implementation uses Bitcoin Core wallet and RPCs: `listunspent`, `getnewaddress`, `scantxoutset`, `createpsbt`, `combinepsbt`, `finalizepsbt` and `sendrawtransaction`. It requires python-nostr library because nostr is used for coordination between peers. Nostr is a decentralized network based on cryptographic keypairs. It is not peer-to-peer however simple and scalable.\n> > > > \n> > > > Every step is published as an event using a nostr relay and 5 peers coordinate to create, sign and broadcast a coinjoin transaction. I need to write a NIP that would be an alternative to blind signatures. Relay will share a random secret with clients for one round which should be present in output registration request although never gets published. If someone tries to register an output without registering any inputs, request would not have the number initially shared with inputs so request would get rejected or published as unverified. Relay would not be able to link inputs and outputs as the number is same for all inputs in a round and they get registered at different times with new keys and IP address. Clients can use multiple relays at the same time to avoid trusting one relay. This would result in different shared secret number but same process. If a relay tries to cheat, users will not sign the transaction and avoid using it in future.\n> > > > \n> > > > Usage:\n> > > > \n> > > > 1)Run `python coinjoin.py` and enter descriptor for one of the inputs.\n> > > > 2)Script will check inputs for this round in every 30 seconds and register a new adddress for output once 5 inputs are registered.\n> > > > 3)Similar check happens every 30 seconds for outputs. Last peer should create a PSBT.\n> > > > 4)Unsigned PSBT will be printed and signed by wallet with `walletprocesspsbt` RPC.\n> > > > 5)Script will check signed PSBTs and last peer to sign should finalize coinjoin transaction once 5 signed PSBTs are received.\n> > > > 6)Coinjoin transaction will be broadcasted and txid will printed.\n> > > > \n> > > > Example:\n> > > > \n> > > > ```\n> > > > List of utxos in wallet:\n> > > > \n> > > > wpkh([53830dca/84'/1'/0'/0/0]02449be5fb74725255eeeb50eba930fa87705f21e99d13cd710cf2c1f21153c808)#x2hyyeg5\n> > > > \n> > > > Enter descriptor for the input registration: wpkh([53830dca/84'/1'/0'/0/0]02449be5fb74725255eeeb50eba930fa87705f21e99d13cd710cf2c1f21153c808)#x2hyyeg5\n> > > > \n> > > > event id: bcbbe62d75d99fed73f1e50ac58a38d1840b658951893e63c0322b378d7d56f0\n> > > > \n> > > > tb1qhxrp4zl54ul0twtyz0gury5399q7z0kvqqrl6m registered for output\n> > > > \n> > > > event id: 9449c9065bef356d21507a98f88b028b17fc1c49eb195c8d4420604fcaaef041\n> > > > \n> > > > Unsigned PSBT: cHNidP8BAP1yAQIAAAAFtMaoJYcXvOG5L3Yaz3YyS7gIt4h5/zzOrRRS3hrVvwoAAAAAAP////+o83geaSm4L76KToIUl5MiZqLAUbIDJLq6DWrjP/3b8AEAAAAA/////zEF3CXIvVHpIa7No1s1yg+KtyOfXTRSyWnOdXMfzcDwAQAAAAD/////wMa4XAgnU+39Ien+KG9rYtv8bLMNYakmZyY/QFfwLRcAAAAAAP/////5M42ID6uLmQTb2tnFHnN7UMpnDD25uN8ZX7A+GNSM3QEAAAAA/////wV4xwEAAAAAABYAFLmGGov0rz71uWQT0cGSkSlB4T7MeMcBAAAAAAAWABSc0/FM6Hdbdxh10IJkYOklVFWqjnjHAQAAAAAAFgAUPSZKe/w6PT6qIF+WhL4wHaFymjd4xwEAAAAAABYAFMx0rxYlpPWB3NFry4Ctk2eVi/UNeMcBAAAAAAAWABSzc4xK0VTfvjK0MHXrAUFLYgYnOgAAAAAAAAAAAAAAAAAAAA==\n> > > > \n> > > > event id: 976744b38fa9343fb79e1b5215512ead6ee08e5890d79a201fc5b872f6de4eba\n> > > > \n> > > > Signed PSBT: cHNidP8BAP1yAQIAAAAFtMaoJYcXvOG5L3Yaz3YyS7gIt4h5/zzOrRRS3hrVvwoAAAAAAP////+o83geaSm4L76KToIUl5MiZqLAUbIDJLq6DWrjP/3b8AEAAAAA/////zEF3CXIvVHpIa7No1s1yg+KtyOfXTRSyWnOdXMfzcDwAQAAAAD/////wMa4XAgnU+39Ien+KG9rYtv8bLMNYakmZyY/QFfwLRcAAAAAAP/////5M42ID6uLmQTb2tnFHnN7UMpnDD25uN8ZX7A+GNSM3QEAAAAA/////wV4xwEAAAAAABYAFLmGGov0rz71uWQT0cGSkSlB4T7MeMcBAAAAAAAWABSc0/FM6Hdbdxh10IJkYOklVFWqjnjHAQAAAAAAFgAUPSZKe/w6PT6qIF+WhL4wHaFymjd4xwEAAAAAABYAFMx0rxYlpPWB3NFry4Ctk2eVi/UNeMcBAAAAAAAWABSzc4xK0VTfvjK0MHXrAUFLYgYnOgAAAAAAAQBxAgAAAAG+qpMXZCy6tBuUlgo8JD0GVXKp60FkhwDeg2sF1fkFkwMAAAAA/f///wLo9wEAAAAAABYAFFfLA5xarC/w/SxeMDQ5tuXrYJLUWwMAAAAAAAAWABRfPf//hwMjHB4OKj87cU19XOSh7yOWAQABAR/o9wEAAAAAABYAFFfLA5xarC/w/SxeMDQ5tuXrYJLUAQhrAkcwRAIgOIhLoC5348U8YkEr4GU1K4yWskIOEXgW4Wsk/W2cR7ICIEJXqtOuDJ5CkwrSuwJLWtzab4dslbN3KuL/pyooMnOCASECRJvl+3RyUlXu61DrqTD6h3BfIemdE81xDPLB8hFTyAgAAAAAACICA77Cnd6o3kr0yc+91eabpOn5igs/MUMbudNYSS6oyMWMGFODDcpUAACAAQAAgAAAAIAAAAAAFAAAAAAAAAAA\n> > > > \n> > > > event id: 5846b6e6902f3c5a43496d7d9785ed62444aa74963f03c33d637d8b09ee7a139\n> > > > \n> > > > Coinjoin tx: 75e490b10b15a6a0422f25ff66ad98ef70390c8fecaac02712705dce8cc3564b\n> > > > \n> > > > event id: 9b5d4bf279b59e2b6e539e683fba83da72dce2b640360aa95db1b1400be93190\n> > > > ```\n> > > > \n> > > > There are lot of things that could be improved and a few suggestions are in the gist that described the idea. I would love read to any opinions about this experiment and will start working on creating an Android app for joinstr next week.\n> > > > \n> > > > Credits:\n> > > > \n> > > > - fiatjaf (Nostr)\n> > > > - Andrew Chow (PSBT)\n> > > > - Jeff Thibault (python-nostr)\n> > > > - Existing coinjoin implmentations\n> > > > \n> > > > /dev/fd0\n> > > > \n> > > > Sent with Proton Mail secure email.\n> > > > \n> > > > bitcoin-dev mailing list\n> > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "joinstr: coinjoin implementation using nostr",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "woltx",
                "alicexbt"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 19391
        }
    },
    {
        "title": "[bitcoin-dev] On a new community process to specify covenants",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2022-09-10T00:10:55",
                "message_text_only": "Hi all,\n\nFollowing up on my July's mail proposing to setup a new community process\ndedicated to covenant R&D, after aggregating all the feedbacks received\nonline/offline, I've started a repository to collect the use-cases and\nknown design constraints:\n\nhttps://github.com/ariard/bitcoin-contracting-primitives-wg\n\nOne notable change, the proposed process has been renamed to \"Bitcoin\nContracting Primitives WG\", as covenants sound for few folks to be\ninaccurate in terms of scope to designate the whole range of techniques to\nenable/empower contracting applications.\n\nSo far, I've documented the extension of the vault and payment pools\nuse-cases. Use-case analysis is following somehow inspired by the reasoning\nframework as laid out by RFC 3426 [0]. This is a first shot and all current\ndescriptions should only be taken as a \"best-effort\" for now. More\nuse-cases descriptions coming soon. Hopefully we'll have a set of\n\"champions\" by use-case emerging with time.\n\nThere is another ongoing effort to document the primitives themselves:\n\nhttps://github.com/bitcoinops/bitcoinops.github.io/pull/806\n\nAbout the starting point for regular meetings, I think the good timing is\nsomewhere in November, after the upcoming cycle of Bitcoin conferences, as\nI guess a good chunk of folks will attend them.\n\nDefining a communication channel is still an open question: IRC, Slack,\nDiscord, Discourse, ...\n\nAs discussed before, softfork activation discussions will be considered as\noff-topic and discouraged. This is first and foremost a long-term R&D\neffort.\n\nContributors, reviewers and co-maintainers to the repository are welcome.\nAll content is licensed under Creative Commons 4.0, though can be\nrelicensed to another thing if it suits more (like all Bitcoin devs I'm\nonly part-time lawyer).\n\nStill open to more feedbacks on what the ideal Bitcoin\ncovenants/contracting primitives community process would looks like.\n\nCheers,\nAntoine\n\n[0] https://www.rfc-editor.org/rfc/rfc3426.html\n\nLe mer. 20 juil. 2022 \u00e0 16:42, Antoine Riard <antoine.riard at gmail.com> a\n\u00e9crit :\n\n> Hi,\n>\n> Discussions on covenants have been prolific and intense on this mailing\n> list and within the wider Bitcoin technical circles, I believe however\n> without succeeding to reach consensus on any new set of contracting\n> primitives satisfying the requirements of known covenant-enabled use-cases.\n> I think that's a fact to deplore as covenants would not only offer vast\n> extensions of the capabilities of Bitcoin as a system, i.e enabling new\n> types of multi-party contract protocols. But also empowering Bitcoin on its\n> fundamental value propositions of store of value (e.g by making vaults more\n> flexible) and payment system (e.g by making realistic channel\n> factories/payment pools).\n>\n> If we retain as a covenant definition, a spending constraint restricting\n> the transaction to which the spent UTXO can be spent, and enabling to\n> program contracts/protocols at the transaction-level instead of the\n> script-level, the list of Script primitives proposed during the last years\n> has grown large : ANYPREVOUT [0], CHECKSIGFROMSTACK [1],\n> CHECK_TEMPLATE_VERIFY [2], TAPROOT_LEAF_UPDATE_VERIFY [3], TXHASH [4],\n> PUSHTXDATA [5], CAT [6], EVICT [7], Grafroot delegation [8], SIGHASH_GROUP\n> [9], MERKLEBRANCHVERIFY [10] and more than I can't remember. Of course, all\n> the listed primitives are at different states of formalization, some\n> already fully fleshed-out in BIPs, other still ideas on whiteboard, yet\n> they all extend the range of workable multi-party contract protocols.\n>\n> Indeed this range has grown wild. Without aiming to be exhaustive (I'm\n> certainly missing some interesting proposals lost in the abyss of\n> bitcointalk.org), we can mention the following use-cases: multi-party\n> stateful contracts [11], congestion trees [12], payment pools [13], \"eltoo\"\n> layered commitments [14], programmable vaults [15], multi-events contracts\n> [16], blockchain-as-oracle bets [17], spacechains [18], trustless\n> collateral lending [19], ...\n>\n> Minding all those facts, I would say the task of technical evaluation of\n> any covenant proposal sounds at least two fold. There is first reasoning\n> about the enabled protocols on a range of criterias such as scalability,\n> efficiency, simplicity, extensibility, robustness, data confidentiality,\n> etc. Asking questions like what are the interactions between layers, if any\n> ? Or how robust is the protocol, not just interactivity failure between\n>  participant nodes but in the face of mempools spikes or internet\n> disruption ? Or if the performance is still acceptable on shared resources\n> like blockspace or routing tables if everyone is using this protocol ? Or\n> if the protocol minimizes regulatory attack surface or centralization\n> vectors ?\n>\n> Though once this step is achieved, there is still more reasoning work to\n> evaluate how good a fit is a proposed Script primitive, the\n> efficiency/simplicity/ease to use trade-offs, but also if there are no\n> functionality overlap or hard constraints on the use-cases design\n> themselves or evolvability w.rt future Script extensions or generalization\n> of the opcode operations.\n>\n> Moreover, if you would like your evaluation of a covenant proposal to be\n> complete, I don't believe you can squeeze the implications with the mempool\n> rules and combination with any consistent fee-bumping strategy. To say\n> things politely, those areas have been a quagmire of vulnerabilities,\n> attacks and defects for second-layers Bitcoin protocols during the last\n> years [20].\n>\n> Considering the abundant problem-space offered by covenants, I believe\n> there is a reasonable groundwork to pursue in building the use-cases\n> understanding (e.g prototype, pseudo-specification, documentation, ...) and\n> building consensus on the framework of criterias on which to evaluate them\n> [21]. It might raise a really high bar for any covenant proposal compared\n> to previous softforks, however I think it would adequately reflect the\n> growth in Bitcoin complexity and funds at stakes during the last years.\n>\n> Moving towards this outcome, I would like to propose a new covenant open\n> specification process, in the same spirit as we have with the BOLTs or\n> dlcspecs. We would have regular meetings (biweekly/monthly ?), an open\n> agenda where topics of discussion can be pinned in advance and\n> documentation artifacts would be built with time driven by consensus (e.g\n> 1st phase could be to collect, pseudo-specify and find champion(s) for\n> known use-cases ?) and no timeframe. Starting date could be September /\n> October / November (later, 2023 ?), giving time for anyone interested in\n> such a covenant process to allocate development and contribution bandwidth\n> in function of their involvement interest.\n>\n> Learning from the good but specially from the bad with setting up the L2\n> onchain support meetings last year, I think it would be better to keep the\n> agenda open, loose and free as much we can in a \"burn-the-roadmap\" spirit,\n> avoiding to create a sense of commitment or perceived signaling in the\n> process participants towards any covenant solution. I would guess things to\n> be experimental and evolutionary and folks to spend the first meetings\n> actually to express what they would like the covenant process to be about\n> (and yes that means if you're a domain expert and you find the pace of\n> things too slow sometimes, you have to learn to handle your own\n> frustration...).\n>\n> In a \"decentralize-everything\" fashion, I believe it would be good to have\n> rotating meeting chairs and multiple covenant documentation archivists. I'm\n> super happy to spend the time and energy bootstrapping well such covenant\n> process effort, though as it's Bitcoin learn to decentralize yourself.\n>\n> I'm really curious what the outcome of such a covenant process would look\n> like. We might end up concluding that complex covenants are too unsafe by\n> enabling sophisticated MEV-attacks against LN [22]. Or even if there is an\n> emergent technical consensus, it doesn't mean there is a real market\n> interest for such covenant solutions. That said, I'm not sure if it's\n> really a subject of concern when you're reasoning as a scientist/engineer\n> and you value technical statements in terms of accuracy, systematic\n> relevance and intrinsic interest.\n>\n> Overall, my motivation to kick-start such a process stays in the fact that\n> covenants are required building blocks to enable scalable payments pools\n> design like CoinPool. I believe payments pools are a) cool and b) a good\n> shot at scaling Bitcoin as a payment system once we have reached\n> scalability limits of Lightning, still under the same security model for\n> users. However, as a community we might sense it's not the good timing for\n> a covenant process. I'm really fine with that outcome as there are still\n> holes to patch in LN to keep me busy enough for the coming years.\n>\n> Zooming out, I believe with any discussion about covenants or other soft\n> forks, the hard part isn't about coming up with the best technical solution\n> to a set of problems but in the iterative process where all voices are\n> listened to reach (or not) consensus on what is actually meant by \"best\"\n> and if the problems are accurate. The real physics of Bitcoin is the\n> physics of people. It's a work of patience.\n>\n> Anyways, eager to collect feedbacks on what the ideal covenant\n> specification process looks like. As usual, all opinions and mistakes are\n> my own.\n>\n> Cheers,\n> Antoine\n>\n> [0] https://github.com/bitcoin/bips/blob/master/bip-0118.mediawiki\n> [1] https://bitcoinops.org/en/topics/op_checksigfromstack/\n> [2] https://github.com/bitcoin/bips/blob/master/bip-0119.mediawiki\n> [3]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019419.html\n> [4]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019813.html\n> [5] https://github.com/jl2012/bips/blob/vault/bip-0ZZZ.mediawiki\n> [6] https://medium.com/blockstream/cat-and-schnorr-tricks-i-faf1b59bd298\n> [7]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019926.html\n> [8]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015700.html\n> [9]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-July/019243.html\n> [10] https://github.com/bitcoin/bips/blob/master/bip-0116.mediawiki\n> [11]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019808.html\n> [12]\n> https://github.com/bitcoin/bips/blob/master/bip-0119.mediawiki#Congestion_Controlled_Transactions\n> [13]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-June/017964.html\n> [14]\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2020-January/002448.html\n> [15] http://fc17.ifca.ai/bitcoin/papers/bitcoin17-final28.pdf\n> [16]\n> https://github.com/ariard/talk-slides/blob/master/advanced-contracts.pdf\n> [17] https://blog.bitmex.com/taproot-you-betcha/\n> [18]\n> https://gist.github.com/RubenSomsen/c9f0a92493e06b0e29acced61ca9f49a#spacechains\n> [19] https://gist.github.com/RubenSomsen/bf08664b3d174551ab7361ffb835fcef\n> [20] https://github.com/jamesob/mempool.work\n> [21] https://github.com/bitcoinops/bitcoinops.github.io/pull/806\n> [22] https://blog.bitmex.com/txwithhold-smart-contracts/\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220909/6d617955/attachment-0001.html>"
            },
            {
                "author": "Buck O Perley",
                "date": "2022-09-12T00:05:18",
                "message_text_only": "Hi Antoine,\n\nFirst just wanted to thank you for taking the initiative to \nput this together. I think that as the community and \necosystem continue to grow, it's going to be an important \npart of the process to have groups like this develop. Hopefully\nthey allow us to resist the \"Tyranny of Structurelessness\" without \nresorting to formalized governance processes and systems. \n\n> Defining a communication channel is still an open question: IRC, Slack,\nDiscord, Discourse, ...\n\nI would vote against Slack. IRC is probably the best but maybe too\nhigh a barrier to entry? Publishing logs at least would counter\nconcerns of it being exclusive. Maybe discord as an alternative. \n\n> About the starting point for regular meetings, I think the good timing is\nsomewhere in November, after the upcoming cycle of Bitcoin conferences,\n\n+1 \n\n> softfork activation discussions will be considered as\noff-topic and discouraged. This is first and foremost a long-term R&D\neffort.\n\nI understand the reason for this but I do have some concerns that\nit's not as off-topic as most of us would like. It shouldn't\nbe a priority but how any of these primitives end up getting activated\nis part of the proposal itself in my opinion. \n\nI think it also became clear in some of the discussions over the past \n~year that maybe there were more concerns than people realized about\neven the taproot activation process, whether the method used or if it \nwas done too quickly. An example of where there might be \nsome intersection with the WG as proposed is the question of how much \nresearch, security audits, etc. are \"enough\" before it should be \nconsidered for activation? \n\nMaybe as a way to keep these topics separate, it would make sense \nfor activation to have its own WG. As norms develop around this one, \nthey could inform creating a separate space focused on forwarding \nresearch and discussion around how to introduce upgrades to bitcoin. \n\nIn general it would be nice to have multiple of these groups\nhappening at once, and finding a way that they can operate separate\nfrom centralized companies. To my mind, there's no good reason why\na supposedly decentralized protocol should have to be focusing on only\none set of protocol advancements at a time. The linear way that\ndiscussions post-Taproot activation took shape (\"What do you think the\nnext bitcoin softfork should be?\") is a sign of weakness in my opinion. \nDefinitely a big red flag that we should be concerned with. \n\nCouple other comments from the proposal/repo:\n\n* it seems like there might be some opportunities to work with \nbipbounty.org which grew out of the organic bounty donations that\nwere made towards finding CTV vulnerabilities. For example, \nif the group develops specific, achievable research goals (building\nout use cases, researching vulnerabilities or limitations, etc.), \nbipbounty.org could help support these efforts in a more decentralized\nway by diversifying funding. \n\n* Any thoughts on starting to commit to an in-person meetup to happen \n~6 months - 1 year after the start of the regular online meetings? \nThat should be plenty of time for people to plan and formalize \na location and it seems like other IRL dev meetups have been \nvery productive in terms of knowledge sharing and setting priorities. \nAn in-person meetup would give a nice goal to work towards and a way\nto measure progress. \n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: publickey - buck.perley at protonmail.com - 0xC64EEB00.asc\nType: application/pgp-keys\nSize: 608 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220912/600d520a/attachment.bin>"
            },
            {
                "author": "Ryan Grant",
                "date": "2022-09-13T16:02:35",
                "message_text_only": "On Mon, Sep 12, 2022 at 2:47 AM Buck O Perley via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> First just wanted to thank you\nfor taking the initiative to\n> put this together. I think that as the community and\n> ecosystem continue to grow, it's going to be an important\n> part of the process to have groups like this develop. Hopefully\n> they allow us to resist the \"Tyranny of Structurelessness\" without\n> resorting to formalized governance processes and systems.\n\nHuh, lots of reading material behind that phrase.  I'd heard it\nbefore, but hadn't looked it up.\n\n> > Defining a communication channel is still an open question: IRC, Slack,\n> Discord, Discourse, ...\n>\n> I would vote against Slack. IRC is probably the best but maybe too\n> high a barrier to entry? Publishing logs at least would counter\n> concerns of it being exclusive. Maybe discord as an alternative.\n\nI found Discord immediately wanted a phone number from me.  I think\nIRC remains the lowest bar for participants to contribute.\n\n> > About the starting point for regular meetings, I think the good timing is\n> somewhere in November, after the upcoming cycle of Bitcoin conferences,\n\n+1\n\n> Maybe as a way to keep these topics separate, it would make sense\n> for activation to have its own WG. As norms develop around this one,\n> they could inform creating a separate space focused on forwarding\n> research and discussion around how to introduce upgrades to bitcoin.\n\nI'd participate in this.\n\n> In general it would be nice to have multiple of these groups\n> happening at once, and finding a way that they can operate separate\n> from centralized companies. To my mind, there's no good reason why\n> a supposedly decentralized protocol should have to be focusing on only\n> one set of protocol advancements at a time. The linear way that\n> discussions post-Taproot activation took shape (\"What do you think the\n> next bitcoin softfork should be?\") is a sign of weakness in my opinion.\n> Definitely a big red flag that we should be concerned with.\n\nYes.\n\n> * Any thoughts on starting to commit to an in-person meetup to happen\n> ~6 months - 1 year after the start of the regular online meetings?\n\nI think that sounds reasonable."
            },
            {
                "author": "Devrandom",
                "date": "2022-09-15T08:05:27",
                "message_text_only": "On Tue, Sep 13, 2022 at 6:03 PM Ryan Grant via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, Sep 12, 2022 at 2:47 AM Buck O Perley via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> First just wanted to thank you\n> for taking the initiative to\n> > put this together. I think that as the community and\n> > ecosystem continue to grow, it's going to be an important\n> > part of the process to have groups like this develop. Hopefully\n> > they allow us to resist the \"Tyranny of Structurelessness\" without\n> > resorting to formalized governance processes and systems.\n>\n> Huh, lots of reading material behind that phrase.  I'd heard it\n> before, but hadn't looked it up.\n>\n> > > Defining a communication channel is still an open question: IRC, Slack,\n> > Discord, Discourse, ...\n> >\n> > I would vote against Slack. IRC is probably the best but maybe too\n> > high a barrier to entry? Publishing logs at least would counter\n> > concerns of it being exclusive. Maybe discord as an alternative.\n>\n> I found Discord immediately wanted a phone number from me.  I think\n> IRC remains the lowest bar for participants to contribute.\n>\n>\nAgreed, anything that requires a phone number makes it difficult to be\npseudonymous.\n\nI recommend Matrix, since it doesn't require any privacy invasive\ninformation and has e2ee by default for 1-1 conversations.\n\nThe Matrix room could optionally bridge to IRC if there is a significant\ndemand for that.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220915/2f70be72/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-09-16T19:08:06",
                "message_text_only": "Hi Devrandom,\n\n> Agreed, anything that requires a phone number makes it difficult to be\n> pseudonymous.\n>\n> I recommend Matrix, since it doesn't require any privacy invasive\n> information and has e2ee by default for 1-1 conversations.\n\nYeah sounds like people are opting for either Matrix or IRC and good to let\ncast open.\n\nIf there are more things that the process could adopt to encourage or stay\nopen to pseudonymous participation that's interesting to bookmark.\n\nBest,\nAntoine\n\nLe jeu. 15 sept. 2022 \u00e0 04:37, Devrandom via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> On Tue, Sep 13, 2022 at 6:03 PM Ryan Grant via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Mon, Sep 12, 2022 at 2:47 AM Buck O Perley via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> First just wanted to thank you\n>> for taking the initiative to\n>> > put this together. I think that as the community and\n>> > ecosystem continue to grow, it's going to be an important\n>> > part of the process to have groups like this develop. Hopefully\n>> > they allow us to resist the \"Tyranny of Structurelessness\" without\n>> > resorting to formalized governance processes and systems.\n>>\n>> Huh, lots of reading material behind that phrase.  I'd heard it\n>> before, but hadn't looked it up.\n>>\n>> > > Defining a communication channel is still an open question: IRC,\n>> Slack,\n>> > Discord, Discourse, ...\n>> >\n>> > I would vote against Slack. IRC is probably the best but maybe too\n>> > high a barrier to entry? Publishing logs at least would counter\n>> > concerns of it being exclusive. Maybe discord as an alternative.\n>>\n>> I found Discord immediately wanted a phone number from me.  I think\n>> IRC remains the lowest bar for participants to contribute.\n>>\n>>\n> Agreed, anything that requires a phone number makes it difficult to be\n> pseudonymous.\n>\n> I recommend Matrix, since it doesn't require any privacy invasive\n> information and has e2ee by default for 1-1 conversations.\n>\n> The Matrix room could optionally bridge to IRC if there is a significant\n> demand for that.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220916/f33e741c/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-09-16T18:59:50",
                "message_text_only": "Hi Buck,\n\n> First just wanted to thank you for taking the initiative to\n> put this together. I think that as the community and\n> ecosystem continue to grow, it's going to be an important\n> part of the process to have groups like this develop. Hopefully\n> they allow us to resist the \"Tyranny of Structurelessness\" without\n> resorting to formalized governance processes and systems.\n\nThanks for the words. Effectively, organic WGs are likely good avenues for\nthe ecosystem to make steady and substantial progress during the coming\nfuture. If there is any structure in the development of Bitcoin it's the\nrich network of open, neutral and decentralized communication networks and\nspaces that has been nurtured through the past decade and I hope that's a\ntradition we'll keep maintaining.\n\n> Defining a communication channel is still an open question: IRC, Slack,\nDiscord, Discourse, ...\n\nI would vote against Slack. IRC is probably the best but maybe too high a\nbarrier to entry? Publishing logs at least would counter concerns of it\nbeing exclusive. Maybe discord as an alternative.\n\nI would say I really like IRC too. The strong text-based format, the lack\nof avatar emoji, the low-bar to participate pseudonymously, the leveling\nfield for non-native speakers contrary to audio and the easiness to grab\nthe mics, all features valuable for such a process I think.\n\nIf IRC is still considered a technical high-bar for a standard\ncommunication organ by many community stakeholders, discord is an\nalternative.\n\n> I understand the reason for this but I do have some concerns that\n> it's not as off-topic as most of us would like. It shouldn't\n> be a priority but how any of these primitives end up getting activated\n> is part of the proposal itself in my opinion.\n>\n> I think it also became clear in some of the discussions over the past\n> ~year that maybe there were more concerns than people realized about\n> even the taproot activation process, whether the method used or if it\n> was done too quickly. An example of where there might be\n> some intersection with the WG as proposed is the question of how much\n> research, security audits, etc. are \"enough\" before it should be\n> considered for activation?\n\n>From my understanding, how any of these primitives end up getting activated\nis more a deployment methodology concern. What is more interesting is why\nany of those primitives would be valuable as a Bitcoin upgrade. Beyond\nproposing and refining primitives design and associated use-cases, there is\nsignificant work to collect feedback on many dimensions and set of\ncriterias that matters to community stakeholders to achieve a consistent\nand sound \"why\".\n\nWhere I believe there is an interaction between the \"why\" and \"how\" is that\nduring activation discussion some participant might bring new information\nabout shortcomings of a proposal, and as such if it's estimated relevant\ncould induce a step back to the \"R&D\" whiteboard phase, in a circular\nfeedback loop fashion. As those steps back are not free in terms of\ncommunity engineering resources, especially if deployment code starts to be\nalready disseminated across the ecosystem, I hope in the future we'll leave\nreasonable time (in function of the complexity of the proposal) between\nupgrade phases for grounded objections to raise.\n\n>From my memory, about the taproot activation process it's correct that a\nlot of people had discussions about producing more proof-of-work, e.g back\nin 2019, LN devs were excited to PoC PTLC in the context of the structured\ntaproot review.\nIt didn't happen because it would have implied good refactoring works in\nall implementations for that to happen and coordination with cryptographic\nlibraries dependencies.\n\nIn fact, it's likely the difficulty target for consensus upgrades to be\ndynamic with the complexity of the ecosystem and stakes at risk increasing\nmodulo the amount of Bitcoin engineering resources dedicated.\n\n> Maybe as a way to keep these topics separate, it would make sense\n> for activation to have its own WG. As norms develop around this one,\n> they could inform creating a separate space focused on forwarding\n> research and discussion around how to introduce upgrades to bitcoin.\n\nI think it could be interesting for activation to have its own WG. I\nwouldn't call myself super knowledgeable in upgrades activation. I believe\nit could be worthy for such WG to do the archival work of documentation and\nreferencing well all\nthe previous upgrades discussions, the set of signals and data points that\nhas been deemed as valuable by the community, etc.\n\n> In general it would be nice to have multiple of these groups\n> happening at once, and finding a way that they can operate separate\n> from centralized companies. To my mind, there's no good reason why\n> a supposedly decentralized protocol should have to be focusing on only\n> one set of protocol advancements at a time. The linear way that\n> discussions post-Taproot activation took shape (\"What do you think the\n> next bitcoin softfork should be?\") is a sign of weakness in my opinion.\n> Definitely a big red flag that we should be concerned with.\n\nI agree with the sentiment, that it would be worthy to have multiple groups\nhappening at once, in a asynchronous and decentralized fashion, neutral\nfrom centralized companies or cultural mobs. However, on the linearity of\nthe discussions post-Taproot, from my perspective the reason doesn't have\nto be found in any community stakeholder bottlenecking or whatever but\nrather in the limited subset of experienced Bitcoin protocol engineers we\nhave across the ecosystem. From quick mental maths, the number of active\nfolks with more than 4/5 years of experience and decent practical knowledge\nof the critical Bitcoin subsystems to be able to work on consensus upgrades\nis likely to be evaluated to two dozens. No more. And they're already the\nmost busy of the ecosystem: maintaining the critical pieces of code,\ncatching the bugs during reviews, doing active security research, caring\nabout the Q&A & release process, sharing back the knowledge towards new\ndevs...\n\nOf course, everyone of them as the choice to prioritize consensus upgrades\nover other tasks, but in the long-term it's likely at the detriment of\noutcome valued by the community as a whole (e.g hardening the base-layer\nP2P stack against high grade attacks, solving Lightning numerous liquidity\nissues, etc).\n\nReal weakness is the fact that as a community we're bleeding too much\nseasoned protocol engineers for XYZ reasons.\n\n> * it seems like there might be some opportunities to work with\n> bipbounty.org which grew out of the organic bounty donations that\n> were made towards finding CTV vulnerabilities. For example,\n> if the group develops specific, achievable research goals (building\n> out use cases, researching vulnerabilities or limitations, etc.),\n> bipbounty.org could help support these efforts in a more decentralized\n> way by diversifying funding.\n\nFirst and foremost, thanks to everyone dedicating resources (engineering,\nfinancial, operational, legal, ...) towards making Bitcoin stronger. About\nbipbounty.org, I would like to observe the neutrality of the\ndecision-making process in the fund allocation could be better, especially\nin terms of high-impact and sensitive subjects like consensus upgrades. It\nsounds like the unique team member is also the technical author of the only\nbounty displayed so far... Academics, law and medecine have centuries-long\ntraditions of board or peer-to-peer decision-making structure to allocate\nscientific and engineering ressources with minimal guarantees of neutrality.\n\nI think it would be valuable for this effort to structure for the\nlong-term, it would be great to have more community people dedicating their\nown personal time on doing the hard operational and legal work to make\nthings sustainable. I would say there is definitively a need for more\nBitcoin researchers working on multiple-years scale \"moonshots\" projects.\n\n> * Any thoughts on starting to commit to an in-person meetup to happen\n> ~6 months - 1 year after the start of the regular online meetings?\n> That should be plenty of time for people to plan and formalize\n> a location and it seems like other IRL dev meetups have been\n> very productive in terms of knowledge sharing and setting priorities.\n> An in-person meetup would give a nice goal to work towards and a way\n> to measure progress.\n\nYeah, I think in-person meetups would be very valuable and personally I've\nalways appreciated the knowledge sharing, priorities setting and\nproductivity boost of all the Bitcoin engineering meetings I've had the\nopportunity to attend. 6 months - 1 year after the start of the regular\nonline meetings sounds like a good timeline, there is a preliminary step of\nfolks flooding and exchanging on their expectations, taking the process\nhabits and doing seminal work.\n\nBest,\nAntoine\n\nLe dim. 11 sept. 2022 \u00e0 22:47, Buck O Perley via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi Antoine,\n>\n> First just wanted to thank you for taking the initiative to\n> put this together. I think that as the community and\n> ecosystem continue to grow, it's going to be an important\n> part of the process to have groups like this develop. Hopefully\n> they allow us to resist the \"Tyranny of Structurelessness\" without\n> resorting to formalized governance processes and systems.\n>\n> > Defining a communication channel is still an open question: IRC, Slack,\n> Discord, Discourse, ...\n>\n> I would vote against Slack. IRC is probably the best but maybe too\n> high a barrier to entry? Publishing logs at least would counter\n> concerns of it being exclusive. Maybe discord as an alternative.\n>\n> > About the starting point for regular meetings, I think the good timing is\n> somewhere in November, after the upcoming cycle of Bitcoin conferences,\n>\n> +1\n>\n> > softfork activation discussions will be considered as\n> off-topic and discouraged. This is first and foremost a long-term R&D\n> effort.\n>\n> I understand the reason for this but I do have some concerns that\n> it's not as off-topic as most of us would like. It shouldn't\n> be a priority but how any of these primitives end up getting activated\n> is part of the proposal itself in my opinion.\n>\n> I think it also became clear in some of the discussions over the past\n> ~year that maybe there were more concerns than people realized about\n> even the taproot activation process, whether the method used or if it\n> was done too quickly. An example of where there might be\n> some intersection with the WG as proposed is the question of how much\n> research, security audits, etc. are \"enough\" before it should be\n> considered for activation?\n>\n> Maybe as a way to keep these topics separate, it would make sense\n> for activation to have its own WG. As norms develop around this one,\n> they could inform creating a separate space focused on forwarding\n> research and discussion around how to introduce upgrades to bitcoin.\n>\n> In general it would be nice to have multiple of these groups\n> happening at once, and finding a way that they can operate separate\n> from centralized companies. To my mind, there's no good reason why\n> a supposedly decentralized protocol should have to be focusing on only\n> one set of protocol advancements at a time. The linear way that\n> discussions post-Taproot activation took shape (\"What do you think the\n> next bitcoin softfork should be?\") is a sign of weakness in my opinion.\n> Definitely a big red flag that we should be concerned with.\n>\n> Couple other comments from the proposal/repo:\n>\n> * it seems like there might be some opportunities to work with\n> bipbounty.org which grew out of the organic bounty donations that\n> were made towards finding CTV vulnerabilities. For example,\n> if the group develops specific, achievable research goals (building\n> out use cases, researching vulnerabilities or limitations, etc.),\n> bipbounty.org could help support these efforts in a more decentralized\n> way by diversifying funding.\n>\n> * Any thoughts on starting to commit to an in-person meetup to happen\n> ~6 months - 1 year after the start of the regular online meetings?\n> That should be plenty of time for people to plan and formalize\n> a location and it seems like other IRL dev meetups have been\n> very productive in terms of knowledge sharing and setting priorities.\n> An in-person meetup would give a nice goal to work towards and a way\n> to measure progress.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220916/53376bec/attachment-0001.html>"
            },
            {
                "author": "Devrandom",
                "date": "2022-09-17T07:52:49",
                "message_text_only": "On Fri, Sep 16, 2022 at 9:18 PM Antoine Riard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Buck,\n>\n[...]\n\n>\n> I would vote against Slack. IRC is probably the best but maybe too high a\n> barrier to entry? Publishing logs at least would counter concerns of it\n> being exclusive. Maybe discord as an alternative.\n>\n> I would say I really like IRC too. The strong text-based format, the lack\n> of avatar emoji, the low-bar to participate pseudonymously, the leveling\n> field for non-native speakers contrary to audio and the easiness to grab\n> the mics, all features valuable for such a process I think.\n>\n> If IRC is still considered a technical high-bar for a standard\n> communication organ by many community stakeholders, discord is an\n> alternative.\n>\n\nI would rule out Discord, since it requires phone numbers.  It doesn't\nrequire them for every user, but it's based on some risk measurement.  The\nphone flow is probably more likely to be triggered by VPN / Tor.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220917/e432d554/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "On a new community process to specify covenants",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Buck O Perley",
                "Devrandom",
                "Ryan Grant",
                "Antoine Riard"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 35481
        }
    },
    {
        "title": "[bitcoin-dev] Full Disclosure: Denial of Service in STONEWALLx2 (p2p coinjoin)",
        "thread_messages": [
            {
                "author": "alicexbt",
                "date": "2022-09-10T10:20:48",
                "message_text_only": "This has been assigned CVE-2022-35913: https://www.cve.org/CVERecord?id=CVE-2022-35913\n\n/dev/fd0\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Thursday, July 14th, 2022 at 9:25 AM, alicexbt via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> Hi bitcoin-dev list members,\n> \n> \n> STONEWALLx2[1] is a p2p coinjoin transaction in Samourai wallet. The miner fee is split between both participants of the transaction.\n> \n> \n> ==========================\n> Problem\n> ==========================\n> \n> Antoine Riard shared the details of DoS attack in an [email][2] on 21 June 2022.\n> \n> Proof of Concept:\n> \n> 1) Download Samourai APK, create testnet wallet, get some coins from faucet and claim a paynym in 2 android devices. Consider Bob and Carol are using these devices.\n> \n> 2) Bob and Carol follow each other's paynyms. Carol is the attacker in this case and she could make several paynyms.\n> \n> 3) Bob initiates a Stonewallx2 transaction that requires collaboration with Carol.\n> \n> 4) Carol confirms this request in the app.\n> \n> 5) Carol spends the UTXO from wallet configured in electrum with same seed before Bob could complete the last step and broadcast STONEWALLx2 transaction. It was non RBF [transaction][3] with 1 sat/vbyte fee rate and was unconfirmed during testing.\n> \n> 6) Bob receives an [error][4] in the app when trying to broadcast Stonewallx2 transaction which disappears in a few seconds. The [progress bar][5] appears as if wallet is still trying to broadcast the transaction until Bob manually go back or close the app.\n> \n> \n> ==========================\n> Solution\n> ==========================\n> \n> Suggestions:\n> \n> a) Error message that states collaborator spent her UTXO used in STONEWALLx2, end the p2p coinjoin process, unfollow collaborator's paynym and suggest user to do such transactions with trusted users only for a while.\n> \n> b) Once full RBF is used by some nodes and miners, attacker's transaction could be replaced with a higher fee rate.\n> \n> Conclusions by Samourai:\n> \n> a) As the threat involves the collaborator attacking the spender. We strongly advise that collab spends be done w/ counterparties with which some measure of trust is shared. As such, this does not seem to have an important threat surface.\n> \n> b) Bumping fee won't be simple as fees are shared 50/50 for STONEWALLx2 spends. Change would have to be recalculated for both spender and collaborator. Collab would either have had already authorized a possible fee bump beforehand or would have to be prompted before broadcast.\n> \n> \n> ==========================\n> Timeline\n> ==========================\n> \n> 22 June 2022: I emailed Antoine after testing STONEWALLx2\n> \n> 23 June 2022: I shared the details of attack in a confidential issue in Samourai wallet [repository][6]\n> \n> 07 July 2022: TDevD (Samourai) acknowledged the issue and wanted to discuss it internally with team\n> \n> 14 July 2022: TDevD shared the conclusions\n> \n> \n> ==========================\n> Credits\n> ==========================\n> \n> Antoine Riard discovered DoS vector in p2p coinjoin transactions and helped by responding to emails during testing.\n> \n> \n> [1]: https://docs.samourai.io/spend-tools\n> [2]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-June/020595.html\n> [3]: https://mempool.space/testnet/tx/42db696460a46f196f457779d60acbf46b31accc5414b9eac54b2e785d4c1cbb\n> [4]: https://i.imgur.com/6uf3VJn.png\n> [5]: https://i.imgur.com/W6ITl4G.gif\n> [6]: https://code.samourai.io/wallet/samourai-wallet-android\n> \n> \n> /dev/fd0\n> \n> \n> Sent with Proton Mail secure email.\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Full Disclosure: Denial of Service in STONEWALLx2 (p2p coinjoin)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "alicexbt"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3803
        }
    },
    {
        "title": "[bitcoin-dev] Spookchains: Drivechain Analog with One-Time Trusted Setup & APO",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-09-14T18:31:55",
                "message_text_only": "*also available here on my blog with nicer\nformatting: https://rubin.io/bitcoin/2022/09/14/drivechain-apo/\n<https://rubin.io/bitcoin/2022/09/14/drivechain-apo/>*\n\nThis post draws heavily from Zmnscpxj's fantastic post showing how to\nmake drivechains with recursive covenants. In this post, I will show\nsimilar tricks that can accomplish something similar using ANYPREVOUT\nwith a one time trusted setup ceremony.\n\nThis post presents general techniques that could be applied to many\ndifferent types of covenant.\n\n# Peano Counters\n\nThe first component we need to build is a Peano counter graph. Instead\nof using sha-256, like in Zmnscpxj's scheme, we will use a key and\nbuild a simple 1 to 5 counter that has inc / dec.\n\nAssume a key K1...K5, and a point NUMS which is e.g.\nHashToCurve(\"Spookchains\").\n\nGenerate scripts as follows:\n\n```\n<1 || K1> CHECKSIG\n...\n<1 || K5> CHECKSIG\n```\n\nNow generate 2 signatures under Ki with flags `SIGHASH_SINGLE |\nSIGHASH_ANYONECANPAY | SIGHASH_ANYPREVOUT`.\n\n\n## Rule Increment\nFor each Ki, when `i < 5`, create a signature that covers a\ntransaction described as:\n\n```\nAmount: 1 satoshi\nKey: Tr(NUMS, {<1 || K{i+1}> CHECKSIG})\n```\n\n## Rule Decrement\nFor each Ki, when `i > 1` The second signature should cover:\n```\nAmount: 1 satoshi\nKey: Tr(NUMS, {<1 || K{i-1}> CHECKSIG})\n```\n\n\n\n_Are these really Peano?_ Sort of. While a traditional Peano numeral\nis defined as a structural type, e.g. `Succ(Succ(Zero))`, here we\ndefine them via a Inc / Dec transaction operator, and we have to\nexplicitly bound these Peano numbers since we need a unique key per\nelement. They're at least spiritually similar.\n\n## Instantiation\nPublish a booklet of all the signatures for the Increment and\nDecrement rules.\n\nHonest parties should destroy the secret key sets `k`.\n\n\nTo create a counter, simply spend to output C:\n\n```\nAmount: 1 satoshi\nKey: Tr(NUMS, {<1 || K1> CHECKSIG})\n```\n\n\nThe signature from K1 can be bound to C to 'transition' it to (+1):\n\n```\nAmount: 1 satoshi\nKey: Tr(NUMS, {<1 || K2> CHECKSIG})\n```\n\nWhich can then transition to (+1):\n\n```\nAmount: 1 satoshi\nKey: Tr(NUMS, {<1 || K3> CHECKSIG})\n```\n\nWhich can then transition (-1) to:\n\n```\nAmount: 1 satoshi\nKey: Tr(NUMS, {<1 || K2> CHECKSIG})\n```\n\nThis can repeat indefinitely.\n\n\nWe can generalize this technique from `1...5` to `1...N`.\n\n\n\n# Handling Arbitrary Deposits / Withdrawals\n\n\nOne issue with the design presented previously is that it does not\nhandle arbitrary deposits well.\n\nOne simple way to handle this is to instantiate the protocol for every\namount you'd like to support.\n\nThis is not particularly efficient and requires a lot of storage\nspace.\n\nAlternatively, divide (using base 2 or another base) the deposit\namount into a counter utxo per bit.\n\nFor each bit, instead of creating outputs with 1 satoshi, create\noutputs with 2^i satoshis.\n\nInstead of using keys `K1...KN`, create keys `K^i_j`, where i\nrepresents the number of sats, and j represents the counter. Multiple\nkeys are required per amount otherwise the signatures would be valid\nfor burning funds.\n\n## Splitting and Joining\n\nFor each `K^i_j`, it may also be desirable to allow splitting or\njoining.\n\nSplitting can be accomplished by pre-signing, for every `K^i_j`, where\n`i!=0`, with `SIGHASH_ALL | SIGHASH_ANYPREVOUT`:\n\n```\nInput: 2^i sats with key K^i_j\nOutputs:\n    - 2^i-1 sats to key K^{i-1}_j\n    - 2^i-1 sats to key K^{i-1}_j\n```\n\nJoining can be accomplished by pre-signing, for every `K^i_j`, where\n`i!=MAX`, with `SIGHASH_ALL | SIGHASH_ANYPREVOUT`:\n\n```\nInputs:\n    - 2^i sats with key K^i_j\n    - 2^i sats with key K^i_j\nOutputs:\n    - 2^i+1 sats to key K^{i+1}_j\n```\n\nN.B.: Joining allows for third parties to deposit money in externally,\nthat is not a part of the covenant.\n\n\nThe splitting and joining behavior means that spookchain operators\nwould be empowered to consolidate UTXOs to a smaller number, while\nallowing arbitrary deposits.\n\n\n# One Vote Per Block\n\nTo enforce that only one vote per block mined is allowed, ensure that\nall signatures set the input sequence to 1 block. No CSV is required\nbecause nSequence is in the signatures already.\n\n# Terminal States / Thresholds\n\nWhen a counter reaches the Nth state, it represents a certain amount\nof accumulated work over a period where progress was agreed on for\nsome outcome.\n\nThere should be some viable state transition at this point.\n\nOne solution would be to have the money at this point sent to an\n`OP_TRUE` output, which the miner incrementing that state is\nresponsible for following the rules of the spookchain. Or, it could be\nspecified to be some administrator key / federation for convenience,\nwith a N block timeout that degrades it to fewer signers (eventually\n0) if the federation is dead to allow recovery.\n\nThis would look like, from any `K^i_j`, a signature for a transaction\nputting it into an `OP_TRUE` and immediately spending it. Other\nspookchain miners would be expected to orphan that miner otherwise.\n\n\n# Open States / Proposals\n\n>From a state `K^i_1`, the transaction transitioning to `K^i_2` can be\ntreated as 'special' and the `OP_RETURN` output type can be used to\ncommit to, e.g., the outputs that must be created in when the Terminal\nState is reached. This clarifies the issue of \"what is being voted\non\".\n\nThis method does not *lock in* at a consensus layer what Terminal\nState is being voted on.\n\nIn certain circumstances, without violating the one-time-setup\nconstraint, if a fixed list of withdrawer's addresses is known in\nadvance, the Open States could cover withdrawals to specific\nparticipants, which then must collect a certain number of votes from\nminers.  However, it seems impossible, without new primitives, for an\narbitrary transaction proposal to be voted on.\n\n# Setup Variants\n\n## xpubs\n\nInstead of using randomly generated keys for each state, define each\nto be an xpub and derive a path where it is k/i/j for each\nstate/satoshi amount. This saves some data, and also requires less\nentropy.\n\n### Trustless Data Commit:\n\ncommit to the hash of the entire program spec as a tweak to the xpub,\nso that someone can quickly verify if they have all the signatures you\nare expected to generate if honest.\n\nOne way to do this is to convert a hash to a list of HD Child Numbers\n(9 of them) deterministically, and tweak the xpub by that. This is a\nconvenient, yet inefficient, way to tweak an xpub because the child\nhas a normal derivation path for signing devices.\n\n## Single Party\n\nA single party pre-signs all the transactions for the spookchain, and\nthen deletes their xpriv.\n\nYou trust them to have deleted the key, and signed properly, but you\ndo not trust whoever served you the spookchain blob to have given you\nall the state transitions because of the trustless data commitment.\n\n## MuSig Multi-Party\n\nDefine a MuSig among all participants in the setup ceremony, N-of-N.\n\nNow you simply trust that any one person in the one-time-setup was\nhonest! Very good.\n\n## Unaggregated Multi-Party\n\n\nAllow for unaggregated multi-sig keys in the spec. This grows with\nO(signers), however, it means that a-la-carte you can aggregate setups\nfrom random participants who never interacted / performed setup\nceremonies independently if they signed the same specs.\n\nCan also combine multiple MuSig Multi-Parties in this way.\n\nThis is nice because MuSig inherently implies the parties colluded at\none point to do a MuSig setup, whereas unaggregated multi-sig could be\nperformed with no connectivity between parties.\n\n## Soft Forking Away Trust\n\nSuppose a spookchain becomes popular. You could configure your client\nto reject invalid state transitions, or restrict the spookchain keys\nto only sign with the known signatures. This soft fork would smoothly\nupgrade the trust assumption.\n\n## Symmetry of State Transition Rules & DAG Covenants\n\nWe could have our increment state transitions be done via a trustless\ncovenant, and our backwards state transitions be done via the setup.\n\nThis would look something like the following for state i:\n\n```\nTr(NUMS, {\n    `<sig for state K_{i+1}> <1 || PK_nonsecret> CHECKSIG`,\n    `<1 || Ki> CHECKSIG`\n})\n```\n\nThe advantage of such an optimization is theoretically nice because it\nmeans that *only* the non-destructuring recursive part of the\ncomputation is subject to the one-time-setup trust assumption, which\nmight be of use in various other protocols, where recursivity might\nonly be unlocked e.g. after a timeout (but for spookchains it is used\nat each step).\n\nA compiler writer might perform this task by starting with an\narbitrary abstract graph, and then removing edges selectively (a\nnumber of heuristics may make sense, e.g., to minimize reliance on\none-time-setup or minimize costs) until the graph is a Directed\nAcyclic Graph, consisting of one or more components, compiling those\nwith committed covenants, and then adding the removed edges back using\nthe one-time-setup key materials.\n\n\n# Commentary on Trust and Covenantiness\n\nIs this a covenant? I would say \"yes\". When I defined covenants in my\n_Calculus of Covenants_ post, it was with a particular set of\nassumptions per covenant.\n\nUnder that model, you could, e.g., call a 7-10 multi-sig with specific\ncommitted instructions as 4-10 honest (requires 4 signatories to be\nhonest to do invalid state transition) and 4-10 killable (requires 4\nsignatories to die to have no way of recovering).\n\nFor emulations that are pre-signed, like the varieties used to emulate\nCTV, it is a different model because if your program is correct and\nyou've pre-gotten the signatures for N-N it is 1-N honest (only 1\nparty must be honest to prevent an invalid state transition) and\nunkillable (all parties can safely delete keys).\n\nI model these types of assumptions around liveness and honesty as\ndifferent 'complexity classes' than one another.\n\nWhat I would point out is that with the counter model presented above,\nthis is entirely a pre-signed 1-N honest and unkillable covenant that\nrequires no liveness from signers. Further, with APO, new instances of\nthe covenant do not require a new set of signers, the setup is truly\none-time. Therefore this type of covenant exists in an even lower\ntrust-complexity class than CTV emulation via presigneds, which\nrequires a new federation to sign off on each contract instance.\n\n\nWith that preface, let us analyze this covenant:\n\n\n1) A set of sets of transaction intents (a family), potentially\nrecursive or co-recursive (e.g., the types of state transitions that\ncan be generated).  These intents can also be represented by a\nlanguage that generates the transactions, rather than the literal\ntransactions themselves. We do the family rather than just sets at\nthis level because to instantiate a covenant we must pick a member of\nthe family to use.\n\n\nThe set of sets of transaction intents is to increment / decrement to\na successor or predecessor, or to halve into two instances or double\nvalue by adding funds. Each successor or predecessor is the same type\nof covenant, with the excetion of the first and last, which have some\nspecial rules.\n\n\n2) A verifier generator function that generates a function that\naccepts an intent that is any element of one member of the family of\nintents and a proof for it and rejects others.\n\nThe verifier generator is the simple APO CHECKSIG script.\n\n3) A prover generator function that generates a function that takes an\nintent that is any element of one member of the family and some extra\ndata and returns either a new prover function, a finished proof, or a\nrejection (if not a valid intent).\n\nThe prover generator is the selection of the correct signature from a\ntable for a given script.\n\nRun the prover generator with the private keys present *once* to\ninitialize over all reachable states, and cache the signatures, then\nthe keys may be deleted for future runs.\n\n4) A set of proofs that the Prover, Verifier, and a set of intents are\n\"impedance matched\", that is, all statements the prover can prove and\nall statements the verifier can verify are one-to-one and onto (or\nsomething similar), and that this also is one-to-one and onto with one\nelement of the intents (a set of transactions) and no other.\n\nAt a given key state the only things that may happen are signed\ntransactions, no other data is interpreted off of the stack. Therefore\nthere is perfect impedance match.\n\n\n5) A set of assumptions under which the covenant is verified (e.g., a\nmulti-sig covenant with at least 1-n honesty, a multisig covenant with\nany 3-n honesty required, Sha256 collision resistance, Discrete Log\nHardness, a SGX module being correct).\n\nUniquely, that during the setup phase at least one of the keys\nwere faithfully deleted.\n\nThe usual suspects for any bitcoin transaction are also assumed for\nsecurity.\n\n\n6) Composability:\n\nThe Terminal State can pay out into a pre-specified covenant if\ndesired from any other family of covenants.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220914/c337d266/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-09-19T22:43:44",
                "message_text_only": "Good morning Jeremy,\n\nExcellent work!\n\n\n\n> # Terminal States / Thresholds\n> \n> When a counter reaches the Nth state, it represents a certain amount\n> of accumulated work over a period where progress was agreed on for\n> some outcome.\n> \n> There should be some viable state transition at this point.\n> \n> One solution would be to have the money at this point sent to an\n> `OP_TRUE` output, which the miner incrementing that state is\n> responsible for following the rules of the spookchain.\n\nThis is not quite Drivechain, as Drivechains precommit to the final state transition when the counter reaches threshold and mainchain-level rules prevent the miner who does the final increment from \"swerving the car\" to a different output, whereas use of `OP_TRUE` would not prevent this; the Spookchain could vote for one transition, and then the lucky last miner can output a different one, and only other miners interested in the sidechain would reject them (whereas in the Drivechain case, even nodes that do not care about the sidechain would reject).\n\nStill, it does come awfully close, and the \"ultimate threat\" (\"nuclear option\") in Drivechains is always that everyone upgrades sidechain rules to mainchain rules, which would still work for Spookchains.\nNot sure how comfortable Drivechain proponents would be with this, though.\n\n(But given the demonstrated difficulty in getting consensus changes for the blockchain, I wonder if this nuclear option is even a viable threat)\n\n> Or, it could be\n> specified to be some administrator key / federation for convenience,\n> with a N block timeout that degrades it to fewer signers (eventually\n> 0) if the federation is dead to allow recovery.\n\nSeems similar to the Blockstream separation of the block-signing functionaries from the money-keeping functionaries.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-09-30T02:00:30",
                "message_text_only": "Hi Jeremy,\n\nThanks for bringing back to awareness covenant-based drivechain designs\nagain!\n\nI'm not super familiar with the thousands shades of sidechains, and\nespecially how the variants of pegging mechanisms influence the soundness\nof the game-theory backing up the functionaries execution. However it could\nbe interesting to give security bounds to the defect of any trusted\ncomponent, such as the one-time trusted setup, and the impacts on funds. If\nit's a full-blown loss, a timevalue loss, a privacy leak, etc...\n\nStarted at least an entry for the ZmnSCPxj design:\nhttps://github.com/ariard/bitcoin-contracting-primitives-wg/pull/9\n\nOne interesting point from the OG post:\n> The recursive covenant could, with the help of `OP_CAT` and\n> `OP_CTV`, check that every transaction spending the UTXO has a\n> second output that is an `OP_RETURN` with a commitment to the\n> sidechain block.\n> We can ensure that only one such transaction exists in each\n> mainchain block by adding a `<1> OP_CSV`, ensuring that only one\n> sidechain-commitment transaction can occur on each mainchain\n> block.\n\nSuch recursive-covenant \"embedded\" sidechains could be used as solution to\nthe double-spend of payment pools and channel factories partitions, as an\ninstantiation of a \"on-chain authoritative board\" for partitions statement,\nas described earlier this year, in a quest to solve the high interactivity\nissue affecting those constructions [0].\n\nBest,\nAntoine\n\n[0]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-April/020370.html\n\nLe mer. 14 sept. 2022 \u00e0 14:32, Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> *also available here on my blog with nicer\n> formatting: https://rubin.io/bitcoin/2022/09/14/drivechain-apo/\n> <https://rubin.io/bitcoin/2022/09/14/drivechain-apo/>*\n>\n> This post draws heavily from Zmnscpxj's fantastic post showing how to\n> make drivechains with recursive covenants. In this post, I will show\n> similar tricks that can accomplish something similar using ANYPREVOUT\n> with a one time trusted setup ceremony.\n>\n> This post presents general techniques that could be applied to many\n> different types of covenant.\n>\n> # Peano Counters\n>\n> The first component we need to build is a Peano counter graph. Instead\n> of using sha-256, like in Zmnscpxj's scheme, we will use a key and\n> build a simple 1 to 5 counter that has inc / dec.\n>\n> Assume a key K1...K5, and a point NUMS which is e.g.\n> HashToCurve(\"Spookchains\").\n>\n> Generate scripts as follows:\n>\n> ```\n> <1 || K1> CHECKSIG\n> ...\n> <1 || K5> CHECKSIG\n> ```\n>\n> Now generate 2 signatures under Ki with flags `SIGHASH_SINGLE |\n> SIGHASH_ANYONECANPAY | SIGHASH_ANYPREVOUT`.\n>\n>\n> ## Rule Increment\n> For each Ki, when `i < 5`, create a signature that covers a\n> transaction described as:\n>\n> ```\n> Amount: 1 satoshi\n> Key: Tr(NUMS, {<1 || K{i+1}> CHECKSIG})\n> ```\n>\n> ## Rule Decrement\n> For each Ki, when `i > 1` The second signature should cover:\n> ```\n> Amount: 1 satoshi\n> Key: Tr(NUMS, {<1 || K{i-1}> CHECKSIG})\n> ```\n>\n>\n>\n> _Are these really Peano?_ Sort of. While a traditional Peano numeral\n> is defined as a structural type, e.g. `Succ(Succ(Zero))`, here we\n> define them via a Inc / Dec transaction operator, and we have to\n> explicitly bound these Peano numbers since we need a unique key per\n> element. They're at least spiritually similar.\n>\n> ## Instantiation\n> Publish a booklet of all the signatures for the Increment and\n> Decrement rules.\n>\n> Honest parties should destroy the secret key sets `k`.\n>\n>\n> To create a counter, simply spend to output C:\n>\n> ```\n> Amount: 1 satoshi\n> Key: Tr(NUMS, {<1 || K1> CHECKSIG})\n> ```\n>\n>\n> The signature from K1 can be bound to C to 'transition' it to (+1):\n>\n> ```\n> Amount: 1 satoshi\n> Key: Tr(NUMS, {<1 || K2> CHECKSIG})\n> ```\n>\n> Which can then transition to (+1):\n>\n> ```\n> Amount: 1 satoshi\n> Key: Tr(NUMS, {<1 || K3> CHECKSIG})\n> ```\n>\n> Which can then transition (-1) to:\n>\n> ```\n> Amount: 1 satoshi\n> Key: Tr(NUMS, {<1 || K2> CHECKSIG})\n> ```\n>\n> This can repeat indefinitely.\n>\n>\n> We can generalize this technique from `1...5` to `1...N`.\n>\n>\n>\n> # Handling Arbitrary Deposits / Withdrawals\n>\n>\n> One issue with the design presented previously is that it does not\n> handle arbitrary deposits well.\n>\n> One simple way to handle this is to instantiate the protocol for every\n> amount you'd like to support.\n>\n> This is not particularly efficient and requires a lot of storage\n> space.\n>\n> Alternatively, divide (using base 2 or another base) the deposit\n> amount into a counter utxo per bit.\n>\n> For each bit, instead of creating outputs with 1 satoshi, create\n> outputs with 2^i satoshis.\n>\n> Instead of using keys `K1...KN`, create keys `K^i_j`, where i\n> represents the number of sats, and j represents the counter. Multiple\n> keys are required per amount otherwise the signatures would be valid\n> for burning funds.\n>\n> ## Splitting and Joining\n>\n> For each `K^i_j`, it may also be desirable to allow splitting or\n> joining.\n>\n> Splitting can be accomplished by pre-signing, for every `K^i_j`, where\n> `i!=0`, with `SIGHASH_ALL | SIGHASH_ANYPREVOUT`:\n>\n> ```\n> Input: 2^i sats with key K^i_j\n> Outputs:\n>     - 2^i-1 sats to key K^{i-1}_j\n>     - 2^i-1 sats to key K^{i-1}_j\n> ```\n>\n> Joining can be accomplished by pre-signing, for every `K^i_j`, where\n> `i!=MAX`, with `SIGHASH_ALL | SIGHASH_ANYPREVOUT`:\n>\n> ```\n> Inputs:\n>     - 2^i sats with key K^i_j\n>     - 2^i sats with key K^i_j\n> Outputs:\n>     - 2^i+1 sats to key K^{i+1}_j\n> ```\n>\n> N.B.: Joining allows for third parties to deposit money in externally,\n> that is not a part of the covenant.\n>\n>\n> The splitting and joining behavior means that spookchain operators\n> would be empowered to consolidate UTXOs to a smaller number, while\n> allowing arbitrary deposits.\n>\n>\n> # One Vote Per Block\n>\n> To enforce that only one vote per block mined is allowed, ensure that\n> all signatures set the input sequence to 1 block. No CSV is required\n> because nSequence is in the signatures already.\n>\n> # Terminal States / Thresholds\n>\n> When a counter reaches the Nth state, it represents a certain amount\n> of accumulated work over a period where progress was agreed on for\n> some outcome.\n>\n> There should be some viable state transition at this point.\n>\n> One solution would be to have the money at this point sent to an\n> `OP_TRUE` output, which the miner incrementing that state is\n> responsible for following the rules of the spookchain. Or, it could be\n> specified to be some administrator key / federation for convenience,\n> with a N block timeout that degrades it to fewer signers (eventually\n> 0) if the federation is dead to allow recovery.\n>\n> This would look like, from any `K^i_j`, a signature for a transaction\n> putting it into an `OP_TRUE` and immediately spending it. Other\n> spookchain miners would be expected to orphan that miner otherwise.\n>\n>\n> # Open States / Proposals\n>\n> From a state `K^i_1`, the transaction transitioning to `K^i_2` can be\n> treated as 'special' and the `OP_RETURN` output type can be used to\n> commit to, e.g., the outputs that must be created in when the Terminal\n> State is reached. This clarifies the issue of \"what is being voted\n> on\".\n>\n> This method does not *lock in* at a consensus layer what Terminal\n> State is being voted on.\n>\n> In certain circumstances, without violating the one-time-setup\n> constraint, if a fixed list of withdrawer's addresses is known in\n> advance, the Open States could cover withdrawals to specific\n> participants, which then must collect a certain number of votes from\n> miners.  However, it seems impossible, without new primitives, for an\n> arbitrary transaction proposal to be voted on.\n>\n> # Setup Variants\n>\n> ## xpubs\n>\n> Instead of using randomly generated keys for each state, define each\n> to be an xpub and derive a path where it is k/i/j for each\n> state/satoshi amount. This saves some data, and also requires less\n> entropy.\n>\n> ### Trustless Data Commit:\n>\n> commit to the hash of the entire program spec as a tweak to the xpub,\n> so that someone can quickly verify if they have all the signatures you\n> are expected to generate if honest.\n>\n> One way to do this is to convert a hash to a list of HD Child Numbers\n> (9 of them) deterministically, and tweak the xpub by that. This is a\n> convenient, yet inefficient, way to tweak an xpub because the child\n> has a normal derivation path for signing devices.\n>\n> ## Single Party\n>\n> A single party pre-signs all the transactions for the spookchain, and\n> then deletes their xpriv.\n>\n> You trust them to have deleted the key, and signed properly, but you\n> do not trust whoever served you the spookchain blob to have given you\n> all the state transitions because of the trustless data commitment.\n>\n> ## MuSig Multi-Party\n>\n> Define a MuSig among all participants in the setup ceremony, N-of-N.\n>\n> Now you simply trust that any one person in the one-time-setup was\n> honest! Very good.\n>\n> ## Unaggregated Multi-Party\n>\n>\n> Allow for unaggregated multi-sig keys in the spec. This grows with\n> O(signers), however, it means that a-la-carte you can aggregate setups\n> from random participants who never interacted / performed setup\n> ceremonies independently if they signed the same specs.\n>\n> Can also combine multiple MuSig Multi-Parties in this way.\n>\n> This is nice because MuSig inherently implies the parties colluded at\n> one point to do a MuSig setup, whereas unaggregated multi-sig could be\n> performed with no connectivity between parties.\n>\n> ## Soft Forking Away Trust\n>\n> Suppose a spookchain becomes popular. You could configure your client\n> to reject invalid state transitions, or restrict the spookchain keys\n> to only sign with the known signatures. This soft fork would smoothly\n> upgrade the trust assumption.\n>\n> ## Symmetry of State Transition Rules & DAG Covenants\n>\n> We could have our increment state transitions be done via a trustless\n> covenant, and our backwards state transitions be done via the setup.\n>\n> This would look something like the following for state i:\n>\n> ```\n> Tr(NUMS, {\n>     `<sig for state K_{i+1}> <1 || PK_nonsecret> CHECKSIG`,\n>     `<1 || Ki> CHECKSIG`\n> })\n> ```\n>\n> The advantage of such an optimization is theoretically nice because it\n> means that *only* the non-destructuring recursive part of the\n> computation is subject to the one-time-setup trust assumption, which\n> might be of use in various other protocols, where recursivity might\n> only be unlocked e.g. after a timeout (but for spookchains it is used\n> at each step).\n>\n> A compiler writer might perform this task by starting with an\n> arbitrary abstract graph, and then removing edges selectively (a\n> number of heuristics may make sense, e.g., to minimize reliance on\n> one-time-setup or minimize costs) until the graph is a Directed\n> Acyclic Graph, consisting of one or more components, compiling those\n> with committed covenants, and then adding the removed edges back using\n> the one-time-setup key materials.\n>\n>\n> # Commentary on Trust and Covenantiness\n>\n> Is this a covenant? I would say \"yes\". When I defined covenants in my\n> _Calculus of Covenants_ post, it was with a particular set of\n> assumptions per covenant.\n>\n> Under that model, you could, e.g., call a 7-10 multi-sig with specific\n> committed instructions as 4-10 honest (requires 4 signatories to be\n> honest to do invalid state transition) and 4-10 killable (requires 4\n> signatories to die to have no way of recovering).\n>\n> For emulations that are pre-signed, like the varieties used to emulate\n> CTV, it is a different model because if your program is correct and\n> you've pre-gotten the signatures for N-N it is 1-N honest (only 1\n> party must be honest to prevent an invalid state transition) and\n> unkillable (all parties can safely delete keys).\n>\n> I model these types of assumptions around liveness and honesty as\n> different 'complexity classes' than one another.\n>\n> What I would point out is that with the counter model presented above,\n> this is entirely a pre-signed 1-N honest and unkillable covenant that\n> requires no liveness from signers. Further, with APO, new instances of\n> the covenant do not require a new set of signers, the setup is truly\n> one-time. Therefore this type of covenant exists in an even lower\n> trust-complexity class than CTV emulation via presigneds, which\n> requires a new federation to sign off on each contract instance.\n>\n>\n> With that preface, let us analyze this covenant:\n>\n>\n> 1) A set of sets of transaction intents (a family), potentially\n> recursive or co-recursive (e.g., the types of state transitions that\n> can be generated).  These intents can also be represented by a\n> language that generates the transactions, rather than the literal\n> transactions themselves. We do the family rather than just sets at\n> this level because to instantiate a covenant we must pick a member of\n> the family to use.\n>\n>\n> The set of sets of transaction intents is to increment / decrement to\n> a successor or predecessor, or to halve into two instances or double\n> value by adding funds. Each successor or predecessor is the same type\n> of covenant, with the excetion of the first and last, which have some\n> special rules.\n>\n>\n> 2) A verifier generator function that generates a function that\n> accepts an intent that is any element of one member of the family of\n> intents and a proof for it and rejects others.\n>\n> The verifier generator is the simple APO CHECKSIG script.\n>\n> 3) A prover generator function that generates a function that takes an\n> intent that is any element of one member of the family and some extra\n> data and returns either a new prover function, a finished proof, or a\n> rejection (if not a valid intent).\n>\n> The prover generator is the selection of the correct signature from a\n> table for a given script.\n>\n> Run the prover generator with the private keys present *once* to\n> initialize over all reachable states, and cache the signatures, then\n> the keys may be deleted for future runs.\n>\n> 4) A set of proofs that the Prover, Verifier, and a set of intents are\n> \"impedance matched\", that is, all statements the prover can prove and\n> all statements the verifier can verify are one-to-one and onto (or\n> something similar), and that this also is one-to-one and onto with one\n> element of the intents (a set of transactions) and no other.\n>\n> At a given key state the only things that may happen are signed\n> transactions, no other data is interpreted off of the stack. Therefore\n> there is perfect impedance match.\n>\n>\n> 5) A set of assumptions under which the covenant is verified (e.g., a\n> multi-sig covenant with at least 1-n honesty, a multisig covenant with\n> any 3-n honesty required, Sha256 collision resistance, Discrete Log\n> Hardness, a SGX module being correct).\n>\n> Uniquely, that during the setup phase at least one of the keys\n> were faithfully deleted.\n>\n> The usual suspects for any bitcoin transaction are also assumed for\n> security.\n>\n>\n> 6) Composability:\n>\n> The Terminal State can pay out into a pre-specified covenant if\n> desired from any other family of covenants.\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220929/0b793626/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Spookchains: Drivechain Analog with One-Time Trusted Setup & APO",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Jeremy Rubin",
                "Antoine Riard"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 30463
        }
    },
    {
        "title": "[bitcoin-dev] bitcoin-inquistion: evaluating soft forks on signet",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2022-09-16T07:15:45",
                "message_text_only": "Subhead: \"Nobody expects a Bitcoin Inquistion? C'mon man, *everyone*\nexpects a Bitcoin Inquisition.\"\n\nAs we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\nin the year [0], the question of \"how to successfully get soft fork\nideas from concept to deployment\" doesn't really have a good answer today.\n\nObviously, a centralised solution to this problem exists: we could\nestablish a trusted group, perhaps one containing devs, industry\nrepresentatives, investors, etc, have them review proposals and their\nimplementations, and follow their lead when they decide that a proposal\nhas met their standards and should be widely deployed. Some might even\nsay \"sipa is precisely that group\". The problem with having a group of\nthat nature isn't one of effectiveness, but rather that they are then\nvulnerable to pressure and corruption, which isn't desirable if we want\neveryone using Bitcoin to truly be peers, and often isn't desirable for\nthe prospective members of the group either. So that's not something we\nshould want people to volunteer for, nor is it a duty we should thrust\non people. Or at least, that's my opinion, anyway.\n\nI think any alternative approach to doing consensus changes (while\navoiding a chain split) has to look something like this:\n\n * propose an idea (research phase)\n * implement the idea (development phase)\n * demonstrate the idea is worthwhile (evaluation phase)\n * once everyone is convinced, activate (deployment phase)\n\nWithout an evaluation phase that is thorough enough to convince (almost)\neveryone, I think deployment becomes controversial and perhaps effectively\nimpossible (at least without some trusted leadership group). But with an\nevaluation phase that demonstrates to everyone who's interested that the\nproposal has actual value, minimal cost and no risk, I think activation\ncould be fairly easy and straightforward.\n\nI contend that the most significant problem we have is in the \"evaluation\nphase\". How do you convince enough people that a change is sufficiently\nbeneficial to justify the risk of messing with their money? If you're\nonly trying to convince a few experts, then perhaps you can do that with\npapers and talks; but limiting the evaluation to only a few experts is\neffectively just falling back to the centralised approach.\n\nSo I think that means that part of the \"evaluation phase\" should involve\nimplementing real systems on top of the proposed change, so that you\ncan demonstrate real value from the change. It's easy to say that\n\"CTV can enable vaults\" or \"CTV can make opening a lightning channel\nnon-interactive\" -- but it's harder to go from saying something\nis possible to actually making it happen, so, at least to me, it\nseems reasonable to be skeptical of people claiming benefits without\ndemonstrating they're achievable in practice.\n\nI contend the easiest way we could make it easy to demonstrate a soft\nfork working as designed is to deploy it on the default global signet,\nessentially as soon as it has a fully specified proposal and a reasonably\nhigh-quality implementation.\n\nThe problem with that idea is that creates a conundrum: you can't activate\na soft fork on the default signet without first merging the code into\nbitcoin core, you can't merge the code into bitcoin core until it's been\nfully evaluated, and the way you evaluate it is by activating it on the\ndefault signet?\n\nI think the weakest link in that loop is the first one: what if we did\nactivate soft forks on the default signet prior to the code being merged\ninto core? To that end, I'm proposing a fork of core that I'm calling\n\"bitcoin-inquisition\", with the idea that it branches from stable\nreleases of core and adds support for proposed changes to consensus\n(CTV, ANYPREVOUT, TLUV, OP_CAT, etc...) and potentially also relay\npolicy (relay changes are often implied by consensus changes, but also\npotentially things like package relay).\n\n  https://github.com/bitcoin-inquisition/bitcoin/wiki\n  https://github.com/bitcoin-inquisition/bitcoin/pulls\n\nThe idea being that if you're trying to work on \"upgrading lightning\nto support eltoo\", you can iterate through changes needed to consensus\n(via bitcoin-inquisition) and client apps (cln, lnd, eclair etc), while\ntesting them in public (on signet) and having any/all the pre-existing\nsignet infrastructure available (faucets, explorers etc) without having\nto redeploy it yourself. Having multiple consensus changes deployed in\none place also seems like it might make it easier to compare alternative\napproaches (eg CTV vs ANYPREVOUT vs OP_TXHASH vs OP_TX, etc).\n\nSo that's the concept. For practical purposes, I haven't yet merged\neither CTV or APO support into the bitcoin-inquisition 23.0 branch yet,\nand before actually mining blocks I want to make the signet miner able\nto automatically detect/recover if the bitcoin-inquisition node either\ncrashes or starts producing incompatible blocks.\n\nAnyway, I wanted to post the idea publicly, both to give folks an\nopportunity to poke holes in the idea, or to suggest any further\nimprovements or otherwise do any review before the CTV and APO patches\nget merged.\n\nSome other details that may be of interest.\n\nThe biggest challenge with soft forks and the idea of \"iterating\nthrough changes\" is that making improvements can create a hard fork,\nwhich then forces everyone running old software to update, which can be\npretty inconvenient, especially if you don't actually care about that\nchange. Since signet (and regtest) mining is effectively permissioned,\nwe can avoid that problem by having all these proposed soft forks\ncome with a pre-baked ability to abandon the soft fork (much as David\nHarding described in [1]). Once a soft fork is abandoned, it can either\nbe ignored forever (and later versions of the software can not include\nthe code to enforce it at all), or it can be replaced by a new version\nof the soft fork.\n\nAnother benefit that comes from signet chains being permissioned is\nthat miners can be expected to coordinate upgrading out of band, so\nthere is no need for a 90% signalling threshold. Instead, activation\n(and abandonment) of a soft fork can be triggered by a single block\nsignalling. That further means there is no need for any individual\nblock to signal for multiple forks, and instead of having 29 different\nsignals, we can instead easily have up to 2**29. I've chosen to make\nthe standard signal have 16 bits for specifying a bip number (0-65535)\nand 8 bits for specifying a version of that bip, which seems like it\nshould be more than enough at least for a while. More details at [2].\n\nI'm basing bitcoin-inquisition solely off stable releases. This is partly\nbecause it can be annoying to constantly rebase consensus changes aginst\nbitcoin core's master branch, but also I think it might help consensus\nchanges be easily backported once they pass the \"evaluation phase\"\nand move into the \"deployment phase\".\n\nI'm not sure what level of code quality PRs should have before being\nmerged into bitcoin-inquisition. I think CTV is plenty good enough,\nbut I'm not sure about APO, particularly its test coverage. If you want\nto influence what becomes the tradition here, contributing a review,\nor posting patches against the upsteam branch might be a good start?\n\nDoes this make the global default signet miners, or perhaps the\nbitcoin-inquisition maintainers the \"trusted group\" that we want to\navoid? Hopefully not -- anyone can run their own fork or do their own\nfork of bitcoin core, so if the miners/maintainers start trying to\narbitrarily block proposals they can be worked around without too much\nhassle. And since they're clearly separate from any of the actions that\nneed to be taken for actual deployment once activation is complete,\nthey shouldn't have any ability to unduly promote fork proposals that\npeople aren't fully satisfied are ready for deployment.\n\nCheers,\naj\n\n[0] https://bitcoinops.org/en/newsletters/2022/04/27/#discussion-about-activating-ctv\n\n[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-April/020242.html\n\n[2] https://github.com/bitcoin-inquisition/bitcoin/wiki/Heretical-Deployments"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-09-16T16:46:53",
                "message_text_only": "Apologies for any typos, somewhat jet-lagged atm.\n\nOn 9/16/22 3:15 AM, Anthony Towns via bitcoin-dev wrote:\n> Subhead: \"Nobody expects a Bitcoin Inquistion? C'mon man, *everyone*\n> expects a Bitcoin Inquisition.\"\n> \n> As we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\n> in the year [0], the question of \"how to successfully get soft fork\n> ideas from concept to deployment\" doesn't really have a good answer today.\n\nI strongly disagree with this. Going back many, many years we've had many discussions about fork \nprocess, and the parts people (historically) agreed with tend to be:\n\n(1) come up with an idea\n(2) socialize the idea in the technical community, see if anyone comes up with any major issues or \ncan suggest better ideas which solve the same use-cases in cleaner ways\n(3) propose the concrete idea with a more well-defined strawman, socialize that, get some kind of \nrough consensus in the loosely-defined, subjective, \"technical community\" (ie just ask people and \nadapt to feedback until you have found some kind of average of the opinions of people you, the \nfork-champion, think are reasonably well-informed!).\n(4) okay, admittedly beyond this is a bit less defined, but we can deal with it when we get there.\n\nTurns out, the issue today is a lack of champions following steps 1-3, we can debate what the \ncorrect answer is to step (4) once we actually have people who want to be champions who are willing \nto (humbly) push an idea forward towards rough agreement of the world of technical bitcoiners \n(without which I highly doubt you'd ever see broader-community consensus).\n\nMatt"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-09-17T06:14:05",
                "message_text_only": "On Fri, Sep 16, 2022 at 12:46:53PM -0400, Matt Corallo via bitcoin-dev wrote:\n> On 9/16/22 3:15 AM, Anthony Towns via bitcoin-dev wrote:\n> > As we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\n> > in the year [0], the question of \"how to successfully get soft fork\n> > ideas from concept to deployment\" doesn't really have a good answer today.\n> I strongly disagree with this.\n\nOkay? \"X is good\" is obviously just a statement of opinion, so if you\nwant to disagree, that's obviously allowed. \n\nI also kind of feel like that's the *least* interesting paragraph in the\nentire email to talk further about; if you think the current answer's\nalready good, then the rest of the mail's just about (hopefully) making\nit better, which would be worthwhile anyway?\n\n> Going back many, many years we've had many\n> discussions about fork process, and the parts people (historically) agreed\n> with tend to be:\n> (1) come up with an idea\n> (2) socialize the idea in the technical community, see if anyone comes up\n> with any major issues or can suggest better ideas which solve the same\n> use-cases in cleaner ways\n> (3) propose the concrete idea with a more well-defined strawman, socialize\n> that, get some kind of rough consensus in the loosely-defined, subjective,\n> \"technical community\" (ie just ask people and adapt to feedback until you\n> have found some kind of average of the opinions of people you, the\n> fork-champion, think are reasonably well-informed!).\n> (4) okay, admittedly beyond this is a bit less defined, but we can deal with it when we get there.\n> Turns out, the issue today is a lack of champions following steps 1-3, we\n> can debate what the correct answer is to step (4) once we actually have\n> people who want to be champions who are willing to (humbly) push an idea\n> forward towards rough agreement of the world of technical bitcoiners\n> (without which I highly doubt you'd ever see broader-community consensus).\n\nPersonally, I think this is easily refuted by contradiction.\n\n1) If we did have a good answer for how to progress a soft-fork, then\nthe great consensus cleanup [0] would have made more progress over the\npast 3.5 years. Maybe not all of the ideas in it were unambiguously good\n[1], but personally, I'm convinced at least some of them are, and I\ndon't think I'm alone in thinking that. Even if the excuse is that its\noriginal champion wasn't humble enough, there's something wrong with\nthe process if there doesn't exist some other potential champion with\nthe right balance of humility, confidence, interest and time who could\nhave taken it over in that timeframe.\n\n2) Many will argue that CTV has already done steps (1) through (3) above:\ncertainly there's been an idea, it's been socialised through giving talks,\nhaving discussion forums, having research workshops [2], documenting use\ncases use cases; there's been a concrete implementation for years now,\nwith a test network that supports the proposed feature, and new tools\nthat demonstrate some of the proposed use cases, and while alternative\napproaches have been suggested [3], none of them have even really made\nit to step (2), let alone step (3). So that leaves a few possibilities\nto my mind:\n\n * CTV should be in step (4), and its lack of definition is a problem,\n   and trying the \"deal with it when we get there\" approach is precisely\n   what didn't work back in April.\n\n * The evaluation process is too inconclusive: it should either be\n   saying \"CTV is not good enough, fix these problems\", or \"CTV hasn't\n   sufficiently demonstrated its value/cost, work on X next\", but it\n   isn't.\n\n * Parts (2) to (3) are too hard, and that's preventing alternatives\n   from making progress, which in turn is preventing people from\n   being able to decide whether CTV is the superior approach, or some\n   alternative is.\n\nBut each of those possibilities indicates a significant problem with\nour answer for developing soft forks.\n\nI guess my belief is that it's mostly (2) and (3) being too hard (which\ntaproot overcame because many were excited about it, and CTV overcame\nbecause Jeremy's put a lot of effort into it; but consensus cleanup,\nAPO, simplicity, TXHASH, etc have not similarly overcome at this point),\nwhich leads to the evaluation process being inconclusive when plausible\nalternatives exist. \n\n(In particular, I think having the process be massively difficult is\ngoing to naturally cause any \"humble\" champion to decide that they're\nnot up to the task of following the process through to completion)\n\nAnyway, that's some additional reasons why I believe what I said above,\nin case that's interesting. But like I said at the start, if you still\ndisagree, that's fine of course.\n\nCheers,\naj\n\n[0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-March/016714.html\n    https://github.com/bitcoin/bitcoin/pull/15481\n    https://github.com/bitcoin/bitcoin/pull/15482\n\n[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-March/016765.html\n    https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-March/016763.html\n    https://github.com/bitcoin/bitcoin/pull/15482#issuecomment-469822630\n\n[2] https://utxos.org/workshops/\n\n[3] TXHASH https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019813.html\n    TX https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-May/020450.html\n    Elements-style opcodes https://twitter.com/rusty_twit/status/1518007923896578048\n      cf https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019851.html\n    ANYPREVOUT? https://twitter.com/darosior/status/1474375301262151684\n    Simplicity? https://twitter.com/Mario_Gibney/status/1403890965903859718\n    Lisp? https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020036.html"
            },
            {
                "author": "Matt Corallo",
                "date": "2022-09-17T08:39:07",
                "message_text_only": "On 9/17/22 2:14 AM, Anthony Towns wrote:\n> On Fri, Sep 16, 2022 at 12:46:53PM -0400, Matt Corallo via bitcoin-dev wrote:\n>> On 9/16/22 3:15 AM, Anthony Towns via bitcoin-dev wrote:\n>>> As we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\n>>> in the year [0], the question of \"how to successfully get soft fork\n>>> ideas from concept to deployment\" doesn't really have a good answer today.\n>> I strongly disagree with this.\n> \n> Okay? \"X is good\" is obviously just a statement of opinion, so if you\n> want to disagree, that's obviously allowed.\n> \n> I also kind of feel like that's the *least* interesting paragraph in the\n> entire email to talk further about; if you think the current answer's\n> already good, then the rest of the mail's just about (hopefully) making\n> it better, which would be worthwhile anyway?\n\nNo, I think its at least a good chunk of the \"statement of problem\". Yes, more testing is good, and \nthis project is a way to get that. Cool. But implying that lack of test frameworks is in any \nmaterial way part of the lack of movement on forks in Bitcoin I think is very wrong, so its worth \npointing out, whether the particular project is useful or not is separate.\n\n>> Going back many, many years we've had many\n>> discussions about fork process, and the parts people (historically) agreed\n>> with tend to be:\n>> (1) come up with an idea\n>> (2) socialize the idea in the technical community, see if anyone comes up\n>> with any major issues or can suggest better ideas which solve the same\n>> use-cases in cleaner ways\n>> (3) propose the concrete idea with a more well-defined strawman, socialize\n>> that, get some kind of rough consensus in the loosely-defined, subjective,\n>> \"technical community\" (ie just ask people and adapt to feedback until you\n>> have found some kind of average of the opinions of people you, the\n>> fork-champion, think are reasonably well-informed!).\n>> (4) okay, admittedly beyond this is a bit less defined, but we can deal with it when we get there.\n>> Turns out, the issue today is a lack of champions following steps 1-3, we\n>> can debate what the correct answer is to step (4) once we actually have\n>> people who want to be champions who are willing to (humbly) push an idea\n>> forward towards rough agreement of the world of technical bitcoiners\n>> (without which I highly doubt you'd ever see broader-community consensus).\n> \n> Personally, I think this is easily refuted by contradiction.\n> \n> 1) If we did have a good answer for how to progress a soft-fork, then\n> the great consensus cleanup [0] would have made more progress over the\n> past 3.5 years\n\nNo? Who is the champion for it? I haven't been. No one else is obliged to take up the reins and run \nwith it, that's not how open-source works. And no one has emerged who has strong interest in doing \nso, and that's totally fine. It means it hasn't made any progress, but that's an indication that no \none feels strongly enough about it that its risen to the top of their personal priority list so \nclearly doesn't *need* to make progress.\n\n> Maybe not all of the ideas in it were unambiguously good\n> [1], but personally, I'm convinced at least some of them are, and I\n> don't think I'm alone in thinking that. Even if the excuse is that its\n> original champion wasn't humble enough, there's something wrong with\n> the process if there doesn't exist some other potential champion with\n> the right balance of humility, confidence, interest and time who could\n> have taken it over in that timeframe.\n\nNo? Its not up to the community to find a champion for someone who wants a fork to happen. Either \nsomeone thinks its a good enough idea that they step up, or no one does. If no one does, then so be \nit. If the original proper (me, in this case) thought it was that important then its *their* \nresponsibility to be the champion, no one else's.\n\n> 2) Many will argue that CTV has already done steps (1) through (3) above:\n> certainly there's been an idea, it's been socialised through giving talks,\n> having discussion forums, having research workshops [2], documenting use\n> cases use cases; there's been a concrete implementation for years now,\n> with a test network that supports the proposed feature, and new tools\n> that demonstrate some of the proposed use cases, and while alternative\n> approaches have been suggested [3], none of them have even really made\n> it to step (2), let alone step (3).\n\nI don't really see how you can make this argument seriously. Honestly, if a soft-fork BIP only has \none author on the list, then I'm not sure one can argue that step (3) has really been completed, and \nmaybe not even step (2).\n\n> So that leaves a few possibilities\n> to my mind:\n\n>   * CTV should be in step (4), and its lack of definition is a problem,\n>     and trying the \"deal with it when we get there\" approach is precisely\n>     what didn't work back in April.\n> \n>   * The evaluation process is too inconclusive: it should either be\n>     saying \"CTV is not good enough, fix these problems\", or \"CTV hasn't\n>     sufficiently demonstrated its value/cost, work on X next\", but it\n>     isn't.\n> \n>   * Parts (2) to (3) are too hard, and that's preventing alternatives\n>     from making progress, which in turn is preventing people from\n>     being able to decide whether CTV is the superior approach, or some\n>     alternative is.\n\nI think this is most of it, but its not that they're too hard, its that people are *too busy*. There \nseemed to be more positive feedback, for example, to Rusty's proposal, but being the champion for a \nsoft-fork is a full-time job for months on end, and last I checked Rusty has a lightning \nimplementation to maintain, which tends to be a more-than-full-time job already.\n\nTo my knowledge, no one but Jeremy has made any serious attempt at being the champion for a \nsoft-fork since Taproot, and before that Segwit (if someone reading this who contributes to Core \nalready wants to, and isn't sure how to, there's lots of people who would happily mentor you! I'm \nsure you can figure out who to reach out to!).\n\nMatt"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-09-17T15:53:48",
                "message_text_only": "I agree with Matt. The less said about the \"Aw shucks Jeremy didn't know that CTV didn't have community consensus at the time\" [0] and \"it was the lack of process that was the problem\" the better. If people don't care about lack of community consensus there is no process in a permissionless, open source community that can stop them or direct them down a more patient, productive path (I tried). I think it is a shame because I think (maybe I'm wrong) at least in the technical community there is an understanding that long term Bitcoin is far from finished in exhausting its potential and we do need people who will work on changes that we'll need in the long term.\n\nThere are a few interesting things in here though. I'm not convinced by the name (bitcoin-inquisition, shedpaint, shedpaint, let's park that for the moment) but I do like the idea of signet having soft fork proposals enabled on it [1] whether that be CTV, APO etc and if that requires more of the signet code to be moved out of the Core repo so be it. I'm surprised more isn't being done on Liquid already with what possible future functionality is (and could be) enabled [2] there but maybe there is more happening than I'm aware of. Protocols or use cases built out and demonstrated on signet (and/or Liquid/sidechains) seem an obvious stepping stone to me for convincing the community that it is potentially worth taking the chain split risk for a particular upgrade. It is a long slog to get community consensus on an upgrade (Taproot certainly was a slog) but I think the vast majority of us think Taproot was worth that slog and Bitcoin would be poorer today without it.\n\nThe Great Consensus Cleanup is an interesting example in that I get Matt doesn't have time to champion it or focus on it with his LDK commitments but I have no idea where it would rank on his long term priority list if he wasn't working on LDK. Similarly I have no idea what people who understand this evolving system much better than I do are thinking with regards to say adding new opcodes, sighash flags versus say waiting on Simplicity and possibly adding new functionality within that potential upgrade. For people like me who are extremely unlikely to propose their own consensus change(s) getting some signal on what to spend time digging into would be useful rather than second guessing what people are thinking. There is a danger that you flirt with perceived public roadmaps when possible authority figures stick their necks out and effectively say \"I'm not in charge but in an imaginary world where I was this is my current thinking of the ordering in which we could improve this system long term. But this could change depending on x, y and z and possible upgrades are only ready when they're ready and they have community consensus.\" There is no way people don't play these exercises in their minds. I do, I just have very few answers :) I personally think APO is in prime position to improve Lightning channel state management with eltoo and if it enables some covenant schemes too that seems like an added bonus. On APO versus waiting for APO like functionality in Simplicity I have no idea. Work on APO/eltoo and Simplicity both seem to be progressing in parallel so the APO versus Simplicity discussion perhaps rests on whether people think certain covenants should only really be enabled once we have the security guarantees of Simplicity [3] (if at all).\n\nAntoine's covenant R&D effort [4] seems really promising and I hope the shenanigans from earlier in the year don't put people off from engaging with that. Hopefully we can see more exploration, development and research in covenants (e.g. this excellent research paper \"Bitcoin Covenants: Three Ways to Control the Future\" [5]) and we can foster a community which has very high standards, is open to new ideas and new research but can avoid these months long resisting chain split fights. I think the discussion would be much more interesting and much more productive if people didn't have to think \"If I express a view now it will be used to attack me personally later\" or worse \"If I express a view now it will be used to justify an upcoming chain split\". \n\n[0]: https://gist.github.com/michaelfolkson/352a503f4f9fc5de89af528d86a1b718\n[1]: https://bitcoin.stackexchange.com/questions/98642/can-we-experiment-on-signet-with-multiple-proposed-soft-forks-whilst-maintaining\n[2]: https://bitcoin.stackexchange.com/questions/109764/what-opcodes-are-supported-on-liquid-but-not-yet-on-bitcoin\n[3]: https://bitcoinops.org/en/topics/simplicity/\n[4]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-September/020912.html\n[5]: https://arxiv.org/pdf/2006.16714.pdf\n\n--\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n\n------- Original Message -------\nOn Saturday, September 17th, 2022 at 09:39, Matt Corallo via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> \n> On 9/17/22 2:14 AM, Anthony Towns wrote:\n> \n> > On Fri, Sep 16, 2022 at 12:46:53PM -0400, Matt Corallo via bitcoin-dev wrote:\n> > \n> > > On 9/16/22 3:15 AM, Anthony Towns via bitcoin-dev wrote:\n> > > \n> > > > As we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\n> > > > in the year [0], the question of \"how to successfully get soft fork\n> > > > ideas from concept to deployment\" doesn't really have a good answer today.\n> > > > I strongly disagree with this.\n> > \n> > Okay? \"X is good\" is obviously just a statement of opinion, so if you\n> > want to disagree, that's obviously allowed.\n> > \n> > I also kind of feel like that's the least interesting paragraph in the\n> > entire email to talk further about; if you think the current answer's\n> > already good, then the rest of the mail's just about (hopefully) making\n> > it better, which would be worthwhile anyway?\n> \n> \n> No, I think its at least a good chunk of the \"statement of problem\". Yes, more testing is good, and\n> this project is a way to get that. Cool. But implying that lack of test frameworks is in any\n> material way part of the lack of movement on forks in Bitcoin I think is very wrong, so its worth\n> pointing out, whether the particular project is useful or not is separate.\n> \n> > > Going back many, many years we've had many\n> > > discussions about fork process, and the parts people (historically) agreed\n> > > with tend to be:\n> > > (1) come up with an idea\n> > > (2) socialize the idea in the technical community, see if anyone comes up\n> > > with any major issues or can suggest better ideas which solve the same\n> > > use-cases in cleaner ways\n> > > (3) propose the concrete idea with a more well-defined strawman, socialize\n> > > that, get some kind of rough consensus in the loosely-defined, subjective,\n> > > \"technical community\" (ie just ask people and adapt to feedback until you\n> > > have found some kind of average of the opinions of people you, the\n> > > fork-champion, think are reasonably well-informed!).\n> > > (4) okay, admittedly beyond this is a bit less defined, but we can deal with it when we get there.\n> > > Turns out, the issue today is a lack of champions following steps 1-3, we\n> > > can debate what the correct answer is to step (4) once we actually have\n> > > people who want to be champions who are willing to (humbly) push an idea\n> > > forward towards rough agreement of the world of technical bitcoiners\n> > > (without which I highly doubt you'd ever see broader-community consensus).\n> > \n> > Personally, I think this is easily refuted by contradiction.\n> > \n> > 1) If we did have a good answer for how to progress a soft-fork, then\n> > the great consensus cleanup [0] would have made more progress over the\n> > past 3.5 years\n> \n> \n> No? Who is the champion for it? I haven't been. No one else is obliged to take up the reins and run\n> with it, that's not how open-source works. And no one has emerged who has strong interest in doing\n> so, and that's totally fine. It means it hasn't made any progress, but that's an indication that no\n> one feels strongly enough about it that its risen to the top of their personal priority list so\n> clearly doesn't need to make progress.\n> \n> > Maybe not all of the ideas in it were unambiguously good\n> > [1], but personally, I'm convinced at least some of them are, and I\n> > don't think I'm alone in thinking that. Even if the excuse is that its\n> > original champion wasn't humble enough, there's something wrong with\n> > the process if there doesn't exist some other potential champion with\n> > the right balance of humility, confidence, interest and time who could\n> > have taken it over in that timeframe.\n> \n> \n> No? Its not up to the community to find a champion for someone who wants a fork to happen. Either\n> someone thinks its a good enough idea that they step up, or no one does. If no one does, then so be\n> it. If the original proper (me, in this case) thought it was that important then its their\n> responsibility to be the champion, no one else's.\n> \n> > 2) Many will argue that CTV has already done steps (1) through (3) above:\n> > certainly there's been an idea, it's been socialised through giving talks,\n> > having discussion forums, having research workshops [2], documenting use\n> > cases use cases; there's been a concrete implementation for years now,\n> > with a test network that supports the proposed feature, and new tools\n> > that demonstrate some of the proposed use cases, and while alternative\n> > approaches have been suggested [3], none of them have even really made\n> > it to step (2), let alone step (3).\n> \n> \n> I don't really see how you can make this argument seriously. Honestly, if a soft-fork BIP only has\n> one author on the list, then I'm not sure one can argue that step (3) has really been completed, and\n> maybe not even step (2).\n> \n> > So that leaves a few possibilities\n> > to my mind:\n> \n> > * CTV should be in step (4), and its lack of definition is a problem,\n> > and trying the \"deal with it when we get there\" approach is precisely\n> > what didn't work back in April.\n> > \n> > * The evaluation process is too inconclusive: it should either be\n> > saying \"CTV is not good enough, fix these problems\", or \"CTV hasn't\n> > sufficiently demonstrated its value/cost, work on X next\", but it\n> > isn't.\n> > \n> > * Parts (2) to (3) are too hard, and that's preventing alternatives\n> > from making progress, which in turn is preventing people from\n> > being able to decide whether CTV is the superior approach, or some\n> > alternative is.\n> \n> \n> I think this is most of it, but its not that they're too hard, its that people are too busy. There\n> seemed to be more positive feedback, for example, to Rusty's proposal, but being the champion for a\n> soft-fork is a full-time job for months on end, and last I checked Rusty has a lightning\n> implementation to maintain, which tends to be a more-than-full-time job already.\n> \n> To my knowledge, no one but Jeremy has made any serious attempt at being the champion for a\n> soft-fork since Taproot, and before that Segwit (if someone reading this who contributes to Core\n> already wants to, and isn't sure how to, there's lots of people who would happily mentor you! I'm\n> sure you can figure out who to reach out to!).\n> \n> Matt\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "alicexbt",
                "date": "2022-09-18T12:27:43",
                "message_text_only": "Hi Michael,\n\n> I agree with Matt. The less said about the \"Aw shucks Jeremy didn't know that CTV didn't have community consensus at the time\" [0] and \"it was the lack of process that was the problem\" the better. \n\nThe linked gist cannot be taken seriously and I am not sure why you keep sharing it as some document to prove there was no technical consensus for BIP 119. Nadav has already mentioned this in the comments. If you care about community consensus, maybe feedback about the links in that gist should also be respected. There was chaos, misinformation and lot of drama on twitter. Some people that opposed CTV on twitter still have no clue what CTV actually does and a few were super enthusiastic because of the author for BIP 119.\n\n> I'm not convinced by the name (bitcoin-inquisition, shedpaint, shedpaint, let's park that for the moment) but I do like the idea of signet having soft fork proposals enabled on it [1] whether that be CTV, APO etc and if that requires more of the signet code to be moved out of the Core repo so be it.\n\nGood to see some positivity, finally. Because tx introspection aka covenants would help everyone involved in bitcoin. This idea of experimenting with soft forks on signet together with research and meetings suggested by Antoine should help in better evaluation phase with less drama, politics etc. and more technical discussions to reach a goal.\n\n> I'm surprised more isn't being done on Liquid already with what possible future functionality is (and could be) enabled [2] there but maybe there is more happening than I'm aware of. \n\n1)Nobody uses Liquid. Signet has more activity than Liquid.\n2)Testing something on Liquid will be completely different as its a separate blockchain with some similarities.\n\nI have summarized a few other positives of testing soft forks on signet based on AJ's email:\n\na)Better evaluation\nb)PR implementing soft fork could be reviewed and merged outside core\nc)Testing on signet with pre-existing signet infrastructure\nd)Can deploy multiple consensus changes so easier to compare alternative approaches (eg CTV vs ANYPREVOUT vs OP_TXHASH vs OP_TX, etc)\ne)Pre-baked ability to abandon the soft fork\nf)No need to regularly rebase consensus changes against bitcoin core's master branch\n\n/dev/fd0\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Saturday, September 17th, 2022 at 3:53 PM, Michael Folkson via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> I agree with Matt. The less said about the \"Aw shucks Jeremy didn't know that CTV didn't have community consensus at the time\" [0] and \"it was the lack of process that was the problem\" the better. If people don't care about lack of community consensus there is no process in a permissionless, open source community that can stop them or direct them down a more patient, productive path (I tried). I think it is a shame because I think (maybe I'm wrong) at least in the technical community there is an understanding that long term Bitcoin is far from finished in exhausting its potential and we do need people who will work on changes that we'll need in the long term.\n> \n> There are a few interesting things in here though. I'm not convinced by the name (bitcoin-inquisition, shedpaint, shedpaint, let's park that for the moment) but I do like the idea of signet having soft fork proposals enabled on it [1] whether that be CTV, APO etc and if that requires more of the signet code to be moved out of the Core repo so be it. I'm surprised more isn't being done on Liquid already with what possible future functionality is (and could be) enabled [2] there but maybe there is more happening than I'm aware of. Protocols or use cases built out and demonstrated on signet (and/or Liquid/sidechains) seem an obvious stepping stone to me for convincing the community that it is potentially worth taking the chain split risk for a particular upgrade. It is a long slog to get community consensus on an upgrade (Taproot certainly was a slog) but I think the vast majority of us think Taproot was worth that slog and Bitcoin would be poorer today without it.\n> \n> The Great Consensus Cleanup is an interesting example in that I get Matt doesn't have time to champion it or focus on it with his LDK commitments but I have no idea where it would rank on his long term priority list if he wasn't working on LDK. Similarly I have no idea what people who understand this evolving system much better than I do are thinking with regards to say adding new opcodes, sighash flags versus say waiting on Simplicity and possibly adding new functionality within that potential upgrade. For people like me who are extremely unlikely to propose their own consensus change(s) getting some signal on what to spend time digging into would be useful rather than second guessing what people are thinking. There is a danger that you flirt with perceived public roadmaps when possible authority figures stick their necks out and effectively say \"I'm not in charge but in an imaginary world where I was this is my current thinking of the ordering in which we could improve this system\n> long term. But this could change depending on x, y and z and possible upgrades are only ready when they're ready and they have community consensus.\" There is no way people don't play these exercises in their minds. I do, I just have very few answers :) I personally think APO is in prime position to improve Lightning channel state management with eltoo and if it enables some covenant schemes too that seems like an added bonus. On APO versus waiting for APO like functionality in Simplicity I have no idea. Work on APO/eltoo and Simplicity both seem to be progressing in parallel so the APO versus Simplicity discussion perhaps rests on whether people think certain covenants should only really be enabled once we have the security guarantees of Simplicity [3] (if at all).\n> \n> Antoine's covenant R&D effort [4] seems really promising and I hope the shenanigans from earlier in the year don't put people off from engaging with that. Hopefully we can see more exploration, development and research in covenants (e.g. this excellent research paper \"Bitcoin Covenants: Three Ways to Control the Future\" [5]) and we can foster a community which has very high standards, is open to new ideas and new research but can avoid these months long resisting chain split fights. I think the discussion would be much more interesting and much more productive if people didn't have to think \"If I express a view now it will be used to attack me personally later\" or worse \"If I express a view now it will be used to justify an upcoming chain split\".\n> \n> [0]: https://gist.github.com/michaelfolkson/352a503f4f9fc5de89af528d86a1b718\n> [1]: https://bitcoin.stackexchange.com/questions/98642/can-we-experiment-on-signet-with-multiple-proposed-soft-forks-whilst-maintaining\n> [2]: https://bitcoin.stackexchange.com/questions/109764/what-opcodes-are-supported-on-liquid-but-not-yet-on-bitcoin\n> [3]: https://bitcoinops.org/en/topics/simplicity/\n> [4]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-September/020912.html\n> [5]: https://arxiv.org/pdf/2006.16714.pdf\n> \n> --\n> Michael Folkson\n> Email: michaelfolkson at protonmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n> \n> \n> ------- Original Message -------\n> On Saturday, September 17th, 2022 at 09:39, Matt Corallo via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n> \n> \n> > On 9/17/22 2:14 AM, Anthony Towns wrote:\n> > \n> > > On Fri, Sep 16, 2022 at 12:46:53PM -0400, Matt Corallo via bitcoin-dev wrote:\n> > > \n> > > > On 9/16/22 3:15 AM, Anthony Towns via bitcoin-dev wrote:\n> > > > \n> > > > > As we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\n> > > > > in the year [0], the question of \"how to successfully get soft fork\n> > > > > ideas from concept to deployment\" doesn't really have a good answer today.\n> > > > > I strongly disagree with this.\n> > > \n> > > Okay? \"X is good\" is obviously just a statement of opinion, so if you\n> > > want to disagree, that's obviously allowed.\n> > > \n> > > I also kind of feel like that's the least interesting paragraph in the\n> > > entire email to talk further about; if you think the current answer's\n> > > already good, then the rest of the mail's just about (hopefully) making\n> > > it better, which would be worthwhile anyway?\n> > \n> > No, I think its at least a good chunk of the \"statement of problem\". Yes, more testing is good, and\n> > this project is a way to get that. Cool. But implying that lack of test frameworks is in any\n> > material way part of the lack of movement on forks in Bitcoin I think is very wrong, so its worth\n> > pointing out, whether the particular project is useful or not is separate.\n> > \n> > > > Going back many, many years we've had many\n> > > > discussions about fork process, and the parts people (historically) agreed\n> > > > with tend to be:\n> > > > (1) come up with an idea\n> > > > (2) socialize the idea in the technical community, see if anyone comes up\n> > > > with any major issues or can suggest better ideas which solve the same\n> > > > use-cases in cleaner ways\n> > > > (3) propose the concrete idea with a more well-defined strawman, socialize\n> > > > that, get some kind of rough consensus in the loosely-defined, subjective,\n> > > > \"technical community\" (ie just ask people and adapt to feedback until you\n> > > > have found some kind of average of the opinions of people you, the\n> > > > fork-champion, think are reasonably well-informed!).\n> > > > (4) okay, admittedly beyond this is a bit less defined, but we can deal with it when we get there.\n> > > > Turns out, the issue today is a lack of champions following steps 1-3, we\n> > > > can debate what the correct answer is to step (4) once we actually have\n> > > > people who want to be champions who are willing to (humbly) push an idea\n> > > > forward towards rough agreement of the world of technical bitcoiners\n> > > > (without which I highly doubt you'd ever see broader-community consensus).\n> > > \n> > > Personally, I think this is easily refuted by contradiction.\n> > > \n> > > 1) If we did have a good answer for how to progress a soft-fork, then\n> > > the great consensus cleanup [0] would have made more progress over the\n> > > past 3.5 years\n> > \n> > No? Who is the champion for it? I haven't been. No one else is obliged to take up the reins and run\n> > with it, that's not how open-source works. And no one has emerged who has strong interest in doing\n> > so, and that's totally fine. It means it hasn't made any progress, but that's an indication that no\n> > one feels strongly enough about it that its risen to the top of their personal priority list so\n> > clearly doesn't need to make progress.\n> > \n> > > Maybe not all of the ideas in it were unambiguously good\n> > > [1], but personally, I'm convinced at least some of them are, and I\n> > > don't think I'm alone in thinking that. Even if the excuse is that its\n> > > original champion wasn't humble enough, there's something wrong with\n> > > the process if there doesn't exist some other potential champion with\n> > > the right balance of humility, confidence, interest and time who could\n> > > have taken it over in that timeframe.\n> > \n> > No? Its not up to the community to find a champion for someone who wants a fork to happen. Either\n> > someone thinks its a good enough idea that they step up, or no one does. If no one does, then so be\n> > it. If the original proper (me, in this case) thought it was that important then its their\n> > responsibility to be the champion, no one else's.\n> > \n> > > 2) Many will argue that CTV has already done steps (1) through (3) above:\n> > > certainly there's been an idea, it's been socialised through giving talks,\n> > > having discussion forums, having research workshops [2], documenting use\n> > > cases use cases; there's been a concrete implementation for years now,\n> > > with a test network that supports the proposed feature, and new tools\n> > > that demonstrate some of the proposed use cases, and while alternative\n> > > approaches have been suggested [3], none of them have even really made\n> > > it to step (2), let alone step (3).\n> > \n> > I don't really see how you can make this argument seriously. Honestly, if a soft-fork BIP only has\n> > one author on the list, then I'm not sure one can argue that step (3) has really been completed, and\n> > maybe not even step (2).\n> > \n> > > So that leaves a few possibilities\n> > > to my mind:\n> > \n> > > * CTV should be in step (4), and its lack of definition is a problem,\n> > > and trying the \"deal with it when we get there\" approach is precisely\n> > > what didn't work back in April.\n> > > \n> > > * The evaluation process is too inconclusive: it should either be\n> > > saying \"CTV is not good enough, fix these problems\", or \"CTV hasn't\n> > > sufficiently demonstrated its value/cost, work on X next\", but it\n> > > isn't.\n> > > \n> > > * Parts (2) to (3) are too hard, and that's preventing alternatives\n> > > from making progress, which in turn is preventing people from\n> > > being able to decide whether CTV is the superior approach, or some\n> > > alternative is.\n> > \n> > I think this is most of it, but its not that they're too hard, its that people are too busy. There\n> > seemed to be more positive feedback, for example, to Rusty's proposal, but being the champion for a\n> > soft-fork is a full-time job for months on end, and last I checked Rusty has a lightning\n> > implementation to maintain, which tends to be a more-than-full-time job already.\n> > \n> > To my knowledge, no one but Jeremy has made any serious attempt at being the champion for a\n> > soft-fork since Taproot, and before that Segwit (if someone reading this who contributes to Core\n> > already wants to, and isn't sure how to, there's lots of people who would happily mentor you! I'm\n> > sure you can figure out who to reach out to!).\n> > \n> > Matt\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-09-18T18:44:31",
                "message_text_only": "Hi alicexbt\n\n> Good to see some positivity, finally.\n\nI had enthusiasm for this concept of enabling proposed soft fork functionality on signet 2 years ago [0]. Nothing has changed, still enthusiastic :) Not enthusiastic about the months wasted on unnecessary contentious soft fork drama since but can't change the past.\n\n> 1)Nobody uses Liquid. Signet has more activity than Liquid.\n  2)Testing something on Liquid will be completely different as its \n  a separate blockchain with some similarities.\n\nPerhaps you should take your own advice with regards to positivity (or at least have more of an open mind) with regards Liquid and sidechains. Signet Bitcoin are totally free [1] and experimentation doesn't ever result in loss of real monetary value so you would expect more experimentation on signet versus Liquid long term. However, building protocols and prototypes with real monetary value is a step up from doing so with worthless signet coins. So I don't really see them as direct competitors. Some things take a lot longer to come to fruition than others but the original vision [2] of sidechains still makes perfect sense to me. Competing sets of consensus rules aren't possible on a single mainnet blockchain. Hence you either go the sidechain(-like) route or you go the altcoin route if you want to take the step up from signet/testnet and start using real monetary value. I much prefer the sidechain model to the altcoin route myself. Especially when in say vaults you do want the equivalent of Bitcoin to be locked up rather than a more volatile altcoin.\n\nThanks\nMichael\n\n[0]: https://bitcoin.stackexchange.com/questions/98642/can-we-experiment-on-signet-with-multiple-proposed-soft-forks-whilst-maintaining\n[1]: https://signetfaucet.com/\n[2]: https://blockstream.com/sidechains.pdf\n\n--\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n\n------- Original Message -------\nOn Sunday, September 18th, 2022 at 13:27, alicexbt <alicexbt at protonmail.com> wrote:\n\n\n> Hi Michael,\n> \n> > I agree with Matt. The less said about the \"Aw shucks Jeremy didn't know that CTV didn't have community consensus at the time\" 0 and \"it was the lack of process that was the problem\" the better.\n> \n> \n> The linked gist cannot be taken seriously and I am not sure why you keep sharing it as some document to prove there was no technical consensus for BIP 119. Nadav has already mentioned this in the comments. If you care about community consensus, maybe feedback about the links in that gist should also be respected. There was chaos, misinformation and lot of drama on twitter. Some people that opposed CTV on twitter still have no clue what CTV actually does and a few were super enthusiastic because of the author for BIP 119.\n> \n> > I'm not convinced by the name (bitcoin-inquisition, shedpaint, shedpaint, let's park that for the moment) but I do like the idea of signet having soft fork proposals enabled on it 1 whether that be CTV, APO etc and if that requires more of the signet code to be moved out of the Core repo so be it.\n> \n> \n> Good to see some positivity, finally. Because tx introspection aka covenants would help everyone involved in bitcoin. This idea of experimenting with soft forks on signet together with research and meetings suggested by Antoine should help in better evaluation phase with less drama, politics etc. and more technical discussions to reach a goal.\n> \n> > I'm surprised more isn't being done on Liquid already with what possible future functionality is (and could be) enabled 2 there but maybe there is more happening than I'm aware of.\n> \n> \n> 1)Nobody uses Liquid. Signet has more activity than Liquid.\n> 2)Testing something on Liquid will be completely different as its a separate blockchain with some similarities.\n> \n> I have summarized a few other positives of testing soft forks on signet based on AJ's email:\n> \n> a)Better evaluation\n> b)PR implementing soft fork could be reviewed and merged outside core\n> c)Testing on signet with pre-existing signet infrastructure\n> d)Can deploy multiple consensus changes so easier to compare alternative approaches (eg CTV vs ANYPREVOUT vs OP_TXHASH vs OP_TX, etc)\n> e)Pre-baked ability to abandon the soft fork\n> f)No need to regularly rebase consensus changes against bitcoin core's master branch\n> \n> /dev/fd0\n> \n> Sent with Proton Mail secure email.\n> \n> ------- Original Message -------\n> On Saturday, September 17th, 2022 at 3:53 PM, Michael Folkson via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n> \n> \n> > I agree with Matt. The less said about the \"Aw shucks Jeremy didn't know that CTV didn't have community consensus at the time\" 0 and \"it was the lack of process that was the problem\" the better. If people don't care about lack of community consensus there is no process in a permissionless, open source community that can stop them or direct them down a more patient, productive path (I tried). I think it is a shame because I think (maybe I'm wrong) at least in the technical community there is an understanding that long term Bitcoin is far from finished in exhausting its potential and we do need people who will work on changes that we'll need in the long term.\n> > \n> > There are a few interesting things in here though. I'm not convinced by the name (bitcoin-inquisition, shedpaint, shedpaint, let's park that for the moment) but I do like the idea of signet having soft fork proposals enabled on it 1 whether that be CTV, APO etc and if that requires more of the signet code to be moved out of the Core repo so be it. I'm surprised more isn't being done on Liquid already with what possible future functionality is (and could be) enabled 2 there but maybe there is more happening than I'm aware of. Protocols or use cases built out and demonstrated on signet (and/or Liquid/sidechains) seem an obvious stepping stone to me for convincing the community that it is potentially worth taking the chain split risk for a particular upgrade. It is a long slog to get community consensus on an upgrade (Taproot certainly was a slog) but I think the vast majority of us think Taproot was worth that slog and Bitcoin would be poorer today without it.\n> > \n> > The Great Consensus Cleanup is an interesting example in that I get Matt doesn't have time to champion it or focus on it with his LDK commitments but I have no idea where it would rank on his long term priority list if he wasn't working on LDK. Similarly I have no idea what people who understand this evolving system much better than I do are thinking with regards to say adding new opcodes, sighash flags versus say waiting on Simplicity and possibly adding new functionality within that potential upgrade. For people like me who are extremely unlikely to propose their own consensus change(s) getting some signal on what to spend time digging into would be useful rather than second guessing what people are thinking. There is a danger that you flirt with perceived public roadmaps when possible authority figures stick their necks out and effectively say \"I'm not in charge but in an imaginary world where I was this is my current thinking of the ordering in which we could improve this system\n> > long term. But this could change depending on x, y and z and possible upgrades are only ready when they're ready and they have community consensus.\" There is no way people don't play these exercises in their minds. I do, I just have very few answers :) I personally think APO is in prime position to improve Lightning channel state management with eltoo and if it enables some covenant schemes too that seems like an added bonus. On APO versus waiting for APO like functionality in Simplicity I have no idea. Work on APO/eltoo and Simplicity both seem to be progressing in parallel so the APO versus Simplicity discussion perhaps rests on whether people think certain covenants should only really be enabled once we have the security guarantees of Simplicity 3 (if at all).\n> > \n> > Antoine's covenant R&D effort 4 seems really promising and I hope the shenanigans from earlier in the year don't put people off from engaging with that. Hopefully we can see more exploration, development and research in covenants (e.g. this excellent research paper \"Bitcoin Covenants: Three Ways to Control the Future\" 5) and we can foster a community which has very high standards, is open to new ideas and new research but can avoid these months long resisting chain split fights. I think the discussion would be much more interesting and much more productive if people didn't have to think \"If I express a view now it will be used to attack me personally later\" or worse \"If I express a view now it will be used to justify an upcoming chain split\".\n> > \n> > --\n> > Michael Folkson\n> > Email: michaelfolkson at protonmail.com\n> > Keybase: michaelfolkson\n> > PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n> > \n> > ------- Original Message -------\n> > On Saturday, September 17th, 2022 at 09:39, Matt Corallo via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> > \n> > > On 9/17/22 2:14 AM, Anthony Towns wrote:\n> > > \n> > > > On Fri, Sep 16, 2022 at 12:46:53PM -0400, Matt Corallo via bitcoin-dev wrote:\n> > > > \n> > > > > On 9/16/22 3:15 AM, Anthony Towns via bitcoin-dev wrote:\n> > > > > \n> > > > > > As we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\n> > > > > > in the year 0, the question of \"how to successfully get soft fork\n> > > > > > ideas from concept to deployment\" doesn't really have a good answer today.\n> > > > > > I strongly disagree with this.\n> > > > \n> > > > Okay? \"X is good\" is obviously just a statement of opinion, so if you\n> > > > want to disagree, that's obviously allowed.\n> > > > \n> > > > I also kind of feel like that's the least interesting paragraph in the\n> > > > entire email to talk further about; if you think the current answer's\n> > > > already good, then the rest of the mail's just about (hopefully) making\n> > > > it better, which would be worthwhile anyway?\n> > > \n> > > No, I think its at least a good chunk of the \"statement of problem\". Yes, more testing is good, and\n> > > this project is a way to get that. Cool. But implying that lack of test frameworks is in any\n> > > material way part of the lack of movement on forks in Bitcoin I think is very wrong, so its worth\n> > > pointing out, whether the particular project is useful or not is separate.\n> > > \n> > > > > Going back many, many years we've had many\n> > > > > discussions about fork process, and the parts people (historically) agreed\n> > > > > with tend to be:\n> > > > > (1) come up with an idea\n> > > > > (2) socialize the idea in the technical community, see if anyone comes up\n> > > > > with any major issues or can suggest better ideas which solve the same\n> > > > > use-cases in cleaner ways\n> > > > > (3) propose the concrete idea with a more well-defined strawman, socialize\n> > > > > that, get some kind of rough consensus in the loosely-defined, subjective,\n> > > > > \"technical community\" (ie just ask people and adapt to feedback until you\n> > > > > have found some kind of average of the opinions of people you, the\n> > > > > fork-champion, think are reasonably well-informed!).\n> > > > > (4) okay, admittedly beyond this is a bit less defined, but we can deal with it when we get there.\n> > > > > Turns out, the issue today is a lack of champions following steps 1-3, we\n> > > > > can debate what the correct answer is to step (4) once we actually have\n> > > > > people who want to be champions who are willing to (humbly) push an idea\n> > > > > forward towards rough agreement of the world of technical bitcoiners\n> > > > > (without which I highly doubt you'd ever see broader-community consensus).\n> > > > \n> > > > Personally, I think this is easily refuted by contradiction.\n> > > > \n> > > > 1) If we did have a good answer for how to progress a soft-fork, then\n> > > > the great consensus cleanup 0 would have made more progress over the\n> > > > past 3.5 years\n> > > \n> > > No? Who is the champion for it? I haven't been. No one else is obliged to take up the reins and run\n> > > with it, that's not how open-source works. And no one has emerged who has strong interest in doing\n> > > so, and that's totally fine. It means it hasn't made any progress, but that's an indication that no\n> > > one feels strongly enough about it that its risen to the top of their personal priority list so\n> > > clearly doesn't need to make progress.\n> > > \n> > > > Maybe not all of the ideas in it were unambiguously good\n> > > > 1, but personally, I'm convinced at least some of them are, and I\n> > > > don't think I'm alone in thinking that. Even if the excuse is that its\n> > > > original champion wasn't humble enough, there's something wrong with\n> > > > the process if there doesn't exist some other potential champion with\n> > > > the right balance of humility, confidence, interest and time who could\n> > > > have taken it over in that timeframe.\n> > > \n> > > No? Its not up to the community to find a champion for someone who wants a fork to happen. Either\n> > > someone thinks its a good enough idea that they step up, or no one does. If no one does, then so be\n> > > it. If the original proper (me, in this case) thought it was that important then its their\n> > > responsibility to be the champion, no one else's.\n> > > \n> > > > 2) Many will argue that CTV has already done steps (1) through (3) above:\n> > > > certainly there's been an idea, it's been socialised through giving talks,\n> > > > having discussion forums, having research workshops 2, documenting use\n> > > > cases use cases; there's been a concrete implementation for years now,\n> > > > with a test network that supports the proposed feature, and new tools\n> > > > that demonstrate some of the proposed use cases, and while alternative\n> > > > approaches have been suggested 3, none of them have even really made\n> > > > it to step (2), let alone step (3).\n> > > \n> > > I don't really see how you can make this argument seriously. Honestly, if a soft-fork BIP only has\n> > > one author on the list, then I'm not sure one can argue that step (3) has really been completed, and\n> > > maybe not even step (2).\n> > > \n> > > > So that leaves a few possibilities\n> > > > to my mind:\n> > > \n> > > > * CTV should be in step (4), and its lack of definition is a problem,\n> > > > and trying the \"deal with it when we get there\" approach is precisely\n> > > > what didn't work back in April.\n> > > > \n> > > > * The evaluation process is too inconclusive: it should either be\n> > > > saying \"CTV is not good enough, fix these problems\", or \"CTV hasn't\n> > > > sufficiently demonstrated its value/cost, work on X next\", but it\n> > > > isn't.\n> > > > \n> > > > * Parts (2) to (3) are too hard, and that's preventing alternatives\n> > > > from making progress, which in turn is preventing people from\n> > > > being able to decide whether CTV is the superior approach, or some\n> > > > alternative is.\n> > > \n> > > I think this is most of it, but its not that they're too hard, its that people are too busy. There\n> > > seemed to be more positive feedback, for example, to Rusty's proposal, but being the champion for a\n> > > soft-fork is a full-time job for months on end, and last I checked Rusty has a lightning\n> > > implementation to maintain, which tends to be a more-than-full-time job already.\n> > > \n> > > To my knowledge, no one but Jeremy has made any serious attempt at being the champion for a\n> > > soft-fork since Taproot, and before that Segwit (if someone reading this who contributes to Core\n> > > already wants to, and isn't sure how to, there's lots of people who would happily mentor you! I'm\n> > > sure you can figure out who to reach out to!).\n> > > \n> > > Matt\n> > > _______________________________________________\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-09-18T18:47:38",
                "message_text_only": "Hi AJ,\n\nThanks to setup a new laboratory for consensus upgrades experiment! Idea\nwas exposed during the last LN Summit, glad to see there is a useful fork\nnow.\n\nWhile I think one of the problem particular in the current stagnation about\nconsensus upgrades has been well scoped by your proposal, namely on\nformalizing the thorough analysis process to which an upgrade proposal\nshould be subject too, there are more issues or bounds to wonder on, at\nleast from my perspective.\n\nLaying out fine-grained stages of the development process (research,\ndevelopment, evaluation, deployment) sounds compelling to bring clarity to\neveryone. However, I doubt it captures well the more realistic, chaotic\nprocess from which new Bitcoin ideas and techniques are emerging. In\npractice, consensus upgrades, akin to the sustenance of new scientific\ntheories or engineering principles in the wider creative areas of life, are\nfollowing an uncertain path of hazardous steps: seminal half-baked\nintuitions, whiteboard modeling, code or quantitative experiments, loose\nset of ideas pollination, peers feedbacks integration, etc before to mature\nin some solidified proposals.\n\nSaid succinctly, in the genesis of creative ideas, evaluation doesn't\nhappen at a single clear point but all along the idea lifetime, where this\nevaluation is as much done by the original author than its peers and a\nwider audience. Sometimes really formally, e.g in academics with PhD thesis\ndefense. For Bitcoin, rather than to \"declare\" on the when and where\nupgrades evaluation should happen once for all, I think a more open\nevaluation process we can carry on is gathering and maintaining the factual\nmaterial and reasoning frameworks around solidified proposals, on which\neach community stakeholder individually can assign a grounded judgement.\nThose judgments are likely sources of new refinement of the upgrades\nthemselves.\n\nUnder that perspective, I believe a functional upgrades experimentation\nplatform as proposed by bitcoin-inquisition is very valuable, as it should\nallow upgrades \"champions\" (CTV, ANYPREVOUT, TLUV, \"fraud proofs\" ops\nprimitives, etc) to loop faster in the R&D cycles, raise earlier awareness\non their work existence and as it's all open to assemble team around their\nproposals. (Effectively, covenants upgrades and their associated use-cases\noffered so much complexity that it's becoming less and less a one-man\njob...).\n\nI would still expose a concern to not downgrade in the pure empiricism in\nmatter of consensus upgrades. I.e, slowly emerging the norm of a working\nprototype running on bitcoin-inquisition` as a determining factor of the\nsoundness of a proposal. E.g with \"upgrading lightning to support eltoo\", a\nrunning e2e won't save us to think the thousands variants of pinnings, the\ngame-theory soundness of a eltoo as mechanism in face of congestions, the\nevolvability of APO with more known upgrades proposals or the\nimplementation complexity of a fully fleshed-out state machine and more\nquestions.\n\nThat said, a e2e implementation, partial or complete, would at least make\nthe serious analysis process easier. Moreover, the benefit of having e2e\nimplems runnable by everyone on bitcoin-inquisition would likely lower the\nbar to have independent consensus upgrade analysis, likely a source of new\nrelevant feedback.\n\nI can only share the sentiment expressed that this alternative open\napproach of consensus changes avoids the gradual establishment of a trusted\ngroup, even informal. In the past, to the best of my knowledge, most of the\nsubstance of the Taproot softfork daily development happened on\nsemi-offline communication channels and the strong design rational\ndecisions at CoreDev editions. While not discrediting the high-quality\nfeedback than one can gather during those types of in-person engineering\nmeetings, for neutrality and openness of the Bitcoin upgrades process it\ncould be great to only consider them as source of feedbacks and move\nprogressively the crux of the upgrades R&D process online, open to anyone\ninterested. Moreover, it would bind more adequately the reality of a\ngrowing development ecosystem, where we have to deal with an increasing\ndiversity of technical, social and geographical community stakeholders. I\nacknowledge there is a hard challenge to maintain high-signal, low-noise\nonline communication channels and spaces about context-rich issues like\nupgrades, however that might be the type of challenge we have to solve if\nwe care about everyone using Bitcoin to truly be peers. At least my 2 sats.\n\nAbout the risk of latent centralization of global default signets\nminers/bitcoin-inquisition maintainers, I don't think I'm worried about it.\nWith time, I would guess we might have multiple experimental signets with\ndifferent series of patches, as some patches might invalidate the\nobservations of another upgrade. E,g if one implements the \"weird\" ideas\nabout changes in the block reward issuance schedule discussed during the\nsummer, another one might not want \"noise\" interferences with new\nfee-bumping primitives as the miner incentives are modified. About the\nbitcoin-inquisition fork maintainers themselves becoming gatekeepers of\nconsensus upgrades changes, best we can do is maintain high-quality\ndocumentation and knowledge base to lower the forking cost of the platform\nfor any community stakeholder.\n\nAnyway, I hold the belief that the more initiatives we see to modernize the\n\"consensus-upgrades\" production factory in order to scale with the current\ndimensions of the Bitcoin ecosystem, better we're. I hope the upcoming\nContracting Primitives WG will be able to document and discuss some of the\nrelevant experiments run on bitcoin-inquisition. Time and work will tell\nhow they fit all together, where they complement each other and synergies\nthat are nurtured.\n\nSpeaking for myself, looking forward to experimenting with CoinPool code\ncomponents on bitcoin-inquistion in the future!\n\nBest,\nAntoine\n\nLe ven. 16 sept. 2022 \u00e0 03:16, Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Subhead: \"Nobody expects a Bitcoin Inquistion? C'mon man, *everyone*\n> expects a Bitcoin Inquisition.\"\n>\n> As we've seen from the attempt at a CHECKTEMPLATEVERIFY activation earlier\n> in the year [0], the question of \"how to successfully get soft fork\n> ideas from concept to deployment\" doesn't really have a good answer today.\n>\n> Obviously, a centralised solution to this problem exists: we could\n> establish a trusted group, perhaps one containing devs, industry\n> representatives, investors, etc, have them review proposals and their\n> implementations, and follow their lead when they decide that a proposal\n> has met their standards and should be widely deployed. Some might even\n> say \"sipa is precisely that group\". The problem with having a group of\n> that nature isn't one of effectiveness, but rather that they are then\n> vulnerable to pressure and corruption, which isn't desirable if we want\n> everyone using Bitcoin to truly be peers, and often isn't desirable for\n> the prospective members of the group either. So that's not something we\n> should want people to volunteer for, nor is it a duty we should thrust\n> on people. Or at least, that's my opinion, anyway.\n>\n> I think any alternative approach to doing consensus changes (while\n> avoiding a chain split) has to look something like this:\n>\n>  * propose an idea (research phase)\n>  * implement the idea (development phase)\n>  * demonstrate the idea is worthwhile (evaluation phase)\n>  * once everyone is convinced, activate (deployment phase)\n>\n> Without an evaluation phase that is thorough enough to convince (almost)\n> everyone, I think deployment becomes controversial and perhaps effectively\n> impossible (at least without some trusted leadership group). But with an\n> evaluation phase that demonstrates to everyone who's interested that the\n> proposal has actual value, minimal cost and no risk, I think activation\n> could be fairly easy and straightforward.\n>\n> I contend that the most significant problem we have is in the \"evaluation\n> phase\". How do you convince enough people that a change is sufficiently\n> beneficial to justify the risk of messing with their money? If you're\n> only trying to convince a few experts, then perhaps you can do that with\n> papers and talks; but limiting the evaluation to only a few experts is\n> effectively just falling back to the centralised approach.\n>\n> So I think that means that part of the \"evaluation phase\" should involve\n> implementing real systems on top of the proposed change, so that you\n> can demonstrate real value from the change. It's easy to say that\n> \"CTV can enable vaults\" or \"CTV can make opening a lightning channel\n> non-interactive\" -- but it's harder to go from saying something\n> is possible to actually making it happen, so, at least to me, it\n> seems reasonable to be skeptical of people claiming benefits without\n> demonstrating they're achievable in practice.\n>\n> I contend the easiest way we could make it easy to demonstrate a soft\n> fork working as designed is to deploy it on the default global signet,\n> essentially as soon as it has a fully specified proposal and a reasonably\n> high-quality implementation.\n>\n> The problem with that idea is that creates a conundrum: you can't activate\n> a soft fork on the default signet without first merging the code into\n> bitcoin core, you can't merge the code into bitcoin core until it's been\n> fully evaluated, and the way you evaluate it is by activating it on the\n> default signet?\n>\n> I think the weakest link in that loop is the first one: what if we did\n> activate soft forks on the default signet prior to the code being merged\n> into core? To that end, I'm proposing a fork of core that I'm calling\n> \"bitcoin-inquisition\", with the idea that it branches from stable\n> releases of core and adds support for proposed changes to consensus\n> (CTV, ANYPREVOUT, TLUV, OP_CAT, etc...) and potentially also relay\n> policy (relay changes are often implied by consensus changes, but also\n> potentially things like package relay).\n>\n>   https://github.com/bitcoin-inquisition/bitcoin/wiki\n>   https://github.com/bitcoin-inquisition/bitcoin/pulls\n>\n> The idea being that if you're trying to work on \"upgrading lightning\n> to support eltoo\", you can iterate through changes needed to consensus\n> (via bitcoin-inquisition) and client apps (cln, lnd, eclair etc), while\n> testing them in public (on signet) and having any/all the pre-existing\n> signet infrastructure available (faucets, explorers etc) without having\n> to redeploy it yourself. Having multiple consensus changes deployed in\n> one place also seems like it might make it easier to compare alternative\n> approaches (eg CTV vs ANYPREVOUT vs OP_TXHASH vs OP_TX, etc).\n>\n> So that's the concept. For practical purposes, I haven't yet merged\n> either CTV or APO support into the bitcoin-inquisition 23.0 branch yet,\n> and before actually mining blocks I want to make the signet miner able\n> to automatically detect/recover if the bitcoin-inquisition node either\n> crashes or starts producing incompatible blocks.\n>\n> Anyway, I wanted to post the idea publicly, both to give folks an\n> opportunity to poke holes in the idea, or to suggest any further\n> improvements or otherwise do any review before the CTV and APO patches\n> get merged.\n>\n> Some other details that may be of interest.\n>\n> The biggest challenge with soft forks and the idea of \"iterating\n> through changes\" is that making improvements can create a hard fork,\n> which then forces everyone running old software to update, which can be\n> pretty inconvenient, especially if you don't actually care about that\n> change. Since signet (and regtest) mining is effectively permissioned,\n> we can avoid that problem by having all these proposed soft forks\n> come with a pre-baked ability to abandon the soft fork (much as David\n> Harding described in [1]). Once a soft fork is abandoned, it can either\n> be ignored forever (and later versions of the software can not include\n> the code to enforce it at all), or it can be replaced by a new version\n> of the soft fork.\n>\n> Another benefit that comes from signet chains being permissioned is\n> that miners can be expected to coordinate upgrading out of band, so\n> there is no need for a 90% signalling threshold. Instead, activation\n> (and abandonment) of a soft fork can be triggered by a single block\n> signalling. That further means there is no need for any individual\n> block to signal for multiple forks, and instead of having 29 different\n> signals, we can instead easily have up to 2**29. I've chosen to make\n> the standard signal have 16 bits for specifying a bip number (0-65535)\n> and 8 bits for specifying a version of that bip, which seems like it\n> should be more than enough at least for a while. More details at [2].\n>\n> I'm basing bitcoin-inquisition solely off stable releases. This is partly\n> because it can be annoying to constantly rebase consensus changes aginst\n> bitcoin core's master branch, but also I think it might help consensus\n> changes be easily backported once they pass the \"evaluation phase\"\n> and move into the \"deployment phase\".\n>\n> I'm not sure what level of code quality PRs should have before being\n> merged into bitcoin-inquisition. I think CTV is plenty good enough,\n> but I'm not sure about APO, particularly its test coverage. If you want\n> to influence what becomes the tradition here, contributing a review,\n> or posting patches against the upsteam branch might be a good start?\n>\n> Does this make the global default signet miners, or perhaps the\n> bitcoin-inquisition maintainers the \"trusted group\" that we want to\n> avoid? Hopefully not -- anyone can run their own fork or do their own\n> fork of bitcoin core, so if the miners/maintainers start trying to\n> arbitrarily block proposals they can be worked around without too much\n> hassle. And since they're clearly separate from any of the actions that\n> need to be taken for actual deployment once activation is complete,\n> they shouldn't have any ability to unduly promote fork proposals that\n> people aren't fully satisfied are ready for deployment.\n>\n> Cheers,\n> aj\n>\n> [0]\n> https://bitcoinops.org/en/newsletters/2022/04/27/#discussion-about-activating-ctv\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-April/020242.html\n>\n> [2]\n> https://github.com/bitcoin-inquisition/bitcoin/wiki/Heretical-Deployments\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220918/e58b78ad/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-09-19T10:05:47",
                "message_text_only": "On Sun, Sep 18, 2022 at 02:47:38PM -0400, Antoine Riard via bitcoin-dev wrote:\n> Said succinctly, in the genesis of creative ideas, evaluation doesn't\n> happen at a single clear point but all along the idea lifetime, where this\n> evaluation is as much done by the original author than its peers and a\n> wider audience.\n\nSure. I definitely didn't mean to imply a waterfall development model,\nor that the phases wouldn't overlap etc.\n\n> I would still expose a concern to not downgrade in the pure empiricism in\n> matter of consensus upgrades. I.e, slowly emerging the norm of a working\n> prototype running on bitcoin-inquisition` as a determining factor of the\n> soundness of a proposal. E.g with \"upgrading lightning to support eltoo\", a\n> running e2e won't save us to think the thousands variants of pinnings, the\n> game-theory soundness of a eltoo as mechanism in face of congestions, the\n> evolvability of APO with more known upgrades proposals or the\n> implementation complexity of a fully fleshed-out state machine and more\n> questions.\n\nI agree here; but I think not doing prototypes also hinders thinking\nabout all the thousands of details in a fork. It's easy to handwave\ndetails away when describing things on a whiteboard; and only realise\nthey're trickier than you thought when you go to implement things.\n\n> E,g if one implements the \"weird\" ideas\n> about changes in the block reward issuance schedule discussed during the\n> summer, another one might not want \"noise\" interferences with new\n> fee-bumping primitives as the miner incentives are modified. \n\n(I don't think \"miner incentives\" are really something that can be\ninvestigated on signet. You can assume how miners will respond to\nincentives and program the mining software to act that way; but there's\nno competitive pressure in signet mining so I don't think that really\ndemonstrates anything very much. Likewise, there's much less demand for\nblockspace on signet than on mainnet, so it's probably hard to experiment\nwith \"fee incentives\" too)\n\n> I hope the upcoming\n> Contracting Primitives WG will be able to document and discuss some of the\n> relevant experiments run on bitcoin-inquisition. \n\nLikewise.\n\n(Lots trimmed due to either agreeing with it or having nothing to add)\n\nCheers,\naj"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-09-28T11:48:32",
                "message_text_only": "I've given this some extra thought and discussed with others who may later chime in on this thread. I'm now convinced this should be done on a custom public signet rather than on the default signet. SegWit was added to a new testnet (Segnet) for testing rather than the pre-existing testnet and I think future soft fork proposals should follow a similar approach.\n\nEven if there is community consensus on what soft fork proposals should be added to the default signet today (which may or may not be case) I find it highly unlikely this will always be the case. We then get into the situation where the block signers (currently AJ and Kalle) are the gatekeepers on what soft fork proposals are added. The default signet is directly supported with the -signet flag in Bitcoin Core. Even if we are moving the proposed soft fork code to an external repo (to avoid it being merged into Core prematurely) it is still determining what soft forks are accessible from the signet flag in Bitcoin Core. I don't think it is fair on the signet block signers to put them in that position and I don't think it is wise to put other Bitcoin Core contributors/maintainers in the position of having to defend why some proposed soft forks are accessible on the default signet while others aren't.\n\nThe default signet was a long term project to address the unreliability and weaknesses of testnet. Many default signet users won't be interested in testing soft fork proposals and it is not reasonable for them to be subject to a stalling or forked blockchain because changes to a soft fork proposal or a buggy soft fork proposal pushed to the default signet makes previous valid/invalid transactions invalid/valid. If they want to test proposed soft forks on a custom signet they are opting in to possible disruption rather than it being forced upon them.\n\nBy focusing on custom signets rather than the default signet it also allows for more experimentation. Don't like the choices of which soft fork proposals have been added to bitcoin-inquisition? Set up your own custom signet with a different set of soft fork proposals and get users for your custom signet on a level playing field to bitcoin-inquisition. A soft fork proposal is found to be strictly inferior to another soft fork proposal? Just spin down the custom signet with that inferior soft fork proposal on it without impacting default signet users.\n\nSo TL;DR still enthusiastic about this concept. Just with a strong preference that it is done on a custom signet rather than on the default signet.\n\nThanks\nMichael\n\n--\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n\n------- Original Message -------\nOn Monday, September 19th, 2022 at 11:05, Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> On Sun, Sep 18, 2022 at 02:47:38PM -0400, Antoine Riard via bitcoin-dev wrote:\n> \n> > Said succinctly, in the genesis of creative ideas, evaluation doesn't\n> > happen at a single clear point but all along the idea lifetime, where this\n> > evaluation is as much done by the original author than its peers and a\n> > wider audience.\n> \n> \n> Sure. I definitely didn't mean to imply a waterfall development model,\n> or that the phases wouldn't overlap etc.\n> \n> > I would still expose a concern to not downgrade in the pure empiricism in\n> > matter of consensus upgrades. I.e, slowly emerging the norm of a working\n> > prototype running on bitcoin-inquisition` as a determining factor of the\n> > soundness of a proposal. E.g with \"upgrading lightning to support eltoo\", a\n> > running e2e won't save us to think the thousands variants of pinnings, the\n> > game-theory soundness of a eltoo as mechanism in face of congestions, the\n> > evolvability of APO with more known upgrades proposals or the\n> > implementation complexity of a fully fleshed-out state machine and more\n> > questions.\n> \n> \n> I agree here; but I think not doing prototypes also hinders thinking\n> about all the thousands of details in a fork. It's easy to handwave\n> details away when describing things on a whiteboard; and only realise\n> they're trickier than you thought when you go to implement things.\n> \n> > E,g if one implements the \"weird\" ideas\n> > about changes in the block reward issuance schedule discussed during the\n> > summer, another one might not want \"noise\" interferences with new\n> > fee-bumping primitives as the miner incentives are modified.\n> \n> \n> (I don't think \"miner incentives\" are really something that can be\n> investigated on signet. You can assume how miners will respond to\n> incentives and program the mining software to act that way; but there's\n> no competitive pressure in signet mining so I don't think that really\n> demonstrates anything very much. Likewise, there's much less demand for\n> blockspace on signet than on mainnet, so it's probably hard to experiment\n> with \"fee incentives\" too)\n> \n> > I hope the upcoming\n> > Contracting Primitives WG will be able to document and discuss some of the\n> > relevant experiments run on bitcoin-inquisition.\n> \n> \n> Likewise.\n> \n> (Lots trimmed due to either agreeing with it or having nothing to add)\n> \n> Cheers,\n> aj\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "alicexbt",
                "date": "2022-09-28T20:01:46",
                "message_text_only": "Hi Michael,\n\n> We then get into the situation where the block signers (currently AJ and Kalle) are the gatekeepers on what soft fork proposals are added.\n\nThings that could solve the gatekeeping issue:\n\n1) Adding more maintainers that are experienced enough to review consensus code. \n2) Every soft fork that is implemented on signet should be discussed on mailing list before merging the pull request.\n3) Pull request that implements a soft fork should have at least 2 ACKs from the maintainers, 3 ACKs from developers who have either authored or reviewed enough consensus related pull requests in bitcoin core and 1 ACK from maintainers of other implementations (knots, btcd, libbitcoin, bcoin etc.)\n4) Every technical NACK from any developer or user in the pull request should be resolved/answered before merging the pull request.\n\nThis was not the case in [implementing BIP][1] 119 however it could be used in future or something similar that everyone agrees upon including AJ and Kalle. Pull request implementing BIP 119 in bitcoin core is already reviewed by lot of developers and I think AJ found a [bug][2] recently.\n\nGatekeeping at several levels already exists in bitcoin development and difficult to solve. If some developers believe a soft fork should have been implemented on global signet but it was not, there is always the possibility of running custom signet with certain trade-offs.\n\n> The default signet is directly supported with the -signet flag in Bitcoin Core. Even if we are moving the proposed soft fork code to an external repo (to avoid it being merged into Core prematurely) it is still determining what soft forks are accessible from the signet flag in Bitcoin Core. I don't think it is fair on the signet block signers to put them in that position and I don't think it is wise to put other Bitcoin Core contributors/maintainers in the position of having to defend why some proposed soft forks are accessible on the default signet while others aren't.\n\nMainnet and Testnet have already been [removed][3] from the 'bitcoin-inquisition/bitcoin' repository, and signet in bitcoin core is largely used by developers or power users, thus it should not be a problem. Signet could also be removed from bitcoin core binaries that are released regularly while being available if built from source.\n\nSince signet coins have no value, utilizing this chain to experiment with soft forks will only help the bitcoin development process. If we can't even agree to implement something on a network used for testing then it will be nearly impossible to do any soft forks on mainnet. This experiment has several advantages. We can implement many consensus changes and compare alternatives in a better way. Pre-baked ability to abandon the soft fork isn't implemented yet AFAIK, however that could further improve things.\n\n\n[1]: https://github.com/bitcoin-inquisition/bitcoin/pull/3\n[2]: https://github.com/bitcoin/bitcoin/pull/21702#discussion_r979273387\n[3]: https://github.com/bitcoin-inquisition/bitcoin/pull/1\n\n/dev/fd0\n\nSent with Proton Mail secure email.\n\n\n------- Original Message -------\nOn Wednesday, September 28th, 2022 at 5:18 PM, Michael Folkson via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> I've given this some extra thought and discussed with others who may later chime in on this thread. I'm now convinced this should be done on a custom public signet rather than on the default signet. SegWit was added to a new testnet (Segnet) for testing rather than the pre-existing testnet and I think future soft fork proposals should follow a similar approach.\n> \n> Even if there is community consensus on what soft fork proposals should be added to the default signet today (which may or may not be case) I find it highly unlikely this will always be the case. We then get into the situation where the block signers (currently AJ and Kalle) are the gatekeepers on what soft fork proposals are added. The default signet is directly supported with the -signet flag in Bitcoin Core. Even if we are moving the proposed soft fork code to an external repo (to avoid it being merged into Core prematurely) it is still determining what soft forks are accessible from the signet flag in Bitcoin Core. I don't think it is fair on the signet block signers to put them in that position and I don't think it is wise to put other Bitcoin Core contributors/maintainers in the position of having to defend why some proposed soft forks are accessible on the default signet while others aren't.\n> \n> The default signet was a long term project to address the unreliability and weaknesses of testnet. Many default signet users won't be interested in testing soft fork proposals and it is not reasonable for them to be subject to a stalling or forked blockchain because changes to a soft fork proposal or a buggy soft fork proposal pushed to the default signet makes previous valid/invalid transactions invalid/valid. If they want to test proposed soft forks on a custom signet they are opting in to possible disruption rather than it being forced upon them.\n> \n> By focusing on custom signets rather than the default signet it also allows for more experimentation. Don't like the choices of which soft fork proposals have been added to bitcoin-inquisition? Set up your own custom signet with a different set of soft fork proposals and get users for your custom signet on a level playing field to bitcoin-inquisition. A soft fork proposal is found to be strictly inferior to another soft fork proposal? Just spin down the custom signet with that inferior soft fork proposal on it without impacting default signet users.\n> \n> So TL;DR still enthusiastic about this concept. Just with a strong preference that it is done on a custom signet rather than on the default signet.\n> \n> Thanks\n> Michael\n> \n> --\n> Michael Folkson\n> Email: michaelfolkson at protonmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n> \n> \n> ------- Original Message -------\n> On Monday, September 19th, 2022 at 11:05, Anthony Towns via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n> \n> \n> > On Sun, Sep 18, 2022 at 02:47:38PM -0400, Antoine Riard via bitcoin-dev wrote:\n> > \n> > > Said succinctly, in the genesis of creative ideas, evaluation doesn't\n> > > happen at a single clear point but all along the idea lifetime, where this\n> > > evaluation is as much done by the original author than its peers and a\n> > > wider audience.\n> > \n> > Sure. I definitely didn't mean to imply a waterfall development model,\n> > or that the phases wouldn't overlap etc.\n> > \n> > > I would still expose a concern to not downgrade in the pure empiricism in\n> > > matter of consensus upgrades. I.e, slowly emerging the norm of a working\n> > > prototype running on bitcoin-inquisition` as a determining factor of the\n> > > soundness of a proposal. E.g with \"upgrading lightning to support eltoo\", a\n> > > running e2e won't save us to think the thousands variants of pinnings, the\n> > > game-theory soundness of a eltoo as mechanism in face of congestions, the\n> > > evolvability of APO with more known upgrades proposals or the\n> > > implementation complexity of a fully fleshed-out state machine and more\n> > > questions.\n> > \n> > I agree here; but I think not doing prototypes also hinders thinking\n> > about all the thousands of details in a fork. It's easy to handwave\n> > details away when describing things on a whiteboard; and only realise\n> > they're trickier than you thought when you go to implement things.\n> > \n> > > E,g if one implements the \"weird\" ideas\n> > > about changes in the block reward issuance schedule discussed during the\n> > > summer, another one might not want \"noise\" interferences with new\n> > > fee-bumping primitives as the miner incentives are modified.\n> > \n> > (I don't think \"miner incentives\" are really something that can be\n> > investigated on signet. You can assume how miners will respond to\n> > incentives and program the mining software to act that way; but there's\n> > no competitive pressure in signet mining so I don't think that really\n> > demonstrates anything very much. Likewise, there's much less demand for\n> > blockspace on signet than on mainnet, so it's probably hard to experiment\n> > with \"fee incentives\" too)\n> > \n> > > I hope the upcoming\n> > > Contracting Primitives WG will be able to document and discuss some of the\n> > > relevant experiments run on bitcoin-inquisition.\n> > \n> > Likewise.\n> > \n> > (Lots trimmed due to either agreeing with it or having nothing to add)\n> > \n> > Cheers,\n> > aj\n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "bitcoin-inquistion: evaluating soft forks on signet",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard",
                "Michael Folkson",
                "Anthony Towns",
                "alicexbt",
                "Matt Corallo"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 95161
        }
    },
    {
        "title": "[bitcoin-dev] More uses for CTV",
        "thread_messages": [
            {
                "author": "Antoine Riard",
                "date": "2022-09-19T01:22:43",
                "message_text_only": "Hi James,\n\nThanks to bringing to awareness the atomic mining pool thing, it's\ninteresting.\n\n> I'm not a mining expert and so I can't speak to the efficacy of the\n> paper as a whole, but direct-from-coinbase payouts seem like a\n> desirable feature which avoids some trust in pools. One limitation is\n> the size of the coinbase outputs owed to constituent miners; this\n> limits the number of participants in the pool.\n\nI'm neither a mining expert, but I wonder if there is not some weird\ndependency here. The coinbase output scriptpubkey being part of the\nheader's merkle root commitment, the CTV hash being part of the\nscriptpubkey and the payout outputs being part of the CTV hash, everytime\nthe payout outputs as re-evaluated in function of the last work share\nsubmitted, as Laurentia is proposing, the whole payout transaction must be\nupdated, then the CTV hash, then the merkle root commitment, leading all\nthe mining devices to re-fetch a header from the job negotiator (in Stratum\nV2 parlance), I think ? I don't know the average shares submission\nfrequency for a local pool of size 100 as targeted by Laurentia though the\nlatency hit might downgrade the worthiness of this CTV-based atomic mining\npool payouts design...\n\nBeyond, I'm not sure about the trust removal statement of this design, as\nthe job negotiator operator, sounds to always have malleability to select\nthe coinbase output scriptpubkey, therefore selecting any CTV hash\nassigning all the reward to itself, at the detriment of other mining pool\nparticipant. I believe this is not a downside of CTV usage itself, but the\nfact that the coinbase output scriptpubkey is ultimately signed by the\nproof-of-work itself.\n\nAbout compactness, I wonder if an atomic payment pool payouts design\nfavoring the payouts settlement directly over Lightning channels wouldn't\noffer a smaller on-chain footprint. E.g, maybe the mining pool operator\ncould send a long-term PTLC to each participant covering the period during\nwhich a block has odds to be mined by the pool. The PTLCs amounts should be\nstable once the block template is agreed on. The coinbase output is locked\nwith some scriptless script point. When it is spent by the mining operator,\nthe PTLCs could be fetched by the participant. If the mining operator\ndoesn't spend before time lock expiration, there could be some on-chain\nfan-out transaction kicking-out. That type of scheme would allow you to\nsave on-chain fees and not to leak the mining pool hashrate distribution.\nHowever, I believe it is more complex to make it fit with the SPLNS\n\"real-time\"  calculation as it sounds to be proposed by the paper. Just a\nstrawman proposal, if relevant, deserves more thinking.\n\nThe paper would deserve to have a fully fleshed-out \"coinbase generation\"\nscheme as the description is a bit loose, imo, like:\n\n\"Block solve reward is distributed directly from the block to each user,\nmeaning each user gets\na 'mined' transaction directly into their wallet as soon as the block is\nsolved so there is no wait\nto get paid and no pool wallet storing user's rewards\"\n\nAnyway, left a scratch of further scheme analysis there:\nhttps://github.com/ariard/bitcoin-contracting-primitives-wg/pull/8\n\nBest,\nAntoine\n\nLe ven. 19 ao\u00fbt 2022 \u00e0 12:33, James O'Beirne via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Over the past few months there have been a few potential uses of\n> OP_CHECKTEMPLATEVERIFY (BIP-119)\n> (https://github.com/bitcoin/bitcoin/pull/21702) that I've found\n> interesting.\n>\n> # Congestion control redux\n>\n> When I first heard of CTV, a presentation Jeremy did at Chaincode back\n> in 2018 or '19, he cited congestion control as one of its main use\n> cases.\n>\n> The pitch went something like\n>\n> > When there is a high demand for blockspace it becomes very expensive\n> > to make transactions. By using OP_CHECKTEMPLATEVERIFY, a large volume\n> > payment processor may aggregate all their payments into a single O(1)\n> > transaction for purposes of confirmation. Then, some time later, the\n> > payments can be expanded out of that UTXO when the demand for\n> > blockspace is decreased.\n>\n> (from https://utxos.org/uses/scaling/)\n>\n> At the time that didn't particularly grab me; the idea of smoothing fee\n> rates seemed nice but marginal.\n>\n> But recently, two particular cases have made me reassess the value of\n> congestion control.\n>\n> The first stems from the necessity of L2 protocols (payment channels,\n> vaults, etc.) to, under certain circumstances, settle to the chain in a\n> timely way in order to prevent abuse of the protocol. If some\n> unexpected condition (a protocol exploit, large network disconnect, en\n> masse vault breach, etc.) creates a situation where a large number of\n> contracts need to settle to the chain in short order, mempools could\n> fill up and protocol failures could happen for want of mempool/block\n> space\n> (\n> https://github.com/jamesob/mempool.work#failure-one-mempool-to-rule-them-all\n> ).\n>\n> In such a case, CTV could be used effectively to \"compress\" settlement\n> commitments, get them on-chain, and then facilitate later unpacking of\n> the CTV ouputs into the contract's true end state.\n>\n> This amounts to `n` contract-control outputs (e.g. a lightning funding\n> transaction outputs) being spent into a single CTV output, which\n> commits to the final settlement state. Multiple parties could\n> trustlessly collaborate to settle into a single CTV output using\n> SIGHASH_ALL | ANYONECANPAY. This requires a level of interaction\n> similar to coinjoins.\n>\n> Put simply, CTV allows deferring the chainspace required for the final\n> settlement outputs, but still immediately requires space for the\n> inputs. This might sound like a temporary reprieve from half-ish of the\n> space required to settle, but in many (most?) cases the outputs require\n> substantially more space than the inputs, given that often we're\n> settling a single UTXO into multiple payouts per party. A 2, 3, or\n> 4-fold increase (depending on the contracting pattern) in capacity\n> isn't a silver bullet, but it could ameliorate the damage of unexpected\n> settlement \"tidal waves.\"\n>\n> Conceptually, CTV is the most parsimonious way to do such a scheme,\n> since you can't really get smaller than a SHA256 commitment, and that's\n> essentially all CTV is.\n>\n> The second congestion control case is related to a recent post Bram\n> made about stability under a no-block-subsidy regime. He posted\n>\n> > If transaction fees came in at an even rate over time all at the\n> > exact same level then they work fine for security, acting similarly\n> > to fixed block rewards. Unfortunately that isn't how it works in the\n> > real world. There's a very well established day/night cycle with fees\n> > going to zero overnight and even longer gaps on weekends and\n> > holidays. If in the future Bitcoin is entirely dependent on fees for\n> > security (scheduled very strongly) and this pattern keeps up\n> > (overwhelmingly likely) then this is going to become a serious\n> > problem.\n>\n> (from\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-July/020702.html\n> )\n>\n> Ryan Grant points out that CTV's congestion control use could help to\n> smooth fees, creating a less spiky incentive to mine\n> (\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-July/020702.html\n> ).\n>\n> Admittedly the original concern is speculative and a ways off from now,\n> as others in the thread pointed out. But having CTV-based fee smoothing\n> as an option certainly doesn't seem like a bad thing.\n>\n>\n> # Atomic mining pool payouts\n>\n> Laurentia is a mining pool design that pays participants out directly\n> from the coinbase of found blocks.\n>\n> > Block solve reward is distributed directly from the block to each\n> > user, meaning each user gets a 'mined' transaction directly into\n> > their wallet as soon as the block is solved so there is no wait to\n> > get paid and no pool wallet storing user's rewards.\n>\n> (from\n>\n> https://laurentiapool.org/wp-content/uploads/2020/05/laurentiapool_whitepaper.pdf\n> )\n>\n> I'm not a mining expert and so I can't speak to the efficacy of the\n> paper as a whole, but direct-from-coinbase payouts seem like a\n> desirable feature which avoids some trust in pools. One limitation is\n> the size of the coinbase outputs owed to constituent miners; this\n> limits the number of participants in the pool.\n>\n> If the payout was instead a single OP_CTV output, an arbitrary number\n> of pool participants could be paid out \"atomically\" within a single\n> coinbase.\n>\n> ---\n>\n> CTV both in concept and implementation is very simple, and I think it\n> is likely to continue to yield potential applications.\n> \"Settlement compression\" seems like a useful thing, especially in light\n> of a possible increase in L2 usage, and CTV seems like the simplest\n> means to enable it.\n>\n> Interestingly, an analogue for this pattern going the other direction\n> is possible, e.g. non-interactive channel openings\n> (https://utxos.org/uses/non-interactive-channels/), which would allow\n> e.g. opening a lightning channel with a merchant who doesn't want to\n> have their spending keys constantly accessible from a point-of-sale,\n> but can still parse/verify CTV commitments.\n>\n> Regards,\n> James\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220918/abfc9232/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "More uses for CTV",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9604
        }
    },
    {
        "title": "[bitcoin-dev] BIP Proposal: Wallet Labels Export Format",
        "thread_messages": [
            {
                "author": "Hugo Nguyen",
                "date": "2022-09-21T06:07:12",
                "message_text_only": "Hello Craig,\nThank you for putting this proposal together. It is indeed another big\nmissing piece of the puzzle.\n\nI would like to echo some of the comments already made by others (and you\nyourself) on this thread, that this proposal seems to have some inherent\nconflicts between the 2 goals it tries to achieve.\n\n> *Allowing users to import and export their labels in a standardized way\nensures that they do not experience lock-in to a particular wallet\napplication. As a secondary goal, by using common formats this BIP seeks to\nmake manual or bulk management of labels accessible to users outside of\nwallet applications and without specific technical expertise.*\n\nIMHO, the reason these conflicts exist is because the first one is an\nengineering requirement, while the second one is a UX / product requirement.\n\nEngineering requirements typically prioritize data integrity,\nreliability/robustness and performance. Do we want some sort of error\ndetection / correction codes? What data format would be the most robust and\nleast error-prone? Is CSV a good fit or not for this purpose? etc.\n\nUX requirements, on the other hand, typically prioritize convenience and\nease of use.\n\nWhen we don\u2019t separate these concerns it can backfire and we might end up\nwith a Frankenstein standard that is the worst of both worlds. That is: not\nquite robust in engineering terms, but also not quite user-friendly in\nproduct terms either.\n\nSLIP-132 is one such example. It tries to solve what are inherently\nengineering challenges \u2014 how to manage the complexities that arose due to\nthe evolution of keys and scripts \u2014 by sadly offloading those complexities\nonto the end users. The end result is user confusion (what kind of [?]PUB\ndo I need here?) and a nightmare for engineers to maintain (the\ncomplexities are better managed via a high level language such as Output\nDescriptors).\n\nKeeping in this mind, I also think having 2 separate BIPs for this is\nbetter.\n\nCheers,\nHugo\n\n\n\n\nOn Mon, Aug 29, 2022 at 4:26 AM Craig Raw via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Thanks for your feedback @Ali.\n>\n> I am attempting to achieve two goals with this proposal, primarily for the\n> benefit of wallet users:\n>\n> Goal #1. Transfer labels between different wallet implementations\n> Goal #2. Manage labels in applications outside of Bitcoin wallets (such as\n> Excel)\n>\n> Much of the feedback so far has indicated the tension between these two\n> goals - it may be that it is too difficult to achieve both, in which case\n> Goal #1 is the most important. That said, I think further exploration is\n> still necessary before abandoning Goal #2, because removing it would\n> significantly reduce the value of this proposal and mean users need to rely\n> on application-specific workarounds.\n>\n> > it is important that a version byte is defined\n> If Goal #2 is to be achieved it's difficult to mandate this, particularly\n> if one requires bit flags to be set. Should an importing wallet fail to\n> import if the version byte is not present, even if all the data is\n> otherwise correct? Although it is difficult to know in advance how a format\n> may be extended, it is certainly possible to extend this format with\n> additional types where the nature of hashes serve as unique identifiers\n> (more on this below).\n>\n>  > Don't mandate the file extension... There is no way to enforce this on\n> a BIP level.\n> I'm not quite sure what you mean here - for example BIP174, which is\n> widely used, states \"Binary PSBT files should use the .psbt file\n> extension.\" Also, this contradicts Goal #2 - Excel and Numbers register as\n> handlers for .csv, and so make it clear that the file is editable outside\n> of a wallet.\n>\n> > ZIP does not have good performance or compression ratio\n> Indeed, but it is very widely available. That said, gzip is supported\n> widely too these days. Unfortunately, gzip does not offer encryption (see\n> next answer).\n>\n> > ZIP is an archiving format, that happens to have its own compression\n> format.\n> I agree this is not ideal. My main reason for choosing ZIP was that it\n> supports encryption. It seems to me that without considering encryption, an\n> application must create label export files that allow privacy-sensitive\n> wallet information to be readable in plain text. Being able to transfer\n> labels without risking privacy is IMO valuable. I considered other\n> encryption formats such as PGP, but they are much more niche and so again\n> contradict Goal #2.\n>\n> > I don't see the benefit of encrypting addresses and labels together...\n> additionally, the password you propose is insecure - anybody with access to\n> the wallet can unlock it\n> I'm not sure I understand your question, but both wallet addresses and\n> wallet labels contain privacy-sensitive information that should be\n> protected. Wrt to the password, there is actually a more fundamental\n> problem with using the wallet xpub - there is no equivalent for multisig\n> wallets. For this reason I'll remove that requirement in future iterations.\n>\n> > Why the need for input and output formats? There is no difference\n> between them on the wallet level, because they are always identified with a\n> txid and output index.\n> The input refers to the txid and the input index (in the set of vin), so\n> the difference is the context in which they are displayed. A wallet will\n> not necessarily store the spent outputs for a funding transaction\n> containing a UTXO coming into the wallet, but it will contain references to\n> the inputs as part of that transaction.\n>\n> > Another important point is that practically nobody labels inputs or\n> outputs\n> To the contrary, UTXOs are very frequently labelled, as they link and\n> reveal information when spent. Inputs are much less frequently labelled,\n> but there is no particular reason to exclude them.\n>\n> > there is a net benefit for the addresses to be exported in ascending\n> order\n> Indeed, but it makes achieving Goal #2 much more difficult for marginal\n> benefit.\n>\n> > It's better to mandate that they should always be double-quoted, since\n> only wallets will generate label exports anyway.\n> Rather I think it's better to mandate RFC4180 is followed, as per\n> recommendations in other feedback.\n>\n> > The importing code is too naive... it should utilize a dedicate item\n> type field that unambiguously identifies the item\n> It's unclear to me what you mean here. As I've indicated it is currently\n> possible to disambiguate between addresses/transactions/etc without the\n> need for a 3rd column, but in any case the hash functions used ensure that\n> labels will not be associated incorrectly. Even in the unlikely event of\n> some future address type being indistinguishable from a txid, it will\n> simply not match any txids in the wallet.\n>\n> Craig\n>\n>\n>\n> On Wed, Aug 24, 2022 at 9:10 PM Ali Sherief <ali at notatether.com> wrote:\n>\n>> Hi Craig,\n>>\n>> This a really good proposal. I studied your BIP and I have some feedback\n>> on some parts of it.\n>>\n>> > The first line in the file is a header, and should be ignored on import.\n>>\n>> From past experience and lessons, most notably BIP39, it is important\n>> that a version byte is defined somewhere in case someone wants to extend it\n>> in the future, currently there is no version byte which someone can\n>> increment if somebody wants to extend it. In the unique case of CSV files,\n>> you should make the header line mandatory (I see you have already implied\n>> this, but you should make it explicit in the BIP), but instead of a line\n>> with columns in it, I suggest instead of Reference,Label, you make the\n>> format like this:\n>>\n>> BIP-wallet-labels,<version>\n>>\n>> Since there are two columns per record, this works out nicely. The first\n>> column can be the name of the BIP - BIPxxxx where the x's are numbers, and\n>> the second column can be an unsigned 32-bit integer (most significant 8\n>> bits reserved for version, the remaining for flags, or perhaps the entirety\n>> for version - but I recommend leaving at least some bits for flags, even if\n>> they all end up being just \"reserved\").\n>>\n>> You should make importing fail if the header line is not exactly as\n>> specified - or appropriate, should you decide a different format for the\n>> header.\n>>\n>> > Files exported should use the <tt>.csv</tt> file extension.\n>> Don't mandate the file extension (read below for why):\n>>\n>> > In order to reduce file size while retaining wide accessibility, the CSV\n>> > file may be compressed using the ZIP file format, using the\n>> <tt>.zip</tt>\n>> > file extension.\n>> I see three problems with this. The first is more important than the\n>> later two because it makes them moot points, but I'll mention them anyway\n>> so you get a background of the situation:\n>> - The BIP is trying to specify in what file format the export format can\n>> be written in onto the filesystem. There is no way to enforce this on a BIP\n>> level (besides, Unix operating systems don't even consider the file\n>> extension, they use its mimetype). Also specifying this in the BIP will\n>> prevent modular \"Layer 2\" protocols and schemes from encoding the Export\n>> labels into another format - for example Base64 or with their own\n>> compression algorithm.\n>>\n>> Now for the two \"moot problems\":\n>> - ZIP does not have good performance or compression ratio, there are\n>> better algorithms out there like gzip (which also happens to be more\n>> ubiquitous; nearly all websites are serving HTML compressed with gzip\n>> compression).\n>> - ZIP is an archiving format, that happens to have its own compression\n>> format. Archiving format parsers can have serious vulnerabilities in their\n>> implementation that can allow malware to swipe private keys and passwords,\n>> since the primary target for this BIP is wallets. For example, there was\n>> Zip Slip[1] in 2018, which allows for remote code execution. So the malware\n>> can even hide in memory until private keys or passwords are written to\n>> memory, then send them accros the network. Assuming it's targeting a\n>> specific wallet software it's not hard to carry out at all.\n>>\n>> There's two solutions for all this:\n>> 1. The duck-tape solution: Use some compression algorithm like gzip\n>> instead of ZIP archive format.\n>> 2. The \"throw it out and buy a new one\" solution: Get rid of the optional\n>> compression specs altogether, because users are responsible for supplying\n>> the export labels in the first place, so all the compression stuff is\n>> redundant and should be left up to the user use if they desire to.\n>>\n>> I prefer the second solution because it hits the nail at the problem\n>> directly instead of putting duck tape on it like the first one.\n>>\n>> > This <tt>.zip</tt> file may optionally be encrypted using either\n>> AES-128 or\n>> > AES-256 encryption, which is supported by numerous applications\n>> including\n>> > Winzip and 7-zip.\n>> > The textual representation of the wallet's extended public key (as\n>> defined\n>> > by BIP32, with an <tt>xpub</tt> header) should be used as the password.\n>> Not specific to AES, but I don't see the benefit of encrypting addresses\n>> and labels together. Can you please elaborate why this would be desireable?\n>>\n>> Like I said though, it's better to leave it up to users to decide how to\n>> store their exports, since BIPs can't enforce that anyway (additionally,\n>> the password you propose is insecure - anybody with access to the wallet\n>> can unlock it, which is not desireable to some users who want their own\n>> security).\n>>\n>> > * Transaction ID (<tt>txid</tt>)\n>> > * Address\n>> > * Input (rendered as <tt>txid<index</tt>)\n>> > * Output (rendered as <tt>txid>index</tt> or <tt>txid:index</tt>)\n>> Why the need for input and output formats? There is no difference between\n>> them on the wallet level, because they are always identified with a txid\n>> and output index. To distinguish between them and hence write them with the\n>> correct format would require a UTXO set and thus access to a full node,\n>> otherwise the CSV cannot be verified to be completely well-formed.\n>>\n>> Another important point is that practically nobody labels inputs or\n>> outputs because most people do not know that those things even exist, and\n>> the rest don't bother to label them.\n>>\n>> But the biggest downside to including them is related to the problem of\n>> information leaking which you make reference to here:\n>> > In both cases, care must be taken when spending to avoid undesirable\n>> leaks\n>> > of private information.\n>> A CSV dump that has inputs/outputs and addresses mixed together can infer\n>> the owner of all those items. In fact, A CVS label dump is basically a\n>> personal information store so everything in it can be correlated as coming\n>> from the same wallet, so it's important that unnecessary types are kept out\n>> of the format. People are known to leave files lying around on their\n>> computer that they don't need anymore, so these files can find their way\n>> via telemetry to surveillence entities. While we can't specify what users\n>> can do with their exports, we can control the information leak by\n>> preventing certain types of items that we know most users will never use\n>> from being exported in the first place.\n>>\n>> > The order in which these records appear is not defined.\n>> Again, since the primary use case for this BIP is wallets, which likely\n>> use heirarchical derivation schemes like BIP44, there is a net benefit for\n>> the addresses to be exported in ascending order of their `address_type`. It\n>> means that wallets can import them in O(n) time as opposed to O(n^2) time\n>> spent serially checking in which index the address appears at. Of course,\n>> this implies that all addresses up to a certain index have to be exported\n>> into the CSV as well, but most wallets I know of like Core, Electrum\n>> already store addresses like that.\n>>\n>> Also if you do this, you will need to group all the transaction records\n>> before the address records or vice versa - you can use lexigraphical\n>> sorting if you want (ie. Addresses before Transactions). The benefit of\n>> this separation of parts is that wallets can split the imported address\n>> records from the transaction records internally, and feed them to separate\n>> functions which set these labels internally.\n>>\n>> If you decide on doing it this way, then you need a 3rd column to\n>> identify the item type, and also you should quote the label (see below). I\n>> strongly recommend using numbers for identification as opposed to character\n>> strings, so you don't have to worry about localization or character case\n>> issues. There is always one unique number, but there could be multiple\n>> strings that reference the same type. This will complicate importing\n>> functions.\n>>\n>> If you insist on include Input and Output types then they can both be\n>> specified as <txid>:<index> if you do this change. They won't be used to\n>> determine the type anyway.\n>>\n>> > The fields may be quoted, but this is unnecessary, as the first comma in\n>> > the line will always be the delimiter.\n>> Don't implement it like that, because that will break CSV parsers which\n>> expect a fixed amount of rows in each record (2 in the header, and some\n>> rows have >2 rows). It's better to mandate that they should always be\n>> double-quoted, since only wallets will generate label exports anyway. If\n>> you plan to use headers then the 3rd column can be blank for it (or you can\n>> split the version and flags from each other).\n>>\n>> > ==Importing==\n>> >\n>> > When importing, a naive algorithm may simply match against any\n>> reference,\n>> > but it is possible to disambiguate between transactions, addresses,\n>> inputs\n>> > and outputs.\n>> > For example in the following pseudocode:\n>> > <pre>\n>> >   if reference length < 64\n>> >     Set address label\n>> >   else if reference length == 64\n>> >     Set transaction label\n>> >   else if reference contains '<'\n>> >     Set input label\n>> >   else\n>> >     Set output label\n>> > </pre>\n>> The importing code is too naive and in its current form will prevent the\n>> BIP from getting a number. It is perhaps the single most important part of\n>> a BIP. When implementing an importer, it should utilize a dedicate item\n>> type field that unambiguously identifies the item. So the naive importer is\n>> not good, you need use a 3rd column for that like I explained above, so\n>> that the importer becomes robust.\n>>\n>> In summary (exclamation marks indicate severity - one means low, two\n>> means medium, and three means high):\n>>\n>> 1. Convert the header into a version line with optional flags, otherwise\n>> nobody can extend this format without compatibility issues (!)\n>> 2. Get rid of the specs related to file compression (!!!)\n>> 3. Add a 3rd column for item type (address, transaction etc.) preferably\n>> as numeric constants and grouping items of one type after items of another\n>> type, or if you insist on strings, then only recognize their Titlecase\n>> ASCII versions <spreadsheet software like Excel always tries to titlecase\n>> the words> (!!)\n>> 4. Require double quotes around the label (or single quotes if you\n>> prefer, as long as spreadsheet software doesn't choke on them) (!!)\n>> 5. Require sorting the records according to the order they are stored in\n>> the wallet implementation. (!)\n>> 6. Consider getting rid of Input and Output item types. (!)\n>> 7. And last and most importantly, please write a more robust importer\n>> algorithm in the example given by the BIP, because code in BIPs are\n>> frequently used as references for software. (!!!)\n>>\n>> I hope you will consider these points in future revisions of your BIP.\n>>\n>> - Ali\n>>\n>> [1] https://github.com/snyk/zip-slip-vulnerability\n>>\n>> On Wed, 24 Aug 2022 11:18:43 +0200, craigraw at gmail.com wrote:\n>> > Hi all,\n>> >\n>> > I would like to propose a BIP that specifies a format for the export and\n>> > import of labels from a wallet. While transferring access to funds\n>> across\n>> > wallet applications has been made simple through standards such as\n>> BIP39,\n>> > wallet labels remain siloed and difficult to extract despite their\n>> value,\n>> > particularly in a privacy context.\n>> >\n>> > The proposed format is a simple two column CSV file, with the reference\n>> to\n>> > a transaction, address, input or output in the first column, and the\n>> label\n>> > in the second column. CSV was chosen for its wide accessibility,\n>> especially\n>> > to users without specific technical expertise. Similarly, the CSV file\n>> may\n>> > be compressed using the ZIP format, and optionally encrypted using AES.\n>> >\n>> > The full text of the BIP can be found at\n>> >\n>> https://github.com/craigraw/bips/blob/master/bip-wallet-labels.mediawiki\n>> > and also copied below.\n>> >\n>> > Feedback is appreciated.\n>> >\n>> > Thanks,\n>> > Craig Raw\n>> >\n>> > ---\n>> >\n>> > <pre>\n>> >   BIP: wallet-labels\n>> >   Layer: Applications\n>> >   Title: Wallet Labels Export Format\n>> >   Author: Craig Raw <craig at sparrowwallet.com>\n>> >   Comments-Summary: No comments yet.\n>> >   Comments-URI:\n>> > https://github.com/bitcoin/bips/wiki/Comments:BIP-wallet-labels\n>> >   Status: Draft\n>> >   Type: Informational\n>> >   Created: 2022-08-23\n>> >   License: BSD-2-Clause\n>> > </pre>\n>> >\n>> > ==Abstract==\n>> >\n>> > This document specifies a format for the export of labels that may be\n>> > attached to the transactions, addresses, input and outputs in a wallet.\n>> >\n>> > ==Copyright==\n>> >\n>> > This BIP is licensed under the BSD 2-clause license.\n>> >\n>> > ==Motivation==\n>> >\n>> > The export and import of funds across different Bitcoin wallet\n>> applications\n>> > is well defined through standards such as BIP39, BIP32, BIP44 etc.\n>> > These standards are well supported and allow users to move easily\n>> between\n>> > different wallets.\n>> > There is, however, no defined standard to transfer any labels the user\n>> may\n>> > have applied to the transactions, addresses, inputs or outputs in their\n>> > wallet.\n>> > The UTXO model that Bitcoin uses makes these labels particularly\n>> valuable\n>> > as they may indicate the source of funds, whether received externally\n>> or as\n>> > a result of change from a prior transaction.\n>> > In both cases, care must be taken when spending to avoid undesirable\n>> leaks\n>> > of private information.\n>> > Labels provide valuable guidance in this regard, and have even become\n>> > mandatory when spending in several Bitcoin wallets.\n>> > Allowing users to export their labels in a standardized way ensures that\n>> > they do not experience lock-in to a particular wallet application.\n>> > In addition, by using common formats, this BIP seeks to make manual or\n>> bulk\n>> > management of labels accessible to users without specific technical\n>> > expertise.\n>> >\n>> > ==Specification==\n>> >\n>> > In order to make the import and export of labels as widely accessible as\n>> > possible, this BIP uses the comma separated values (CSV) format, which\n>> is\n>> > widely supported by consumer, business, and scientific applications.\n>> > Although the technical specification of CSV in RFC4180 is not always\n>> > followed, the application of the format in this BIP is simple enough\n>> that\n>> > compatibility should not present a problem.\n>> > Moreover, the simplicity and forgiving nature of CSV (over for example\n>> > JSON) lends itself well to bulk label editing using spreadsheet and text\n>> > editing tools.\n>> >\n>> > A CSV export of labels from a wallet must be a UTF-8 encoded text file,\n>> > containing one record per line, with records containing two fields\n>> > delimited by a comma.\n>> > The fields may be quoted, but this is unnecessary, as the first comma in\n>> > the line will always be the delimiter.\n>> > The first line in the file is a header, and should be ignored on import.\n>> > Thereafter, each line represents a record that refers to a label\n>> applied in\n>> > the wallet.\n>> > The order in which these records appear is not defined.\n>> >\n>> > The first field in the record contains a reference to the transaction,\n>> > address, input or output in the wallet.\n>> > This is specified as one of the following:\n>> > * Transaction ID (<tt>txid</tt>)\n>> > * Address\n>> > * Input (rendered as <tt>txid<index</tt>)\n>> > * Output (rendered as <tt>txid>index</tt> or <tt>txid:index</tt>)\n>> >\n>> > The second field contains the label applied to the reference.\n>> > Exporting applications may omit records with no labels or labels of zero\n>> > length.\n>> > Files exported should use the <tt>.csv</tt> file extension.\n>> >\n>> > In order to reduce file size while retaining wide accessibility, the CSV\n>> > file may be compressed using the ZIP file format, using the\n>> <tt>.zip</tt>\n>> > file extension.\n>> > This <tt>.zip</tt> file may optionally be encrypted using either\n>> AES-128 or\n>> > AES-256 encryption, which is supported by numerous applications\n>> including\n>> > Winzip and 7-zip.\n>> > In order to ensure that weak encryption does not proliferate, importers\n>> > following this standard must refuse to import <tt>.zip</tt> files\n>> encrypted\n>> > with the weaker Zip 2.0 standard.\n>> > The textual representation of the wallet's extended public key (as\n>> defined\n>> > by BIP32, with an <tt>xpub</tt> header) should be used as the password.\n>> >\n>> > ==Importing==\n>> >\n>> > When importing, a naive algorithm may simply match against any\n>> reference,\n>> > but it is possible to disambiguate between transactions, addresses,\n>> inputs\n>> > and outputs.\n>> > For example in the following pseudocode:\n>> > <pre>\n>> >   if reference length < 64\n>> >     Set address label\n>> >   else if reference length == 64\n>> >     Set transaction label\n>> >   else if reference contains '<'\n>> >     Set input label\n>> >   else\n>> >     Set output label\n>> > </pre>\n>> >\n>> > Importing applications may truncate labels if necessary.\n>> >\n>> > ==Test Vectors==\n>> >\n>> > The following fragment represents a wallet label export:\n>> > <pre>\n>> > Reference,Label\n>> >\n>> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b?,Transaction\n>> > 1A69TXnEM2ms9fMaY9UuiJ7415X7xZaUSg,Address\n>> >\n>> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b?<0,Input\n>> >\n>> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b?>0,Output\n>> >\n>> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b?:0,Output\n>> > (alternative)\n>> > </pre>\n>> >\n>> > ==Reference Implementation==\n>> >\n>> > TBD\n>>\n>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220920/d85742d6/attachment-0001.html>"
            },
            {
                "author": "Craig Raw",
                "date": "2022-09-26T08:23:18",
                "message_text_only": "Following discussion with several wallet developers, I have come to the\nconclusion that the secondary goal of managing labels in non-specialized\napplications must be sacrificed in order to achieve the primary goal of\nwide implementation across different wallets. While this tradeoff was\nperhaps inevitable, it was worth a try!\n\nAs such I have rewritten the specification to use JSON, specifically the\nJSON Lines format suggested by Ryan Havar and others (thank you). This\nallows documents to be split or streamed, and is convenient for\ncommand-line processing. The format is also now self describing via a type\nfield, permitting simple type identification (thank you Ali Sherief and\nothers). Public keys and xpubs have been added as types following further\nsuggestions. To keep the specification simple, compression and encryption\nhave been removed - with the strong recommendation to consider protecting\nthe data in a way suitable to its application.\n\nThe rewritten BIP can be found at\nhttps://github.com/craigraw/bips/blob/master/bip-wallet-labels.mediawiki\n\nIt is perhaps simplest to understand it by looking at an example export:\n\n{ \"type\": \"tx\", \"ref\":\n\"f91d0a8a78462bc59398f2c5d7a84fcff491c26ba54c4833478b202796c8aafd\",\n\"label\": \"Transaction\" }\n{ \"type\": \"addr\", \"ref\": \"bc1q34aq5drpuwy3wgl9lhup9892qp6svr8ldzyy7c\",\n\"label\": \"Address\" }\n{ \"type\": \"pubkey\", \"ref\":\n\"0283409659355b6d1cc3c32decd5d561abaac86c37a353b52895a5e6c196d6f448\",\n\"label\": \"Public Key\" }\n{ \"type\": \"input\", \"ref\":\n\"f91d0a8a78462bc59398f2c5d7a84fcff491c26ba54c4833478b202796c8aafd:0\",\n\"label\": \"Input\" }\n{ \"type\": \"output\", \"ref\":\n\"f91d0a8a78462bc59398f2c5d7a84fcff491c26ba54c4833478b202796c8aafd:1\",\n\"label\": \"Output\" }\n{ \"type\": \"xpub\", \"ref\":\n\"xpub661MyMwAqRbcFtXgS5sYJABqqG9YLmC4Q1Rdap9gSE8Nq...\", \"label\": \"Extended\nPublic Key\" }\n\nFeedback is always appreciated.\n\nCraig\n\n\nOn Wed, Aug 24, 2022 at 11:18 AM Craig Raw <craigraw at gmail.com> wrote:\n\n> Hi all,\n>\n> I would like to propose a BIP that specifies a format for the export and\n> import of labels from a wallet. While transferring access to funds across\n> wallet applications has been made simple through standards such as BIP39,\n> wallet labels remain siloed and difficult to extract despite their value,\n> particularly in a privacy context.\n>\n> The proposed format is a simple two column CSV file, with the reference to\n> a transaction, address, input or output in the first column, and the label\n> in the second column. CSV was chosen for its wide accessibility, especially\n> to users without specific technical expertise. Similarly, the CSV file may\n> be compressed using the ZIP format, and optionally encrypted using AES.\n>\n> The full text of the BIP can be found at\n> https://github.com/craigraw/bips/blob/master/bip-wallet-labels.mediawiki\n> and also copied below.\n>\n> Feedback is appreciated.\n>\n> Thanks,\n> Craig Raw\n>\n> ---\n>\n> <pre>\n>   BIP: wallet-labels\n>   Layer: Applications\n>   Title: Wallet Labels Export Format\n>   Author: Craig Raw <craig at sparrowwallet.com>\n>   Comments-Summary: No comments yet.\n>   Comments-URI:\n> https://github.com/bitcoin/bips/wiki/Comments:BIP-wallet-labels\n>   Status: Draft\n>   Type: Informational\n>   Created: 2022-08-23\n>   License: BSD-2-Clause\n> </pre>\n>\n> ==Abstract==\n>\n> This document specifies a format for the export of labels that may be\n> attached to the transactions, addresses, input and outputs in a wallet.\n>\n> ==Copyright==\n>\n> This BIP is licensed under the BSD 2-clause license.\n>\n> ==Motivation==\n>\n> The export and import of funds across different Bitcoin wallet\n> applications is well defined through standards such as BIP39, BIP32, BIP44\n> etc.\n> These standards are well supported and allow users to move easily between\n> different wallets.\n> There is, however, no defined standard to transfer any labels the user may\n> have applied to the transactions, addresses, inputs or outputs in their\n> wallet.\n> The UTXO model that Bitcoin uses makes these labels particularly valuable\n> as they may indicate the source of funds, whether received externally or as\n> a result of change from a prior transaction.\n> In both cases, care must be taken when spending to avoid undesirable leaks\n> of private information.\n> Labels provide valuable guidance in this regard, and have even become\n> mandatory when spending in several Bitcoin wallets.\n> Allowing users to export their labels in a standardized way ensures that\n> they do not experience lock-in to a particular wallet application.\n> In addition, by using common formats, this BIP seeks to make manual or\n> bulk management of labels accessible to users without specific technical\n> expertise.\n>\n> ==Specification==\n>\n> In order to make the import and export of labels as widely accessible as\n> possible, this BIP uses the comma separated values (CSV) format, which is\n> widely supported by consumer, business, and scientific applications.\n> Although the technical specification of CSV in RFC4180 is not always\n> followed, the application of the format in this BIP is simple enough that\n> compatibility should not present a problem.\n> Moreover, the simplicity and forgiving nature of CSV (over for example\n> JSON) lends itself well to bulk label editing using spreadsheet and text\n> editing tools.\n>\n> A CSV export of labels from a wallet must be a UTF-8 encoded text file,\n> containing one record per line, with records containing two fields\n> delimited by a comma.\n> The fields may be quoted, but this is unnecessary, as the first comma in\n> the line will always be the delimiter.\n> The first line in the file is a header, and should be ignored on import.\n> Thereafter, each line represents a record that refers to a label applied\n> in the wallet.\n> The order in which these records appear is not defined.\n>\n> The first field in the record contains a reference to the transaction,\n> address, input or output in the wallet.\n> This is specified as one of the following:\n> * Transaction ID (<tt>txid</tt>)\n> * Address\n> * Input (rendered as <tt>txid<index</tt>)\n> * Output (rendered as <tt>txid>index</tt> or <tt>txid:index</tt>)\n>\n> The second field contains the label applied to the reference.\n> Exporting applications may omit records with no labels or labels of zero\n> length.\n> Files exported should use the <tt>.csv</tt> file extension.\n>\n> In order to reduce file size while retaining wide accessibility, the CSV\n> file may be compressed using the ZIP file format, using the <tt>.zip</tt>\n> file extension.\n> This <tt>.zip</tt> file may optionally be encrypted using either AES-128\n> or AES-256 encryption, which is supported by numerous applications\n> including Winzip and 7-zip.\n> In order to ensure that weak encryption does not proliferate, importers\n> following this standard must refuse to import <tt>.zip</tt> files encrypted\n> with the weaker Zip 2.0 standard.\n> The textual representation of the wallet's extended public key (as defined\n> by BIP32, with an <tt>xpub</tt> header) should be used as the password.\n>\n> ==Importing==\n>\n> When importing, a naive algorithm may simply match against any reference,\n> but it is possible to disambiguate between transactions, addresses, inputs\n> and outputs.\n> For example in the following pseudocode:\n> <pre>\n>   if reference length < 64\n>     Set address label\n>   else if reference length == 64\n>     Set transaction label\n>   else if reference contains '<'\n>     Set input label\n>   else\n>     Set output label\n> </pre>\n>\n> Importing applications may truncate labels if necessary.\n>\n> ==Test Vectors==\n>\n> The following fragment represents a wallet label export:\n> <pre>\n> Reference,Label\n>\n> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b\u200e,Transaction\n> 1A69TXnEM2ms9fMaY9UuiJ7415X7xZaUSg,Address\n> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b\u200e<0,Input\n> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b\u200e>0,Output\n> c3bdad6e7dcd7997e16a5b7b7cf4d8f6079820ff2eedd5fcbb2ad088f767b37b\u200e:0,Output\n> (alternative)\n> </pre>\n>\n> ==Reference Implementation==\n>\n> TBD\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220926/96cc3f8d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP Proposal: Wallet Labels Export Format",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Hugo Nguyen",
                "Craig Raw"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 32986
        }
    },
    {
        "title": "[bitcoin-dev] RFC for a BIP32 recurrent address derivation scheme",
        "thread_messages": [
            {
                "author": "El_Hoy",
                "date": "2022-09-22T03:06:50",
                "message_text_only": "There is a known issue on bitcoin, that is that every transaction requires\na new address to prevent address reuse, making it uncomfortable to make\nrecurring payments, as every payment requires a new off-chain interaction.\nA scheme is already mentioned on the [on the BIP32 itself][1], but it\ncannot be implemented as is.\n\nHere I propose a scheme that follows the structure described on [BIP44]\nthat should make it possible to send recurring payments using a single\noffline interaction.\n\nThe proposed scheme is:\n\n    master / purpose' / coin_type' / contact' / index\n\nWhere the definitions of all the levels follow BIP44, except for `contact`\nthat is described below.\n\nExample usage: Bob wants to make recurring payments to Carol, so he asks\nher for a _contact address_, that is, an extended public key.\n\nBob can use that public key to generate multiple derived addresses to make\nmultiple recurring payments to Carol, the contact address is stored\noff-chain, anyone inspecting the chain will just see normal transactions\non-chain.\n\n## Considerations\n\n[BIP47] tries to solve the same issue, but the solution is more complex and\ninvolves more on-chain transactions that involve data, this implementation\nsimpler and requires less work to implement.\n\nAlso, the derivation path might need some adjustments for different address\ntypes on bitcoin.\n\nFinally, this only works in a single direction and does not make it\npossible for Carol to send anything to Bob, as it would require Bob sending\nher a contact address.\n\n## Advantages\n\nA positive side effect of using this, is that Bob can choose to send\npayments to Carol using multiple outputs, giving him more privacy.\n\nAlso, those payments can be easily labeled by the receiving wallet, as they\nare received.\n\nRegards.\n\n### References\n\n[1]:\nhttps://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#recurrent-business-to-business-transactions-nmih0\n[BIP47]: https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n\"Reusable Payment Codes for Hierarchical Deterministic Wallets\"\n[BIP43]:\nhttps://github.com/bitcoin/bips/blob/master/bip-0043.mediawiki#Purpose\n\n--- Eloy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220922/4d4aee8b/attachment.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2022-09-29T22:41:29",
                "message_text_only": "Hi Eloy,\n\nNice idea.\n\nNote I thought about and succinctly described a similar scheme here (which\nin turn was derived from work by Kixunil):\nhttps://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8#xpub-sharing\n\nI agree with your general assessment that this is a scheme that seems like\nan improvement over the status quo. Note that both BIP47 and Silent\nPayments don't require any interaction with the sender, while this scheme\nrequires one-time interaction (e.g. this wouldn't be suitable for one-time\ndonations). I think this would mostly be a convenience feature that\nimproves the regular interactive payment flow (interact once, instead of\nrepeatedly asking for addresses with each payment).\n\n>master / purpose' / coin_type' / contact' / index\n\nDespite your explanation, it's still not fully clear to me how \"contact\" is\ndefined, but I assume it's just a counter? Just in case, note that you\ncan't let Bob define it for Carol, as then you can't deterministically\nrecover your payments without also backing up how it's defined (the seed\nalone won't be enough).\n\nThe gap limit also needs to be kept in mind. If we allow each xpub to have\nits own gap limit, you potentially get an exponential blowup (gaps in the\nxpub * gaps in the addresses generated from the xpubs). It may be OK to\ndefine a low default gap limit for these xpubs, since there should be no\nreason to expect the same sender to leave any gaps, though this may depend\non how the xpubs are used (e.g. it may also be used to derive addresses for\nothers) so it's probably important to be explicit about this.\n\nCheers,\nRuben\n\n\n\nOn Thu, Sep 22, 2022 at 5:18 PM El_Hoy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There is a known issue on bitcoin, that is that every transaction requires\n> a new address to prevent address reuse, making it uncomfortable to make\n> recurring payments, as every payment requires a new off-chain interaction.\n> A scheme is already mentioned on the [on the BIP32 itself][1], but it\n> cannot be implemented as is.\n>\n> Here I propose a scheme that follows the structure described on [BIP44]\n> that should make it possible to send recurring payments using a single\n> offline interaction.\n>\n> The proposed scheme is:\n>\n>     master / purpose' / coin_type' / contact' / index\n>\n> Where the definitions of all the levels follow BIP44, except for `contact`\n> that is described below.\n>\n> Example usage: Bob wants to make recurring payments to Carol, so he asks\n> her for a _contact address_, that is, an extended public key.\n>\n> Bob can use that public key to generate multiple derived addresses to make\n> multiple recurring payments to Carol, the contact address is stored\n> off-chain, anyone inspecting the chain will just see normal transactions\n> on-chain.\n>\n> ## Considerations\n>\n> [BIP47] tries to solve the same issue, but the solution is more complex\n> and involves more on-chain transactions that involve data, this\n> implementation simpler and requires less work to implement.\n>\n> Also, the derivation path might need some adjustments for different\n> address types on bitcoin.\n>\n> Finally, this only works in a single direction and does not make it\n> possible for Carol to send anything to Bob, as it would require Bob sending\n> her a contact address.\n>\n> ## Advantages\n>\n> A positive side effect of using this, is that Bob can choose to send\n> payments to Carol using multiple outputs, giving him more privacy.\n>\n> Also, those payments can be easily labeled by the receiving wallet, as\n> they are received.\n>\n> Regards.\n>\n> ### References\n>\n> [1]:\n> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#recurrent-business-to-business-transactions-nmih0\n> [BIP47]: https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n> \"Reusable Payment Codes for Hierarchical Deterministic Wallets\"\n> [BIP43]:\n> https://github.com/bitcoin/bips/blob/master/bip-0043.mediawiki#Purpose\n>\n> --- Eloy\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220930/d73e6790/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "RFC for a BIP32 recurrent address derivation scheme",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "El_Hoy",
                "Ruben Somsen"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6610
        }
    },
    {
        "title": "[bitcoin-dev] New transaction policies (nVersion=3) for contracting protocols",
        "thread_messages": [
            {
                "author": "Gloria Zhao",
                "date": "2022-09-23T15:18:21",
                "message_text_only": "Hi everyone,\n\nI'm writing to propose a very simple set of mempool/transaction relay\npolicies intended to aid L2/contract protocols. I realized that\nthe previously proposed Package Mempool Accept package RBF [1]\nhad a few remaining problems after digging into the RBF logic more [2].\nThis additional set of policies solves them without requiring a huge RBF\noverhaul.\n\nI've written an implementation (and docs) for Bitcoin Core:\nhttps://github.com/bitcoin/bitcoin/pull/25038\n\n(You may notice that this proposal incorporates feedback on the PR - thanks\nSuhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns, and\nothers.)\n\nIf you are interested in using package RBF/relay to bump presigned\ntransactions, I think you may be interested in reviewing this proposal.\nThis should solve Rule 3 pinning and perhaps allow us\nto get rid of CPFP carve-out (yay!). I'm keen to hear if people find\nthe 1-anchor-output, 1000vB child limit too restrictive. Also, if you find a\npinning attack or something that makes it unusable for you, I would\nreally really like to know.\n\nNote that transactions with nVersion=3 (\"V3 transactions\") are\ncurrently non-standard in Bitcoin Core. That means **anything that was\nstandard before this policy change would still be standard\nafterwards.** If you don't want your transactions to be subject to\nthese rules, just continue whatever you're doing and don't use\nnVersion=3. AFAICT this shouldn't break anything, but let me know if\nthis would be disruptive for you?\n\n**New Policies:**\n\nThis includes:\n- a set of additional policy rules applying to V3 transactions\n- modifications to package RBF rules\n\n**V3 transactions:**\n\nExisting standardness rules apply to V3 (e.g. min/max tx weight,\nstandard output types, cleanstack, etc.). The following additional\nrules apply to V3:\n\n1. A V3 transaction can be replaced, even if it does not signal BIP125\n   replaceability. (It must also meet the other RBF rules around fees,\netc. for replacement to happen).\n\n2. Any descendant of an unconfirmed V3 transaction must also be V3.\n\n*Rationale*: Combined with Rule 1, this gives us the property of\n\"inherited\" replaceability signaling when descendants of unconfirmed\ntransactions are created. Additionally, checking whether a transaction\nsignals replaceability this way does not require mempool traversal,\nand does not change based on what transactions are mined. It also\nmakes subsequent rules about descendant limits much easier to check.\n\n*Note*: The descendant of a *confirmed* V3 transaction does not need to be\nV3.\n\n3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n\n*Rationale*: (Upper bound) the larger the descendant limit, the more\ntransactions may need to be replaced. This is a problematic pinning\nattack, i.e., a malicious counterparty prevents the transaction from\nbeing replaced by adding many descendant transactions that aren't\nfee-bumping.\n\n(Lower bound) at least 1 descendant is required to allow CPFP of the\npresigned transaction. The contract protocol can create presigned\ntransactions paying 0 fees and 1 output for attaching a CPFP at\nbroadcast time (\"anchor output\"). Without package RBF, multiple anchor\noutputs would be required to allow each counterparty to fee-bump any\npresigned transaction. With package RBF, since the presigned\ntransactions can replace each other, 1 anchor output is sufficient.\n\n4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n   larger than 1000 virtual bytes.\n\n*Rationale*: (Upper bound) the larger the descendant size limit, the\nmore vbytes may need to be replaced. With default limits, if the child\nis e.g. 100,000vB, that might be an additional 100,000sats (at\n1sat/vbyte) or more, depending on the feerate.\n\n(Lower bound) the smaller this limit, the fewer UTXOs a child may use\nto fund this fee-bump. For example, only allowing the V3 child to have\n2 inputs would require L2 protocols to manage a wallet with high-value\nUTXOs and make batched fee-bumping impossible. However, as the\nfee-bumping child only needs to fund fees (as opposed to payments),\njust a few UTXOs should suffice.\n\nWith a limit of 1000 virtual bytes, depending on the output types, the\nchild can have 6-15 UTXOs, which should be enough to fund a fee-bump\nwithout requiring a carefully-managed UTXO pool. With 1000 virtual\nbytes as the descendant limit, the cost to replace a V3 transaction\nhas much lower variance.\n\n*Rationale*: This makes the rule very easily \"tacked on\" to existing\nlogic for policy and wallets. A transaction may be up to 100KvB on its\nown (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n(`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\nin the mempool is 100KvB, its descendant can only be 1000vB, even if\nthe policy is 10KvB.\n\n**Package RBF modifications:**\n\n1. The rule around unconfirmed inputs was\noriginally \"A package may include new unconfirmed inputs, but the\nancestor feerate of the child must be at least as high as the ancestor\nfeerates of every transaction being replaced.\"\n\nThe package may still include new unconfirmed inputs. However,\nthe new rule is modified to be \"The minimum between package feerate\nand ancestor feerate of the child is not lower than the individual\nfeerates of all directly conflicting transactions and the ancestor\nfeerates of all original transactions.\"\n\n*Rationale*: We are attempting to ensure that the replacement\ntransactions are not less incentive-compatible to mine. However, a\npackage/transaction's ancestor feerate is not perfectly representative\nof its incentive compatibility; it may overestimate (some subset of\nthe ancestors could be included by itself if it has other high-feerate\ndescendants or are themselves higher feerate than this\npackage/transaction). Instead, we use the minimum between the package\nfeerate and ancestor feerate of the child as a more conservative value\nthan what was proposed originally.\n\n2. A new rule is added, requiring that all package transactions with\nmempool conflicts to be V3. This also means the \"sponsoring\"\nchild transaction must be V3.\n\n*Note*: Combined with the V3 rules, this means the package must be\na child-with-parents package. Since package validation is only\nattempted if the transactions do not pay sufficient fees to be\naccepted on their own, this effectively means that only V3\ntransactions can pay to replace their ancestors' conflicts, and only\nV3 transactions' replacements may be paid for by a descendant.\n\n*Rationale*: The fee-related rules are economically rational for\nancestor packages, but not necessarily other types of packages.\nA child-with-parents package is a type of ancestor package. It\nmay be fine to allow any ancestor package, but it's more difficult\nto account for all of the possibilities. For example, it gets much\nharder to see that we're applying the descendant limits correctly if\nthe package has a gnarly, many-generation, non-tree shape. I'm also\nnot sure if this policy is 100% incentive-compatible if the sponsor\nis not a direct descendant of the sponsee.\n\nPlease see doc/policy/version3_transactions.md and\ndoc/policy/packages.md in the PR for the full set of rules.\n\n**Intended usage for LN:**\n\nCommitment transactions should be V3 and have 1 anchor output. They\ncan be signed with 0 fees (or 1sat/vbyte) once package relay is deployed\non a significant portion of the network. If the commitment tx must\nbe broadcast, determine the desired feerate at broadcast time and\nspend the anchor output in a high feerate transaction. I'm going to\ncall the broadcasted commitment tx \"the parent\" and the attached\nfee-bumping tx \"the child.\"\n\n- This child must be V3.\n- This child must be at most 1000vB. Note this restricts the\n  number of inputs you can use to fund the fee bump. Depending\non the output types, this is around 6-15.\n- One child may fund fees for multiple commitment tx (\"batched\n  fee-bumping\").\n- To do a second fee-bump to add more fees, replace the\n  *child* with a higher-feerate tx. Do not try to attach a grandchild.\n\nOtherwise, never try to spend from an unconfirmed V3 transaction. The\ndescendant limits for V3 transactions are very restrictive.\n\n**Expected Questions:**\n\n\"Does this fix Rule 3 Pinning?\"\nYes. The V3 descendant limit restricts both you and your counterparty.\nAssuming nodes adopted this policy, you may reasonably assume that you\nonly need to replace the commitment transaction + up to 1000vB.\n\n\"Only 1 anchor output? What if I need to bump counterparty's commitment tx\nin mempool?\"\nYou won't need to fee-bump a counterparty's commitment tx using CPFP.\nYou would just package RBF it by attaching a high-feerate child to\nyour commitment tx.\n\n\"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\ntransactions based on nVersion?\"\nIndeed it may be unrealistic to assume V3 transactions will be in\nwidespread use outside of L2. IIUC, unilateral closes are already\nobvious LN transactions because of the HTLC inputs. For e.g.\ncooperative closes and opens, I think it makes sense to continue using\nV2. So, unless I'm missing something, this shouldn't make it worse.\n\n\"So a V3 transaction that doesn't signal BIP125 replaceability is\nreplaceable? Is that a backward compatibility issue?\"\nYes it's replaceable. It's not an issue AFAICT because,\nunder previous policy, the V3 transaction wouldn't have been\nin the mempool in the first place.\n\n\"Can a V2 transaction replace a V3 transaction and vice versa?\"\nYes, otherwise someone can use V3 transactions to censor V2\ntransactions spending shared inputs. Note if the\noriginal V3 transaction has an unconfirmed V3 parent, this would\nviolate the \"inherited V3\" rule and would be rejected.\n\nThanks for reading! Feedback and review would be much appreciated.\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n[2]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n\nBest,\nGloria\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220923/9b52c0c4/attachment.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2022-09-23T18:48:39",
                "message_text_only": "Hello Gloria,\n\nGreat work on synthesizing so much feedback into a proposal like this!\n\nDeath to carve-out rule.\n\nI'd like to elaborate on some caveats and give a few incomplete thoughts.\n\nThere are basically two types of pinning in my estimation today:\n\n1) rule#3 pinning: Make it uneconomical to replace whatever is in mempool\nvia large in size but low feerate junk that won't get mined anytime soon.\nReplacing this with feerate-based policy seems apt, but fraught with DoS\nrisks.\n\n2) package limit pinning: disallowing transaction propagation by package\nlimits being hit: size, ancestor count, descendant count. Today it is\nmitigated by having all outputs be 1 csv timelocked, and having up to 2\nanchor outputs(1 without carve-out rule).\n\nWould kind of be nice if package RBF would detect a \"sibling output spend\"\nconflict, and knock it out of the mempool via the other replacement rules?\nGetting rid of the requirement to 1 block csv lock every output would be\nquite nice from a smart contracting composability point of view.\n\n> \"Does this fix Rule 3 Pinning?\"\n\nAs you likely know from previous discussions the biggest scenario this does\nnot fix in my estimation is ANYONECANPAY situations. If the parent\ntransaction can be \"inflated\" by tacking on additional inputs, this means\nthe total weight of the parent tx lowers the effective feerate of the\npackage. Due to this pinning attack there aren't many(?) deployed schemes\nthat use the signature type.\n\nTo mitigate this we would likely have to opt into a more complex policy\nscheme, committing in the annex to \"total mempool package weight\", which\nwould allow mempool package limits to be picked at signing time.\n\nMaybe ANYONECANPAY isn't a very useful paradigm in general, I cannot speak\nto that, but it came up in eltoo-related designs using BIP118, which adopts\nACP-like signing behavior. This can be mitigated via straight forward\npolicy updates as well for BIP118 deployment, but off topic so will leave\nit there.\n\nThe other scenario it doesn't really fix is where HTLC/commitment-like\ntransactions are being resolved in a batch, but due to relative time\nconstraints, you may want to accelerate some and not others. Now you must\npay higher rates to replace all of the transaction bumps. This is a\n\"self-pin\" and \"get good at utxos noob\" type problem, but it's something\nthat axing rule#3 in favor of a Replace-by-ancestor-feerate system would\nget us.\n\n> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n\nCircling back to my ACP point, this regime still allows pinning anytime you\nare sharing a transaction with someone else where you don't have control\nover *all* the inputs. So anytime you are doing a coinjoin-like\ntransaction, someone else's inputs can be self-double-spent, requiring you\nto satisfy rule#3 when replacing theirs, if they're bip125-signaling. If\nthey're not bip125 signaling, you'll have to somehow detect this and/or\ndouble-spend your input back to yourself.\n\n\nFinally, a couple suggestions I've already made elsewhere:\n\n1) I do think that we should seriously consider allowing OP_TRUE to become\na standard script type as part of this policy update. If pinning is solved,\nthen there's no reason to require all those extra bytes for \"binding\" an\nanchor to a specific wallet/user. We can save quite a few bytes by having\nthe input be empty of witness data.\n\n2) If we allow for a single dust-value(0 on up) output which is immediately\nspent by the package, anchors become even easier to to design. No value has\nto be \"sapped\" from contract participants to make an anchor output. There's\nmore complications for this, such as making sure the parent transaction is\ndropped if the child spend is dropped, but maybe it's worth the squeeze. I\ndo think that any L2 uptake of these new rules will take significant\ntime... maybe we should be a bit more ambitious?\n\nCheers,\nGreg\n\nOn Fri, Sep 23, 2022 at 11:27 AM Gloria Zhao via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi everyone,\n>\n> I'm writing to propose a very simple set of mempool/transaction relay\n> policies intended to aid L2/contract protocols. I realized that\n> the previously proposed Package Mempool Accept package RBF [1]\n> had a few remaining problems after digging into the RBF logic more [2].\n> This additional set of policies solves them without requiring a huge RBF\n> overhaul.\n>\n> I've written an implementation (and docs) for Bitcoin Core:\n> https://github.com/bitcoin/bitcoin/pull/25038\n>\n> (You may notice that this proposal incorporates feedback on the PR -\n> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n> and others.)\n>\n> If you are interested in using package RBF/relay to bump presigned\n> transactions, I think you may be interested in reviewing this proposal.\n> This should solve Rule 3 pinning and perhaps allow us\n> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n> the 1-anchor-output, 1000vB child limit too restrictive. Also, if you find\n> a\n> pinning attack or something that makes it unusable for you, I would\n> really really like to know.\n>\n> Note that transactions with nVersion=3 (\"V3 transactions\") are\n> currently non-standard in Bitcoin Core. That means **anything that was\n> standard before this policy change would still be standard\n> afterwards.** If you don't want your transactions to be subject to\n> these rules, just continue whatever you're doing and don't use\n> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n> this would be disruptive for you?\n>\n> **New Policies:**\n>\n> This includes:\n> - a set of additional policy rules applying to V3 transactions\n> - modifications to package RBF rules\n>\n> **V3 transactions:**\n>\n> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n> standard output types, cleanstack, etc.). The following additional\n> rules apply to V3:\n>\n> 1. A V3 transaction can be replaced, even if it does not signal BIP125\n>    replaceability. (It must also meet the other RBF rules around fees,\n> etc. for replacement to happen).\n>\n> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>\n> *Rationale*: Combined with Rule 1, this gives us the property of\n> \"inherited\" replaceability signaling when descendants of unconfirmed\n> transactions are created. Additionally, checking whether a transaction\n> signals replaceability this way does not require mempool traversal,\n> and does not change based on what transactions are mined. It also\n> makes subsequent rules about descendant limits much easier to check.\n>\n> *Note*: The descendant of a *confirmed* V3 transaction does not need to be\n> V3.\n>\n> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>\n> *Rationale*: (Upper bound) the larger the descendant limit, the more\n> transactions may need to be replaced. This is a problematic pinning\n> attack, i.e., a malicious counterparty prevents the transaction from\n> being replaced by adding many descendant transactions that aren't\n> fee-bumping.\n>\n> (Lower bound) at least 1 descendant is required to allow CPFP of the\n> presigned transaction. The contract protocol can create presigned\n> transactions paying 0 fees and 1 output for attaching a CPFP at\n> broadcast time (\"anchor output\"). Without package RBF, multiple anchor\n> outputs would be required to allow each counterparty to fee-bump any\n> presigned transaction. With package RBF, since the presigned\n> transactions can replace each other, 1 anchor output is sufficient.\n>\n> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>    larger than 1000 virtual bytes.\n>\n> *Rationale*: (Upper bound) the larger the descendant size limit, the\n> more vbytes may need to be replaced. With default limits, if the child\n> is e.g. 100,000vB, that might be an additional 100,000sats (at\n> 1sat/vbyte) or more, depending on the feerate.\n>\n> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n> to fund this fee-bump. For example, only allowing the V3 child to have\n> 2 inputs would require L2 protocols to manage a wallet with high-value\n> UTXOs and make batched fee-bumping impossible. However, as the\n> fee-bumping child only needs to fund fees (as opposed to payments),\n> just a few UTXOs should suffice.\n>\n> With a limit of 1000 virtual bytes, depending on the output types, the\n> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n> without requiring a carefully-managed UTXO pool. With 1000 virtual\n> bytes as the descendant limit, the cost to replace a V3 transaction\n> has much lower variance.\n>\n> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n> logic for policy and wallets. A transaction may be up to 100KvB on its\n> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n> the policy is 10KvB.\n>\n> **Package RBF modifications:**\n>\n> 1. The rule around unconfirmed inputs was\n> originally \"A package may include new unconfirmed inputs, but the\n> ancestor feerate of the child must be at least as high as the ancestor\n> feerates of every transaction being replaced.\"\n>\n> The package may still include new unconfirmed inputs. However,\n> the new rule is modified to be \"The minimum between package feerate\n> and ancestor feerate of the child is not lower than the individual\n> feerates of all directly conflicting transactions and the ancestor\n> feerates of all original transactions.\"\n>\n> *Rationale*: We are attempting to ensure that the replacement\n> transactions are not less incentive-compatible to mine. However, a\n> package/transaction's ancestor feerate is not perfectly representative\n> of its incentive compatibility; it may overestimate (some subset of\n> the ancestors could be included by itself if it has other high-feerate\n> descendants or are themselves higher feerate than this\n> package/transaction). Instead, we use the minimum between the package\n> feerate and ancestor feerate of the child as a more conservative value\n> than what was proposed originally.\n>\n> 2. A new rule is added, requiring that all package transactions with\n> mempool conflicts to be V3. This also means the \"sponsoring\"\n> child transaction must be V3.\n>\n> *Note*: Combined with the V3 rules, this means the package must be\n> a child-with-parents package. Since package validation is only\n> attempted if the transactions do not pay sufficient fees to be\n> accepted on their own, this effectively means that only V3\n> transactions can pay to replace their ancestors' conflicts, and only\n> V3 transactions' replacements may be paid for by a descendant.\n>\n> *Rationale*: The fee-related rules are economically rational for\n> ancestor packages, but not necessarily other types of packages.\n> A child-with-parents package is a type of ancestor package. It\n> may be fine to allow any ancestor package, but it's more difficult\n> to account for all of the possibilities. For example, it gets much\n> harder to see that we're applying the descendant limits correctly if\n> the package has a gnarly, many-generation, non-tree shape. I'm also\n> not sure if this policy is 100% incentive-compatible if the sponsor\n> is not a direct descendant of the sponsee.\n>\n> Please see doc/policy/version3_transactions.md and\n> doc/policy/packages.md in the PR for the full set of rules.\n>\n> **Intended usage for LN:**\n>\n> Commitment transactions should be V3 and have 1 anchor output. They\n> can be signed with 0 fees (or 1sat/vbyte) once package relay is deployed\n> on a significant portion of the network. If the commitment tx must\n> be broadcast, determine the desired feerate at broadcast time and\n> spend the anchor output in a high feerate transaction. I'm going to\n> call the broadcasted commitment tx \"the parent\" and the attached\n> fee-bumping tx \"the child.\"\n>\n> - This child must be V3.\n> - This child must be at most 1000vB. Note this restricts the\n>   number of inputs you can use to fund the fee bump. Depending\n> on the output types, this is around 6-15.\n> - One child may fund fees for multiple commitment tx (\"batched\n>   fee-bumping\").\n> - To do a second fee-bump to add more fees, replace the\n>   *child* with a higher-feerate tx. Do not try to attach a grandchild.\n>\n> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n> descendant limits for V3 transactions are very restrictive.\n>\n> **Expected Questions:**\n>\n> \"Does this fix Rule 3 Pinning?\"\n> Yes. The V3 descendant limit restricts both you and your counterparty.\n> Assuming nodes adopted this policy, you may reasonably assume that you\n> only need to replace the commitment transaction + up to 1000vB.\n>\n> \"Only 1 anchor output? What if I need to bump counterparty's commitment tx\n> in mempool?\"\n> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n> You would just package RBF it by attaching a high-feerate child to\n> your commitment tx.\n>\n> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n> transactions based on nVersion?\"\n> Indeed it may be unrealistic to assume V3 transactions will be in\n> widespread use outside of L2. IIUC, unilateral closes are already\n> obvious LN transactions because of the HTLC inputs. For e.g.\n> cooperative closes and opens, I think it makes sense to continue using\n> V2. So, unless I'm missing something, this shouldn't make it worse.\n>\n> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n> replaceable? Is that a backward compatibility issue?\"\n> Yes it's replaceable. It's not an issue AFAICT because,\n> under previous policy, the V3 transaction wouldn't have been\n> in the mempool in the first place.\n>\n> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n> Yes, otherwise someone can use V3 transactions to censor V2\n> transactions spending shared inputs. Note if the\n> original V3 transaction has an unconfirmed V3 parent, this would\n> violate the \"inherited V3\" rule and would be rejected.\n>\n> Thanks for reading! Feedback and review would be much appreciated.\n>\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n> [2]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>\n> Best,\n> Gloria\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220923/43403b3a/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-09-25T23:59:22",
                "message_text_only": "Hi Gloria,\n\nThanks for the progress on package RBF, few early questions.\n\n> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n\n> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n\nIf you're a miner and you receive a non-V3, second descendant of an\nunconfirmed V3 transaction, if the offered fee is in the top mempool\nbacklog, I think you would have an interest to accept such a transaction.\n\nSo I'm not sure if those two rules are compatible with miners incentives...\n\n> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>    larger than 1000 virtual bytes.\n\nIf I understand correctly the 1000 vb upper bound rational, it would be to\nconstraint the pinning counterparty to attach a high fee to a child due to\nthe limited size, if they would like this transaction to be stuck in the\nnetwork mempools. By doing so  this child has high odds to confirm.\n\nI still wonder if this compatible with miner incentives in period of empty\nmempools, in the sense that if you've already a V3 transaction of size\n100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\ncandidate of size 1000 vb offering 10 sat/vb. It could be argued the former\nshould be conserved.\n\n(That said, the hard thing with any replacement strategy we might evict a\nparent transaction *now* to which is attached a high-feerate child *latter*\nmaking for a utxo considered the best ancestor set. Maybe in the long-term\nminers should keep every transaction ever accepted...)\n\n> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n> to fund this fee-bump. For example, only allowing the V3 child to have\n> 2 inputs would require L2 protocols to manage a wallet with high-value\n> UTXOs and make batched fee-bumping impossible. However, as the\n> fee-bumping child only needs to fund fees (as opposed to payments),\n> just a few UTXOs should suffice.\n\nReminder for L2 devs, batched fee-bumping of time-sensitive confirmations\nof commitment transactions is unsafe, as the counterparty could enter in a\n\"cat-and-mouse\" game to replace one of the batch element at each block to\ndelay confirmation of the remaining elements in the batch, I think.\n\nOn the other hand, I wonder if we wouldn't want a higher bound. LN wallets\nare likely to have one big UTXO in their fee-bumping reserve pool, as the\ncost of acquiring UTXO is non-null and in the optimistic case, you don't\nneed to do unilateral closure. Let's say you close dozens of channels at\nthe same time, a UTXO pool management strategy might be to fan-out the\nfirst spends UTXOs in N fan-out outputs ready to feed the remaining\nin-flight channels.\n\n> 1. The rule around unconfirmed inputs was\n> originally \"A package may include new unconfirmed inputs, but the\n> ancestor feerate of the child must be at least as high as the ancestor\n> feerates of every transaction being replaced.\"\n\nNote, I think we would like this new RBF rule to also apply to single\ntransaction package, e.g second-stage HTLC transactions, where a\ncounterparty pins a HTLC-preimage by abusing rule 3. In that case, the\nhonest LN node should be able to broadcast a \"at least as high ancestor\nfeerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\nunconfirmed ancestor to replace, as the commitment transaction, whatever\nthe party it is originating from, should already be confirmed.\n\n> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\ntransactions based on nVersion?\"\n\nAs of today, I think yes you can already fingerprint LN transactions on\nthe  spec-defined amount value of the anchor outputs, 330 sats. There is\nalways one of them on post-anchor commitment transactions. And sadly I\nwould say we'll always have tricky fingerprints leaking from unilateral LN\nclosures such as HTLC/PTLC timelocks...\n\n> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n\nIIUC, a V3 package could replace a V2 package, with the benefit of the new\npackage RBF rules applied. I think this would be a significant advantage\nfor LN, as for the current ~85k of opened channels, the old V2 states\nshouldn't be pinning vectors. Currently, commitment transactions signal\nreplaceability.\n\nLe ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi everyone,\n>\n> I'm writing to propose a very simple set of mempool/transaction relay\n> policies intended to aid L2/contract protocols. I realized that\n> the previously proposed Package Mempool Accept package RBF [1]\n> had a few remaining problems after digging into the RBF logic more [2].\n> This additional set of policies solves them without requiring a huge RBF\n> overhaul.\n>\n> I've written an implementation (and docs) for Bitcoin Core:\n> https://github.com/bitcoin/bitcoin/pull/25038\n>\n> (You may notice that this proposal incorporates feedback on the PR -\n> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n> and others.)\n>\n> If you are interested in using package RBF/relay to bump presigned\n> transactions, I think you may be interested in reviewing this proposal.\n> This should solve Rule 3 pinning and perhaps allow us\n> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n> the 1-anchor-output, 1000vB child limit too restrictive. Also, if you find\n> a\n> pinning attack or something that makes it unusable for you, I would\n> really really like to know.\n>\n> Note that transactions with nVersion=3 (\"V3 transactions\") are\n> currently non-standard in Bitcoin Core. That means **anything that was\n> standard before this policy change would still be standard\n> afterwards.** If you don't want your transactions to be subject to\n> these rules, just continue whatever you're doing and don't use\n> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n> this would be disruptive for you?\n>\n> **New Policies:**\n>\n> This includes:\n> - a set of additional policy rules applying to V3 transactions\n> - modifications to package RBF rules\n>\n> **V3 transactions:**\n>\n> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n> standard output types, cleanstack, etc.). The following additional\n> rules apply to V3:\n>\n> 1. A V3 transaction can be replaced, even if it does not signal BIP125\n>    replaceability. (It must also meet the other RBF rules around fees,\n> etc. for replacement to happen).\n>\n> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>\n> *Rationale*: Combined with Rule 1, this gives us the property of\n> \"inherited\" replaceability signaling when descendants of unconfirmed\n> transactions are created. Additionally, checking whether a transaction\n> signals replaceability this way does not require mempool traversal,\n> and does not change based on what transactions are mined. It also\n> makes subsequent rules about descendant limits much easier to check.\n>\n> *Note*: The descendant of a *confirmed* V3 transaction does not need to be\n> V3.\n>\n> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>\n> *Rationale*: (Upper bound) the larger the descendant limit, the more\n> transactions may need to be replaced. This is a problematic pinning\n> attack, i.e., a malicious counterparty prevents the transaction from\n> being replaced by adding many descendant transactions that aren't\n> fee-bumping.\n>\n> (Lower bound) at least 1 descendant is required to allow CPFP of the\n> presigned transaction. The contract protocol can create presigned\n> transactions paying 0 fees and 1 output for attaching a CPFP at\n> broadcast time (\"anchor output\"). Without package RBF, multiple anchor\n> outputs would be required to allow each counterparty to fee-bump any\n> presigned transaction. With package RBF, since the presigned\n> transactions can replace each other, 1 anchor output is sufficient.\n>\n> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>    larger than 1000 virtual bytes.\n>\n> *Rationale*: (Upper bound) the larger the descendant size limit, the\n> more vbytes may need to be replaced. With default limits, if the child\n> is e.g. 100,000vB, that might be an additional 100,000sats (at\n> 1sat/vbyte) or more, depending on the feerate.\n>\n> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n> to fund this fee-bump. For example, only allowing the V3 child to have\n> 2 inputs would require L2 protocols to manage a wallet with high-value\n> UTXOs and make batched fee-bumping impossible. However, as the\n> fee-bumping child only needs to fund fees (as opposed to payments),\n> just a few UTXOs should suffice.\n>\n> With a limit of 1000 virtual bytes, depending on the output types, the\n> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n> without requiring a carefully-managed UTXO pool. With 1000 virtual\n> bytes as the descendant limit, the cost to replace a V3 transaction\n> has much lower variance.\n>\n> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n> logic for policy and wallets. A transaction may be up to 100KvB on its\n> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n> the policy is 10KvB.\n>\n> **Package RBF modifications:**\n>\n> 1. The rule around unconfirmed inputs was\n> originally \"A package may include new unconfirmed inputs, but the\n> ancestor feerate of the child must be at least as high as the ancestor\n> feerates of every transaction being replaced.\"\n>\n> The package may still include new unconfirmed inputs. However,\n> the new rule is modified to be \"The minimum between package feerate\n> and ancestor feerate of the child is not lower than the individual\n> feerates of all directly conflicting transactions and the ancestor\n> feerates of all original transactions.\"\n>\n> *Rationale*: We are attempting to ensure that the replacement\n> transactions are not less incentive-compatible to mine. However, a\n> package/transaction's ancestor feerate is not perfectly representative\n> of its incentive compatibility; it may overestimate (some subset of\n> the ancestors could be included by itself if it has other high-feerate\n> descendants or are themselves higher feerate than this\n> package/transaction). Instead, we use the minimum between the package\n> feerate and ancestor feerate of the child as a more conservative value\n> than what was proposed originally.\n>\n> 2. A new rule is added, requiring that all package transactions with\n> mempool conflicts to be V3. This also means the \"sponsoring\"\n> child transaction must be V3.\n>\n> *Note*: Combined with the V3 rules, this means the package must be\n> a child-with-parents package. Since package validation is only\n> attempted if the transactions do not pay sufficient fees to be\n> accepted on their own, this effectively means that only V3\n> transactions can pay to replace their ancestors' conflicts, and only\n> V3 transactions' replacements may be paid for by a descendant.\n>\n> *Rationale*: The fee-related rules are economically rational for\n> ancestor packages, but not necessarily other types of packages.\n> A child-with-parents package is a type of ancestor package. It\n> may be fine to allow any ancestor package, but it's more difficult\n> to account for all of the possibilities. For example, it gets much\n> harder to see that we're applying the descendant limits correctly if\n> the package has a gnarly, many-generation, non-tree shape. I'm also\n> not sure if this policy is 100% incentive-compatible if the sponsor\n> is not a direct descendant of the sponsee.\n>\n> Please see doc/policy/version3_transactions.md and\n> doc/policy/packages.md in the PR for the full set of rules.\n>\n> **Intended usage for LN:**\n>\n> Commitment transactions should be V3 and have 1 anchor output. They\n> can be signed with 0 fees (or 1sat/vbyte) once package relay is deployed\n> on a significant portion of the network. If the commitment tx must\n> be broadcast, determine the desired feerate at broadcast time and\n> spend the anchor output in a high feerate transaction. I'm going to\n> call the broadcasted commitment tx \"the parent\" and the attached\n> fee-bumping tx \"the child.\"\n>\n> - This child must be V3.\n> - This child must be at most 1000vB. Note this restricts the\n>   number of inputs you can use to fund the fee bump. Depending\n> on the output types, this is around 6-15.\n> - One child may fund fees for multiple commitment tx (\"batched\n>   fee-bumping\").\n> - To do a second fee-bump to add more fees, replace the\n>   *child* with a higher-feerate tx. Do not try to attach a grandchild.\n>\n> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n> descendant limits for V3 transactions are very restrictive.\n>\n> **Expected Questions:**\n>\n> \"Does this fix Rule 3 Pinning?\"\n> Yes. The V3 descendant limit restricts both you and your counterparty.\n> Assuming nodes adopted this policy, you may reasonably assume that you\n> only need to replace the commitment transaction + up to 1000vB.\n>\n> \"Only 1 anchor output? What if I need to bump counterparty's commitment tx\n> in mempool?\"\n> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n> You would just package RBF it by attaching a high-feerate child to\n> your commitment tx.\n>\n> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n> transactions based on nVersion?\"\n> Indeed it may be unrealistic to assume V3 transactions will be in\n> widespread use outside of L2. IIUC, unilateral closes are already\n> obvious LN transactions because of the HTLC inputs. For e.g.\n> cooperative closes and opens, I think it makes sense to continue using\n> V2. So, unless I'm missing something, this shouldn't make it worse.\n>\n> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n> replaceable? Is that a backward compatibility issue?\"\n> Yes it's replaceable. It's not an issue AFAICT because,\n> under previous policy, the V3 transaction wouldn't have been\n> in the mempool in the first place.\n>\n> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n> Yes, otherwise someone can use V3 transactions to censor V2\n> transactions spending shared inputs. Note if the\n> original V3 transaction has an unconfirmed V3 parent, this would\n> violate the \"inherited V3\" rule and would be rejected.\n>\n> Thanks for reading! Feedback and review would be much appreciated.\n>\n> [1]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n> [2]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>\n> Best,\n> Gloria\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220925/a8ab458a/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-09-26T15:27:40",
                "message_text_only": "Thanks Gloria for this great post.\n\nThis is very valuable work for L2 contracts, and will greatly improve\ntheir security model.\n\n> \"Only 1 anchor output? What if I need to bump counterparty's commitment\ntx in mempool?\"\n> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n> You would just package RBF it by attaching a high-feerate child to\n> your commitment tx.\n\nNote that we can also very easily make that single anchor spendable by\nboth participants (or even anyone), so if you see your counterparty's\ncommitment in your mempool, you can bump it without publishing your\nown commitment, which is quite desirable (your own commitment tx has\nCSV delays on your outputs, whereas your counterparty's commitment tx\ndoesn't).\n\n> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\ntransactions based on nVersion?\"\n\nI agree with you, this isn't worse than today, unilateral closes will\nprobably always be identifiable on-chain.\n\n> Would kind of be nice if package RBF would detect a \"sibling output spend\"\n> conflict, and knock it out of the mempool via the other replacement rules?\n> Getting rid of the requirement to 1 block csv lock every output would be\n> quite nice from a smart contracting composability point of view.\n\n+1, that would be very neat!\n\nThis may be already covered by the current package RBF logic, in that\nscenario we are simply replacing [ParentTx, ChildTx1] with\n[ParentTx, ChildTx2] that pays more fees, right?\n\n> 1) I do think that we should seriously consider allowing OP_TRUE to become\n> a standard script type as part of this policy update. If pinning is\nsolved,\n> then there's no reason to require all those extra bytes for \"binding\" an\n> anchor to a specific wallet/user. We can save quite a few bytes by having\n> the input be empty of witness data.\n> 2) If we allow for a single dust-value(0 on up) output which is\nimmediately\n> spent by the package, anchors become even easier to to design. No value\nhas\n> to be \"sapped\" from contract participants to make an anchor output.\nThere's\n> more complications for this, such as making sure the parent transaction is\n> dropped if the child spend is dropped, but maybe it's worth the squeeze.\n\nI also think both of these could be quite useful. This would probably always\nbe used in combination with a parent transaction that pays 0 fees, so the\n0-value output would always be spent in the same block.\n\nBut this means we could end up with 0-value outputs in the utxo set, if for\nsome reason the parent tx is CPFP-ed via another output than the 0-value\none,\nwhich would be a utxo set bloat issue. But I'd argue that we're probably\nalready creating utxo set bloat with the 330 sat anchor outputs (especially\nsince we use two of them, but only one is usually spent), so it would\nprobably be *better* than what we're doing today.\n\nThanks,\nBastien\n\nLe lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi Gloria,\n>\n> Thanks for the progress on package RBF, few early questions.\n>\n> > 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>\n> > 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>\n> If you're a miner and you receive a non-V3, second descendant of an\n> unconfirmed V3 transaction, if the offered fee is in the top mempool\n> backlog, I think you would have an interest to accept such a transaction.\n>\n> So I'm not sure if those two rules are compatible with miners incentives...\n>\n> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n> >    larger than 1000 virtual bytes.\n>\n> If I understand correctly the 1000 vb upper bound rational, it would be to\n> constraint the pinning counterparty to attach a high fee to a child due to\n> the limited size, if they would like this transaction to be stuck in the\n> network mempools. By doing so  this child has high odds to confirm.\n>\n> I still wonder if this compatible with miner incentives in period of empty\n> mempools, in the sense that if you've already a V3 transaction of size\n> 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n> should be conserved.\n>\n> (That said, the hard thing with any replacement strategy we might evict a\n> parent transaction *now* to which is attached a high-feerate child *latter*\n> making for a utxo considered the best ancestor set. Maybe in the long-term\n> miners should keep every transaction ever accepted...)\n>\n> > (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n> > to fund this fee-bump. For example, only allowing the V3 child to have\n> > 2 inputs would require L2 protocols to manage a wallet with high-value\n> > UTXOs and make batched fee-bumping impossible. However, as the\n> > fee-bumping child only needs to fund fees (as opposed to payments),\n> > just a few UTXOs should suffice.\n>\n> Reminder for L2 devs, batched fee-bumping of time-sensitive confirmations\n> of commitment transactions is unsafe, as the counterparty could enter in a\n> \"cat-and-mouse\" game to replace one of the batch element at each block to\n> delay confirmation of the remaining elements in the batch, I think.\n>\n> On the other hand, I wonder if we wouldn't want a higher bound. LN wallets\n> are likely to have one big UTXO in their fee-bumping reserve pool, as the\n> cost of acquiring UTXO is non-null and in the optimistic case, you don't\n> need to do unilateral closure. Let's say you close dozens of channels at\n> the same time, a UTXO pool management strategy might be to fan-out the\n> first spends UTXOs in N fan-out outputs ready to feed the remaining\n> in-flight channels.\n>\n> > 1. The rule around unconfirmed inputs was\n> > originally \"A package may include new unconfirmed inputs, but the\n> > ancestor feerate of the child must be at least as high as the ancestor\n> > feerates of every transaction being replaced.\"\n>\n> Note, I think we would like this new RBF rule to also apply to single\n> transaction package, e.g second-stage HTLC transactions, where a\n> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n> honest LN node should be able to broadcast a \"at least as high ancestor\n> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n> unconfirmed ancestor to replace, as the commitment transaction, whatever\n> the party it is originating from, should already be confirmed.\n>\n> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n> transactions based on nVersion?\"\n>\n> As of today, I think yes you can already fingerprint LN transactions on\n> the  spec-defined amount value of the anchor outputs, 330 sats. There is\n> always one of them on post-anchor commitment transactions. And sadly I\n> would say we'll always have tricky fingerprints leaking from unilateral LN\n> closures such as HTLC/PTLC timelocks...\n>\n> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>\n> IIUC, a V3 package could replace a V2 package, with the benefit of the new\n> package RBF rules applied. I think this would be a significant advantage\n> for LN, as for the current ~85k of opened channels, the old V2 states\n> shouldn't be pinning vectors. Currently, commitment transactions signal\n> replaceability.\n>\n> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi everyone,\n>>\n>> I'm writing to propose a very simple set of mempool/transaction relay\n>> policies intended to aid L2/contract protocols. I realized that\n>> the previously proposed Package Mempool Accept package RBF [1]\n>> had a few remaining problems after digging into the RBF logic more [2].\n>> This additional set of policies solves them without requiring a huge RBF\n>> overhaul.\n>>\n>> I've written an implementation (and docs) for Bitcoin Core:\n>> https://github.com/bitcoin/bitcoin/pull/25038\n>>\n>> (You may notice that this proposal incorporates feedback on the PR -\n>> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>> and others.)\n>>\n>> If you are interested in using package RBF/relay to bump presigned\n>> transactions, I think you may be interested in reviewing this proposal.\n>> This should solve Rule 3 pinning and perhaps allow us\n>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if you\n>> find a\n>> pinning attack or something that makes it unusable for you, I would\n>> really really like to know.\n>>\n>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>> currently non-standard in Bitcoin Core. That means **anything that was\n>> standard before this policy change would still be standard\n>> afterwards.** If you don't want your transactions to be subject to\n>> these rules, just continue whatever you're doing and don't use\n>> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n>> this would be disruptive for you?\n>>\n>> **New Policies:**\n>>\n>> This includes:\n>> - a set of additional policy rules applying to V3 transactions\n>> - modifications to package RBF rules\n>>\n>> **V3 transactions:**\n>>\n>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>> standard output types, cleanstack, etc.). The following additional\n>> rules apply to V3:\n>>\n>> 1. A V3 transaction can be replaced, even if it does not signal BIP125\n>>    replaceability. (It must also meet the other RBF rules around fees,\n>> etc. for replacement to happen).\n>>\n>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>\n>> *Rationale*: Combined with Rule 1, this gives us the property of\n>> \"inherited\" replaceability signaling when descendants of unconfirmed\n>> transactions are created. Additionally, checking whether a transaction\n>> signals replaceability this way does not require mempool traversal,\n>> and does not change based on what transactions are mined. It also\n>> makes subsequent rules about descendant limits much easier to check.\n>>\n>> *Note*: The descendant of a *confirmed* V3 transaction does not need to\n>> be V3.\n>>\n>> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>\n>> *Rationale*: (Upper bound) the larger the descendant limit, the more\n>> transactions may need to be replaced. This is a problematic pinning\n>> attack, i.e., a malicious counterparty prevents the transaction from\n>> being replaced by adding many descendant transactions that aren't\n>> fee-bumping.\n>>\n>> (Lower bound) at least 1 descendant is required to allow CPFP of the\n>> presigned transaction. The contract protocol can create presigned\n>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>> broadcast time (\"anchor output\"). Without package RBF, multiple anchor\n>> outputs would be required to allow each counterparty to fee-bump any\n>> presigned transaction. With package RBF, since the presigned\n>> transactions can replace each other, 1 anchor output is sufficient.\n>>\n>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>    larger than 1000 virtual bytes.\n>>\n>> *Rationale*: (Upper bound) the larger the descendant size limit, the\n>> more vbytes may need to be replaced. With default limits, if the child\n>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>> 1sat/vbyte) or more, depending on the feerate.\n>>\n>> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>> to fund this fee-bump. For example, only allowing the V3 child to have\n>> 2 inputs would require L2 protocols to manage a wallet with high-value\n>> UTXOs and make batched fee-bumping impossible. However, as the\n>> fee-bumping child only needs to fund fees (as opposed to payments),\n>> just a few UTXOs should suffice.\n>>\n>> With a limit of 1000 virtual bytes, depending on the output types, the\n>> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>> bytes as the descendant limit, the cost to replace a V3 transaction\n>> has much lower variance.\n>>\n>> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n>> logic for policy and wallets. A transaction may be up to 100KvB on its\n>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n>> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n>> the policy is 10KvB.\n>>\n>> **Package RBF modifications:**\n>>\n>> 1. The rule around unconfirmed inputs was\n>> originally \"A package may include new unconfirmed inputs, but the\n>> ancestor feerate of the child must be at least as high as the ancestor\n>> feerates of every transaction being replaced.\"\n>>\n>> The package may still include new unconfirmed inputs. However,\n>> the new rule is modified to be \"The minimum between package feerate\n>> and ancestor feerate of the child is not lower than the individual\n>> feerates of all directly conflicting transactions and the ancestor\n>> feerates of all original transactions.\"\n>>\n>> *Rationale*: We are attempting to ensure that the replacement\n>> transactions are not less incentive-compatible to mine. However, a\n>> package/transaction's ancestor feerate is not perfectly representative\n>> of its incentive compatibility; it may overestimate (some subset of\n>> the ancestors could be included by itself if it has other high-feerate\n>> descendants or are themselves higher feerate than this\n>> package/transaction). Instead, we use the minimum between the package\n>> feerate and ancestor feerate of the child as a more conservative value\n>> than what was proposed originally.\n>>\n>> 2. A new rule is added, requiring that all package transactions with\n>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>> child transaction must be V3.\n>>\n>> *Note*: Combined with the V3 rules, this means the package must be\n>> a child-with-parents package. Since package validation is only\n>> attempted if the transactions do not pay sufficient fees to be\n>> accepted on their own, this effectively means that only V3\n>> transactions can pay to replace their ancestors' conflicts, and only\n>> V3 transactions' replacements may be paid for by a descendant.\n>>\n>> *Rationale*: The fee-related rules are economically rational for\n>> ancestor packages, but not necessarily other types of packages.\n>> A child-with-parents package is a type of ancestor package. It\n>> may be fine to allow any ancestor package, but it's more difficult\n>> to account for all of the possibilities. For example, it gets much\n>> harder to see that we're applying the descendant limits correctly if\n>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>> not sure if this policy is 100% incentive-compatible if the sponsor\n>> is not a direct descendant of the sponsee.\n>>\n>> Please see doc/policy/version3_transactions.md and\n>> doc/policy/packages.md in the PR for the full set of rules.\n>>\n>> **Intended usage for LN:**\n>>\n>> Commitment transactions should be V3 and have 1 anchor output. They\n>> can be signed with 0 fees (or 1sat/vbyte) once package relay is deployed\n>> on a significant portion of the network. If the commitment tx must\n>> be broadcast, determine the desired feerate at broadcast time and\n>> spend the anchor output in a high feerate transaction. I'm going to\n>> call the broadcasted commitment tx \"the parent\" and the attached\n>> fee-bumping tx \"the child.\"\n>>\n>> - This child must be V3.\n>> - This child must be at most 1000vB. Note this restricts the\n>>   number of inputs you can use to fund the fee bump. Depending\n>> on the output types, this is around 6-15.\n>> - One child may fund fees for multiple commitment tx (\"batched\n>>   fee-bumping\").\n>> - To do a second fee-bump to add more fees, replace the\n>>   *child* with a higher-feerate tx. Do not try to attach a grandchild.\n>>\n>> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n>> descendant limits for V3 transactions are very restrictive.\n>>\n>> **Expected Questions:**\n>>\n>> \"Does this fix Rule 3 Pinning?\"\n>> Yes. The V3 descendant limit restricts both you and your counterparty.\n>> Assuming nodes adopted this policy, you may reasonably assume that you\n>> only need to replace the commitment transaction + up to 1000vB.\n>>\n>> \"Only 1 anchor output? What if I need to bump counterparty's commitment\n>> tx in mempool?\"\n>> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>> You would just package RBF it by attaching a high-feerate child to\n>> your commitment tx.\n>>\n>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>> transactions based on nVersion?\"\n>> Indeed it may be unrealistic to assume V3 transactions will be in\n>> widespread use outside of L2. IIUC, unilateral closes are already\n>> obvious LN transactions because of the HTLC inputs. For e.g.\n>> cooperative closes and opens, I think it makes sense to continue using\n>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>\n>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>> replaceable? Is that a backward compatibility issue?\"\n>> Yes it's replaceable. It's not an issue AFAICT because,\n>> under previous policy, the V3 transaction wouldn't have been\n>> in the mempool in the first place.\n>>\n>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>> Yes, otherwise someone can use V3 transactions to censor V2\n>> transactions spending shared inputs. Note if the\n>> original V3 transaction has an unconfirmed V3 parent, this would\n>> violate the \"inherited V3\" rule and would be rejected.\n>>\n>> Thanks for reading! Feedback and review would be much appreciated.\n>>\n>> [1]:\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>> [2]:\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>\n>> Best,\n>> Gloria\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220926/d99c55d5/attachment-0001.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2022-09-26T16:01:54",
                "message_text_only": "Bastien,\n\n> This may be already covered by the current package RBF logic, in that\nscenario we are simply replacing [ParentTx, ChildTx1] with\n[ParentTx, ChildTx2] that pays more fees, right?\n\nFor clarification, package RBF is ParentTx*s*(plural), and\nChildTx(singular), so it might be a bit more complicated than we're\nthinking, and currently the V3 proposal would first de-duplicate the\nParentTx based on what is in the mempool, then look at the \"rest\" of the\ntransactions as a package, then individually. Not the same, not sure how\ndifferent. I'll defer to experts.\n\nBest,\nGreg\n\nOn Mon, Sep 26, 2022 at 11:48 AM Bastien TEINTURIER via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Thanks Gloria for this great post.\n>\n> This is very valuable work for L2 contracts, and will greatly improve\n> their security model.\n>\n> > \"Only 1 anchor output? What if I need to bump counterparty's commitment\n> tx in mempool?\"\n> > You won't need to fee-bump a counterparty's commitment tx using CPFP.\n> > You would just package RBF it by attaching a high-feerate child to\n> > your commitment tx.\n>\n> Note that we can also very easily make that single anchor spendable by\n> both participants (or even anyone), so if you see your counterparty's\n> commitment in your mempool, you can bump it without publishing your\n> own commitment, which is quite desirable (your own commitment tx has\n> CSV delays on your outputs, whereas your counterparty's commitment tx\n> doesn't).\n>\n> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n> transactions based on nVersion?\"\n>\n> I agree with you, this isn't worse than today, unilateral closes will\n> probably always be identifiable on-chain.\n>\n> > Would kind of be nice if package RBF would detect a \"sibling output\n> spend\"\n> > conflict, and knock it out of the mempool via the other replacement\n> rules?\n> > Getting rid of the requirement to 1 block csv lock every output would be\n> > quite nice from a smart contracting composability point of view.\n>\n> +1, that would be very neat!\n>\n> This may be already covered by the current package RBF logic, in that\n> scenario we are simply replacing [ParentTx, ChildTx1] with\n> [ParentTx, ChildTx2] that pays more fees, right?\n>\n> > 1) I do think that we should seriously consider allowing OP_TRUE to\n> become\n> > a standard script type as part of this policy update. If pinning is\n> solved,\n> > then there's no reason to require all those extra bytes for \"binding\" an\n> > anchor to a specific wallet/user. We can save quite a few bytes by having\n> > the input be empty of witness data.\n> > 2) If we allow for a single dust-value(0 on up) output which is\n> immediately\n> > spent by the package, anchors become even easier to to design. No value\n> has\n> > to be \"sapped\" from contract participants to make an anchor output.\n> There's\n> > more complications for this, such as making sure the parent transaction\n> is\n> > dropped if the child spend is dropped, but maybe it's worth the squeeze.\n>\n> I also think both of these could be quite useful. This would probably\n> always\n> be used in combination with a parent transaction that pays 0 fees, so the\n> 0-value output would always be spent in the same block.\n>\n> But this means we could end up with 0-value outputs in the utxo set, if for\n> some reason the parent tx is CPFP-ed via another output than the 0-value\n> one,\n> which would be a utxo set bloat issue. But I'd argue that we're probably\n> already creating utxo set bloat with the 330 sat anchor outputs (especially\n> since we use two of them, but only one is usually spent), so it would\n> probably be *better* than what we're doing today.\n>\n> Thanks,\n> Bastien\n>\n> Le lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi Gloria,\n>>\n>> Thanks for the progress on package RBF, few early questions.\n>>\n>> > 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>\n>> > 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>\n>> If you're a miner and you receive a non-V3, second descendant of an\n>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>> backlog, I think you would have an interest to accept such a transaction.\n>>\n>> So I'm not sure if those two rules are compatible with miners\n>> incentives...\n>>\n>> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>> >    larger than 1000 virtual bytes.\n>>\n>> If I understand correctly the 1000 vb upper bound rational, it would be\n>> to constraint the pinning counterparty to attach a high fee to a child due\n>> to the limited size, if they would like this transaction to be stuck in the\n>> network mempools. By doing so  this child has high odds to confirm.\n>>\n>> I still wonder if this compatible with miner incentives in period of\n>> empty mempools, in the sense that if you've already a V3 transaction of\n>> size 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n>> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n>> should be conserved.\n>>\n>> (That said, the hard thing with any replacement strategy we might evict a\n>> parent transaction *now* to which is attached a high-feerate child *latter*\n>> making for a utxo considered the best ancestor set. Maybe in the long-term\n>> miners should keep every transaction ever accepted...)\n>>\n>> > (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>> > to fund this fee-bump. For example, only allowing the V3 child to have\n>> > 2 inputs would require L2 protocols to manage a wallet with high-value\n>> > UTXOs and make batched fee-bumping impossible. However, as the\n>> > fee-bumping child only needs to fund fees (as opposed to payments),\n>> > just a few UTXOs should suffice.\n>>\n>> Reminder for L2 devs, batched fee-bumping of time-sensitive confirmations\n>> of commitment transactions is unsafe, as the counterparty could enter in a\n>> \"cat-and-mouse\" game to replace one of the batch element at each block to\n>> delay confirmation of the remaining elements in the batch, I think.\n>>\n>> On the other hand, I wonder if we wouldn't want a higher bound. LN\n>> wallets are likely to have one big UTXO in their fee-bumping reserve pool,\n>> as the cost of acquiring UTXO is non-null and in the optimistic case, you\n>> don't need to do unilateral closure. Let's say you close dozens of channels\n>> at the same time, a UTXO pool management strategy might be to fan-out the\n>> first spends UTXOs in N fan-out outputs ready to feed the remaining\n>> in-flight channels.\n>>\n>> > 1. The rule around unconfirmed inputs was\n>> > originally \"A package may include new unconfirmed inputs, but the\n>> > ancestor feerate of the child must be at least as high as the ancestor\n>> > feerates of every transaction being replaced.\"\n>>\n>> Note, I think we would like this new RBF rule to also apply to single\n>> transaction package, e.g second-stage HTLC transactions, where a\n>> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n>> honest LN node should be able to broadcast a \"at least as high ancestor\n>> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n>> unconfirmed ancestor to replace, as the commitment transaction, whatever\n>> the party it is originating from, should already be confirmed.\n>>\n>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>> transactions based on nVersion?\"\n>>\n>> As of today, I think yes you can already fingerprint LN transactions on\n>> the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>> always one of them on post-anchor commitment transactions. And sadly I\n>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>> closures such as HTLC/PTLC timelocks...\n>>\n>> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>\n>> IIUC, a V3 package could replace a V2 package, with the benefit of the\n>> new package RBF rules applied. I think this would be a significant\n>> advantage for LN, as for the current ~85k of opened channels, the old V2\n>> states shouldn't be pinning vectors. Currently, commitment transactions\n>> signal replaceability.\n>>\n>> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi everyone,\n>>>\n>>> I'm writing to propose a very simple set of mempool/transaction relay\n>>> policies intended to aid L2/contract protocols. I realized that\n>>> the previously proposed Package Mempool Accept package RBF [1]\n>>> had a few remaining problems after digging into the RBF logic more [2].\n>>> This additional set of policies solves them without requiring a huge RBF\n>>> overhaul.\n>>>\n>>> I've written an implementation (and docs) for Bitcoin Core:\n>>> https://github.com/bitcoin/bitcoin/pull/25038\n>>>\n>>> (You may notice that this proposal incorporates feedback on the PR -\n>>> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>>> and others.)\n>>>\n>>> If you are interested in using package RBF/relay to bump presigned\n>>> transactions, I think you may be interested in reviewing this proposal.\n>>> This should solve Rule 3 pinning and perhaps allow us\n>>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n>>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if you\n>>> find a\n>>> pinning attack or something that makes it unusable for you, I would\n>>> really really like to know.\n>>>\n>>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>>> currently non-standard in Bitcoin Core. That means **anything that was\n>>> standard before this policy change would still be standard\n>>> afterwards.** If you don't want your transactions to be subject to\n>>> these rules, just continue whatever you're doing and don't use\n>>> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n>>> this would be disruptive for you?\n>>>\n>>> **New Policies:**\n>>>\n>>> This includes:\n>>> - a set of additional policy rules applying to V3 transactions\n>>> - modifications to package RBF rules\n>>>\n>>> **V3 transactions:**\n>>>\n>>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>>> standard output types, cleanstack, etc.). The following additional\n>>> rules apply to V3:\n>>>\n>>> 1. A V3 transaction can be replaced, even if it does not signal BIP125\n>>>    replaceability. (It must also meet the other RBF rules around fees,\n>>> etc. for replacement to happen).\n>>>\n>>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>\n>>> *Rationale*: Combined with Rule 1, this gives us the property of\n>>> \"inherited\" replaceability signaling when descendants of unconfirmed\n>>> transactions are created. Additionally, checking whether a transaction\n>>> signals replaceability this way does not require mempool traversal,\n>>> and does not change based on what transactions are mined. It also\n>>> makes subsequent rules about descendant limits much easier to check.\n>>>\n>>> *Note*: The descendant of a *confirmed* V3 transaction does not need to\n>>> be V3.\n>>>\n>>> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>\n>>> *Rationale*: (Upper bound) the larger the descendant limit, the more\n>>> transactions may need to be replaced. This is a problematic pinning\n>>> attack, i.e., a malicious counterparty prevents the transaction from\n>>> being replaced by adding many descendant transactions that aren't\n>>> fee-bumping.\n>>>\n>>> (Lower bound) at least 1 descendant is required to allow CPFP of the\n>>> presigned transaction. The contract protocol can create presigned\n>>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>>> broadcast time (\"anchor output\"). Without package RBF, multiple anchor\n>>> outputs would be required to allow each counterparty to fee-bump any\n>>> presigned transaction. With package RBF, since the presigned\n>>> transactions can replace each other, 1 anchor output is sufficient.\n>>>\n>>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>    larger than 1000 virtual bytes.\n>>>\n>>> *Rationale*: (Upper bound) the larger the descendant size limit, the\n>>> more vbytes may need to be replaced. With default limits, if the child\n>>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>>> 1sat/vbyte) or more, depending on the feerate.\n>>>\n>>> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>> to fund this fee-bump. For example, only allowing the V3 child to have\n>>> 2 inputs would require L2 protocols to manage a wallet with high-value\n>>> UTXOs and make batched fee-bumping impossible. However, as the\n>>> fee-bumping child only needs to fund fees (as opposed to payments),\n>>> just a few UTXOs should suffice.\n>>>\n>>> With a limit of 1000 virtual bytes, depending on the output types, the\n>>> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n>>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>>> bytes as the descendant limit, the cost to replace a V3 transaction\n>>> has much lower variance.\n>>>\n>>> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n>>> logic for policy and wallets. A transaction may be up to 100KvB on its\n>>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n>>> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n>>> the policy is 10KvB.\n>>>\n>>> **Package RBF modifications:**\n>>>\n>>> 1. The rule around unconfirmed inputs was\n>>> originally \"A package may include new unconfirmed inputs, but the\n>>> ancestor feerate of the child must be at least as high as the ancestor\n>>> feerates of every transaction being replaced.\"\n>>>\n>>> The package may still include new unconfirmed inputs. However,\n>>> the new rule is modified to be \"The minimum between package feerate\n>>> and ancestor feerate of the child is not lower than the individual\n>>> feerates of all directly conflicting transactions and the ancestor\n>>> feerates of all original transactions.\"\n>>>\n>>> *Rationale*: We are attempting to ensure that the replacement\n>>> transactions are not less incentive-compatible to mine. However, a\n>>> package/transaction's ancestor feerate is not perfectly representative\n>>> of its incentive compatibility; it may overestimate (some subset of\n>>> the ancestors could be included by itself if it has other high-feerate\n>>> descendants or are themselves higher feerate than this\n>>> package/transaction). Instead, we use the minimum between the package\n>>> feerate and ancestor feerate of the child as a more conservative value\n>>> than what was proposed originally.\n>>>\n>>> 2. A new rule is added, requiring that all package transactions with\n>>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>>> child transaction must be V3.\n>>>\n>>> *Note*: Combined with the V3 rules, this means the package must be\n>>> a child-with-parents package. Since package validation is only\n>>> attempted if the transactions do not pay sufficient fees to be\n>>> accepted on their own, this effectively means that only V3\n>>> transactions can pay to replace their ancestors' conflicts, and only\n>>> V3 transactions' replacements may be paid for by a descendant.\n>>>\n>>> *Rationale*: The fee-related rules are economically rational for\n>>> ancestor packages, but not necessarily other types of packages.\n>>> A child-with-parents package is a type of ancestor package. It\n>>> may be fine to allow any ancestor package, but it's more difficult\n>>> to account for all of the possibilities. For example, it gets much\n>>> harder to see that we're applying the descendant limits correctly if\n>>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>>> not sure if this policy is 100% incentive-compatible if the sponsor\n>>> is not a direct descendant of the sponsee.\n>>>\n>>> Please see doc/policy/version3_transactions.md and\n>>> doc/policy/packages.md in the PR for the full set of rules.\n>>>\n>>> **Intended usage for LN:**\n>>>\n>>> Commitment transactions should be V3 and have 1 anchor output. They\n>>> can be signed with 0 fees (or 1sat/vbyte) once package relay is deployed\n>>> on a significant portion of the network. If the commitment tx must\n>>> be broadcast, determine the desired feerate at broadcast time and\n>>> spend the anchor output in a high feerate transaction. I'm going to\n>>> call the broadcasted commitment tx \"the parent\" and the attached\n>>> fee-bumping tx \"the child.\"\n>>>\n>>> - This child must be V3.\n>>> - This child must be at most 1000vB. Note this restricts the\n>>>   number of inputs you can use to fund the fee bump. Depending\n>>> on the output types, this is around 6-15.\n>>> - One child may fund fees for multiple commitment tx (\"batched\n>>>   fee-bumping\").\n>>> - To do a second fee-bump to add more fees, replace the\n>>>   *child* with a higher-feerate tx. Do not try to attach a grandchild.\n>>>\n>>> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n>>> descendant limits for V3 transactions are very restrictive.\n>>>\n>>> **Expected Questions:**\n>>>\n>>> \"Does this fix Rule 3 Pinning?\"\n>>> Yes. The V3 descendant limit restricts both you and your counterparty.\n>>> Assuming nodes adopted this policy, you may reasonably assume that you\n>>> only need to replace the commitment transaction + up to 1000vB.\n>>>\n>>> \"Only 1 anchor output? What if I need to bump counterparty's commitment\n>>> tx in mempool?\"\n>>> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>> You would just package RBF it by attaching a high-feerate child to\n>>> your commitment tx.\n>>>\n>>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>> transactions based on nVersion?\"\n>>> Indeed it may be unrealistic to assume V3 transactions will be in\n>>> widespread use outside of L2. IIUC, unilateral closes are already\n>>> obvious LN transactions because of the HTLC inputs. For e.g.\n>>> cooperative closes and opens, I think it makes sense to continue using\n>>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>>\n>>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>>> replaceable? Is that a backward compatibility issue?\"\n>>> Yes it's replaceable. It's not an issue AFAICT because,\n>>> under previous policy, the V3 transaction wouldn't have been\n>>> in the mempool in the first place.\n>>>\n>>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>> Yes, otherwise someone can use V3 transactions to censor V2\n>>> transactions spending shared inputs. Note if the\n>>> original V3 transaction has an unconfirmed V3 parent, this would\n>>> violate the \"inherited V3\" rule and would be rejected.\n>>>\n>>> Thanks for reading! Feedback and review would be much appreciated.\n>>>\n>>> [1]:\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>> [2]:\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>>\n>>> Best,\n>>> Gloria\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220926/7e18188a/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2022-09-26T16:47:49",
                "message_text_only": "Hi Greg, Antoine, Bastien,\n\nThanks very much for the feedback! I interpret most of the discussion\naround limitations as ideas for future improvements rather than criticisms\nof the proposal (please correct me if I'm wrong). I'll try to respond to as\nmuch as possible.\n\nAlso I realize that I didn't contextualize this proposal clearly enough; it\nis very tailored for LN Penalty and definitely doesn't close all pinning\nattacks possible (sorry for confusing anyone). I also agree that some bits\ncan be a little ugly or tack-on; I would definitely prefer a comprehensive\nRBF revamp to fix all our problems and enable other fee-bumping strategies\nsuch as\nsign-ANYONECANPAY-then-bring-your-own-fees-by-adding-inputs-at-broadcast. I\nwas hoping to get some ideas with the \"RBF Improvements\" post in January,\nbut it doesn't seem like we're much closer to a workable proposal. I think\nthis is a minimally-invasive step that works for Lightning today, a small\nfix similar to CPFP carve out.\n\n> As you likely know from previous discussions the biggest scenario this\ndoes not fix in my estimation is ANYONECANPAY situations. If the parent\ntransaction can be \"inflated\" by tacking on additional inputs, this means\nthe total weight of the parent tx lowers the effective feerate of the\npackage.\n\n(For more context to other readers I wrote an explanation for this in\n\"SIGHASH_ANYONECANPAY Pinning\" section of RBF ML post).  Yes, this\nunfortunately doesn't fix any of the existing pinning attacks for single\ntransaction RBF but also doesn't make them worse. This boils down to adding\nan incentive compatibility rule that ensures you can't replace a\ntransaction with something that will confirm slower. Package RBF has an\nancestor feerate-based rule for this (note it is quite conservative and not\nperfect).\n\nSo in the scenario above with the \"inflated\" parent that was signed ACP,\nthe replacement would be rejected because the package ancestor feerate is\nlower than the feerate of what is being replaced. But it is imperfect\n(explained below) and thus I wouldn't recommend it for single transaction\nreplacement. So that attack still exists for single transactions, yes.\n\nThe strategy of using ACP to bring-your-own-fees has its own challenges but\nhopefully has no current use cases as you say. AFAIK LN Penalty is not\naffected by this since it doesn't use ACP, though obviously I agree we\nshould fix it for the future.\n\nSo when I said \"this is intended for fee-bumping presigned txns in\ncontracting protocols,\" I should have said \"this is intended for\nfee-bumping presigned txns specifically using CPFP and anchor outputs.\"\nApologies for forgetting to contextualize, I've been sitting on this for\ntoo long.\n\n> The other scenario it doesn't really fix is where HTLC/commitment-like\ntransactions are being resolved in a batch, but due to relative time\nconstraints, you may want to accelerate some and not others. Now you must\npay higher rates to replace all of the transaction bumps. This is a\n\"self-pin\" and \"get good at utxos noob\" type problem, but it's something\nthat axing rule#3 in favor of a Replace-by-ancestor-feerate system would\nget us.\n\nI understand you to mean \"if you don't have enough UTXOs and you're forced\nto batch-bump, you over-pay because you need to bring them all to the\nhighest target feerate.\" Isn't this kind of separate, wallet-related\nproblem? Contracting or not, surely every wallet needs to have enough UTXOs\nto not batch transactions that shouldn't be batched... I don't see how a\nreplace-by-ancestor-feerate policy would make any difference for this?\n\nAlso in general I'd like to reiterate that ancestor feerate is not a\npanacea to all our RBF incentive compatibility concerns. Like individual\nfeerate, unless we run the mining algorithm, it cannot tell us exactly how\nquickly this transaction would be mined.\n\nWe're estimating the incentive compatibility of the original transaction(s)\nand replacement transaction(s), with the goal of not letting a transaction\nreplace something that would have been more incentive compatible to mine.\nAs such, we don't want to overestimate how good the replacement is, and we\ndon't want to underestimate how good the original transactions are. This\nrule \"The minimum between package feerate and ancestor feerate of the child\nis not lower than the individual feerates of all directly conflicting\ntransactions and the ancestor feerates of all original transactions\" is a\nconservative estimate.\n\n> Would kind of be nice if package RBF would detect a \"sibling output\nspend\" conflict, and knock it out of the mempool via the other replacement\nrules? Getting rid of the requirement to 1 block csv lock every output\nwould be quite nice from a smart contracting composability point of view.\n\nInteresting, so when a transaction hits a mempool tx's descendant limit, we\nconsider evicting one of its descendants in favor of this transaction,\nbased on the RBF rules.\nCool idea! After chewing on this for a bit, I think this *also* just boils\ndown to the fact that RBF should require replacements to be better mining\ncandidates. As in, if we added this policy and it can make us evict the\nsibling and accept a transaction with a bunch of low-feerate ancestor junk,\nit would be a new pinning vector.\n\n> If you're a miner and you receive a non-V3, second descendant of an\nunconfirmed V3 transaction, if the offered fee is in the top mempool\nbacklog, I think you would have an interest to accept such a transaction.\n\n> So I'm not sure if those two rules are compatible with miners\nincentives...\n\nThe same argument can be made for the 26th descendant of a mempool\ntransaction; it's also not entirely incentive-compatible to reject it, but\nthat is not the *only* design goal in mempool policy. Of course, the\ndifference here is that the 25-descendant limit rule is a sensible DoS\nprotection, while this 1-descendant limit rule is more of a \"help the\nBitcoin ecosystem\" policy, just like CPFP carve-out, dust limit, etc. I can\nof course understand why not everyone would be in favor of this, but I do\nthink it's worth it.\n\n> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n> >    larger than 1000 virtual bytes.\n\n> If I understand correctly the 1000 vb upper bound rational, it would be\nto constraint the pinning counterparty to attach a high fee to a child due\nto the limited size, if they would like this transaction to be stuck in the\nnetwork mempools. By doing so  this child has high odds to confirm.\n\nYeah exactly, the \"Rule 3 pin\" is done by adding a child that's high-fee\n(so you have to pay that much to evict it). Because they *don't* want this\ntx to confirm, normally, this child would be really large. If they only\nhave 1000vB for the child, they can't increase the replacement cost without\nalso fee-bumping the transaction to make it confirm faster.\n\n> As of today, I think yes you can already fingerprint LN transactions on\nthe  spec-defined amount value of the anchor outputs, 330 sats. There is\nalways one of them on post-anchor commitment transactions. And sadly I\nwould say we'll always have tricky fingerprints leaking from unilateral LN\nclosures such as HTLC/PTLC timelocks...\n\n> I agree with you, this isn't worse than today, unilateral closes will\nprobably always be identifiable on-chain.\n\nGreat to hear that there is no privacy worsening!\n\nBest,\nGloria\n\nOn Mon, Sep 26, 2022 at 5:02 PM Greg Sanders <gsanders87 at gmail.com> wrote:\n\n> Bastien,\n>\n> > This may be already covered by the current package RBF logic, in that\n> scenario we are simply replacing [ParentTx, ChildTx1] with\n> [ParentTx, ChildTx2] that pays more fees, right?\n>\n> For clarification, package RBF is ParentTx*s*(plural), and\n> ChildTx(singular), so it might be a bit more complicated than we're\n> thinking, and currently the V3 proposal would first de-duplicate the\n> ParentTx based on what is in the mempool, then look at the \"rest\" of the\n> transactions as a package, then individually. Not the same, not sure how\n> different. I'll defer to experts.\n>\n> Best,\n> Greg\n>\n> On Mon, Sep 26, 2022 at 11:48 AM Bastien TEINTURIER via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Thanks Gloria for this great post.\n>>\n>> This is very valuable work for L2 contracts, and will greatly improve\n>> their security model.\n>>\n>> > \"Only 1 anchor output? What if I need to bump counterparty's commitment\n>> tx in mempool?\"\n>> > You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>> > You would just package RBF it by attaching a high-feerate child to\n>> > your commitment tx.\n>>\n>> Note that we can also very easily make that single anchor spendable by\n>> both participants (or even anyone), so if you see your counterparty's\n>> commitment in your mempool, you can bump it without publishing your\n>> own commitment, which is quite desirable (your own commitment tx has\n>> CSV delays on your outputs, whereas your counterparty's commitment tx\n>> doesn't).\n>>\n>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>> transactions based on nVersion?\"\n>>\n>> I agree with you, this isn't worse than today, unilateral closes will\n>> probably always be identifiable on-chain.\n>>\n>> > Would kind of be nice if package RBF would detect a \"sibling output\n>> spend\"\n>> > conflict, and knock it out of the mempool via the other replacement\n>> rules?\n>> > Getting rid of the requirement to 1 block csv lock every output would be\n>> > quite nice from a smart contracting composability point of view.\n>>\n>> +1, that would be very neat!\n>>\n>> This may be already covered by the current package RBF logic, in that\n>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>> [ParentTx, ChildTx2] that pays more fees, right?\n>>\n>> > 1) I do think that we should seriously consider allowing OP_TRUE to\n>> become\n>> > a standard script type as part of this policy update. If pinning is\n>> solved,\n>> > then there's no reason to require all those extra bytes for \"binding\" an\n>> > anchor to a specific wallet/user. We can save quite a few bytes by\n>> having\n>> > the input be empty of witness data.\n>> > 2) If we allow for a single dust-value(0 on up) output which is\n>> immediately\n>> > spent by the package, anchors become even easier to to design. No value\n>> has\n>> > to be \"sapped\" from contract participants to make an anchor output.\n>> There's\n>> > more complications for this, such as making sure the parent transaction\n>> is\n>> > dropped if the child spend is dropped, but maybe it's worth the squeeze.\n>>\n>> I also think both of these could be quite useful. This would probably\n>> always\n>> be used in combination with a parent transaction that pays 0 fees, so the\n>> 0-value output would always be spent in the same block.\n>>\n>> But this means we could end up with 0-value outputs in the utxo set, if\n>> for\n>> some reason the parent tx is CPFP-ed via another output than the 0-value\n>> one,\n>> which would be a utxo set bloat issue. But I'd argue that we're probably\n>> already creating utxo set bloat with the 330 sat anchor outputs\n>> (especially\n>> since we use two of them, but only one is usually spent), so it would\n>> probably be *better* than what we're doing today.\n>>\n>> Thanks,\n>> Bastien\n>>\n>> Le lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi Gloria,\n>>>\n>>> Thanks for the progress on package RBF, few early questions.\n>>>\n>>> > 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>\n>>> > 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>\n>>> If you're a miner and you receive a non-V3, second descendant of an\n>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>> backlog, I think you would have an interest to accept such a transaction.\n>>>\n>>> So I'm not sure if those two rules are compatible with miners\n>>> incentives...\n>>>\n>>> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>> >    larger than 1000 virtual bytes.\n>>>\n>>> If I understand correctly the 1000 vb upper bound rational, it would be\n>>> to constraint the pinning counterparty to attach a high fee to a child due\n>>> to the limited size, if they would like this transaction to be stuck in the\n>>> network mempools. By doing so  this child has high odds to confirm.\n>>>\n>>> I still wonder if this compatible with miner incentives in period of\n>>> empty mempools, in the sense that if you've already a V3 transaction of\n>>> size 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n>>> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n>>> should be conserved.\n>>>\n>>> (That said, the hard thing with any replacement strategy we might evict\n>>> a parent transaction *now* to which is attached a high-feerate child\n>>> *latter* making for a utxo considered the best ancestor set. Maybe in the\n>>> long-term miners should keep every transaction ever accepted...)\n>>>\n>>> > (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>> > to fund this fee-bump. For example, only allowing the V3 child to have\n>>> > 2 inputs would require L2 protocols to manage a wallet with high-value\n>>> > UTXOs and make batched fee-bumping impossible. However, as the\n>>> > fee-bumping child only needs to fund fees (as opposed to payments),\n>>> > just a few UTXOs should suffice.\n>>>\n>>> Reminder for L2 devs, batched fee-bumping of time-sensitive\n>>> confirmations of commitment transactions is unsafe, as the counterparty\n>>> could enter in a \"cat-and-mouse\" game to replace one of the batch element\n>>> at each block to delay confirmation of the remaining elements in the batch,\n>>> I think.\n>>>\n>>> On the other hand, I wonder if we wouldn't want a higher bound. LN\n>>> wallets are likely to have one big UTXO in their fee-bumping reserve pool,\n>>> as the cost of acquiring UTXO is non-null and in the optimistic case, you\n>>> don't need to do unilateral closure. Let's say you close dozens of channels\n>>> at the same time, a UTXO pool management strategy might be to fan-out the\n>>> first spends UTXOs in N fan-out outputs ready to feed the remaining\n>>> in-flight channels.\n>>>\n>>> > 1. The rule around unconfirmed inputs was\n>>> > originally \"A package may include new unconfirmed inputs, but the\n>>> > ancestor feerate of the child must be at least as high as the ancestor\n>>> > feerates of every transaction being replaced.\"\n>>>\n>>> Note, I think we would like this new RBF rule to also apply to single\n>>> transaction package, e.g second-stage HTLC transactions, where a\n>>> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n>>> honest LN node should be able to broadcast a \"at least as high ancestor\n>>> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n>>> unconfirmed ancestor to replace, as the commitment transaction, whatever\n>>> the party it is originating from, should already be confirmed.\n>>>\n>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>> transactions based on nVersion?\"\n>>>\n>>> As of today, I think yes you can already fingerprint LN transactions on\n>>> the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>> always one of them on post-anchor commitment transactions. And sadly I\n>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>> closures such as HTLC/PTLC timelocks...\n>>>\n>>> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>\n>>> IIUC, a V3 package could replace a V2 package, with the benefit of the\n>>> new package RBF rules applied. I think this would be a significant\n>>> advantage for LN, as for the current ~85k of opened channels, the old V2\n>>> states shouldn't be pinning vectors. Currently, commitment transactions\n>>> signal replaceability.\n>>>\n>>> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>\n>>>> Hi everyone,\n>>>>\n>>>> I'm writing to propose a very simple set of mempool/transaction relay\n>>>> policies intended to aid L2/contract protocols. I realized that\n>>>> the previously proposed Package Mempool Accept package RBF [1]\n>>>> had a few remaining problems after digging into the RBF logic more [2].\n>>>> This additional set of policies solves them without requiring a huge\n>>>> RBF overhaul.\n>>>>\n>>>> I've written an implementation (and docs) for Bitcoin Core:\n>>>> https://github.com/bitcoin/bitcoin/pull/25038\n>>>>\n>>>> (You may notice that this proposal incorporates feedback on the PR -\n>>>> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>>>> and others.)\n>>>>\n>>>> If you are interested in using package RBF/relay to bump presigned\n>>>> transactions, I think you may be interested in reviewing this proposal.\n>>>> This should solve Rule 3 pinning and perhaps allow us\n>>>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n>>>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if you\n>>>> find a\n>>>> pinning attack or something that makes it unusable for you, I would\n>>>> really really like to know.\n>>>>\n>>>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>>>> currently non-standard in Bitcoin Core. That means **anything that was\n>>>> standard before this policy change would still be standard\n>>>> afterwards.** If you don't want your transactions to be subject to\n>>>> these rules, just continue whatever you're doing and don't use\n>>>> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n>>>> this would be disruptive for you?\n>>>>\n>>>> **New Policies:**\n>>>>\n>>>> This includes:\n>>>> - a set of additional policy rules applying to V3 transactions\n>>>> - modifications to package RBF rules\n>>>>\n>>>> **V3 transactions:**\n>>>>\n>>>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>>>> standard output types, cleanstack, etc.). The following additional\n>>>> rules apply to V3:\n>>>>\n>>>> 1. A V3 transaction can be replaced, even if it does not signal BIP125\n>>>>    replaceability. (It must also meet the other RBF rules around fees,\n>>>> etc. for replacement to happen).\n>>>>\n>>>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>\n>>>> *Rationale*: Combined with Rule 1, this gives us the property of\n>>>> \"inherited\" replaceability signaling when descendants of unconfirmed\n>>>> transactions are created. Additionally, checking whether a transaction\n>>>> signals replaceability this way does not require mempool traversal,\n>>>> and does not change based on what transactions are mined. It also\n>>>> makes subsequent rules about descendant limits much easier to check.\n>>>>\n>>>> *Note*: The descendant of a *confirmed* V3 transaction does not need to\n>>>> be V3.\n>>>>\n>>>> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>\n>>>> *Rationale*: (Upper bound) the larger the descendant limit, the more\n>>>> transactions may need to be replaced. This is a problematic pinning\n>>>> attack, i.e., a malicious counterparty prevents the transaction from\n>>>> being replaced by adding many descendant transactions that aren't\n>>>> fee-bumping.\n>>>>\n>>>> (Lower bound) at least 1 descendant is required to allow CPFP of the\n>>>> presigned transaction. The contract protocol can create presigned\n>>>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>>>> broadcast time (\"anchor output\"). Without package RBF, multiple anchor\n>>>> outputs would be required to allow each counterparty to fee-bump any\n>>>> presigned transaction. With package RBF, since the presigned\n>>>> transactions can replace each other, 1 anchor output is sufficient.\n>>>>\n>>>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>    larger than 1000 virtual bytes.\n>>>>\n>>>> *Rationale*: (Upper bound) the larger the descendant size limit, the\n>>>> more vbytes may need to be replaced. With default limits, if the child\n>>>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>>>> 1sat/vbyte) or more, depending on the feerate.\n>>>>\n>>>> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>>> to fund this fee-bump. For example, only allowing the V3 child to have\n>>>> 2 inputs would require L2 protocols to manage a wallet with high-value\n>>>> UTXOs and make batched fee-bumping impossible. However, as the\n>>>> fee-bumping child only needs to fund fees (as opposed to payments),\n>>>> just a few UTXOs should suffice.\n>>>>\n>>>> With a limit of 1000 virtual bytes, depending on the output types, the\n>>>> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n>>>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>>>> bytes as the descendant limit, the cost to replace a V3 transaction\n>>>> has much lower variance.\n>>>>\n>>>> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n>>>> logic for policy and wallets. A transaction may be up to 100KvB on its\n>>>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>>>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n>>>> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n>>>> the policy is 10KvB.\n>>>>\n>>>> **Package RBF modifications:**\n>>>>\n>>>> 1. The rule around unconfirmed inputs was\n>>>> originally \"A package may include new unconfirmed inputs, but the\n>>>> ancestor feerate of the child must be at least as high as the ancestor\n>>>> feerates of every transaction being replaced.\"\n>>>>\n>>>> The package may still include new unconfirmed inputs. However,\n>>>> the new rule is modified to be \"The minimum between package feerate\n>>>> and ancestor feerate of the child is not lower than the individual\n>>>> feerates of all directly conflicting transactions and the ancestor\n>>>> feerates of all original transactions.\"\n>>>>\n>>>> *Rationale*: We are attempting to ensure that the replacement\n>>>> transactions are not less incentive-compatible to mine. However, a\n>>>> package/transaction's ancestor feerate is not perfectly representative\n>>>> of its incentive compatibility; it may overestimate (some subset of\n>>>> the ancestors could be included by itself if it has other high-feerate\n>>>> descendants or are themselves higher feerate than this\n>>>> package/transaction). Instead, we use the minimum between the package\n>>>> feerate and ancestor feerate of the child as a more conservative value\n>>>> than what was proposed originally.\n>>>>\n>>>> 2. A new rule is added, requiring that all package transactions with\n>>>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>>>> child transaction must be V3.\n>>>>\n>>>> *Note*: Combined with the V3 rules, this means the package must be\n>>>> a child-with-parents package. Since package validation is only\n>>>> attempted if the transactions do not pay sufficient fees to be\n>>>> accepted on their own, this effectively means that only V3\n>>>> transactions can pay to replace their ancestors' conflicts, and only\n>>>> V3 transactions' replacements may be paid for by a descendant.\n>>>>\n>>>> *Rationale*: The fee-related rules are economically rational for\n>>>> ancestor packages, but not necessarily other types of packages.\n>>>> A child-with-parents package is a type of ancestor package. It\n>>>> may be fine to allow any ancestor package, but it's more difficult\n>>>> to account for all of the possibilities. For example, it gets much\n>>>> harder to see that we're applying the descendant limits correctly if\n>>>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>>>> not sure if this policy is 100% incentive-compatible if the sponsor\n>>>> is not a direct descendant of the sponsee.\n>>>>\n>>>> Please see doc/policy/version3_transactions.md and\n>>>> doc/policy/packages.md in the PR for the full set of rules.\n>>>>\n>>>> **Intended usage for LN:**\n>>>>\n>>>> Commitment transactions should be V3 and have 1 anchor output. They\n>>>> can be signed with 0 fees (or 1sat/vbyte) once package relay is deployed\n>>>> on a significant portion of the network. If the commitment tx must\n>>>> be broadcast, determine the desired feerate at broadcast time and\n>>>> spend the anchor output in a high feerate transaction. I'm going to\n>>>> call the broadcasted commitment tx \"the parent\" and the attached\n>>>> fee-bumping tx \"the child.\"\n>>>>\n>>>> - This child must be V3.\n>>>> - This child must be at most 1000vB. Note this restricts the\n>>>>   number of inputs you can use to fund the fee bump. Depending\n>>>> on the output types, this is around 6-15.\n>>>> - One child may fund fees for multiple commitment tx (\"batched\n>>>>   fee-bumping\").\n>>>> - To do a second fee-bump to add more fees, replace the\n>>>>   *child* with a higher-feerate tx. Do not try to attach a grandchild.\n>>>>\n>>>> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n>>>> descendant limits for V3 transactions are very restrictive.\n>>>>\n>>>> **Expected Questions:**\n>>>>\n>>>> \"Does this fix Rule 3 Pinning?\"\n>>>> Yes. The V3 descendant limit restricts both you and your counterparty.\n>>>> Assuming nodes adopted this policy, you may reasonably assume that you\n>>>> only need to replace the commitment transaction + up to 1000vB.\n>>>>\n>>>> \"Only 1 anchor output? What if I need to bump counterparty's commitment\n>>>> tx in mempool?\"\n>>>> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>>> You would just package RBF it by attaching a high-feerate child to\n>>>> your commitment tx.\n>>>>\n>>>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>> transactions based on nVersion?\"\n>>>> Indeed it may be unrealistic to assume V3 transactions will be in\n>>>> widespread use outside of L2. IIUC, unilateral closes are already\n>>>> obvious LN transactions because of the HTLC inputs. For e.g.\n>>>> cooperative closes and opens, I think it makes sense to continue using\n>>>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>>>\n>>>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>>>> replaceable? Is that a backward compatibility issue?\"\n>>>> Yes it's replaceable. It's not an issue AFAICT because,\n>>>> under previous policy, the V3 transaction wouldn't have been\n>>>> in the mempool in the first place.\n>>>>\n>>>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>> Yes, otherwise someone can use V3 transactions to censor V2\n>>>> transactions spending shared inputs. Note if the\n>>>> original V3 transaction has an unconfirmed V3 parent, this would\n>>>> violate the \"inherited V3\" rule and would be rejected.\n>>>>\n>>>> Thanks for reading! Feedback and review would be much appreciated.\n>>>>\n>>>> [1]:\n>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>>> [2]:\n>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>>>\n>>>> Best,\n>>>> Gloria\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220926/f053868f/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-09-29T09:15:02",
                "message_text_only": "Hi Gloria, Greg,\n\n> I interpret most of the discussion around limitations as ideas for\n> future improvements rather than criticisms of the proposal\n\nAs far as I'm concerned, definitely!\n\nMy current understanding is that the main change/improvement that would\nmake sense here is restricting the whole v3 package's size (instead of\njust the child) via committing to a specific value in the taproot annex\n(also note that it's probably not just the v3 package's size, it should\nbe the whole unconfirmed package including potential v2 unconfirmed\nancestors).\n\nWhile I think this would be very valuable and would like to see this\nhappen, I believe that can be done in a second, separate step since this\nwould make relay policy stricter (some v3 transactions that previously\npropagated wouldn't propagate under this new rule). As long as you are\nable to find a path to miners through upgraded peers that use this annex\napproach, you should be able to resolve ACP pinning issues?\n\nI'm curious to know how other people feel about that: is it ok to do\nlater or should we try to implement this for the first release of v3\ntransactions?\n\nThe other change mentioned (making OP_TRUE standard and allowing outputs\nthat are below dust) can be added later, as those won't be standard until\nwe start allowing them, so there shouldn't be any backwards-compatibility\nissue with postponing this change. But maybe it's still worth having from\nthe get-go, even though it may take a bit more time? Again, I'm curious to\nhave other people's opinion here, I'd be happy to get all of those directly\nin the first release of v3 transactions, but I don't know how much\nimplementation will have to go into that.\n\n> For clarification, package RBF is ParentTx*s*(plural), and\nChildTx(singular),\n> so it might be a bit more complicated than we're thinking\n\nRight, good catch, this does require new logic to handle this case.\nAs Gloria points out, this should be doable, and is definitely worth\nadding (those CSV 1 on every other output are really hacky, glad to\nfind a way to get rid of them).\n\nThanks,\nBastien\n\nLe lun. 26 sept. 2022 \u00e0 18:48, Gloria Zhao <gloriajzhao at gmail.com> a \u00e9crit :\n\n> Hi Greg, Antoine, Bastien,\n>\n> Thanks very much for the feedback! I interpret most of the discussion\n> around limitations as ideas for future improvements rather than criticisms\n> of the proposal (please correct me if I'm wrong). I'll try to respond to as\n> much as possible.\n>\n> Also I realize that I didn't contextualize this proposal clearly enough;\n> it is very tailored for LN Penalty and definitely doesn't close all pinning\n> attacks possible (sorry for confusing anyone). I also agree that some bits\n> can be a little ugly or tack-on; I would definitely prefer a comprehensive\n> RBF revamp to fix all our problems and enable other fee-bumping strategies\n> such as\n> sign-ANYONECANPAY-then-bring-your-own-fees-by-adding-inputs-at-broadcast. I\n> was hoping to get some ideas with the \"RBF Improvements\" post in January,\n> but it doesn't seem like we're much closer to a workable proposal. I think\n> this is a minimally-invasive step that works for Lightning today, a small\n> fix similar to CPFP carve out.\n>\n> > As you likely know from previous discussions the biggest scenario this\n> does not fix in my estimation is ANYONECANPAY situations. If the parent\n> transaction can be \"inflated\" by tacking on additional inputs, this means\n> the total weight of the parent tx lowers the effective feerate of the\n> package.\n>\n> (For more context to other readers I wrote an explanation for this in\n> \"SIGHASH_ANYONECANPAY Pinning\" section of RBF ML post).  Yes, this\n> unfortunately doesn't fix any of the existing pinning attacks for single\n> transaction RBF but also doesn't make them worse. This boils down to adding\n> an incentive compatibility rule that ensures you can't replace a\n> transaction with something that will confirm slower. Package RBF has an\n> ancestor feerate-based rule for this (note it is quite conservative and not\n> perfect).\n>\n> So in the scenario above with the \"inflated\" parent that was signed ACP,\n> the replacement would be rejected because the package ancestor feerate is\n> lower than the feerate of what is being replaced. But it is imperfect\n> (explained below) and thus I wouldn't recommend it for single transaction\n> replacement. So that attack still exists for single transactions, yes.\n>\n> The strategy of using ACP to bring-your-own-fees has its own challenges\n> but hopefully has no current use cases as you say. AFAIK LN Penalty is not\n> affected by this since it doesn't use ACP, though obviously I agree we\n> should fix it for the future.\n>\n> So when I said \"this is intended for fee-bumping presigned txns in\n> contracting protocols,\" I should have said \"this is intended for\n> fee-bumping presigned txns specifically using CPFP and anchor outputs.\"\n> Apologies for forgetting to contextualize, I've been sitting on this for\n> too long.\n>\n> > The other scenario it doesn't really fix is where HTLC/commitment-like\n> transactions are being resolved in a batch, but due to relative time\n> constraints, you may want to accelerate some and not others. Now you must\n> pay higher rates to replace all of the transaction bumps. This is a\n> \"self-pin\" and \"get good at utxos noob\" type problem, but it's something\n> that axing rule#3 in favor of a Replace-by-ancestor-feerate system would\n> get us.\n>\n> I understand you to mean \"if you don't have enough UTXOs and you're forced\n> to batch-bump, you over-pay because you need to bring them all to the\n> highest target feerate.\" Isn't this kind of separate, wallet-related\n> problem? Contracting or not, surely every wallet needs to have enough UTXOs\n> to not batch transactions that shouldn't be batched... I don't see how a\n> replace-by-ancestor-feerate policy would make any difference for this?\n>\n> Also in general I'd like to reiterate that ancestor feerate is not a\n> panacea to all our RBF incentive compatibility concerns. Like individual\n> feerate, unless we run the mining algorithm, it cannot tell us exactly how\n> quickly this transaction would be mined.\n>\n> We're estimating the incentive compatibility of the original\n> transaction(s) and replacement transaction(s), with the goal of not letting\n> a transaction replace something that would have been more incentive\n> compatible to mine. As such, we don't want to overestimate how good the\n> replacement is, and we don't want to underestimate how good the original\n> transactions are. This rule \"The minimum between package feerate and\n> ancestor feerate of the child is not lower than the individual feerates of\n> all directly conflicting transactions and the ancestor feerates of all\n> original transactions\" is a conservative estimate.\n>\n> > Would kind of be nice if package RBF would detect a \"sibling output\n> spend\" conflict, and knock it out of the mempool via the other replacement\n> rules? Getting rid of the requirement to 1 block csv lock every output\n> would be quite nice from a smart contracting composability point of view.\n>\n> Interesting, so when a transaction hits a mempool tx's descendant limit,\n> we consider evicting one of its descendants in favor of this transaction,\n> based on the RBF rules.\n> Cool idea! After chewing on this for a bit, I think this *also* just boils\n> down to the fact that RBF should require replacements to be better mining\n> candidates. As in, if we added this policy and it can make us evict the\n> sibling and accept a transaction with a bunch of low-feerate ancestor junk,\n> it would be a new pinning vector.\n>\n> > If you're a miner and you receive a non-V3, second descendant of an\n> unconfirmed V3 transaction, if the offered fee is in the top mempool\n> backlog, I think you would have an interest to accept such a transaction.\n>\n> > So I'm not sure if those two rules are compatible with miners\n> incentives...\n>\n> The same argument can be made for the 26th descendant of a mempool\n> transaction; it's also not entirely incentive-compatible to reject it, but\n> that is not the *only* design goal in mempool policy. Of course, the\n> difference here is that the 25-descendant limit rule is a sensible DoS\n> protection, while this 1-descendant limit rule is more of a \"help the\n> Bitcoin ecosystem\" policy, just like CPFP carve-out, dust limit, etc. I can\n> of course understand why not everyone would be in favor of this, but I do\n> think it's worth it.\n>\n> > > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n> > >    larger than 1000 virtual bytes.\n>\n> > If I understand correctly the 1000 vb upper bound rational, it would be\n> to constraint the pinning counterparty to attach a high fee to a child due\n> to the limited size, if they would like this transaction to be stuck in the\n> network mempools. By doing so  this child has high odds to confirm.\n>\n> Yeah exactly, the \"Rule 3 pin\" is done by adding a child that's high-fee\n> (so you have to pay that much to evict it). Because they *don't* want this\n> tx to confirm, normally, this child would be really large. If they only\n> have 1000vB for the child, they can't increase the replacement cost without\n> also fee-bumping the transaction to make it confirm faster.\n>\n> > As of today, I think yes you can already fingerprint LN transactions on\n> the  spec-defined amount value of the anchor outputs, 330 sats. There is\n> always one of them on post-anchor commitment transactions. And sadly I\n> would say we'll always have tricky fingerprints leaking from unilateral LN\n> closures such as HTLC/PTLC timelocks...\n>\n> > I agree with you, this isn't worse than today, unilateral closes will\n> probably always be identifiable on-chain.\n>\n> Great to hear that there is no privacy worsening!\n>\n> Best,\n> Gloria\n>\n> On Mon, Sep 26, 2022 at 5:02 PM Greg Sanders <gsanders87 at gmail.com> wrote:\n>\n>> Bastien,\n>>\n>> > This may be already covered by the current package RBF logic, in that\n>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>> [ParentTx, ChildTx2] that pays more fees, right?\n>>\n>> For clarification, package RBF is ParentTx*s*(plural), and\n>> ChildTx(singular), so it might be a bit more complicated than we're\n>> thinking, and currently the V3 proposal would first de-duplicate the\n>> ParentTx based on what is in the mempool, then look at the \"rest\" of the\n>> transactions as a package, then individually. Not the same, not sure how\n>> different. I'll defer to experts.\n>>\n>> Best,\n>> Greg\n>>\n>> On Mon, Sep 26, 2022 at 11:48 AM Bastien TEINTURIER via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Thanks Gloria for this great post.\n>>>\n>>> This is very valuable work for L2 contracts, and will greatly improve\n>>> their security model.\n>>>\n>>> > \"Only 1 anchor output? What if I need to bump counterparty's\n>>> commitment tx in mempool?\"\n>>> > You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>> > You would just package RBF it by attaching a high-feerate child to\n>>> > your commitment tx.\n>>>\n>>> Note that we can also very easily make that single anchor spendable by\n>>> both participants (or even anyone), so if you see your counterparty's\n>>> commitment in your mempool, you can bump it without publishing your\n>>> own commitment, which is quite desirable (your own commitment tx has\n>>> CSV delays on your outputs, whereas your counterparty's commitment tx\n>>> doesn't).\n>>>\n>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>> transactions based on nVersion?\"\n>>>\n>>> I agree with you, this isn't worse than today, unilateral closes will\n>>> probably always be identifiable on-chain.\n>>>\n>>> > Would kind of be nice if package RBF would detect a \"sibling output\n>>> spend\"\n>>> > conflict, and knock it out of the mempool via the other replacement\n>>> rules?\n>>> > Getting rid of the requirement to 1 block csv lock every output would\n>>> be\n>>> > quite nice from a smart contracting composability point of view.\n>>>\n>>> +1, that would be very neat!\n>>>\n>>> This may be already covered by the current package RBF logic, in that\n>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>\n>>> > 1) I do think that we should seriously consider allowing OP_TRUE to\n>>> become\n>>> > a standard script type as part of this policy update. If pinning is\n>>> solved,\n>>> > then there's no reason to require all those extra bytes for \"binding\"\n>>> an\n>>> > anchor to a specific wallet/user. We can save quite a few bytes by\n>>> having\n>>> > the input be empty of witness data.\n>>> > 2) If we allow for a single dust-value(0 on up) output which is\n>>> immediately\n>>> > spent by the package, anchors become even easier to to design. No\n>>> value has\n>>> > to be \"sapped\" from contract participants to make an anchor output.\n>>> There's\n>>> > more complications for this, such as making sure the parent\n>>> transaction is\n>>> > dropped if the child spend is dropped, but maybe it's worth the\n>>> squeeze.\n>>>\n>>> I also think both of these could be quite useful. This would probably\n>>> always\n>>> be used in combination with a parent transaction that pays 0 fees, so the\n>>> 0-value output would always be spent in the same block.\n>>>\n>>> But this means we could end up with 0-value outputs in the utxo set, if\n>>> for\n>>> some reason the parent tx is CPFP-ed via another output than the 0-value\n>>> one,\n>>> which would be a utxo set bloat issue. But I'd argue that we're probably\n>>> already creating utxo set bloat with the 330 sat anchor outputs\n>>> (especially\n>>> since we use two of them, but only one is usually spent), so it would\n>>> probably be *better* than what we're doing today.\n>>>\n>>> Thanks,\n>>> Bastien\n>>>\n>>> Le lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>\n>>>> Hi Gloria,\n>>>>\n>>>> Thanks for the progress on package RBF, few early questions.\n>>>>\n>>>> > 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>\n>>>> > 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>\n>>>> If you're a miner and you receive a non-V3, second descendant of an\n>>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>>> backlog, I think you would have an interest to accept such a transaction.\n>>>>\n>>>> So I'm not sure if those two rules are compatible with miners\n>>>> incentives...\n>>>>\n>>>> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>> >    larger than 1000 virtual bytes.\n>>>>\n>>>> If I understand correctly the 1000 vb upper bound rational, it would be\n>>>> to constraint the pinning counterparty to attach a high fee to a child due\n>>>> to the limited size, if they would like this transaction to be stuck in the\n>>>> network mempools. By doing so  this child has high odds to confirm.\n>>>>\n>>>> I still wonder if this compatible with miner incentives in period of\n>>>> empty mempools, in the sense that if you've already a V3 transaction of\n>>>> size 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n>>>> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n>>>> should be conserved.\n>>>>\n>>>> (That said, the hard thing with any replacement strategy we might evict\n>>>> a parent transaction *now* to which is attached a high-feerate child\n>>>> *latter* making for a utxo considered the best ancestor set. Maybe in the\n>>>> long-term miners should keep every transaction ever accepted...)\n>>>>\n>>>> > (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>>> > to fund this fee-bump. For example, only allowing the V3 child to have\n>>>> > 2 inputs would require L2 protocols to manage a wallet with high-value\n>>>> > UTXOs and make batched fee-bumping impossible. However, as the\n>>>> > fee-bumping child only needs to fund fees (as opposed to payments),\n>>>> > just a few UTXOs should suffice.\n>>>>\n>>>> Reminder for L2 devs, batched fee-bumping of time-sensitive\n>>>> confirmations of commitment transactions is unsafe, as the counterparty\n>>>> could enter in a \"cat-and-mouse\" game to replace one of the batch element\n>>>> at each block to delay confirmation of the remaining elements in the batch,\n>>>> I think.\n>>>>\n>>>> On the other hand, I wonder if we wouldn't want a higher bound. LN\n>>>> wallets are likely to have one big UTXO in their fee-bumping reserve pool,\n>>>> as the cost of acquiring UTXO is non-null and in the optimistic case, you\n>>>> don't need to do unilateral closure. Let's say you close dozens of channels\n>>>> at the same time, a UTXO pool management strategy might be to fan-out the\n>>>> first spends UTXOs in N fan-out outputs ready to feed the remaining\n>>>> in-flight channels.\n>>>>\n>>>> > 1. The rule around unconfirmed inputs was\n>>>> > originally \"A package may include new unconfirmed inputs, but the\n>>>> > ancestor feerate of the child must be at least as high as the ancestor\n>>>> > feerates of every transaction being replaced.\"\n>>>>\n>>>> Note, I think we would like this new RBF rule to also apply to single\n>>>> transaction package, e.g second-stage HTLC transactions, where a\n>>>> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n>>>> honest LN node should be able to broadcast a \"at least as high ancestor\n>>>> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n>>>> unconfirmed ancestor to replace, as the commitment transaction, whatever\n>>>> the party it is originating from, should already be confirmed.\n>>>>\n>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>> transactions based on nVersion?\"\n>>>>\n>>>> As of today, I think yes you can already fingerprint LN transactions on\n>>>> the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>>> always one of them on post-anchor commitment transactions. And sadly I\n>>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>>> closures such as HTLC/PTLC timelocks...\n>>>>\n>>>> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>\n>>>> IIUC, a V3 package could replace a V2 package, with the benefit of the\n>>>> new package RBF rules applied. I think this would be a significant\n>>>> advantage for LN, as for the current ~85k of opened channels, the old V2\n>>>> states shouldn't be pinning vectors. Currently, commitment transactions\n>>>> signal replaceability.\n>>>>\n>>>> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>\n>>>>> Hi everyone,\n>>>>>\n>>>>> I'm writing to propose a very simple set of mempool/transaction relay\n>>>>> policies intended to aid L2/contract protocols. I realized that\n>>>>> the previously proposed Package Mempool Accept package RBF [1]\n>>>>> had a few remaining problems after digging into the RBF logic more [2].\n>>>>> This additional set of policies solves them without requiring a huge\n>>>>> RBF overhaul.\n>>>>>\n>>>>> I've written an implementation (and docs) for Bitcoin Core:\n>>>>> https://github.com/bitcoin/bitcoin/pull/25038\n>>>>>\n>>>>> (You may notice that this proposal incorporates feedback on the PR -\n>>>>> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>>>>> and others.)\n>>>>>\n>>>>> If you are interested in using package RBF/relay to bump presigned\n>>>>> transactions, I think you may be interested in reviewing this proposal.\n>>>>> This should solve Rule 3 pinning and perhaps allow us\n>>>>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n>>>>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if you\n>>>>> find a\n>>>>> pinning attack or something that makes it unusable for you, I would\n>>>>> really really like to know.\n>>>>>\n>>>>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>>>>> currently non-standard in Bitcoin Core. That means **anything that was\n>>>>> standard before this policy change would still be standard\n>>>>> afterwards.** If you don't want your transactions to be subject to\n>>>>> these rules, just continue whatever you're doing and don't use\n>>>>> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n>>>>> this would be disruptive for you?\n>>>>>\n>>>>> **New Policies:**\n>>>>>\n>>>>> This includes:\n>>>>> - a set of additional policy rules applying to V3 transactions\n>>>>> - modifications to package RBF rules\n>>>>>\n>>>>> **V3 transactions:**\n>>>>>\n>>>>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>>>>> standard output types, cleanstack, etc.). The following additional\n>>>>> rules apply to V3:\n>>>>>\n>>>>> 1. A V3 transaction can be replaced, even if it does not signal BIP125\n>>>>>    replaceability. (It must also meet the other RBF rules around fees,\n>>>>> etc. for replacement to happen).\n>>>>>\n>>>>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>\n>>>>> *Rationale*: Combined with Rule 1, this gives us the property of\n>>>>> \"inherited\" replaceability signaling when descendants of unconfirmed\n>>>>> transactions are created. Additionally, checking whether a transaction\n>>>>> signals replaceability this way does not require mempool traversal,\n>>>>> and does not change based on what transactions are mined. It also\n>>>>> makes subsequent rules about descendant limits much easier to check.\n>>>>>\n>>>>> *Note*: The descendant of a *confirmed* V3 transaction does not need\n>>>>> to be V3.\n>>>>>\n>>>>> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>>\n>>>>> *Rationale*: (Upper bound) the larger the descendant limit, the more\n>>>>> transactions may need to be replaced. This is a problematic pinning\n>>>>> attack, i.e., a malicious counterparty prevents the transaction from\n>>>>> being replaced by adding many descendant transactions that aren't\n>>>>> fee-bumping.\n>>>>>\n>>>>> (Lower bound) at least 1 descendant is required to allow CPFP of the\n>>>>> presigned transaction. The contract protocol can create presigned\n>>>>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>>>>> broadcast time (\"anchor output\"). Without package RBF, multiple anchor\n>>>>> outputs would be required to allow each counterparty to fee-bump any\n>>>>> presigned transaction. With package RBF, since the presigned\n>>>>> transactions can replace each other, 1 anchor output is sufficient.\n>>>>>\n>>>>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>    larger than 1000 virtual bytes.\n>>>>>\n>>>>> *Rationale*: (Upper bound) the larger the descendant size limit, the\n>>>>> more vbytes may need to be replaced. With default limits, if the child\n>>>>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>>>>> 1sat/vbyte) or more, depending on the feerate.\n>>>>>\n>>>>> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>>>> to fund this fee-bump. For example, only allowing the V3 child to have\n>>>>> 2 inputs would require L2 protocols to manage a wallet with high-value\n>>>>> UTXOs and make batched fee-bumping impossible. However, as the\n>>>>> fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>> just a few UTXOs should suffice.\n>>>>>\n>>>>> With a limit of 1000 virtual bytes, depending on the output types, the\n>>>>> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n>>>>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>>>>> bytes as the descendant limit, the cost to replace a V3 transaction\n>>>>> has much lower variance.\n>>>>>\n>>>>> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n>>>>> logic for policy and wallets. A transaction may be up to 100KvB on its\n>>>>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>>>>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n>>>>> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n>>>>> the policy is 10KvB.\n>>>>>\n>>>>> **Package RBF modifications:**\n>>>>>\n>>>>> 1. The rule around unconfirmed inputs was\n>>>>> originally \"A package may include new unconfirmed inputs, but the\n>>>>> ancestor feerate of the child must be at least as high as the ancestor\n>>>>> feerates of every transaction being replaced.\"\n>>>>>\n>>>>> The package may still include new unconfirmed inputs. However,\n>>>>> the new rule is modified to be \"The minimum between package feerate\n>>>>> and ancestor feerate of the child is not lower than the individual\n>>>>> feerates of all directly conflicting transactions and the ancestor\n>>>>> feerates of all original transactions.\"\n>>>>>\n>>>>> *Rationale*: We are attempting to ensure that the replacement\n>>>>> transactions are not less incentive-compatible to mine. However, a\n>>>>> package/transaction's ancestor feerate is not perfectly representative\n>>>>> of its incentive compatibility; it may overestimate (some subset of\n>>>>> the ancestors could be included by itself if it has other high-feerate\n>>>>> descendants or are themselves higher feerate than this\n>>>>> package/transaction). Instead, we use the minimum between the package\n>>>>> feerate and ancestor feerate of the child as a more conservative value\n>>>>> than what was proposed originally.\n>>>>>\n>>>>> 2. A new rule is added, requiring that all package transactions with\n>>>>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>>>>> child transaction must be V3.\n>>>>>\n>>>>> *Note*: Combined with the V3 rules, this means the package must be\n>>>>> a child-with-parents package. Since package validation is only\n>>>>> attempted if the transactions do not pay sufficient fees to be\n>>>>> accepted on their own, this effectively means that only V3\n>>>>> transactions can pay to replace their ancestors' conflicts, and only\n>>>>> V3 transactions' replacements may be paid for by a descendant.\n>>>>>\n>>>>> *Rationale*: The fee-related rules are economically rational for\n>>>>> ancestor packages, but not necessarily other types of packages.\n>>>>> A child-with-parents package is a type of ancestor package. It\n>>>>> may be fine to allow any ancestor package, but it's more difficult\n>>>>> to account for all of the possibilities. For example, it gets much\n>>>>> harder to see that we're applying the descendant limits correctly if\n>>>>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>>>>> not sure if this policy is 100% incentive-compatible if the sponsor\n>>>>> is not a direct descendant of the sponsee.\n>>>>>\n>>>>> Please see doc/policy/version3_transactions.md and\n>>>>> doc/policy/packages.md in the PR for the full set of rules.\n>>>>>\n>>>>> **Intended usage for LN:**\n>>>>>\n>>>>> Commitment transactions should be V3 and have 1 anchor output. They\n>>>>> can be signed with 0 fees (or 1sat/vbyte) once package relay is\n>>>>> deployed\n>>>>> on a significant portion of the network. If the commitment tx must\n>>>>> be broadcast, determine the desired feerate at broadcast time and\n>>>>> spend the anchor output in a high feerate transaction. I'm going to\n>>>>> call the broadcasted commitment tx \"the parent\" and the attached\n>>>>> fee-bumping tx \"the child.\"\n>>>>>\n>>>>> - This child must be V3.\n>>>>> - This child must be at most 1000vB. Note this restricts the\n>>>>>   number of inputs you can use to fund the fee bump. Depending\n>>>>> on the output types, this is around 6-15.\n>>>>> - One child may fund fees for multiple commitment tx (\"batched\n>>>>>   fee-bumping\").\n>>>>> - To do a second fee-bump to add more fees, replace the\n>>>>>   *child* with a higher-feerate tx. Do not try to attach a grandchild.\n>>>>>\n>>>>> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n>>>>> descendant limits for V3 transactions are very restrictive.\n>>>>>\n>>>>> **Expected Questions:**\n>>>>>\n>>>>> \"Does this fix Rule 3 Pinning?\"\n>>>>> Yes. The V3 descendant limit restricts both you and your counterparty.\n>>>>> Assuming nodes adopted this policy, you may reasonably assume that you\n>>>>> only need to replace the commitment transaction + up to 1000vB.\n>>>>>\n>>>>> \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>> commitment tx in mempool?\"\n>>>>> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>>>> You would just package RBF it by attaching a high-feerate child to\n>>>>> your commitment tx.\n>>>>>\n>>>>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>> transactions based on nVersion?\"\n>>>>> Indeed it may be unrealistic to assume V3 transactions will be in\n>>>>> widespread use outside of L2. IIUC, unilateral closes are already\n>>>>> obvious LN transactions because of the HTLC inputs. For e.g.\n>>>>> cooperative closes and opens, I think it makes sense to continue using\n>>>>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>>>>\n>>>>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>>>>> replaceable? Is that a backward compatibility issue?\"\n>>>>> Yes it's replaceable. It's not an issue AFAICT because,\n>>>>> under previous policy, the V3 transaction wouldn't have been\n>>>>> in the mempool in the first place.\n>>>>>\n>>>>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>> Yes, otherwise someone can use V3 transactions to censor V2\n>>>>> transactions spending shared inputs. Note if the\n>>>>> original V3 transaction has an unconfirmed V3 parent, this would\n>>>>> violate the \"inherited V3\" rule and would be rejected.\n>>>>>\n>>>>> Thanks for reading! Feedback and review would be much appreciated.\n>>>>>\n>>>>> [1]:\n>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>>>> [2]:\n>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>>>>\n>>>>> Best,\n>>>>> Gloria\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220929/dba37dc7/attachment-0001.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2022-09-29T14:41:28",
                "message_text_only": "> Right, good catch, this does require new logic to handle this case.\nAs Gloria points out, this should be doable, and is definitely worth\nadding (those CSV 1 on every other output are really hacky, glad to\nfind a way to get rid of them).\n\nFor the record, it turns out ephemeral anchors + v3 solves this already, as\nthe anchor must be spent, and the parent tx may only have one child.\nSomehow I missed this implication for a few months. It's great news if we\ncan directly source fees from any output claimable, including HTLCs!\n\nOn Thu, Sep 29, 2022 at 5:15 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hi Gloria, Greg,\n>\n> > I interpret most of the discussion around limitations as ideas for\n> > future improvements rather than criticisms of the proposal\n>\n> As far as I'm concerned, definitely!\n>\n> My current understanding is that the main change/improvement that would\n> make sense here is restricting the whole v3 package's size (instead of\n> just the child) via committing to a specific value in the taproot annex\n> (also note that it's probably not just the v3 package's size, it should\n> be the whole unconfirmed package including potential v2 unconfirmed\n> ancestors).\n>\n> While I think this would be very valuable and would like to see this\n> happen, I believe that can be done in a second, separate step since this\n> would make relay policy stricter (some v3 transactions that previously\n> propagated wouldn't propagate under this new rule). As long as you are\n> able to find a path to miners through upgraded peers that use this annex\n> approach, you should be able to resolve ACP pinning issues?\n>\n> I'm curious to know how other people feel about that: is it ok to do\n> later or should we try to implement this for the first release of v3\n> transactions?\n>\n> The other change mentioned (making OP_TRUE standard and allowing outputs\n> that are below dust) can be added later, as those won't be standard until\n> we start allowing them, so there shouldn't be any backwards-compatibility\n> issue with postponing this change. But maybe it's still worth having from\n> the get-go, even though it may take a bit more time? Again, I'm curious to\n> have other people's opinion here, I'd be happy to get all of those directly\n> in the first release of v3 transactions, but I don't know how much\n> implementation will have to go into that.\n>\n> > For clarification, package RBF is ParentTx*s*(plural), and\n> ChildTx(singular),\n> > so it might be a bit more complicated than we're thinking\n>\n> Right, good catch, this does require new logic to handle this case.\n> As Gloria points out, this should be doable, and is definitely worth\n> adding (those CSV 1 on every other output are really hacky, glad to\n> find a way to get rid of them).\n>\n> Thanks,\n> Bastien\n>\n> Le lun. 26 sept. 2022 \u00e0 18:48, Gloria Zhao <gloriajzhao at gmail.com> a\n> \u00e9crit :\n>\n>> Hi Greg, Antoine, Bastien,\n>>\n>> Thanks very much for the feedback! I interpret most of the discussion\n>> around limitations as ideas for future improvements rather than criticisms\n>> of the proposal (please correct me if I'm wrong). I'll try to respond to as\n>> much as possible.\n>>\n>> Also I realize that I didn't contextualize this proposal clearly enough;\n>> it is very tailored for LN Penalty and definitely doesn't close all pinning\n>> attacks possible (sorry for confusing anyone). I also agree that some bits\n>> can be a little ugly or tack-on; I would definitely prefer a comprehensive\n>> RBF revamp to fix all our problems and enable other fee-bumping strategies\n>> such as\n>> sign-ANYONECANPAY-then-bring-your-own-fees-by-adding-inputs-at-broadcast. I\n>> was hoping to get some ideas with the \"RBF Improvements\" post in January,\n>> but it doesn't seem like we're much closer to a workable proposal. I think\n>> this is a minimally-invasive step that works for Lightning today, a small\n>> fix similar to CPFP carve out.\n>>\n>> > As you likely know from previous discussions the biggest scenario this\n>> does not fix in my estimation is ANYONECANPAY situations. If the parent\n>> transaction can be \"inflated\" by tacking on additional inputs, this means\n>> the total weight of the parent tx lowers the effective feerate of the\n>> package.\n>>\n>> (For more context to other readers I wrote an explanation for this in\n>> \"SIGHASH_ANYONECANPAY Pinning\" section of RBF ML post).  Yes, this\n>> unfortunately doesn't fix any of the existing pinning attacks for single\n>> transaction RBF but also doesn't make them worse. This boils down to adding\n>> an incentive compatibility rule that ensures you can't replace a\n>> transaction with something that will confirm slower. Package RBF has an\n>> ancestor feerate-based rule for this (note it is quite conservative and not\n>> perfect).\n>>\n>> So in the scenario above with the \"inflated\" parent that was signed ACP,\n>> the replacement would be rejected because the package ancestor feerate is\n>> lower than the feerate of what is being replaced. But it is imperfect\n>> (explained below) and thus I wouldn't recommend it for single transaction\n>> replacement. So that attack still exists for single transactions, yes.\n>>\n>> The strategy of using ACP to bring-your-own-fees has its own challenges\n>> but hopefully has no current use cases as you say. AFAIK LN Penalty is not\n>> affected by this since it doesn't use ACP, though obviously I agree we\n>> should fix it for the future.\n>>\n>> So when I said \"this is intended for fee-bumping presigned txns in\n>> contracting protocols,\" I should have said \"this is intended for\n>> fee-bumping presigned txns specifically using CPFP and anchor outputs.\"\n>> Apologies for forgetting to contextualize, I've been sitting on this for\n>> too long.\n>>\n>> > The other scenario it doesn't really fix is where HTLC/commitment-like\n>> transactions are being resolved in a batch, but due to relative time\n>> constraints, you may want to accelerate some and not others. Now you must\n>> pay higher rates to replace all of the transaction bumps. This is a\n>> \"self-pin\" and \"get good at utxos noob\" type problem, but it's something\n>> that axing rule#3 in favor of a Replace-by-ancestor-feerate system would\n>> get us.\n>>\n>> I understand you to mean \"if you don't have enough UTXOs and you're\n>> forced to batch-bump, you over-pay because you need to bring them all to\n>> the highest target feerate.\" Isn't this kind of separate, wallet-related\n>> problem? Contracting or not, surely every wallet needs to have enough UTXOs\n>> to not batch transactions that shouldn't be batched... I don't see how a\n>> replace-by-ancestor-feerate policy would make any difference for this?\n>>\n>> Also in general I'd like to reiterate that ancestor feerate is not a\n>> panacea to all our RBF incentive compatibility concerns. Like individual\n>> feerate, unless we run the mining algorithm, it cannot tell us exactly how\n>> quickly this transaction would be mined.\n>>\n>> We're estimating the incentive compatibility of the original\n>> transaction(s) and replacement transaction(s), with the goal of not letting\n>> a transaction replace something that would have been more incentive\n>> compatible to mine. As such, we don't want to overestimate how good the\n>> replacement is, and we don't want to underestimate how good the original\n>> transactions are. This rule \"The minimum between package feerate and\n>> ancestor feerate of the child is not lower than the individual feerates of\n>> all directly conflicting transactions and the ancestor feerates of all\n>> original transactions\" is a conservative estimate.\n>>\n>> > Would kind of be nice if package RBF would detect a \"sibling output\n>> spend\" conflict, and knock it out of the mempool via the other replacement\n>> rules? Getting rid of the requirement to 1 block csv lock every output\n>> would be quite nice from a smart contracting composability point of view.\n>>\n>> Interesting, so when a transaction hits a mempool tx's descendant limit,\n>> we consider evicting one of its descendants in favor of this transaction,\n>> based on the RBF rules.\n>> Cool idea! After chewing on this for a bit, I think this *also* just\n>> boils down to the fact that RBF should require replacements to be better\n>> mining candidates. As in, if we added this policy and it can make us evict\n>> the sibling and accept a transaction with a bunch of low-feerate ancestor\n>> junk, it would be a new pinning vector.\n>>\n>> > If you're a miner and you receive a non-V3, second descendant of an\n>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>> backlog, I think you would have an interest to accept such a transaction.\n>>\n>> > So I'm not sure if those two rules are compatible with miners\n>> incentives...\n>>\n>> The same argument can be made for the 26th descendant of a mempool\n>> transaction; it's also not entirely incentive-compatible to reject it, but\n>> that is not the *only* design goal in mempool policy. Of course, the\n>> difference here is that the 25-descendant limit rule is a sensible DoS\n>> protection, while this 1-descendant limit rule is more of a \"help the\n>> Bitcoin ecosystem\" policy, just like CPFP carve-out, dust limit, etc. I can\n>> of course understand why not everyone would be in favor of this, but I do\n>> think it's worth it.\n>>\n>> > > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>> > >    larger than 1000 virtual bytes.\n>>\n>> > If I understand correctly the 1000 vb upper bound rational, it would be\n>> to constraint the pinning counterparty to attach a high fee to a child due\n>> to the limited size, if they would like this transaction to be stuck in the\n>> network mempools. By doing so  this child has high odds to confirm.\n>>\n>> Yeah exactly, the \"Rule 3 pin\" is done by adding a child that's high-fee\n>> (so you have to pay that much to evict it). Because they *don't* want this\n>> tx to confirm, normally, this child would be really large. If they only\n>> have 1000vB for the child, they can't increase the replacement cost without\n>> also fee-bumping the transaction to make it confirm faster.\n>>\n>> > As of today, I think yes you can already fingerprint LN transactions on\n>> the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>> always one of them on post-anchor commitment transactions. And sadly I\n>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>> closures such as HTLC/PTLC timelocks...\n>>\n>> > I agree with you, this isn't worse than today, unilateral closes will\n>> probably always be identifiable on-chain.\n>>\n>> Great to hear that there is no privacy worsening!\n>>\n>> Best,\n>> Gloria\n>>\n>> On Mon, Sep 26, 2022 at 5:02 PM Greg Sanders <gsanders87 at gmail.com>\n>> wrote:\n>>\n>>> Bastien,\n>>>\n>>> > This may be already covered by the current package RBF logic, in that\n>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>\n>>> For clarification, package RBF is ParentTx*s*(plural), and\n>>> ChildTx(singular), so it might be a bit more complicated than we're\n>>> thinking, and currently the V3 proposal would first de-duplicate the\n>>> ParentTx based on what is in the mempool, then look at the \"rest\" of the\n>>> transactions as a package, then individually. Not the same, not sure how\n>>> different. I'll defer to experts.\n>>>\n>>> Best,\n>>> Greg\n>>>\n>>> On Mon, Sep 26, 2022 at 11:48 AM Bastien TEINTURIER via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Thanks Gloria for this great post.\n>>>>\n>>>> This is very valuable work for L2 contracts, and will greatly improve\n>>>> their security model.\n>>>>\n>>>> > \"Only 1 anchor output? What if I need to bump counterparty's\n>>>> commitment tx in mempool?\"\n>>>> > You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>>> > You would just package RBF it by attaching a high-feerate child to\n>>>> > your commitment tx.\n>>>>\n>>>> Note that we can also very easily make that single anchor spendable by\n>>>> both participants (or even anyone), so if you see your counterparty's\n>>>> commitment in your mempool, you can bump it without publishing your\n>>>> own commitment, which is quite desirable (your own commitment tx has\n>>>> CSV delays on your outputs, whereas your counterparty's commitment tx\n>>>> doesn't).\n>>>>\n>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>> transactions based on nVersion?\"\n>>>>\n>>>> I agree with you, this isn't worse than today, unilateral closes will\n>>>> probably always be identifiable on-chain.\n>>>>\n>>>> > Would kind of be nice if package RBF would detect a \"sibling output\n>>>> spend\"\n>>>> > conflict, and knock it out of the mempool via the other replacement\n>>>> rules?\n>>>> > Getting rid of the requirement to 1 block csv lock every output would\n>>>> be\n>>>> > quite nice from a smart contracting composability point of view.\n>>>>\n>>>> +1, that would be very neat!\n>>>>\n>>>> This may be already covered by the current package RBF logic, in that\n>>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>>\n>>>> > 1) I do think that we should seriously consider allowing OP_TRUE to\n>>>> become\n>>>> > a standard script type as part of this policy update. If pinning is\n>>>> solved,\n>>>> > then there's no reason to require all those extra bytes for \"binding\"\n>>>> an\n>>>> > anchor to a specific wallet/user. We can save quite a few bytes by\n>>>> having\n>>>> > the input be empty of witness data.\n>>>> > 2) If we allow for a single dust-value(0 on up) output which is\n>>>> immediately\n>>>> > spent by the package, anchors become even easier to to design. No\n>>>> value has\n>>>> > to be \"sapped\" from contract participants to make an anchor output.\n>>>> There's\n>>>> > more complications for this, such as making sure the parent\n>>>> transaction is\n>>>> > dropped if the child spend is dropped, but maybe it's worth the\n>>>> squeeze.\n>>>>\n>>>> I also think both of these could be quite useful. This would probably\n>>>> always\n>>>> be used in combination with a parent transaction that pays 0 fees, so\n>>>> the\n>>>> 0-value output would always be spent in the same block.\n>>>>\n>>>> But this means we could end up with 0-value outputs in the utxo set, if\n>>>> for\n>>>> some reason the parent tx is CPFP-ed via another output than the\n>>>> 0-value one,\n>>>> which would be a utxo set bloat issue. But I'd argue that we're probably\n>>>> already creating utxo set bloat with the 330 sat anchor outputs\n>>>> (especially\n>>>> since we use two of them, but only one is usually spent), so it would\n>>>> probably be *better* than what we're doing today.\n>>>>\n>>>> Thanks,\n>>>> Bastien\n>>>>\n>>>> Le lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>\n>>>>> Hi Gloria,\n>>>>>\n>>>>> Thanks for the progress on package RBF, few early questions.\n>>>>>\n>>>>> > 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>\n>>>>> > 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>>\n>>>>> If you're a miner and you receive a non-V3, second descendant of an\n>>>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>>>> backlog, I think you would have an interest to accept such a transaction.\n>>>>>\n>>>>> So I'm not sure if those two rules are compatible with miners\n>>>>> incentives...\n>>>>>\n>>>>> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>> >    larger than 1000 virtual bytes.\n>>>>>\n>>>>> If I understand correctly the 1000 vb upper bound rational, it would\n>>>>> be to constraint the pinning counterparty to attach a high fee to a child\n>>>>> due to the limited size, if they would like this transaction to be stuck in\n>>>>> the network mempools. By doing so  this child has high odds to confirm.\n>>>>>\n>>>>> I still wonder if this compatible with miner incentives in period of\n>>>>> empty mempools, in the sense that if you've already a V3 transaction of\n>>>>> size 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n>>>>> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n>>>>> should be conserved.\n>>>>>\n>>>>> (That said, the hard thing with any replacement strategy we might\n>>>>> evict a parent transaction *now* to which is attached a high-feerate child\n>>>>> *latter* making for a utxo considered the best ancestor set. Maybe in the\n>>>>> long-term miners should keep every transaction ever accepted...)\n>>>>>\n>>>>> > (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>>>> > to fund this fee-bump. For example, only allowing the V3 child to\n>>>>> have\n>>>>> > 2 inputs would require L2 protocols to manage a wallet with\n>>>>> high-value\n>>>>> > UTXOs and make batched fee-bumping impossible. However, as the\n>>>>> > fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>> > just a few UTXOs should suffice.\n>>>>>\n>>>>> Reminder for L2 devs, batched fee-bumping of time-sensitive\n>>>>> confirmations of commitment transactions is unsafe, as the counterparty\n>>>>> could enter in a \"cat-and-mouse\" game to replace one of the batch element\n>>>>> at each block to delay confirmation of the remaining elements in the batch,\n>>>>> I think.\n>>>>>\n>>>>> On the other hand, I wonder if we wouldn't want a higher bound. LN\n>>>>> wallets are likely to have one big UTXO in their fee-bumping reserve pool,\n>>>>> as the cost of acquiring UTXO is non-null and in the optimistic case, you\n>>>>> don't need to do unilateral closure. Let's say you close dozens of channels\n>>>>> at the same time, a UTXO pool management strategy might be to fan-out the\n>>>>> first spends UTXOs in N fan-out outputs ready to feed the remaining\n>>>>> in-flight channels.\n>>>>>\n>>>>> > 1. The rule around unconfirmed inputs was\n>>>>> > originally \"A package may include new unconfirmed inputs, but the\n>>>>> > ancestor feerate of the child must be at least as high as the\n>>>>> ancestor\n>>>>> > feerates of every transaction being replaced.\"\n>>>>>\n>>>>> Note, I think we would like this new RBF rule to also apply to single\n>>>>> transaction package, e.g second-stage HTLC transactions, where a\n>>>>> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n>>>>> honest LN node should be able to broadcast a \"at least as high ancestor\n>>>>> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n>>>>> unconfirmed ancestor to replace, as the commitment transaction, whatever\n>>>>> the party it is originating from, should already be confirmed.\n>>>>>\n>>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>> transactions based on nVersion?\"\n>>>>>\n>>>>> As of today, I think yes you can already fingerprint LN transactions\n>>>>> on the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>>>> always one of them on post-anchor commitment transactions. And sadly I\n>>>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>>>> closures such as HTLC/PTLC timelocks...\n>>>>>\n>>>>> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>\n>>>>> IIUC, a V3 package could replace a V2 package, with the benefit of the\n>>>>> new package RBF rules applied. I think this would be a significant\n>>>>> advantage for LN, as for the current ~85k of opened channels, the old V2\n>>>>> states shouldn't be pinning vectors. Currently, commitment transactions\n>>>>> signal replaceability.\n>>>>>\n>>>>> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>\n>>>>>> Hi everyone,\n>>>>>>\n>>>>>> I'm writing to propose a very simple set of mempool/transaction relay\n>>>>>> policies intended to aid L2/contract protocols. I realized that\n>>>>>> the previously proposed Package Mempool Accept package RBF [1]\n>>>>>> had a few remaining problems after digging into the RBF logic more\n>>>>>> [2].\n>>>>>> This additional set of policies solves them without requiring a huge\n>>>>>> RBF overhaul.\n>>>>>>\n>>>>>> I've written an implementation (and docs) for Bitcoin Core:\n>>>>>> https://github.com/bitcoin/bitcoin/pull/25038\n>>>>>>\n>>>>>> (You may notice that this proposal incorporates feedback on the PR -\n>>>>>> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>>>>>> and others.)\n>>>>>>\n>>>>>> If you are interested in using package RBF/relay to bump presigned\n>>>>>> transactions, I think you may be interested in reviewing this\n>>>>>> proposal.\n>>>>>> This should solve Rule 3 pinning and perhaps allow us\n>>>>>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n>>>>>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if you\n>>>>>> find a\n>>>>>> pinning attack or something that makes it unusable for you, I would\n>>>>>> really really like to know.\n>>>>>>\n>>>>>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>>>>>> currently non-standard in Bitcoin Core. That means **anything that was\n>>>>>> standard before this policy change would still be standard\n>>>>>> afterwards.** If you don't want your transactions to be subject to\n>>>>>> these rules, just continue whatever you're doing and don't use\n>>>>>> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n>>>>>> this would be disruptive for you?\n>>>>>>\n>>>>>> **New Policies:**\n>>>>>>\n>>>>>> This includes:\n>>>>>> - a set of additional policy rules applying to V3 transactions\n>>>>>> - modifications to package RBF rules\n>>>>>>\n>>>>>> **V3 transactions:**\n>>>>>>\n>>>>>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>>>>>> standard output types, cleanstack, etc.). The following additional\n>>>>>> rules apply to V3:\n>>>>>>\n>>>>>> 1. A V3 transaction can be replaced, even if it does not signal BIP125\n>>>>>>    replaceability. (It must also meet the other RBF rules around fees,\n>>>>>> etc. for replacement to happen).\n>>>>>>\n>>>>>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>>\n>>>>>> *Rationale*: Combined with Rule 1, this gives us the property of\n>>>>>> \"inherited\" replaceability signaling when descendants of unconfirmed\n>>>>>> transactions are created. Additionally, checking whether a transaction\n>>>>>> signals replaceability this way does not require mempool traversal,\n>>>>>> and does not change based on what transactions are mined. It also\n>>>>>> makes subsequent rules about descendant limits much easier to check.\n>>>>>>\n>>>>>> *Note*: The descendant of a *confirmed* V3 transaction does not need\n>>>>>> to be V3.\n>>>>>>\n>>>>>> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>>>\n>>>>>> *Rationale*: (Upper bound) the larger the descendant limit, the more\n>>>>>> transactions may need to be replaced. This is a problematic pinning\n>>>>>> attack, i.e., a malicious counterparty prevents the transaction from\n>>>>>> being replaced by adding many descendant transactions that aren't\n>>>>>> fee-bumping.\n>>>>>>\n>>>>>> (Lower bound) at least 1 descendant is required to allow CPFP of the\n>>>>>> presigned transaction. The contract protocol can create presigned\n>>>>>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>>>>>> broadcast time (\"anchor output\"). Without package RBF, multiple anchor\n>>>>>> outputs would be required to allow each counterparty to fee-bump any\n>>>>>> presigned transaction. With package RBF, since the presigned\n>>>>>> transactions can replace each other, 1 anchor output is sufficient.\n>>>>>>\n>>>>>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>>    larger than 1000 virtual bytes.\n>>>>>>\n>>>>>> *Rationale*: (Upper bound) the larger the descendant size limit, the\n>>>>>> more vbytes may need to be replaced. With default limits, if the child\n>>>>>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>>>>>> 1sat/vbyte) or more, depending on the feerate.\n>>>>>>\n>>>>>> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>>>>> to fund this fee-bump. For example, only allowing the V3 child to have\n>>>>>> 2 inputs would require L2 protocols to manage a wallet with high-value\n>>>>>> UTXOs and make batched fee-bumping impossible. However, as the\n>>>>>> fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>>> just a few UTXOs should suffice.\n>>>>>>\n>>>>>> With a limit of 1000 virtual bytes, depending on the output types, the\n>>>>>> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n>>>>>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>>>>>> bytes as the descendant limit, the cost to replace a V3 transaction\n>>>>>> has much lower variance.\n>>>>>>\n>>>>>> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n>>>>>> logic for policy and wallets. A transaction may be up to 100KvB on its\n>>>>>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>>>>>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n>>>>>> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n>>>>>> the policy is 10KvB.\n>>>>>>\n>>>>>> **Package RBF modifications:**\n>>>>>>\n>>>>>> 1. The rule around unconfirmed inputs was\n>>>>>> originally \"A package may include new unconfirmed inputs, but the\n>>>>>> ancestor feerate of the child must be at least as high as the ancestor\n>>>>>> feerates of every transaction being replaced.\"\n>>>>>>\n>>>>>> The package may still include new unconfirmed inputs. However,\n>>>>>> the new rule is modified to be \"The minimum between package feerate\n>>>>>> and ancestor feerate of the child is not lower than the individual\n>>>>>> feerates of all directly conflicting transactions and the ancestor\n>>>>>> feerates of all original transactions.\"\n>>>>>>\n>>>>>> *Rationale*: We are attempting to ensure that the replacement\n>>>>>> transactions are not less incentive-compatible to mine. However, a\n>>>>>> package/transaction's ancestor feerate is not perfectly representative\n>>>>>> of its incentive compatibility; it may overestimate (some subset of\n>>>>>> the ancestors could be included by itself if it has other high-feerate\n>>>>>> descendants or are themselves higher feerate than this\n>>>>>> package/transaction). Instead, we use the minimum between the package\n>>>>>> feerate and ancestor feerate of the child as a more conservative value\n>>>>>> than what was proposed originally.\n>>>>>>\n>>>>>> 2. A new rule is added, requiring that all package transactions with\n>>>>>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>>>>>> child transaction must be V3.\n>>>>>>\n>>>>>> *Note*: Combined with the V3 rules, this means the package must be\n>>>>>> a child-with-parents package. Since package validation is only\n>>>>>> attempted if the transactions do not pay sufficient fees to be\n>>>>>> accepted on their own, this effectively means that only V3\n>>>>>> transactions can pay to replace their ancestors' conflicts, and only\n>>>>>> V3 transactions' replacements may be paid for by a descendant.\n>>>>>>\n>>>>>> *Rationale*: The fee-related rules are economically rational for\n>>>>>> ancestor packages, but not necessarily other types of packages.\n>>>>>> A child-with-parents package is a type of ancestor package. It\n>>>>>> may be fine to allow any ancestor package, but it's more difficult\n>>>>>> to account for all of the possibilities. For example, it gets much\n>>>>>> harder to see that we're applying the descendant limits correctly if\n>>>>>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>>>>>> not sure if this policy is 100% incentive-compatible if the sponsor\n>>>>>> is not a direct descendant of the sponsee.\n>>>>>>\n>>>>>> Please see doc/policy/version3_transactions.md and\n>>>>>> doc/policy/packages.md in the PR for the full set of rules.\n>>>>>>\n>>>>>> **Intended usage for LN:**\n>>>>>>\n>>>>>> Commitment transactions should be V3 and have 1 anchor output. They\n>>>>>> can be signed with 0 fees (or 1sat/vbyte) once package relay is\n>>>>>> deployed\n>>>>>> on a significant portion of the network. If the commitment tx must\n>>>>>> be broadcast, determine the desired feerate at broadcast time and\n>>>>>> spend the anchor output in a high feerate transaction. I'm going to\n>>>>>> call the broadcasted commitment tx \"the parent\" and the attached\n>>>>>> fee-bumping tx \"the child.\"\n>>>>>>\n>>>>>> - This child must be V3.\n>>>>>> - This child must be at most 1000vB. Note this restricts the\n>>>>>>   number of inputs you can use to fund the fee bump. Depending\n>>>>>> on the output types, this is around 6-15.\n>>>>>> - One child may fund fees for multiple commitment tx (\"batched\n>>>>>>   fee-bumping\").\n>>>>>> - To do a second fee-bump to add more fees, replace the\n>>>>>>   *child* with a higher-feerate tx. Do not try to attach a grandchild.\n>>>>>>\n>>>>>> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n>>>>>> descendant limits for V3 transactions are very restrictive.\n>>>>>>\n>>>>>> **Expected Questions:**\n>>>>>>\n>>>>>> \"Does this fix Rule 3 Pinning?\"\n>>>>>> Yes. The V3 descendant limit restricts both you and your counterparty.\n>>>>>> Assuming nodes adopted this policy, you may reasonably assume that you\n>>>>>> only need to replace the commitment transaction + up to 1000vB.\n>>>>>>\n>>>>>> \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>>> commitment tx in mempool?\"\n>>>>>> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>>>>> You would just package RBF it by attaching a high-feerate child to\n>>>>>> your commitment tx.\n>>>>>>\n>>>>>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>> transactions based on nVersion?\"\n>>>>>> Indeed it may be unrealistic to assume V3 transactions will be in\n>>>>>> widespread use outside of L2. IIUC, unilateral closes are already\n>>>>>> obvious LN transactions because of the HTLC inputs. For e.g.\n>>>>>> cooperative closes and opens, I think it makes sense to continue using\n>>>>>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>>>>>\n>>>>>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>>>>>> replaceable? Is that a backward compatibility issue?\"\n>>>>>> Yes it's replaceable. It's not an issue AFAICT because,\n>>>>>> under previous policy, the V3 transaction wouldn't have been\n>>>>>> in the mempool in the first place.\n>>>>>>\n>>>>>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>> Yes, otherwise someone can use V3 transactions to censor V2\n>>>>>> transactions spending shared inputs. Note if the\n>>>>>> original V3 transaction has an unconfirmed V3 parent, this would\n>>>>>> violate the \"inherited V3\" rule and would be rejected.\n>>>>>>\n>>>>>> Thanks for reading! Feedback and review would be much appreciated.\n>>>>>>\n>>>>>> [1]:\n>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>>>>> [2]:\n>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>>>>>\n>>>>>> Best,\n>>>>>> Gloria\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220929/fa260464/attachment-0001.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2022-09-30T00:13:53",
                "message_text_only": "Hi Bastien,\n\n>The other change mentioned (making OP_TRUE standard and allowing outputs\nthat are below dust) can be added later, as those won't be standard until\nwe start allowing them, so there shouldn't be any backwards-compatibility\nissue with postponing this change. But maybe it's still worth having from\nthe get-go, even though it may take a bit more time? Again, I'm curious to\nhave other people's opinion here\n\nI'm sensitive to not wanting to overload the current discussion but this\nalso interests me, provided it can be done in a way that is acceptable\n(i.e. minimizing the potential UTXO set impact). It would solve a big cost\nissue in my spacechains design if transactions could be 0 fees and have a 0\nsat output that could be used in order to pay all the fees with CPFP.\n\nMy current view is that a tx containing a single 0 sat OP_TRUE output\nshould only get relayed if it is a package where the OP_TRUE output is\ncurrently being spent in a way that increases the overall fee rate. But\neven then, one theoretical edge case remains:\n- Another CPFP tx can feebump the package on a different (non-OP_TRUE)\noutput with an even higher fee rate\n- Subsequently, the tx that is spending the OP_TRUE might fall out of the\nmempool if the mempool fee rate rises\n- This could cause the 0 sat output to enter the UTXO set (specifically,\nrational miners wouldn't refuse to mine such a tx)\n\nIt doesn't seem like this would happen much in practice (nor is there an\nincentive to do it on purpose), but the chance isn't 0.\n\nCheers,\nRuben\n\n\n\nOn Thu, Sep 29, 2022 at 4:50 PM Greg Sanders via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > Right, good catch, this does require new logic to handle this case.\n> As Gloria points out, this should be doable, and is definitely worth\n> adding (those CSV 1 on every other output are really hacky, glad to\n> find a way to get rid of them).\n>\n> For the record, it turns out ephemeral anchors + v3 solves this already,\n> as the anchor must be spent, and the parent tx may only have one child.\n> Somehow I missed this implication for a few months. It's great news if we\n> can directly source fees from any output claimable, including HTLCs!\n>\n> On Thu, Sep 29, 2022 at 5:15 AM Bastien TEINTURIER <bastien at acinq.fr>\n> wrote:\n>\n>> Hi Gloria, Greg,\n>>\n>> > I interpret most of the discussion around limitations as ideas for\n>> > future improvements rather than criticisms of the proposal\n>>\n>> As far as I'm concerned, definitely!\n>>\n>> My current understanding is that the main change/improvement that would\n>> make sense here is restricting the whole v3 package's size (instead of\n>> just the child) via committing to a specific value in the taproot annex\n>> (also note that it's probably not just the v3 package's size, it should\n>> be the whole unconfirmed package including potential v2 unconfirmed\n>> ancestors).\n>>\n>> While I think this would be very valuable and would like to see this\n>> happen, I believe that can be done in a second, separate step since this\n>> would make relay policy stricter (some v3 transactions that previously\n>> propagated wouldn't propagate under this new rule). As long as you are\n>> able to find a path to miners through upgraded peers that use this annex\n>> approach, you should be able to resolve ACP pinning issues?\n>>\n>> I'm curious to know how other people feel about that: is it ok to do\n>> later or should we try to implement this for the first release of v3\n>> transactions?\n>>\n>> The other change mentioned (making OP_TRUE standard and allowing outputs\n>> that are below dust) can be added later, as those won't be standard until\n>> we start allowing them, so there shouldn't be any backwards-compatibility\n>> issue with postponing this change. But maybe it's still worth having from\n>> the get-go, even though it may take a bit more time? Again, I'm curious to\n>> have other people's opinion here, I'd be happy to get all of those\n>> directly\n>> in the first release of v3 transactions, but I don't know how much\n>> implementation will have to go into that.\n>>\n>> > For clarification, package RBF is ParentTx*s*(plural), and\n>> ChildTx(singular),\n>> > so it might be a bit more complicated than we're thinking\n>>\n>> Right, good catch, this does require new logic to handle this case.\n>> As Gloria points out, this should be doable, and is definitely worth\n>> adding (those CSV 1 on every other output are really hacky, glad to\n>> find a way to get rid of them).\n>>\n>> Thanks,\n>> Bastien\n>>\n>> Le lun. 26 sept. 2022 \u00e0 18:48, Gloria Zhao <gloriajzhao at gmail.com> a\n>> \u00e9crit :\n>>\n>>> Hi Greg, Antoine, Bastien,\n>>>\n>>> Thanks very much for the feedback! I interpret most of the discussion\n>>> around limitations as ideas for future improvements rather than criticisms\n>>> of the proposal (please correct me if I'm wrong). I'll try to respond to as\n>>> much as possible.\n>>>\n>>> Also I realize that I didn't contextualize this proposal clearly enough;\n>>> it is very tailored for LN Penalty and definitely doesn't close all pinning\n>>> attacks possible (sorry for confusing anyone). I also agree that some bits\n>>> can be a little ugly or tack-on; I would definitely prefer a comprehensive\n>>> RBF revamp to fix all our problems and enable other fee-bumping strategies\n>>> such as\n>>> sign-ANYONECANPAY-then-bring-your-own-fees-by-adding-inputs-at-broadcast. I\n>>> was hoping to get some ideas with the \"RBF Improvements\" post in January,\n>>> but it doesn't seem like we're much closer to a workable proposal. I think\n>>> this is a minimally-invasive step that works for Lightning today, a small\n>>> fix similar to CPFP carve out.\n>>>\n>>> > As you likely know from previous discussions the biggest scenario this\n>>> does not fix in my estimation is ANYONECANPAY situations. If the parent\n>>> transaction can be \"inflated\" by tacking on additional inputs, this means\n>>> the total weight of the parent tx lowers the effective feerate of the\n>>> package.\n>>>\n>>> (For more context to other readers I wrote an explanation for this in\n>>> \"SIGHASH_ANYONECANPAY Pinning\" section of RBF ML post).  Yes, this\n>>> unfortunately doesn't fix any of the existing pinning attacks for single\n>>> transaction RBF but also doesn't make them worse. This boils down to adding\n>>> an incentive compatibility rule that ensures you can't replace a\n>>> transaction with something that will confirm slower. Package RBF has an\n>>> ancestor feerate-based rule for this (note it is quite conservative and not\n>>> perfect).\n>>>\n>>> So in the scenario above with the \"inflated\" parent that was signed ACP,\n>>> the replacement would be rejected because the package ancestor feerate is\n>>> lower than the feerate of what is being replaced. But it is imperfect\n>>> (explained below) and thus I wouldn't recommend it for single transaction\n>>> replacement. So that attack still exists for single transactions, yes.\n>>>\n>>> The strategy of using ACP to bring-your-own-fees has its own challenges\n>>> but hopefully has no current use cases as you say. AFAIK LN Penalty is not\n>>> affected by this since it doesn't use ACP, though obviously I agree we\n>>> should fix it for the future.\n>>>\n>>> So when I said \"this is intended for fee-bumping presigned txns in\n>>> contracting protocols,\" I should have said \"this is intended for\n>>> fee-bumping presigned txns specifically using CPFP and anchor outputs.\"\n>>> Apologies for forgetting to contextualize, I've been sitting on this for\n>>> too long.\n>>>\n>>> > The other scenario it doesn't really fix is where HTLC/commitment-like\n>>> transactions are being resolved in a batch, but due to relative time\n>>> constraints, you may want to accelerate some and not others. Now you must\n>>> pay higher rates to replace all of the transaction bumps. This is a\n>>> \"self-pin\" and \"get good at utxos noob\" type problem, but it's something\n>>> that axing rule#3 in favor of a Replace-by-ancestor-feerate system would\n>>> get us.\n>>>\n>>> I understand you to mean \"if you don't have enough UTXOs and you're\n>>> forced to batch-bump, you over-pay because you need to bring them all to\n>>> the highest target feerate.\" Isn't this kind of separate, wallet-related\n>>> problem? Contracting or not, surely every wallet needs to have enough UTXOs\n>>> to not batch transactions that shouldn't be batched... I don't see how a\n>>> replace-by-ancestor-feerate policy would make any difference for this?\n>>>\n>>> Also in general I'd like to reiterate that ancestor feerate is not a\n>>> panacea to all our RBF incentive compatibility concerns. Like individual\n>>> feerate, unless we run the mining algorithm, it cannot tell us exactly how\n>>> quickly this transaction would be mined.\n>>>\n>>> We're estimating the incentive compatibility of the original\n>>> transaction(s) and replacement transaction(s), with the goal of not letting\n>>> a transaction replace something that would have been more incentive\n>>> compatible to mine. As such, we don't want to overestimate how good the\n>>> replacement is, and we don't want to underestimate how good the original\n>>> transactions are. This rule \"The minimum between package feerate and\n>>> ancestor feerate of the child is not lower than the individual feerates of\n>>> all directly conflicting transactions and the ancestor feerates of all\n>>> original transactions\" is a conservative estimate.\n>>>\n>>> > Would kind of be nice if package RBF would detect a \"sibling output\n>>> spend\" conflict, and knock it out of the mempool via the other replacement\n>>> rules? Getting rid of the requirement to 1 block csv lock every output\n>>> would be quite nice from a smart contracting composability point of view.\n>>>\n>>> Interesting, so when a transaction hits a mempool tx's descendant limit,\n>>> we consider evicting one of its descendants in favor of this transaction,\n>>> based on the RBF rules.\n>>> Cool idea! After chewing on this for a bit, I think this *also* just\n>>> boils down to the fact that RBF should require replacements to be better\n>>> mining candidates. As in, if we added this policy and it can make us evict\n>>> the sibling and accept a transaction with a bunch of low-feerate ancestor\n>>> junk, it would be a new pinning vector.\n>>>\n>>> > If you're a miner and you receive a non-V3, second descendant of an\n>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>> backlog, I think you would have an interest to accept such a transaction.\n>>>\n>>> > So I'm not sure if those two rules are compatible with miners\n>>> incentives...\n>>>\n>>> The same argument can be made for the 26th descendant of a mempool\n>>> transaction; it's also not entirely incentive-compatible to reject it, but\n>>> that is not the *only* design goal in mempool policy. Of course, the\n>>> difference here is that the 25-descendant limit rule is a sensible DoS\n>>> protection, while this 1-descendant limit rule is more of a \"help the\n>>> Bitcoin ecosystem\" policy, just like CPFP carve-out, dust limit, etc. I can\n>>> of course understand why not everyone would be in favor of this, but I do\n>>> think it's worth it.\n>>>\n>>> > > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>> > >    larger than 1000 virtual bytes.\n>>>\n>>> > If I understand correctly the 1000 vb upper bound rational, it would\n>>> be to constraint the pinning counterparty to attach a high fee to a child\n>>> due to the limited size, if they would like this transaction to be stuck in\n>>> the network mempools. By doing so  this child has high odds to confirm.\n>>>\n>>> Yeah exactly, the \"Rule 3 pin\" is done by adding a child that's high-fee\n>>> (so you have to pay that much to evict it). Because they *don't* want this\n>>> tx to confirm, normally, this child would be really large. If they only\n>>> have 1000vB for the child, they can't increase the replacement cost without\n>>> also fee-bumping the transaction to make it confirm faster.\n>>>\n>>> > As of today, I think yes you can already fingerprint LN transactions\n>>> on the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>> always one of them on post-anchor commitment transactions. And sadly I\n>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>> closures such as HTLC/PTLC timelocks...\n>>>\n>>> > I agree with you, this isn't worse than today, unilateral closes will\n>>> probably always be identifiable on-chain.\n>>>\n>>> Great to hear that there is no privacy worsening!\n>>>\n>>> Best,\n>>> Gloria\n>>>\n>>> On Mon, Sep 26, 2022 at 5:02 PM Greg Sanders <gsanders87 at gmail.com>\n>>> wrote:\n>>>\n>>>> Bastien,\n>>>>\n>>>> > This may be already covered by the current package RBF logic, in that\n>>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>>\n>>>> For clarification, package RBF is ParentTx*s*(plural), and\n>>>> ChildTx(singular), so it might be a bit more complicated than we're\n>>>> thinking, and currently the V3 proposal would first de-duplicate the\n>>>> ParentTx based on what is in the mempool, then look at the \"rest\" of the\n>>>> transactions as a package, then individually. Not the same, not sure how\n>>>> different. I'll defer to experts.\n>>>>\n>>>> Best,\n>>>> Greg\n>>>>\n>>>> On Mon, Sep 26, 2022 at 11:48 AM Bastien TEINTURIER via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>>> Thanks Gloria for this great post.\n>>>>>\n>>>>> This is very valuable work for L2 contracts, and will greatly improve\n>>>>> their security model.\n>>>>>\n>>>>> > \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>> commitment tx in mempool?\"\n>>>>> > You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>>>> > You would just package RBF it by attaching a high-feerate child to\n>>>>> > your commitment tx.\n>>>>>\n>>>>> Note that we can also very easily make that single anchor spendable by\n>>>>> both participants (or even anyone), so if you see your counterparty's\n>>>>> commitment in your mempool, you can bump it without publishing your\n>>>>> own commitment, which is quite desirable (your own commitment tx has\n>>>>> CSV delays on your outputs, whereas your counterparty's commitment tx\n>>>>> doesn't).\n>>>>>\n>>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>> transactions based on nVersion?\"\n>>>>>\n>>>>> I agree with you, this isn't worse than today, unilateral closes will\n>>>>> probably always be identifiable on-chain.\n>>>>>\n>>>>> > Would kind of be nice if package RBF would detect a \"sibling output\n>>>>> spend\"\n>>>>> > conflict, and knock it out of the mempool via the other replacement\n>>>>> rules?\n>>>>> > Getting rid of the requirement to 1 block csv lock every output\n>>>>> would be\n>>>>> > quite nice from a smart contracting composability point of view.\n>>>>>\n>>>>> +1, that would be very neat!\n>>>>>\n>>>>> This may be already covered by the current package RBF logic, in that\n>>>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>>>\n>>>>> > 1) I do think that we should seriously consider allowing OP_TRUE to\n>>>>> become\n>>>>> > a standard script type as part of this policy update. If pinning is\n>>>>> solved,\n>>>>> > then there's no reason to require all those extra bytes for\n>>>>> \"binding\" an\n>>>>> > anchor to a specific wallet/user. We can save quite a few bytes by\n>>>>> having\n>>>>> > the input be empty of witness data.\n>>>>> > 2) If we allow for a single dust-value(0 on up) output which is\n>>>>> immediately\n>>>>> > spent by the package, anchors become even easier to to design. No\n>>>>> value has\n>>>>> > to be \"sapped\" from contract participants to make an anchor output.\n>>>>> There's\n>>>>> > more complications for this, such as making sure the parent\n>>>>> transaction is\n>>>>> > dropped if the child spend is dropped, but maybe it's worth the\n>>>>> squeeze.\n>>>>>\n>>>>> I also think both of these could be quite useful. This would probably\n>>>>> always\n>>>>> be used in combination with a parent transaction that pays 0 fees, so\n>>>>> the\n>>>>> 0-value output would always be spent in the same block.\n>>>>>\n>>>>> But this means we could end up with 0-value outputs in the utxo set,\n>>>>> if for\n>>>>> some reason the parent tx is CPFP-ed via another output than the\n>>>>> 0-value one,\n>>>>> which would be a utxo set bloat issue. But I'd argue that we're\n>>>>> probably\n>>>>> already creating utxo set bloat with the 330 sat anchor outputs\n>>>>> (especially\n>>>>> since we use two of them, but only one is usually spent), so it would\n>>>>> probably be *better* than what we're doing today.\n>>>>>\n>>>>> Thanks,\n>>>>> Bastien\n>>>>>\n>>>>> Le lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\n>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>\n>>>>>> Hi Gloria,\n>>>>>>\n>>>>>> Thanks for the progress on package RBF, few early questions.\n>>>>>>\n>>>>>> > 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>>\n>>>>>> > 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>>>\n>>>>>> If you're a miner and you receive a non-V3, second descendant of an\n>>>>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>>>>> backlog, I think you would have an interest to accept such a transaction.\n>>>>>>\n>>>>>> So I'm not sure if those two rules are compatible with miners\n>>>>>> incentives...\n>>>>>>\n>>>>>> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>> >    larger than 1000 virtual bytes.\n>>>>>>\n>>>>>> If I understand correctly the 1000 vb upper bound rational, it would\n>>>>>> be to constraint the pinning counterparty to attach a high fee to a child\n>>>>>> due to the limited size, if they would like this transaction to be stuck in\n>>>>>> the network mempools. By doing so  this child has high odds to confirm.\n>>>>>>\n>>>>>> I still wonder if this compatible with miner incentives in period of\n>>>>>> empty mempools, in the sense that if you've already a V3 transaction of\n>>>>>> size 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n>>>>>> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n>>>>>> should be conserved.\n>>>>>>\n>>>>>> (That said, the hard thing with any replacement strategy we might\n>>>>>> evict a parent transaction *now* to which is attached a high-feerate child\n>>>>>> *latter* making for a utxo considered the best ancestor set. Maybe in the\n>>>>>> long-term miners should keep every transaction ever accepted...)\n>>>>>>\n>>>>>> > (Lower bound) the smaller this limit, the fewer UTXOs a child may\n>>>>>> use\n>>>>>> > to fund this fee-bump. For example, only allowing the V3 child to\n>>>>>> have\n>>>>>> > 2 inputs would require L2 protocols to manage a wallet with\n>>>>>> high-value\n>>>>>> > UTXOs and make batched fee-bumping impossible. However, as the\n>>>>>> > fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>>> > just a few UTXOs should suffice.\n>>>>>>\n>>>>>> Reminder for L2 devs, batched fee-bumping of time-sensitive\n>>>>>> confirmations of commitment transactions is unsafe, as the counterparty\n>>>>>> could enter in a \"cat-and-mouse\" game to replace one of the batch element\n>>>>>> at each block to delay confirmation of the remaining elements in the batch,\n>>>>>> I think.\n>>>>>>\n>>>>>> On the other hand, I wonder if we wouldn't want a higher bound. LN\n>>>>>> wallets are likely to have one big UTXO in their fee-bumping reserve pool,\n>>>>>> as the cost of acquiring UTXO is non-null and in the optimistic case, you\n>>>>>> don't need to do unilateral closure. Let's say you close dozens of channels\n>>>>>> at the same time, a UTXO pool management strategy might be to fan-out the\n>>>>>> first spends UTXOs in N fan-out outputs ready to feed the remaining\n>>>>>> in-flight channels.\n>>>>>>\n>>>>>> > 1. The rule around unconfirmed inputs was\n>>>>>> > originally \"A package may include new unconfirmed inputs, but the\n>>>>>> > ancestor feerate of the child must be at least as high as the\n>>>>>> ancestor\n>>>>>> > feerates of every transaction being replaced.\"\n>>>>>>\n>>>>>> Note, I think we would like this new RBF rule to also apply to single\n>>>>>> transaction package, e.g second-stage HTLC transactions, where a\n>>>>>> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n>>>>>> honest LN node should be able to broadcast a \"at least as high ancestor\n>>>>>> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n>>>>>> unconfirmed ancestor to replace, as the commitment transaction, whatever\n>>>>>> the party it is originating from, should already be confirmed.\n>>>>>>\n>>>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>> transactions based on nVersion?\"\n>>>>>>\n>>>>>> As of today, I think yes you can already fingerprint LN transactions\n>>>>>> on the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>>>>> always one of them on post-anchor commitment transactions. And sadly I\n>>>>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>>>>> closures such as HTLC/PTLC timelocks...\n>>>>>>\n>>>>>> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>>\n>>>>>> IIUC, a V3 package could replace a V2 package, with the benefit of\n>>>>>> the new package RBF rules applied. I think this would be a significant\n>>>>>> advantage for LN, as for the current ~85k of opened channels, the old V2\n>>>>>> states shouldn't be pinning vectors. Currently, commitment transactions\n>>>>>> signal replaceability.\n>>>>>>\n>>>>>> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n>>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>>\n>>>>>>> Hi everyone,\n>>>>>>>\n>>>>>>> I'm writing to propose a very simple set of mempool/transaction relay\n>>>>>>> policies intended to aid L2/contract protocols. I realized that\n>>>>>>> the previously proposed Package Mempool Accept package RBF [1]\n>>>>>>> had a few remaining problems after digging into the RBF logic more\n>>>>>>> [2].\n>>>>>>> This additional set of policies solves them without requiring a huge\n>>>>>>> RBF overhaul.\n>>>>>>>\n>>>>>>> I've written an implementation (and docs) for Bitcoin Core:\n>>>>>>> https://github.com/bitcoin/bitcoin/pull/25038\n>>>>>>>\n>>>>>>> (You may notice that this proposal incorporates feedback on the PR -\n>>>>>>> thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>>>>>>> and others.)\n>>>>>>>\n>>>>>>> If you are interested in using package RBF/relay to bump presigned\n>>>>>>> transactions, I think you may be interested in reviewing this\n>>>>>>> proposal.\n>>>>>>> This should solve Rule 3 pinning and perhaps allow us\n>>>>>>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n>>>>>>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if\n>>>>>>> you find a\n>>>>>>> pinning attack or something that makes it unusable for you, I would\n>>>>>>> really really like to know.\n>>>>>>>\n>>>>>>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>>>>>>> currently non-standard in Bitcoin Core. That means **anything that\n>>>>>>> was\n>>>>>>> standard before this policy change would still be standard\n>>>>>>> afterwards.** If you don't want your transactions to be subject to\n>>>>>>> these rules, just continue whatever you're doing and don't use\n>>>>>>> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n>>>>>>> this would be disruptive for you?\n>>>>>>>\n>>>>>>> **New Policies:**\n>>>>>>>\n>>>>>>> This includes:\n>>>>>>> - a set of additional policy rules applying to V3 transactions\n>>>>>>> - modifications to package RBF rules\n>>>>>>>\n>>>>>>> **V3 transactions:**\n>>>>>>>\n>>>>>>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>>>>>>> standard output types, cleanstack, etc.). The following additional\n>>>>>>> rules apply to V3:\n>>>>>>>\n>>>>>>> 1. A V3 transaction can be replaced, even if it does not signal\n>>>>>>> BIP125\n>>>>>>>    replaceability. (It must also meet the other RBF rules around\n>>>>>>> fees,\n>>>>>>> etc. for replacement to happen).\n>>>>>>>\n>>>>>>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>>>\n>>>>>>> *Rationale*: Combined with Rule 1, this gives us the property of\n>>>>>>> \"inherited\" replaceability signaling when descendants of unconfirmed\n>>>>>>> transactions are created. Additionally, checking whether a\n>>>>>>> transaction\n>>>>>>> signals replaceability this way does not require mempool traversal,\n>>>>>>> and does not change based on what transactions are mined. It also\n>>>>>>> makes subsequent rules about descendant limits much easier to check.\n>>>>>>>\n>>>>>>> *Note*: The descendant of a *confirmed* V3 transaction does not need\n>>>>>>> to be V3.\n>>>>>>>\n>>>>>>> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>>>>\n>>>>>>> *Rationale*: (Upper bound) the larger the descendant limit, the more\n>>>>>>> transactions may need to be replaced. This is a problematic pinning\n>>>>>>> attack, i.e., a malicious counterparty prevents the transaction from\n>>>>>>> being replaced by adding many descendant transactions that aren't\n>>>>>>> fee-bumping.\n>>>>>>>\n>>>>>>> (Lower bound) at least 1 descendant is required to allow CPFP of the\n>>>>>>> presigned transaction. The contract protocol can create presigned\n>>>>>>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>>>>>>> broadcast time (\"anchor output\"). Without package RBF, multiple\n>>>>>>> anchor\n>>>>>>> outputs would be required to allow each counterparty to fee-bump any\n>>>>>>> presigned transaction. With package RBF, since the presigned\n>>>>>>> transactions can replace each other, 1 anchor output is sufficient.\n>>>>>>>\n>>>>>>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>>>    larger than 1000 virtual bytes.\n>>>>>>>\n>>>>>>> *Rationale*: (Upper bound) the larger the descendant size limit, the\n>>>>>>> more vbytes may need to be replaced. With default limits, if the\n>>>>>>> child\n>>>>>>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>>>>>>> 1sat/vbyte) or more, depending on the feerate.\n>>>>>>>\n>>>>>>> (Lower bound) the smaller this limit, the fewer UTXOs a child may use\n>>>>>>> to fund this fee-bump. For example, only allowing the V3 child to\n>>>>>>> have\n>>>>>>> 2 inputs would require L2 protocols to manage a wallet with\n>>>>>>> high-value\n>>>>>>> UTXOs and make batched fee-bumping impossible. However, as the\n>>>>>>> fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>>>> just a few UTXOs should suffice.\n>>>>>>>\n>>>>>>> With a limit of 1000 virtual bytes, depending on the output types,\n>>>>>>> the\n>>>>>>> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n>>>>>>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>>>>>>> bytes as the descendant limit, the cost to replace a V3 transaction\n>>>>>>> has much lower variance.\n>>>>>>>\n>>>>>>> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n>>>>>>> logic for policy and wallets. A transaction may be up to 100KvB on\n>>>>>>> its\n>>>>>>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>>>>>>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n>>>>>>> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n>>>>>>> the policy is 10KvB.\n>>>>>>>\n>>>>>>> **Package RBF modifications:**\n>>>>>>>\n>>>>>>> 1. The rule around unconfirmed inputs was\n>>>>>>> originally \"A package may include new unconfirmed inputs, but the\n>>>>>>> ancestor feerate of the child must be at least as high as the\n>>>>>>> ancestor\n>>>>>>> feerates of every transaction being replaced.\"\n>>>>>>>\n>>>>>>> The package may still include new unconfirmed inputs. However,\n>>>>>>> the new rule is modified to be \"The minimum between package feerate\n>>>>>>> and ancestor feerate of the child is not lower than the individual\n>>>>>>> feerates of all directly conflicting transactions and the ancestor\n>>>>>>> feerates of all original transactions.\"\n>>>>>>>\n>>>>>>> *Rationale*: We are attempting to ensure that the replacement\n>>>>>>> transactions are not less incentive-compatible to mine. However, a\n>>>>>>> package/transaction's ancestor feerate is not perfectly\n>>>>>>> representative\n>>>>>>> of its incentive compatibility; it may overestimate (some subset of\n>>>>>>> the ancestors could be included by itself if it has other\n>>>>>>> high-feerate\n>>>>>>> descendants or are themselves higher feerate than this\n>>>>>>> package/transaction). Instead, we use the minimum between the package\n>>>>>>> feerate and ancestor feerate of the child as a more conservative\n>>>>>>> value\n>>>>>>> than what was proposed originally.\n>>>>>>>\n>>>>>>> 2. A new rule is added, requiring that all package transactions with\n>>>>>>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>>>>>>> child transaction must be V3.\n>>>>>>>\n>>>>>>> *Note*: Combined with the V3 rules, this means the package must be\n>>>>>>> a child-with-parents package. Since package validation is only\n>>>>>>> attempted if the transactions do not pay sufficient fees to be\n>>>>>>> accepted on their own, this effectively means that only V3\n>>>>>>> transactions can pay to replace their ancestors' conflicts, and only\n>>>>>>> V3 transactions' replacements may be paid for by a descendant.\n>>>>>>>\n>>>>>>> *Rationale*: The fee-related rules are economically rational for\n>>>>>>> ancestor packages, but not necessarily other types of packages.\n>>>>>>> A child-with-parents package is a type of ancestor package. It\n>>>>>>> may be fine to allow any ancestor package, but it's more difficult\n>>>>>>> to account for all of the possibilities. For example, it gets much\n>>>>>>> harder to see that we're applying the descendant limits correctly if\n>>>>>>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>>>>>>> not sure if this policy is 100% incentive-compatible if the sponsor\n>>>>>>> is not a direct descendant of the sponsee.\n>>>>>>>\n>>>>>>> Please see doc/policy/version3_transactions.md and\n>>>>>>> doc/policy/packages.md in the PR for the full set of rules.\n>>>>>>>\n>>>>>>> **Intended usage for LN:**\n>>>>>>>\n>>>>>>> Commitment transactions should be V3 and have 1 anchor output. They\n>>>>>>> can be signed with 0 fees (or 1sat/vbyte) once package relay is\n>>>>>>> deployed\n>>>>>>> on a significant portion of the network. If the commitment tx must\n>>>>>>> be broadcast, determine the desired feerate at broadcast time and\n>>>>>>> spend the anchor output in a high feerate transaction. I'm going to\n>>>>>>> call the broadcasted commitment tx \"the parent\" and the attached\n>>>>>>> fee-bumping tx \"the child.\"\n>>>>>>>\n>>>>>>> - This child must be V3.\n>>>>>>> - This child must be at most 1000vB. Note this restricts the\n>>>>>>>   number of inputs you can use to fund the fee bump. Depending\n>>>>>>> on the output types, this is around 6-15.\n>>>>>>> - One child may fund fees for multiple commitment tx (\"batched\n>>>>>>>   fee-bumping\").\n>>>>>>> - To do a second fee-bump to add more fees, replace the\n>>>>>>>   *child* with a higher-feerate tx. Do not try to attach a\n>>>>>>> grandchild.\n>>>>>>>\n>>>>>>> Otherwise, never try to spend from an unconfirmed V3 transaction. The\n>>>>>>> descendant limits for V3 transactions are very restrictive.\n>>>>>>>\n>>>>>>> **Expected Questions:**\n>>>>>>>\n>>>>>>> \"Does this fix Rule 3 Pinning?\"\n>>>>>>> Yes. The V3 descendant limit restricts both you and your\n>>>>>>> counterparty.\n>>>>>>> Assuming nodes adopted this policy, you may reasonably assume that\n>>>>>>> you\n>>>>>>> only need to replace the commitment transaction + up to 1000vB.\n>>>>>>>\n>>>>>>> \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>>>> commitment tx in mempool?\"\n>>>>>>> You won't need to fee-bump a counterparty's commitment tx using CPFP.\n>>>>>>> You would just package RBF it by attaching a high-feerate child to\n>>>>>>> your commitment tx.\n>>>>>>>\n>>>>>>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>>> transactions based on nVersion?\"\n>>>>>>> Indeed it may be unrealistic to assume V3 transactions will be in\n>>>>>>> widespread use outside of L2. IIUC, unilateral closes are already\n>>>>>>> obvious LN transactions because of the HTLC inputs. For e.g.\n>>>>>>> cooperative closes and opens, I think it makes sense to continue\n>>>>>>> using\n>>>>>>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>>>>>>\n>>>>>>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>>>>>>> replaceable? Is that a backward compatibility issue?\"\n>>>>>>> Yes it's replaceable. It's not an issue AFAICT because,\n>>>>>>> under previous policy, the V3 transaction wouldn't have been\n>>>>>>> in the mempool in the first place.\n>>>>>>>\n>>>>>>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>>> Yes, otherwise someone can use V3 transactions to censor V2\n>>>>>>> transactions spending shared inputs. Note if the\n>>>>>>> original V3 transaction has an unconfirmed V3 parent, this would\n>>>>>>> violate the \"inherited V3\" rule and would be rejected.\n>>>>>>>\n>>>>>>> Thanks for reading! Feedback and review would be much appreciated.\n>>>>>>>\n>>>>>>> [1]:\n>>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>>>>>> [2]:\n>>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>>>>>>\n>>>>>>> Best,\n>>>>>>> Gloria\n>>>>>>> _______________________________________________\n>>>>>>> bitcoin-dev mailing list\n>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220930/a5096f54/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2022-09-30T12:08:41",
                "message_text_only": "Hey Ruben,\n\nI discussed this further over IRC, and I now agree that this particular\nchange would be very desirable and can likely fit in the initial release\n(even though I'm not the one writing that code, but I'd be happy to\nreview it and test it).\n\nGreg already has a draft design that addresses your concerns: if there is\nan \"ephemeral output\" (0-value, OP_TRUE) in an unconfirmed v3 transaction,\nit MUST be spent by any child v3 transaction. This way, you ensure that\nany child transaction spending the unconfirmed parent spends the ephemeral\noutput(s). @Greg, correct me if I misunderstood something here. Note that\nwe will need to precisely define the criteria for those \"ephemeral outputs\"\n(it can probably simply be \"outputs that are 0 sats\").\n\nCoupled with transactions that pay no fees (and thus require a child to\nCPFP in order to be included in a block), this ensures those outputs can\nnever leak into the utxo set. How does that sound?\n\nI'm curious why you would need more than one such output, can you detail?\nI believe we only ever need one, spendable by anyone.\n\nCheers,\nBastien\n\nLe ven. 30 sept. 2022 \u00e0 02:14, Ruben Somsen <rsomsen at gmail.com> a \u00e9crit :\n\n> Hi Bastien,\n>\n> >The other change mentioned (making OP_TRUE standard and allowing outputs\n> that are below dust) can be added later, as those won't be standard until\n> we start allowing them, so there shouldn't be any backwards-compatibility\n> issue with postponing this change. But maybe it's still worth having from\n> the get-go, even though it may take a bit more time? Again, I'm curious to\n> have other people's opinion here\n>\n> I'm sensitive to not wanting to overload the current discussion but this\n> also interests me, provided it can be done in a way that is acceptable\n> (i.e. minimizing the potential UTXO set impact). It would solve a big cost\n> issue in my spacechains design if transactions could be 0 fees and have a 0\n> sat output that could be used in order to pay all the fees with CPFP.\n>\n> My current view is that a tx containing a single 0 sat OP_TRUE output\n> should only get relayed if it is a package where the OP_TRUE output is\n> currently being spent in a way that increases the overall fee rate. But\n> even then, one theoretical edge case remains:\n> - Another CPFP tx can feebump the package on a different (non-OP_TRUE)\n> output with an even higher fee rate\n> - Subsequently, the tx that is spending the OP_TRUE might fall out of the\n> mempool if the mempool fee rate rises\n> - This could cause the 0 sat output to enter the UTXO set (specifically,\n> rational miners wouldn't refuse to mine such a tx)\n>\n> It doesn't seem like this would happen much in practice (nor is there an\n> incentive to do it on purpose), but the chance isn't 0.\n>\n> Cheers,\n> Ruben\n>\n>\n>\n> On Thu, Sep 29, 2022 at 4:50 PM Greg Sanders via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> > Right, good catch, this does require new logic to handle this case.\n>> As Gloria points out, this should be doable, and is definitely worth\n>> adding (those CSV 1 on every other output are really hacky, glad to\n>> find a way to get rid of them).\n>>\n>> For the record, it turns out ephemeral anchors + v3 solves this already,\n>> as the anchor must be spent, and the parent tx may only have one child.\n>> Somehow I missed this implication for a few months. It's great news if we\n>> can directly source fees from any output claimable, including HTLCs!\n>>\n>> On Thu, Sep 29, 2022 at 5:15 AM Bastien TEINTURIER <bastien at acinq.fr>\n>> wrote:\n>>\n>>> Hi Gloria, Greg,\n>>>\n>>> > I interpret most of the discussion around limitations as ideas for\n>>> > future improvements rather than criticisms of the proposal\n>>>\n>>> As far as I'm concerned, definitely!\n>>>\n>>> My current understanding is that the main change/improvement that would\n>>> make sense here is restricting the whole v3 package's size (instead of\n>>> just the child) via committing to a specific value in the taproot annex\n>>> (also note that it's probably not just the v3 package's size, it should\n>>> be the whole unconfirmed package including potential v2 unconfirmed\n>>> ancestors).\n>>>\n>>> While I think this would be very valuable and would like to see this\n>>> happen, I believe that can be done in a second, separate step since this\n>>> would make relay policy stricter (some v3 transactions that previously\n>>> propagated wouldn't propagate under this new rule). As long as you are\n>>> able to find a path to miners through upgraded peers that use this annex\n>>> approach, you should be able to resolve ACP pinning issues?\n>>>\n>>> I'm curious to know how other people feel about that: is it ok to do\n>>> later or should we try to implement this for the first release of v3\n>>> transactions?\n>>>\n>>> The other change mentioned (making OP_TRUE standard and allowing outputs\n>>> that are below dust) can be added later, as those won't be standard until\n>>> we start allowing them, so there shouldn't be any backwards-compatibility\n>>> issue with postponing this change. But maybe it's still worth having from\n>>> the get-go, even though it may take a bit more time? Again, I'm curious\n>>> to\n>>> have other people's opinion here, I'd be happy to get all of those\n>>> directly\n>>> in the first release of v3 transactions, but I don't know how much\n>>> implementation will have to go into that.\n>>>\n>>> > For clarification, package RBF is ParentTx*s*(plural), and\n>>> ChildTx(singular),\n>>> > so it might be a bit more complicated than we're thinking\n>>>\n>>> Right, good catch, this does require new logic to handle this case.\n>>> As Gloria points out, this should be doable, and is definitely worth\n>>> adding (those CSV 1 on every other output are really hacky, glad to\n>>> find a way to get rid of them).\n>>>\n>>> Thanks,\n>>> Bastien\n>>>\n>>> Le lun. 26 sept. 2022 \u00e0 18:48, Gloria Zhao <gloriajzhao at gmail.com> a\n>>> \u00e9crit :\n>>>\n>>>> Hi Greg, Antoine, Bastien,\n>>>>\n>>>> Thanks very much for the feedback! I interpret most of the discussion\n>>>> around limitations as ideas for future improvements rather than criticisms\n>>>> of the proposal (please correct me if I'm wrong). I'll try to respond to as\n>>>> much as possible.\n>>>>\n>>>> Also I realize that I didn't contextualize this proposal clearly\n>>>> enough; it is very tailored for LN Penalty and definitely doesn't close all\n>>>> pinning attacks possible (sorry for confusing anyone). I also agree that\n>>>> some bits can be a little ugly or tack-on; I would definitely prefer a\n>>>> comprehensive RBF revamp to fix all our problems and enable other\n>>>> fee-bumping strategies such as\n>>>> sign-ANYONECANPAY-then-bring-your-own-fees-by-adding-inputs-at-broadcast. I\n>>>> was hoping to get some ideas with the \"RBF Improvements\" post in January,\n>>>> but it doesn't seem like we're much closer to a workable proposal. I think\n>>>> this is a minimally-invasive step that works for Lightning today, a small\n>>>> fix similar to CPFP carve out.\n>>>>\n>>>> > As you likely know from previous discussions the biggest scenario\n>>>> this does not fix in my estimation is ANYONECANPAY situations. If the\n>>>> parent transaction can be \"inflated\" by tacking on additional inputs, this\n>>>> means the total weight of the parent tx lowers the effective feerate of the\n>>>> package.\n>>>>\n>>>> (For more context to other readers I wrote an explanation for this in\n>>>> \"SIGHASH_ANYONECANPAY Pinning\" section of RBF ML post).  Yes, this\n>>>> unfortunately doesn't fix any of the existing pinning attacks for single\n>>>> transaction RBF but also doesn't make them worse. This boils down to adding\n>>>> an incentive compatibility rule that ensures you can't replace a\n>>>> transaction with something that will confirm slower. Package RBF has an\n>>>> ancestor feerate-based rule for this (note it is quite conservative and not\n>>>> perfect).\n>>>>\n>>>> So in the scenario above with the \"inflated\" parent that was signed\n>>>> ACP, the replacement would be rejected because the package ancestor feerate\n>>>> is lower than the feerate of what is being replaced. But it is imperfect\n>>>> (explained below) and thus I wouldn't recommend it for single transaction\n>>>> replacement. So that attack still exists for single transactions, yes.\n>>>>\n>>>> The strategy of using ACP to bring-your-own-fees has its own challenges\n>>>> but hopefully has no current use cases as you say. AFAIK LN Penalty is not\n>>>> affected by this since it doesn't use ACP, though obviously I agree we\n>>>> should fix it for the future.\n>>>>\n>>>> So when I said \"this is intended for fee-bumping presigned txns in\n>>>> contracting protocols,\" I should have said \"this is intended for\n>>>> fee-bumping presigned txns specifically using CPFP and anchor outputs.\"\n>>>> Apologies for forgetting to contextualize, I've been sitting on this for\n>>>> too long.\n>>>>\n>>>> > The other scenario it doesn't really fix is where\n>>>> HTLC/commitment-like transactions are being resolved in a batch, but due to\n>>>> relative time constraints, you may want to accelerate some and not others.\n>>>> Now you must pay higher rates to replace all of the transaction bumps. This\n>>>> is a \"self-pin\" and \"get good at utxos noob\" type problem, but it's\n>>>> something that axing rule#3 in favor of a Replace-by-ancestor-feerate\n>>>> system would get us.\n>>>>\n>>>> I understand you to mean \"if you don't have enough UTXOs and you're\n>>>> forced to batch-bump, you over-pay because you need to bring them all to\n>>>> the highest target feerate.\" Isn't this kind of separate, wallet-related\n>>>> problem? Contracting or not, surely every wallet needs to have enough UTXOs\n>>>> to not batch transactions that shouldn't be batched... I don't see how a\n>>>> replace-by-ancestor-feerate policy would make any difference for this?\n>>>>\n>>>> Also in general I'd like to reiterate that ancestor feerate is not a\n>>>> panacea to all our RBF incentive compatibility concerns. Like individual\n>>>> feerate, unless we run the mining algorithm, it cannot tell us exactly how\n>>>> quickly this transaction would be mined.\n>>>>\n>>>> We're estimating the incentive compatibility of the original\n>>>> transaction(s) and replacement transaction(s), with the goal of not letting\n>>>> a transaction replace something that would have been more incentive\n>>>> compatible to mine. As such, we don't want to overestimate how good the\n>>>> replacement is, and we don't want to underestimate how good the original\n>>>> transactions are. This rule \"The minimum between package feerate and\n>>>> ancestor feerate of the child is not lower than the individual feerates of\n>>>> all directly conflicting transactions and the ancestor feerates of all\n>>>> original transactions\" is a conservative estimate.\n>>>>\n>>>> > Would kind of be nice if package RBF would detect a \"sibling output\n>>>> spend\" conflict, and knock it out of the mempool via the other replacement\n>>>> rules? Getting rid of the requirement to 1 block csv lock every output\n>>>> would be quite nice from a smart contracting composability point of view.\n>>>>\n>>>> Interesting, so when a transaction hits a mempool tx's descendant\n>>>> limit, we consider evicting one of its descendants in favor of this\n>>>> transaction, based on the RBF rules.\n>>>> Cool idea! After chewing on this for a bit, I think this *also* just\n>>>> boils down to the fact that RBF should require replacements to be better\n>>>> mining candidates. As in, if we added this policy and it can make us evict\n>>>> the sibling and accept a transaction with a bunch of low-feerate ancestor\n>>>> junk, it would be a new pinning vector.\n>>>>\n>>>> > If you're a miner and you receive a non-V3, second descendant of an\n>>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>>> backlog, I think you would have an interest to accept such a transaction.\n>>>>\n>>>> > So I'm not sure if those two rules are compatible with miners\n>>>> incentives...\n>>>>\n>>>> The same argument can be made for the 26th descendant of a mempool\n>>>> transaction; it's also not entirely incentive-compatible to reject it, but\n>>>> that is not the *only* design goal in mempool policy. Of course, the\n>>>> difference here is that the 25-descendant limit rule is a sensible DoS\n>>>> protection, while this 1-descendant limit rule is more of a \"help the\n>>>> Bitcoin ecosystem\" policy, just like CPFP carve-out, dust limit, etc. I can\n>>>> of course understand why not everyone would be in favor of this, but I do\n>>>> think it's worth it.\n>>>>\n>>>> > > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>\n>>>> > >    larger than 1000 virtual bytes.\n>>>>\n>>>> > If I understand correctly the 1000 vb upper bound rational, it would\n>>>> be to constraint the pinning counterparty to attach a high fee to a child\n>>>> due to the limited size, if they would like this transaction to be stuck in\n>>>> the network mempools. By doing so  this child has high odds to confirm.\n>>>>\n>>>> Yeah exactly, the \"Rule 3 pin\" is done by adding a child that's\n>>>> high-fee (so you have to pay that much to evict it). Because they *don't*\n>>>> want this tx to confirm, normally, this child would be really large. If\n>>>> they only have 1000vB for the child, they can't increase the replacement\n>>>> cost without also fee-bumping the transaction to make it confirm faster.\n>>>>\n>>>> > As of today, I think yes you can already fingerprint LN transactions\n>>>> on the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>>> always one of them on post-anchor commitment transactions. And sadly I\n>>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>>> closures such as HTLC/PTLC timelocks...\n>>>>\n>>>> > I agree with you, this isn't worse than today, unilateral closes will\n>>>> probably always be identifiable on-chain.\n>>>>\n>>>> Great to hear that there is no privacy worsening!\n>>>>\n>>>> Best,\n>>>> Gloria\n>>>>\n>>>> On Mon, Sep 26, 2022 at 5:02 PM Greg Sanders <gsanders87 at gmail.com>\n>>>> wrote:\n>>>>\n>>>>> Bastien,\n>>>>>\n>>>>> > This may be already covered by the current package RBF logic, in that\n>>>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>>>\n>>>>> For clarification, package RBF is ParentTx*s*(plural), and\n>>>>> ChildTx(singular), so it might be a bit more complicated than we're\n>>>>> thinking, and currently the V3 proposal would first de-duplicate the\n>>>>> ParentTx based on what is in the mempool, then look at the \"rest\" of the\n>>>>> transactions as a package, then individually. Not the same, not sure how\n>>>>> different. I'll defer to experts.\n>>>>>\n>>>>> Best,\n>>>>> Greg\n>>>>>\n>>>>> On Mon, Sep 26, 2022 at 11:48 AM Bastien TEINTURIER via bitcoin-dev <\n>>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>>\n>>>>>> Thanks Gloria for this great post.\n>>>>>>\n>>>>>> This is very valuable work for L2 contracts, and will greatly improve\n>>>>>> their security model.\n>>>>>>\n>>>>>> > \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>>> commitment tx in mempool?\"\n>>>>>> > You won't need to fee-bump a counterparty's commitment tx using\n>>>>>> CPFP.\n>>>>>> > You would just package RBF it by attaching a high-feerate child to\n>>>>>> > your commitment tx.\n>>>>>>\n>>>>>> Note that we can also very easily make that single anchor spendable by\n>>>>>> both participants (or even anyone), so if you see your counterparty's\n>>>>>> commitment in your mempool, you can bump it without publishing your\n>>>>>> own commitment, which is quite desirable (your own commitment tx has\n>>>>>> CSV delays on your outputs, whereas your counterparty's commitment tx\n>>>>>> doesn't).\n>>>>>>\n>>>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>> transactions based on nVersion?\"\n>>>>>>\n>>>>>> I agree with you, this isn't worse than today, unilateral closes will\n>>>>>> probably always be identifiable on-chain.\n>>>>>>\n>>>>>> > Would kind of be nice if package RBF would detect a \"sibling output\n>>>>>> spend\"\n>>>>>> > conflict, and knock it out of the mempool via the other replacement\n>>>>>> rules?\n>>>>>> > Getting rid of the requirement to 1 block csv lock every output\n>>>>>> would be\n>>>>>> > quite nice from a smart contracting composability point of view.\n>>>>>>\n>>>>>> +1, that would be very neat!\n>>>>>>\n>>>>>> This may be already covered by the current package RBF logic, in that\n>>>>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>>>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>>>>\n>>>>>> > 1) I do think that we should seriously consider allowing OP_TRUE to\n>>>>>> become\n>>>>>> > a standard script type as part of this policy update. If pinning is\n>>>>>> solved,\n>>>>>> > then there's no reason to require all those extra bytes for\n>>>>>> \"binding\" an\n>>>>>> > anchor to a specific wallet/user. We can save quite a few bytes by\n>>>>>> having\n>>>>>> > the input be empty of witness data.\n>>>>>> > 2) If we allow for a single dust-value(0 on up) output which is\n>>>>>> immediately\n>>>>>> > spent by the package, anchors become even easier to to design. No\n>>>>>> value has\n>>>>>> > to be \"sapped\" from contract participants to make an anchor output.\n>>>>>> There's\n>>>>>> > more complications for this, such as making sure the parent\n>>>>>> transaction is\n>>>>>> > dropped if the child spend is dropped, but maybe it's worth the\n>>>>>> squeeze.\n>>>>>>\n>>>>>> I also think both of these could be quite useful. This would probably\n>>>>>> always\n>>>>>> be used in combination with a parent transaction that pays 0 fees, so\n>>>>>> the\n>>>>>> 0-value output would always be spent in the same block.\n>>>>>>\n>>>>>> But this means we could end up with 0-value outputs in the utxo set,\n>>>>>> if for\n>>>>>> some reason the parent tx is CPFP-ed via another output than the\n>>>>>> 0-value one,\n>>>>>> which would be a utxo set bloat issue. But I'd argue that we're\n>>>>>> probably\n>>>>>> already creating utxo set bloat with the 330 sat anchor outputs\n>>>>>> (especially\n>>>>>> since we use two of them, but only one is usually spent), so it would\n>>>>>> probably be *better* than what we're doing today.\n>>>>>>\n>>>>>> Thanks,\n>>>>>> Bastien\n>>>>>>\n>>>>>> Le lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\n>>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>>\n>>>>>>> Hi Gloria,\n>>>>>>>\n>>>>>>> Thanks for the progress on package RBF, few early questions.\n>>>>>>>\n>>>>>>> > 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>>>\n>>>>>>> > 3. An unconfirmed V3 transaction cannot have more than 1\n>>>>>>> descendant.\n>>>>>>>\n>>>>>>> If you're a miner and you receive a non-V3, second descendant of an\n>>>>>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>>>>>> backlog, I think you would have an interest to accept such a transaction.\n>>>>>>>\n>>>>>>> So I'm not sure if those two rules are compatible with miners\n>>>>>>> incentives...\n>>>>>>>\n>>>>>>> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>>> >    larger than 1000 virtual bytes.\n>>>>>>>\n>>>>>>> If I understand correctly the 1000 vb upper bound rational, it would\n>>>>>>> be to constraint the pinning counterparty to attach a high fee to a child\n>>>>>>> due to the limited size, if they would like this transaction to be stuck in\n>>>>>>> the network mempools. By doing so  this child has high odds to confirm.\n>>>>>>>\n>>>>>>> I still wonder if this compatible with miner incentives in period of\n>>>>>>> empty mempools, in the sense that if you've already a V3 transaction of\n>>>>>>> size 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n>>>>>>> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n>>>>>>> should be conserved.\n>>>>>>>\n>>>>>>> (That said, the hard thing with any replacement strategy we might\n>>>>>>> evict a parent transaction *now* to which is attached a high-feerate child\n>>>>>>> *latter* making for a utxo considered the best ancestor set. Maybe in the\n>>>>>>> long-term miners should keep every transaction ever accepted...)\n>>>>>>>\n>>>>>>> > (Lower bound) the smaller this limit, the fewer UTXOs a child may\n>>>>>>> use\n>>>>>>> > to fund this fee-bump. For example, only allowing the V3 child to\n>>>>>>> have\n>>>>>>> > 2 inputs would require L2 protocols to manage a wallet with\n>>>>>>> high-value\n>>>>>>> > UTXOs and make batched fee-bumping impossible. However, as the\n>>>>>>> > fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>>>> > just a few UTXOs should suffice.\n>>>>>>>\n>>>>>>> Reminder for L2 devs, batched fee-bumping of time-sensitive\n>>>>>>> confirmations of commitment transactions is unsafe, as the counterparty\n>>>>>>> could enter in a \"cat-and-mouse\" game to replace one of the batch element\n>>>>>>> at each block to delay confirmation of the remaining elements in the batch,\n>>>>>>> I think.\n>>>>>>>\n>>>>>>> On the other hand, I wonder if we wouldn't want a higher bound. LN\n>>>>>>> wallets are likely to have one big UTXO in their fee-bumping reserve pool,\n>>>>>>> as the cost of acquiring UTXO is non-null and in the optimistic case, you\n>>>>>>> don't need to do unilateral closure. Let's say you close dozens of channels\n>>>>>>> at the same time, a UTXO pool management strategy might be to fan-out the\n>>>>>>> first spends UTXOs in N fan-out outputs ready to feed the remaining\n>>>>>>> in-flight channels.\n>>>>>>>\n>>>>>>> > 1. The rule around unconfirmed inputs was\n>>>>>>> > originally \"A package may include new unconfirmed inputs, but the\n>>>>>>> > ancestor feerate of the child must be at least as high as the\n>>>>>>> ancestor\n>>>>>>> > feerates of every transaction being replaced.\"\n>>>>>>>\n>>>>>>> Note, I think we would like this new RBF rule to also apply to\n>>>>>>> single transaction package, e.g second-stage HTLC transactions, where a\n>>>>>>> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n>>>>>>> honest LN node should be able to broadcast a \"at least as high ancestor\n>>>>>>> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n>>>>>>> unconfirmed ancestor to replace, as the commitment transaction, whatever\n>>>>>>> the party it is originating from, should already be confirmed.\n>>>>>>>\n>>>>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>>> transactions based on nVersion?\"\n>>>>>>>\n>>>>>>> As of today, I think yes you can already fingerprint LN transactions\n>>>>>>> on the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>>>>>> always one of them on post-anchor commitment transactions. And sadly I\n>>>>>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>>>>>> closures such as HTLC/PTLC timelocks...\n>>>>>>>\n>>>>>>> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>>>\n>>>>>>> IIUC, a V3 package could replace a V2 package, with the benefit of\n>>>>>>> the new package RBF rules applied. I think this would be a significant\n>>>>>>> advantage for LN, as for the current ~85k of opened channels, the old V2\n>>>>>>> states shouldn't be pinning vectors. Currently, commitment transactions\n>>>>>>> signal replaceability.\n>>>>>>>\n>>>>>>> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n>>>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>>>\n>>>>>>>> Hi everyone,\n>>>>>>>>\n>>>>>>>> I'm writing to propose a very simple set of mempool/transaction\n>>>>>>>> relay\n>>>>>>>> policies intended to aid L2/contract protocols. I realized that\n>>>>>>>> the previously proposed Package Mempool Accept package RBF [1]\n>>>>>>>> had a few remaining problems after digging into the RBF logic more\n>>>>>>>> [2].\n>>>>>>>> This additional set of policies solves them without requiring a\n>>>>>>>> huge RBF overhaul.\n>>>>>>>>\n>>>>>>>> I've written an implementation (and docs) for Bitcoin Core:\n>>>>>>>> https://github.com/bitcoin/bitcoin/pull/25038\n>>>>>>>>\n>>>>>>>> (You may notice that this proposal incorporates feedback on the PR\n>>>>>>>> - thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>>>>>>>> and others.)\n>>>>>>>>\n>>>>>>>> If you are interested in using package RBF/relay to bump presigned\n>>>>>>>> transactions, I think you may be interested in reviewing this\n>>>>>>>> proposal.\n>>>>>>>> This should solve Rule 3 pinning and perhaps allow us\n>>>>>>>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people find\n>>>>>>>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if\n>>>>>>>> you find a\n>>>>>>>> pinning attack or something that makes it unusable for you, I would\n>>>>>>>> really really like to know.\n>>>>>>>>\n>>>>>>>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>>>>>>>> currently non-standard in Bitcoin Core. That means **anything that\n>>>>>>>> was\n>>>>>>>> standard before this policy change would still be standard\n>>>>>>>> afterwards.** If you don't want your transactions to be subject to\n>>>>>>>> these rules, just continue whatever you're doing and don't use\n>>>>>>>> nVersion=3. AFAICT this shouldn't break anything, but let me know if\n>>>>>>>> this would be disruptive for you?\n>>>>>>>>\n>>>>>>>> **New Policies:**\n>>>>>>>>\n>>>>>>>> This includes:\n>>>>>>>> - a set of additional policy rules applying to V3 transactions\n>>>>>>>> - modifications to package RBF rules\n>>>>>>>>\n>>>>>>>> **V3 transactions:**\n>>>>>>>>\n>>>>>>>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>>>>>>>> standard output types, cleanstack, etc.). The following additional\n>>>>>>>> rules apply to V3:\n>>>>>>>>\n>>>>>>>> 1. A V3 transaction can be replaced, even if it does not signal\n>>>>>>>> BIP125\n>>>>>>>>    replaceability. (It must also meet the other RBF rules around\n>>>>>>>> fees,\n>>>>>>>> etc. for replacement to happen).\n>>>>>>>>\n>>>>>>>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>>>>\n>>>>>>>> *Rationale*: Combined with Rule 1, this gives us the property of\n>>>>>>>> \"inherited\" replaceability signaling when descendants of unconfirmed\n>>>>>>>> transactions are created. Additionally, checking whether a\n>>>>>>>> transaction\n>>>>>>>> signals replaceability this way does not require mempool traversal,\n>>>>>>>> and does not change based on what transactions are mined. It also\n>>>>>>>> makes subsequent rules about descendant limits much easier to check.\n>>>>>>>>\n>>>>>>>> *Note*: The descendant of a *confirmed* V3 transaction does not\n>>>>>>>> need to be V3.\n>>>>>>>>\n>>>>>>>> 3. An unconfirmed V3 transaction cannot have more than 1 descendant.\n>>>>>>>>\n>>>>>>>> *Rationale*: (Upper bound) the larger the descendant limit, the more\n>>>>>>>> transactions may need to be replaced. This is a problematic pinning\n>>>>>>>> attack, i.e., a malicious counterparty prevents the transaction from\n>>>>>>>> being replaced by adding many descendant transactions that aren't\n>>>>>>>> fee-bumping.\n>>>>>>>>\n>>>>>>>> (Lower bound) at least 1 descendant is required to allow CPFP of the\n>>>>>>>> presigned transaction. The contract protocol can create presigned\n>>>>>>>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>>>>>>>> broadcast time (\"anchor output\"). Without package RBF, multiple\n>>>>>>>> anchor\n>>>>>>>> outputs would be required to allow each counterparty to fee-bump any\n>>>>>>>> presigned transaction. With package RBF, since the presigned\n>>>>>>>> transactions can replace each other, 1 anchor output is sufficient.\n>>>>>>>>\n>>>>>>>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>>>>    larger than 1000 virtual bytes.\n>>>>>>>>\n>>>>>>>> *Rationale*: (Upper bound) the larger the descendant size limit, the\n>>>>>>>> more vbytes may need to be replaced. With default limits, if the\n>>>>>>>> child\n>>>>>>>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>>>>>>>> 1sat/vbyte) or more, depending on the feerate.\n>>>>>>>>\n>>>>>>>> (Lower bound) the smaller this limit, the fewer UTXOs a child may\n>>>>>>>> use\n>>>>>>>> to fund this fee-bump. For example, only allowing the V3 child to\n>>>>>>>> have\n>>>>>>>> 2 inputs would require L2 protocols to manage a wallet with\n>>>>>>>> high-value\n>>>>>>>> UTXOs and make batched fee-bumping impossible. However, as the\n>>>>>>>> fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>>>>> just a few UTXOs should suffice.\n>>>>>>>>\n>>>>>>>> With a limit of 1000 virtual bytes, depending on the output types,\n>>>>>>>> the\n>>>>>>>> child can have 6-15 UTXOs, which should be enough to fund a fee-bump\n>>>>>>>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>>>>>>>> bytes as the descendant limit, the cost to replace a V3 transaction\n>>>>>>>> has much lower variance.\n>>>>>>>>\n>>>>>>>> *Rationale*: This makes the rule very easily \"tacked on\" to existing\n>>>>>>>> logic for policy and wallets. A transaction may be up to 100KvB on\n>>>>>>>> its\n>>>>>>>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>>>>>>>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3 transaction\n>>>>>>>> in the mempool is 100KvB, its descendant can only be 1000vB, even if\n>>>>>>>> the policy is 10KvB.\n>>>>>>>>\n>>>>>>>> **Package RBF modifications:**\n>>>>>>>>\n>>>>>>>> 1. The rule around unconfirmed inputs was\n>>>>>>>> originally \"A package may include new unconfirmed inputs, but the\n>>>>>>>> ancestor feerate of the child must be at least as high as the\n>>>>>>>> ancestor\n>>>>>>>> feerates of every transaction being replaced.\"\n>>>>>>>>\n>>>>>>>> The package may still include new unconfirmed inputs. However,\n>>>>>>>> the new rule is modified to be \"The minimum between package feerate\n>>>>>>>> and ancestor feerate of the child is not lower than the individual\n>>>>>>>> feerates of all directly conflicting transactions and the ancestor\n>>>>>>>> feerates of all original transactions.\"\n>>>>>>>>\n>>>>>>>> *Rationale*: We are attempting to ensure that the replacement\n>>>>>>>> transactions are not less incentive-compatible to mine. However, a\n>>>>>>>> package/transaction's ancestor feerate is not perfectly\n>>>>>>>> representative\n>>>>>>>> of its incentive compatibility; it may overestimate (some subset of\n>>>>>>>> the ancestors could be included by itself if it has other\n>>>>>>>> high-feerate\n>>>>>>>> descendants or are themselves higher feerate than this\n>>>>>>>> package/transaction). Instead, we use the minimum between the\n>>>>>>>> package\n>>>>>>>> feerate and ancestor feerate of the child as a more conservative\n>>>>>>>> value\n>>>>>>>> than what was proposed originally.\n>>>>>>>>\n>>>>>>>> 2. A new rule is added, requiring that all package transactions with\n>>>>>>>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>>>>>>>> child transaction must be V3.\n>>>>>>>>\n>>>>>>>> *Note*: Combined with the V3 rules, this means the package must be\n>>>>>>>> a child-with-parents package. Since package validation is only\n>>>>>>>> attempted if the transactions do not pay sufficient fees to be\n>>>>>>>> accepted on their own, this effectively means that only V3\n>>>>>>>> transactions can pay to replace their ancestors' conflicts, and only\n>>>>>>>> V3 transactions' replacements may be paid for by a descendant.\n>>>>>>>>\n>>>>>>>> *Rationale*: The fee-related rules are economically rational for\n>>>>>>>> ancestor packages, but not necessarily other types of packages.\n>>>>>>>> A child-with-parents package is a type of ancestor package. It\n>>>>>>>> may be fine to allow any ancestor package, but it's more difficult\n>>>>>>>> to account for all of the possibilities. For example, it gets much\n>>>>>>>> harder to see that we're applying the descendant limits correctly if\n>>>>>>>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>>>>>>>> not sure if this policy is 100% incentive-compatible if the sponsor\n>>>>>>>> is not a direct descendant of the sponsee.\n>>>>>>>>\n>>>>>>>> Please see doc/policy/version3_transactions.md and\n>>>>>>>> doc/policy/packages.md in the PR for the full set of rules.\n>>>>>>>>\n>>>>>>>> **Intended usage for LN:**\n>>>>>>>>\n>>>>>>>> Commitment transactions should be V3 and have 1 anchor output. They\n>>>>>>>> can be signed with 0 fees (or 1sat/vbyte) once package relay is\n>>>>>>>> deployed\n>>>>>>>> on a significant portion of the network. If the commitment tx must\n>>>>>>>> be broadcast, determine the desired feerate at broadcast time and\n>>>>>>>> spend the anchor output in a high feerate transaction. I'm going to\n>>>>>>>> call the broadcasted commitment tx \"the parent\" and the attached\n>>>>>>>> fee-bumping tx \"the child.\"\n>>>>>>>>\n>>>>>>>> - This child must be V3.\n>>>>>>>> - This child must be at most 1000vB. Note this restricts the\n>>>>>>>>   number of inputs you can use to fund the fee bump. Depending\n>>>>>>>> on the output types, this is around 6-15.\n>>>>>>>> - One child may fund fees for multiple commitment tx (\"batched\n>>>>>>>>   fee-bumping\").\n>>>>>>>> - To do a second fee-bump to add more fees, replace the\n>>>>>>>>   *child* with a higher-feerate tx. Do not try to attach a\n>>>>>>>> grandchild.\n>>>>>>>>\n>>>>>>>> Otherwise, never try to spend from an unconfirmed V3 transaction.\n>>>>>>>> The\n>>>>>>>> descendant limits for V3 transactions are very restrictive.\n>>>>>>>>\n>>>>>>>> **Expected Questions:**\n>>>>>>>>\n>>>>>>>> \"Does this fix Rule 3 Pinning?\"\n>>>>>>>> Yes. The V3 descendant limit restricts both you and your\n>>>>>>>> counterparty.\n>>>>>>>> Assuming nodes adopted this policy, you may reasonably assume that\n>>>>>>>> you\n>>>>>>>> only need to replace the commitment transaction + up to 1000vB.\n>>>>>>>>\n>>>>>>>> \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>>>>> commitment tx in mempool?\"\n>>>>>>>> You won't need to fee-bump a counterparty's commitment tx using\n>>>>>>>> CPFP.\n>>>>>>>> You would just package RBF it by attaching a high-feerate child to\n>>>>>>>> your commitment tx.\n>>>>>>>>\n>>>>>>>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>>>> transactions based on nVersion?\"\n>>>>>>>> Indeed it may be unrealistic to assume V3 transactions will be in\n>>>>>>>> widespread use outside of L2. IIUC, unilateral closes are already\n>>>>>>>> obvious LN transactions because of the HTLC inputs. For e.g.\n>>>>>>>> cooperative closes and opens, I think it makes sense to continue\n>>>>>>>> using\n>>>>>>>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>>>>>>>\n>>>>>>>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>>>>>>>> replaceable? Is that a backward compatibility issue?\"\n>>>>>>>> Yes it's replaceable. It's not an issue AFAICT because,\n>>>>>>>> under previous policy, the V3 transaction wouldn't have been\n>>>>>>>> in the mempool in the first place.\n>>>>>>>>\n>>>>>>>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>>>> Yes, otherwise someone can use V3 transactions to censor V2\n>>>>>>>> transactions spending shared inputs. Note if the\n>>>>>>>> original V3 transaction has an unconfirmed V3 parent, this would\n>>>>>>>> violate the \"inherited V3\" rule and would be rejected.\n>>>>>>>>\n>>>>>>>> Thanks for reading! Feedback and review would be much appreciated.\n>>>>>>>>\n>>>>>>>> [1]:\n>>>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>>>>>>> [2]:\n>>>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>>>>>>>\n>>>>>>>> Best,\n>>>>>>>> Gloria\n>>>>>>>> _______________________________________________\n>>>>>>>> bitcoin-dev mailing list\n>>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>>\n>>>>>>> _______________________________________________\n>>>>>>> bitcoin-dev mailing list\n>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>\n>>>>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220930/2a6057da/attachment-0001.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2022-09-30T12:17:38",
                "message_text_only": "It's likely better if the ephemeral output can be any value, including\ndust. This lets contract designers put \"trimmed output\" value indirectly\ntowards CPFP fees without making the parent tx have fees itself.\n\nOn Fri, Sep 30, 2022, 8:08 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hey Ruben,\n>\n> I discussed this further over IRC, and I now agree that this particular\n> change would be very desirable and can likely fit in the initial release\n> (even though I'm not the one writing that code, but I'd be happy to\n> review it and test it).\n>\n> Greg already has a draft design that addresses your concerns: if there is\n> an \"ephemeral output\" (0-value, OP_TRUE) in an unconfirmed v3 transaction,\n> it MUST be spent by any child v3 transaction. This way, you ensure that\n> any child transaction spending the unconfirmed parent spends the ephemeral\n> output(s). @Greg, correct me if I misunderstood something here. Note that\n> we will need to precisely define the criteria for those \"ephemeral outputs\"\n> (it can probably simply be \"outputs that are 0 sats\").\n>\n> Coupled with transactions that pay no fees (and thus require a child to\n> CPFP in order to be included in a block), this ensures those outputs can\n> never leak into the utxo set. How does that sound?\n>\n> I'm curious why you would need more than one such output, can you detail?\n> I believe we only ever need one, spendable by anyone.\n>\n> Cheers,\n> Bastien\n>\n> Le ven. 30 sept. 2022 \u00e0 02:14, Ruben Somsen <rsomsen at gmail.com> a \u00e9crit :\n>\n>> Hi Bastien,\n>>\n>> >The other change mentioned (making OP_TRUE standard and allowing outputs\n>> that are below dust) can be added later, as those won't be standard until\n>> we start allowing them, so there shouldn't be any backwards-compatibility\n>> issue with postponing this change. But maybe it's still worth having from\n>> the get-go, even though it may take a bit more time? Again, I'm curious to\n>> have other people's opinion here\n>>\n>> I'm sensitive to not wanting to overload the current discussion but this\n>> also interests me, provided it can be done in a way that is acceptable\n>> (i.e. minimizing the potential UTXO set impact). It would solve a big cost\n>> issue in my spacechains design if transactions could be 0 fees and have a 0\n>> sat output that could be used in order to pay all the fees with CPFP.\n>>\n>> My current view is that a tx containing a single 0 sat OP_TRUE output\n>> should only get relayed if it is a package where the OP_TRUE output is\n>> currently being spent in a way that increases the overall fee rate. But\n>> even then, one theoretical edge case remains:\n>> - Another CPFP tx can feebump the package on a different (non-OP_TRUE)\n>> output with an even higher fee rate\n>> - Subsequently, the tx that is spending the OP_TRUE might fall out of the\n>> mempool if the mempool fee rate rises\n>> - This could cause the 0 sat output to enter the UTXO set (specifically,\n>> rational miners wouldn't refuse to mine such a tx)\n>>\n>> It doesn't seem like this would happen much in practice (nor is there an\n>> incentive to do it on purpose), but the chance isn't 0.\n>>\n>> Cheers,\n>> Ruben\n>>\n>>\n>>\n>> On Thu, Sep 29, 2022 at 4:50 PM Greg Sanders via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> > Right, good catch, this does require new logic to handle this case.\n>>> As Gloria points out, this should be doable, and is definitely worth\n>>> adding (those CSV 1 on every other output are really hacky, glad to\n>>> find a way to get rid of them).\n>>>\n>>> For the record, it turns out ephemeral anchors + v3 solves this already,\n>>> as the anchor must be spent, and the parent tx may only have one child.\n>>> Somehow I missed this implication for a few months. It's great news if we\n>>> can directly source fees from any output claimable, including HTLCs!\n>>>\n>>> On Thu, Sep 29, 2022 at 5:15 AM Bastien TEINTURIER <bastien at acinq.fr>\n>>> wrote:\n>>>\n>>>> Hi Gloria, Greg,\n>>>>\n>>>> > I interpret most of the discussion around limitations as ideas for\n>>>> > future improvements rather than criticisms of the proposal\n>>>>\n>>>> As far as I'm concerned, definitely!\n>>>>\n>>>> My current understanding is that the main change/improvement that would\n>>>> make sense here is restricting the whole v3 package's size (instead of\n>>>> just the child) via committing to a specific value in the taproot annex\n>>>> (also note that it's probably not just the v3 package's size, it should\n>>>> be the whole unconfirmed package including potential v2 unconfirmed\n>>>> ancestors).\n>>>>\n>>>> While I think this would be very valuable and would like to see this\n>>>> happen, I believe that can be done in a second, separate step since this\n>>>> would make relay policy stricter (some v3 transactions that previously\n>>>> propagated wouldn't propagate under this new rule). As long as you are\n>>>> able to find a path to miners through upgraded peers that use this annex\n>>>> approach, you should be able to resolve ACP pinning issues?\n>>>>\n>>>> I'm curious to know how other people feel about that: is it ok to do\n>>>> later or should we try to implement this for the first release of v3\n>>>> transactions?\n>>>>\n>>>> The other change mentioned (making OP_TRUE standard and allowing outputs\n>>>> that are below dust) can be added later, as those won't be standard\n>>>> until\n>>>> we start allowing them, so there shouldn't be any\n>>>> backwards-compatibility\n>>>> issue with postponing this change. But maybe it's still worth having\n>>>> from\n>>>> the get-go, even though it may take a bit more time? Again, I'm curious\n>>>> to\n>>>> have other people's opinion here, I'd be happy to get all of those\n>>>> directly\n>>>> in the first release of v3 transactions, but I don't know how much\n>>>> implementation will have to go into that.\n>>>>\n>>>> > For clarification, package RBF is ParentTx*s*(plural), and\n>>>> ChildTx(singular),\n>>>> > so it might be a bit more complicated than we're thinking\n>>>>\n>>>> Right, good catch, this does require new logic to handle this case.\n>>>> As Gloria points out, this should be doable, and is definitely worth\n>>>> adding (those CSV 1 on every other output are really hacky, glad to\n>>>> find a way to get rid of them).\n>>>>\n>>>> Thanks,\n>>>> Bastien\n>>>>\n>>>> Le lun. 26 sept. 2022 \u00e0 18:48, Gloria Zhao <gloriajzhao at gmail.com> a\n>>>> \u00e9crit :\n>>>>\n>>>>> Hi Greg, Antoine, Bastien,\n>>>>>\n>>>>> Thanks very much for the feedback! I interpret most of the discussion\n>>>>> around limitations as ideas for future improvements rather than criticisms\n>>>>> of the proposal (please correct me if I'm wrong). I'll try to respond to as\n>>>>> much as possible.\n>>>>>\n>>>>> Also I realize that I didn't contextualize this proposal clearly\n>>>>> enough; it is very tailored for LN Penalty and definitely doesn't close all\n>>>>> pinning attacks possible (sorry for confusing anyone). I also agree that\n>>>>> some bits can be a little ugly or tack-on; I would definitely prefer a\n>>>>> comprehensive RBF revamp to fix all our problems and enable other\n>>>>> fee-bumping strategies such as\n>>>>> sign-ANYONECANPAY-then-bring-your-own-fees-by-adding-inputs-at-broadcast. I\n>>>>> was hoping to get some ideas with the \"RBF Improvements\" post in January,\n>>>>> but it doesn't seem like we're much closer to a workable proposal. I think\n>>>>> this is a minimally-invasive step that works for Lightning today, a small\n>>>>> fix similar to CPFP carve out.\n>>>>>\n>>>>> > As you likely know from previous discussions the biggest scenario\n>>>>> this does not fix in my estimation is ANYONECANPAY situations. If the\n>>>>> parent transaction can be \"inflated\" by tacking on additional inputs, this\n>>>>> means the total weight of the parent tx lowers the effective feerate of the\n>>>>> package.\n>>>>>\n>>>>> (For more context to other readers I wrote an explanation for this in\n>>>>> \"SIGHASH_ANYONECANPAY Pinning\" section of RBF ML post).  Yes, this\n>>>>> unfortunately doesn't fix any of the existing pinning attacks for single\n>>>>> transaction RBF but also doesn't make them worse. This boils down to adding\n>>>>> an incentive compatibility rule that ensures you can't replace a\n>>>>> transaction with something that will confirm slower. Package RBF has an\n>>>>> ancestor feerate-based rule for this (note it is quite conservative and not\n>>>>> perfect).\n>>>>>\n>>>>> So in the scenario above with the \"inflated\" parent that was signed\n>>>>> ACP, the replacement would be rejected because the package ancestor feerate\n>>>>> is lower than the feerate of what is being replaced. But it is imperfect\n>>>>> (explained below) and thus I wouldn't recommend it for single transaction\n>>>>> replacement. So that attack still exists for single transactions, yes.\n>>>>>\n>>>>> The strategy of using ACP to bring-your-own-fees has its own\n>>>>> challenges but hopefully has no current use cases as you say. AFAIK LN\n>>>>> Penalty is not affected by this since it doesn't use ACP, though obviously\n>>>>> I agree we should fix it for the future.\n>>>>>\n>>>>> So when I said \"this is intended for fee-bumping presigned txns in\n>>>>> contracting protocols,\" I should have said \"this is intended for\n>>>>> fee-bumping presigned txns specifically using CPFP and anchor outputs.\"\n>>>>> Apologies for forgetting to contextualize, I've been sitting on this for\n>>>>> too long.\n>>>>>\n>>>>> > The other scenario it doesn't really fix is where\n>>>>> HTLC/commitment-like transactions are being resolved in a batch, but due to\n>>>>> relative time constraints, you may want to accelerate some and not others.\n>>>>> Now you must pay higher rates to replace all of the transaction bumps. This\n>>>>> is a \"self-pin\" and \"get good at utxos noob\" type problem, but it's\n>>>>> something that axing rule#3 in favor of a Replace-by-ancestor-feerate\n>>>>> system would get us.\n>>>>>\n>>>>> I understand you to mean \"if you don't have enough UTXOs and you're\n>>>>> forced to batch-bump, you over-pay because you need to bring them all to\n>>>>> the highest target feerate.\" Isn't this kind of separate, wallet-related\n>>>>> problem? Contracting or not, surely every wallet needs to have enough UTXOs\n>>>>> to not batch transactions that shouldn't be batched... I don't see how a\n>>>>> replace-by-ancestor-feerate policy would make any difference for this?\n>>>>>\n>>>>> Also in general I'd like to reiterate that ancestor feerate is not a\n>>>>> panacea to all our RBF incentive compatibility concerns. Like individual\n>>>>> feerate, unless we run the mining algorithm, it cannot tell us exactly how\n>>>>> quickly this transaction would be mined.\n>>>>>\n>>>>> We're estimating the incentive compatibility of the original\n>>>>> transaction(s) and replacement transaction(s), with the goal of not letting\n>>>>> a transaction replace something that would have been more incentive\n>>>>> compatible to mine. As such, we don't want to overestimate how good the\n>>>>> replacement is, and we don't want to underestimate how good the original\n>>>>> transactions are. This rule \"The minimum between package feerate and\n>>>>> ancestor feerate of the child is not lower than the individual feerates of\n>>>>> all directly conflicting transactions and the ancestor feerates of all\n>>>>> original transactions\" is a conservative estimate.\n>>>>>\n>>>>> > Would kind of be nice if package RBF would detect a \"sibling output\n>>>>> spend\" conflict, and knock it out of the mempool via the other replacement\n>>>>> rules? Getting rid of the requirement to 1 block csv lock every output\n>>>>> would be quite nice from a smart contracting composability point of view.\n>>>>>\n>>>>> Interesting, so when a transaction hits a mempool tx's descendant\n>>>>> limit, we consider evicting one of its descendants in favor of this\n>>>>> transaction, based on the RBF rules.\n>>>>> Cool idea! After chewing on this for a bit, I think this *also* just\n>>>>> boils down to the fact that RBF should require replacements to be better\n>>>>> mining candidates. As in, if we added this policy and it can make us evict\n>>>>> the sibling and accept a transaction with a bunch of low-feerate ancestor\n>>>>> junk, it would be a new pinning vector.\n>>>>>\n>>>>> > If you're a miner and you receive a non-V3, second descendant of an\n>>>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>>>> backlog, I think you would have an interest to accept such a transaction.\n>>>>>\n>>>>> > So I'm not sure if those two rules are compatible with miners\n>>>>> incentives...\n>>>>>\n>>>>> The same argument can be made for the 26th descendant of a mempool\n>>>>> transaction; it's also not entirely incentive-compatible to reject it, but\n>>>>> that is not the *only* design goal in mempool policy. Of course, the\n>>>>> difference here is that the 25-descendant limit rule is a sensible DoS\n>>>>> protection, while this 1-descendant limit rule is more of a \"help the\n>>>>> Bitcoin ecosystem\" policy, just like CPFP carve-out, dust limit, etc. I can\n>>>>> of course understand why not everyone would be in favor of this, but I do\n>>>>> think it's worth it.\n>>>>>\n>>>>> > > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>\n>>>>> > >    larger than 1000 virtual bytes.\n>>>>>\n>>>>> > If I understand correctly the 1000 vb upper bound rational, it would\n>>>>> be to constraint the pinning counterparty to attach a high fee to a child\n>>>>> due to the limited size, if they would like this transaction to be stuck in\n>>>>> the network mempools. By doing so  this child has high odds to confirm.\n>>>>>\n>>>>> Yeah exactly, the \"Rule 3 pin\" is done by adding a child that's\n>>>>> high-fee (so you have to pay that much to evict it). Because they *don't*\n>>>>> want this tx to confirm, normally, this child would be really large. If\n>>>>> they only have 1000vB for the child, they can't increase the replacement\n>>>>> cost without also fee-bumping the transaction to make it confirm faster.\n>>>>>\n>>>>> > As of today, I think yes you can already fingerprint LN transactions\n>>>>> on the  spec-defined amount value of the anchor outputs, 330 sats. There is\n>>>>> always one of them on post-anchor commitment transactions. And sadly I\n>>>>> would say we'll always have tricky fingerprints leaking from unilateral LN\n>>>>> closures such as HTLC/PTLC timelocks...\n>>>>>\n>>>>> > I agree with you, this isn't worse than today, unilateral closes will\n>>>>> probably always be identifiable on-chain.\n>>>>>\n>>>>> Great to hear that there is no privacy worsening!\n>>>>>\n>>>>> Best,\n>>>>> Gloria\n>>>>>\n>>>>> On Mon, Sep 26, 2022 at 5:02 PM Greg Sanders <gsanders87 at gmail.com>\n>>>>> wrote:\n>>>>>\n>>>>>> Bastien,\n>>>>>>\n>>>>>> > This may be already covered by the current package RBF logic, in\n>>>>>> that\n>>>>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>>>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>>>>\n>>>>>> For clarification, package RBF is ParentTx*s*(plural), and\n>>>>>> ChildTx(singular), so it might be a bit more complicated than we're\n>>>>>> thinking, and currently the V3 proposal would first de-duplicate the\n>>>>>> ParentTx based on what is in the mempool, then look at the \"rest\" of the\n>>>>>> transactions as a package, then individually. Not the same, not sure how\n>>>>>> different. I'll defer to experts.\n>>>>>>\n>>>>>> Best,\n>>>>>> Greg\n>>>>>>\n>>>>>> On Mon, Sep 26, 2022 at 11:48 AM Bastien TEINTURIER via bitcoin-dev <\n>>>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>>>\n>>>>>>> Thanks Gloria for this great post.\n>>>>>>>\n>>>>>>> This is very valuable work for L2 contracts, and will greatly improve\n>>>>>>> their security model.\n>>>>>>>\n>>>>>>> > \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>>>> commitment tx in mempool?\"\n>>>>>>> > You won't need to fee-bump a counterparty's commitment tx using\n>>>>>>> CPFP.\n>>>>>>> > You would just package RBF it by attaching a high-feerate child to\n>>>>>>> > your commitment tx.\n>>>>>>>\n>>>>>>> Note that we can also very easily make that single anchor spendable\n>>>>>>> by\n>>>>>>> both participants (or even anyone), so if you see your counterparty's\n>>>>>>> commitment in your mempool, you can bump it without publishing your\n>>>>>>> own commitment, which is quite desirable (your own commitment tx has\n>>>>>>> CSV delays on your outputs, whereas your counterparty's commitment tx\n>>>>>>> doesn't).\n>>>>>>>\n>>>>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>>> transactions based on nVersion?\"\n>>>>>>>\n>>>>>>> I agree with you, this isn't worse than today, unilateral closes will\n>>>>>>> probably always be identifiable on-chain.\n>>>>>>>\n>>>>>>> > Would kind of be nice if package RBF would detect a \"sibling\n>>>>>>> output spend\"\n>>>>>>> > conflict, and knock it out of the mempool via the other\n>>>>>>> replacement rules?\n>>>>>>> > Getting rid of the requirement to 1 block csv lock every output\n>>>>>>> would be\n>>>>>>> > quite nice from a smart contracting composability point of view.\n>>>>>>>\n>>>>>>> +1, that would be very neat!\n>>>>>>>\n>>>>>>> This may be already covered by the current package RBF logic, in that\n>>>>>>> scenario we are simply replacing [ParentTx, ChildTx1] with\n>>>>>>> [ParentTx, ChildTx2] that pays more fees, right?\n>>>>>>>\n>>>>>>> > 1) I do think that we should seriously consider allowing OP_TRUE\n>>>>>>> to become\n>>>>>>> > a standard script type as part of this policy update. If pinning\n>>>>>>> is solved,\n>>>>>>> > then there's no reason to require all those extra bytes for\n>>>>>>> \"binding\" an\n>>>>>>> > anchor to a specific wallet/user. We can save quite a few bytes by\n>>>>>>> having\n>>>>>>> > the input be empty of witness data.\n>>>>>>> > 2) If we allow for a single dust-value(0 on up) output which is\n>>>>>>> immediately\n>>>>>>> > spent by the package, anchors become even easier to to design. No\n>>>>>>> value has\n>>>>>>> > to be \"sapped\" from contract participants to make an anchor\n>>>>>>> output. There's\n>>>>>>> > more complications for this, such as making sure the parent\n>>>>>>> transaction is\n>>>>>>> > dropped if the child spend is dropped, but maybe it's worth the\n>>>>>>> squeeze.\n>>>>>>>\n>>>>>>> I also think both of these could be quite useful. This would\n>>>>>>> probably always\n>>>>>>> be used in combination with a parent transaction that pays 0 fees,\n>>>>>>> so the\n>>>>>>> 0-value output would always be spent in the same block.\n>>>>>>>\n>>>>>>> But this means we could end up with 0-value outputs in the utxo set,\n>>>>>>> if for\n>>>>>>> some reason the parent tx is CPFP-ed via another output than the\n>>>>>>> 0-value one,\n>>>>>>> which would be a utxo set bloat issue. But I'd argue that we're\n>>>>>>> probably\n>>>>>>> already creating utxo set bloat with the 330 sat anchor outputs\n>>>>>>> (especially\n>>>>>>> since we use two of them, but only one is usually spent), so it would\n>>>>>>> probably be *better* than what we're doing today.\n>>>>>>>\n>>>>>>> Thanks,\n>>>>>>> Bastien\n>>>>>>>\n>>>>>>> Le lun. 26 sept. 2022 \u00e0 03:22, Antoine Riard via bitcoin-dev <\n>>>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>>>\n>>>>>>>> Hi Gloria,\n>>>>>>>>\n>>>>>>>> Thanks for the progress on package RBF, few early questions.\n>>>>>>>>\n>>>>>>>> > 2. Any descendant of an unconfirmed V3 transaction must also be\n>>>>>>>> V3.\n>>>>>>>>\n>>>>>>>> > 3. An unconfirmed V3 transaction cannot have more than 1\n>>>>>>>> descendant.\n>>>>>>>>\n>>>>>>>> If you're a miner and you receive a non-V3, second descendant of an\n>>>>>>>> unconfirmed V3 transaction, if the offered fee is in the top mempool\n>>>>>>>> backlog, I think you would have an interest to accept such a transaction.\n>>>>>>>>\n>>>>>>>> So I'm not sure if those two rules are compatible with miners\n>>>>>>>> incentives...\n>>>>>>>>\n>>>>>>>> > 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>>>> >    larger than 1000 virtual bytes.\n>>>>>>>>\n>>>>>>>> If I understand correctly the 1000 vb upper bound rational, it\n>>>>>>>> would be to constraint the pinning counterparty to attach a high fee to a\n>>>>>>>> child due to the limited size, if they would like this transaction to be\n>>>>>>>> stuck in the network mempools. By doing so  this child has high odds to\n>>>>>>>> confirm.\n>>>>>>>>\n>>>>>>>> I still wonder if this compatible with miner incentives in period\n>>>>>>>> of empty mempools, in the sense that if you've already a V3 transaction of\n>>>>>>>> size 100Kvb offering 2 sat/vb, it's more interesting than a V3 replacement\n>>>>>>>> candidate of size 1000 vb offering 10 sat/vb. It could be argued the former\n>>>>>>>> should be conserved.\n>>>>>>>>\n>>>>>>>> (That said, the hard thing with any replacement strategy we might\n>>>>>>>> evict a parent transaction *now* to which is attached a high-feerate child\n>>>>>>>> *latter* making for a utxo considered the best ancestor set. Maybe in the\n>>>>>>>> long-term miners should keep every transaction ever accepted...)\n>>>>>>>>\n>>>>>>>> > (Lower bound) the smaller this limit, the fewer UTXOs a child may\n>>>>>>>> use\n>>>>>>>> > to fund this fee-bump. For example, only allowing the V3 child to\n>>>>>>>> have\n>>>>>>>> > 2 inputs would require L2 protocols to manage a wallet with\n>>>>>>>> high-value\n>>>>>>>> > UTXOs and make batched fee-bumping impossible. However, as the\n>>>>>>>> > fee-bumping child only needs to fund fees (as opposed to\n>>>>>>>> payments),\n>>>>>>>> > just a few UTXOs should suffice.\n>>>>>>>>\n>>>>>>>> Reminder for L2 devs, batched fee-bumping of time-sensitive\n>>>>>>>> confirmations of commitment transactions is unsafe, as the counterparty\n>>>>>>>> could enter in a \"cat-and-mouse\" game to replace one of the batch element\n>>>>>>>> at each block to delay confirmation of the remaining elements in the batch,\n>>>>>>>> I think.\n>>>>>>>>\n>>>>>>>> On the other hand, I wonder if we wouldn't want a higher bound. LN\n>>>>>>>> wallets are likely to have one big UTXO in their fee-bumping reserve pool,\n>>>>>>>> as the cost of acquiring UTXO is non-null and in the optimistic case, you\n>>>>>>>> don't need to do unilateral closure. Let's say you close dozens of channels\n>>>>>>>> at the same time, a UTXO pool management strategy might be to fan-out the\n>>>>>>>> first spends UTXOs in N fan-out outputs ready to feed the remaining\n>>>>>>>> in-flight channels.\n>>>>>>>>\n>>>>>>>> > 1. The rule around unconfirmed inputs was\n>>>>>>>> > originally \"A package may include new unconfirmed inputs, but the\n>>>>>>>> > ancestor feerate of the child must be at least as high as the\n>>>>>>>> ancestor\n>>>>>>>> > feerates of every transaction being replaced.\"\n>>>>>>>>\n>>>>>>>> Note, I think we would like this new RBF rule to also apply to\n>>>>>>>> single transaction package, e.g second-stage HTLC transactions, where a\n>>>>>>>> counterparty pins a HTLC-preimage by abusing rule 3. In that case, the\n>>>>>>>> honest LN node should be able to broadcast a \"at least as high ancestor\n>>>>>>>> feerate\" HTLC-timeout transaction. With `option_anchor_outputs\" there is no\n>>>>>>>> unconfirmed ancestor to replace, as the commitment transaction, whatever\n>>>>>>>> the party it is originating from, should already be confirmed.\n>>>>>>>>\n>>>>>>>> > \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>>>> transactions based on nVersion?\"\n>>>>>>>>\n>>>>>>>> As of today, I think yes you can already fingerprint LN\n>>>>>>>> transactions on the  spec-defined amount value of the anchor outputs, 330\n>>>>>>>> sats. There is always one of them on post-anchor commitment transactions.\n>>>>>>>> And sadly I would say we'll always have tricky fingerprints leaking from\n>>>>>>>> unilateral LN closures such as HTLC/PTLC timelocks...\n>>>>>>>>\n>>>>>>>> > \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>>>>\n>>>>>>>> IIUC, a V3 package could replace a V2 package, with the benefit of\n>>>>>>>> the new package RBF rules applied. I think this would be a significant\n>>>>>>>> advantage for LN, as for the current ~85k of opened channels, the old V2\n>>>>>>>> states shouldn't be pinning vectors. Currently, commitment transactions\n>>>>>>>> signal replaceability.\n>>>>>>>>\n>>>>>>>> Le ven. 23 sept. 2022 \u00e0 11:26, Gloria Zhao via bitcoin-dev <\n>>>>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>>>>\n>>>>>>>>> Hi everyone,\n>>>>>>>>>\n>>>>>>>>> I'm writing to propose a very simple set of mempool/transaction\n>>>>>>>>> relay\n>>>>>>>>> policies intended to aid L2/contract protocols. I realized that\n>>>>>>>>> the previously proposed Package Mempool Accept package RBF [1]\n>>>>>>>>> had a few remaining problems after digging into the RBF logic more\n>>>>>>>>> [2].\n>>>>>>>>> This additional set of policies solves them without requiring a\n>>>>>>>>> huge RBF overhaul.\n>>>>>>>>>\n>>>>>>>>> I've written an implementation (and docs) for Bitcoin Core:\n>>>>>>>>> https://github.com/bitcoin/bitcoin/pull/25038\n>>>>>>>>>\n>>>>>>>>> (You may notice that this proposal incorporates feedback on the PR\n>>>>>>>>> - thanks Suhas Daftuar, Gregory Sanders, Bastien Teinturier, Anthony Towns,\n>>>>>>>>> and others.)\n>>>>>>>>>\n>>>>>>>>> If you are interested in using package RBF/relay to bump presigned\n>>>>>>>>> transactions, I think you may be interested in reviewing this\n>>>>>>>>> proposal.\n>>>>>>>>> This should solve Rule 3 pinning and perhaps allow us\n>>>>>>>>> to get rid of CPFP carve-out (yay!). I'm keen to hear if people\n>>>>>>>>> find\n>>>>>>>>> the 1-anchor-output, 1000vB child limit too restrictive. Also, if\n>>>>>>>>> you find a\n>>>>>>>>> pinning attack or something that makes it unusable for you, I would\n>>>>>>>>> really really like to know.\n>>>>>>>>>\n>>>>>>>>> Note that transactions with nVersion=3 (\"V3 transactions\") are\n>>>>>>>>> currently non-standard in Bitcoin Core. That means **anything that\n>>>>>>>>> was\n>>>>>>>>> standard before this policy change would still be standard\n>>>>>>>>> afterwards.** If you don't want your transactions to be subject to\n>>>>>>>>> these rules, just continue whatever you're doing and don't use\n>>>>>>>>> nVersion=3. AFAICT this shouldn't break anything, but let me know\n>>>>>>>>> if\n>>>>>>>>> this would be disruptive for you?\n>>>>>>>>>\n>>>>>>>>> **New Policies:**\n>>>>>>>>>\n>>>>>>>>> This includes:\n>>>>>>>>> - a set of additional policy rules applying to V3 transactions\n>>>>>>>>> - modifications to package RBF rules\n>>>>>>>>>\n>>>>>>>>> **V3 transactions:**\n>>>>>>>>>\n>>>>>>>>> Existing standardness rules apply to V3 (e.g. min/max tx weight,\n>>>>>>>>> standard output types, cleanstack, etc.). The following additional\n>>>>>>>>> rules apply to V3:\n>>>>>>>>>\n>>>>>>>>> 1. A V3 transaction can be replaced, even if it does not signal\n>>>>>>>>> BIP125\n>>>>>>>>>    replaceability. (It must also meet the other RBF rules around\n>>>>>>>>> fees,\n>>>>>>>>> etc. for replacement to happen).\n>>>>>>>>>\n>>>>>>>>> 2. Any descendant of an unconfirmed V3 transaction must also be V3.\n>>>>>>>>>\n>>>>>>>>> *Rationale*: Combined with Rule 1, this gives us the property of\n>>>>>>>>> \"inherited\" replaceability signaling when descendants of\n>>>>>>>>> unconfirmed\n>>>>>>>>> transactions are created. Additionally, checking whether a\n>>>>>>>>> transaction\n>>>>>>>>> signals replaceability this way does not require mempool traversal,\n>>>>>>>>> and does not change based on what transactions are mined. It also\n>>>>>>>>> makes subsequent rules about descendant limits much easier to\n>>>>>>>>> check.\n>>>>>>>>>\n>>>>>>>>> *Note*: The descendant of a *confirmed* V3 transaction does not\n>>>>>>>>> need to be V3.\n>>>>>>>>>\n>>>>>>>>> 3. An unconfirmed V3 transaction cannot have more than 1\n>>>>>>>>> descendant.\n>>>>>>>>>\n>>>>>>>>> *Rationale*: (Upper bound) the larger the descendant limit, the\n>>>>>>>>> more\n>>>>>>>>> transactions may need to be replaced. This is a problematic pinning\n>>>>>>>>> attack, i.e., a malicious counterparty prevents the transaction\n>>>>>>>>> from\n>>>>>>>>> being replaced by adding many descendant transactions that aren't\n>>>>>>>>> fee-bumping.\n>>>>>>>>>\n>>>>>>>>> (Lower bound) at least 1 descendant is required to allow CPFP of\n>>>>>>>>> the\n>>>>>>>>> presigned transaction. The contract protocol can create presigned\n>>>>>>>>> transactions paying 0 fees and 1 output for attaching a CPFP at\n>>>>>>>>> broadcast time (\"anchor output\"). Without package RBF, multiple\n>>>>>>>>> anchor\n>>>>>>>>> outputs would be required to allow each counterparty to fee-bump\n>>>>>>>>> any\n>>>>>>>>> presigned transaction. With package RBF, since the presigned\n>>>>>>>>> transactions can replace each other, 1 anchor output is sufficient.\n>>>>>>>>>\n>>>>>>>>> 4. A V3 transaction that has an unconfirmed V3 ancestor cannot be\n>>>>>>>>>    larger than 1000 virtual bytes.\n>>>>>>>>>\n>>>>>>>>> *Rationale*: (Upper bound) the larger the descendant size limit,\n>>>>>>>>> the\n>>>>>>>>> more vbytes may need to be replaced. With default limits, if the\n>>>>>>>>> child\n>>>>>>>>> is e.g. 100,000vB, that might be an additional 100,000sats (at\n>>>>>>>>> 1sat/vbyte) or more, depending on the feerate.\n>>>>>>>>>\n>>>>>>>>> (Lower bound) the smaller this limit, the fewer UTXOs a child may\n>>>>>>>>> use\n>>>>>>>>> to fund this fee-bump. For example, only allowing the V3 child to\n>>>>>>>>> have\n>>>>>>>>> 2 inputs would require L2 protocols to manage a wallet with\n>>>>>>>>> high-value\n>>>>>>>>> UTXOs and make batched fee-bumping impossible. However, as the\n>>>>>>>>> fee-bumping child only needs to fund fees (as opposed to payments),\n>>>>>>>>> just a few UTXOs should suffice.\n>>>>>>>>>\n>>>>>>>>> With a limit of 1000 virtual bytes, depending on the output types,\n>>>>>>>>> the\n>>>>>>>>> child can have 6-15 UTXOs, which should be enough to fund a\n>>>>>>>>> fee-bump\n>>>>>>>>> without requiring a carefully-managed UTXO pool. With 1000 virtual\n>>>>>>>>> bytes as the descendant limit, the cost to replace a V3 transaction\n>>>>>>>>> has much lower variance.\n>>>>>>>>>\n>>>>>>>>> *Rationale*: This makes the rule very easily \"tacked on\" to\n>>>>>>>>> existing\n>>>>>>>>> logic for policy and wallets. A transaction may be up to 100KvB on\n>>>>>>>>> its\n>>>>>>>>> own (`MAX_STANDARD_TX_WEIGHT`) and 101KvB with descendants\n>>>>>>>>> (`DEFAULT_DESCENDANT_SIZE_LIMIT_KVB`). If an existing V3\n>>>>>>>>> transaction\n>>>>>>>>> in the mempool is 100KvB, its descendant can only be 1000vB, even\n>>>>>>>>> if\n>>>>>>>>> the policy is 10KvB.\n>>>>>>>>>\n>>>>>>>>> **Package RBF modifications:**\n>>>>>>>>>\n>>>>>>>>> 1. The rule around unconfirmed inputs was\n>>>>>>>>> originally \"A package may include new unconfirmed inputs, but the\n>>>>>>>>> ancestor feerate of the child must be at least as high as the\n>>>>>>>>> ancestor\n>>>>>>>>> feerates of every transaction being replaced.\"\n>>>>>>>>>\n>>>>>>>>> The package may still include new unconfirmed inputs. However,\n>>>>>>>>> the new rule is modified to be \"The minimum between package feerate\n>>>>>>>>> and ancestor feerate of the child is not lower than the individual\n>>>>>>>>> feerates of all directly conflicting transactions and the ancestor\n>>>>>>>>> feerates of all original transactions.\"\n>>>>>>>>>\n>>>>>>>>> *Rationale*: We are attempting to ensure that the replacement\n>>>>>>>>> transactions are not less incentive-compatible to mine. However, a\n>>>>>>>>> package/transaction's ancestor feerate is not perfectly\n>>>>>>>>> representative\n>>>>>>>>> of its incentive compatibility; it may overestimate (some subset of\n>>>>>>>>> the ancestors could be included by itself if it has other\n>>>>>>>>> high-feerate\n>>>>>>>>> descendants or are themselves higher feerate than this\n>>>>>>>>> package/transaction). Instead, we use the minimum between the\n>>>>>>>>> package\n>>>>>>>>> feerate and ancestor feerate of the child as a more conservative\n>>>>>>>>> value\n>>>>>>>>> than what was proposed originally.\n>>>>>>>>>\n>>>>>>>>> 2. A new rule is added, requiring that all package transactions\n>>>>>>>>> with\n>>>>>>>>> mempool conflicts to be V3. This also means the \"sponsoring\"\n>>>>>>>>> child transaction must be V3.\n>>>>>>>>>\n>>>>>>>>> *Note*: Combined with the V3 rules, this means the package must be\n>>>>>>>>> a child-with-parents package. Since package validation is only\n>>>>>>>>> attempted if the transactions do not pay sufficient fees to be\n>>>>>>>>> accepted on their own, this effectively means that only V3\n>>>>>>>>> transactions can pay to replace their ancestors' conflicts, and\n>>>>>>>>> only\n>>>>>>>>> V3 transactions' replacements may be paid for by a descendant.\n>>>>>>>>>\n>>>>>>>>> *Rationale*: The fee-related rules are economically rational for\n>>>>>>>>> ancestor packages, but not necessarily other types of packages.\n>>>>>>>>> A child-with-parents package is a type of ancestor package. It\n>>>>>>>>> may be fine to allow any ancestor package, but it's more difficult\n>>>>>>>>> to account for all of the possibilities. For example, it gets much\n>>>>>>>>> harder to see that we're applying the descendant limits correctly\n>>>>>>>>> if\n>>>>>>>>> the package has a gnarly, many-generation, non-tree shape. I'm also\n>>>>>>>>> not sure if this policy is 100% incentive-compatible if the sponsor\n>>>>>>>>> is not a direct descendant of the sponsee.\n>>>>>>>>>\n>>>>>>>>> Please see doc/policy/version3_transactions.md and\n>>>>>>>>> doc/policy/packages.md in the PR for the full set of rules.\n>>>>>>>>>\n>>>>>>>>> **Intended usage for LN:**\n>>>>>>>>>\n>>>>>>>>> Commitment transactions should be V3 and have 1 anchor output. They\n>>>>>>>>> can be signed with 0 fees (or 1sat/vbyte) once package relay is\n>>>>>>>>> deployed\n>>>>>>>>> on a significant portion of the network. If the commitment tx must\n>>>>>>>>> be broadcast, determine the desired feerate at broadcast time and\n>>>>>>>>> spend the anchor output in a high feerate transaction. I'm going to\n>>>>>>>>> call the broadcasted commitment tx \"the parent\" and the attached\n>>>>>>>>> fee-bumping tx \"the child.\"\n>>>>>>>>>\n>>>>>>>>> - This child must be V3.\n>>>>>>>>> - This child must be at most 1000vB. Note this restricts the\n>>>>>>>>>   number of inputs you can use to fund the fee bump. Depending\n>>>>>>>>> on the output types, this is around 6-15.\n>>>>>>>>> - One child may fund fees for multiple commitment tx (\"batched\n>>>>>>>>>   fee-bumping\").\n>>>>>>>>> - To do a second fee-bump to add more fees, replace the\n>>>>>>>>>   *child* with a higher-feerate tx. Do not try to attach a\n>>>>>>>>> grandchild.\n>>>>>>>>>\n>>>>>>>>> Otherwise, never try to spend from an unconfirmed V3 transaction.\n>>>>>>>>> The\n>>>>>>>>> descendant limits for V3 transactions are very restrictive.\n>>>>>>>>>\n>>>>>>>>> **Expected Questions:**\n>>>>>>>>>\n>>>>>>>>> \"Does this fix Rule 3 Pinning?\"\n>>>>>>>>> Yes. The V3 descendant limit restricts both you and your\n>>>>>>>>> counterparty.\n>>>>>>>>> Assuming nodes adopted this policy, you may reasonably assume that\n>>>>>>>>> you\n>>>>>>>>> only need to replace the commitment transaction + up to 1000vB.\n>>>>>>>>>\n>>>>>>>>> \"Only 1 anchor output? What if I need to bump counterparty's\n>>>>>>>>> commitment tx in mempool?\"\n>>>>>>>>> You won't need to fee-bump a counterparty's commitment tx using\n>>>>>>>>> CPFP.\n>>>>>>>>> You would just package RBF it by attaching a high-feerate child to\n>>>>>>>>> your commitment tx.\n>>>>>>>>>\n>>>>>>>>> \"Is this a privacy issue, i.e. doesn't it allow fingerprinting LN\n>>>>>>>>> transactions based on nVersion?\"\n>>>>>>>>> Indeed it may be unrealistic to assume V3 transactions will be in\n>>>>>>>>> widespread use outside of L2. IIUC, unilateral closes are already\n>>>>>>>>> obvious LN transactions because of the HTLC inputs. For e.g.\n>>>>>>>>> cooperative closes and opens, I think it makes sense to continue\n>>>>>>>>> using\n>>>>>>>>> V2. So, unless I'm missing something, this shouldn't make it worse.\n>>>>>>>>>\n>>>>>>>>> \"So a V3 transaction that doesn't signal BIP125 replaceability is\n>>>>>>>>> replaceable? Is that a backward compatibility issue?\"\n>>>>>>>>> Yes it's replaceable. It's not an issue AFAICT because,\n>>>>>>>>> under previous policy, the V3 transaction wouldn't have been\n>>>>>>>>> in the mempool in the first place.\n>>>>>>>>>\n>>>>>>>>> \"Can a V2 transaction replace a V3 transaction and vice versa?\"\n>>>>>>>>> Yes, otherwise someone can use V3 transactions to censor V2\n>>>>>>>>> transactions spending shared inputs. Note if the\n>>>>>>>>> original V3 transaction has an unconfirmed V3 parent, this would\n>>>>>>>>> violate the \"inherited V3\" rule and would be rejected.\n>>>>>>>>>\n>>>>>>>>> Thanks for reading! Feedback and review would be much appreciated.\n>>>>>>>>>\n>>>>>>>>> [1]:\n>>>>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019464.html\n>>>>>>>>> [2]:\n>>>>>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019817.html\n>>>>>>>>>\n>>>>>>>>> Best,\n>>>>>>>>> Gloria\n>>>>>>>>> _______________________________________________\n>>>>>>>>> bitcoin-dev mailing list\n>>>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>>>\n>>>>>>>> _______________________________________________\n>>>>>>>> bitcoin-dev mailing list\n>>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>>\n>>>>>>> _______________________________________________\n>>>>>>> bitcoin-dev mailing list\n>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>\n>>>>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220930/10024929/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "New transaction policies (nVersion=3) for contracting protocols",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard",
                "Bastien TEINTURIER",
                "Gloria Zhao",
                "Ruben Somsen",
                "Greg Sanders"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 276092
        }
    },
    {
        "title": "[bitcoin-dev] Packaged Transaction Relay",
        "thread_messages": [
            {
                "author": "alicexbt",
                "date": "2022-09-26T17:50:48",
                "message_text_only": "Hi Eric,\n\n\nThis email wasn't answered by anyone on mailing list however I did some research about packages yesterday including this email and below are my observations, questions etc.\n\n> The sole objective, as expressed in the OP proposal, is to:\n> \n> \"Propagate transactions that are incentive-compatible to mine, even if they don't meet minimum feerate alone.\"\n\n\nAccording to [bitcoinops][1]: Without package relay, it\u2019s not possible to effectively CPFP fee bump a transaction that\u2019s below the minimum feerate nodes accept.\n\nMatt Corallo's thoughts in a bitcoin core [issue][2]:\n\n\"Matt Corallo recently wrote about an example on the bitcoin-dev mailing list involving lightning transactions, where pre-signed transactions might be broadcast to the blockchain long after they were generated, and thus not have been created with a fee that is sufficient to be confirmed quickly (or even be accepted to node mempools). In such situations, channel participants may need to use chained transactions (CPFP) in order to increase the confirmation speed of such transactions, and that implies we may need to introduce a mechanism for those parent transactions to be relayed along with their higher feerate children, even if the parent transaction would be rejected by itself.\"\n\n1)Is it possible to have multiple pre-signed transactions with different fee rates in a range? Example: PSBT1: 5 sat/vbyte, PSBT2: 10 sat/vbyte, PSBT3: 20 sat/vbyte and PSBT4: 100 sat/vbyte\n2)How would covenants affect this problem?\n3)How often does it happen that a pre-signed tx gets rejected by nodes because it did not meet the minimum fee rate? Is it predictable and could be managed in a different way?\n\nAfter reading several links related to packages and bitcoin core pull requests, I found it anti-bitcoin to introduce so much complexity because its not possible to CPFP fee bump a tx below minimum fee rate. \n\n\n> Furthermore any tx that is \"stuck\" can be freed by simply sending another tx. The nodes at which the tx has become stuck will just package it up and relay it to peers. In other words, there is no impact on wallet implementation apart from raising the aggregate fee using a descendant transaction.\n\nIt is easy to send another tx if there is only one user involved however packages are trying to fix issues in which multiple users and transaction pre-signed between them are involved. So, it will be difficult to coordinate and create new pre-signed transactions in some cases although it is possible for some use cases.\n\n\n> This is barely a protocol change - it's primarily implementation. All that should be required is an additional INV element type, such as MSG_TX_PACKAGE.\n\n> * All elements of MSG_TX_PACKAGE in one INV message MUST to be of the same package.\n> * A package MUST must define a set that can be mined into one block (size/sigops constraint).\n> * A package SHOULD not contain confirmed txs (a race may cause this).\n> * A package MUST minimally satisfy peer.feerate.\n> * A partial tx order, as in the manner of the block.txs ordering, MUST be imposed.\n> * A node SHOULD drop a peer that sends a package (or tx) below node.feerate.\n> * A node MAY drop a peer that sends a non-minimal package according to node.feerate.\n\nThis makes sense particularly if multiple node implementations are used in future. \n\nMy other questions:\n\na)If a package has tx1, tx2, tx3, tx4 and tx5 and miner just include tx1 and tx2 in the block, how does this affect the projects considered for packages proposal?\n\nb)How does changing the order of txs in a package affect these transactions?\n\nc)Do packages introduce more attack vectors in bitcoin for front running or MEV? MEV in bitcoin currently only affects the projects that are considered in packages proposal.\n\nd)What if the package contains a transactions with sanctioned address?\n\ne)Why would miners use packages if the existing scenario in terms of fees per block is beneficial for them?\n\n\n/dev/fd0\n\n[1]: https://bitcoinops.org/en/topics/package-relay/\n[2]: https://github.com/bitcoin/bitcoin/issues/14895\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Thursday, June 9th, 2022 at 4:13 AM, Eric Voskuil via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\n> Hi Suhas/Gloria,\n> \n> Good questions. I've started a new thread because it became something else...\n> \n> Various ideas about packaging seem to be focused on the idea of an atomic message that is gossiped around the network like a transaction or block. From my perspective that seems to create a set of problems without good solutions, and it is not a proper analogy to those atomic structures. It may be worth taking the time to step back and take a close look at the underlying objective.\n> \n> The sole objective, as expressed in the OP proposal, is to:\n> \n> \"Propagate transactions that are incentive-compatible to mine, even if they don't meet minimum feerate alone.\"\n> \n> Effectively producing this outcome with an atomic packaging approach while at the same time maintaining network invariants seems unlikely, if not impossible.\n> \n> Fees:\n> \n> A node knows what fee rate a peer will accept, and announces individual txs that satisfy peer.feerate. Similarly a node knows its own feerate, and SHOULD drop any peer that announces txs that do not satisfy node.feerate.\n> \n> Orphans:\n> \n> A node MAY drop a peer that announces txs that the node sees as orphans against its DAG. It SHOULD drop the orphan tx and MAY request missing ancestors. Presumably after some amount of time connected to peer, node does not expect to see any more orphans from that peer, so these choices could evolve with the channel. However, the design that can only consider each tx in isolation will continue to cause orphan announcements on the channel. A below peer.feerate tx does not get announced to peer, and later a descendant high peer.feerate does get announced to the peer - as an orphan.\n> \n> BIP133 (feefilter):\n> \n> \"There could be a small number of edge cases where a node's mempool min fee is actually less than the filter value a peer is aware of and transactions with fee rates between these values will now be newly inhibited.\"\n> \n> https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki\n> \n> Whether the problem is \"small\" or not depends on the disparity between node fee rates, which is not a matter of protocol. This is an existing problem that can and should be dealt with in packaging, as part of the above objective.\n> \n> Packaged Transaction Relay:\n> \n> One might instead think of packaging as a per-connection function, operating over its transaction (input->output) DAG and the feerate of its own node and that of the peer. Logically a \"package\" is nothing more than a set of transactions (optimized by announcement). Only a node can effectively determine the packaging required by each of its peers, since only the node is aware of peer.feerate.\n> \n> \n> The only way to avoid dead-ending packages (including individual transactions, as is the objective) is for a node to package txs for each peer. The origination of any package is then just a wallet peer doing what a node does - packaging transactions that satisfy peer.feerate (i.e. that of its node).\n> \n> Current transaction relay (txB->txA):\n> \n> ===============================\n> Node0\n> txA.feerate > node.feerate, and not orphaned (accept txA)\n> \n> txA.feerate > peer1.feerate (announce txA to peer1)\n> \n> txA.feerate < peer2.feerate (do not announce txA to peer2)\n> -----\n> txB.feerate > node.feerate (accept txB)\n> \n> txB.feerate > peer1.feerate (announce txB to peer1)\n> \n> txB.feerate > peer2.feerate (announce txB to peer2)\n> \n> \n> Node1\n> Sees/accepts txA and txB.\n> \n> Node2\n> Never sees txA, sees/rejects txB (as an orphan).\n> \n> Packaged transaction relay (txB->txA):\n> \n> ===============================\n> Node0\n> txA.feerate > node.feerate, and not orphaned (accept txA)\n> \n> txA.feerate > peer1.feerate (announce txA to peer1)\n> \n> txA.feerate < peer2.feerate (do not announce txA to peer2)\n> -----\n> txB.feerate > node1.feerate (accept txB)\n> \n> txB.feerate > peer1.feerate (announce txB to peer1)\n> \n> txB.feerate > peer2.feerate (do not announce txB to peer2) <== avoid predictable orphan\n> \n> txA.feerate + txB.feerate > peer2.feerate (announce pkg(A, B) to peer2) <= create minimal package\n> \n> \n> Node1\n> Sees/accepts txA and txB.\n> \n> Node2\n> pkg(A, B) > node2.feerate (accept txA, txB)\n> \n> txA.feerate > peer3.feerate (announce txA to peer3)\n> \n> txB.feerate > peer3.feerate (announce txB to peer3)\n> \n> \n> Sees/accepts pkg(A, B).\n> \n> Node3\n> Sees/accepts txA and txB. <= avoided unnecessary packaging\n> \n> Summary:\n> \n> In this design, any node that receives an announcement for a pkg (or tx) later determined to be less than node.feerate SHOULD drop the announcing peer. Unlike with existing tx relay, a node can become \"current\" and subsequently see few if any tx or pkg orphans, and MAY at some point decide to drop any peer that announces one. Notice that packages are created dynamically, and any package that doesn't need to be grouped gets trimmed down to individual transactions. Furthermore any tx that is \"stuck\" can be freed by simply sending another tx. The nodes at which the tx has become stuck will just package it up and relay it to peers. In other words, there is no impact on wallet implementation apart from raising the aggregate fee using a descendant transaction.\n> \n> This is barely a protocol change - it's primarily implementation. All that should be required is an additional INV element type, such as MSG_TX_PACKAGE.\n> \n> Additional constraints:\n> \n> * All elements of MSG_TX_PACKAGE in one INV message MUST to be of the same package.\n> * A package MUST must define a set that can be mined into one block (size/sigops constraint).\n> * A package SHOULD not contain confirmed txs (a race may cause this).\n> * A package MUST minimally satisfy peer.feerate.\n> * A partial tx order, as in the manner of the block.txs ordering, MUST be imposed.\n> * A node SHOULD drop a peer that sends a package (or tx) below node.feerate.\n> * A node MAY drop a peer that sends a non-minimal package according to node.feerate.\n> \n> The partial ordering of block.txs introduces an ordering constraint that precludes full parallelism in validating input attachment. This is an implementation artifact that made its way into consensus. However in the case of packaging, the set of txs is not presumed to be valid under the proof of work DoS guard. As such constraints should minimize the work/traffic required to invalidate the message. The partial order constraint ensures that the DAG can be built incrementally, dropping the attempt (and peer as desired) as soon as the first orphan is discovered. As a result the network traffic and work required is not materially different than with tx relay, with two exceptions.\n> \n> These are the two central aspects of this approach (Avoiding Predictable Orphans and Creating Minimal Packages). These are graph search algorithms, some basic computer science. Minimality requires only that the package does not introduce txs that are not necessary to reach the peer.feerate (as these can always be packaged separately). It does not require that nodes all generate the same packages. It does not require negotiation, package identity, cryptography, or hashing. As a graph search it should be O(n) where n is the unconfirmed ancestry of the package, but should typically be much lower, if not a single step.\n> \n> Sufficiently-low-fee nodes will see only single txs. Moderate-fee nodes may cause partial breakup of packages. Sufficiently high fee nodes will cause peers (having received and completed the acceptance of a tx/pkg with pkg.feerate < peer.feerate) to navigate from each tx/package external input until reaching txs above peer.feerate, or confirmed (both of which the peer is presumed to already have). If the pkg.feerate is sufficiently high to connect all external inputs to the intervening txs, they are added to the package and it is announced to the high fee peer. Note that the individual tx.feerate > peer.feerate is insufficient to ensure that the peer should have the tx, as there may be ancestor txs that do not, and for which the tx was insufficient to cause them to be packaged. So a non-caching algorithm must be able to chase each package external input to a confirmed tx (or cache the unconfirmed ancestry fee rate at each tx). Note that fee rates are not directly additive, both size/\n> \n> weight and fee are required for summation (and aggregate sigops should be considered).\n> \n> This makes no assumptions about current implementations. The design would call for maintenance of a transaction (input->output) DAG with tx.feerate on each tx. This could be the unconfirmed tx graph (i.e. \"memory pool\") though it does not require maintenance of anything more than the parameters necessary to confirm a set of validated txs within a block. It is very reasonable to require this of any participating node. A simple version negotiation can identify a package-accepting/sending nodes.\n> \n> \n> I have thought about this for some time, but have not implemented either the graph search, source code, or BIP. Just wrote this off the top of my head. So I am sure there are some things I have incorrect or failed to consider. But I think it's worth discussing it at this point.\n> \n> e\n> \n> > -----Original Message-----\n> > From: bitcoin-dev bitcoin-dev-bounces at lists.linuxfoundation.org On\n> > Behalf Of Suhas Daftuar via bitcoin-dev\n> > Sent: Wednesday, June 8, 2022 8:59 AM\n> > To: Bitcoin Protocol Discussion bitcoin-dev at lists.linuxfoundation.org\n> > Subject: Re: [bitcoin-dev] Package Relay Proposal\n> > \n> > Hi,\n> > \n> > Thanks again for your work on this!\n> > \n> > One question I have is about potential bandwidth waste in the case of nodes\n> > running with different policy rules. Here's my understanding of a scenario I\n> > think could happen:\n> > \n> > 1) Transaction A is both low-fee and non-standard to some nodes on the\n> > network.\n> > 2) Whenever a transaction T that spends A is relayed, new nodes will send\n> > INV(PKGINFO1, T) to all package-relay peers.\n> > 3) Nodes on the network that have implemented package relay, but do not\n> > accept A, will send getdata(PKGINFO1, T) and learn all of T's unconfirmed\n> > parents (~32 bytes * number of parents(T)).\n> > 4) Such nodes will reject T. But because of transaction malleability, and to\n> > avoid being blinded to a transaction unnecessarily, these nodes will likely still\n> > send getdata(PKGINFO1, T) to every node that announces T, in case\n> > someone has a transaction that includes an alternate set of parent\n> > transactions that would pass policy checks.\n> > \n> > Is that understanding correct? I think a good design goal would be to not\n> > waste bandwidth in non-adversarial situations. In this case, there would be\n> > bandwidth waste from downloading duplicate data from all your peers, just\n> > because the announcement doesn't commit to the set of parent wtxids that\n> > we'd get from the peer (and so we are unable to determine that all our peers\n> > would be telling us the same thing, just based on the announcement).\n> > \n> > Some ways to mitigate this might be to: (a) include a hash (maybe even just a\n> > 20-byte hash -- is that enough security?) of the package wtxids (in some\n> > canonical ordering) along with the wtxid of the child in the initial\n> > announcement; (b) limit the use of v1 packages to transactions with very few\n> > parents (I don't know if this is reasonable for the use cases we have in mind).\n> > \n> > Another point I wanted to bring up is about the rules around v1 package\n> > validation generally, and the use of a blockhash in transaction relay\n> > specifically. My first observation is that it won't always be the case that a v1\n> > package relay node will be able to validate that a set of package transactions\n> > is fully sorted topologically, because there may be (non-parent) ancestors\n> > that are missing from the package and the best a peer can validate is\n> > topology within the package -- this means that a peer can validly (under this\n> > BIP) relay transaction packages out of the true topological sort (if all\n> > ancestors were included).\n> > \n> > This makes me wonder how useful this topological rule is. I suppose there is\n> > some value in preventing completely broken implementations from staying\n> > connected and so there is no harm in having the rule, but perhaps it would\n> > be helpful to add that nodes SHOULD order transactions based on topological\n> > sort in the complete transaction graph, so that if missing-from-package\n> > ancestors are already known by a peer (which is the expected case when\n> > using v1 package relay on transactions that have more than one generation\n> > of unconfirmed ancestor) then the remaining transactions are already\n> > properly ordered, and this is helpful even if unenforceable in general.\n> > \n> > The other observation I wanted to make was that having transaction relay\n> > gated on whether two nodes agree on chain tip seems like an overly\n> > restrictive criteria. I think an important design principle is that we want to\n> > minimize disruption from network splits -- if there are competing blocks\n> > found in a small window of time, it's likely that the utxo set is not materially\n> > different on the two chains (assuming miners are selecting from roughly the\n> > same sets of transactions when this happens, which is typical). Having\n> > transaction relay bifurcate on the two network halves would seem to\n> > exacerbate the difference between the two sides of the split -- users ought\n> > to be agnostic about how benign splits are resolved and would likely want\n> > their transactions to relay across the whole network.\n> > \n> > Additionally, use of a chain tip might impose a larger burden than is necessary\n> > on software that would seek to participate in transaction relay without\n> > implementing headers sync/validation. I don't know what software exists on\n> > the network, but I imagine there are a lot of scripts out there for transaction\n> > submission to the public p2p network, and in thinking about modifying such a\n> > script to utilize package relay it seems like an unnecessary added burden to\n> > first learn a node's tip before trying to relay a transaction.\n> > \n> > Could you explain again what the benefit of including the blockhash is? It\n> > seems like it is just so that a node could prioritize transaction relay from\n> > peers with the same chain tip to maximize the likelihood of transaction\n> > acceptance, but in the common case this seems like a pretty negligible\n> > concern, and in the case of a chain fork that persists for many minutes it\n> > seems better to me that we not partition the network into package-relay\n> > regimes and just risk a little extra bandwidth in one direction or the other. If\n> > we solve the problem I brought up at the beginning (of de-duplicating\n> > package data across peers with a package-wtxid-commitment in the\n> > announcement), I think this is just some wasted pkginfo bandwidth on a\n> > single-link, and not across links (as we could cache validation failure for a\n> > package-hash to avoid re-requesting duplicate pkginfo1 messages).\n> > \n> > Best,\n> > Suhas\n> > \n> > On Tue, Jun 7, 2022 at 1:57 PM Gloria Zhao via bitcoin-dev <bitcoin-\n> > dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > dev at lists.linuxfoundation.org> > wrote:\n> > \n> > Hi Eric, aj, all,\n> > \n> > Sorry for the delayed response. @aj I'm including some paraphrased\n> > points from our offline discussion (thanks).\n> > \n> > > Other idea: what if you encode the parent txs as a short hash of the\n> > > wtxid (something like bip152 short ids? perhaps seeded per peer so collisions\n> > > will be different per peer?) and include that in the inv announcement?\n> > > Would that work to avoid a round trip almost all of the time, while still giving\n> > > you enough info to save bw by deduping parents?\n> > \n> > > As I suggested earlier, a package is fundamentally a compact block\n> > > (or\n> > > block) announcement without the header. Compact block (BIP152)\n> > > announcement\n> > > is already well-defined and widely implemented...\n> > \n> > > Let us not reinvent the wheel and/or introduce accidental\n> > > complexity. I see\n> > > no reason why packaging is not simply BIP152 without the 'header'\n> > > field, an\n> > > updated protocol version, and the following sort of changes to\n> > > names\n> > \n> > Interestingly, \"why not use BIP 152 shortids to save bandwidth?\" is\n> > by far the most common suggestion I hear (including offline feedback).\n> > Here's a full explanation:\n> > \n> > BIP 152 shortens transaction hashes (32 bytes) to shortids (6 bytes)\n> > to save a significant amount of network bandwidth, which is extremely\n> > important in block relay. However, this comes at the expense of\n> > computational complexity. There is no way to directly calculate a transaction\n> > hash from a shortid; upon receipt of a compact block, a node is expected to\n> > calculate the shortids of every unconfirmed transaction it knows about to\n> > find the matches (BIP 152: 1, Bitcoin Core: [2]). This is expensive but\n> > appropriate for block relay, since the block must have a valid Proof of Work\n> > and new blocks only come every ~10 minutes. On the other hand, if we\n> > require nodes to calculate shortids for every transaction in their mempools\n> > every time they receive a package, we are creating a DoS vector.\n> > Unconfirmed transactions don't need PoW and, to have a live transaction\n> > relay network, we should expect nodes to handle transactions at a high-ish\n> > rate (i.e. at least 1000s of times more transactions than blocks). We can't pre-\n> > calculate or cache shortids for mempool transactions, since the SipHash key\n> > depends on the block hash and a per-connection salt.\n> > \n> > Additionally, shortid calculation is not designed to prevent intentional\n> > individual collisions. If we were to use these shortids to deduplicate\n> > transactions we've supposedly already seen, we may have a censorship\n> > vector. Again, these tradeoffs make sense for compact block relay (see\n> > shortid section in BIP 152 [3]), but not package relay.\n> > \n> > TLDR: DoSy if we calculate shortids on every package and censorship\n> > vector if we use shortids for deduplication.\n> > \n> > > Given this message there is no reason\n> > > to send a (potentially bogus) fee rate with every package. It can\n> > > only be\n> > > validated by obtaining the full set of txs, and the only recourse is\n> > > dropping (etc.) the peer, as is the case with single txs.\n> > \n> > Yeah, I agree with this. Combined with the previous discussion with\n> > aj (i.e. we can't accurately communicate the incentive compatibility of a\n> > package without sending the full graph, and this whole dance is to avoid\n> > downloading a few low-fee transactions in uncommon edge cases), I've\n> > realized I should remove the fee + weight information from pkginfo. Yay for\n> > less complexity!\n> > \n> > Also, this might be pedantic, but I said something incorrect earlier\n> > and would like to correct myself:\n> > \n> > > > In theory, yes, but maybe it was announced earlier (while our\n> > > > node was down?) or had dropped from our mempool or similar, either way\n> > > > we don't have those txs yet.\n> > \n> > I said \"It's fine if they have Erlay, since a sender would know in\n> > advance that B is missing and announce it as a package.\" But this isn't true\n> > since we're only using reconciliation in place of flooding to announce\n> > transactions as they arrive, not for rebroadcast, and we're not doing full\n> > mempool set reconciliation. In any case, making sure a node receives the\n> > transactions announced when it was offline is not something we guarantee,\n> > not an intended use case for package relay, and not worsened by this.\n> > \n> > Thanks for your feedback!\n> > \n> > Best,\n> > \n> > Gloria\n> > \n> > 0152.mediawiki#cmpctblock\n> > [2]:\n> > https://github.com/bitcoin/bitcoin/blob/master/src/blockencodings.cpp#L49\n> > [3]: https://github.com/bitcoin/bips/blob/master/bip-\n> > 0152.mediawiki#short-transaction-id-calculation\n> > \n> > On Thu, May 26, 2022 at 3:59 AM <eric at voskuil.org\n> > mailto:eric at voskuil.org > wrote:\n> > \n> > Given that packages have no header, the package requires\n> > identity in a\n> > BIP152 scheme. For example 'header' and 'blockhash' fields\n> > can be replaced\n> > with a Merkle root (e.g. \"identity\" field) for the package,\n> > uniquely\n> > identifying the partially-ordered set of txs. And use of\n> > 'getdata' (to\n> > obtain a package by hash) can be eliminated (not a use case).\n> > \n> > e\n> > \n> > > -----Original Message-----\n> > > From: eric at voskuil.org mailto:eric at voskuil.org\n> > <eric at voskuil.org mailto:eric at voskuil.org >\n> > > Sent: Wednesday, May 25, 2022 1:52 PM\n> > > To: 'Anthony Towns' <aj at erisian.com.au\n> > mailto:aj at erisian.com.au >; 'Bitcoin Protocol Discussion'\n> > > <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > dev at lists.linuxfoundation.org> >; 'Gloria Zhao'\n> > > <gloriajzhao at gmail.com mailto:gloriajzhao at gmail.com >\n> > > Subject: RE: [bitcoin-dev] Package Relay Proposal\n> > >\n> > > > From: bitcoin-dev <bitcoin-dev-\n> > bounces at lists.linuxfoundation.org <mailto:bitcoin-dev-\n> > bounces at lists.linuxfoundation.org> > On\n> > > Behalf\n> > > > Of Anthony Towns via bitcoin-dev\n> > > > Sent: Wednesday, May 25, 2022 11:56 AM\n> > >\n> > > > So the other thing is what happens if the peer\n> > announcing packages to us\n> > > is\n> > > > dishonest?\n> > > >\n> > > > They announce pkg X, say X has parents A B C and the fee\n> > rate is\n> > garbage.\n> > > But\n> > > > actually X has parent D and the fee rate is excellent. Do\n> > we request the\n> > > > package from another peer, or every peer, to double\n> > check? Otherwise\n> > > we're\n> > > > allowing the first peer we ask about a package to censor\n> > that tx from\n> > us?\n> > > >\n> > > > I think the fix for that is just to provide the fee and weight\n> > when\n> > > announcing\n> > > > the package rather than only being asked for its info?\n> > Then if one peer\n> > > makes\n> > > > it sound like a good deal you ask for the parent txids from\n> > them,\n> > dedupe,\n> > > > request, and verify they were honest about the parents.\n> > >\n> > > Single tx broadcasts do not carry an advertised fee rate,\n> > however the'\n> > > feefilter' message (BIP133) provides this distinction. This\n> > should be\n> > > interpreted as applicable to packages. Given this message\n> > there is no\n> > reason\n> > > to send a (potentially bogus) fee rate with every package. It\n> > can only be\n> > > validated by obtaining the full set of txs, and the only\n> > recourse is\n> > > dropping (etc.) the peer, as is the case with single txs.\n> > Relying on the\n> > > existing message is simpler, more consistent, and more\n> > efficient.\n> > >\n> > > > >> Is it plausible to add the graph in?\n> > > >\n> > > > Likewise, I think you'd have to have the graph info from\n> > many nodes if\n> > > you're\n> > > > going to make decisions based on it and don't want\n> > hostile peers to be\n> > > able to\n> > > > trick you into ignoring txs.\n> > > >\n> > > > Other idea: what if you encode the parent txs as a short\n> > hash of the\n> > wtxid\n> > > > (something like bip152 short ids? perhaps seeded per\n> > peer so collisions\n> > > will\n> > > > be different per peer?) and include that in the inv\n> > announcement? Would\n> > > > that work to avoid a round trip almost all of the time,\n> > while still\n> > giving\n> > > you\n> > > > enough info to save bw by deduping parents?\n> > >\n> > > As I suggested earlier, a package is fundamentally a\n> > compact block (or\n> > > block) announcement without the header. Compact block\n> > (BIP152)\n> > > announcement\n> > > is already well-defined and widely implemented. A node\n> > should never be\n> > > required to retain an orphan, and BIP152 ensures this is not\n> > required.\n> > >\n> > > Once a validated set of txs within the package has been\n> > obtained with\n> > > sufficient fee, a fee-optimal node would accept the largest\n> > subgraph of\n> > the\n> > > package that conforms to fee constraints and drop any\n> > peer that provides a\n> > > package for which the full graph does not.\n> > >\n> > > Let us not reinvent the wheel and/or introduce accidental\n> > complexity. I\n> > see\n> > > no reason why packaging is not simply BIP152 without the\n> > 'header' field,\n> > an\n> > > updated protocol version, and the following sort of changes\n> > to names:\n> > >\n> > > sendpkg\n> > > MSG_CMPCT_PKG\n> > > cmpctpkg\n> > > getpkgtxn\n> > > pkgtxn\n> > >\n> > > > > For a maximum 25 transactions,\n> > > > >2324/2 = 276, seems like 36 bytes for a child-with-\n> > parents package.\n> > > >\n> > > > If you're doing short ids that's maybe 254B=100B\n> > already, then the\n> > above\n> > > is\n> > > > up to 36% overhead, I guess. Might be worth thinking\n> > more about, but\n> > > maybe\n> > > > more interesting with ancestors than just parents.\n> > > >\n> > > > >Also side note, since there are no size/count params,\n> > >\n> > > Size is restricted in the same manner as block and\n> > transaction broadcasts,\n> > > by consensus. If the fee rate is sufficient there would be no\n> > reason to\n> > > preclude any valid size up to what can be mined in one\n> > block (packaging\n> > > across blocks is not economically rational under the\n> > assumption that one\n> > > miner cannot expect to mine multiple blocks in a row).\n> > Count is\n> > incorporated\n> > > into BIP152 as 'shortids_length'.\n> > >\n> > > > > wondering if we\n> > > > >should just have \"version\" in \"sendpackages\" be a bit\n> > field instead of\n> > > > >sending a message for each version. 32 versions should\n> > be enough right?\n> > >\n> > > Adding versioning to individual protocols is just a reflection\n> > of the\n> > > insufficiency of the initial protocol versioning design, and\n> > that of the\n> > > various ad-hoc changes to it (including yet another\n> > approach in this\n> > > proposal) that have been introduced to compensate for it,\n> > though I'll\n> > > address this in an independent post at some point.\n> > >\n> > > Best,\n> > > e\n> > >\n> > > > Maybe but a couple of messages per connection doesn't\n> > really seem worth\n> > > > arguing about?\n> > > >\n> > > > Cheers,\n> > > > aj\n> > > >\n> > > >\n> > > > --\n> > > > Sent from my phone.\n> > > >\n> > _______________________________________________\n> > > > bitcoin-dev mailing list\n> > > > bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > dev at lists.linuxfoundation.org>\n> > > >\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > dev at lists.linuxfoundation.org>\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "eric at voskuil.org",
                "date": "2022-09-26T21:19:39",
                "message_text_only": "> Hi Eric,\n> \n> \n> This email wasn't answered by anyone on mailing list however I did some\n> research about packages yesterday including this email and below are my\n> observations, questions etc.\n\nHello, thanks for the reply.\n\n> > The sole objective, as expressed in the OP proposal, is to:\n> >\n> > \"Propagate transactions that are incentive-compatible to mine, even if they\n> don't meet minimum feerate alone.\"\n> \n> According to [bitcoinops][1]: Without package relay, it\u2019s not possible to\n> effectively CPFP fee bump a transaction that\u2019s below the minimum feerate\n> nodes accept.\n\nYes, the problem statement is not in question, just the mechanism of resolution. The problem of stuck txs arises from minimum fee rate policy, which is a necessary DOS guard.\n\nA secondary issue is that of orphan relay. As a node must allow receipt of orphans, it has no means to differentiate a flood of unconfirmable txs from those that are confirmable.\n\n> Matt Corallo's thoughts in a bitcoin core [issue][2]:\n> \n> \"Matt Corallo recently wrote about an example on the bitcoin-dev mailing list\n> involving lightning transactions, where pre-signed transactions might be\n> broadcast to the blockchain long after they were generated, and thus not\n> have been created with a fee that is sufficient to be confirmed quickly (or\n> even be accepted to node mempools). In such situations, channel\n> participants may need to use chained transactions (CPFP) in order to increase\n> the confirmation speed of such transactions, and that implies we may need\n> to introduce a mechanism for those parent transactions to be relayed along\n> with their higher feerate children, even if the parent transaction would be\n> rejected by itself.\"\n\nWhile this is a valid scenario, the problems directly affect Bitcoin. Those problems propagate to layers, but are not unique to layering.\n\n> 1)Is it possible to have multiple pre-signed transactions with different fee\n> rates in a range? Example: PSBT1: 5 sat/vbyte, PSBT2: 10 sat/vbyte, PSBT3: 20\n> sat/vbyte and PSBT4: 100 sat/vbyte\n\nIf by \"range\" you mean a connected tx subgraph, I don't see why not. But note that nodes only operate over signed txs. PSBT is a wallet workflow.\n\n> 2)How would covenants affect this problem?\n\nThere are a good number of covenant proposals, though I assume they are all implemented within script. If a tx is confirmable and satisfies fee rate (for DOS protection), it is relayable. Covenants affect confirmability and should not have any unique impact on relay.\n\n> 3)How often does it happen that a pre-signed tx gets rejected by nodes\n> because it did not meet the minimum fee rate? Is it predictable and could be\n> managed in a different way?\n\nAlways. Only signed transactions are accepted. But assuming you are referring to a transaction that has been produced by a pre-signing workflow, I'm not sure how this would be distinct from any other tx.\n\n> After reading several links related to packages and bitcoin core pull requests,\n> I found it anti-bitcoin to introduce so much complexity because its not\n> possible to CPFP fee bump a tx below minimum fee rate.\n\nI'm not sure I follow this, maybe you could reword it. But it seems that you are saying that CPFP fee-bumping is a problem scenario and the complexity of the proposed solutions are not justified by such scenarios.\n\nI would say that the problem is real, and that the least complex option is generally preferred. There are always tradeoffs, and balancing these is part of protocol development. But as a rule, complexity within a protocol (communication) is to be avoided where possible.\n\n> > Furthermore any tx that is \"stuck\" can be freed by simply sending another\n> tx. The nodes at which the tx has become stuck will just package it up and\n> relay it to peers. In other words, there is no impact on wallet implementation\n> apart from raising the aggregate fee using a descendant transaction.\n> \n> It is easy to send another tx if there is only one user involved however\n> packages are trying to fix issues in which multiple users and transaction pre-\n> signed between them are involved. So, it will be difficult to coordinate and\n> create new pre-signed transactions in some cases although it is possible for\n> some use cases.\n\nGiven that nodes do not deal in presigned txs, this coordination difficulty could not be increased in any scenario.\n\nA node produces sets of txs (\"packages\") dynamically to satisfy its peer's feerate. When a wallet broadcasts a tx/package to a node, it is operating as a peer on the p2p network. The wallet simply implements the same dynamic packaging algorithm as any peer - because it is a peer. \n\n> > This is barely a protocol change - it's primarily implementation. All that\n> should be required is an additional INV element type, such as\n> MSG_TX_PACKAGE.\n> \n> > * All elements of MSG_TX_PACKAGE in one INV message MUST to be of\n> the same package.\n> > * A package MUST must define a set that can be mined into one block\n> (size/sigops constraint).\n> > * A package SHOULD not contain confirmed txs (a race may cause this).\n> > * A package MUST minimally satisfy peer.feerate.\n> > * A partial tx order, as in the manner of the block.txs ordering, MUST be\n> imposed.\n> > * A node SHOULD drop a peer that sends a package (or tx) below\n> node.feerate.\n> > * A node MAY drop a peer that sends a non-minimal package according to\n> node.feerate.\n> \n> This makes sense particularly if multiple node implementations are used in\n> future.\n\nThere are many node implementations used presently. And of course these are protocol proposals, which presumes more than one implementation.\n\n> My other questions:\n> \n> a)If a package has tx1, tx2, tx3, tx4 and tx5 and miner just include tx1 and tx2\n> in the block, how does this affect the projects considered for packages\n> proposal?\n\nI will leave that to authors of such proposals to answer. However in what I have proposed it just means tx3/4/5 get considered for subsequent block inclusion to the extent that fee rate policy is satisfied.\n\nOne of the several problems with static construction of packages is that they can still get stuck by fee rate policy. This is just kicking the can down the road while complicating the protocol.\n\n> b)How does changing the order of txs in a package affect these transactions?\n\nThere is no impact. I proposed the partial ordering to facilitate fail fast.\n\nThe partial ordering in block txs is unnecessary (given the PoW DOS guard). This is a consequence of the order imposed by Satoshi's implementation and only serves to slow validation (order constrains concurrency).\n\n> c)Do packages introduce more attack vectors in bitcoin for front running or\n> MEV? MEV in bitcoin currently only affects the projects that are considered\n> in packages proposal.\n\nI don't consider this relevant to any protocol considerations. Miners should always be expected to select the most optimal set of txs available in the time available to do so.\n\n> d)What if the package contains a transactions with sanctioned address?\n\nOne can consider this a policy, much like fee rate. Any policy that is applied to transactions and not known to its peers will result in the node receiving orphans. As such the node either must allow orphans or drop peers sending orphans under the assumption that the peer is expected to have implemented the same policy.\n\n> e)Why would miners use packages if the existing scenario in terms of fees\n> per block is beneficial for them?\n\nThe presumption is that the miner is only ever seeing txs that satisfy its fee rate policy, so this is just more opportunity.\n\nI'd add that the problem of \"pinning\" is related, but exacerbated by opaque policy (internal to certain implementations). Any node that ejects txs from its pool of valid but unconfirmed txs that satisfy fee rate policy is going to see orphans and going to cause txs to get stuck. This is one of the many problems with placing an arbitrary bound on the size of this pool.\n\nA subset of this problem is RBF policy. It is nice to see some movement toward generalizing RBF. The term is really a misnomer. Conflicting txs and subgraphs of txs are only problematic in the case of DOS, which is also resolved through advertised fee policy. Any node that imposes policy beyond this will also see orphans and cause txs to get stuck.\n\nThe scenario and therefore complexity consequences of an implementation-specific memory-constrained tx pool are becoming increasingly apparent. These are implementation issues, not protocol issues. This can be observed in a recent thread: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-September/020937.html\n\nOver time we are likely to see that the only policies that remain in widespread application are those that are necessary for DOS protection (fee rate), as other restrictions are not economically rational and cannot be enforced. We've seen recent debate regarding dust policy, and op_return policy. \"non-standard\" txs are perfectly valid but get stuck very easily. I'll reiterate, any policy beyond what is published via the protocol will cause the above problems.\n\ne\n\n> /dev/fd0\n> \n> [1]: https://bitcoinops.org/en/topics/package-relay/\n> [2]: https://github.com/bitcoin/bitcoin/issues/14895\n> \n> Sent with Proton Mail secure email.\n> \n> ------- Original Message -------\n> On Thursday, June 9th, 2022 at 4:13 AM, Eric Voskuil via bitcoin-dev <bitcoin-\n> dev at lists.linuxfoundation.org> wrote:\n> \n> \n> > Hi Suhas/Gloria,\n> >\n> > Good questions. I've started a new thread because it became something\n> else...\n> >\n> > Various ideas about packaging seem to be focused on the idea of an atomic\n> message that is gossiped around the network like a transaction or block.\n> From my perspective that seems to create a set of problems without good\n> solutions, and it is not a proper analogy to those atomic structures. It may be\n> worth taking the time to step back and take a close look at the underlying\n> objective.\n> >\n> > The sole objective, as expressed in the OP proposal, is to:\n> >\n> > \"Propagate transactions that are incentive-compatible to mine, even if they\n> don't meet minimum feerate alone.\"\n> >\n> > Effectively producing this outcome with an atomic packaging approach\n> while at the same time maintaining network invariants seems unlikely, if not\n> impossible.\n> >\n> > Fees:\n> >\n> > A node knows what fee rate a peer will accept, and announces individual\n> txs that satisfy peer.feerate. Similarly a node knows its own feerate, and\n> SHOULD drop any peer that announces txs that do not satisfy node.feerate.\n> >\n> > Orphans:\n> >\n> > A node MAY drop a peer that announces txs that the node sees as orphans\n> against its DAG. It SHOULD drop the orphan tx and MAY request missing\n> ancestors. Presumably after some amount of time connected to peer, node\n> does not expect to see any more orphans from that peer, so these choices\n> could evolve with the channel. However, the design that can only consider\n> each tx in isolation will continue to cause orphan announcements on the\n> channel. A below peer.feerate tx does not get announced to peer, and later\n> a descendant high peer.feerate does get announced to the peer - as an\n> orphan.\n> >\n> > BIP133 (feefilter):\n> >\n> > \"There could be a small number of edge cases where a node's mempool\n> min fee is actually less than the filter value a peer is aware of and\n> transactions with fee rates between these values will now be newly\n> inhibited.\"\n> >\n> > https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki\n> >\n> > Whether the problem is \"small\" or not depends on the disparity between\n> node fee rates, which is not a matter of protocol. This is an existing problem\n> that can and should be dealt with in packaging, as part of the above\n> objective.\n> >\n> > Packaged Transaction Relay:\n> >\n> > One might instead think of packaging as a per-connection function,\n> operating over its transaction (input->output) DAG and the feerate of its\n> own node and that of the peer. Logically a \"package\" is nothing more than a\n> set of transactions (optimized by announcement). Only a node can\n> effectively determine the packaging required by each of its peers, since only\n> the node is aware of peer.feerate.\n> >\n> >\n> > The only way to avoid dead-ending packages (including individual\n> transactions, as is the objective) is for a node to package txs for each peer.\n> The origination of any package is then just a wallet peer doing what a node\n> does - packaging transactions that satisfy peer.feerate (i.e. that of its node).\n> >\n> > Current transaction relay (txB->txA):\n> >\n> > ===============================\n> > Node0\n> > txA.feerate > node.feerate, and not orphaned (accept txA)\n> >\n> > txA.feerate > peer1.feerate (announce txA to peer1)\n> >\n> > txA.feerate < peer2.feerate (do not announce txA to peer2)\n> > -----\n> > txB.feerate > node.feerate (accept txB)\n> >\n> > txB.feerate > peer1.feerate (announce txB to peer1)\n> >\n> > txB.feerate > peer2.feerate (announce txB to peer2)\n> >\n> >\n> > Node1\n> > Sees/accepts txA and txB.\n> >\n> > Node2\n> > Never sees txA, sees/rejects txB (as an orphan).\n> >\n> > Packaged transaction relay (txB->txA):\n> >\n> > ===============================\n> > Node0\n> > txA.feerate > node.feerate, and not orphaned (accept txA)\n> >\n> > txA.feerate > peer1.feerate (announce txA to peer1)\n> >\n> > txA.feerate < peer2.feerate (do not announce txA to peer2)\n> > -----\n> > txB.feerate > node1.feerate (accept txB)\n> >\n> > txB.feerate > peer1.feerate (announce txB to peer1)\n> >\n> > txB.feerate > peer2.feerate (do not announce txB to peer2) <== avoid\n> > predictable orphan\n> >\n> > txA.feerate + txB.feerate > peer2.feerate (announce pkg(A, B) to\n> > peer2) <= create minimal package\n> >\n> >\n> > Node1\n> > Sees/accepts txA and txB.\n> >\n> > Node2\n> > pkg(A, B) > node2.feerate (accept txA, txB)\n> >\n> > txA.feerate > peer3.feerate (announce txA to peer3)\n> >\n> > txB.feerate > peer3.feerate (announce txB to peer3)\n> >\n> >\n> > Sees/accepts pkg(A, B).\n> >\n> > Node3\n> > Sees/accepts txA and txB. <= avoided unnecessary packaging\n> >\n> > Summary:\n> >\n> > In this design, any node that receives an announcement for a pkg (or tx)\n> later determined to be less than node.feerate SHOULD drop the announcing\n> peer. Unlike with existing tx relay, a node can become \"current\" and\n> subsequently see few if any tx or pkg orphans, and MAY at some point\n> decide to drop any peer that announces one. Notice that packages are\n> created dynamically, and any package that doesn't need to be grouped gets\n> trimmed down to individual transactions. Furthermore any tx that is \"stuck\"\n> can be freed by simply sending another tx. The nodes at which the tx has\n> become stuck will just package it up and relay it to peers. In other words,\n> there is no impact on wallet implementation apart from raising the aggregate\n> fee using a descendant transaction.\n> >\n> > This is barely a protocol change - it's primarily implementation. All that\n> should be required is an additional INV element type, such as\n> MSG_TX_PACKAGE.\n> >\n> > Additional constraints:\n> >\n> > * All elements of MSG_TX_PACKAGE in one INV message MUST to be of\n> the same package.\n> > * A package MUST must define a set that can be mined into one block\n> (size/sigops constraint).\n> > * A package SHOULD not contain confirmed txs (a race may cause this).\n> > * A package MUST minimally satisfy peer.feerate.\n> > * A partial tx order, as in the manner of the block.txs ordering, MUST be\n> imposed.\n> > * A node SHOULD drop a peer that sends a package (or tx) below\n> node.feerate.\n> > * A node MAY drop a peer that sends a non-minimal package according to\n> node.feerate.\n> >\n> > The partial ordering of block.txs introduces an ordering constraint that\n> precludes full parallelism in validating input attachment. This is an\n> implementation artifact that made its way into consensus. However in the\n> case of packaging, the set of txs is not presumed to be valid under the proof\n> of work DoS guard. As such constraints should minimize the work/traffic\n> required to invalidate the message. The partial order constraint ensures that\n> the DAG can be built incrementally, dropping the attempt (and peer as\n> desired) as soon as the first orphan is discovered. As a result the network\n> traffic and work required is not materially different than with tx relay, with\n> two exceptions.\n> >\n> > These are the two central aspects of this approach (Avoiding Predictable\n> Orphans and Creating Minimal Packages). These are graph search algorithms,\n> some basic computer science. Minimality requires only that the package does\n> not introduce txs that are not necessary to reach the peer.feerate (as these\n> can always be packaged separately). It does not require that nodes all\n> generate the same packages. It does not require negotiation, package\n> identity, cryptography, or hashing. As a graph search it should be O(n) where\n> n is the unconfirmed ancestry of the package, but should typically be much\n> lower, if not a single step.\n> >\n> > Sufficiently-low-fee nodes will see only single txs. Moderate-fee\n> > nodes may cause partial breakup of packages. Sufficiently high fee\n> > nodes will cause peers (having received and completed the acceptance\n> > of a tx/pkg with pkg.feerate < peer.feerate) to navigate from each\n> > tx/package external input until reaching txs above peer.feerate, or\n> > confirmed (both of which the peer is presumed to already have). If the\n> > pkg.feerate is sufficiently high to connect all external inputs to the\n> > intervening txs, they are added to the package and it is announced to\n> > the high fee peer. Note that the individual tx.feerate > peer.feerate\n> > is insufficient to ensure that the peer should have the tx, as there\n> > may be ancestor txs that do not, and for which the tx was insufficient\n> > to cause them to be packaged. So a non-caching algorithm must be able\n> > to chase each package external input to a confirmed tx (or cache the\n> > unconfirmed ancestry fee rate at each tx). Note that fee rates are not\n> > directly additive, both size/\n> >\n> > weight and fee are required for summation (and aggregate sigops should\n> be considered).\n> >\n> > This makes no assumptions about current implementations. The design\n> would call for maintenance of a transaction (input->output) DAG with\n> tx.feerate on each tx. This could be the unconfirmed tx graph (i.e. \"memory\n> pool\") though it does not require maintenance of anything more than the\n> parameters necessary to confirm a set of validated txs within a block. It is\n> very reasonable to require this of any participating node. A simple version\n> negotiation can identify a package-accepting/sending nodes.\n> >\n> >\n> > I have thought about this for some time, but have not implemented either\n> the graph search, source code, or BIP. Just wrote this off the top of my head.\n> So I am sure there are some things I have incorrect or failed to consider. But I\n> think it's worth discussing it at this point.\n> >\n> > e\n> >\n> > > -----Original Message-----\n> > > From: bitcoin-dev bitcoin-dev-bounces at lists.linuxfoundation.org On\n> > > Behalf Of Suhas Daftuar via bitcoin-dev\n> > > Sent: Wednesday, June 8, 2022 8:59 AM\n> > > To: Bitcoin Protocol Discussion\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > Subject: Re: [bitcoin-dev] Package Relay Proposal\n> > >\n> > > Hi,\n> > >\n> > > Thanks again for your work on this!\n> > >\n> > > One question I have is about potential bandwidth waste in the case\n> > > of nodes running with different policy rules. Here's my\n> > > understanding of a scenario I think could happen:\n> > >\n> > > 1) Transaction A is both low-fee and non-standard to some nodes on\n> > > the network.\n> > > 2) Whenever a transaction T that spends A is relayed, new nodes will\n> > > send INV(PKGINFO1, T) to all package-relay peers.\n> > > 3) Nodes on the network that have implemented package relay, but do\n> > > not accept A, will send getdata(PKGINFO1, T) and learn all of T's\n> > > unconfirmed parents (~32 bytes * number of parents(T)).\n> > > 4) Such nodes will reject T. But because of transaction\n> > > malleability, and to avoid being blinded to a transaction\n> > > unnecessarily, these nodes will likely still send getdata(PKGINFO1,\n> > > T) to every node that announces T, in case someone has a transaction\n> > > that includes an alternate set of parent transactions that would pass\n> policy checks.\n> > >\n> > > Is that understanding correct? I think a good design goal would be\n> > > to not waste bandwidth in non-adversarial situations. In this case,\n> > > there would be bandwidth waste from downloading duplicate data from\n> > > all your peers, just because the announcement doesn't commit to the\n> > > set of parent wtxids that we'd get from the peer (and so we are\n> > > unable to determine that all our peers would be telling us the same thing,\n> just based on the announcement).\n> > >\n> > > Some ways to mitigate this might be to: (a) include a hash (maybe\n> > > even just a 20-byte hash -- is that enough security?) of the package\n> > > wtxids (in some canonical ordering) along with the wtxid of the\n> > > child in the initial announcement; (b) limit the use of v1 packages\n> > > to transactions with very few parents (I don't know if this is reasonable\n> for the use cases we have in mind).\n> > >\n> > > Another point I wanted to bring up is about the rules around v1\n> > > package validation generally, and the use of a blockhash in\n> > > transaction relay specifically. My first observation is that it\n> > > won't always be the case that a v1 package relay node will be able\n> > > to validate that a set of package transactions is fully sorted\n> > > topologically, because there may be (non-parent) ancestors that are\n> > > missing from the package and the best a peer can validate is\n> > > topology within the package -- this means that a peer can validly\n> > > (under this\n> > > BIP) relay transaction packages out of the true topological sort (if\n> > > all ancestors were included).\n> > >\n> > > This makes me wonder how useful this topological rule is. I suppose\n> > > there is some value in preventing completely broken implementations\n> > > from staying connected and so there is no harm in having the rule,\n> > > but perhaps it would be helpful to add that nodes SHOULD order\n> > > transactions based on topological sort in the complete transaction\n> > > graph, so that if missing-from-package ancestors are already known\n> > > by a peer (which is the expected case when using v1 package relay on\n> > > transactions that have more than one generation of unconfirmed\n> > > ancestor) then the remaining transactions are already properly ordered,\n> and this is helpful even if unenforceable in general.\n> > >\n> > > The other observation I wanted to make was that having transaction\n> > > relay gated on whether two nodes agree on chain tip seems like an\n> > > overly restrictive criteria. I think an important design principle\n> > > is that we want to minimize disruption from network splits -- if\n> > > there are competing blocks found in a small window of time, it's\n> > > likely that the utxo set is not materially different on the two\n> > > chains (assuming miners are selecting from roughly the same sets of\n> > > transactions when this happens, which is typical). Having\n> > > transaction relay bifurcate on the two network halves would seem to\n> > > exacerbate the difference between the two sides of the split --\n> > > users ought to be agnostic about how benign splits are resolved and\n> would likely want their transactions to relay across the whole network.\n> > >\n> > > Additionally, use of a chain tip might impose a larger burden than\n> > > is necessary on software that would seek to participate in\n> > > transaction relay without implementing headers sync/validation. I\n> > > don't know what software exists on the network, but I imagine there\n> > > are a lot of scripts out there for transaction submission to the\n> > > public p2p network, and in thinking about modifying such a script to\n> > > utilize package relay it seems like an unnecessary added burden to first\n> learn a node's tip before trying to relay a transaction.\n> > >\n> > > Could you explain again what the benefit of including the blockhash\n> > > is? It seems like it is just so that a node could prioritize\n> > > transaction relay from peers with the same chain tip to maximize the\n> > > likelihood of transaction acceptance, but in the common case this\n> > > seems like a pretty negligible concern, and in the case of a chain\n> > > fork that persists for many minutes it seems better to me that we\n> > > not partition the network into package-relay regimes and just risk a\n> > > little extra bandwidth in one direction or the other. If we solve\n> > > the problem I brought up at the beginning (of de-duplicating package\n> > > data across peers with a package-wtxid-commitment in the\n> > > announcement), I think this is just some wasted pkginfo bandwidth on\n> > > a single-link, and not across links (as we could cache validation failure for a\n> package-hash to avoid re-requesting duplicate pkginfo1 messages).\n> > >\n> > > Best,\n> > > Suhas\n> > >\n> > > On Tue, Jun 7, 2022 at 1:57 PM Gloria Zhao via bitcoin-dev <bitcoin-\n> > > dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > dev at lists.linuxfoundation.org> > wrote:\n> > >\n> > > Hi Eric, aj, all,\n> > >\n> > > Sorry for the delayed response. @aj I'm including some paraphrased\n> > > points from our offline discussion (thanks).\n> > >\n> > > > Other idea: what if you encode the parent txs as a short hash of\n> > > > the wtxid (something like bip152 short ids? perhaps seeded per\n> > > > peer so collisions will be different per peer?) and include that in the inv\n> announcement?\n> > > > Would that work to avoid a round trip almost all of the time,\n> > > > while still giving you enough info to save bw by deduping parents?\n> > >\n> > > > As I suggested earlier, a package is fundamentally a compact block\n> > > > (or\n> > > > block) announcement without the header. Compact block (BIP152)\n> > > > announcement is already well-defined and widely implemented...\n> > >\n> > > > Let us not reinvent the wheel and/or introduce accidental\n> > > > complexity. I see no reason why packaging is not simply BIP152\n> > > > without the 'header'\n> > > > field, an\n> > > > updated protocol version, and the following sort of changes to\n> > > > names\n> > >\n> > > Interestingly, \"why not use BIP 152 shortids to save bandwidth?\" is\n> > > by far the most common suggestion I hear (including offline feedback).\n> > > Here's a full explanation:\n> > >\n> > > BIP 152 shortens transaction hashes (32 bytes) to shortids (6 bytes)\n> > > to save a significant amount of network bandwidth, which is\n> > > extremely important in block relay. However, this comes at the\n> > > expense of computational complexity. There is no way to directly\n> > > calculate a transaction hash from a shortid; upon receipt of a\n> > > compact block, a node is expected to calculate the shortids of every\n> > > unconfirmed transaction it knows about to find the matches (BIP 152:\n> > > 1, Bitcoin Core: [2]). This is expensive but appropriate for block\n> > > relay, since the block must have a valid Proof of Work and new\n> > > blocks only come every ~10 minutes. On the other hand, if we require\n> > > nodes to calculate shortids for every transaction in their mempools every\n> time they receive a package, we are creating a DoS vector.\n> > > Unconfirmed transactions don't need PoW and, to have a live\n> > > transaction relay network, we should expect nodes to handle\n> > > transactions at a high-ish rate (i.e. at least 1000s of times more\n> > > transactions than blocks). We can't pre- calculate or cache shortids\n> > > for mempool transactions, since the SipHash key depends on the block\n> hash and a per-connection salt.\n> > >\n> > > Additionally, shortid calculation is not designed to prevent\n> > > intentional individual collisions. If we were to use these shortids\n> > > to deduplicate transactions we've supposedly already seen, we may\n> > > have a censorship vector. Again, these tradeoffs make sense for\n> > > compact block relay (see shortid section in BIP 152 [3]), but not package\n> relay.\n> > >\n> > > TLDR: DoSy if we calculate shortids on every package and censorship\n> > > vector if we use shortids for deduplication.\n> > >\n> > > > Given this message there is no reason to send a (potentially\n> > > > bogus) fee rate with every package. It can only be validated by\n> > > > obtaining the full set of txs, and the only recourse is dropping\n> > > > (etc.) the peer, as is the case with single txs.\n> > >\n> > > Yeah, I agree with this. Combined with the previous discussion with\n> > > aj (i.e. we can't accurately communicate the incentive compatibility\n> > > of a package without sending the full graph, and this whole dance is\n> > > to avoid downloading a few low-fee transactions in uncommon edge\n> > > cases), I've realized I should remove the fee + weight information\n> > > from pkginfo. Yay for less complexity!\n> > >\n> > > Also, this might be pedantic, but I said something incorrect earlier\n> > > and would like to correct myself:\n> > >\n> > > > > In theory, yes, but maybe it was announced earlier (while our\n> > > > > node was down?) or had dropped from our mempool or similar,\n> > > > > either way we don't have those txs yet.\n> > >\n> > > I said \"It's fine if they have Erlay, since a sender would know in\n> > > advance that B is missing and announce it as a package.\" But this\n> > > isn't true since we're only using reconciliation in place of\n> > > flooding to announce transactions as they arrive, not for\n> > > rebroadcast, and we're not doing full mempool set reconciliation. In\n> > > any case, making sure a node receives the transactions announced\n> > > when it was offline is not something we guarantee, not an intended use\n> case for package relay, and not worsened by this.\n> > >\n> > > Thanks for your feedback!\n> > >\n> > > Best,\n> > >\n> > > Gloria\n> > >\n> > > 0152.mediawiki#cmpctblock\n> > > [2]:\n> > > https://github.com/bitcoin/bitcoin/blob/master/src/blockencodings.cp\n> > > p#L49\n> > > [3]: https://github.com/bitcoin/bips/blob/master/bip-\n> > > 0152.mediawiki#short-transaction-id-calculation\n> > >\n> > > On Thu, May 26, 2022 at 3:59 AM <eric at voskuil.org\n> > > mailto:eric at voskuil.org > wrote:\n> > >\n> > > Given that packages have no header, the package requires identity in\n> > > a\n> > > BIP152 scheme. For example 'header' and 'blockhash' fields can be\n> > > replaced with a Merkle root (e.g. \"identity\" field) for the package,\n> > > uniquely identifying the partially-ordered set of txs. And use of\n> > > 'getdata' (to obtain a package by hash) can be eliminated (not a use\n> > > case).\n> > >\n> > > e\n> > >\n> > > > -----Original Message-----\n> > > > From: eric at voskuil.org mailto:eric at voskuil.org\n> > > <eric at voskuil.org mailto:eric at voskuil.org >\n> > > > Sent: Wednesday, May 25, 2022 1:52 PM\n> > > > To: 'Anthony Towns' <aj at erisian.com.au\n> > > mailto:aj at erisian.com.au >; 'Bitcoin Protocol Discussion'\n> > > > <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > dev at lists.linuxfoundation.org> >; 'Gloria Zhao'\n> > > > <gloriajzhao at gmail.com mailto:gloriajzhao at gmail.com >\n> > > > Subject: RE: [bitcoin-dev] Package Relay Proposal\n> > > >\n> > > > > From: bitcoin-dev <bitcoin-dev-\n> > > bounces at lists.linuxfoundation.org <mailto:bitcoin-dev-\n> > > bounces at lists.linuxfoundation.org> > On\n> > > > Behalf\n> > > > > Of Anthony Towns via bitcoin-dev\n> > > > > Sent: Wednesday, May 25, 2022 11:56 AM\n> > > >\n> > > > > So the other thing is what happens if the peer\n> > > announcing packages to us\n> > > > is\n> > > > > dishonest?\n> > > > >\n> > > > > They announce pkg X, say X has parents A B C and the fee\n> > > rate is\n> > > garbage.\n> > > > But\n> > > > > actually X has parent D and the fee rate is excellent. Do\n> > > we request the\n> > > > > package from another peer, or every peer, to double\n> > > check? Otherwise\n> > > > we're\n> > > > > allowing the first peer we ask about a package to censor\n> > > that tx from\n> > > us?\n> > > > >\n> > > > > I think the fix for that is just to provide the fee and weight\n> > > when\n> > > > announcing\n> > > > > the package rather than only being asked for its info?\n> > > Then if one peer\n> > > > makes\n> > > > > it sound like a good deal you ask for the parent txids from\n> > > them,\n> > > dedupe,\n> > > > > request, and verify they were honest about the parents.\n> > > >\n> > > > Single tx broadcasts do not carry an advertised fee rate,\n> > > however the'\n> > > > feefilter' message (BIP133) provides this distinction. This\n> > > should be\n> > > > interpreted as applicable to packages. Given this message\n> > > there is no\n> > > reason\n> > > > to send a (potentially bogus) fee rate with every package. It\n> > > can only be\n> > > > validated by obtaining the full set of txs, and the only\n> > > recourse is\n> > > > dropping (etc.) the peer, as is the case with single txs.\n> > > Relying on the\n> > > > existing message is simpler, more consistent, and more\n> > > efficient.\n> > > >\n> > > > > >> Is it plausible to add the graph in?\n> > > > >\n> > > > > Likewise, I think you'd have to have the graph info from\n> > > many nodes if\n> > > > you're\n> > > > > going to make decisions based on it and don't want\n> > > hostile peers to be\n> > > > able to\n> > > > > trick you into ignoring txs.\n> > > > >\n> > > > > Other idea: what if you encode the parent txs as a short\n> > > hash of the\n> > > wtxid\n> > > > > (something like bip152 short ids? perhaps seeded per\n> > > peer so collisions\n> > > > will\n> > > > > be different per peer?) and include that in the inv\n> > > announcement? Would\n> > > > > that work to avoid a round trip almost all of the time,\n> > > while still\n> > > giving\n> > > > you\n> > > > > enough info to save bw by deduping parents?\n> > > >\n> > > > As I suggested earlier, a package is fundamentally a\n> > > compact block (or\n> > > > block) announcement without the header. Compact block\n> > > (BIP152)\n> > > > announcement\n> > > > is already well-defined and widely implemented. A node\n> > > should never be\n> > > > required to retain an orphan, and BIP152 ensures this is not\n> > > required.\n> > > >\n> > > > Once a validated set of txs within the package has been\n> > > obtained with\n> > > > sufficient fee, a fee-optimal node would accept the largest\n> > > subgraph of\n> > > the\n> > > > package that conforms to fee constraints and drop any\n> > > peer that provides a\n> > > > package for which the full graph does not.\n> > > >\n> > > > Let us not reinvent the wheel and/or introduce accidental\n> > > complexity. I\n> > > see\n> > > > no reason why packaging is not simply BIP152 without the\n> > > 'header' field,\n> > > an\n> > > > updated protocol version, and the following sort of changes\n> > > to names:\n> > > >\n> > > > sendpkg\n> > > > MSG_CMPCT_PKG\n> > > > cmpctpkg\n> > > > getpkgtxn\n> > > > pkgtxn\n> > > >\n> > > > > > For a maximum 25 transactions,\n> > > > > >2324/2 = 276, seems like 36 bytes for a child-with-\n> > > parents package.\n> > > > >\n> > > > > If you're doing short ids that's maybe 254B=100B\n> > > already, then the\n> > > above\n> > > > is\n> > > > > up to 36% overhead, I guess. Might be worth thinking\n> > > more about, but\n> > > > maybe\n> > > > > more interesting with ancestors than just parents.\n> > > > >\n> > > > > >Also side note, since there are no size/count params,\n> > > >\n> > > > Size is restricted in the same manner as block and\n> > > transaction broadcasts,\n> > > > by consensus. If the fee rate is sufficient there would be no\n> > > reason to\n> > > > preclude any valid size up to what can be mined in one\n> > > block (packaging\n> > > > across blocks is not economically rational under the\n> > > assumption that one\n> > > > miner cannot expect to mine multiple blocks in a row).\n> > > Count is\n> > > incorporated\n> > > > into BIP152 as 'shortids_length'.\n> > > >\n> > > > > > wondering if we\n> > > > > >should just have \"version\" in \"sendpackages\" be a bit\n> > > field instead of\n> > > > > >sending a message for each version. 32 versions should\n> > > be enough right?\n> > > >\n> > > > Adding versioning to individual protocols is just a reflection\n> > > of the\n> > > > insufficiency of the initial protocol versioning design, and\n> > > that of the\n> > > > various ad-hoc changes to it (including yet another\n> > > approach in this\n> > > > proposal) that have been introduced to compensate for it,\n> > > though I'll\n> > > > address this in an independent post at some point.\n> > > >\n> > > > Best,\n> > > > e\n> > > >\n> > > > > Maybe but a couple of messages per connection doesn't\n> > > really seem worth\n> > > > > arguing about?\n> > > > >\n> > > > > Cheers,\n> > > > > aj\n> > > > >\n> > > > >\n> > > > > --\n> > > > > Sent from my phone.\n> > > > >\n> > > _______________________________________________\n> > > > > bitcoin-dev mailing list\n> > > > > bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > dev at lists.linuxfoundation.org>\n> > > > >\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > >\n> > > _______________________________________________\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > dev at lists.linuxfoundation.org>\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "alicexbt",
                "date": "2022-09-27T09:29:19",
                "message_text_only": "Hi Eric,\n\n\n> If by \"range\" you mean a connected tx subgraph, I don't see why not. But note that nodes only operate over signed txs. PSBT is a wallet workflow.\n\nMatt Corallo mentioned that pre-signed transactions created with low fee rate become an issue when they are broadcasted after a long time and there is a high demand for block space at that moment.\n\nExample: \n\nBob created PSBT1 in a multi party contract with fee rate 5 sat/vbyte however its taking hours/days to confirm the transaction with such low fee rate.\n\nCarol created PSBT1 (5 sat/vbyte), PSBT2 (10 sat/vbyte) and PSBT3 (20 sat/vbyte) for spending same inputs. She broadcasted PSBT3 which got confirmed in a few minutes. \n\n\n> Always. Only signed transactions are accepted. But assuming you are referring to a transaction that has been produced by a pre-signing workflow, I'm not sure how this would be distinct from any other tx.\n\n\n`minfeefilter` for all peers of my node was 0.00001000 at the time of writing this email. I am assuming nobody creates pre-signed transaction with fee rate below 1 sat/vbyte. How often does it happen that `minfeefilter` is above this default value?\n\n\n> I'm not sure I follow this, maybe you could reword it. But it seems that you are saying that CPFP fee-bumping is a problem scenario and the complexity of the proposed solutions are not justified by such scenarios.\n\n\nSorry that sentence was confusing. Yes complexity isn't justified for CPFP fee-bumping txs below minimum fee rate.\n\n\n> There are many node implementations used presently. And of course these are protocol proposals, which presumes more than one implementation.\n\n\nYes, a few implementations exist (knots, libbitcoin, btcd, bcoin etc.) however they aren't used by lot of nodes. Based on this [chart][1] 98% nodes use bitcoin core. Lot of bitcoin protocol proposals are influenced by bitcoin core contributors and things could be different if even 30% nodes used other implementations.\n\n\n> I don't consider this relevant to any protocol considerations. Miners should always be expected to select the most optimal set of txs available in the time available to do so.\n\n\nAgree, miners should be expected to select most optimal set of txs. However, according to one [comment][2] by Pieter Wuille, miners could affect the security of some bitcoin projects with MEV.\n\n\n> Over time we are likely to see that the only policies that remain in widespread application are those that are necessary for DOS protection (fee rate), as other restrictions are not economically rational and cannot be enforced. We've seen recent debate regarding dust policy, and op_return policy. \"non-standard\" txs are perfectly valid but get stuck very easily. I'll reiterate, any policy beyond what is published via the protocol will cause the above problems.\n\nI completely agree with this.\n\n\n[1]: https://luke.dashjr.org/programs/bitcoin/files/charts/software.html\n[2]: https://bitcoin.stackexchange.com/questions/107787/front-running-in-bitcoin#comment123441_107796\n\n\n/dev/fd0\n\nSent with Proton Mail secure email.\n\n------- Original Message -------\nOn Tuesday, September 27th, 2022 at 2:49 AM, <eric at voskuil.org> wrote:\n\n\n> > Hi Eric,\n> > \n> > This email wasn't answered by anyone on mailing list however I did some\n> > research about packages yesterday including this email and below are my\n> > observations, questions etc.\n> \n> \n> Hello, thanks for the reply.\n> \n> > > The sole objective, as expressed in the OP proposal, is to:\n> > > \n> > > \"Propagate transactions that are incentive-compatible to mine, even if they\n> > > don't meet minimum feerate alone.\"\n> > \n> > According to bitcoinops: Without package relay, it\u2019s not possible to\n> > effectively CPFP fee bump a transaction that\u2019s below the minimum feerate\n> > nodes accept.\n> \n> \n> Yes, the problem statement is not in question, just the mechanism of resolution. The problem of stuck txs arises from minimum fee rate policy, which is a necessary DOS guard.\n> \n> A secondary issue is that of orphan relay. As a node must allow receipt of orphans, it has no means to differentiate a flood of unconfirmable txs from those that are confirmable.\n> \n> > Matt Corallo's thoughts in a bitcoin core issue:\n> > \n> > \"Matt Corallo recently wrote about an example on the bitcoin-dev mailing list\n> > involving lightning transactions, where pre-signed transactions might be\n> > broadcast to the blockchain long after they were generated, and thus not\n> > have been created with a fee that is sufficient to be confirmed quickly (or\n> > even be accepted to node mempools). In such situations, channel\n> > participants may need to use chained transactions (CPFP) in order to increase\n> > the confirmation speed of such transactions, and that implies we may need\n> > to introduce a mechanism for those parent transactions to be relayed along\n> > with their higher feerate children, even if the parent transaction would be\n> > rejected by itself.\"\n> \n> \n> While this is a valid scenario, the problems directly affect Bitcoin. Those problems propagate to layers, but are not unique to layering.\n> \n> > 1)Is it possible to have multiple pre-signed transactions with different fee\n> > rates in a range? Example: PSBT1: 5 sat/vbyte, PSBT2: 10 sat/vbyte, PSBT3: 20\n> > sat/vbyte and PSBT4: 100 sat/vbyte\n> \n> \n> If by \"range\" you mean a connected tx subgraph, I don't see why not. But note that nodes only operate over signed txs. PSBT is a wallet workflow.\n> \n> > 2)How would covenants affect this problem?\n> \n> \n> There are a good number of covenant proposals, though I assume they are all implemented within script. If a tx is confirmable and satisfies fee rate (for DOS protection), it is relayable. Covenants affect confirmability and should not have any unique impact on relay.\n> \n> > 3)How often does it happen that a pre-signed tx gets rejected by nodes\n> > because it did not meet the minimum fee rate? Is it predictable and could be\n> > managed in a different way?\n> \n> \n> Always. Only signed transactions are accepted. But assuming you are referring to a transaction that has been produced by a pre-signing workflow, I'm not sure how this would be distinct from any other tx.\n> \n> > After reading several links related to packages and bitcoin core pull requests,\n> > I found it anti-bitcoin to introduce so much complexity because its not\n> > possible to CPFP fee bump a tx below minimum fee rate.\n> \n> \n> I'm not sure I follow this, maybe you could reword it. But it seems that you are saying that CPFP fee-bumping is a problem scenario and the complexity of the proposed solutions are not justified by such scenarios.\n> \n> I would say that the problem is real, and that the least complex option is generally preferred. There are always tradeoffs, and balancing these is part of protocol development. But as a rule, complexity within a protocol (communication) is to be avoided where possible.\n> \n> > > Furthermore any tx that is \"stuck\" can be freed by simply sending another\n> > > tx. The nodes at which the tx has become stuck will just package it up and\n> > > relay it to peers. In other words, there is no impact on wallet implementation\n> > > apart from raising the aggregate fee using a descendant transaction.\n> > \n> > It is easy to send another tx if there is only one user involved however\n> > packages are trying to fix issues in which multiple users and transaction pre-\n> > signed between them are involved. So, it will be difficult to coordinate and\n> > create new pre-signed transactions in some cases although it is possible for\n> > some use cases.\n> \n> \n> Given that nodes do not deal in presigned txs, this coordination difficulty could not be increased in any scenario.\n> \n> A node produces sets of txs (\"packages\") dynamically to satisfy its peer's feerate. When a wallet broadcasts a tx/package to a node, it is operating as a peer on the p2p network. The wallet simply implements the same dynamic packaging algorithm as any peer - because it is a peer.\n> \n> > > This is barely a protocol change - it's primarily implementation. All that\n> > > should be required is an additional INV element type, such as\n> > > MSG_TX_PACKAGE.\n> > \n> > > * All elements of MSG_TX_PACKAGE in one INV message MUST to be of\n> > > the same package.\n> > > * A package MUST must define a set that can be mined into one block\n> > > (size/sigops constraint).\n> > > * A package SHOULD not contain confirmed txs (a race may cause this).\n> > > * A package MUST minimally satisfy peer.feerate.\n> > > * A partial tx order, as in the manner of the block.txs ordering, MUST be\n> > > imposed.\n> > > * A node SHOULD drop a peer that sends a package (or tx) below\n> > > node.feerate.\n> > > * A node MAY drop a peer that sends a non-minimal package according to\n> > > node.feerate.\n> > \n> > This makes sense particularly if multiple node implementations are used in\n> > future.\n> \n> \n> There are many node implementations used presently. And of course these are protocol proposals, which presumes more than one implementation.\n> \n> > My other questions:\n> > \n> > a)If a package has tx1, tx2, tx3, tx4 and tx5 and miner just include tx1 and tx2\n> > in the block, how does this affect the projects considered for packages\n> > proposal?\n> \n> \n> I will leave that to authors of such proposals to answer. However in what I have proposed it just means tx3/4/5 get considered for subsequent block inclusion to the extent that fee rate policy is satisfied.\n> \n> One of the several problems with static construction of packages is that they can still get stuck by fee rate policy. This is just kicking the can down the road while complicating the protocol.\n> \n> > b)How does changing the order of txs in a package affect these transactions?\n> \n> \n> There is no impact. I proposed the partial ordering to facilitate fail fast.\n> \n> The partial ordering in block txs is unnecessary (given the PoW DOS guard). This is a consequence of the order imposed by Satoshi's implementation and only serves to slow validation (order constrains concurrency).\n> \n> > c)Do packages introduce more attack vectors in bitcoin for front running or\n> > MEV? MEV in bitcoin currently only affects the projects that are considered\n> > in packages proposal.\n> \n> \n> I don't consider this relevant to any protocol considerations. Miners should always be expected to select the most optimal set of txs available in the time available to do so.\n> \n> > d)What if the package contains a transactions with sanctioned address?\n> \n> \n> One can consider this a policy, much like fee rate. Any policy that is applied to transactions and not known to its peers will result in the node receiving orphans. As such the node either must allow orphans or drop peers sending orphans under the assumption that the peer is expected to have implemented the same policy.\n> \n> > e)Why would miners use packages if the existing scenario in terms of fees\n> > per block is beneficial for them?\n> \n> \n> The presumption is that the miner is only ever seeing txs that satisfy its fee rate policy, so this is just more opportunity.\n> \n> I'd add that the problem of \"pinning\" is related, but exacerbated by opaque policy (internal to certain implementations). Any node that ejects txs from its pool of valid but unconfirmed txs that satisfy fee rate policy is going to see orphans and going to cause txs to get stuck. This is one of the many problems with placing an arbitrary bound on the size of this pool.\n> \n> A subset of this problem is RBF policy. It is nice to see some movement toward generalizing RBF. The term is really a misnomer. Conflicting txs and subgraphs of txs are only problematic in the case of DOS, which is also resolved through advertised fee policy. Any node that imposes policy beyond this will also see orphans and cause txs to get stuck.\n> \n> The scenario and therefore complexity consequences of an implementation-specific memory-constrained tx pool are becoming increasingly apparent. These are implementation issues, not protocol issues. This can be observed in a recent thread: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-September/020937.html\n> \n> Over time we are likely to see that the only policies that remain in widespread application are those that are necessary for DOS protection (fee rate), as other restrictions are not economically rational and cannot be enforced. We've seen recent debate regarding dust policy, and op_return policy. \"non-standard\" txs are perfectly valid but get stuck very easily. I'll reiterate, any policy beyond what is published via the protocol will cause the above problems.\n> \n> e\n> \n> > /dev/fd0\n> > \n> > Sent with Proton Mail secure email.\n> > \n> > ------- Original Message -------\n> > On Thursday, June 9th, 2022 at 4:13 AM, Eric Voskuil via bitcoin-dev <bitcoin-\n> > dev at lists.linuxfoundation.org> wrote:\n> > \n> > > Hi Suhas/Gloria,\n> > > \n> > > Good questions. I've started a new thread because it became something\n> > > else...\n> > > \n> > > Various ideas about packaging seem to be focused on the idea of an atomic\n> > > message that is gossiped around the network like a transaction or block.\n> > > From my perspective that seems to create a set of problems without good\n> > > solutions, and it is not a proper analogy to those atomic structures. It may be\n> > > worth taking the time to step back and take a close look at the underlying\n> > > objective.\n> > > \n> > > The sole objective, as expressed in the OP proposal, is to:\n> > > \n> > > \"Propagate transactions that are incentive-compatible to mine, even if they\n> > > don't meet minimum feerate alone.\"\n> > > \n> > > Effectively producing this outcome with an atomic packaging approach\n> > > while at the same time maintaining network invariants seems unlikely, if not\n> > > impossible.\n> > > \n> > > Fees:\n> > > \n> > > A node knows what fee rate a peer will accept, and announces individual\n> > > txs that satisfy peer.feerate. Similarly a node knows its own feerate, and\n> > > SHOULD drop any peer that announces txs that do not satisfy node.feerate.\n> > > \n> > > Orphans:\n> > > \n> > > A node MAY drop a peer that announces txs that the node sees as orphans\n> > > against its DAG. It SHOULD drop the orphan tx and MAY request missing\n> > > ancestors. Presumably after some amount of time connected to peer, node\n> > > does not expect to see any more orphans from that peer, so these choices\n> > > could evolve with the channel. However, the design that can only consider\n> > > each tx in isolation will continue to cause orphan announcements on the\n> > > channel. A below peer.feerate tx does not get announced to peer, and later\n> > > a descendant high peer.feerate does get announced to the peer - as an\n> > > orphan.\n> > > \n> > > BIP133 (feefilter):\n> > > \n> > > \"There could be a small number of edge cases where a node's mempool\n> > > min fee is actually less than the filter value a peer is aware of and\n> > > transactions with fee rates between these values will now be newly\n> > > inhibited.\"\n> > > \n> > > https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki\n> > > \n> > > Whether the problem is \"small\" or not depends on the disparity between\n> > > node fee rates, which is not a matter of protocol. This is an existing problem\n> > > that can and should be dealt with in packaging, as part of the above\n> > > objective.\n> > > \n> > > Packaged Transaction Relay:\n> > > \n> > > One might instead think of packaging as a per-connection function,\n> > > operating over its transaction (input->output) DAG and the feerate of its\n> > > own node and that of the peer. Logically a \"package\" is nothing more than a\n> > > set of transactions (optimized by announcement). Only a node can\n> > > effectively determine the packaging required by each of its peers, since only\n> > > the node is aware of peer.feerate.\n> > > \n> > > The only way to avoid dead-ending packages (including individual\n> > > transactions, as is the objective) is for a node to package txs for each peer.\n> > > The origination of any package is then just a wallet peer doing what a node\n> > > does - packaging transactions that satisfy peer.feerate (i.e. that of its node).\n> > > \n> > > Current transaction relay (txB->txA):\n> > > \n> > > ===============================\n> > > Node0\n> > > txA.feerate > node.feerate, and not orphaned (accept txA)\n> > > \n> > > txA.feerate > peer1.feerate (announce txA to peer1)\n> > > \n> > > txA.feerate < peer2.feerate (do not announce txA to peer2)\n> > > -----\n> > > txB.feerate > node.feerate (accept txB)\n> > > \n> > > txB.feerate > peer1.feerate (announce txB to peer1)\n> > > \n> > > txB.feerate > peer2.feerate (announce txB to peer2)\n> > > \n> > > Node1\n> > > Sees/accepts txA and txB.\n> > > \n> > > Node2\n> > > Never sees txA, sees/rejects txB (as an orphan).\n> > > \n> > > Packaged transaction relay (txB->txA):\n> > > \n> > > ===============================\n> > > Node0\n> > > txA.feerate > node.feerate, and not orphaned (accept txA)\n> > > \n> > > txA.feerate > peer1.feerate (announce txA to peer1)\n> > > \n> > > txA.feerate < peer2.feerate (do not announce txA to peer2)\n> > > -----\n> > > txB.feerate > node1.feerate (accept txB)\n> > > \n> > > txB.feerate > peer1.feerate (announce txB to peer1)\n> > > \n> > > txB.feerate > peer2.feerate (do not announce txB to peer2) <== avoid\n> > > predictable orphan\n> > > \n> > > txA.feerate + txB.feerate > peer2.feerate (announce pkg(A, B) to\n> > > peer2) <= create minimal package\n> > > \n> > > Node1\n> > > Sees/accepts txA and txB.\n> > > \n> > > Node2\n> > > pkg(A, B) > node2.feerate (accept txA, txB)\n> > > \n> > > txA.feerate > peer3.feerate (announce txA to peer3)\n> > > \n> > > txB.feerate > peer3.feerate (announce txB to peer3)\n> > > \n> > > Sees/accepts pkg(A, B).\n> > > \n> > > Node3\n> > > Sees/accepts txA and txB. <= avoided unnecessary packaging\n> > > \n> > > Summary:\n> > > \n> > > In this design, any node that receives an announcement for a pkg (or tx)\n> > > later determined to be less than node.feerate SHOULD drop the announcing\n> > > peer. Unlike with existing tx relay, a node can become \"current\" and\n> > > subsequently see few if any tx or pkg orphans, and MAY at some point\n> > > decide to drop any peer that announces one. Notice that packages are\n> > > created dynamically, and any package that doesn't need to be grouped gets\n> > > trimmed down to individual transactions. Furthermore any tx that is \"stuck\"\n> > > can be freed by simply sending another tx. The nodes at which the tx has\n> > > become stuck will just package it up and relay it to peers. In other words,\n> > > there is no impact on wallet implementation apart from raising the aggregate\n> > > fee using a descendant transaction.\n> > > \n> > > This is barely a protocol change - it's primarily implementation. All that\n> > > should be required is an additional INV element type, such as\n> > > MSG_TX_PACKAGE.\n> > > \n> > > Additional constraints:\n> > > \n> > > * All elements of MSG_TX_PACKAGE in one INV message MUST to be of\n> > > the same package.\n> > > * A package MUST must define a set that can be mined into one block\n> > > (size/sigops constraint).\n> > > * A package SHOULD not contain confirmed txs (a race may cause this).\n> > > * A package MUST minimally satisfy peer.feerate.\n> > > * A partial tx order, as in the manner of the block.txs ordering, MUST be\n> > > imposed.\n> > > * A node SHOULD drop a peer that sends a package (or tx) below\n> > > node.feerate.\n> > > * A node MAY drop a peer that sends a non-minimal package according to\n> > > node.feerate.\n> > > \n> > > The partial ordering of block.txs introduces an ordering constraint that\n> > > precludes full parallelism in validating input attachment. This is an\n> > > implementation artifact that made its way into consensus. However in the\n> > > case of packaging, the set of txs is not presumed to be valid under the proof\n> > > of work DoS guard. As such constraints should minimize the work/traffic\n> > > required to invalidate the message. The partial order constraint ensures that\n> > > the DAG can be built incrementally, dropping the attempt (and peer as\n> > > desired) as soon as the first orphan is discovered. As a result the network\n> > > traffic and work required is not materially different than with tx relay, with\n> > > two exceptions.\n> > > \n> > > These are the two central aspects of this approach (Avoiding Predictable\n> > > Orphans and Creating Minimal Packages). These are graph search algorithms,\n> > > some basic computer science. Minimality requires only that the package does\n> > > not introduce txs that are not necessary to reach the peer.feerate (as these\n> > > can always be packaged separately). It does not require that nodes all\n> > > generate the same packages. It does not require negotiation, package\n> > > identity, cryptography, or hashing. As a graph search it should be O(n) where\n> > > n is the unconfirmed ancestry of the package, but should typically be much\n> > > lower, if not a single step.\n> > > \n> > > Sufficiently-low-fee nodes will see only single txs. Moderate-fee\n> > > nodes may cause partial breakup of packages. Sufficiently high fee\n> > > nodes will cause peers (having received and completed the acceptance\n> > > of a tx/pkg with pkg.feerate < peer.feerate) to navigate from each\n> > > tx/package external input until reaching txs above peer.feerate, or\n> > > confirmed (both of which the peer is presumed to already have). If the\n> > > pkg.feerate is sufficiently high to connect all external inputs to the\n> > > intervening txs, they are added to the package and it is announced to\n> > > the high fee peer. Note that the individual tx.feerate > peer.feerate\n> > > is insufficient to ensure that the peer should have the tx, as there\n> > > may be ancestor txs that do not, and for which the tx was insufficient\n> > > to cause them to be packaged. So a non-caching algorithm must be able\n> > > to chase each package external input to a confirmed tx (or cache the\n> > > unconfirmed ancestry fee rate at each tx). Note that fee rates are not\n> > > directly additive, both size/\n> > > \n> > > weight and fee are required for summation (and aggregate sigops should\n> > > be considered).\n> > > \n> > > This makes no assumptions about current implementations. The design\n> > > would call for maintenance of a transaction (input->output) DAG with\n> > > tx.feerate on each tx. This could be the unconfirmed tx graph (i.e. \"memory\n> > > pool\") though it does not require maintenance of anything more than the\n> > > parameters necessary to confirm a set of validated txs within a block. It is\n> > > very reasonable to require this of any participating node. A simple version\n> > > negotiation can identify a package-accepting/sending nodes.\n> > > \n> > > I have thought about this for some time, but have not implemented either\n> > > the graph search, source code, or BIP. Just wrote this off the top of my head.\n> > > So I am sure there are some things I have incorrect or failed to consider. But I\n> > > think it's worth discussing it at this point.\n> > > \n> > > e\n> > > \n> > > > -----Original Message-----\n> > > > From: bitcoin-dev bitcoin-dev-bounces at lists.linuxfoundation.org On\n> > > > Behalf Of Suhas Daftuar via bitcoin-dev\n> > > > Sent: Wednesday, June 8, 2022 8:59 AM\n> > > > To: Bitcoin Protocol Discussion\n> > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > Subject: Re: [bitcoin-dev] Package Relay Proposal\n> > > > \n> > > > Hi,\n> > > > \n> > > > Thanks again for your work on this!\n> > > > \n> > > > One question I have is about potential bandwidth waste in the case\n> > > > of nodes running with different policy rules. Here's my\n> > > > understanding of a scenario I think could happen:\n> > > > \n> > > > 1) Transaction A is both low-fee and non-standard to some nodes on\n> > > > the network.\n> > > > 2) Whenever a transaction T that spends A is relayed, new nodes will\n> > > > send INV(PKGINFO1, T) to all package-relay peers.\n> > > > 3) Nodes on the network that have implemented package relay, but do\n> > > > not accept A, will send getdata(PKGINFO1, T) and learn all of T's\n> > > > unconfirmed parents (~32 bytes * number of parents(T)).\n> > > > 4) Such nodes will reject T. But because of transaction\n> > > > malleability, and to avoid being blinded to a transaction\n> > > > unnecessarily, these nodes will likely still send getdata(PKGINFO1,\n> > > > T) to every node that announces T, in case someone has a transaction\n> > > > that includes an alternate set of parent transactions that would pass\n> > > > policy checks.\n> > > > \n> > > > Is that understanding correct? I think a good design goal would be\n> > > > to not waste bandwidth in non-adversarial situations. In this case,\n> > > > there would be bandwidth waste from downloading duplicate data from\n> > > > all your peers, just because the announcement doesn't commit to the\n> > > > set of parent wtxids that we'd get from the peer (and so we are\n> > > > unable to determine that all our peers would be telling us the same thing,\n> > > > just based on the announcement).\n> > > > \n> > > > Some ways to mitigate this might be to: (a) include a hash (maybe\n> > > > even just a 20-byte hash -- is that enough security?) of the package\n> > > > wtxids (in some canonical ordering) along with the wtxid of the\n> > > > child in the initial announcement; (b) limit the use of v1 packages\n> > > > to transactions with very few parents (I don't know if this is reasonable\n> > > > for the use cases we have in mind).\n> > > > \n> > > > Another point I wanted to bring up is about the rules around v1\n> > > > package validation generally, and the use of a blockhash in\n> > > > transaction relay specifically. My first observation is that it\n> > > > won't always be the case that a v1 package relay node will be able\n> > > > to validate that a set of package transactions is fully sorted\n> > > > topologically, because there may be (non-parent) ancestors that are\n> > > > missing from the package and the best a peer can validate is\n> > > > topology within the package -- this means that a peer can validly\n> > > > (under this\n> > > > BIP) relay transaction packages out of the true topological sort (if\n> > > > all ancestors were included).\n> > > > \n> > > > This makes me wonder how useful this topological rule is. I suppose\n> > > > there is some value in preventing completely broken implementations\n> > > > from staying connected and so there is no harm in having the rule,\n> > > > but perhaps it would be helpful to add that nodes SHOULD order\n> > > > transactions based on topological sort in the complete transaction\n> > > > graph, so that if missing-from-package ancestors are already known\n> > > > by a peer (which is the expected case when using v1 package relay on\n> > > > transactions that have more than one generation of unconfirmed\n> > > > ancestor) then the remaining transactions are already properly ordered,\n> > > > and this is helpful even if unenforceable in general.\n> > > > \n> > > > The other observation I wanted to make was that having transaction\n> > > > relay gated on whether two nodes agree on chain tip seems like an\n> > > > overly restrictive criteria. I think an important design principle\n> > > > is that we want to minimize disruption from network splits -- if\n> > > > there are competing blocks found in a small window of time, it's\n> > > > likely that the utxo set is not materially different on the two\n> > > > chains (assuming miners are selecting from roughly the same sets of\n> > > > transactions when this happens, which is typical). Having\n> > > > transaction relay bifurcate on the two network halves would seem to\n> > > > exacerbate the difference between the two sides of the split --\n> > > > users ought to be agnostic about how benign splits are resolved and\n> > > > would likely want their transactions to relay across the whole network.\n> > > > \n> > > > Additionally, use of a chain tip might impose a larger burden than\n> > > > is necessary on software that would seek to participate in\n> > > > transaction relay without implementing headers sync/validation. I\n> > > > don't know what software exists on the network, but I imagine there\n> > > > are a lot of scripts out there for transaction submission to the\n> > > > public p2p network, and in thinking about modifying such a script to\n> > > > utilize package relay it seems like an unnecessary added burden to first\n> > > > learn a node's tip before trying to relay a transaction.\n> > > > \n> > > > Could you explain again what the benefit of including the blockhash\n> > > > is? It seems like it is just so that a node could prioritize\n> > > > transaction relay from peers with the same chain tip to maximize the\n> > > > likelihood of transaction acceptance, but in the common case this\n> > > > seems like a pretty negligible concern, and in the case of a chain\n> > > > fork that persists for many minutes it seems better to me that we\n> > > > not partition the network into package-relay regimes and just risk a\n> > > > little extra bandwidth in one direction or the other. If we solve\n> > > > the problem I brought up at the beginning (of de-duplicating package\n> > > > data across peers with a package-wtxid-commitment in the\n> > > > announcement), I think this is just some wasted pkginfo bandwidth on\n> > > > a single-link, and not across links (as we could cache validation failure for a\n> > > > package-hash to avoid re-requesting duplicate pkginfo1 messages).\n> > > > \n> > > > Best,\n> > > > Suhas\n> > > > \n> > > > On Tue, Jun 7, 2022 at 1:57 PM Gloria Zhao via bitcoin-dev <bitcoin-\n> > > > dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > > dev at lists.linuxfoundation.org> > wrote:\n> > > > \n> > > > Hi Eric, aj, all,\n> > > > \n> > > > Sorry for the delayed response. @aj I'm including some paraphrased\n> > > > points from our offline discussion (thanks).\n> > > > \n> > > > > Other idea: what if you encode the parent txs as a short hash of\n> > > > > the wtxid (something like bip152 short ids? perhaps seeded per\n> > > > > peer so collisions will be different per peer?) and include that in the inv\n> > > > > announcement?\n> > > > > Would that work to avoid a round trip almost all of the time,\n> > > > > while still giving you enough info to save bw by deduping parents?\n> > > > \n> > > > > As I suggested earlier, a package is fundamentally a compact block\n> > > > > (or\n> > > > > block) announcement without the header. Compact block (BIP152)\n> > > > > announcement is already well-defined and widely implemented...\n> > > > \n> > > > > Let us not reinvent the wheel and/or introduce accidental\n> > > > > complexity. I see no reason why packaging is not simply BIP152\n> > > > > without the 'header'\n> > > > > field, an\n> > > > > updated protocol version, and the following sort of changes to\n> > > > > names\n> > > > \n> > > > Interestingly, \"why not use BIP 152 shortids to save bandwidth?\" is\n> > > > by far the most common suggestion I hear (including offline feedback).\n> > > > Here's a full explanation:\n> > > > \n> > > > BIP 152 shortens transaction hashes (32 bytes) to shortids (6 bytes)\n> > > > to save a significant amount of network bandwidth, which is\n> > > > extremely important in block relay. However, this comes at the\n> > > > expense of computational complexity. There is no way to directly\n> > > > calculate a transaction hash from a shortid; upon receipt of a\n> > > > compact block, a node is expected to calculate the shortids of every\n> > > > unconfirmed transaction it knows about to find the matches (BIP 152:\n> > > > 1, Bitcoin Core: 2). This is expensive but appropriate for block\n> > > > relay, since the block must have a valid Proof of Work and new\n> > > > blocks only come every ~10 minutes. On the other hand, if we require\n> > > > nodes to calculate shortids for every transaction in their mempools every\n> > > > time they receive a package, we are creating a DoS vector.\n> > > > Unconfirmed transactions don't need PoW and, to have a live\n> > > > transaction relay network, we should expect nodes to handle\n> > > > transactions at a high-ish rate (i.e. at least 1000s of times more\n> > > > transactions than blocks). We can't pre- calculate or cache shortids\n> > > > for mempool transactions, since the SipHash key depends on the block\n> > > > hash and a per-connection salt.\n> > > > \n> > > > Additionally, shortid calculation is not designed to prevent\n> > > > intentional individual collisions. If we were to use these shortids\n> > > > to deduplicate transactions we've supposedly already seen, we may\n> > > > have a censorship vector. Again, these tradeoffs make sense for\n> > > > compact block relay (see shortid section in BIP 152 [3]), but not package\n> > > > relay.\n> > > > \n> > > > TLDR: DoSy if we calculate shortids on every package and censorship\n> > > > vector if we use shortids for deduplication.\n> > > > \n> > > > > Given this message there is no reason to send a (potentially\n> > > > > bogus) fee rate with every package. It can only be validated by\n> > > > > obtaining the full set of txs, and the only recourse is dropping\n> > > > > (etc.) the peer, as is the case with single txs.\n> > > > \n> > > > Yeah, I agree with this. Combined with the previous discussion with\n> > > > aj (i.e. we can't accurately communicate the incentive compatibility\n> > > > of a package without sending the full graph, and this whole dance is\n> > > > to avoid downloading a few low-fee transactions in uncommon edge\n> > > > cases), I've realized I should remove the fee + weight information\n> > > > from pkginfo. Yay for less complexity!\n> > > > \n> > > > Also, this might be pedantic, but I said something incorrect earlier\n> > > > and would like to correct myself:\n> > > > \n> > > > > > In theory, yes, but maybe it was announced earlier (while our\n> > > > > > node was down?) or had dropped from our mempool or similar,\n> > > > > > either way we don't have those txs yet.\n> > > > \n> > > > I said \"It's fine if they have Erlay, since a sender would know in\n> > > > advance that B is missing and announce it as a package.\" But this\n> > > > isn't true since we're only using reconciliation in place of\n> > > > flooding to announce transactions as they arrive, not for\n> > > > rebroadcast, and we're not doing full mempool set reconciliation. In\n> > > > any case, making sure a node receives the transactions announced\n> > > > when it was offline is not something we guarantee, not an intended use\n> > > > case for package relay, and not worsened by this.\n> > > > \n> > > > Thanks for your feedback!\n> > > > \n> > > > Best,\n> > > > \n> > > > Gloria\n> > > > \n> > > > 0152.mediawiki#cmpctblock\n> > > > 2:\n> > > > https://github.com/bitcoin/bitcoin/blob/master/src/blockencodings.cp\n> > > > p#L49\n> > > > [3]: https://github.com/bitcoin/bips/blob/master/bip-\n> > > > 0152.mediawiki#short-transaction-id-calculation\n> > > > \n> > > > On Thu, May 26, 2022 at 3:59 AM <eric at voskuil.org\n> > > > mailto:eric at voskuil.org > wrote:\n> > > > \n> > > > Given that packages have no header, the package requires identity in\n> > > > a\n> > > > BIP152 scheme. For example 'header' and 'blockhash' fields can be\n> > > > replaced with a Merkle root (e.g. \"identity\" field) for the package,\n> > > > uniquely identifying the partially-ordered set of txs. And use of\n> > > > 'getdata' (to obtain a package by hash) can be eliminated (not a use\n> > > > case).\n> > > > \n> > > > e\n> > > > \n> > > > > -----Original Message-----\n> > > > > From: eric at voskuil.org mailto:eric at voskuil.org\n> > > > > <eric at voskuil.org mailto:eric at voskuil.org >\n> > > > > Sent: Wednesday, May 25, 2022 1:52 PM\n> > > > > To: 'Anthony Towns' <aj at erisian.com.au\n> > > > > mailto:aj at erisian.com.au >; 'Bitcoin Protocol Discussion'\n> > > > > <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > > > dev at lists.linuxfoundation.org> >; 'Gloria Zhao'\n> > > > > <gloriajzhao at gmail.com mailto:gloriajzhao at gmail.com >\n> > > > > Subject: RE: [bitcoin-dev] Package Relay Proposal\n> > > > > \n> > > > > > From: bitcoin-dev <bitcoin-dev-\n> > > > > > bounces at lists.linuxfoundation.org <mailto:bitcoin-dev-\n> > > > > > bounces at lists.linuxfoundation.org> > On\n> > > > > > Behalf\n> > > > > > Of Anthony Towns via bitcoin-dev\n> > > > > > Sent: Wednesday, May 25, 2022 11:56 AM\n> > > > > \n> > > > > > So the other thing is what happens if the peer\n> > > > > > announcing packages to us\n> > > > > > is\n> > > > > > dishonest?\n> > > > > > \n> > > > > > They announce pkg X, say X has parents A B C and the fee\n> > > > > > rate is\n> > > > > > garbage.\n> > > > > > But\n> > > > > > actually X has parent D and the fee rate is excellent. Do\n> > > > > > we request the\n> > > > > > package from another peer, or every peer, to double\n> > > > > > check? Otherwise\n> > > > > > we're\n> > > > > > allowing the first peer we ask about a package to censor\n> > > > > > that tx from\n> > > > > > us?\n> > > > > > \n> > > > > > I think the fix for that is just to provide the fee and weight\n> > > > > > when\n> > > > > > announcing\n> > > > > > the package rather than only being asked for its info?\n> > > > > > Then if one peer\n> > > > > > makes\n> > > > > > it sound like a good deal you ask for the parent txids from\n> > > > > > them,\n> > > > > > dedupe,\n> > > > > > request, and verify they were honest about the parents.\n> > > > > \n> > > > > Single tx broadcasts do not carry an advertised fee rate,\n> > > > > however the'\n> > > > > feefilter' message (BIP133) provides this distinction. This\n> > > > > should be\n> > > > > interpreted as applicable to packages. Given this message\n> > > > > there is no\n> > > > > reason\n> > > > > to send a (potentially bogus) fee rate with every package. It\n> > > > > can only be\n> > > > > validated by obtaining the full set of txs, and the only\n> > > > > recourse is\n> > > > > dropping (etc.) the peer, as is the case with single txs.\n> > > > > Relying on the\n> > > > > existing message is simpler, more consistent, and more\n> > > > > efficient.\n> > > > > \n> > > > > > > > Is it plausible to add the graph in?\n> > > > > > \n> > > > > > Likewise, I think you'd have to have the graph info from\n> > > > > > many nodes if\n> > > > > > you're\n> > > > > > going to make decisions based on it and don't want\n> > > > > > hostile peers to be\n> > > > > > able to\n> > > > > > trick you into ignoring txs.\n> > > > > > \n> > > > > > Other idea: what if you encode the parent txs as a short\n> > > > > > hash of the\n> > > > > > wtxid\n> > > > > > (something like bip152 short ids? perhaps seeded per\n> > > > > > peer so collisions\n> > > > > > will\n> > > > > > be different per peer?) and include that in the inv\n> > > > > > announcement? Would\n> > > > > > that work to avoid a round trip almost all of the time,\n> > > > > > while still\n> > > > > > giving\n> > > > > > you\n> > > > > > enough info to save bw by deduping parents?\n> > > > > \n> > > > > As I suggested earlier, a package is fundamentally a\n> > > > > compact block (or\n> > > > > block) announcement without the header. Compact block\n> > > > > (BIP152)\n> > > > > announcement\n> > > > > is already well-defined and widely implemented. A node\n> > > > > should never be\n> > > > > required to retain an orphan, and BIP152 ensures this is not\n> > > > > required.\n> > > > > \n> > > > > Once a validated set of txs within the package has been\n> > > > > obtained with\n> > > > > sufficient fee, a fee-optimal node would accept the largest\n> > > > > subgraph of\n> > > > > the\n> > > > > package that conforms to fee constraints and drop any\n> > > > > peer that provides a\n> > > > > package for which the full graph does not.\n> > > > > \n> > > > > Let us not reinvent the wheel and/or introduce accidental\n> > > > > complexity. I\n> > > > > see\n> > > > > no reason why packaging is not simply BIP152 without the\n> > > > > 'header' field,\n> > > > > an\n> > > > > updated protocol version, and the following sort of changes\n> > > > > to names:\n> > > > > \n> > > > > sendpkg\n> > > > > MSG_CMPCT_PKG\n> > > > > cmpctpkg\n> > > > > getpkgtxn\n> > > > > pkgtxn\n> > > > > \n> > > > > > > For a maximum 25 transactions,\n> > > > > > > 2324/2 = 276, seems like 36 bytes for a child-with-\n> > > > > > > parents package.\n> > > > > > \n> > > > > > If you're doing short ids that's maybe 254B=100B\n> > > > > > already, then the\n> > > > > > above\n> > > > > > is\n> > > > > > up to 36% overhead, I guess. Might be worth thinking\n> > > > > > more about, but\n> > > > > > maybe\n> > > > > > more interesting with ancestors than just parents.\n> > > > > > \n> > > > > > > Also side note, since there are no size/count params,\n> > > > > \n> > > > > Size is restricted in the same manner as block and\n> > > > > transaction broadcasts,\n> > > > > by consensus. If the fee rate is sufficient there would be no\n> > > > > reason to\n> > > > > preclude any valid size up to what can be mined in one\n> > > > > block (packaging\n> > > > > across blocks is not economically rational under the\n> > > > > assumption that one\n> > > > > miner cannot expect to mine multiple blocks in a row).\n> > > > > Count is\n> > > > > incorporated\n> > > > > into BIP152 as 'shortids_length'.\n> > > > > \n> > > > > > > wondering if we\n> > > > > > > should just have \"version\" in \"sendpackages\" be a bit\n> > > > > > > field instead of\n> > > > > > > sending a message for each version. 32 versions should\n> > > > > > > be enough right?\n> > > > > \n> > > > > Adding versioning to individual protocols is just a reflection\n> > > > > of the\n> > > > > insufficiency of the initial protocol versioning design, and\n> > > > > that of the\n> > > > > various ad-hoc changes to it (including yet another\n> > > > > approach in this\n> > > > > proposal) that have been introduced to compensate for it,\n> > > > > though I'll\n> > > > > address this in an independent post at some point.\n> > > > > \n> > > > > Best,\n> > > > > e\n> > > > > \n> > > > > > Maybe but a couple of messages per connection doesn't\n> > > > > > really seem worth\n> > > > > > arguing about?\n> > > > > > \n> > > > > > Cheers,\n> > > > > > aj\n> > > > > > \n> > > > > > --\n> > > > > > Sent from my phone.\n> > > > \n> > > > _______________________________________________\n> > > > \n> > > > > > bitcoin-dev mailing list\n> > > > > > bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > > > > dev at lists.linuxfoundation.org>\n> > > > \n> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > > > \n> > > > _______________________________________________\n> > > > bitcoin-dev mailing list\n> > > > bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-\n> > > > dev at lists.linuxfoundation.org>\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > > \n> > > _______________________________________________\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Eric Voskuil",
                "date": "2022-09-27T19:21:35",
                "message_text_only": "Thanks again for the feedback. Comments inline.\n\n> On Sep 27, 2022, at 02:29, alicexbt <alicexbt at protonmail.com> wrote:\n> \n> \ufeffHi Eric,\n> \n> \n>> If by \"range\" you mean a connected tx subgraph, I don't see why not. But note that nodes only operate over signed txs. PSBT is a wallet workflow.\n> \n> Matt Corallo mentioned that pre-signed transactions created with low fee rate become an issue when they are broadcasted after a long time and there is a high demand for block space at that moment.\n\nYes, I understood this. There are many ways that a fee may be created which is too low for propagation.\n\n> Example: \n> \n> Bob created PSBT1 in a multi party contract with fee rate 5 sat/vbyte however its taking hours/days to confirm the transaction with such low fee rate.\n> \n> Carol created PSBT1 (5 sat/vbyte), PSBT2 (10 sat/vbyte) and PSBT3 (20 sat/vbyte) for spending same inputs. She broadcasted PSBT3 which got confirmed in a few minutes. \n> \n> \n>> Always. Only signed transactions are accepted. But assuming you are referring to a transaction that has been produced by a pre-signing workflow, I'm not sure how this would be distinct from any other tx.\n> \n> \n> `minfeefilter` for all peers of my node was 0.00001000 at the time of writing this email. I am assuming nobody creates pre-signed transaction with fee rate below 1 sat/vbyte. How often does it happen that `minfeefilter` is above this default value?\n\nI don\u2019t consider node configuration relevant, regardless of its apparent consistency.\n\n>> I'm not sure I follow this, maybe you could reword it. But it seems that you are saying that CPFP fee-bumping is a problem scenario and the complexity of the proposed solutions are not justified by such scenarios.\n> \n> \n> Sorry that sentence was confusing. Yes complexity isn't justified for CPFP fee-bumping txs below minimum fee rate.\n> \n> \n>> There are many node implementations used presently. And of course these are protocol proposals, which presumes more than one implementation.\n> \n> \n> Yes, a few implementations exist (knots, libbitcoin, btcd, bcoin etc.) however they aren't used by lot of nodes. Based on this [chart][1] 98% nodes use bitcoin core. Lot of bitcoin protocol proposals are influenced by bitcoin core contributors and things could be different if even 30% nodes used other implementations.\n\nI don\u2019t consider such a measure relevant. This is a protocol consideration. Also consider that many nodes are not visible, and aspects of nodes, such as for p2p communication, are embedded into applications such as wallets - which could easily exceed the number of visible nodes.\n\n>> I don't consider this relevant to any protocol considerations. Miners should always be expected to select the most optimal set of txs available in the time available to do so.\n> \n> \n> Agree, miners should be expected to select most optimal set of txs. However, according to one [comment][2] by Pieter Wuille, miners could affect the security of some bitcoin projects with MEV.\n\nThis would be a deficiency in such projects, by assuming economic irrationality. The fact that fees will become a greater percentage of the block reward is a surprise to no one.\n\n>> Over time we are likely to see that the only policies that remain in widespread application are those that are necessary for DOS protection (fee rate), as other restrictions are not economically rational and cannot be enforced. We've seen recent debate regarding dust policy, and op_return policy. \"non-standard\" txs are perfectly valid but get stuck very easily. I'll reiterate, any policy beyond what is published via the protocol will cause the above problems.\n> \n> I completely agree with this.\n> \n> \n> [1]: https://luke.dashjr.org/programs/bitcoin/files/charts/software.html\n> [2]: https://bitcoin.stackexchange.com/questions/107787/front-running-in-bitcoin#comment123441_107796\n> \n> \n> /dev/fd0"
            }
        ],
        "thread_summary": {
            "title": "Packaged Transaction Relay",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "alicexbt",
                "eric at voskuil.org",
                "Eric Voskuil"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 115275
        }
    },
    {
        "title": "[bitcoin-dev] Trustless Address Server \u2013 Outsourcing handing out addresses to prevent address reuse",
        "thread_messages": [
            {
                "author": "Ruben Somsen",
                "date": "2022-09-29T15:39:18",
                "message_text_only": "Hi all,\n\nIn short, this is yet another way to hand out addresses without interaction\nbetween sender and recipient (Silent Payments, BIP47). The idea here is\nthat in non-ideal cases where you're already exposing your xpub to a server\n(most light clients today, unfortunately), you might as well rely on them\nto hand out addresses on your behalf.\n\nOther than BTCPay Server, I am not aware of any serious attempts of\nexploring this direction. Perhaps this is justified, due to the difficulty\nof dealing with the gap limit, but it seems worth discussing nonetheless.\n\nThe write-up is available (and kept up-to-date) here as a gist:\nhttps://gist.github.com/RubenSomsen/960ae7eb52b79cc826d5b6eaa61291f6\n\nAnd here's a copy for the list:\n\n\n### Introduction\n\nAddress reuse prevention generally requires interacting with the recipient\nin order to receive a fresh address for each payment. There are various\nprotocols that ensure no interaction is required such as BIP47[^1] and\nSilent Payments[^2], though neither is without downsides.\n\nOne area that is seemingly underexplored is that of outsourced interaction.\nBTCPay Server[^3] is an example of this. The sender interacts with a\nserver, which acts on behalf of the recipient and hands out an address from\nan xpub. The recipient controls and therefore trusts the server, so\nmalicious addresses won't be given out.\n\n### Outsourcing and Malicious Keys\n\nThe vast majority of light clients today (even ones that support BIP47,\ncuriously) already control the user's xpub, so it seems logical to think\nthe interaction can be outsourced to them. However, unlike when running\nyour own server, a third party server *could* potentially hand out\nmalicious addresses (i.e. addresses that belong to someone other than you).\n\nThe solution to this is identity. As long as the sender knows a public key\nby which the recipient can be identified, the recipient can sign the\naddresses that are derived from their xpub[^4]. This way the sender can be\nsure that the address it receives from the server belongs to the recipient.\n\n### Gap Limit\n\nOne big remaining problem is the gap limit[^5]. When an adversary\nrepeatedly requests addresses from the server but then never uses them,\nthis could result in a large gap of unused addresses. This is a problem\nbecause when recovering from backup the wallet stops looking for payments\nwhen a large enough gap is encountered. Unfortunately there is no perfect\nsolution, but mitigations are still possible.\n\nWhenever a sender wants to make their first payment, they could be expected\nto obtain an address at a cost (solving captchas, paying over LN,\nproof-of-burn[^6]). If the sender doesn't mind (or maybe even desires)\nhaving their payments correlated by the recipient, a fresh xpub[^7] can be\nhanded out instead of an address in order to enable repeated payments. If\nnon-correlated payments are preferable, after each successful payment the\nserver could hand out a blind ecash[^8] token that entitles the sender to\nanother address.\n\nAn alternative mitigation (more user friendly, but more implementation\ncomplexity) would be to require the sender to reveal their intended\ntransaction to the server prior to receiving the address[^9]. This is not a\nprivacy degradation, since the server could already learn this information\nregardless. If the transaction doesn't end up getting sent, any subsequent\nattempt to reuse one of the inputs should either be (temporarily)\nblacklisted or responded to with the same address that was given out\nearlier[^10].\n\nIf despite best efforts the gap limit is inadvertently reached anyway, the\nrecipient may have to be instructed to ensure they properly receive a\npayment to bridge the gap before new addresses can be handed out. The\nalternative is to forego privacy when this happens, but this seems unwise.\n\n### Use Case\n\nThis protocol seems useful for users that a.) want to use light clients,\nb.) accept the privacy degradation of handing out their xpub to a third\nparty, and c.) want to receive payments non-interactively. If any one of\nthese is not true, other protocols are likely to be a better choice[^11].\nFinally, it should be acknowledged that this protocol introduces more\nfriction on the sender side due to the need for a gap limit mitigation\nstrategy.\n\n-- Ruben Somsen\n\n\n[^1]: BIP47: https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n\n[^2]: Silent Payments:\nhttps://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8\n\n[^3]: BTCPay Server https://btcpayserver.org/\n\n[^4]: *Specifically, this could be a single signature on a merkle root, so\nthe amount of data that the recipient needs to send to the server can be\nminimized and the server can just generate the same tree from the xpub and\nhand out merkle proofs to senders. The order of the leaves should be\nrandomized so senders cannot learn how many payments were made.*\n\n[^5]: Gap limit:\nhttps://bitcoin.stackexchange.com/questions/111534/bitcoin-address-gap-limit\n\n[^6]: Efficient Proof-of-Burn:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-July/020746.html\n\n[^7]: Xpub sharing:\nhttps://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8#xpub-sharing\n\n[^8]: Blind ecash:\nhttps://gist.github.com/RubenSomsen/be7a4760dd4596d06963d67baf140406\n\n[^9]: *This would essentially look like an incomplete but signed\ntransaction where the output address is still missing.*\n\n[^10]: *Keep in mind the edge case where e.g. two inputs are presented but\nnot used, followed by two separate transactions which each use one of the\npriorly presented inputs.*\n\n[^11]: Protocol considerations:\nhttps://twitter.com/SomsenRuben/status/1530096037414707200\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220929/896947be/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Trustless Address Server \u2013 Outsourcing handing out addresses to prevent address reuse",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Ruben Somsen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5824
        }
    },
    {
        "title": "[bitcoin-dev] Third version of Silent Payment implementation",
        "thread_messages": [
            {
                "author": "woltx",
                "date": "2022-09-29T22:19:38",
                "message_text_only": "This new version addresses most (or all) requests made in PR:\n\n. Implements the new scheme suggested by Ruben Somsen that allows multiple silent addresses per wallet with minimal overhead.\n. Implements a new RPC to retrieve silent addresses, which allows users to assign different labels to different addresses. That way, the user knows which silent address the UTXO came from.\n\nExample:\n\n./src/bitcoin-cli -signet -rpcwallet=\"receiver\" getspaddress\ntsp001pjgcwd9p6f2rcgf35dlgvj77h2afylg6lp5cdn0cztrk4k54w99kqxn48tq\n\n# This will return the same address as above (both have no label)\n./src/bitcoin-cli -signet -rpcwallet=\"receiver\" getspaddress\ntsp001pjgcwd9p6f2rcgf35dlgvj77h2afylg6lp5cdn0cztrk4k54w99kqxn48tq\n\n# New label, new address\n./src/bitcoin-cli -signet -rpcwallet=\"receiver\" getspaddress 'donation'\ntsp011pjgcwd9p6f2rcgf35dlgvj77h2afylg6lp5cdn0cztrk4k54w99kq80t7lt\n\nIn this new scheme, the address has a new field called identifier, which tells the receiver and sender how to set the address correctly.\n\nIf the receiver, for whatever reason, doesn't know which identifiers have been used, there is no problem. The wallet can scan all identifiers from 0 to 99. Currently, only 100 different identifiers per wallet are allowed. This limit, however, can be increased at any time in the future.\n\nUnlike address formats so far, sp addresses are not script-related and may eventually include any additional information needed, such as an expiration timestamp (or block height). That way, users don't have to track the address indefinitely.\n\nAs usual I wrote a step by step tutorial:\nhttps://gist.github.com/w0xlt/c81277ae8677b6c0d3dd073893210875\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220929/bd368220/attachment.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2022-09-29T23:03:36",
                "message_text_only": "Hi woltx,\n\nExcellent work.\n\n>Implements the new scheme suggested by Ruben Somsen that allows multiple\nsilent addresses per wallet with minimal overhead\n\nTo expand on this, the scheme basically allows the resulting address to be\nrecognizably marked (only recognizable by the recipient of course), which\nenables you to distinguish between different payment purposes (e.g. some\npeople donate to you for purpose A, others for purpose B). Here's my\noriginal comment describing it:\n\n\"Naively, the issue is that two keys means twice the scanning, but an\ninteresting alternative would be to simply use the same key (assuming\nyou're OK with using the same identity) but add a public identifier f to it\nwhen tweaking. So instead of hash(i*X)*G + X you get hash(i*X)*G + X + f*G\n. This means every additional \"address\" only costs one additional ECC\naddition when scanning (relatively cheap compared to doing ECC\nmultiplications).\n\nThe main downside with this is that f becomes crucial for recovering from\nbackup. If we set f as an index (0, 1, 2, 3...) then you'd only have to\nremember how many \"addresses\" you issued (and perhaps overshoot when\nunsure) to ensure recovery of funds, though of course you'd rather also\nremember which index is associated with what payment reason.\n\nAbsolute worst case scenario you could even do something similar to the gap\nlimit where you scan the full history (not just the UTXO set so you don't\nmiss spent outputs) with a default max index of e.g. 100, and then if you\nfind out most of them are in use, you scan the next 100, etc. Costly, but\nthorough, and only needed as a last resort.\"\n\nOriginal comment here:\nhttps://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8#xpub-sharing\n\nAlso good to note that f needs to be communicated to the sender somehow,\nperhaps as part of the address format.\n\nCheers,\nRuben\n\nOn Fri, Sep 30, 2022 at 12:35 AM woltx via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> This new version addresses most (or all) requests made in PR:\n>\n> . Implements the new scheme suggested by Ruben Somsen that allows multiple\n> silent addresses per wallet with minimal overhead.\n> . Implements a new RPC to retrieve silent addresses, which allows users to\n> assign different labels to different addresses. That way, the user knows\n> which silent address the UTXO came from.\n>\n> Example:\n>\n> ./src/bitcoin-cli -signet -rpcwallet=\"receiver\" getspaddress\n> tsp001pjgcwd9p6f2rcgf35dlgvj77h2afylg6lp5cdn0cztrk4k54w99kqxn48tq\n>\n> # This will return the same address as above (both have no label)\n> ./src/bitcoin-cli -signet -rpcwallet=\"receiver\" getspaddress\n> tsp001pjgcwd9p6f2rcgf35dlgvj77h2afylg6lp5cdn0cztrk4k54w99kqxn48tq\n>\n> # New label, new address\n> ./src/bitcoin-cli -signet -rpcwallet=\"receiver\" getspaddress 'donation'\n> tsp011pjgcwd9p6f2rcgf35dlgvj77h2afylg6lp5cdn0cztrk4k54w99kq80t7lt\n>\n> In this new scheme, the address has a new field called identifier, which\n> tells the receiver and sender how to set the address correctly.\n>\n> If the receiver, for whatever reason, doesn't know which identifiers have\n> been used, there is no problem. The wallet can scan all identifiers from 0\n> to 99. Currently, only 100 different identifiers per wallet are allowed.\n> This limit, however, can be increased at any time in the future.\n>\n> Unlike address formats so far, sp addresses are not script-related and may\n> eventually include any additional information needed, such as an expiration\n> timestamp (or block height). That way, users don't have to track the\n> address indefinitely.\n>\n> As usual I wrote a step by step tutorial:\n> https://gist.github.com/w0xlt/c81277ae8677b6c0d3dd073893210875\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220930/2cf4fe86/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Third version of Silent Payment implementation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "woltx",
                "Ruben Somsen"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5870
        }
    },
    {
        "title": "[bitcoin-dev] Wallet policies for descriptor wallets",
        "thread_messages": [
            {
                "author": "Andrew Poelstra",
                "date": "2022-09-29T23:56:51",
                "message_text_only": "I'm really happy to see this discussion. I don't have any comments on the spec\nbecause I think I'd have to be more in-the-weeds trying to implement a hww to\nunderstand how well it works for realistic use cases. But a strong concept-ACk\nfrom me and thanks to Salvatore for exploring this!\n\nOn Mon, May 09, 2022 at 11:36:47AM +0000, darosior via bitcoin-dev wrote:\n> \n> Unrelated question, since you mentioned `musig2` descriptors in this context. I thought Musig2 wasn't really\n> feasible for hardware signing devices, especially stateless ones. Do you think/know whether it is actually\n> possible for a HW to take part in a Musig2?\n>\n\nAs Salvatore mentioned in his reply, there are a couple ways that hwws can deal\nwith musig2 -- specifically, having state (and I believe you can get away with\nas little state as a single monotonic counter) or having a RNG which is reliable\nenough that it at least won't repeat values.\n\nBecause these aren't blockers for all hwws, even if they are blockers for some,\nI'd really like to see musig2 support in these protocols, or at least for musig2\nto be considered in their design.\n \n\n-- \nAndrew Poelstra\nDirector of Research, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\nThe sun is always shining in space\n    -Justin Lewis-Webster\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220929/b9bc0273/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Wallet policies for descriptor wallets",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Andrew Poelstra"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1584
        }
    }
]