[
    {
        "title": "[bitcoin-dev] Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "Chris Stewart",
                "date": "2019-10-01T12:23:47",
                "message_text_only": "I do have some concerns about SIGHASH_NOINPUT, mainly that it does\nintroduce another footgun into the bitcoin protocol with address reuse.\nIt's common practice for bitcoin businesses to re-use addresses. Many\nexchanges [1] reuse addresses for cold storage with very large sums of\nmoney that is stored in these addreses.\n\nIt is my understanding with this part of BIP118\n\n>Using NOINPUT the input containing the signature no longer references a\nspecific output. Any participant can take a transaction and rewrite it by\nchanging the hash reference to the previous output, without invalidating\nthe signatures. This allows transactions to be bound to any output that\nmatches the value committed to in the witness and whose witnessProgram,\ncombined with the spending transaction's witness returns true.\n\nif an exchange were to once produce a digital signature from that cold\nstorage address with a SIGHASH_NOINPUT signature, that signature can be\nreplayed again and again on the blockchain until their wallet is drained.\nThis might be able to mitigated since the signatures commit to outputs,\nwhich may be small in value for the transaction that SIGHASH_NOINPUT was\nused. This means that an exchange could move coins from the address with a\nlarger transaction that spends money to a new output (and presumably pays a\nhigher fee than the smaller transactions).\n\n### Why does this matter?\n\nIt seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain\nprotocols like Lightning. This gives us the building blocks for enforcing\nspecific offchain states to end up onchain [2].\n\nSince this tool is useful, we can presume that it will be integrated into\nthe signing path of large economic entities in bitcoin -- namely exchanges.\nMany exchanges have specific signing procedures for transactions that are\nleaving an exchange that is custom software. Now -- presuming wide adoption\nof off chain protocols -- they will need to have a _second unique signing\npath that uses SIGHASH_NOINPUT_.\n\nIt is imperative that this second signing path -- which uses\nSIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that\ncontrols an exchanges onchain funds. If this were to happen, fund lost\ncould occur if the exchange is reusing address, which seems to be common\npractice.\n\nThis is stated here in BIP118:\n\n>This also means that particular care has to be taken in order to avoid\nunintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be\nused, unless it is explicitly needed for the application, e.g., it MUST NOT\nbe a default signing flag in a wallet implementation. Rebinding is only\npossible when the outputs the transaction may bind to all use the same\npublic keys. Any public key that is used in a NOINPUT signature MUST only\nbe used for outputs that the input may bind to, and they MUST NOT be used\nfor transactions that the input may not bind to. For example an application\nSHOULD generate a new key-pair for the application instance using NOINPUT\nsignatures and MUST NOT reuse them afterwards.\n\nThis means we need to encourage onchain hot wallet signing procedures to be\nkept separate from offchain hot wallet signing procedures, which introduces\nmore complexity for key management (two keychains).\n\nOne (of the few) upsides of the current Lightning penalty mechanism is that\nfund loss can be contained to balance of the channel. You cannot do\nsomething in the current protocol that will effect your funds outside of\nthat channel. With SIGHASH_NOINPUT, that property changes.\n\n### A side note\nIn general, i think we should start disallowing uses of the SIGHASH\nprotocols that have unexpected behavior. The classic example of this is\nSIGHASH_SINGLE [3]. I get uneasy about adding more footguns to the\nprotocol, which with current network behavior (address re-use)\nSIGHASH_NOINPUT would be a big one.\n\n\n[1] - https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n[2] -\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002136.html\n[3] -\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016048.html\n\nOn Mon, Sep 30, 2019 at 9:24 AM Christian Decker via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> With the recently renewed interest in eltoo, a proof-of-concept\n> implementation\n> [1], and the discussions regarding clean abstractions for off-chain\n> protocols\n> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n>\n> (sorry for the long e-mail. I wanted to give enough context and describe\n> the\n> various tradeoffs so people don't have to stitch them together from\n> memory. If\n> you're impatient there are a couple of open questions at the bottom)\n>\n> Both proposals are ways to allow rebinding of transactions to new outputs,\n> by\n> adding a sighash flag that excludes the output when signing. This allows\n> the\n> transaction to be bound to any output, without needing a new signature, as\n> long as output script and input script are compatible, e.g., the signature\n> matches the public key specified in the output.\n>\n> BIP-118 is limited to explaining the details of signature verification, and\n> omits anything related to deployment and dependency on other proposals.\n> This\n> was done in order not to depend on bip-taproot which is also in draft-phase\n> currently, and to allow deployment alongside the next version of segwit\n> script. `bip-anyprevout` builds on top of BIP-118, adding integration with\n> `bip-taproot`, chaperone signatures, limits the use of the sighash flag to\n> script path spends, as well as a new pubkey serialization which uses the\n> first\n> byte to signal opt-in.\n>\n> I'd like to stress that both proposals are complementary and not competing,\n> which is something that I've heard a couple of times.\n>\n> There remain a couple of unclear points which I hope we can address in the\n> coming days, to get this thing moving again, and hopefully get a new tool\n> in\n> our toolbox soon(ish).\n>\n> In the following I will quote a couple of things that were discussed during\n> the CoreDev meeting earlier this year, but not everybody could join, and\n> it is\n> important that we engage the wider community, to get a better picture, and\n> I\n> think not everybody is up-to-date about the current state.\n>\n>\n> ## Dangers of `sighash_noinput`\n>\n> An argument I have heard against noinput is that it is slightly less\n> complex\n> or compute intensive than `sighash_all` signatures, which may encourage\n> wallet\n> creators to only implement the noinput variant, and use it indiscrimi-\n> nately. This is certainly a good argument, and indeed we have seen at least\n> one developer proposing to use noinput for all transactions to discourage\n> address reuse.\n>\n> This was also mentioned at CoreDev [6]:\n>\n> > When [...] said he wanted to write a wallet that only used\n> SIGHASH\\_NOINPUT,\n> > that was pause for concern. Some people might want to use\n> SIGHASH\\_NOINPUT as a\n> > way to cheapen or reduce the complexity of making a wallet\n> > implementation. SIGHASH\\_NOINPUT is from a purely procedural point of\n> view\n> > easier than doing a SIGHASH\\_ALL, that's all I'm saying. So you're\n> hashing\n> > less. It's way faster. That concern has been brought to my attention and\n> it's\n> > something I can see. Do we want to avoid people being stupid and shooting\n> > themselves and their customers in the foot? Or do we treat this as a\n> special\n> > case where you mark we're aware of how it should be used and we just try\n> to\n> > get that awareness out?\n>\n> Another issue that is sometimes brought up is that an external user may\n> attempt to send funds to a script that was really part of a higher-level\n> protocol. This leads to those funds becoming inaccessible unless you gather\n> all the participants and sign off on those funds. I don't believe this is\n> anything new, and if users really want to shoot themselves in the foot and\n> send funds to random addresses they fish out of a blockexplorer there's\n> little\n> we can do. What we could do is make the scripts used internally in our\n> protocols unaddressable (see output tagging below), removing this issue\n> altogether.\n>\n>\n> ## Chaperone signatures\n>\n> Chaperone signatures are signatures that ensure that there is no\n> third-party\n> malleability of transactions. The idea is to have an additional signature,\n> that doesn't use noinput, or any of its variants, and therefore needs to be\n> authored by one of the pubkeys in the output script, i.e., one or more of\n> the\n> participants of the contract the transaction belongs to. Concretely in\n> eltoo\n> we'd be using a shared key known to all participants in the eltoo\n> instance, so\n> any participant can sign an update to rebind it to the desired output.\n>\n> Chaperone signatures have a number of downsides however:\n>\n> -   Additional size: both the public key and the signature actually need\n> to be\n>     stored along with the real noinput signature, resulting in transfer,\n>     computational and storage overhead. We can't reuse the same pubkey\n> from the\n>     noinput signature since that'd require access to the matching privkey\n> which\n>     is what we want to get rid of using noinput in the first place.\n> -   Protocols can still simply use a globally known privkey, voiding the\n>     benefit of chaperone signatures, since third-parties can sign again. I\n>     argue that third-party malleability is a subset of first-party\n>     malleability, and we should protect against first-party malleability\n> first\n>     and foremost. My counterparty has the incentive to trick me, a\n> third-party\n>     may not.\n>\n> On the plus side chaperone signatures certainly address the lazy-wallet-dev\n> scenario, and as AJ points out in [bip-anyprevout] we get back the same\n> security guarantees as we had without noinput.\n>\n> From what I remember and the transcript (thanks Kanzure for your awesome\n> work\n> by the way), there was no strong support for chaperone signatures during\n> the\n> meeting [6], but feedback from people that were not present is needed:\n>\n> > if everyone who wanted to use NOINPUT was convinced there was a problem,\n> then\n> > they would pick the right thing, but clearly people aren't. It's not a\n> > foot-gun defense mechanism because it's easily bypassed, and it's easier\n> to\n> > bypass it than to use it. Whereas for tagged outputs, it's that if you\n> want\n> > any NOINPUT then you must tag.\n>\n>\n> ## Output tagging\n>\n> One proposal that I found rather fascinating during the discussion in\n> Amsterdam was that we could achieve the same disincentive to use on\n> non-smart-contract cases by simply making the output scripts\n> unaddressable. This can be done by specifying a version of taproot outputs\n> for\n> which the bech32 addressing scheme simply doesn't have a representation\n> [6]:\n>\n> > The tagged outputs idea is that we don't have NOINPUT ANYPREVOUT\n> supported for\n> > taproot v1 outputs, instead we have a segwit version 16 v16 that supports\n> > taproot. The reason for v16 is that we redefine bech32 to not cover\n> > v16. There's no addresses for this type of output. If you're an exchange\n> and\n> > receive a bech32 address, you declare it invalid. You make it less user\n> > friendly here; and there shouldn't be an address anyway. You might want\n> to see\n> > it on a block explorer, but you don't want to pass it around to anyone.\n>\n> We don't need addresses in our contract constructions because we deal\n> directly\n> with the scripts. This would also have the desired effect of no allowing\n> generic wallets to send to these addresses, or users accidentally sending\n> funds to what was supposed to be a one-off script used internally in the\n> off-chain contract.\n>\n> Notice that this idea was already used by Russell O'Connor when performing\n> a\n> transaction on elements using his new scripting language simplicity\n> [7]:\n>\n> > For this experimental development, we created an improper segwit version,\n> > \"version 31\" for Simplicity addresses. The payload of this segwit\n> version 31\n> > address contains a commitment Merkle root of a Simplicity program to\n> control\n> > the UTXO.\n>\n> The concern with output tagging is that it hurts fungibility, marking\n> outputs\n> used in a contract as such and making them identifiable. But maybe it\n> would be\n> a good idea to create two domains anyway: one for user-addressable\n> destinations which users can use with their general purpose wallets, and\n> one\n> domain for contracts, which users cannot send to directly.\n>\n> This also came up during the CoreDev meeting [ams-coredev]:\n>\n> > these sort of NOINPUT signatures are only things that are within some\n> > application or within some protocol that gets negotiated between\n> participants,\n> > but they don't cross-independent domains where you see a wallet or a\n> protocol\n> > as a kind of domain. You can't tell the difference, is this an address I\n> can\n> > give to someone else or not? It's all scripts, no real addresses. There\n> are\n> > types of outputs that are completely insecure unconditionally; there are\n> > things that are protected and I can give to anyone, you don't want to\n> reuse\n> > it, but there's no security issue from doing so. This is an additional\n> class\n> > that is secure perfectly but only when used in the right way.\n>\n>\n> ## Open questions\n>\n> The questions that remain to be addressed are the following:\n>\n> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>     anyprevout. While at the CoreDev meeting I think everybody agreed that\n>     these proposals a useful, also beyond eltoo, not everybody could be\n>     there. I'd therefore like to elicit some feedback from the wider\n> community.\n> 2.  Is there strong support or opposition to the chaperone signatures\n>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n>     formulate a concrete set of pros and contras, rather than talk about\n>     abstract dangers or advantages.\n> 3.  The same for output tagging / explicit opt-in. What are the advantages\n> and\n>     disadvantages?\n> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>     confusion and make for simpler discussions in the end.\n> 5.  Anything I forgot to mention :-)\n>\n> Cheers,\n> Christian\n>\n> [1] <\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002131.html\n> >\n> [2] <\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017285.html\n> >\n> [3] <\n> https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-August/001383.html\n> >\n> [4] <https://github.com/bitcoin/bips/blob/master/bip-0118.mediawiki>\n> [5] <\n> https://github.com/ajtowns/bips/blob/bip-anyprevout/bip-anyprevout.mediawiki\n> >\n> [6] <\n> http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2019-06-06-noinput-etc/\n> >\n> [7] <https://lists.ozlabs.org/pipermail/simplicity/2019/000018.html>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191001/6260f1ba/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-01T14:20:25",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> I rather strongly oppose output tagging.\n>\n> The entire point of for example Taproot was to reduce the variability\n> of how outputs look like, so that unspent Taproot outputs look exactly\n> like other unspent Taproot outputs regardless of the SCRIPT (or lack\n> of SCRIPT) used to protect the outputs.  That is the reason why we\n> would prefer to not support P2SH-wrapped Taproot even though\n> P2SH-wrapping was intended to cover all future uses of SegWit,\n> including SegWit v1 that Taproot will eventually get.\n\nThat is a bit reductive if you ask me. Taproot brings a number of\nimprovements such as the reduction of on-chain footprint in the\ncollaborative spend case, the hiding of complex logic in that case, and\nyes, the uniformity of UTXOs that you mentioned. I do agree that it'd be\nto make everything look identical to the outside observer, but saying\nthat separating outputs into two coarse-grained domains is equivalent to\nthrowing the baby out with the bath-water :-)\n\nThat being said, I should clarify that I would prefer not having to make\nspecial accomodations on top of the raw sighash_noinput proposal, for\nsome perceived, but abstract danger that someone might shoot themselves\nin the foot. I think we're all old enough not to need too much\nhandholding :-)\n\nOutput tagging is my second choice, since it minimizes the need for\npeople to get creative to work around other proposals, and minimizes the\non-chain footprint, and finally chaperone signatures are my least\npreferred option due to its heavy-handed nature and the increased cost.\n\n> Indeed, if it is output tagging that gets into Bitcoin base layer, I\n> would strongly suggest the below for all Decker-Russell-Osuntokun\n> implementations:\n>\n> * A standard MuSig 2-of-2 bip-schnorr SegWit v1 Funding Transaction Output, confirmed onchain\n> * A \"translator transaction\" spending the above and paying out to a SegWit v16 output-tagged output, kept offchain.\n> * Decker-Russell-Osuntokun update transaction, signed with `SIGHASH_NOINPUT` spending the translator transaction output.\n> * Decker-Russell-Osuntokun state transaction, signed with `SIGHASH_NOINPUT` spending the update transaction output.\n\nThat is very much how I was planning to implement it anyway, using a\ntrigger transaction to separate timeout start and the actual\nupdate/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo\nthere shouldn't be an issue here :-)\n\n> The point regarding use of a commonly-known privkey to work around\n> chaperone signatures is appropriate to the above, incidentally.  In\n> short: this is a workaround, plain and simple, and one wonders the\n> point of adding *either* chaperones *or* output tagging if we will, in\n> practice, just work around them anyway.\n\nExactly, why introduce the extra burden of chaperone signatures or\noutput tagging if we're just going to sidestep it?\n\n> Again, the *more* important point is that special blockchain\n> constructions should only be used in the \"bad\" unilateral close case.\n> In the cooperative case, we want to use simple plain\n> bip-schnorr-signed outputs getting spent to further bip-schnor/Taproot\n> SegWit v1 addresses, to increase the anonymity set of all uses of\n> Decker-Russell-Osuntokun and other applications that might use\n> `SIGHASH_NOINPUT` in some edge case (but which resolve down to simple\n> bip-schnorr-signed n-of-n cases when the protocol is completed\n> successfully by all participants).\n\nWhile I do agree that we should keep outputs as unidentifiable as\npossible, I am starting to question whether that is possible for\noff-chain payment networks since we are gossiping about the existence of\nchannels and binding them to outpoints to prove their existence anyway.\n\nNot the strongest argument I know, but there's little point in talking\nideal cases when we need to weaken that later again. \n\n>> Open questions\n>>\n>> ---------------\n>>\n>> The questions that remain to be addressed are the following:\n>>\n>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout. While at the CoreDev meeting I think everybody agreed that\n>>     these proposals a useful, also beyond eltoo, not everybody could be\n>>     there. I'd therefore like to elicit some feedback from the wider community.\n>\n> I strongly agree that `NOINPUT` is useful, and I was not able to attend CoreDev (at least, not with any human fleshbot already known to you --- I checked).\n\nGreat, good to know that I'm not shouting into the void, and that I'm\nnot just that crazy guy trying to get his hairbrained scheme to work :-)\n\n>> 2.  Is there strong support or opposition to the chaperone signatures\n>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n>>     formulate a concrete set of pros and contras, rather than talk about\n>>     abstract dangers or advantages.\n>\n> No opposition, we will just work around this by publishing a common\n> known private key to use for all chaperone signatures, since all the\n> important security is in the `NOINPUT` signature anyway.\n>\n>>\n>> 3.  The same for output tagging / explicit opt-in. What are the advantages and\n>>     disadvantages?\n>\n> Strongly oppose, see above about my argument.\n>\n>>\n>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n> Ambivalent, mildly support.\n>\n>>\n>> 5.  Anything I forgot to mention :-)\n>\n> Cats are very interesting creatures, and are irrelevant to `SIGHASH_NOINPUT` discussion, but are extremely cute nonetheless.\n\nDefinitely agreed :+1:"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-01T15:35:34",
                "message_text_only": "Good morning Christian,\n\n> > -   A standard MuSig 2-of-2 bip-schnorr SegWit v1 Funding Transaction Output, confirmed onchain\n> > -   A \"translator transaction\" spending the above and paying out to a SegWit v16 output-tagged output, kept offchain.\n> > -   Decker-Russell-Osuntokun update transaction, signed with `SIGHASH_NOINPUT` spending the translator transaction output.\n> > -   Decker-Russell-Osuntokun state transaction, signed with `SIGHASH_NOINPUT` spending the update transaction output.\n>\n> That is very much how I was planning to implement it anyway, using a\n> trigger transaction to separate timeout start and the actual\n> update/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo\n> there shouldn't be an issue here :-)\n\nMy understanding is that a trigger transaction is not in fact necessary for Decker-Russell-Osuntokun: any update transaction could spend the funding transaction output directly, and thereby start the relative timelock.\nAt least, if we could arrange the funding transaction output to be spendable directly using `SIGHASH_NOINPUT` or variants thereof.\n\n\n> > Again, the more important point is that special blockchain\n> > constructions should only be used in the \"bad\" unilateral close case.\n> > In the cooperative case, we want to use simple plain\n> > bip-schnorr-signed outputs getting spent to further bip-schnor/Taproot\n> > SegWit v1 addresses, to increase the anonymity set of all uses of\n> > Decker-Russell-Osuntokun and other applications that might use\n> > `SIGHASH_NOINPUT` in some edge case (but which resolve down to simple\n> > bip-schnorr-signed n-of-n cases when the protocol is completed\n> > successfully by all participants).\n>\n> While I do agree that we should keep outputs as unidentifiable as\n> possible, I am starting to question whether that is possible for\n> off-chain payment networks since we are gossiping about the existence of\n> channels and binding them to outpoints to prove their existence anyway.\n\n* Lightning supports unpublished channels, so we do not gossip some outpoints even though they are in fact channels underneath.\n  * I confess the existence of unpublished channels in the spec fails to summon any reaction other than incredulity from me, but they exist nonetheless, my incredulity notwithstanding.\n* Historical channels that have been cooperatively closed are no longer normally gossiped, so the fact that they used to be channels is no longer widely broadcast, and may eventually be forgotten by most or all of the network.\n  * This means anyone who wants to record the historical use of Lightning will have to retain the information themselves, rather than delegating it to fullnodes everywhere.\n\n>\n> Not the strongest argument I know, but there's little point in talking\n> ideal cases when we need to weaken that later again.\n\nThe point of ideal cases is to strive to approach them, not necessarily achieve them.\nJust as a completely unbiased rational reasoner is almost impossible to achieve, does not mean we should give up all attempts to reduce bias.\n\nOutpoints that used to be channels, but have now been closed using cooperative closes, will potentially no longer be widely gossiped as having once been channels, thus it may happen that they will eventually be forgotten by most of the network as once having been channels.\nBut if the outpoints of those channels are specially marked, then that cannot be forgotten, as the initial block download thereafter will have that history indelibly etched forevermore.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T09:42:00",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n>> That is very much how I was planning to implement it anyway, using a\n>> trigger transaction to separate timeout start and the actual\n>> update/settlement pairs (cfr. eltoo paper Section 4.2). So for eltoo\n>> there shouldn't be an issue here :-)\n>\n> My understanding is that a trigger transaction is not in fact\n> necessary for Decker-Russell-Osuntokun: any update transaction could\n> spend the funding transaction output directly, and thereby start the\n> relative timelock.  At least, if we could arrange the funding\n> transaction output to be spendable directly using `SIGHASH_NOINPUT` or\n> variants thereof.\n\nThis is the case in which we don't have a pre-signed settlement\ntransaction (or in this case refund transaction) that uses a relative\ntimelock. In order to have a refund transaction we would need to have\nthe first update and settlement pair be signed before funding (otherwise\nthe funder isn't sure she is getting her funds back). Since that first\nupdate and settlement pair do not need to be rebound (they can only ever\nbe bound to the funding transaction) they can be signed without\nnoinput/anyprevoutanyscript. If we use output tagging we would mandate\nthat this first update must be published, so that the funding output is\nindistinguishable from a normal output, and the first update switches\nfrom non-noinput/anyprevoutanyscript to enabling it. Collaborative\ncloses are still indistinguishable, unilateral closes require the\nswitch, but then would be identifiable anyway.\n\nThe one downside I can see is that we now mandate that unilateral closes\nalso publish the first update, which is a bit annoying.\n\n>> While I do agree that we should keep outputs as unidentifiable as\n>> possible, I am starting to question whether that is possible for\n>> off-chain payment networks since we are gossiping about the existence of\n>> channels and binding them to outpoints to prove their existence anyway.\n>\n> * Lightning supports unpublished channels, so we do not gossip some outpoints even though they are in fact channels underneath.\n>   * I confess the existence of unpublished channels in the spec fails to summon any reaction other than incredulity from me, but they exist nonetheless, my incredulity notwithstanding.\n\nThat is true, we do however selectively tell others about the channel's\nexistence (in invoices, our peers, ...) so I wouldn't consider that to\nbe the most secret information :-)\n\nAs for why they exist: nodes need to have the option of not announcing\ntheir channels to reduce the noise in the network with channels that are\nunlikely to be useable in order to forward payments. If every node were\nto announce their channels we'd have a much larger routing table, mostly\nconsisting of unusable channels going to leafs in the\nnetwork. Furthermore, the sheer threat that there might be unannounced\nchannels adds uncertainty for attackers trying to profile nodes: \"I see\nonly my channel with my peer, but he might have unannounced channels, so\nI can't really tell whether the payment I forwarded to it is destined\nfor it or one of its unannounced peers\".\n\n> * Historical channels that have been cooperatively closed are no longer normally gossiped, so the fact that they used to be channels is no longer widely broadcast, and may eventually be forgotten by most or all of the network.\n>   * This means anyone who wants to record the historical use of Lightning will have to retain the information themselves, rather than delegating it to fullnodes everywhere.\n\nGood point, it requires storing the ephemeral data from gossip, that's\nnot all that hard, but I agree that it puts up a small barrier for\nnewcomers."
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-01T14:26:39",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> To elucidate further ---\n>\n> Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode,\n> `OP_CHECKSIG_WITHOUT_INPUT`.\n>\n> This new opcode ignores any `SIGHASH` flags, if present, on a\n> signature, but instead hashes the current transaction without the\n> input references, then checks that hash to the signature.\n>\n> This is equivalent to `SIGHASH_NOINPUT`.\n>\n> Yet as an opcode, it would be possible to embed in a Taproot script.\n>\n> For example, a Decker-Russell-Osuntokun would have an internal Taproot\n> point be a 2-of-2, then have a script `OP_1\n> OP_CHECKSIG_WITHOUT_INPUT`.  Unilateral closes would expose the hidden\n> script, but cooperative closes would use the 2-of-2 directly.\n>\n> Of note, is that any special SCRIPT would already be supportable by Taproot.\n> This includes SCRIPTs that may potentially lose funds for the user.\n> Yet such SCRIPTs are already targetable by a Taproot address.\n>\n> If we are so concerned about `SIGHASH_NOINPUT` abuse, why are we not\n> so concerned about Taproot abuse?\n\nThat would certainly be another possibility, which I have not explored\nin detail so far. Due to the similarity between the various signature\nchecking op-codes it felt that it should be a sighash flag, and it\nneatly slotted into the already existing flags. If we go for a separate\nopcode we might end up reinventing the wheel, and to be honest I feared\nthat proposing a new opcode would get us into bikeshedding territory\n(which I apparently failed to avoid with the sighash flag anyway...).\n\nThe advantage would be that with the sighash flag the spender is in\ncharge of specifying the flags, whereas with an opcode the output\ndictates the signature verification modalities. The downside is the\nincreased design space.\n\nWhat do others think? Would this be an acceptable opt-in mechanism that\naddresses the main concerns?\n\nCheers,\nChristian"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-01T14:45:48",
                "message_text_only": "On Mon, Sep 30, 2019 at 11:28:43PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode, `OP_CHECKSIG_WITHOUT_INPUT`.\n\nI don't think there's any meaningful difference between making a new\nopcode and making a new tapscript public key type; the difference is\njust one of encoding:\n\n   3301<key>AC   [CHECKSIG of public key type 0x01]\n   32<key>B3     [CHECKSIG_WITHOUT_INPUT (replacing NOP4) of key]\n\n> This new opcode ignores any `SIGHASH` flags, if present, on a signature,\n\n(How sighash flags are treated can be redefined by new public key types;\nif that's not obvious already)\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-01T15:42:08",
                "message_text_only": "Good morning aj,\n\n\n> On Mon, Sep 30, 2019 at 11:28:43PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > Suppose rather than `SIGHASH_NOINPUT`, we created a new opcode, `OP_CHECKSIG_WITHOUT_INPUT`.\n>\n> I don't think there's any meaningful difference between making a new\n> opcode and making a new tapscript public key type; the difference is\n> just one of encoding:\n>\n> 3301<key>AC [CHECKSIG of public key type 0x01]\n> 32<key>B3 [CHECKSIG_WITHOUT_INPUT (replacing NOP4) of key]\n>\n> > This new opcode ignores any `SIGHASH` flags, if present, on a signature,\n>\n> (How sighash flags are treated can be redefined by new public key types;\n> if that's not obvious already)\n\n\nThank you for this thought,\nI believe under tapscript v0 we can give `OP_1` as the public key to `OP_CHECKSIG` to mean to reuse the internal Taproot pubkey, would it be possible to have some similar mechanism here, to copy the internal Taproot pubkey but also to enable new `SIGHASH` flag for this particular script only?\n\nThis seems fine, as then a Decker-Russell-Osuntokun funding tx output between nodes A, B, and C would have:\n\n* Taproot internal key: `P = MuSig(A, B, C)`\n* Script 1: leaf version 0, `<MuSig(A,B,C) + pubkeytype 1> OP_CHECKSIG`\n\nThen, update transactions could use `MuSig(A,B,C)` for signing along the \"update\" path, with unique \"state\" keys.\nAnd cooperative closes would sign using `P + h(P | MAST(<MuSig(A,B,C) + pubkeytype 1> OPCHECKSIG)) * G`, not revealing the fact that this was in fact a Decker-Russell-Osuntokun output.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-01T15:59:29",
                "message_text_only": "On Mon, Sep 30, 2019 at 03:23:56PM +0200, Christian Decker via bitcoin-dev wrote:\n> With the recently renewed interest in eltoo, a proof-of-concept implementation\n> [1], and the discussions regarding clean abstractions for off-chain protocols\n> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n\nHey Christian, thanks for the write up!\n\n> ## Open questions\n> The questions that remain to be addressed are the following:\n> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>     anyprevout[?]\n> 2.  Is there strong support or opposition to the chaperone signatures[?]\n> 3.  The same for output tagging / explicit opt-in[?]\n> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>     confusion and make for simpler discussions in the end.\n\nI think there's an important open question you missed from this list:\n(1.5) do we really understand what the dangers of noinput/anyprevout-style\nconstructions actually are?\n\nMy impression on the first 3.5 q's is: (1) yes, (1.5) not really,\n(2) weak opposition for requiring chaperone sigs, (3) mixed (weak)\nsupport/opposition for output tagging.\n\nMy thinking at the moment (subject to change!) is:\n\n * anyprevout signatures make the address you're signing for less safe,\n   which may cause you to lose funds when additional coins are sent to\n   the same address; this can be avoided if handled with care (or if you\n   don't care about losing funds in the event of address reuse)\n\n * being able to guarantee that an address can never be signed for with\n   an anyprevout signature is therefore valuable; so having it be opt-in\n   at the tapscript level, rather than a sighash flag available for\n   key-path spends is valuable (I call this \"opt-in\", but it's hidden\n   until use via taproot rather than \"explicit\" as output tagging\n   would be)\n\n * receiving funds spent via an anyprevout signature does not involve any\n   qualitatively new double-spending/malleability risks.\n   \n   (eltoo is unavoidably malleable if there are multiple update\n   transactions (and chaperone signatures aren't used or are used with\n   well known keys), but while it is better to avoid this where possible,\n   it's something that's already easily dealt with simply by waiting\n   for confirmations, and whether a transaction is malleable is always\n   under the control of the sender not the receiver)\n\n * as such, output tagging is also unnecessary, and there is also no\n   need for users to mark anyprevout spends as \"tainted\" in order to\n   wait for more confirmations than normal before considering those funds\n   \"safe\"\n\nI think it might be good to have a public testnet (based on Richard Myers\net al's signet2 work?) where we have some fake exchanges/merchants/etc\nand scheduled reorgs, and demo every weird noinput/anyprevout case anyone\ncan think of, and just work out if we need any extra code/tagging/whatever\nto keep those fake exchanges/merchants from losing money (and write up\nthe weird cases we've found in a wiki or a paper so people can easily\ntell if we missed something obvious).\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-02T02:03:43",
                "message_text_only": "Good morning lists,\n\nLet me propose the below radical idea:\n\n* `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n  * 1 RETURN\n  * higher-`nSequence` replacement\n  * DER-encoded pubkeys\n  * unrestricted `scriptPubKey`\n  * Payee-security-paid-by-payer (i.e. lack of P2SH)\n  * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n  * transaction malleability\n  * probably many more\n\nSo let me propose the more radical excision, starting with SegWit v1:\n\n* Remove `SIGHASH` from signatures.\n* Put `SIGHASH` on public keys.\n\nPublic keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n`OP_CHECKSIG` and friends then look at the *public key* to determine sighash algorithm rather than the signature.\n\nAs we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\nHowever, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n\nI propose also the addition of the opcode:\n\n    <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n\n* `sighash` must be one byte.\n* `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n* `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n* `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n* If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n\nThis retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\nThis is done by using the script:\n\n    <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n\nThen the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\nThis is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n\nHowever, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n\nThis removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n\nWould this not be a superior solution?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "s7r",
                "date": "2019-10-02T15:11:25",
                "message_text_only": "Anthony Towns via bitcoin-dev wrote:\n[SNIP]\n> \n> My thinking at the moment (subject to change!) is:\n> \n>  * anyprevout signatures make the address you're signing for less safe,\n>    which may cause you to lose funds when additional coins are sent to\n>    the same address; this can be avoided if handled with care (or if you\n>    don't care about losing funds in the event of address reuse)\n> \n\nIt's not necessarily like this. Address re-use is many times OUTSIDE the\ncontrol of the address owner. Say I give my address to a counterparty.\nThey send me a transaction which I successfully spend. So far so good.\n\nAfter that, I have no control over that counterparty. If they decide to\nre-use that address, it does not mean I wanted to re-use it and it also\ndoes not mean that I don't care about those funds being lost.\n\nThis could create a lot of problems in the industry and I think it\nshould be avoided. Address re-use has been strongly discouraged ever\nsince I can remember, and all (proper) wallet implementations try as\nhard as possible to enforce it, but it's not always possible. A\ncounterparty that decides to re-use an address, either accidentally or\nnot, is not under the control of the user who handed out the address in\nthe first place.\n\nThere are also a lot of use cases with P2SH addresses that are some\nsmart contracts particularly designed to be re-used multiple times over\ntime.\n\nMy 2 cents are that this is not a good way to go. If you try to index\nthe entire blockchain until now you'll see that address re-use is more\ncommon than we'd want it to be and there's no clear way to prevent this\nfrom further happening without hurting the economic interests of the users.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191002/282e8910/attachment.sig>"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T11:08:29",
                "message_text_only": "Anthony Towns <aj at erisian.com.au> writes:\n\n> On Mon, Sep 30, 2019 at 03:23:56PM +0200, Christian Decker via bitcoin-dev wrote:\n>> With the recently renewed interest in eltoo, a proof-of-concept implementation\n>> [1], and the discussions regarding clean abstractions for off-chain protocols\n>> [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n>> (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n>\n> Hey Christian, thanks for the write up!\n>\n>> ## Open questions\n>> The questions that remain to be addressed are the following:\n>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout[?]\n>> 2.  Is there strong support or opposition to the chaperone signatures[?]\n>> 3.  The same for output tagging / explicit opt-in[?]\n>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n> I think there's an important open question you missed from this list:\n> (1.5) do we really understand what the dangers of noinput/anyprevout-style\n> constructions actually are?\n>\n> My impression on the first 3.5 q's is: (1) yes, (1.5) not really,\n> (2) weak opposition for requiring chaperone sigs, (3) mixed (weak)\n> support/opposition for output tagging.\n>\n> My thinking at the moment (subject to change!) is:\n>\n>  * anyprevout signatures make the address you're signing for less safe,\n>    which may cause you to lose funds when additional coins are sent to\n>    the same address; this can be avoided if handled with care (or if you\n>    don't care about losing funds in the event of address reuse)\n>\n>  * being able to guarantee that an address can never be signed for with\n>    an anyprevout signature is therefore valuable; so having it be opt-in\n>    at the tapscript level, rather than a sighash flag available for\n>    key-path spends is valuable (I call this \"opt-in\", but it's hidden\n>    until use via taproot rather than \"explicit\" as output tagging\n>    would be)\n>\n>  * receiving funds spent via an anyprevout signature does not involve any\n>    qualitatively new double-spending/malleability risks.\n>    \n>    (eltoo is unavoidably malleable if there are multiple update\n>    transactions (and chaperone signatures aren't used or are used with\n>    well known keys), but while it is better to avoid this where possible,\n>    it's something that's already easily dealt with simply by waiting\n>    for confirmations, and whether a transaction is malleable is always\n>    under the control of the sender not the receiver)\n>\n>  * as such, output tagging is also unnecessary, and there is also no\n>    need for users to mark anyprevout spends as \"tainted\" in order to\n>    wait for more confirmations than normal before considering those funds\n>    \"safe\"\n\nExcellent points, I had missed the hidden nature of the opt-in via\npubkey prefix while reading your proposal. I'm starting to like that\noption more and more. In that case we'd only ever be revealing that we\nopted into anyprevout when we're revealing the entire script anyway, at\nwhich point all fungibility concerns go out the window anyway.\n\nWould this scheme be extendable to opt into all sighash flags the\noutpoint would like to allow (e.g., adding opt-in for sighash_none and\nsighash_anyonecanpay as well)? That way the pubkey prefix could act as a\nmask for the sighash flags and fail verification if they don't match.\n\n> I think it might be good to have a public testnet (based on Richard Myers\n> et al's signet2 work?) where we have some fake exchanges/merchants/etc\n> and scheduled reorgs, and demo every weird noinput/anyprevout case anyone\n> can think of, and just work out if we need any extra code/tagging/whatever\n> to keep those fake exchanges/merchants from losing money (and write up\n> the weird cases we've found in a wiki or a paper so people can easily\n> tell if we missed something obvious).\n\nThat'd be great, however even that will not ensure that every possible\ncorner case is handled and from experience it seems that people are\nunwilling to invest a lot of time testing on a network unless their\nmoney is on the line. That's not to say that we shouldn't try, we\nabsolutely should, I'm just not sure it alone is enough to dispell all\nremaining doubts :-)\n\nCheers,\nChristian"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-05T10:06:15",
                "message_text_only": "On Thu, Oct 03, 2019 at 01:08:29PM +0200, Christian Decker wrote:\n> >  * anyprevout signatures make the address you're signing for less safe,\n> >    which may cause you to lose funds when additional coins are sent to\n> >    the same address; this can be avoided if handled with care (or if you\n> >    don't care about losing funds in the event of address reuse)\n> Excellent points, I had missed the hidden nature of the opt-in via\n> pubkey prefix while reading your proposal. I'm starting to like that\n> option more and more. In that case we'd only ever be revealing that we\n> opted into anyprevout when we're revealing the entire script anyway, at\n> which point all fungibility concerns go out the window anyway.\n>\n> Would this scheme be extendable to opt into all sighash flags the\n> outpoint would like to allow (e.g., adding opt-in for sighash_none and\n> sighash_anyonecanpay as well)? That way the pubkey prefix could act as a\n> mask for the sighash flags and fail verification if they don't match.\n\nFor me, the thing that distinguishes ANYPREVOUT/NOINPUT as warranting\nan opt-in step is that it affects the security of potentially many\nUTXOs at once; whereas all the other combinations (ALL,SINGLE,NONE\ncross ALL,ANYONECANPAY) still commit to the specific UTXO being spent,\nso at most you only risk somehow losing the funds from the specific UTXO\nyou're working with (apart from the SINGLE bug, which taproot doesn't\nsupport anyway).\n\nHaving a meaningful prefix on the taproot scriptpubkey (ie paying to\n\"[SIGHASH_SINGLE][32B pubkey]\") seems like it would make it a bit easier\nto distinguish wallets, which taproot otherwise avoids -- \"oh this address\nis going to be a SIGHASH_SINGLE? probably some hacker, let's ban it\".\n\n> > I think it might be good to have a public testnet (based on Richard Myers\n> > et al's signet2 work?) where we have some fake exchanges/merchants/etc\n> > and scheduled reorgs, and demo every weird noinput/anyprevout case anyone\n> > can think of, and just work out if we need any extra code/tagging/whatever\n> > to keep those fake exchanges/merchants from losing money (and write up\n> > the weird cases we've found in a wiki or a paper so people can easily\n> > tell if we missed something obvious).\n> That'd be great, however even that will not ensure that every possible\n> corner case is handled [...]\n\nWell, sure. I'm thinking of it more as a *necessary* step than a\n*sufficient* one, though. If we can't demonstrate that we can deal with\nthe theoretical attacks people have dreamt up in a \"laboratory\" setting,\nthen it doesn't make much sense to deploy things in a real world setting,\ndoes it?\n\nI think if it turns out that we can handle every case we can think of\neasily, that will be good evidence that output tagging and the like isn't\nnecessary; and conversely if it turns out we can't handle them easily,\nit at least gives us a chance to see how output tagging (or chaperone\nsigs, or whatever else) would actually work, and if they'd provide any\nmeaningful protection at all. At the moment the best we've got is ideas\nand handwaving...\n\nCheers,\naj"
            }
        ],
        "thread_summary": {
            "title": "Continuing the discussion about noinput / anyprevout",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Stewart",
                "Anthony Towns",
                "s7r",
                "ZmnSCPxj",
                "Christian Decker"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 47162
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "Christian Decker",
                "date": "2019-10-03T09:57:05",
                "message_text_only": "Chris Stewart <chris at suredbits.com> writes:\n\n> I do have some concerns about SIGHASH_NOINPUT, mainly that it does\n> introduce another footgun into the bitcoin protocol with address reuse.\n> It's common practice for bitcoin businesses to re-use addresses. Many\n> exchanges [1] reuse addresses for cold storage with very large sums of\n> money that is stored in these addreses.\n>\n> It is my understanding with this part of BIP118\n>\n>>Using NOINPUT the input containing the signature no longer references a\n> specific output. Any participant can take a transaction and rewrite it by\n> changing the hash reference to the previous output, without invalidating\n> the signatures. This allows transactions to be bound to any output that\n> matches the value committed to in the witness and whose witnessProgram,\n> combined with the spending transaction's witness returns true.\n>\n> if an exchange were to once produce a digital signature from that cold\n> storage address with a SIGHASH_NOINPUT signature, that signature can be\n> replayed again and again on the blockchain until their wallet is drained.\n> This might be able to mitigated since the signatures commit to outputs,\n> which may be small in value for the transaction that SIGHASH_NOINPUT was\n> used. This means that an exchange could move coins from the address with a\n> larger transaction that spends money to a new output (and presumably pays a\n> higher fee than the smaller transactions).\n\nThanks for sharing your concerns Chris, I do agree that noinput and\nfriends are a very sharp knife that needs to be treated carefully, but\nultimately it's exactly its sharpness that makes it useful :-)\n\n> ### Why does this matter?\n>\n> It seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain\n> protocols like Lightning. This gives us the building blocks for enforcing\n> specific offchain states to end up onchain [2].\n>\n> Since this tool is useful, we can presume that it will be integrated into\n> the signing path of large economic entities in bitcoin -- namely exchanges.\n> Many exchanges have specific signing procedures for transactions that are\n> leaving an exchange that is custom software. Now -- presuming wide adoption\n> of off chain protocols -- they will need to have a _second unique signing\n> path that uses SIGHASH_NOINPUT_.\n>\n> It is imperative that this second signing path -- which uses\n> SIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that\n> controls an exchanges onchain funds. If this were to happen, fund lost\n> could occur if the exchange is reusing address, which seems to be common\n> practice.\n\nTotally agreed, and as you point out, BIP118 is careful to mandate\nseparate private keys be used for off-chain contracts and that the\noff-chain contract never be mixed with the remainder of your funds. The\nway eltoo uses noinput we selectively open us up to replay attacks\n(because that's what the update mechanism is after all) by controlling\nthe way the transactions can be replayed very carefully, and any other\nuse of noinput would need to make sure to have the same guarantees.\nHowever, once we have separated the two domains, we can simply use a\nseparate (hardened) derivation path from a seed key, and never mix them\nafterwards. We never exchange any private keys, so even leaking info\nacross derived keys is not an issue here.\n\n> This is stated here in BIP118:\n>\n>>This also means that particular care has to be taken in order to avoid\n> unintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be\n> used, unless it is explicitly needed for the application, e.g., it MUST NOT\n> be a default signing flag in a wallet implementation. Rebinding is only\n> possible when the outputs the transaction may bind to all use the same\n> public keys. Any public key that is used in a NOINPUT signature MUST only\n> be used for outputs that the input may bind to, and they MUST NOT be used\n> for transactions that the input may not bind to. For example an application\n> SHOULD generate a new key-pair for the application instance using NOINPUT\n> signatures and MUST NOT reuse them afterwards.\n>\n> This means we need to encourage onchain hot wallet signing procedures to be\n> kept separate from offchain hot wallet signing procedures, which introduces\n> more complexity for key management (two keychains).\n\nThis is already the case: off-chain systems always require access to the\nsigning key in real-time in order to be useful. If any state change is\nperformed in a channel, even just adjusting fees or receiving a payment,\nrequires the signature from the key associated with the channel. With\nhigh security on-chain systems on the other hand you should never have a\nhot key that automatically signs off on transfers without human\nintervention. So I find it unlikely that mandating the on-chain keys to\nbe kept separate from off-chain keys is any harder than what should be\ndone with the current systems.\n\n> One (of the few) upsides of the current Lightning penalty mechanism is that\n> fund loss can be contained to balance of the channel. You cannot do\n> something in the current protocol that will effect your funds outside of\n> that channel. With SIGHASH_NOINPUT, that property changes.\n\nGood point, but if the key hygiene is maintained as detailed in BIP118,\ni.e., off-chain keys must be kept separate from on-chain keys, and that\neach off-chain contract instance uses a separate set of keys, that\nproperty is maintained.\n\nRegards,\nChristian"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-01T13:31:49",
                "message_text_only": "Good morning lists,\n\nLet me summarize concerns brought up:\n\n* Chris concern, is that an ordinary UTXO that is not allocated for `SIGHASH_NOINPUT` use, is inadvertently spent using `SIGHASH_NOINPUT`.\n* My concern, is that unless a UTXO allocated for `SIGHASH_NOINPUT` use, is *indeed* used with SIGHASH_NOINPUT`, it should look exactly the same as any other SegWit v1 output.\n\nI propose the below instead:\n\n* Do ***NOT*** allocate SegWit v16 for `SIGHASH_NOINPUT`.\n* Instead, allocate SegWit v1 Tapscript v16 for `SIGHASH_NOINPUT`.\n\nThen, on usage:\n\n* Exchange hoards can be protected by simple MuSig bip-schnorr SegWit v1 outputs, or a NUMS Taproot internal point with a MAST branch Tapscript v0 `OP_CHECKSIG_ADD` sequence.\n* Decker-Russell-Osuntokun constructions are backed by a n-of-n MuSig Taproot internal point, with a MAST branch containing a Tapscript v16 with `OP_1 OP_CHECKSIG`.\n\nThis solves both concerns:\n\n* Ordinary UTXOs not allocated for `SIGHASH_NOINPUT` use simply do not commit to any Taproot that has a Tapscript v16 branch, and thus `SIGHASH_NOINPUT` is unuseable to claim it.\n* If a UTXO used for an offchain protocol ends up in a cooperative-resolution state, nobody has to know that a Tapscript v16 branch existed that could have used `SIGHASH_NOINPUT`.\n\nAgain, my objection to output tagging is that it is **publicly visible** as soon as the funding transaction is confirmed onchain that this is a special output used for a Decker-Russell-Osuntokun construction, greatly damaging privacy.\nBut if this fact is kept secret *unless* the very specific case of unilateral uncooperative enforcement, then it is quite fine with me.\n\nWould this alternate proposal hold better muster?\n\nRegards,\nZmnSCPxj\n\n\n\n> I do have some concerns about SIGHASH_NOINPUT, mainly that it does introduce another footgun into the bitcoin protocol with address reuse. It's common practice for bitcoin businesses to re-use addresses. Many exchanges [1] reuse addresses for cold storage with very large sums of money that is stored in these addreses.\n>\n> It is my understanding with this part of BIP118\n>\n> >Using NOINPUT the input containing the signature no longer references a specific output. Any participant can take a transaction and rewrite it by changing the hash reference to the previous output, without invalidating the signatures. This allows transactions to be bound to any output that matches the value committed to in the witness and whose witnessProgram, combined with the spending transaction's witness returns true.\n>\n> if an exchange were to once produce a digital signature from that cold storage address with a SIGHASH_NOINPUT signature, that signature can be replayed again and again on the blockchain until their wallet is drained. This might be able to mitigated since the signatures commit to outputs, which may be small in value for the transaction that SIGHASH_NOINPUT was used. This means that an exchange could move coins from the address with a larger transaction that spends money to a new output (and presumably pays a higher fee than the smaller transactions).\n>\n> ### Why does this matter?\n>\n> It seems that SIGHASH_NOINPUT will be an extremely useful tool for offchain protocols like Lightning. This gives us the building blocks for enforcing specific offchain states to end up onchain [2].\n>\n> Since this tool is useful, we can presume that it will be integrated into the signing path of large economic entities in bitcoin -- namely exchanges. Many exchanges have specific signing procedures for transactions that are leaving an exchange that is custom software. Now -- presuming wide adoption of off chain protocols -- they will need to have a _second unique signing path that uses SIGHASH_NOINPUT_.\n>\n> It is imperative that this second signing path -- which uses SIGHASH_NOINPUT -- does NOT get mixed up with the first signing path that controls an exchanges onchain funds. If this were to happen, fund lost could occur if the exchange is reusing address, which seems to be common practice.\n>\n> This is stated here in BIP118:\n>\n> >This also means that particular care has to be taken in order to avoid unintentionally enabling this rebinding mechanism. NOINPUT MUST NOT be used, unless it is explicitly needed for the application, e.g., it MUST NOT be a default signing flag in a wallet implementation. Rebinding is only possible when the outputs the transaction may bind to all use the same public keys. Any public key that is used in a NOINPUT signature MUST only be used for outputs that the input may bind to, and they MUST NOT be used for transactions that the input may not bind to. For example an application SHOULD generate a new key-pair for the application instance using NOINPUT signatures and MUST NOT reuse them afterwards.\n>\n> This means we need to encourage onchain hot wallet signing procedures to be kept separate from offchain hot wallet signing procedures, which introduces more complexity for key management (two keychains).\n>\n> One (of the few) upsides of the current Lightning penalty mechanism is that fund loss can be contained to balance of the channel. You cannot do something in the current protocol that will effect your funds outside of that channel. With SIGHASH_NOINPUT, that property changes.\n>\n> ### A side note\n> In general, i think we should start disallowing uses of the SIGHASH protocols that have unexpected behavior. The classic example of this is SIGHASH_SINGLE [3]. I get uneasy about adding more footguns to the protocol, which with current network behavior (address re-use) SIGHASH_NOINPUT would be a big one.\n>\n> [1] - https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\n> [2] - https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002136.html\n> [3] - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016048.html\n>\n> On Mon, Sep 30, 2019 at 9:24 AM Christian Decker via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > With the recently renewed interest in eltoo, a proof-of-concept implementation\n> > [1], and the discussions regarding clean abstractions for off-chain protocols\n> > [2,3], I thought it might be time to revisit the `sighash_noinput` proposal\n> > (BIP-118 [4]), and AJ's `bip-anyprevout` proposal [5].\n> >\n> > (sorry for the long e-mail. I wanted to give enough context and describe the\n> > various tradeoffs so people don't have to stitch them together from memory. If\n> > you're impatient there are a couple of open questions at the bottom)\n> >\n> > Both proposals are ways to allow rebinding of transactions to new outputs, by\n> > adding a sighash flag that excludes the output when signing. This allows the\n> > transaction to be bound to any output, without needing a new signature, as\n> > long as output script and input script are compatible, e.g., the signature\n> > matches the public key specified in the output.\n> >\n> > BIP-118 is limited to explaining the details of signature verification, and\n> > omits anything related to deployment and dependency on other proposals. This\n> > was done in order not to depend on bip-taproot which is also in draft-phase\n> > currently, and to allow deployment alongside the next version of segwit\n> > script. `bip-anyprevout` builds on top of BIP-118, adding integration with\n> > `bip-taproot`, chaperone signatures, limits the use of the sighash flag to\n> > script path spends, as well as a new pubkey serialization which uses the first\n> > byte to signal opt-in.\n> >\n> > I'd like to stress that both proposals are complementary and not competing,\n> > which is something that I've heard a couple of times.\n> >\n> > There remain a couple of unclear points which I hope we can address in the\n> > coming days, to get this thing moving again, and hopefully get a new tool in\n> > our toolbox soon(ish).\n> >\n> > In the following I will quote a couple of things that were discussed during\n> > the CoreDev meeting earlier this year, but not everybody could join, and it is\n> > important that we engage the wider community, to get a better picture, and I\n> > think not everybody is up-to-date about the current state.\n> >\n> > ## Dangers of `sighash_noinput`\n> >\n> > An argument I have heard against noinput is that it is slightly less complex\n> > or compute intensive than `sighash_all` signatures, which may encourage wallet\n> > creators to only implement the noinput variant, and use it indiscrimi-\n> > nately. This is certainly a good argument, and indeed we have seen at least\n> > one developer proposing to use noinput for all transactions to discourage\n> > address reuse.\n> >\n> > This was also mentioned at CoreDev [6]:\n> >\n> > > When [...] said he wanted to write a wallet that only used SIGHASH\\_NOINPUT,\n> > > that was pause for concern. Some people might want to use SIGHASH\\_NOINPUT as a\n> > > way to cheapen or reduce the complexity of making a wallet\n> > > implementation. SIGHASH\\_NOINPUT is from a purely procedural point of view\n> > > easier than doing a SIGHASH\\_ALL, that's all I'm saying. So you're hashing\n> > > less. It's way faster. That concern has been brought to my attention and it's\n> > > something I can see. Do we want to avoid people being stupid and shooting\n> > > themselves and their customers in the foot? Or do we treat this as a special\n> > > case where you mark we're aware of how it should be used and we just try to\n> > > get that awareness out?\n> >\n> > Another issue that is sometimes brought up is that an external user may\n> > attempt to send funds to a script that was really part of a higher-level\n> > protocol. This leads to those funds becoming inaccessible unless you gather\n> > all the participants and sign off on those funds. I don't believe this is\n> > anything new, and if users really want to shoot themselves in the foot and\n> > send funds to random addresses they fish out of a blockexplorer there's little\n> > we can do. What we could do is make the scripts used internally in our\n> > protocols unaddressable (see output tagging below), removing this issue\n> > altogether.\n> >\n> > ## Chaperone signatures\n> >\n> > Chaperone signatures are signatures that ensure that there is no third-party\n> > malleability of transactions. The idea is to have an additional signature,\n> > that doesn't use noinput, or any of its variants, and therefore needs to be\n> > authored by one of the pubkeys in the output script, i.e., one or more of the\n> > participants of the contract the transaction belongs to. Concretely in eltoo\n> > we'd be using a shared key known to all participants in the eltoo instance, so\n> > any participant can sign an update to rebind it to the desired output.\n> >\n> > Chaperone signatures have a number of downsides however:\n> >\n> > -   Additional size: both the public key and the signature actually need to be\n> >     stored along with the real noinput signature, resulting in transfer,\n> >     computational and storage overhead. We can't reuse the same pubkey from the\n> >     noinput signature since that'd require access to the matching privkey which\n> >     is what we want to get rid of using noinput in the first place.\n> > -   Protocols can still simply use a globally known privkey, voiding the\n> >     benefit of chaperone signatures, since third-parties can sign again. I\n> >     argue that third-party malleability is a subset of first-party\n> >     malleability, and we should protect against first-party malleability first\n> >     and foremost. My counterparty has the incentive to trick me, a third-party\n> >     may not.\n> >\n> > On the plus side chaperone signatures certainly address the lazy-wallet-dev\n> > scenario, and as AJ points out in [bip-anyprevout] we get back the same\n> > security guarantees as we had without noinput.\n> >\n> > From what I remember and the transcript (thanks Kanzure for your awesome work\n> > by the way), there was no strong support for chaperone signatures during the\n> > meeting [6], but feedback from people that were not present is needed:\n> >\n> > > if everyone who wanted to use NOINPUT was convinced there was a problem, then\n> > > they would pick the right thing, but clearly people aren't. It's not a\n> > > foot-gun defense mechanism because it's easily bypassed, and it's easier to\n> > > bypass it than to use it. Whereas for tagged outputs, it's that if you want\n> > > any NOINPUT then you must tag.\n> >\n> > ## Output tagging\n> >\n> > One proposal that I found rather fascinating during the discussion in\n> > Amsterdam was that we could achieve the same disincentive to use on\n> > non-smart-contract cases by simply making the output scripts\n> > unaddressable. This can be done by specifying a version of taproot outputs for\n> > which the bech32 addressing scheme simply doesn't have a representation [6]:\n> >\n> > > The tagged outputs idea is that we don't have NOINPUT ANYPREVOUT supported for\n> > > taproot v1 outputs, instead we have a segwit version 16 v16 that supports\n> > > taproot. The reason for v16 is that we redefine bech32 to not cover\n> > > v16. There's no addresses for this type of output. If you're an exchange and\n> > > receive a bech32 address, you declare it invalid. You make it less user\n> > > friendly here; and there shouldn't be an address anyway. You might want to see\n> > > it on a block explorer, but you don't want to pass it around to anyone.\n> >\n> > We don't need addresses in our contract constructions because we deal directly\n> > with the scripts. This would also have the desired effect of no allowing\n> > generic wallets to send to these addresses, or users accidentally sending\n> > funds to what was supposed to be a one-off script used internally in the\n> > off-chain contract.\n> >\n> > Notice that this idea was already used by Russell O'Connor when performing a\n> > transaction on elements using his new scripting language simplicity\n> > [7]:\n> >\n> > > For this experimental development, we created an improper segwit version,\n> > > \"version 31\" for Simplicity addresses. The payload of this segwit version 31\n> > > address contains a commitment Merkle root of a Simplicity program to control\n> > > the UTXO.\n> >\n> > The concern with output tagging is that it hurts fungibility, marking outputs\n> > used in a contract as such and making them identifiable. But maybe it would be\n> > a good idea to create two domains anyway: one for user-addressable\n> > destinations which users can use with their general purpose wallets, and one\n> > domain for contracts, which users cannot send to directly.\n> >\n> > This also came up during the CoreDev meeting [ams-coredev]:\n> >\n> > > these sort of NOINPUT signatures are only things that are within some\n> > > application or within some protocol that gets negotiated between participants,\n> > > but they don't cross-independent domains where you see a wallet or a protocol\n> > > as a kind of domain. You can't tell the difference, is this an address I can\n> > > give to someone else or not? It's all scripts, no real addresses. There are\n> > > types of outputs that are completely insecure unconditionally; there are\n> > > things that are protected and I can give to anyone, you don't want to reuse\n> > > it, but there's no security issue from doing so. This is an additional class\n> > > that is secure perfectly but only when used in the right way.\n> >\n> > ## Open questions\n> >\n> > The questions that remain to be addressed are the following:\n> >\n> > 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n> >     anyprevout. While at the CoreDev meeting I think everybody agreed that\n> >     these proposals a useful, also beyond eltoo, not everybody could be\n> >     there. I'd therefore like to elicit some feedback from the wider community.\n> > 2.  Is there strong support or opposition to the chaperone signatures\n> >     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n> >     formulate a concrete set of pros and contras, rather than talk about\n> >     abstract dangers or advantages.\n> > 3.  The same for output tagging / explicit opt-in. What are the advantages and\n> >     disadvantages?\n> > 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n> >     confusion and make for simpler discussions in the end.\n> > 5.  Anything I forgot to mention :-)\n> >\n> > Cheers,\n> > Christian\n> >\n> > [1] <https://lists.linuxfoundation.org/pipermail/lightning-dev/2019-September/002131.html>\n> > [2] <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017285.html>\n> > [3] <https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-August/001383.html>\n> > [4] <https://github.com/bitcoin/bips/blob/master/bip-0118.mediawiki>\n> > [5] <https://github.com/ajtowns/bips/blob/bip-anyprevout/bip-anyprevout.mediawiki>\n> > [6] <http://diyhpl.us/wiki/transcripts/bitcoin-core-dev-tech/2019-06-06-noinput-etc/>\n> > [7] <https://lists.ozlabs.org/pipermail/simplicity/2019/000018.html>\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T10:01:58",
                "message_text_only": "ZmnSCPxj via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n\n> Good morning lists,\n>\n> Let me summarize concerns brought up:\n>\n> * Chris concern, is that an ordinary UTXO that is not allocated for `SIGHASH_NOINPUT` use, is inadvertently spent using `SIGHASH_NOINPUT`.\n> * My concern, is that unless a UTXO allocated for `SIGHASH_NOINPUT` use, is *indeed* used with SIGHASH_NOINPUT`, it should look exactly the same as any other SegWit v1 output.\n>\n> I propose the below instead:\n>\n> * Do ***NOT*** allocate SegWit v16 for `SIGHASH_NOINPUT`.\n> * Instead, allocate SegWit v1 Tapscript v16 for `SIGHASH_NOINPUT`.\n>\n> Then, on usage:\n>\n> * Exchange hoards can be protected by simple MuSig bip-schnorr SegWit v1 outputs, or a NUMS Taproot internal point with a MAST branch Tapscript v0 `OP_CHECKSIG_ADD` sequence.\n> * Decker-Russell-Osuntokun constructions are backed by a n-of-n MuSig Taproot internal point, with a MAST branch containing a Tapscript v16 with `OP_1 OP_CHECKSIG`.\n>\n> This solves both concerns:\n>\n> * Ordinary UTXOs not allocated for `SIGHASH_NOINPUT` use simply do not commit to any Taproot that has a Tapscript v16 branch, and thus `SIGHASH_NOINPUT` is unuseable to claim it.\n> * If a UTXO used for an offchain protocol ends up in a cooperative-resolution state, nobody has to know that a Tapscript v16 branch existed that could have used `SIGHASH_NOINPUT`.\n>\n> Again, my objection to output tagging is that it is **publicly visible** as soon as the funding transaction is confirmed onchain that this is a special output used for a Decker-Russell-Osuntokun construction, greatly damaging privacy.\n> But if this fact is kept secret *unless* the very specific case of unilateral uncooperative enforcement, then it is quite fine with me.\n>\n> Would this alternate proposal hold better muster?\n\nIntriguing idea, this would be an invisible tagging, since the opt-in to\nnoinput and friends is hidden inside the committed script, which only\ngets revealed whenever we actually need it.\n\nFor eltoo this would mean that the funding output would be invisibly\ntagged, and the cooperative close would use the taproot pubkey, while\nthe uncooperative close, which would require noinput opt-in, reveals the\nscript, proving prior opt-in, and provides a matching signature.\n\nIf I'm not mistaken this would require AJ's alternative pubkey encoding\n(0x01 or 0x00 prefixed pubkey) to make the opt-in visible, correct?"
            },
            {
                "author": "Ethan Heilman",
                "date": "2019-10-01T14:27:21",
                "message_text_only": ">I don't find too compelling the potential problem of a 'bad wallet designer', whether lazy or dogmatic, misusing noinput. I think there are simpler ways to cut corners and there will always be plenty of good wallet options people can choose.\n\nI want to second this. The most expensive part of wallet design is\nengineering time. Writing code that uses a new sighash or a custom\nscript with a OP_CODE is a very large barrier to use. How many wallets\nsupport multisig or RBF? How much BTC has been stolen over the entire\nhistory of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE\nvs ECDSA nonce reuse?\n\nOn Tue, Oct 1, 2019 at 9:35 AM Richard Myers <rich at gotenna.com> wrote:\n>\n> Thanks Christian for pulling together this concise summary.\n>\n>> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout.\n>\n>\n> I certainly support the unification and adoption of the sighash_noinput and anyprevoutput* proposals to enable eltoo, but also to make possible better off-chain protocol designs generally.\n>\n> Among the various advantages previously discussed, the particular use case benefits from eltoo I want to take advantage of is less interactive payment channel negotiation.\n>\n> In talking with people about eltoo this summer, I found most people generally support adding this as an option to Lightning. The only general concern I heard, if any,  was the vague idea that rebindable transactions could be somehow misused or abused.\n>\n> I believe when these concerns are made more concrete they can be classified and addressed.\n>\n> I don't find too compelling the potential problem of a 'bad wallet designer', whether lazy or dogmatic, misusing noinput. I think there are simpler ways to cut corners and there will always be plenty of good wallet options people can choose.\n>\n> Because scripts signed with no_input signatures are only really exchanged and used for off-chain negotiations, very few should ever appear on chain. Those that do should represent non-cooperative situations that involve signing parties who know not to reuse or share scripts with these public keys again. No third party has any reason to spend value to a multisignature script they don't control, whether or not a no_input signature exists for it.\n>\n>> 2.  Is there strong support or opposition to the chaperone signatures\n>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best to\n>>     formulate a concrete set of pros and contras, rather than talk about\n>>     abstract dangers or advantages.\n>\n>\n> As I mentioned before, I don't think the lazy wallet designer advantage is enough to justify the downsides of chaperone signatures. One additional downside is the additional code complexity required to flag whether or not a chaperone output is included. By comparison, the code changes for creating a no_input digest that skips the prevout and prevscript parts of a tx is much less intrusive and easier to maintain.\n>\n>> 3.  The same for output tagging / explicit opt-in. What are the advantages and\n>>     disadvantages?\n>\n>\n> I see the point ZmnSCPxj makes about tagged outputs negatively impacting the anonymity set of taproot transactions. The suggested work around would impose a cost to unilateral closes of an additional translation transaction and not using the work around would cause a hit to anonymity for off-chain script users. I feel both costs are too high relative to the benefit gained of preventing sloppy reuse of public keys.\n>\n>> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n>\n> I believe they should be merged. I also think both chaperone signatures and output tagging should become part of the discussion of security alternatives, but not part of the initial specification.\n>\n> I understand the desire to be conservative with protocol changes that could be misused. However, with just taproot and taproot public key types the anyprevout functionality is already very opt-in and not something that might accidentally get used. Belt-and-suspender protections like chaperone signatures and tagged outputs have their own impacts on code complexity, on-chain transaction sizes and transaction anonymity that also must be considered.\n>\n> I believe efforts like descriptors will help people follow best practices when working with complex scripts without pushing extra complexity for safety into the consensus layer of bitcoin. Anywhere we can make core code simpler, and handle foot-guns in higher level non-consensus code, the better.\n>\n> _______________________________________________\n>>\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Chris Stewart",
                "date": "2019-10-01T15:14:56",
                "message_text_only": "> I don't find too compelling the potential problem of a 'bad wallet\ndesigner', whether lazy or dogmatic, misusing noinput. I think there are\nsimpler ways to cut corners and there will always be plenty of good wallet\noptions people can choose.\n\nIn my original post, the business that I am talking about don't use \"off\nthe shelf\" wallet options. It isn't a \"let's switch from wallet A to wallet\nB\" kind of situation. Usually this involves design from ground up with\nsecurity considerations that businesses of scale need to consider (signing\nprocedures and key handling being the most important!).\n\n>Because scripts signed with no_input signatures are only really exchanged\nand used for off-chain negotiations, very few should ever appear on chain.\nThose that do should represent non-cooperative situations that involve\nsigning parties who know not to reuse or share scripts with these public\nkeys again. No third party has any reason to spend value to a\nmultisignature script they don't control, whether or not a no_input\nsignature exists for it.\n\nJust because some one is your friend today, doesn't mean they aren't\nnecessarily your adversary tomorrow. I don't think a signature being\nonchain really matters, as you have to give it to your counterparty\nregardless. How do you know your counterparty won't replay that\nSIGHASH_NOINPUT signature later? Offchain protocols shouldn't rely on\n\"good-will\" for their counter parties for security.\n\n>As I mentioned before, I don't think the lazy wallet designer advantage is\nenough to justify the downsides of chaperone signatures. One additional\ndownside is the additional code complexity required to flag whether or not\na chaperone output is included. By comparison, the code changes for\ncreating a no_input digest that skips the prevout and prevscript parts of a\ntx is much less intrusive and easier to maintain.\n\n>I want to second this. The most expensive part of wallet design is\nengineering time. Writing code that uses a new sighash or a custom\nscript with a OP_CODE is a very large barrier to use. How many wallets\nsupport multisig or RBF? How much BTC has been stolen over the entire\nhistory of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE\nvs ECDSA nonce reuse\n\nI actually think lazy wallet designer is a really compelling reason to fix\nfootguns in the bitcoin protocol. Mt Gox is allegedly a product of lazy\nwallet design. Now we have non-malleable transactions in the form of segwit\n(yay!) that prevent this exploit. We can wish that the Mt Gox wallet\ndesigners were more aware of bitcoin protocol vulnerabilities, but at the\nend of the day the best thing to do was offering an alternative that\ncircumvents the vulnerability all together.\n\nEthan made a great point about SIGHASH_NONE or SIGHASH_SINGLE -- which have\nvirtually no use AFAIK -- vs the ECDSA nonce reuse which is used in nearly\nevery transaction. The feature -- ECDSA in this case -- was managed to be\ndone wrong by wallet developers causing fund loss. Unfortunately we can't\nprotect against this type of bug in the protocol.\n\nIf things aren't used -- such as SIGHASH_NONE or SIGHASH_SINGLE -- it\ndoesn't matter if they are secure or insecure. I'm hopefully that offchain\nprotocols will achieve wide adoption, and I would hate to see money lost\nbecause of this. Even though they aren't used, in my OP I do advocate for\nfixing these.\n\n> understand the desire to be conservative with protocol changes that could\nbe misused. However, with just taproot and taproot public key types the\nanyprevout functionality is already very opt-in and not something that\nmight accidentally get used. Belt-and-suspender protections like chaperone\nsignatures and tagged outputs have their own impacts on code complexity,\non-chain transaction sizes and transaction anonymity that also must be\nconsidered.\n\nI'm making the assumption that the business has decided to use this\nfeature, and in my OP I talk about the engineering decisions that will have\nto be made support this. I'm hoping the \"lazy wallet designers\" -- or\nperhaps people that don't follow bitcoin protocol development as rabidly as\nus :-) -- can see that nuance.\n\n-Chris\n\n\n\nOn Tue, Oct 1, 2019 at 8:36 AM Richard Myers <rich at gotenna.com> wrote:\n\n> Thanks Christian for pulling together this concise summary.\n>\n> 1.  General agreement on the usefulness of noinput / anyprevoutanyscript /\n>>     anyprevout.\n>>\n>\n> I certainly support the unification and adoption of the sighash_noinput\n> and anyprevoutput* proposals to enable eltoo, but also to make possible\n> better off-chain protocol designs generally.\n>\n> Among the various advantages previously discussed, the particular use case\n> benefits from eltoo I want to take advantage of is less interactive payment\n> channel negotiation.\n>\n> In talking with people about eltoo this summer, I found most people\n> generally support adding this as an option to Lightning. The only general\n> concern I heard, if any,  was the vague idea that rebindable transactions\n> could be somehow misused or abused.\n>\n> I believe when these concerns are made more concrete they can be\n> classified and addressed.\n>\n> I don't find too compelling the potential problem of a 'bad wallet\n> designer', whether lazy or dogmatic, misusing noinput. I think there are\n> simpler ways to cut corners and there will always be plenty of good wallet\n> options people can choose.\n>\n> Because scripts signed with no_input signatures are only really exchanged\n> and used for off-chain negotiations, very few should ever appear on chain.\n> Those that do should represent non-cooperative situations that involve\n> signing parties who know not to reuse or share scripts with these public\n> keys again. No third party has any reason to spend value to a\n> multisignature script they don't control, whether or not a no_input\n> signature exists for it.\n>\n> 2.  Is there strong support or opposition to the chaperone signatures\n>>     introduced in anyprevout / anyprevoutanyscript? I think it'd be best\n>> to\n>>     formulate a concrete set of pros and contras, rather than talk about\n>>     abstract dangers or advantages.\n>>\n>\n> As I mentioned before, I don't think the lazy wallet designer advantage is\n> enough to justify the downsides of chaperone signatures. One additional\n> downside is the additional code complexity required to flag whether or not\n> a chaperone output is included. By comparison, the code changes for\n> creating a no_input digest that skips the prevout and prevscript parts of a\n> tx is much less intrusive and easier to maintain.\n>\n> 3.  The same for output tagging / explicit opt-in. What are the advantages\n>> and\n>>     disadvantages?\n>>\n>\n> I see the point ZmnSCPxj makes about tagged outputs negatively impacting\n> the anonymity set of taproot transactions. The suggested work around would\n> impose a cost to unilateral closes of an additional translation transaction\n> and not using the work around would cause a hit to anonymity for off-chain\n> script users. I feel both costs are too high relative to the benefit gained\n> of preventing sloppy reuse of public keys.\n>\n> 4.  Shall we merge BIP-118 and bip-anyprevout. This would likely reduce the\n>>     confusion and make for simpler discussions in the end.\n>\n>\n> I believe they should be merged. I also think both chaperone signatures\n> and output tagging should become part of the discussion of security\n> alternatives, but not part of the initial specification.\n>\n> I understand the desire to be conservative with protocol changes that\n> could be misused. However, with just taproot and taproot public key types\n> the anyprevout functionality is already very opt-in and not something that\n> might accidentally get used. Belt-and-suspender protections like chaperone\n> signatures and tagged outputs have their own impacts on code complexity,\n> on-chain transaction sizes and transaction anonymity that also must be\n> considered.\n>\n> I believe efforts like descriptors will help people follow best practices\n> when working with complex scripts without pushing extra complexity for\n> safety into the consensus layer of bitcoin. Anywhere we can make core code\n> simpler, and handle foot-guns in higher level non-consensus code, the\n> better.\n>\n> _______________________________________________\n>\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191001/4f71dd93/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2019-10-03T10:30:03",
                "message_text_only": "Chris Stewart <chris at suredbits.com> writes:\n\n>> I don't find too compelling the potential problem of a 'bad wallet\n> designer', whether lazy or dogmatic, misusing noinput. I think there are\n> simpler ways to cut corners and there will always be plenty of good wallet\n> options people can choose.\n>\n> In my original post, the business that I am talking about don't use \"off\n> the shelf\" wallet options. It isn't a \"let's switch from wallet A to wallet\n> B\" kind of situation. Usually this involves design from ground up with\n> security considerations that businesses of scale need to consider (signing\n> procedures and key handling being the most important!).\n\nIn this case I'd hope that the custom wallet designers/developers are\nwell-versed in the issues they might encounter when implementing their\nwallet. This is especially true if they decide to opt into using some\nlesser known sighash flags, such as noinput, that come with huge warning\nsigns (I forgot to mention that renaming noinput to noinput_dangerous is\nalso still on the table).\n\n>>Because scripts signed with no_input signatures are only really exchanged\n> and used for off-chain negotiations, very few should ever appear on chain.\n> Those that do should represent non-cooperative situations that involve\n> signing parties who know not to reuse or share scripts with these public\n> keys again. No third party has any reason to spend value to a\n> multisignature script they don't control, whether or not a no_input\n> signature exists for it.\n>\n> Just because some one is your friend today, doesn't mean they aren't\n> necessarily your adversary tomorrow. I don't think a signature being\n> onchain really matters, as you have to give it to your counterparty\n> regardless. How do you know your counterparty won't replay that\n> SIGHASH_NOINPUT signature later? Offchain protocols shouldn't rely on\n> \"good-will\" for their counter parties for security.\n>\n>>As I mentioned before, I don't think the lazy wallet designer advantage is\n> enough to justify the downsides of chaperone signatures. One additional\n> downside is the additional code complexity required to flag whether or not\n> a chaperone output is included. By comparison, the code changes for\n> creating a no_input digest that skips the prevout and prevscript parts of a\n> tx is much less intrusive and easier to maintain.\n>\n>>I want to second this. The most expensive part of wallet design is\n> engineering time. Writing code that uses a new sighash or a custom\n> script with a OP_CODE is a very large barrier to use. How many wallets\n> support multisig or RBF? How much BTC has been stolen over the entire\n> history of Bitcoin because of sighash SIGHASH_NONE or SIGHASH_SINGLE\n> vs ECDSA nonce reuse\n>\n> I actually think lazy wallet designer is a really compelling reason to fix\n> footguns in the bitcoin protocol. Mt Gox is allegedly a product of lazy\n> wallet design. Now we have non-malleable transactions in the form of segwit\n> (yay!) that prevent this exploit. We can wish that the Mt Gox wallet\n> designers were more aware of bitcoin protocol vulnerabilities, but at the\n> end of the day the best thing to do was offering an alternative that\n> circumvents the vulnerability all together.\n\nIt's worth pointing out that the transaction malleability issue and the\nintroduction of a new sighash flag are fundamentally different: a wallet\ndeveloper has to take active measures to guard against transaction\nmalleability since it was present even for the most minimal\nimplementation, whereas with sighash flags the developers have to\nactively add support for it. Where transaction malleability you just had\nto know that it might be an issue, with noinput you actively have to do\nwork yo expose yourself to it.\n\nI'd argue that you have to have a very compelling reason to opt into\nsupporting noinput, and that's usually because you want to support a\nmore complex protocol such as an off-chain contract anyway, at which\npoint I'd hope you know about the tradeoffs of various sighash flags :-)\n\n> Ethan made a great point about SIGHASH_NONE or SIGHASH_SINGLE -- which have\n> virtually no use AFAIK -- vs the ECDSA nonce reuse which is used in nearly\n> every transaction. The feature -- ECDSA in this case -- was managed to be\n> done wrong by wallet developers causing fund loss. Unfortunately we can't\n> protect against this type of bug in the protocol.\n>\n> If things aren't used -- such as SIGHASH_NONE or SIGHASH_SINGLE -- it\n> doesn't matter if they are secure or insecure. I'm hopefully that offchain\n> protocols will achieve wide adoption, and I would hate to see money lost\n> because of this. Even though they aren't used, in my OP I do advocate for\n> fixing these.\n\nI do share the feeling that we better make a commonly used sighash flag\nas useable and safe as possible, but it's rather unrealistic to have a\ndeveloper that is able to implement a complex off-chain system, but\nfails to understand the importance of using the correct sighash flags in\ntheir wallet. That being said, I think this concern would be addressed\nby any form of explicit opt-in on the output side (whether hidden or\nnot), right?\n\n\nCheers,\nChristian"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-10-03T01:47:58",
                "message_text_only": "On Wed, Oct 02, 2019 at 02:03:43AM +0000, ZmnSCPxj via Lightning-dev wrote:\n> So let me propose the more radical excision, starting with SegWit v1:\n> * Remove `SIGHASH` from signatures.\n> * Put `SIGHASH` on public keys.\n>     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n\nI don't think you could reasonably do this for key path spends -- if\nyou included the sighash as part of the scriptpubkey explicitly, that\nwould lose some of the indistinguishability of taproot addresses, and be\nmore expensive than having the sighash be in witness data. So I think\nthat means sighashes would still be included in key path signatures,\nwhich would make the behaviour a little confusingly different between\nsigning for key path and script path spends.\n\n> This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n\nI don't think the problems with NONE and SINGLE are any worse than using\nSIGHASH_ALL to pay to \"1*G\" -- someone may steal the money you send,\nbut that's as far as it goes. NOINPUT/ANYPREVOUT is worse in that if\nyou use it, someone may steal funds from other UTXOs too -- similar\nto nonce-reuse. So I think having to commit to enabling NOINPUT for an\naddress may make sense; but I don't really see the need for doing the\nsame for other sighashes generally.\n\nFWIW, one way of looking at a transaction spending UTXO \"U\" to address\n\"A\" is something like:\n\n * \"script\" lets you enforce conditions on the transaction when you\n   create \"A\" [0]\n * \"sighash\" lets you enforce conditions on the transaction when\n   you sign the transaction\n * nlocktime, nsequence, taproot annex are ways you express conditions\n   on the transaction\n\nIn that view, \"sighash\" is actually an *extremely* simple scripting\nlanguage itself (with a total of six possible scripts).\n\nThat doesn't seem like a bad design to me, fwiw.\n\nCheers,\naj\n\n[0] \"graftroot\" lets you update those conditions for address \"A\" after\n    the fact"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-03T03:07:55",
                "message_text_only": "> > let me propose the more radical excision, starting with SegWit v1:\n> >\n> > -   Remove `SIGHASH` from signatures.\n> > -   Put `SIGHASH` on public keys.\n> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> >\n>\n> I don't think you could reasonably do this for key path spends -- if\n> you included the sighash as part of the scriptpubkey explicitly, that\n> would lose some of the indistinguishability of taproot addresses, and be\n> more expensive than having the sighash be in witness data.\n\nNonexistence of sighash byte implies `SIGHASH_ALL`, and for offchain anyway the desired path is to end up with an n-of-n MuSig `SIGHASH_ALL` signed mutual close transaction.\nIndeed we can even restrict keypath spends to not having a sighash byte and just implicitly requiring `SIGHASH_ALL` with no loss of privacy for offchain while attaining safety against `SIGHASH_NOINPUT` for MuSig and VSSS multisignature adresses.\n\n\n> So I think\n> that means sighashes would still be included in key path signatures,\n> which would make the behaviour a little confusingly different between\n> signing for key path and script path spends.\n>\n> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n>\n> I don't think the problems with NONE and SINGLE are any worse than using\n> SIGHASH_ALL to pay to \"1*G\" -- someone may steal the money you send,\n> but that's as far as it goes. NOINPUT/ANYPREVOUT is worse in that if\n> you use it, someone may steal funds from other UTXOs too -- similar\n> to nonce-reuse. So I think having to commit to enabling NOINPUT for an\n> address may make sense; but I don't really see the need for doing the\n> same for other sighashes generally.\n\nAs the existing sighashes are not particularly used anyway, additional restrictions on them are relatively immaterial.\n\n>\n> FWIW, one way of looking at a transaction spending UTXO \"U\" to address\n> \"A\" is something like:\n>\n> -   \"script\" lets you enforce conditions on the transaction when you\n>     create \"A\" [0]\n>\n> -   \"sighash\" lets you enforce conditions on the transaction when\n>     you sign the transaction\n>\n> -   nlocktime, nsequence, taproot annex are ways you express conditions\n>     on the transaction\n>\n>     In that view, \"sighash\" is actually an extremely simple scripting\n>     language itself (with a total of six possible scripts).\n>\n>     That doesn't seem like a bad design to me, fwiw.\n\n\nOnly one of the scripts is widely used, another has an edge use it sucks at (assurance contracts).\n\nDoes not seem to be good design, rather legacy cruft.\n\nRegards,\nZmnSCPxj\n\n>\n>     Cheers,\n>     aj\n>\n>     [0] \"graftroot\" lets you update those conditions for address \"A\" after\n>     the fact\n>"
            }
        ],
        "thread_summary": {
            "title": "Continuing the discussion about noinput / anyprevout",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Chris Stewart",
                "Anthony Towns",
                "ZmnSCPxj",
                "Ethan Heilman",
                "Christian Decker"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 48556
        }
    },
    {
        "title": "[bitcoin-dev] OP_CAT was Re: Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "Ethan Heilman",
                "date": "2019-10-03T15:05:52",
                "message_text_only": "To avoid derailing the NO_INPUT conversation, I have changed the\nsubject to OP_CAT.\n\nResponding to:\n\"\"\"\n* `SIGHASH` flags attached to signatures are a misdesign, sadly\nretained from the original BitCoin 0.1.0 Alpha for Windows design, on\npar with:\n[..]\n* `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n[..]\n\"\"\"\n\nOP_CAT is an extremely valuable op code. I understand why it was\nremoved as the situation at the time with scripts was dire. However\nmost of the protocols I've wanted to build on Bitcoin run into the\nlimitation that stack values can not be concatenated. For instance\nTumbleBit would have far smaller transaction sizes if OP_CAT was\nsupported in Bitcoin. If it happens to me as a researcher it is\nprobably holding other people back as well. If I could wave a magic\nwand and turn on one of the disabled op codes it would be OP_CAT.  Of\ncourse with the change that size of each concatenated value must be 64\nBytes or less.\n\n\nOn Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Good morning lists,\n>\n> Let me propose the below radical idea:\n>\n> * `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n>   * 1 RETURN\n>   * higher-`nSequence` replacement\n>   * DER-encoded pubkeys\n>   * unrestricted `scriptPubKey`\n>   * Payee-security-paid-by-payer (i.e. lack of P2SH)\n>   * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n>   * transaction malleability\n>   * probably many more\n>\n> So let me propose the more radical excision, starting with SegWit v1:\n>\n> * Remove `SIGHASH` from signatures.\n> * Put `SIGHASH` on public keys.\n>\n> Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> `OP_CHECKSIG` and friends then look at the *public key* to determine sighash algorithm rather than the signature.\n>\n> As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n>\n> I propose also the addition of the opcode:\n>\n>     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n>\n> * `sighash` must be one byte.\n> * `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> * `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> * `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> * If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n>\n> This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> This is done by using the script:\n>\n>     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n>\n> Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n>\n> However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n>\n> This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n>\n> Would this not be a superior solution?\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "OP_CAT was Re: Continuing the discussion about noinput / anyprevout",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Ethan Heilman"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3838
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] OP_CAT was Re: Continuing the discussion about noinput / anyprevout",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-03T23:42:25",
                "message_text_only": "Good morning Ethan,\n\n\n> To avoid derailing the NO_INPUT conversation, I have changed the\n> subject to OP_CAT.\n>\n> Responding to:\n> \"\"\"\n>\n> -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n>     retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n>     par with:\n>     [..]\n>\n> -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n>     [..]\n>     \"\"\"\n>\n>     OP_CAT is an extremely valuable op code. I understand why it was\n>     removed as the situation at the time with scripts was dire. However\n>     most of the protocols I've wanted to build on Bitcoin run into the\n>     limitation that stack values can not be concatenated. For instance\n>     TumbleBit would have far smaller transaction sizes if OP_CAT was\n>     supported in Bitcoin. If it happens to me as a researcher it is\n>     probably holding other people back as well. If I could wave a magic\n>     wand and turn on one of the disabled op codes it would be OP_CAT. Of\n>     course with the change that size of each concatenated value must be 64\n>     Bytes or less.\n\nWhy 64 bytes in particular?\n\nIt seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.\n\nHowever we have had issues with the use of Merkle trees in Bitcoin blocks.\nSpecifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.\nMy understanding is that this is the reason for now requiring transactions to be at least 80 bytes.\n\nThe obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.\nTaproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.\n\nThis seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.\n\nOr we could implement tagged SHA256 as a new opcode...\n\nRegards,\nZmnSCPxj\n\n\n>\n>     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n>     bitcoin-dev at lists.linuxfoundation.org wrote:\n>\n>\n> > Good morning lists,\n> > Let me propose the below radical idea:\n> >\n> > -   `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> >     -   1 RETURN\n> >     -   higher-`nSequence` replacement\n> >     -   DER-encoded pubkeys\n> >     -   unrestricted `scriptPubKey`\n> >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> >     -   transaction malleability\n> >     -   probably many more\n> >\n> > So let me propose the more radical excision, starting with SegWit v1:\n> >\n> > -   Remove `SIGHASH` from signatures.\n> > -   Put `SIGHASH` on public keys.\n> >\n> > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.\n> > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > I propose also the addition of the opcode:\n> >\n> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> >\n> >\n> > -   `sighash` must be one byte.\n> > -   `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> > -   `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> > -   `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> > -   If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> >\n> > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> > This is done by using the script:\n> >\n> >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> >\n> >\n> > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n> > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n> > Would this not be a superior solution?\n> > Regards,\n> > ZmnSCPxj\n> >\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Ethan Heilman",
                "date": "2019-10-04T00:48:17",
                "message_text_only": "I hope you are having an great afternoon ZmnSCPxj,\n\nYou make an excellent point!\n\nI had thought about doing the following to tag nodes\n\n|| means OP_CAT\n\n`node = SHA256(type||SHA256(data))`\nso a subnode would be\n`subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\nand a leaf node would be\n`leafnode = SHA256(0||SHA256(leafdata))`\n\nYet, I like your idea better. Increasing the size of the two inputs to\nOP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\nsize of object on the stack seems sensible and also doesn't special\ncase the logic of OP_CAT.\n\nIt would also increase performance. SHA256(tag||subnode2||subnode3)\nrequires 2 compression function calls whereas\nSHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\nfunction calls (due to padding).\n\n>Or we could implement tagged SHA256 as a new opcode...\n\nI agree that tagged SHA256 as an op code that would certainty be\nuseful, but OP_CAT provides far more utility and is a simpler change.\n\nThanks,\nEthan\n\nOn Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>\n> Good morning Ethan,\n>\n>\n> > To avoid derailing the NO_INPUT conversation, I have changed the\n> > subject to OP_CAT.\n> >\n> > Responding to:\n> > \"\"\"\n> >\n> > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> >     retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> >     par with:\n> >     [..]\n> >\n> > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> >     [..]\n> >     \"\"\"\n> >\n> >     OP_CAT is an extremely valuable op code. I understand why it was\n> >     removed as the situation at the time with scripts was dire. However\n> >     most of the protocols I've wanted to build on Bitcoin run into the\n> >     limitation that stack values can not be concatenated. For instance\n> >     TumbleBit would have far smaller transaction sizes if OP_CAT was\n> >     supported in Bitcoin. If it happens to me as a researcher it is\n> >     probably holding other people back as well. If I could wave a magic\n> >     wand and turn on one of the disabled op codes it would be OP_CAT. Of\n> >     course with the change that size of each concatenated value must be 64\n> >     Bytes or less.\n>\n> Why 64 bytes in particular?\n>\n> It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.\n>\n> However we have had issues with the use of Merkle trees in Bitcoin blocks.\n> Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.\n> My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.\n>\n> The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.\n> Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.\n>\n> This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.\n>\n> Or we could implement tagged SHA256 as a new opcode...\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> >\n> >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> >     bitcoin-dev at lists.linuxfoundation.org wrote:\n> >\n> >\n> > > Good morning lists,\n> > > Let me propose the below radical idea:\n> > >\n> > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> > >     -   1 RETURN\n> > >     -   higher-`nSequence` replacement\n> > >     -   DER-encoded pubkeys\n> > >     -   unrestricted `scriptPubKey`\n> > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > >     -   transaction malleability\n> > >     -   probably many more\n> > >\n> > > So let me propose the more radical excision, starting with SegWit v1:\n> > >\n> > > -   Remove `SIGHASH` from signatures.\n> > > -   Put `SIGHASH` on public keys.\n> > >\n> > > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> > > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.\n> > > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> > > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > > I propose also the addition of the opcode:\n> > >\n> > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > >\n> > >\n> > > -   `sighash` must be one byte.\n> > > -   `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> > > -   `pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> > > -   `pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> > > -   If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > >\n> > > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> > > This is done by using the script:\n> > >\n> > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > >\n> > >\n> > > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> > > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n> > > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n> > > Would this not be a superior solution?\n> > > Regards,\n> > > ZmnSCPxj\n> > >\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-04T05:02:14",
                "message_text_only": "Awhile back, Ethan and I discussed having, rather than OP_CAT, an\nOP_SHA256STREAM that uses the streaming properties of a SHA256 hash\nfunction to allow concatenation of an unlimited amount of data, provided\nthe only use is to hash it.\n\nYou can then use it perhaps as follows:\n\n// start a new hash with item\nOP_SHA256STREAM  (-1) -> [state]\n// Add item to the hash in state\nOP_SHA256STREAM n [item] [state] -> [state]\n// Finalize\nOP_SHA256STREAM (-2) [state] -> [Hash]\n\n<-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2>\nOP_SHA256STREAM\n\n\nOr it coul\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:\n\n> I hope you are having an great afternoon ZmnSCPxj,\n>\n> You make an excellent point!\n>\n> I had thought about doing the following to tag nodes\n>\n> || means OP_CAT\n>\n> `node = SHA256(type||SHA256(data))`\n> so a subnode would be\n> `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\n> and a leaf node would be\n> `leafnode = SHA256(0||SHA256(leafdata))`\n>\n> Yet, I like your idea better. Increasing the size of the two inputs to\n> OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\n> size of object on the stack seems sensible and also doesn't special\n> case the logic of OP_CAT.\n>\n> It would also increase performance. SHA256(tag||subnode2||subnode3)\n> requires 2 compression function calls whereas\n> SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\n> function calls (due to padding).\n>\n> >Or we could implement tagged SHA256 as a new opcode...\n>\n> I agree that tagged SHA256 as an op code that would certainty be\n> useful, but OP_CAT provides far more utility and is a simpler change.\n>\n> Thanks,\n> Ethan\n>\n> On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> >\n> > Good morning Ethan,\n> >\n> >\n> > > To avoid derailing the NO_INPUT conversation, I have changed the\n> > > subject to OP_CAT.\n> > >\n> > > Responding to:\n> > > \"\"\"\n> > >\n> > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> > >     retained from the original BitCoin 0.1.0 Alpha for Windows design,\n> on\n> > >     par with:\n> > >     [..]\n> > >\n> > > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > >     [..]\n> > >     \"\"\"\n> > >\n> > >     OP_CAT is an extremely valuable op code. I understand why it was\n> > >     removed as the situation at the time with scripts was dire. However\n> > >     most of the protocols I've wanted to build on Bitcoin run into the\n> > >     limitation that stack values can not be concatenated. For instance\n> > >     TumbleBit would have far smaller transaction sizes if OP_CAT was\n> > >     supported in Bitcoin. If it happens to me as a researcher it is\n> > >     probably holding other people back as well. If I could wave a magic\n> > >     wand and turn on one of the disabled op codes it would be OP_CAT.\n> Of\n> > >     course with the change that size of each concatenated value must\n> be 64\n> > >     Bytes or less.\n> >\n> > Why 64 bytes in particular?\n> >\n> > It seems obvious to me that this 64 bytes is most suited for building\n> Merkle trees, being the size of two SHA256 hashes.\n> >\n> > However we have had issues with the use of Merkle trees in Bitcoin\n> blocks.\n> > Specifically, it is difficult to determine if a hash on a Merkle node is\n> the hash of a Merkle subnode, or a leaf transaction.\n> > My understanding is that this is the reason for now requiring\n> transactions to be at least 80 bytes.\n> >\n> > The obvious fix would be to prepend the type of the hashed object, i.e.\n> add at least one byte to determine this type.\n> > Taproot for example uses tagged hash functions, with a different tag for\n> leaves, and tagged hashes are just\n> prepend-this-32-byte-constant-twice-before-you-SHA256.\n> >\n> > This seems to indicate that to check merkle tree proofs, an `OP_CAT`\n> with only 64 bytes max output size would not be sufficient.\n> >\n> > Or we could implement tagged SHA256 as a new opcode...\n> >\n> > Regards,\n> > ZmnSCPxj\n> >\n> >\n> > >\n> > >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> > >     bitcoin-dev at lists.linuxfoundation.org wrote:\n> > >\n> > >\n> > > > Good morning lists,\n> > > > Let me propose the below radical idea:\n> > > >\n> > > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> retained from the original BitCoin 0.1.0 Alpha for Windows design, on par\n> with:\n> > > >     -   1 RETURN\n> > > >     -   higher-`nSequence` replacement\n> > > >     -   DER-encoded pubkeys\n> > > >     -   unrestricted `scriptPubKey`\n> > > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > >     -   transaction malleability\n> > > >     -   probably many more\n> > > >\n> > > > So let me propose the more radical excision, starting with SegWit v1:\n> > > >\n> > > > -   Remove `SIGHASH` from signatures.\n> > > > -   Put `SIGHASH` on public keys.\n> > > >\n> > > > Public keys are now encoded as either 33-bytes (implicit\n> `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type,\n> followed by pubkey coordinate).\n> > > > `OP_CHECKSIG` and friends then look at the public key to determine\n> sighash algorithm rather than the signature.\n> > > > As we expect public keys to be indirectly committed to on every\n> output `scriptPubKey`, this is automatically output tagging to allow\n> particular `SIGHASH`.\n> > > > However, we can then utilize the many many ways to hide public keys\n> away until they are needed, exemplified in MAST-inside-Taproot.\n> > > > I propose also the addition of the opcode:\n> > > >\n> > > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > > >\n> > > >\n> > > > -   `sighash` must be one byte.\n> > > > -   `pubkey` may be the special byte `0x1`, meaning \"just use the\n> Taproot internal pubkey\".\n> > > > -   `pubkey` may be 33-byte public key, in which case the `sighash`\n> byte is just prepended to it.\n> > > > -   `pubkey` may be 34-byte public key with sighash, in which case\n> the first byte is replaced with `sighash` byte.\n> > > > -   If `sighash` is `0x00` then the result is a 33-byte public key\n> (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > > >\n> > > > This retains the old feature where the sighash is selected at\n> time-of-spending rather than time-of-payment.\n> > > > This is done by using the script:\n> > > >\n> > > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > > >\n> > > >\n> > > > Then the sighash can be put in the witness stack after the\n> signature, letting the `SIGHASH` flag be selected at time-of-signing, but\n> only if the SCRIPT specifically is formed to do so.\n> > > > This is malleability-safe as the signature still commits to the\n> `SIGHASH` it was created for.\n> > > > However, by default, public keys will not have an attached `SIGHASH`\n> byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as\n> they are allowed only if the output specifically says they are allowed.\n> > > > Would this not be a superior solution?\n> > > > Regards,\n> > > > ZmnSCPxj\n> > > >\n> > > > bitcoin-dev mailing list\n> > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > >\n> > > Lightning-dev mailing list\n> > > Lightning-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> >\n> >\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/e3a709c9/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-04T07:00:13",
                "message_text_only": "Good morning Jeremy,\n\n> Awhile back, Ethan and I discussed having, rather than OP_CAT, an OP_SHA256STREAM that uses the streaming properties of a SHA256 hash function to allow concatenation of an unlimited amount of data, provided the only use is to hash it.\n>\n> You can then use it perhaps as follows:\n>\n> // start a new hash with item\n> OP_SHA256STREAM\u00a0 (-1) -> [state]\n> // Add item to the hash in state\n> OP_SHA256STREAM n [item] [state] -> [state]\n> // Finalize\n> OP_SHA256STREAM (-2) [state] -> [Hash]\n>\n> <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2> OP_SHA256STREAM\n>\n> Or it coul\n>\n\nThis seems a good idea.\n\nThough it brings up the age-old tension between:\n\n* Generically-useable components, but due to generalization are less efficient.\n* Specific-use components, which are efficient, but which may end up not being useable in the future.\n\nIn particular, `OP_SHA256STREAM` would no longer be useable if SHA256 eventually is broken, while the `OP_CAT` will still be useable in the indefinite future.\nIn the future a new hash function can simply be defined and the same technique with `OP_CAT` would still be useable.\n\n\nRegards,\nZmnSCPxj\n\n> --\n> @JeremyRubin\n>\n> On Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:\n>\n> > I hope you are having an great afternoon ZmnSCPxj,\n> >\n> > You make an excellent point!\n> >\n> > I had thought about doing the following to tag nodes\n> >\n> > || means OP_CAT\n> >\n> > `node = SHA256(type||SHA256(data))`\n> > so a subnode would be\n> > `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\n> > and a leaf node would be\n> > `leafnode = SHA256(0||SHA256(leafdata))`\n> >\n> > Yet, I like your idea better. Increasing the size of the two inputs to\n> > OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\n> > size of object on the stack seems sensible and also doesn't special\n> > case the logic of OP_CAT.\n> >\n> > It would also increase performance. SHA256(tag||subnode2||subnode3)\n> > requires 2 compression function calls whereas\n> > SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\n> > function calls (due to padding).\n> >\n> > >Or we could implement tagged SHA256 as a new opcode...\n> >\n> > I agree that tagged SHA256 as an op code that would certainty be\n> > useful, but OP_CAT provides far more utility and is a simpler change.\n> >\n> > Thanks,\n> > Ethan\n> >\n> > On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> > >\n> > > Good morning Ethan,\n> > >\n> > >\n> > > > To avoid derailing the NO_INPUT conversation, I have changed the\n> > > > subject to OP_CAT.\n> > > >\n> > > > Responding to:\n> > > > \"\"\"\n> > > >\n> > > > -\u00a0 \u00a0`SIGHASH` flags attached to signatures are a misdesign, sadly\n> > > >\u00a0 \u00a0 \u00a0retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> > > >\u00a0 \u00a0 \u00a0par with:\n> > > >\u00a0 \u00a0 \u00a0[..]\n> > > >\n> > > > -\u00a0 \u00a0`OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > >\u00a0 \u00a0 \u00a0[..]\n> > > >\u00a0 \u00a0 \u00a0\"\"\"\n> > > >\n> > > >\u00a0 \u00a0 \u00a0OP_CAT is an extremely valuable op code. I understand why it was\n> > > >\u00a0 \u00a0 \u00a0removed as the situation at the time with scripts was dire. However\n> > > >\u00a0 \u00a0 \u00a0most of the protocols I've wanted to build on Bitcoin run into the\n> > > >\u00a0 \u00a0 \u00a0limitation that stack values can not be concatenated. For instance\n> > > >\u00a0 \u00a0 \u00a0TumbleBit would have far smaller transaction sizes if OP_CAT was\n> > > >\u00a0 \u00a0 \u00a0supported in Bitcoin. If it happens to me as a researcher it is\n> > > >\u00a0 \u00a0 \u00a0probably holding other people back as well. If I could wave a magic\n> > > >\u00a0 \u00a0 \u00a0wand and turn on one of the disabled op codes it would be OP_CAT. Of\n> > > >\u00a0 \u00a0 \u00a0course with the change that size of each concatenated value must be 64\n> > > >\u00a0 \u00a0 \u00a0Bytes or less.\n> > >\n> > > Why 64 bytes in particular?\n> > >\n> > > It seems obvious to me that this 64 bytes is most suited for building Merkle trees, being the size of two SHA256 hashes.\n> > >\n> > > However we have had issues with the use of Merkle trees in Bitcoin blocks.\n> > > Specifically, it is difficult to determine if a hash on a Merkle node is the hash of a Merkle subnode, or a leaf transaction.\n> > > My understanding is that this is the reason for now requiring transactions to be at least 80 bytes.\n> > >\n> > > The obvious fix would be to prepend the type of the hashed object, i.e. add at least one byte to determine this type.\n> > > Taproot for example uses tagged hash functions, with a different tag for leaves, and tagged hashes are just prepend-this-32-byte-constant-twice-before-you-SHA256.\n> > >\n> > > This seems to indicate that to check merkle tree proofs, an `OP_CAT` with only 64 bytes max output size would not be sufficient.\n> > >\n> > > Or we could implement tagged SHA256 as a new opcode...\n> > >\n> > > Regards,\n> > > ZmnSCPxj\n> > >\n> > >\n> > > >\n> > > >\u00a0 \u00a0 \u00a0On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> > > >\u00a0 \u00a0 \u00a0bitcoin-dev at lists.linuxfoundation.org wrote:\n> > > >\n> > > >\n> > > > > Good morning lists,\n> > > > > Let me propose the below radical idea:\n> > > > >\n> > > > > -\u00a0 \u00a0`SIGHASH` flags attached to signatures are a misdesign, sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a01 RETURN\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0higher-`nSequence` replacement\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0DER-encoded pubkeys\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0unrestricted `scriptPubKey`\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0`OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0transaction malleability\n> > > > >\u00a0 \u00a0 \u00a0-\u00a0 \u00a0probably many more\n> > > > >\n> > > > > So let me propose the more radical excision, starting with SegWit v1:\n> > > > >\n> > > > > -\u00a0 \u00a0Remove `SIGHASH` from signatures.\n> > > > > -\u00a0 \u00a0Put `SIGHASH` on public keys.\n> > > > >\n> > > > > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey coordinate).\n> > > > > `OP_CHECKSIG` and friends then look at the public key to determine sighash algorithm rather than the signature.\n> > > > > As we expect public keys to be indirectly committed to on every output `scriptPubKey`, this is automatically output tagging to allow particular `SIGHASH`.\n> > > > > However, we can then utilize the many many ways to hide public keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > > > > I propose also the addition of the opcode:\n> > > > >\n> > > > >\u00a0 \u00a0 \u00a0<sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > > > >\n> > > > >\n> > > > > -\u00a0 \u00a0`sighash` must be one byte.\n> > > > > -\u00a0 \u00a0`pubkey` may be the special byte `0x1`, meaning \"just use the Taproot internal pubkey\".\n> > > > > -\u00a0 \u00a0`pubkey` may be 33-byte public key, in which case the `sighash` byte is just prepended to it.\n> > > > > -\u00a0 \u00a0`pubkey` may be 34-byte public key with sighash, in which case the first byte is replaced with `sighash` byte.\n> > > > > -\u00a0 \u00a0If `sighash` is `0x00` then the result is a 33-byte public key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > > > >\n> > > > > This retains the old feature where the sighash is selected at time-of-spending rather than time-of-payment.\n> > > > > This is done by using the script:\n> > > > >\n> > > > >\u00a0 \u00a0 \u00a0<pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > > > >\n> > > > >\n> > > > > Then the sighash can be put in the witness stack after the signature, letting the `SIGHASH` flag be selected at time-of-signing, but only if the SCRIPT specifically is formed to do so.\n> > > > > This is malleability-safe as the signature still commits to the `SIGHASH` it was created for.\n> > > > > However, by default, public keys will not have an attached `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> > > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they are allowed only if the output specifically says they are allowed.\n> > > > > Would this not be a superior solution?\n> > > > > Regards,\n> > > > > ZmnSCPxj\n> > > > >\n> > > > > bitcoin-dev mailing list\n> > > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > > >\n> > > > Lightning-dev mailing list\n> > > > Lightning-dev at lists.linuxfoundation.org\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> > >\n> > >\n> > _______________________________________________\n> > Lightning-dev mailing list\n> > Lightning-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-04T18:33:09",
                "message_text_only": "Good point -- in our discussion, we called it OP_FFS -- Fold Functional\nStream, and it could be initialized with a different integer to select for\ndifferent functions. Therefore the stream processing opcodes would be\ngeneric, but extensible.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Oct 4, 2019 at 12:00 AM ZmnSCPxj via Lightning-dev <\nlightning-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Jeremy,\n>\n> > Awhile back, Ethan and I discussed having, rather than OP_CAT, an\n> OP_SHA256STREAM that uses the streaming properties of a SHA256 hash\n> function to allow concatenation of an unlimited amount of data, provided\n> the only use is to hash it.\n> >\n> > You can then use it perhaps as follows:\n> >\n> > // start a new hash with item\n> > OP_SHA256STREAM  (-1) -> [state]\n> > // Add item to the hash in state\n> > OP_SHA256STREAM n [item] [state] -> [state]\n> > // Finalize\n> > OP_SHA256STREAM (-2) [state] -> [Hash]\n> >\n> > <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM\n> <-2> OP_SHA256STREAM\n> >\n> > Or it coul\n> >\n>\n> This seems a good idea.\n>\n> Though it brings up the age-old tension between:\n>\n> * Generically-useable components, but due to generalization are less\n> efficient.\n> * Specific-use components, which are efficient, but which may end up not\n> being useable in the future.\n>\n> In particular, `OP_SHA256STREAM` would no longer be useable if SHA256\n> eventually is broken, while the `OP_CAT` will still be useable in the\n> indefinite future.\n> In the future a new hash function can simply be defined and the same\n> technique with `OP_CAT` would still be useable.\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n> > --\n> > @JeremyRubin\n> >\n> > On Thu, Oct 3, 2019 at 8:04 PM Ethan Heilman <eth3rs at gmail.com> wrote:\n> >\n> > > I hope you are having an great afternoon ZmnSCPxj,\n> > >\n> > > You make an excellent point!\n> > >\n> > > I had thought about doing the following to tag nodes\n> > >\n> > > || means OP_CAT\n> > >\n> > > `node = SHA256(type||SHA256(data))`\n> > > so a subnode would be\n> > > `subnode1 = SHA256(1||SHA256(subnode2||subnode3))`\n> > > and a leaf node would be\n> > > `leafnode = SHA256(0||SHA256(leafdata))`\n> > >\n> > > Yet, I like your idea better. Increasing the size of the two inputs to\n> > > OP_CAT to be 260 Bytes each where 520 Bytes is the maximum allowable\n> > > size of object on the stack seems sensible and also doesn't special\n> > > case the logic of OP_CAT.\n> > >\n> > > It would also increase performance. SHA256(tag||subnode2||subnode3)\n> > > requires 2 compression function calls whereas\n> > > SHA256(1||SHA256(subnode2||subnode3)) requires 2+1=3 compression\n> > > function calls (due to padding).\n> > >\n> > > >Or we could implement tagged SHA256 as a new opcode...\n> > >\n> > > I agree that tagged SHA256 as an op code that would certainty be\n> > > useful, but OP_CAT provides far more utility and is a simpler change.\n> > >\n> > > Thanks,\n> > > Ethan\n> > >\n> > > On Thu, Oct 3, 2019 at 7:42 PM ZmnSCPxj <ZmnSCPxj at protonmail.com>\n> wrote:\n> > > >\n> > > > Good morning Ethan,\n> > > >\n> > > >\n> > > > > To avoid derailing the NO_INPUT conversation, I have changed the\n> > > > > subject to OP_CAT.\n> > > > >\n> > > > > Responding to:\n> > > > > \"\"\"\n> > > > >\n> > > > > -   `SIGHASH` flags attached to signatures are a misdesign, sadly\n> > > > >     retained from the original BitCoin 0.1.0 Alpha for Windows\n> design, on\n> > > > >     par with:\n> > > > >     [..]\n> > > > >\n> > > > > -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > > >     [..]\n> > > > >     \"\"\"\n> > > > >\n> > > > >     OP_CAT is an extremely valuable op code. I understand why it\n> was\n> > > > >     removed as the situation at the time with scripts was dire.\n> However\n> > > > >     most of the protocols I've wanted to build on Bitcoin run into\n> the\n> > > > >     limitation that stack values can not be concatenated. For\n> instance\n> > > > >     TumbleBit would have far smaller transaction sizes if OP_CAT\n> was\n> > > > >     supported in Bitcoin. If it happens to me as a researcher it is\n> > > > >     probably holding other people back as well. If I could wave a\n> magic\n> > > > >     wand and turn on one of the disabled op codes it would be\n> OP_CAT. Of\n> > > > >     course with the change that size of each concatenated value\n> must be 64\n> > > > >     Bytes or less.\n> > > >\n> > > > Why 64 bytes in particular?\n> > > >\n> > > > It seems obvious to me that this 64 bytes is most suited for\n> building Merkle trees, being the size of two SHA256 hashes.\n> > > >\n> > > > However we have had issues with the use of Merkle trees in Bitcoin\n> blocks.\n> > > > Specifically, it is difficult to determine if a hash on a Merkle\n> node is the hash of a Merkle subnode, or a leaf transaction.\n> > > > My understanding is that this is the reason for now requiring\n> transactions to be at least 80 bytes.\n> > > >\n> > > > The obvious fix would be to prepend the type of the hashed object,\n> i.e. add at least one byte to determine this type.\n> > > > Taproot for example uses tagged hash functions, with a different tag\n> for leaves, and tagged hashes are just\n> prepend-this-32-byte-constant-twice-before-you-SHA256.\n> > > >\n> > > > This seems to indicate that to check merkle tree proofs, an `OP_CAT`\n> with only 64 bytes max output size would not be sufficient.\n> > > >\n> > > > Or we could implement tagged SHA256 as a new opcode...\n> > > >\n> > > > Regards,\n> > > > ZmnSCPxj\n> > > >\n> > > >\n> > > > >\n> > > > >     On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> > > > >     bitcoin-dev at lists.linuxfoundation.org wrote:\n> > > > >\n> > > > >\n> > > > > > Good morning lists,\n> > > > > > Let me propose the below radical idea:\n> > > > > >\n> > > > > > -   `SIGHASH` flags attached to signatures are a misdesign,\n> sadly retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> par with:\n> > > > > >     -   1 RETURN\n> > > > > >     -   higher-`nSequence` replacement\n> > > > > >     -   DER-encoded pubkeys\n> > > > > >     -   unrestricted `scriptPubKey`\n> > > > > >     -   Payee-security-paid-by-payer (i.e. lack of P2SH)\n> > > > > >     -   `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> > > > > >     -   transaction malleability\n> > > > > >     -   probably many more\n> > > > > >\n> > > > > > So let me propose the more radical excision, starting with\n> SegWit v1:\n> > > > > >\n> > > > > > -   Remove `SIGHASH` from signatures.\n> > > > > > -   Put `SIGHASH` on public keys.\n> > > > > >\n> > > > > > Public keys are now encoded as either 33-bytes (implicit\n> `SIGHASH_ALL`) or 34-bytes (`SIGHASH` byte, followed by pubkey type,\n> followed by pubkey coordinate).\n> > > > > > `OP_CHECKSIG` and friends then look at the public key to\n> determine sighash algorithm rather than the signature.\n> > > > > > As we expect public keys to be indirectly committed to on every\n> output `scriptPubKey`, this is automatically output tagging to allow\n> particular `SIGHASH`.\n> > > > > > However, we can then utilize the many many ways to hide public\n> keys away until they are needed, exemplified in MAST-inside-Taproot.\n> > > > > > I propose also the addition of the opcode:\n> > > > > >\n> > > > > >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> > > > > >\n> > > > > >\n> > > > > > -   `sighash` must be one byte.\n> > > > > > -   `pubkey` may be the special byte `0x1`, meaning \"just use\n> the Taproot internal pubkey\".\n> > > > > > -   `pubkey` may be 33-byte public key, in which case the\n> `sighash` byte is just prepended to it.\n> > > > > > -   `pubkey` may be 34-byte public key with sighash, in which\n> case the first byte is replaced with `sighash` byte.\n> > > > > > -   If `sighash` is `0x00` then the result is a 33-byte public\n> key (the sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> > > > > >\n> > > > > > This retains the old feature where the sighash is selected at\n> time-of-spending rather than time-of-payment.\n> > > > > > This is done by using the script:\n> > > > > >\n> > > > > >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> > > > > >\n> > > > > >\n> > > > > > Then the sighash can be put in the witness stack after the\n> signature, letting the `SIGHASH` flag be selected at time-of-signing, but\n> only if the SCRIPT specifically is formed to do so.\n> > > > > > This is malleability-safe as the signature still commits to the\n> `SIGHASH` it was created for.\n> > > > > > However, by default, public keys will not have an attached\n> `SIGHASH` byte, implying `SIGHASH_ALL` (and disallowing-by-default\n> non-`SIGHASH_ALL`).\n> > > > > > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`,\n> as they are allowed only if the output specifically says they are allowed.\n> > > > > > Would this not be a superior solution?\n> > > > > > Regards,\n> > > > > > ZmnSCPxj\n> > > > > >\n> > > > > > bitcoin-dev mailing list\n> > > > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > > > >\n> > > > > Lightning-dev mailing list\n> > > > > Lightning-dev at lists.linuxfoundation.org\n> > > > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> > > >\n> > > >\n> > > _______________________________________________\n> > > Lightning-dev mailing list\n> > > Lightning-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/709217b1/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2019-10-04T11:15:36",
                "message_text_only": "On Thu, Oct 03, 2019 at 10:02:14PM -0700, Jeremy via bitcoin-dev wrote:\n> Awhile back, Ethan and I discussed having, rather than OP_CAT, an\n> OP_SHA256STREAM that uses the streaming properties of a SHA256 hash\n> function to allow concatenation of an unlimited amount of data, provided\n> the only use is to hash it.\n> \n> You can then use it perhaps as follows:\n> \n> // start a new hash with item\n> OP_SHA256STREAM  (-1) -> [state]\n> // Add item to the hash in state\n> OP_SHA256STREAM n [item] [state] -> [state]\n> // Finalize\n> OP_SHA256STREAM (-2) [state] -> [Hash]\n> \n> <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM <-2>\n> OP_SHA256STREAM\n\nOne issue with this is the simplest implementation where the state is just raw\nbytes would expose raw SHA256 midstates, allowing people to use them directly;\npreventing that would require adding types to the stack. Specifically I could\nwrite a script that rather than initializing the state correctly from the\nofficial IV, instead takes an untrusted state as input.\n\nSHA256 isn't designed to be used in situations where adversaries control the\ninitialization vector. I personally don't know one way or the other if anyone\nhas analyzed this in detail, but I'd be surprised if that's secure. I\nconsidered adding midstate support to OpenTimestamps but decided against it for\nexactly that reason.\n\nI don't have the link handy but there's even an example of an experienced\ncryptographer on this very list (bitcoin-dev) proposing a design that falls\nvictim to this attack. It's a subtle issue and we probably don't want to\nencourage it.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/9c383745/attachment.sig>"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-04T18:40:53",
                "message_text_only": "Interesting point.\n\nThe script is under your control, so you should be able to ensure that you\nare always using a correctly constructed midstate, e.g., something like:\n\nscriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>\nOP_SHA256STREAM\n<hash> OP_EQUALVERIFY\n\nwould hash all the elements on the stack and compare to a known hash.\nHow is that sort of thing weak to midstateattacks?\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Oct 4, 2019 at 4:16 AM Peter Todd <pete at petertodd.org> wrote:\n\n> On Thu, Oct 03, 2019 at 10:02:14PM -0700, Jeremy via bitcoin-dev wrote:\n> > Awhile back, Ethan and I discussed having, rather than OP_CAT, an\n> > OP_SHA256STREAM that uses the streaming properties of a SHA256 hash\n> > function to allow concatenation of an unlimited amount of data, provided\n> > the only use is to hash it.\n> >\n> > You can then use it perhaps as follows:\n> >\n> > // start a new hash with item\n> > OP_SHA256STREAM  (-1) -> [state]\n> > // Add item to the hash in state\n> > OP_SHA256STREAM n [item] [state] -> [state]\n> > // Finalize\n> > OP_SHA256STREAM (-2) [state] -> [Hash]\n> >\n> > <-1> OP_SHA256STREAM <tag> <subnode 2> <subnode 3> <3> OP_SHA256STREAM\n> <-2>\n> > OP_SHA256STREAM\n>\n> One issue with this is the simplest implementation where the state is just\n> raw\n> bytes would expose raw SHA256 midstates, allowing people to use them\n> directly;\n> preventing that would require adding types to the stack. Specifically I\n> could\n> write a script that rather than initializing the state correctly from the\n> official IV, instead takes an untrusted state as input.\n>\n> SHA256 isn't designed to be used in situations where adversaries control\n> the\n> initialization vector. I personally don't know one way or the other if\n> anyone\n> has analyzed this in detail, but I'd be surprised if that's secure. I\n> considered adding midstate support to OpenTimestamps but decided against\n> it for\n> exactly that reason.\n>\n> I don't have the link handy but there's even an example of an experienced\n> cryptographer on this very list (bitcoin-dev) proposing a design that falls\n> victim to this attack. It's a subtle issue and we probably don't want to\n> encourage it.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/bda6e24c/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2019-10-05T15:49:02",
                "message_text_only": "On Fri, Oct 04, 2019 at 11:40:53AM -0700, Jeremy wrote:\n> Interesting point.\n> \n> The script is under your control, so you should be able to ensure that you\n> are always using a correctly constructed midstate, e.g., something like:\n> \n> scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>\n> OP_SHA256STREAM\n> <hash> OP_EQUALVERIFY\n> \n> would hash all the elements on the stack and compare to a known hash.\n> How is that sort of thing weak to midstateattacks?\n\nObviously with care you can get the computation right. But at that point what's\nthe actual advantage over OP_CAT?\n\nWe're limited by the size of the script anyway; if the OP_CAT output size limit\nis comparable to that for almost anything you could use SHA256STREAM on you\ncould just as easily use OP_CAT, followed by a single OP_SHA256.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/213e4e81/attachment.sig>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-06T08:46:59",
                "message_text_only": "Good morning Peter, Jeremy, and lists,\n\n> On Fri, Oct 04, 2019 at 11:40:53AM -0700, Jeremy wrote:\n>\n> > Interesting point.\n> > The script is under your control, so you should be able to ensure that you\n> > are always using a correctly constructed midstate, e.g., something like:\n> > scriptPubKey: <-1> OP_SHA256STREAM DEPTH OP_SHA256STREAM <-2>\n> > OP_SHA256STREAM\n> > <hash> OP_EQUALVERIFY\n> > would hash all the elements on the stack and compare to a known hash.\n> > How is that sort of thing weak to midstateattacks?\n>\n> Obviously with care you can get the computation right. But at that point what's\n> the actual advantage over OP_CAT?\n>\n> We're limited by the size of the script anyway; if the OP_CAT output size limit\n> is comparable to that for almost anything you could use SHA256STREAM on you\n> could just as easily use OP_CAT, followed by a single OP_SHA256.\n\nTheoretically, `OP_CAT` is less efficient.\n\nIn cases where the memory area used to back the data cannot be resized, new backing memory must be allocated elsewhere and the existing data copied.\nThis leads to possible O( n^2 ) behavior for `OP_CAT` (degenerate case where we add 1 byte per `OP_CAT` and each time find that the memory area currently in use is exactly fitting the data and cannot be resized in-place).\n\n`OP_SHASTREAM` would not require new allocations once the stream state is in place and would not require any copying.\n\n\nThis may be relevant in considering the cost of executing `OP_CAT`.\n\nAdmittedly a sufficiently-limited  maximum `OP_CAT` output would be helpful in reducing the worst-case `OP_CAT` behavior.\nThe question is what limit would be reasonable.\n64 bytes feels too small if one considers Merkle tree proofs, due to mentioned issues of lack of typechecking.\n\n\nRegards,\nZmnSCPxj\n\n\n>\n> --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n>\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev"
            },
            {
                "author": "Peter Todd",
                "date": "2019-10-06T09:12:21",
                "message_text_only": "On Sun, Oct 06, 2019 at 08:46:59AM +0000, ZmnSCPxj wrote:\n> > Obviously with care you can get the computation right. But at that point what's\n> > the actual advantage over OP_CAT?\n> >\n> > We're limited by the size of the script anyway; if the OP_CAT output size limit\n> > is comparable to that for almost anything you could use SHA256STREAM on you\n> > could just as easily use OP_CAT, followed by a single OP_SHA256.\n> \n> Theoretically, `OP_CAT` is less efficient.\n> \n> In cases where the memory area used to back the data cannot be resized, new backing memory must be allocated elsewhere and the existing data copied.\n> This leads to possible O( n^2 ) behavior for `OP_CAT` (degenerate case where we add 1 byte per `OP_CAT` and each time find that the memory area currently in use is exactly fitting the data and cannot be resized in-place).\n\nIn even that degenerate case allocators also free memory.\n\nAnyway, every execution step in script evaluation has a maximum output size,\nand the number of steps is limited. At worst you can allocate the entire\npossible stack up-front for relatively little cost (eg fitting in the MB or two\nthat is a common size for L2 cache).\n\n> Admittedly a sufficiently-limited  maximum `OP_CAT` output would be helpful in reducing the worst-case `OP_CAT` behavior.\n> The question is what limit would be reasonable.\n> 64 bytes feels too small if one considers Merkle tree proofs, due to mentioned issues of lack of typechecking.\n\n256 bytes is more than enough for even the most complex summed merkle tree with\n512-byte hashes and full-sized sum commitments. Yet that's still less than the\n~500byte limit proposed elsewhere.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191006/13e0402e/attachment.sig>"
            },
            {
                "author": "Lloyd Fournier",
                "date": "2019-10-06T07:02:52",
                "message_text_only": "Hi Thread,\n\nI made a reply to the OP but didn't \"reply all\" so it just went directly to\nEthan. Since the comments were interesting I'll attempt to salvage them by\nposting them in full:\n\n== Lloyd's post ==\nHi Ethan,\n\nI'd be interested to know what protocols you need OP_CAT for. I'm trying to\nfigure out if there really exists any script based protocol that doesn't\nhave a more efficient scriptless counterpart.  For example,\nA\u00b2L[1] achieves the same thing as Tumblebit but requires no script. I can\nimagine paying based on a merkle path could be useful, but a protocol was\nrecently suggested on lightning-dev [2] that does this but without OP_CAT\n(and without any script!).\n\n\n[1] https://eprint.iacr.org/2019/589.pdf\n[2]\nhttps://www.mail-archive.com/lightning-dev@lists.linuxfoundation.org/msg01427.html\n(*I linked to the wrong thread in the original email*).\n\nLL\n\n== Ethan's response ==\nHi Lloyd,\n\nThanks for your response. I am not sure if you intended to take this off\nlist or not.\n\nI plan to at some point to enumerate in detail protocols that OP_CAT would\nbenefit. A more important point is that OP_CAT is a basic building block\nand that we don't know what future protocols it would allow. In my own\nresearch I have avoiding going down certain paths because it isn't worth\nthe time to investigate knowing that OP_CAT wouldn't make the protocol\npractical.\n\nIn regards to scriptless scripts they almost always require an interactive\nprotocol and sometimes ZKPs. A2L is very impressive but like TumbleBit it\nplaces a large burden on the developer. Additionally I am aware of no way\nto reveal a subset of preimages with scriptless scripts, do a conditioned\nreveal i.e. these preimages can only spend under these two pubkeys and\ntimelockA where after timelockZ this other pubkey can spend without a\npreimages. Scriptless scripts are a fantastic tool but they shouldn't be\nthe only tool that we have.\n\nI'm not sure I follow what you are saying with [2]\n\nThis brings me back a philosophical point:\nBitcoin should give people basic tools to build protocols without first\nknowing what all those protocols are especially when those tools have very\nlittle downside.\n\nI really appreciate your comments.\n\nThanks,\nEthan\n==\n\n*Back to normal thread*\n\nHi Ethan,\n\nThanks for the insightful reply and sorry for my mailing list errors.\n\n> I plan to at some point to enumerate in detail protocols that OP_CAT\nwould benefit.\n\nSweet. Thanks.\n\n> Additionally I am aware of no way to reveal a subset of preimages with\nscriptless scripts, do a conditioned reveal i.e. these preimages can only\nspend under these two pubkeys and timelockA where after timelockZ this\nother pubkey can spend without a preimages. Scriptless scripts are a\nfantastic tool but they shouldn't be the only tool that we have.\n\nYes. With adaptor signatures there is no way to reveal more than one\npre-image; you are limited to revealing a single scalar. But you can have\nmultiple transactions spending from the same output, each with a different\nset of scriptless conditions (absolute time locks, relative time locks and\npre-image reveal). This is enough to achieve what I think you are\ndescribing. FWIW there's a growing consensus that you can do lightning\nwithout script [1]. Perhaps we can't do everything with this technique. My\ncurrent focus is figuring out what useful things we can't do like this\n(even if we were to go wild and add whatever opcodes we wanted). So far it\nlooks like covenants are the main exception.\n\n> I'm not sure I follow what you are saying with [2]\n\nThat is perfectly understandable as I linked the wrong thread (sorry!).\nHere's the right one:\nhttps://www.mail-archive.com/lightning-dev@lists.linuxfoundation.org/msg01427.html\n\nI was pointing to the surprising result that you can actually pay for a\nmerkle path with a particular merkle root leading to a particular leaf that\nyou're interested in without validating the merkle path on chain (e.g.\nOP_CAT and OP_SHA256). The catch is that the leaves have to be pedersen\ncommitments and you prove the existence of your data in the merkle root by\nshowing an opening to the leaf pedersen commitment. This may not be general\nenough to cover every merkle tree use case (but I'm not sure what those\nare!).\n\n> This brings me back a philosophical point:\n> Bitcoin should give people basic tools to build protocols without first\nknowing what all those protocols are especially when those tools have very\nlittle downside.\n\nThis is a really powerful idea. But I've started feeling like you have to\njust design the layer 2 protocols first and then design layer 1! It seems\nlike almost every protocol that people want to make requires very\nparticular fundamental changes: SegWit for LN-penalty and NOINPUT for eltoo\nfor example. On top of that it seems like just having the right signature\nscheme (schnorr) at layer 1 is enough to enable most useful stuff in an\nelegant way.\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017309.html\n\nCheers,\n\nLL\n\nOn Fri, Oct 4, 2019 at 1:08 AM Ethan Heilman <eth3rs at gmail.com> wrote:\n\n> To avoid derailing the NO_INPUT conversation, I have changed the\n> subject to OP_CAT.\n>\n> Responding to:\n> \"\"\"\n> * `SIGHASH` flags attached to signatures are a misdesign, sadly\n> retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> par with:\n> [..]\n> * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> [..]\n> \"\"\"\n>\n> OP_CAT is an extremely valuable op code. I understand why it was\n> removed as the situation at the time with scripts was dire. However\n> most of the protocols I've wanted to build on Bitcoin run into the\n> limitation that stack values can not be concatenated. For instance\n> TumbleBit would have far smaller transaction sizes if OP_CAT was\n> supported in Bitcoin. If it happens to me as a researcher it is\n> probably holding other people back as well. If I could wave a magic\n> wand and turn on one of the disabled op codes it would be OP_CAT.  Of\n> course with the change that size of each concatenated value must be 64\n> Bytes or less.\n>\n>\n> On Tue, Oct 1, 2019 at 10:04 PM ZmnSCPxj via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > Good morning lists,\n> >\n> > Let me propose the below radical idea:\n> >\n> > * `SIGHASH` flags attached to signatures are a misdesign, sadly retained\n> from the original BitCoin 0.1.0 Alpha for Windows design, on par with:\n> >   * 1 RETURN\n> >   * higher-`nSequence` replacement\n> >   * DER-encoded pubkeys\n> >   * unrestricted `scriptPubKey`\n> >   * Payee-security-paid-by-payer (i.e. lack of P2SH)\n> >   * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> >   * transaction malleability\n> >   * probably many more\n> >\n> > So let me propose the more radical excision, starting with SegWit v1:\n> >\n> > * Remove `SIGHASH` from signatures.\n> > * Put `SIGHASH` on public keys.\n> >\n> > Public keys are now encoded as either 33-bytes (implicit `SIGHASH_ALL`)\n> or 34-bytes (`SIGHASH` byte, followed by pubkey type, followed by pubkey\n> coordinate).\n> > `OP_CHECKSIG` and friends then look at the *public key* to determine\n> sighash algorithm rather than the signature.\n> >\n> > As we expect public keys to be indirectly committed to on every output\n> `scriptPubKey`, this is automatically output tagging to allow particular\n> `SIGHASH`.\n> > However, we can then utilize the many many ways to hide public keys away\n> until they are needed, exemplified in MAST-inside-Taproot.\n> >\n> > I propose also the addition of the opcode:\n> >\n> >     <sighash> <pubkey> OP_SETPUBKEYSIGHASH\n> >\n> > * `sighash` must be one byte.\n> > * `pubkey` may be the special byte `0x1`, meaning \"just use the Taproot\n> internal pubkey\".\n> > * `pubkey` may be 33-byte public key, in which case the `sighash` byte\n> is just prepended to it.\n> > * `pubkey` may be 34-byte public key with sighash, in which case the\n> first byte is replaced with `sighash` byte.\n> > * If `sighash` is `0x00` then the result is a 33-byte public key (the\n> sighash byte is removed) i.e. `SIGHASH_ALL` implicit.\n> >\n> > This retains the old feature where the sighash is selected at\n> time-of-spending rather than time-of-payment.\n> > This is done by using the script:\n> >\n> >     <pubkey> OP_SETPUBKEYSIGHASH OP_CHECKSIG\n> >\n> > Then the sighash can be put in the witness stack after the signature,\n> letting the `SIGHASH` flag be selected at time-of-signing, but only if the\n> SCRIPT specifically is formed to do so.\n> > This is malleability-safe as the signature still commits to the\n> `SIGHASH` it was created for.\n> >\n> > However, by default, public keys will not have an attached `SIGHASH`\n> byte, implying `SIGHASH_ALL` (and disallowing-by-default non-`SIGHASH_ALL`).\n> >\n> > This removes the problems with `SIGHASH_NONE` `SIGHASH_SINGLE`, as they\n> are allowed only if the output specifically says they are allowed.\n> >\n> > Would this not be a superior solution?\n> >\n> > Regards,\n> > ZmnSCPxj\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191006/f0e27c02/attachment-0001.html>"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2019-10-09T16:56:51",
                "message_text_only": "On Thu, Oct 03, 2019 at 11:05:52AM -0400, Ethan Heilman wrote:\n> To avoid derailing the NO_INPUT conversation, I have changed the\n> subject to OP_CAT.\n> \n> Responding to:\n> \"\"\"\n> * `SIGHASH` flags attached to signatures are a misdesign, sadly\n> retained from the original BitCoin 0.1.0 Alpha for Windows design, on\n> par with:\n> [..]\n> * `OP_CAT` and `OP_MULT` and `OP_ADD` and friends\n> [..]\n> \"\"\"\n> \n> OP_CAT is an extremely valuable op code. I understand why it was\n> removed as the situation at the time with scripts was dire. However\n> most of the protocols I've wanted to build on Bitcoin run into the\n> limitation that stack values can not be concatenated. For instance\n> TumbleBit would have far smaller transaction sizes if OP_CAT was\n> supported in Bitcoin. If it happens to me as a researcher it is\n> probably holding other people back as well. If I could wave a magic\n> wand and turn on one of the disabled op codes it would be OP_CAT.  Of\n> course with the change that size of each concatenated value must be 64\n> Bytes or less.\n>\n\nJust throwing my two cents in here - as others have noted, OP_CAT\nlets you create Merkle trees (allowing e.g. log-sized accountable\nthreshold sigs, at least in a post-Schnorr future).\n\nIt also allows manipulating signatures - e.g. forcing the revelation\nof discrete logs by requiring the user use the (1/2) point as a nonce\n(this starts with 11 zero bytes, which no other computationally\naccessible point does), or by requiring two sigs with the same nonce.\n\nIt also lets you do proof-of-work-like computations on hashes or\ncurvepoints; or enforce that EC points come from a hash and have\nno known discrete log. You can also switch on hashes, something\ncurrently impossible because of the 4-byte limitation on numeric\nopcodes. I don't have specific application of these in mind but\ndefinitely have cut off many lines of inquiry because they were\nimpossible.\n\nYou could build a crappy Lamport signature, though the key would\nbe so big that you'd never do this pre-MAST :P.\n\n\n-- \nAndrew Poelstra\nDirector of Research, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\nThe sun is always shining in space\n    -Justin Lewis-Webster\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191009/8edaa1f9/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "OP_CAT was Re: Continuing the discussion about noinput / anyprevout",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Jeremy",
                "Lloyd Fournier",
                "Peter Todd",
                "ZmnSCPxj",
                "Andrew Poelstra",
                "Ethan Heilman"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 59639
        }
    },
    {
        "title": "[bitcoin-dev] PSBT global key for network",
        "thread_messages": [
            {
                "author": "Jimmy Song",
                "date": "2019-10-03T20:14:22",
                "message_text_only": "Hey all,\n\nI wanted to propose a new key in the global context for BIP174,\nPartially-Signed Bitcoin Transactions.\n\n= Rationale\n\nEach signer should make sure that the inputs being referenced in the PSBT\nexist (with the exception of a Proof-of-Reserves input). In order to do\nthis, it's critical to know which network the coins are on (mainnet or\ntestnet). This could potentially be extended to other networks should they\nwant to use something like PSBT, much in the same way that HD keys from\nBIP0044 reserved 0' and 1' as coins for mainnet Bitcoin and testnet Bitcoin\nrespectively.\n\n= Proposal\n\nAdd the key 0x03 for network in the global key-value store. Value is a\nvariable integer with 0x00 indicating Bitcoin mainnet and 0x01 indicating\nBitcoin testnet. Other coins that want to use the PSBT should use the coin\nnetwork number from SLIP-0044 with the high bit removed.\n\n---------------------------\n\nBest,\n\nJimmy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/36eecd03/attachment.html>"
            },
            {
                "author": "\u6728\u30ce\u4e0b\u3058\u3087\u306a",
                "date": "2019-10-04T05:54:52",
                "message_text_only": "Hi Jimmy,\n\nThe only time I could see this being a problem is in the case of a\nfork-coin.\nOtherwise the likelihood that two unrelated networks could have a tx with\nan id that is identical are low.\n\nEverything included in PSBT thus far is info for verifying something\nhelpful, and providing the information needed for signing and verifying\nwhat will be signed.\n\nAdding a network section will be the inverse of that. The info doesn't help\nyou verify anything, since I could lie about the network, and you will need\nto go out and check the network is as the PSBT says it is anyways.\nNetwork is also not needed for signing.\n\nIn fact, come to think of it, even if there was a fork-coin incident, even\nif you were able to separate PSBTs via network info, it won't matter if\nthere's no replay protection anyways, so giving a false sense of security\nin thinking \"I have explicitly stated my network so I should be ok\"\n(developers will think this, I guarantee) is actually a security minus IMO.\n\nCurrently BitcoinJS only uses network parameters to allow for the use of\naddresses in addOutput... but I'm starting to think we should remove it...\nnot sure...\n\nCheers,\nJon\n\n2019\u5e7410\u67084\u65e5(\u91d1) 11:04 Jimmy Song via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> Hey all,\n>\n> I wanted to propose a new key in the global context for BIP174,\n> Partially-Signed Bitcoin Transactions.\n>\n> = Rationale\n>\n> Each signer should make sure that the inputs being referenced in the PSBT\n> exist (with the exception of a Proof-of-Reserves input). In order to do\n> this, it's critical to know which network the coins are on (mainnet or\n> testnet). This could potentially be extended to other networks should they\n> want to use something like PSBT, much in the same way that HD keys from\n> BIP0044 reserved 0' and 1' as coins for mainnet Bitcoin and testnet Bitcoin\n> respectively.\n>\n> = Proposal\n>\n> Add the key 0x03 for network in the global key-value store. Value is a\n> variable integer with 0x00 indicating Bitcoin mainnet and 0x01 indicating\n> Bitcoin testnet. Other coins that want to use the PSBT should use the coin\n> network number from SLIP-0044 with the high bit removed.\n>\n> ---------------------------\n>\n> Best,\n>\n> Jimmy\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \n-----BEGIN PGP PUBLIC KEY BLOCK-----\nComment: http://openpgpjs.org\n\nxsBNBFTmJ8oBB/9rd+7XLxZG/x/KnhkVK2WBG8ySx91fs+qQfHIK1JrakSV3\nx6x0cK3XLClASLLDomm7Od3Q/fMFzdwCEqj6z60T8wgKxsjWYSGL3mq8ucdv\niBjC3wGauk5dQKtT7tkCFyQQbX/uMsBM4ccGBICoDmIJlwJIj7fAZVqGxGOM\nbO1RhYb4dbQA2qxYP7wSsHJ6/ZNAXyEphOj6blUzdqO0exAbCOZWWF+E/1SC\nEuKO4RmL7Imdep7uc2Qze1UpJCZx7ASHl2IZ4UD0G3Qr3pI6/jvNlaqCTa3U\n3/YeJwEubFsd0AVy0zs809RcKKgX3W1q+hVDTeWinem9RiOG/vT+Eec/ABEB\nAAHNI2tpbm9zaGl0YSA8a2lub3NoaXRham9uYUBnbWFpbC5jb20+wsByBBAB\nCAAmBQJU5ifRBgsJCAcDAgkQRB9iZ30dlisEFQgCCgMWAgECGwMCHgEAAC6Z\nB/9otobf0ASHYdlUBeIPXdDopyjQhR2RiZGYaS0VZ5zzHYLDDMW6ZIYm5CjO\nFc09ETLGKFxH2RcCOK2dzwz+KRU4xqOrt/l5gyd50cFE1nOhUN9+/XaPgrou\nWhyT9xLeGit7Xqhht93z2+VanTtJAG6lWbAZLIZAMGMuLX6sJDCO0GiO5zxa\n02Q2D3kh5GL57A5+oVOna12JBRaIA5eBGKVCp3KToT/z48pxBe3WAmLo0zXr\nhEgTSzssfb2zTwtB3Ogoedj+cU2bHJvJ8upS/jMr3TcdguySmxJlGpocVC/e\nqxq12Njv+LiETOrD8atGmXCnA+nFNljBkz+l6ADl93jHzsBNBFTmJ9EBCACu\nQq9ZnP+aLU/Rt6clAfiHfTFBsJvLKsdIKeE6qHzsU1E7A7bGQKTtLEnhCCQE\nW+OQP+sgbOWowIdH9PpwLJ3Op+NhvLlMxRvbT36LwCmBL0yD7bMqxxmmVj8n\nvlMMRSe4wDSIG19Oy7701imnHZPm/pnPlneg/Meu/UffpcDWYBbAFX8nrXPY\nvkVULcI/qTcCxW/+S9fwoXjQhWHaiJJ6y3cYOSitN31W9zgcMvLwLX3JgDxE\nflkwq/M+ZkfCYnS3GAPEt8GkVKy2eHtCJuNkGFlCAmKMX0yWzHRAkqOMN5KP\nLFbkKY2GQl13ztWp82QYJZpj5af6dmyUosurn6AZABEBAAHCwF8EGAEIABMF\nAlTmJ9QJEEQfYmd9HZYrAhsMAABKbgf/Ulu5JAk4fXgH0DtkMmdkFiKEFdkW\n0Wkw7Vhd5eZ4NzeP9kOkD01OGweT9hqzwhfT2CNXCGxh4UnvEM1ZMFypIKdq\n0XpLLJMrDOQO021UjAa56vHZPAVmAM01z5VzHJ7ekjgwrgMLmVkm0jWKEKaO\nn/MW7CyphG7QcZ6cJX2f6uJcekBlZRw9TNYRnojMjkutlOVhYJ3J78nc/k0p\nkcgV63GB6D7wHRF4TVe4xIBqKpbBhhN+ISwFN1z+gx3lfyRMSmiTSrGdKEQe\nXSIQKG8XZQZUDhLNkqPS+7EMV1g7+lOfT4GhLL68dUXDa1e9YxGH6zkpVECw\nSpe3vsHZr6CqFg==\n=/vUJ\n-----END PGP PUBLIC KEY BLOCK-----\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/8730e81e/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "PSBT global key for network",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "\u6728\u30ce\u4e0b\u3058\u3087\u306a",
                "Jimmy Song"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5403
        }
    },
    {
        "title": "[bitcoin-dev] OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY)",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2019-10-03T23:22:47",
                "message_text_only": "I've updated the BIP to no longer be based on Taproot, and instead based on\na OP_NOP upgrade. The example implementation and tests have also been\nupdated.\n\nBIP:\nhttps://github.com/JeremyRubin/bips/blob/op-secure-the-bag/bip-secure-the-bag.mediawiki\nImplementation:\nhttps://github.com/bitcoin/bitcoin/compare/master...JeremyRubin:securethebag_master\n\nThe BIP defines OP_NOP4 with the same semantics as previously presented.\nThis enables OP_SECURETHEBAG for segwit and bare script, but not p2sh\n(because of hash cycle, it's impossible to put the redeemscript on the\nscriptSig without changing the bag hash). The implementation also makes a\nbare OP_SECURETHEBAG script standard as that is a common use case.\n\nTo address Russel's feedback, once Tapscript is fully prepared (with more\nthorough script parsing improvements), multibyte opcodes can be more\ncleanly specified.\n\nBest,\n\nJeremy\n\nn.b. the prior BIP version remains at\nhttps://github.com/JeremyRubin/bips/blob/op-secure-the-bag-taproot/bip-secure-the-bag.mediawiki\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Mon, Jul 8, 2019 at 3:25 AM Dmitry Petukhov <dp at simplexum.com> wrote:\n\n> If you make ANYPREVOUTANYSCRIPT signature not the only signature that\n> controls this UTXO, but use it solely for restricting the spending\n> conditions such as the set of outputs, and require another signature\n> that would commit to the whole transaction, you can eliminate\n> malleability, for the price of additional signature, of course.\n>\n> <control-sig> <control-P> CHECKSIG <P> CHECKSIG\n>\n> (CHECKMULTISIG/CHECKSIGADD might be used instead)\n>\n> where control-P can even be a pubkey of a key that is publicly known,\n> and the whole purpose of control-sig would be to restrict the outputs\n> (control-sig would be created with flags meaning ANYPREVOUTANYSCRIPT).\n> Because control-sig does not depend on the script and on the current\n> input, there should be no circular dependency, and it can be part of\n> the redeem script.\n>\n> P would be the pubkey of the actual key that is needed to spend this\n> UTXO, and the signature of P can commit to all the inputs and outputs,\n> preventing malleability.\n>\n> I would like to add that it may make sense to just have 2 additional\n> flags for sighash: NOPREVOUT and NOSCRIPT.\n>\n> NOPREVOUT would mean that previous output is not committed to, and when\n> combined with ANYONECANPAY, this will mean ANYPREVOUT/NOINPUT:\n> ANYONECANPAY means exclude all inputs except the current, and NOPREVOUT\n> means exclude the current input. Thus NOPREVOUT|ANYONECANPAY = NOINPUT\n>\n> With NOPREVOUT|ANYONECANPAY|NOSCRIPT you would have ANYPREVOUTANYSCRIPT\n>\n> with NOPREVOUT|NOSCRIPT you can commit to \"all the inputs beside the\n> current one\", which would allow to create a spending restriction like\n> \"this UTXO, and also one (or more) other UTXO\", which might be useful\n> to retroactively revoke or transfer the rights to spend certain UTXO\n> without actually moving it:\n>\n> think 'vault' UTXO that is controlled by Alice, but requires additional\n> 'control' UTXO to spend. Alice have keys for both 'vault' UTXO, and\n> 'control' UTXO, but Bob have only key for 'control' UTXO.\n>\n> If Bob learnsthat Alice's vault UTXO key is at risk of compromize,\n> he spends the control UTXO, and then Alice's ability to spend vault\n> UTXO vanishes.\n>\n> You can use this mechanism to transfer this right to spend if you\n> prepare a number of taproot branches with different pairs of (vault,\n> control) keys and a chain of transactions that each spend the previous\n> control UTXO such that the newly created UTXO becomes controlled by the\n> control key of the next pair, together with vault key in that pair.\n>\n> \u0412 Sat, 22 Jun 2019 23:43:22 -0700\n> Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > This is insufficient: sequences must be committed to because they\n> > affect TXID. As with scriptsigs (witness data fine to ignore). NUM_IN\n> > too.\n> >\n> > Any malleability makes this much less useful.\n> > --\n> > @JeremyRubin <https://twitter.com/JeremyRubin>\n> > <https://twitter.com/JeremyRubin>\n> >\n> >\n> > On Fri, Jun 21, 2019 at 10:31 AM Anthony Towns via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > > On Tue, Jun 18, 2019 at 04:57:34PM -0400, Russell O'Connor wrote:\n> > > > So with regards to OP_SECURETHEBAG, I am also \"not really seeing\n> > > > any\n> > > reason to\n> > > > complicate the spec to ensure the digest is precommitted as part\n> > > > of the opcode.\"\n> > >\n> > > Also, I think you can simulate OP_SECURETHEBAG with an ANYPREVOUT\n> > > (NOINPUT) sighash (Johnson Lau's mentioned this before, but not\n> > > sure if it's been spelled out anywhere); ie instead of constructing\n> > >\n> > >   X = Hash_BagHash( version, locktime, [outputs], [sequences],\n> > > num_in )\n> > >\n> > > and having the script be \"<X> OP_SECURETHEBAG\" you calculate an\n> > > ANYPREVOUT sighash for SIGHASH_ANYPREVOUTANYSCRIPT | SIGHASH_ALL:\n> > >\n> > >   Y = Hash_TapSighash( 0, 0xc1, version, locktime, [outputs], 0,\n> > >                        amount, sequence)\n> > >\n> > > and calculate a signature sig = Schnorr(P,m) for some pubkey P, and\n> > > make your script be \"<sig> <P> CHECKSIG\".\n> > >\n> > > That loses the ability to commit to the number of inputs or restrict\n> > > the nsequence of other inputs, and requires a bigger script (sig\n> > > and P are ~96 bytes instead of X's 32 bytes), but is otherwise\n> > > pretty much the same as far as I can tell. Both scripts are\n> > > automatically satisfied when revealed (with the correct set of\n> > > outputs), and don't need any additional witness data.\n> > >\n> > > If you wanted to construct \"X\" via script instead of hardcoding a\n> > > value because it got you generalised covenants or whatever; I think\n> > > you could get the same effect with CAT,LEFT, and RIGHT: you'd\n> > > construct Y in much the same way you construct X, but you'd then\n> > > need to turn that into a signature. You could do so by using pubkey\n> > > P=G and nonce R=G, which means you need to calculate\n> > > s=1+hash(G,G,Y)*1 -- calculating the hash part is easy, multiplying\n> > > it by 1 is easy, and to add 1 you can probably do something along\n> > > the lines of:\n> > >\n> > >     OP_DUP 4 OP_RIGHT 1 OP_ADD OP_SWAP 28 OP_LEFT OP_SWAP OP_CAT\n> > >\n> > > (ie, take the last 4 bytes, increment it using 4-byte arithmetic,\n> > > then cat the first 28 bytes and the result. There's overflow issues,\n> > > but I think they can be worked around either by allowing you to\n> > > choose different locktimes, or by more complicated script)\n> > >\n> > > Cheers,\n> > > aj\n> > >\n> > > _______________________________________________\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > >\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/216d81a2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "OP_SECURETHEBAG (supersedes OP_CHECKOUTPUTSVERIFY)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 7039
        }
    },
    {
        "title": "[bitcoin-dev] Chain width expansion",
        "thread_messages": [
            {
                "author": "Braydon Fuller",
                "date": "2019-10-04T00:38:36",
                "message_text_only": "Hi everyone,\n\nWe would like to share a paper for broad discussion, it is titled\n\"Bitcoin Chain Width Expansion Denial-of-Service Attacks\".\n\n>From the abstract: The attacks leverage unprotected resources for a\ndenial-of-service by filling the disk and exhausting the CPU with\nunnecessary header and block data. This forces the node to halt\noperation. The attack difficulty ranges from difficult to easy. There\nare currently limited guards for some of the attacks that require\ncheckpoints to be enabled. This paper describes a solution that does not\nrequire enabling or maintaining checkpoints and provides improved security.\n\nAs the checkpoints in Bitcoin Core have not been maintained or updated\nsince mid 2014, this is especially relevant. Bitcoin Core implements\nheaders-first synchronization, since 2014, that provides the base for\nthe further improvements upon that design.\n\nThe paper is available at:\nhttps://bcoin.io/papers/bitcoin-chain-expansion.pdf\n\nThe proposed solution has been implemented in Bcoin and is available at:\nhttps://github.com/bcoin-org/bcoin/tree/chain-expansion\n\nBest,\nBraydon Fuller"
            },
            {
                "author": "David A. Harding",
                "date": "2019-10-04T08:20:31",
                "message_text_only": "On Thu, Oct 03, 2019 at 05:38:36PM -0700, Braydon Fuller via bitcoin-dev wrote:\n> This paper describes a solution [to DoS attacks] that does not\n> require enabling or maintaining checkpoints and provides improved security.\n> [...] \n> The paper is available at:\n> https://bcoin.io/papers/bitcoin-chain-expansion.pdf\n\nHi Braydon,\n\nThank you for researching this important issue.  An alternative solution\nproposed some time ago (I believe originally by Gregory Maxwell) was a\nsoft fork to raise the minimum difficulty.  You can find discussion of\nit in various old IRC conversations[1,2] as well as in related changes\nto Bitcoin Core such as PR #9053 addining minimum chain work[3] and the\nassumed-valid change added in Bitcoin Core 0.14.0[4].\n\n[1] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.2016-10-27-19.01.log.html#l-121\n[2] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2017/bitcoin-core-dev.2017-03-02-19.01.log.html#l-57\n[3] https://github.com/bitcoin/bitcoin/pull/9053/commits/fd46136dfaf68a7046cf7b8693824d73ac6b1caf\n[4] https://bitcoincore.org/en/2017/03/08/release-0.14.0/#assumed-valid-blocks\n\nThe solutions proposed in section 4.2 and 4.3 of your paper have the\nadvantage of not requiring any consensus changes.  However, I find it\nhard to analyze the full consequences of the throttling solution in\n4.3 and the pruning solution in 4.2.  If we assume a node is on the\nmost-PoW valid chain and that a huge fork is unlikely, it seems fine.\nBut I worry that the mechanisms could also be used to keep a node that\nsynced to a long-but-lower-PoW chain on that false chain (or other false\nchain) indefinitely even if it had connections to honest peers that\ntried to tell it about the most-PoW chain.\n\nFor example, with your maximum throttle of 5 seconds between\n`getheaders` requests and the `headers` P2P message maximum of 2,000\nheaders per instance, it would take about half an hour to get a full\nchain worth of headers.  If a peer was disconnected before sending\nenough headers to establish they were on the most-PoW chain, your\npruning solution would delete whatever progress was made, forcing the\nnext peer to start from genesis and taking them at least half an hour\ntoo.  On frequently-suspended laptops or poor connections, it's possible\na node could be be operational for a long time before it kept the same\nconnection open for half an hour.  All that time, it would be on a\ndishonest chain.\n\nBy comparison, I find it easy to analyze the effect of raising the\nminimum difficulty.  It is a change to the consensus rules, so it's\nsomething we should be careful about, but it's the kind of\nbasically-one-line change that I expect should be easy for a large\nnumber of people to review directly.  Assuming the choice of a new\nminimum (and what point in the chain to use it) is sane, I think it\nwould be easy to get acceptance, and I think it would further be easy\nincrease it again every five years or so as overall hashrate increases.\n\n-Dave"
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-04T19:51:26",
                "message_text_only": "On 10/4/19 1:20 AM, David A. Harding wrote:\n\n> On Thu, Oct 03, 2019 at 05:38:36PM -0700, Braydon Fuller via bitcoin-dev wrote:\n>> This paper describes a solution [to DoS attacks] that does not\n>> require enabling or maintaining checkpoints and provides improved security.\n>> [...] \n>> The paper is available at:\n>> https://bcoin.io/papers/bitcoin-chain-expansion.pdf\n> [..] But I worry that the mechanisms could also be used to keep a node that\n> synced to a long-but-lower-PoW chain on that false chain (or other false\n> chain) indefinitely even if it had connections to honest peers that\n> tried to tell it about the most-PoW chain.\n\nHere is an example: An attacker eclipses a target node during the\ninitial block download; all of the target's outgoing peers are the\nattacker. The attacker has a low work chain that is sent to the target.\nThe total chainwork for the low work chain is 0x09104210421039 at a\nheight of 593,975. The target is now in the state of a fully validated\nlow work dishonest chain. The target node then connects to an honest\npeer and learns about the honest chain. The chainwork of the honest\nchain is 0x085b67d9e07a751e53679d68 at a height of 593,975. The first\n69,500 headers of the honest chain would have a delay, however the\nremaining 52,4475 would not be delayed. Given a maximum of 5 seconds,\nthis would be a total delay of only 157 seconds."
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-11T21:24:27",
                "message_text_only": "On 10/4/19 1:20 AM, David A. Harding wrote:\n\n> On Thu, Oct 03, 2019 at 05:38:36PM -0700, Braydon Fuller via bitcoin-dev wrote:\n>> This paper describes a solution [to DoS attacks] that does not\n>> require enabling or maintaining checkpoints and provides improved security.\n>> [...] \n>> The paper is available at:\n>> https://bcoin.io/papers/bitcoin-chain-expansion.pdf\n> Hi Braydon,\n>\n> Thank you for researching this important issue.  An alternative solution\n> proposed some time ago (I believe originally by Gregory Maxwell) was a\n> soft fork to raise the minimum difficulty.  You can find discussion of\n> it in various old IRC conversations[1,2] as well as in related changes\n> to Bitcoin Core such as PR #9053 addining minimum chain work[3] and the\n> assumed-valid change added in Bitcoin Core 0.14.0[4].\n>\n> [1] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2016/bitcoin-core-dev.2016-10-27-19.01.log.html#l-121\n> [2] http://www.erisian.com.au/meetbot/bitcoin-core-dev/2017/bitcoin-core-dev.2017-03-02-19.01.log.html#l-57\n> [3] https://github.com/bitcoin/bitcoin/pull/9053/commits/fd46136dfaf68a7046cf7b8693824d73ac6b1caf\n> [4] https://bitcoincore.org/en/2017/03/08/release-0.14.0/#assumed-valid-blocks\n>\nOkay, here is an overview of what I have found for the minimum\ndifficulty proposal:\n\nIt describes having a new consensus rule to not fork or accept headers\nprior to, or below, a minimum difficulty once the best chain work is\nachieved at release time of the software. This would be instead of the\nrule to not fork before the last checkpoint, as checkpoints are removed.\n\nIt has an advantage to the existing checkpoint solution as it does not\nrequire checkpoints to be enabled. This is not a surprise as the\nproposal was to remove checkpoints entirely. It would increase the cost\nof the attack without checkpoints. Long header chains would need to be\nbuilt using this minimum difficulty, instead of the current lowest\ndifficulty of the genesis block. The exact cost of that is not yet\ncalculated.\n\nThere are a few caveats with the approach mentioned; nodes are\nvulnerable if the initial loader peer is the attacker, it could leave\nminority hashpower without an ability to softfork away during a\ncontentious hardfork, and requires period consensus changes to continue\nto maintain:\n\u00a0 - Nodes are vulnerable during the initial sync when joining the\nnetwork until the minimum chainwork is achieved. This is possible if the\nloader peer is the attacker. To mitigate this there would need to be a\nminimum chainwork defined based on the current chainwork. However, such\ncould also be used to prevent nodes from joining the network as it's\nrejecting rather that throttling.\n\u00a0 - A contentious hardfork could leave a minority hashpower without an\nability to softfork away without agreeing on a hardfork. This was the\nreason why the minimum difficulty was about 10 devices instead of 10,000.\n\u00a0 - It's technically a consensus change each time the minimum difficulty\nor best chainwork is updated. It is a similar consensus change as\nmaintaining the last checkpoint, as it's used to prevent forking prior\nto the last checkpoint.\n\nI think the solution proposed in the Bitcoin Chain Width Expansion paper\nsolves those issues by limiting chain width and throttling based on\nchainwork, instead of rejecting blocks based on the minimum difficulty."
            },
            {
                "author": "Tier Nolan",
                "date": "2019-10-04T23:31:18",
                "message_text_only": "Are you assuming no network protocol changes?\n\nAt root, the requirement is that peers can prove their total chain POW.\n\nSince each block has the height in the coinbase, a peer can send a short\nproof of height for a disconnected header and could assert the POW for that\nheader.\n\nEach peer could send the the N strongest headers (lowest digest/most POW)\nfor their main chain and prove the height of each one.\n\nThe total chain work can be estimated as N times the POW for the lowest in\nthe list.  This is an interesting property of how POW works.  The 10th best\nPOW block will have about 10% of the total POW.\n\nThe N blocks would be spread along the chain and the peer could ask for all\nheaders between any 2 of them and check the different in claimed POW.  If\ndishonesty is discovered, the peer can be banned and all info from that\npeer wiped.\n\nYou can apply the rule hierarchically.  The honest peers would have a much\nhigher POW chain.  You could ask the peer to give you the N strongest\nheaders between 2 headers that they gave for their best chain.  You can\ncheck that their height is between the two limits.\n\nThe peer would effectively be proving their total POW recursively.\n\nThis would require a new set of messages so you can request info about the\nbest chain.\n\nIt also has the nice feature that it allows you to see if multiple peers\nare on the same chain, since they will have the same best blocks.\n\nThe most elegant would be something like using SNARKS to directly prove\nthat your chain tip has a particular POW.  The download would go tip to\ngenesis, unlike now when it is in the other direction.\n\n------------------------------------------------------------------------\n\nIn regard to your proposal, I think the key is to limit things by peer,\nrather than globally.\n\nThe limit to header width should be split between peers.  If you have N\noutgoing peers, they get 1/N of your header download resources each.\n\nYou store the current best/most POW header chain and at least one\nalternative chain per outgoing peer.\n\nYou could still prune old chains based on POW, but the best chain and the\ncurrent chain for each outgoing peer should not be pruned.\n\nThe security assumption is that a node is connected to at least one honest\nnode.\n\nIf you split resources between all peers, then it prevents the dishonest\nnodes from flooding and wiping out the progress for the honest peer.\n\n- Message Limiting -\n\nI have the same objection here.  The message limiting should be per peer.\n\nAn honest peer who has just been connected to shouldn't suffer a penalty.\n\nYour point that it is only a few minutes anyway may make this point moot\nthough.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/ea973505/attachment.html>"
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-10T16:16:16",
                "message_text_only": "On 10/4/19 4:31 PM, Tier Nolan via bitcoin-dev wrote\n\n> [..] At root, the requirement is that peers can prove their total chain POW. [...]\nIndeed, it's currently necessary to receive all of the chain headers to\ndetermine. It would be interesting to have a succinct chainwork proof\nfor all cases. Chainwork being a sum of the total proof-of-work in a\nchain. Such proofs currently only require a few headers for common cases\nand the other cases can be identified.\n> In regard to your proposal, I think the key is to limit things by peer, rather than globally. [...]\n\nYeah, there should be enough width available for every active\nconnection, only one chain of headers is requested at a time per peer.\nPeer based limiting is susceptible to Sybil attacks; A peer could\nbroadcast a few low-work header chains, reconnect and repeat ad nauseam.\n\nThe delay for the next set of headers is based on the chainwork of the\nlast received headers from the peer. The peer could change identity and\nrun into the same limit. The unrequested header rate is tracked per peer.\n\nA header chain with more chainwork will be requested at a faster rate\nthan a header chain with less chainwork. The chainwork is compared to\nthe current fully validated chain. Honest peers with more chainwork will\nhave a time advantage over dishonest peers with less chainwork.\n\nFor example, let's assume a case that the initial chain of headers was\ndishonest and with low chainwork. The initial block download retrieves\nthe header chain from a single loader peer first. Once recent time is\nreached, header chains are downloaded from all outgoing peers. A single\nhonest peer will have an advantage over many dishonest peers. Thus, as\nyou have mentioned, there is a security assumption that there is at\nleast one connected honest node."
            },
            {
                "author": "Tier Nolan",
                "date": "2019-10-12T16:27:42",
                "message_text_only": "On Thu, Oct 10, 2019 at 5:20 PM Braydon Fuller via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>  It would be interesting to have a succinct chainwork proof\n> for all cases. Chainwork being a sum of the total proof-of-work in a\n> chain. Such proofs currently only require a few headers for common cases\n> and the other cases can be identified.\n>\n\nI wonder if a \"seed\" based system would be useful.\n\nA seed is defined as a header with a very low digest.\n\nWhen a new peer connects, you ask him to send you the header with the\nlowest digest on his main chain.\n\nChains ending at the strongest seeds are kept preferentially when\ndiscarding chains.\n\nThis requires a way to download chains backwards, which the protocol\ndoesn't support at the moment.\n\nThe most chain work chain is overwhelmingly likely to contain the header\nwith the strongest digest.\n\nThis means that the honest peer's chain would be kept preferentially.\n\nIt also means that a node that is synced to the main chain can easily\ndiscard noise from dishonest peers.  Before downloading, they could ask the\npeer to provide a header with at least 1% of the POW of the best header on\nthe main chain starting at the fork point.  If they can't then their fork\nprobably has less POW than the main chain.\n\n\n> A peer could\n> broadcast a few low-work header chains, reconnect and repeat ad nauseam.\n>\n\nI meant connected peer rather than peer.  If a peer disconnects and then\nreconnects as a new peer, then their allocation of bandwidth/RAM resets to\nzero.\n\nEach peer would be allocated a certain bandwidth per minute for headers as\nin a token bucket system.   New peers would start with empty buckets.\n\nIf an active (outgoing) peer is building on a header chain, then that chain\nis preferentially kept.  Essentially, the last chain that each outgoing\npeer built on may not be discarded.\n\nIn retrospect, that works out as the same as throttling peer download, just\nwith a different method for throttling.\n\nIn your system, peers who extend the best chain don't get throttled, but\nthe other peers do (but with a gradual transition).\n\nThis could be accomplished by adding 80 bytes into the peers bucket if it\nextends the main chain.\n\n\n> For example, let's assume a case that the initial chain of headers was\n> dishonest and with low chainwork. The initial block download retrieves\n> the header chain from a single loader peer first. Once recent time is\n> reached, header chains are downloaded from all outgoing peers.\n\n\nThe key it that it must not be possible to prevent a single honest peer\nfrom making progress by flooding with other peers and getting the honest\npeer's chain discarded.\n\nI think parallel downloading would be better than focusing on one peer\ninitially.  Otherwise, a dishonest peer can slowly send their headers to\nprevent moving to parallel mode.\n\nEach connected peer is given a bandwidth and RAM allowance.  If a connected\npeer forks off their own chain before reaching current time, then the fork\nis just discarded.\n\nThe RAM allowance would be sufficient to hold one header per minute since\ngenesis.\n\nThe header chains are relatively small (50MB), so it is not unreasonable to\nexpect the honest peer to send the entire chain in one go.\n\nI wonder if there is a formula that gives the minimum chain work required\nto have a particular chain length by now.\n\n1 minute per header would mean that the difficulty would increase every\nadjustment, so it couldn't be maintained without an exponentially rising\ntotal chain work.\n\nOn Sat, Oct 12, 2019 at 2:41 AM Braydon Fuller via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>   - Nodes are vulnerable during the initial sync when joining the\n> network until the minimum chainwork is achieved.\n\n\nNodes should stay \"headers-only\" until they have hit the threshold.\n\nIt isn't really any different from a checkpoint anyway.\n\nDownload headers until you hit this header is about the same as download\nheaders until you hit this chain work.\n\nIt would be different if header chains were downloaded from the final\ncheckpoint backwards.\n\nYou would start at a final checkpoint and work backwards.  Each ancestor\nheader is committed to by the final checkpoint, so it would not be possible\na dishonest peer to fool the node during IBD.\n\n\n> This is possible if the\n> loader peer is the attacker. To mitigate this there would need to be a\n> minimum chainwork defined based on the current chainwork. However, such\n> could also be used to prevent nodes from joining the network as it's\n> rejecting rather that throttling.\n>\n\nI think mixing two different concepts makes this problem more complex than\nneeded.\n\nIt looks like they are aiming for hard-coding\n\nA) \"The main chain has at least C chainwork\"\nB) \"All blocks after A is satisfied have at least X POW\"\n\nTo me, this is equivalent to a checkpoint, without it having it be called a\ncheckpoint.\n\nThe point about excluding checkpoints is that it means that (in theory) two\nclients can't end up on incompatible forks due to different checkpoints.\n\nThe \"checkpoint\" is replaced by a statement by the dev team that\n\n\"There exists at least one valid chain with C chainwork\"\n\nwhich is equivalent to\n\n\"The longest valid chain has at least C chainwork\"\n\nTwo client making those statements can't cause a permanent\nincompatibility.  If they pick a different C, then eventually, once the\nmain chain has more than the larger chain work, they will agree again.\n\nCheckpoints don't automatically heal.\n\nAdding in a minimum POW requirement could break the requirement for that to\nhappen.\n\nJust because B was met on the original main chain, a fork isn't required to\nmeet it.\n\n  - It's technically a consensus change each time the minimum difficulty\n> or best chainwork is updated. It is a similar consensus change as\n> maintaining the last checkpoint, as it's used to prevent forking prior\n> to the last checkpoint.\n>\n\nI agree on the min difficulty being a consensus change.\n\nThe minimum chain work is just the devs making a true statement and then\nusing it to optimize things.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/24bb4eaa/attachment.html>"
            },
            {
                "author": "Joachim Str\u00f6mbergson",
                "date": "2019-10-12T17:56:51",
                "message_text_only": "I like the backwards syncing idea. First you provide proof of your best block height via coinbase, then sync backwards. It solves lots of related problems. You know how much you can expect from the given peer.\n\nOn different note, one of the problems that I haven't seen mentioned here yet is the timewarp attack. It is relevant to some of the proposed solutions. It should be possible, IIRC, for a malicious node to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). This could produce chain that is ~2500 times longer than main chain without having multiple branches.\n\nI also agree that there is no big difference between hash checkpoints and \"min. diff. checkpoints\".\n\nJoachim\n\nSent with [ProtonMail](https://protonmail.com) Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Saturday, October 12, 2019 4:27 PM, Tier Nolan via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, Oct 10, 2019 at 5:20 PM Braydon Fuller via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>  It would be interesting to have a succinct chainwork proof\n>> for all cases. Chainwork being a sum of the total proof-of-work in a\n>> chain. Such proofs currently only require a few headers for common cases\n>> and the other cases can be identified.\n>\n> I wonder if a \"seed\" based system would be useful.\n>\n> A seed is defined as a header with a very low digest.\n>\n> When a new peer connects, you ask him to send you the header with the lowest digest on his main chain.\n>\n> Chains ending at the strongest seeds are kept preferentially when discarding chains.\n>\n> This requires a way to download chains backwards, which the protocol doesn't support at the moment.\n>\n> The most chain work chain is overwhelmingly likely to contain the header with the strongest digest.\n>\n> This means that the honest peer's chain would be kept preferentially.\n>\n> It also means that a node that is synced to the main chain can easily discard noise from dishonest peers.  Before downloading, they could ask the peer to provide a header with at least 1% of the POW of the best header on the main chain starting at the fork point.  If they can't then their fork probably has less POW than the main chain.\n>\n>> A peer could\n>> broadcast a few low-work header chains, reconnect and repeat ad nauseam.\n>\n> I meant connected peer rather than peer.  If a peer disconnects and then reconnects as a new peer, then their allocation of bandwidth/RAM resets to zero.\n>\n> Each peer would be allocated a certain bandwidth per minute for headers as in a token bucket system.   New peers would start with empty buckets.\n>\n> If an active (outgoing) peer is building on a header chain, then that chain is preferentially kept.  Essentially, the last chain that each outgoing peer built on may not be discarded.\n>\n> In retrospect, that works out as the same as throttling peer download, just with a different method for throttling.\n>\n> In your system, peers who extend the best chain don't get throttled, but the other peers do (but with a gradual transition).\n>\n> This could be accomplished by adding 80 bytes into the peers bucket if it extends the main chain.\n>\n>> For example, let's assume a case that the initial chain of headers was\n>> dishonest and with low chainwork. The initial block download retrieves\n>> the header chain from a single loader peer first. Once recent time is\n>> reached, header chains are downloaded from all outgoing peers.\n>\n> The key it that it must not be possible to prevent a single honest peer from making progress by flooding with other peers and getting the honest peer's chain discarded.\n>\n> I think parallel downloading would be better than focusing on one peer initially.  Otherwise, a dishonest peer can slowly send their headers to prevent moving to parallel mode.\n>\n> Each connected peer is given a bandwidth and RAM allowance.  If a connected peer forks off their own chain before reaching current time, then the fork is just discarded.\n>\n> The RAM allowance would be sufficient to hold one header per minute since genesis.\n>\n> The header chains are relatively small (50MB), so it is not unreasonable to expect the honest peer to send the entire chain in one go.\n>\n> I wonder if there is a formula that gives the minimum chain work required to have a particular chain length by now.\n>\n> 1 minute per header would mean that the difficulty would increase every adjustment, so it couldn't be maintained without an exponentially rising total chain work.\n>\n> On Sat, Oct 12, 2019 at 2:41 AM Braydon Fuller via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>   - Nodes are vulnerable during the initial sync when joining the\n>> network until the minimum chainwork is achieved.\n>\n> Nodes should stay \"headers-only\" until they have hit the threshold.\n>\n> It isn't really any different from a checkpoint anyway.\n>\n> Download headers until you hit this header is about the same as download headers until you hit this chain work.\n>\n> It would be different if header chains were downloaded from the final checkpoint backwards.\n>\n> You would start at a final checkpoint and work backwards.  Each ancestor header is committed to by the final checkpoint, so it would not be possible a dishonest peer to fool the node during IBD.\n>\n>> This is possible if the\n>> loader peer is the attacker. To mitigate this there would need to be a\n>> minimum chainwork defined based on the current chainwork. However, such\n>> could also be used to prevent nodes from joining the network as it's\n>> rejecting rather that throttling.\n>\n> I think mixing two different concepts makes this problem more complex than needed.\n>\n> It looks like they are aiming for hard-coding\n>\n> A) \"The main chain has at least C chainwork\"\n> B) \"All blocks after A is satisfied have at least X POW\"\n>\n> To me, this is equivalent to a checkpoint, without it having it be called a checkpoint.\n>\n> The point about excluding checkpoints is that it means that (in theory) two clients can't end up on incompatible forks due to different checkpoints.\n>\n> The \"checkpoint\" is replaced by a statement by the dev team that\n>\n> \"There exists at least one valid chain with C chainwork\"\n>\n> which is equivalent to\n>\n> \"The longest valid chain has at least C chainwork\"\n>\n> Two client making those statements can't cause a permanent incompatibility.  If they pick a different C, then eventually, once the main chain has more than the larger chain work, they will agree again.\n>\n> Checkpoints don't automatically heal.\n>\n> Adding in a minimum POW requirement could break the requirement for that to happen.\n>\n> Just because B was met on the original main chain, a fork isn't required to meet it.\n>\n>>   - It's technically a consensus change each time the minimum difficulty\n>> or best chainwork is updated. It is a similar consensus change as\n>> maintaining the last checkpoint, as it's used to prevent forking prior\n>> to the last checkpoint.\n>\n> I agree on the min difficulty being a consensus change.\n>\n> The minimum chain work is just the devs making a true statement and then using it to optimize things.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/d2a9a41c/attachment-0001.html>"
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-15T00:42:06",
                "message_text_only": "On 10/12/19 10:56 AM, Joachim Str\u00f6mbergson via bitcoin-dev wrote:\n\n> [...] First you provide proof of your best block height via coinbase [...]\n\nSo I don't think you can use the height in the coinbase for that\npurpose, as it's not possible to validate it without the previous\nheaders. That's common for more than just the height.\n\n> [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]\n\nIn that case, it would take about 7 minutes of block time seconds for\nthe next retarget period, every 2016 blocks, and the difficulty would\nadjust. The difficulty would adjust in that case as if 2 weeks of blocks\nhad been mined in 7 minutes. For the difficulty to remain the same the\ntime between blocks needs to be 10 minutes."
            },
            {
                "author": "Joachim Str\u00f6mbergson",
                "date": "2019-10-15T07:20:07",
                "message_text_only": "> > [...] First you provide proof of your best block height via coinbase [...]\n>\n> So I don't think you can use the height in the coinbase for that\n> purpose, as it's not possible to validate it without the previous\n> headers. That's common for more than just the height.\n\nYou can fake it of course, but it means you are spending money for PoW and I can't see viable attack here. You can commit to either lower height than actual or higher height, if you are malicious. If it is lower, then your chain will look weaker with some heuristic based on PoW of the tip and the chain length. So you are not going to do that. It thus seem it only makes sense to commit to higher than actual height. OK, this could convince me to be interested in your chain over others. So let's say the real chain is 600k blocks, you claim 1m blocks, and you prove 1m height block to me. So I can ask you about\n\n1) 2000 headers from the tip down\n2) AND another proof of height for the block at 1m-2000.\n\nTo be able to provide that you need to have such a chain and you can reuse any real subchain from mainnet. It's still possible that you can deliver that, but not for high difficulty.\n\nNow if time warp attack is blocked, you will have pretty hard time to create such a chain that would look strongest in cumulative work. I don't have actual numbers though, so I can't say this would mitigate everything.\n\n\n> > [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]\n>\n> In that case, it would take about 7 minutes of block time seconds for\n> the next retarget period, every 2016 blocks, and the difficulty would\n> adjust. The difficulty would adjust in that case as if 2 weeks of blocks\n> had been mined in 7 minutes. For the difficulty to remain the same the\n> time between blocks needs to be 10 minutes.\n\nThis calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window."
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-15T08:12:09",
                "message_text_only": "On 10/15/19 12:20 AM, Joachim Str\u00f6mbergson wrote:\n\n>>> [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]\n>> In that case, it would take about 7 minutes of block time seconds for\n>> the next retarget period, every 2016 blocks, and the difficulty would\n>> adjust. The difficulty would adjust in that case as if 2 weeks of blocks\n>> had been mined in 7 minutes. For the difficulty to remain the same the\n>> time between blocks needs to be 10 minutes.\n> This calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window.\n\nThis must be in reference to the non-overlapping difficulty calculation\nand off-by-one bug?"
            },
            {
                "author": "Joachim Str\u00f6mbergson",
                "date": "2019-10-15T15:50:29",
                "message_text_only": "> > > > [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]\n> > > > In that case, it would take about 7 minutes of block time seconds for\n> > > > the next retarget period, every 2016 blocks, and the difficulty would\n> > > > adjust. The difficulty would adjust in that case as if 2 weeks of blocks\n> > > > had been mined in 7 minutes. For the difficulty to remain the same the\n> > > > time between blocks needs to be 10 minutes.\n> > > > This calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window.\n>\n> This must be in reference to the non-overlapping difficulty calculation\n> and off-by-one bug?\n\nIndeed."
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-16T19:25:31",
                "message_text_only": "On 10/15/19 8:50 AM, Joachim Str\u00f6mbergson wrote:\n\n>>>>> [...] to generate much longer chain with superslow timestamp increase (~5 blocks in 1 second) without increasing difficulty (i.e. staying at min. diff.). [...]\n>>>>> In that case, it would take about 7 minutes of block time seconds for\n>>>>> the next retarget period, every 2016 blocks, and the difficulty would\n>>>>> adjust. The difficulty would adjust in that case as if 2 weeks of blocks\n>>>>> had been mined in 7 minutes. For the difficulty to remain the same the\n>>>>> time between blocks needs to be 10 minutes.\n>>>>> This calculation does not apply under time warp attack. You can fake timestamps of all blocks except for those relevant to the retarget calculation. Those are only the first and the last block in the 2016 block window.\n>> This must be in reference to the non-overlapping difficulty calculation\n>> and off-by-one bug?\n> Indeed.\n>\nYeah, limiting the width of the chain would not be effective unless the\ntimewarp off-by-one bug is resolved \u2014 the height can be extended instead.\n\nRate limiting based on chainwork would slow down a timewarped low work\nheader chain. There would be a maximum rate at which the headers could\nbe sent. It would be around 32KB/s. It would take about a month to send\n100GB."
            },
            {
                "author": "Tier Nolan",
                "date": "2019-10-15T18:30:58",
                "message_text_only": "On Tue, Oct 15, 2019 at 7:29 AM Braydon Fuller via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> So I don't think you can use the height in the coinbase for that\n> purpose, as it's not possible to validate it without the previous\n> headers. That's common for more than just the height.\n>\n\nIt is a property of blockchains that the lowest digest for a chain\nrepresents the total chainwork.\n\nEstimate total hash count = N * (2^256) / (Nth lowest (i.e. strongest)\ndigest over all headers)\n\nTo produce a fake set of 10 headers that give a higher work estimate than\nthe main chain would require around the same effort as went into the main\nchain in the first place.  You might as well completely build an\nalternative chain.\n\nWorking backwards for one of those headers, you have to follow the actual\nchain back to genesis.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191015/449fd686/attachment.html>"
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-15T00:38:55",
                "message_text_only": "On 10/12/19 9:27 AM, Tier Nolan via bitcoin-dev wrote:\n\n> [...]\n>\n> I think parallel downloading would be better than focusing on one peer\n> initially.  Otherwise, a dishonest peer can slowly send their headers to\n> prevent moving to parallel mode.\n>\n> [...]\n\nAs implemented, there is a timeout for that loader peer based on the\namount of time it should take to request all the headers. The time\nperiod is defined as a base time plus the number of expected headers\ntimes an expected amount of time per header. For example, the timeout\nwould be 25 minutes with a base time of 15 minutes, 1 millisecond per\nheader and an expected 600000 headers."
            },
            {
                "author": "Tier Nolan",
                "date": "2019-10-12T20:46:40",
                "message_text_only": "On Sat, Oct 12, 2019 at 6:56 PM Joachim Str\u00f6mbergson <\njoachimstr at protonmail.com> wrote:\n\n> I like the backwards syncing idea. First you provide proof of your best\n> block height via coinbase, then sync backwards. It solves lots of related\n> problems. You know how much you can expect from the given peer.\n>\n\nIt shows you which nodes are on the same chain too.\n\nIf you have 8 peers and you ask the 8 of them for their 8 best, then they\nshould all agree on most of them.\n\nYou can then ask each of the 8 to start sending you headers backwards from\none of the 8 seeds.\n\nThey will all roughly split the chain into 8 equal pieces, though the split\nwill be based on work rather than header height.\n\nIf there is disagreement, you can give priority to the node(s) with the\nlowest headers until they have completed their download.\n\nIt requires a network protocol change to allow reverse block downloads\nthough (and messages to indicate lowest headers etc.)\n\nOn different note, one of the problems that I haven't seen mentioned here\n> yet is the timewarp attack. It is relevant to some of the proposed\n> solutions. It should be possible, IIRC, for a malicious node to generate\n> much longer chain with superslow timestamp increase (~5 blocks in 1 second)\n> without increasing difficulty (i.e. staying at min. diff.). This could\n> produce chain that is ~2500 times longer than main chain without having\n> multiple branches.\n>\n\nThat is a good point.  It answers my question about formula for maximum\nnumber of blocks.\n\n5 * 60 * 60 * 24 * 365 = 157,680,000\n\nThat's around 150 million blocks per year at that rate.\n\nI assume the 5 per second limit is that it is greater that the median of\nthe last 11 rather than greater or equal?\n\nThe timewarp bug can be fixed by a basic soft fork.  You just need to limit\nthe maximum difference between the timestamp for the first header in a\nperiod and the last header in the previous period.\n\nAn alternative would be to soft fork in a maximum block rate.  In addition\nto the current rules, you could limit it to a maximum of 1 block every 2\nmins.  That rule shouldn't activate normally.\n\n   block.height <= (block.timestamp - genesis.timestamp) / (2 mins)\n\nIt could have some weird incentives if it actually activated though.\nMiners would have to shutdown mining if they were finding blocks to fast.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/557b7e19/attachment.html>"
            },
            {
                "author": "Braydon Fuller",
                "date": "2019-10-16T19:07:25",
                "message_text_only": "On 10/12/19 1:46 PM, Tier Nolan via bitcoin-dev wrote:\n\n> On Sat, Oct 12, 2019 at 6:56 PM Joachim Str\u00f6mbergson <joachimstr at protonmail.com> wrote:\n>> On different note, one of the problems that I haven't seen mentioned here\n>> yet is the timewarp attack. It is relevant to some of the proposed\n>> solutions. It should be possible, IIRC, for a malicious node to generate\n>> much longer chain with superslow timestamp increase (~5 blocks in 1 second)\n>> without increasing difficulty (i.e. staying at min. diff.). This could\n>> produce chain that is ~2500 times longer than main chain without having\n>> multiple branches.\n>>\n> [..]\n>\n> The timewarp bug can be fixed by a basic soft fork.  You just need to limit\n> the maximum difference between the timestamp for the first header in a\n> period and the last header in the previous period.\n\nYeah, that makes sense as it corrects the off-by-one error. I think this\nsolution has been included in a draft proposal \"The Great Consensus\nCleanup\". It would need to be effective for not only the main chain but\nalso for any future forked chain."
            }
        ],
        "thread_summary": {
            "title": "Chain width expansion",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Joachim Str\u00f6mbergson",
                "Braydon Fuller",
                "Tier Nolan",
                "David A. Harding"
            ],
            "messages_count": 17,
            "total_messages_chars_count": 38187
        }
    },
    {
        "title": "[bitcoin-dev] Smaller \"Bitcoin address\" accounts in the blockchain.",
        "thread_messages": [
            {
                "author": "Dave Scotese",
                "date": "2019-10-04T01:37:33",
                "message_text_only": "Currently, bitcoin must be redeemed by providing input to a script which\nresults in the required output.  This causes the attached amount of bitcoin\nto become available for use in the outputs of a transaction.  Is there any\nwork on creating a shorter \"transaction\" which, instead of creating a new\noutput, points to (creates a virtual copy of) an existing (unspent) output\nwith a larger amount attached to it?  This would invalidate the smaller,\nearlier UTXO and replace it with the new one without requiring the earlier\none to be redeemed, and also without requiring the original script to be\nduplicated.  It is a method for aggregating bitcoin to a UTXO which may\notherwise not be economically viable.\n\nThe idea is that there already exists a script that must be satisfied to\nspend X1, and if the owner of X1 would like to have the same requirements\nfor spending X2, this would be a transaction that does that using fewer\ndata bytes.  Since the script already exists, the transaction can simply\npoint to it instead of duplicating it.\n\nThis would also enable the capacity of lightning channels to be increased\non the fly without closing the existing channel and re-opening a new one.\nThe LN layer would have to cope with the possibility that the \"short\nchannel ID\" could change.\n\nDave.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191003/4b3fcfb3/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-10-04T06:45:35",
                "message_text_only": "Good morning David,\n\n> Currently, bitcoin must be redeemed by providing input to a script which results in the required output.\u00a0 This causes the attached amount of bitcoin to become available for use in the outputs of a transaction.\u00a0 Is there any work on creating a shorter \"transaction\" which, instead of creating a new output, points to (creates a virtual copy of) an existing (unspent) output with a larger amount attached to it?\u00a0 This would invalidate the smaller, earlier UTXO and replace it with the new one without requiring the earlier one to be redeemed, and also without requiring the original script to be duplicated.\u00a0 It is a method for aggregating bitcoin to a UTXO which may otherwise not be economically viable.\n>\n> The idea is that there already exists a script that must be satisfied to spend X1, and if the owner of X1 would like to have the same requirements for spending X2, this would be a transaction that does that using fewer data bytes.\u00a0 Since the script already exists, the transaction can simply point to it instead of duplicating it.\n>\n> This would also enable the capacity of lightning channels to be increased on the fly without closing the existing channel and re-opening a new one.\u00a0 The LN layer would have to cope with the possibility that the \"short channel ID\" could change.\n>\n> Dave.\n\nThis moves us closer to an \"account\"-style rather than \"UTXO\"-style.\nThe advantage of UTXO-style is that it becomes easy to validate a transaction as valid when putting it into the mempool, and as long as the UTXO it consumes remains valid, revalidation of the transaction when it is seen in a block is unnecessary.\n\nAdmittedly, the issue with account-style is when the account is overdrawn --- with UTXOs every spend drains the entire \"account\" and the \"account\" subsequently is definitely no longer spendable, whereas with accounts, every fullnode has to consider what would happen if two or more transactions spend from the account.\nIn your case, it seems to just *add* to the amount of a UTXO.\n\nIn any case, this might not be easy to implement in current Bitcoin.\nThe UTXO-style is deeply ingrained to Bitcoin design, and cannot be easily hacked in a softfork.\n\nSee also https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-July/017135.html and its thread for the difficulties involved with \"just copy some existing `scriptPubKey`\" and why such a thing will be very unlikely to come in Bitcoin.\n\n\nBut I think this can be done, in spirit, by pay-to-endpoint / payjoin.\n\nIn P2EP/Payjoin, the payer contacts the payee and offers to coinjoin simultaneously to the payment.\nThis does what you want:\n\n* Refers to a previous UTXO owned by the payee, and deletes it (by normal transaction spending rules).\n* Creates a new UTO, owned by the payee, which contains the total value of the below:\n  * The above old UTXO.\n  * The value to be transferred from payer to payee.\n\nThe only issues are that:\n\n* Payee has to be online and cooperate.\n* Payee has to provide signatures for the old UTXO, adding more blockchain data.\n* New UTXO has to publish a SCRIPT too.\n  * In terms of *privacy*, of course you *have* to use a new SCRIPT with a new public key anyway.\n    Thus this is superior to your proposal where the pubkey is reused, as P2EP/Payjoin preserves privacy.\n\n\nRegards,\nZmnSCPXj"
            }
        ],
        "thread_summary": {
            "title": "Smaller \"Bitcoin address\" accounts in the blockchain.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Dave Scotese",
                "ZmnSCPxj"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4775
        }
    },
    {
        "title": "[bitcoin-dev] ChainWallet - A way to prevent loss of funds by physical violence",
        "thread_messages": [
            {
                "author": "Saulo Fonseca",
                "date": "2019-10-04T09:15:40",
                "message_text_only": "Hi everyone\n\nIf you are a hodler, I like to propose the creation of a key stretching as a new layer of protection over your current wallet. I call it ChainWallet. Whatever is the method used to generate your private key, we can do the following:\n\nnewPrivKey = sha256(sha256(sha256(\u2026sha256(privKey)\u2026)))\nNewWallet = PubAddress(newPrivKey)\nIn this way we create a chain of hashes over your private key and generate a new wallet from it. If the chain is very long (billions or trillions of hashes) it will take a long time to be created. If you don\u2019t keep the newPrivKey, the only way to move coins in the NewWallet is to generate the chain again.\n\nThe length of the chain can be easy memorized as an exponent such as 2^40 or 10^12.\n\nWhat is that gut for? You will not be able to move your coins in an unplanned way such as being tortured by a kidnaper. You can create a wallet that takes days or even months to return the final address.\n\nComparison with a BrainWallet\n\nIf the first privKey is the hash of a password, your ChainWallet can be compared to a BrainWallet with a chain added to it. BrainWallets have a bad reputation because it is possible to create a brute-force attack against it. There are reports where the attacker was able to guess the password by generating hundreds of thousands of hashes per second. But, if you use a ChainWallet that takes one second to be generated, it means that the speed of an attack would be reduced to one guess per second. This makes a brute force attack practically impossible.\n\nEntropy\n\nThe ChainWallet adds only a few bits of entropy to your key. The idea here is not to increase the entropy, but to add \u201ctime\u201d as part of the puzzle.\n\nSHA-256\n\nI am suggesting the use of SHA-256 because it is the most popular hash algorithm in the crypto community. But you could use SHA-512 or a slower hash algorithm such as Bcrypt to do it. But keep in mind that other hash algorithms can reduce the entropy.\n\nThe idea is to add time to the key generation. If you use many SHA-256 or a few SHA-512, as long as both need the same time to be generated, there is no difference.\n\nOther hashes have the advantage that a hardware implementation of it is not widespread.\n\nASICs\n\nSomeone could mention that ASICs get more and more powerful and could crack a ChainWallet. But they have a huge hash rate because they calculate it in parallel. A ChainWallet requires that the output of a hash would be the input of the next calculation. This dramatically reduces the speed of a hardware implementation of such algorithms.\n\nLet\u2019s pick an example:  The Bitfury Clarke has 8.154 cores and runs 120 Gh/s. This means that each core can perform about 14.72 Mh/s. This speed is all that you can get with one of the best ASIC on the market. 17.72 Mh/s is only about 17,7 times faster than a typical computer. This speed can only increase slowly, as technology needs time to make the transistors run faster. So, the best way to generate a ChainWallet is by using such an ASIC core.\n\nMisuse\n\nSomeone could argue that people would misuse it by picking easy to remember passwords or small chain length. A wallet implementation could solve it by forcing a minimum length for the chain and block commonly used words for the password. It is a matter of design.\n\nTheft\n\nThe major advantage of a ChainWallet is the ability to avoid a theft. If your wallet takes a really long time to be generated and someone tries to force you to give your private key, you would not be able to do it, even if you really want. You could also give away a wrong password or chain length and he/she is not able to verify it. The chances are very small that he/she will wait weeks of months for the chain generation of even that he/she is able to do the chain calculation.\n\nFinal Thoughts\n\nA ChainWallet could be used as an alternative to BIP39. Instead of keeping 24 words, you would have a password and two numbers, a base and an exponent, that defines the length of the chain. This is easier to memorize, so you do not need to write it down.\n\nThis is only meant as an additional option along with all others available in the crypto environment, such as multisig and smart contracts. As for those other ideas, the ChainWallet is not applicable in every case.\n\nWhen the day arrives at which you want to stop hodling and transferring your coins to another location, you should re-generate your wallet in a planned way with the same original private key and length of the chain. Then, after waiting until the program concludes, you will get the new private key back.\n\nWeb Links\n\t\nThe original idea can be found on this post:\n\nhttps://www.reddit.com/user/sauloqf/comments/a3q8dt/chainwallet <https://www.reddit.com/user/sauloqf/comments/a3q8dt/chainwallet>\n\nA proof of concept in C++ can be found on this link:\n\nhttps://github.com/Saulo-Fonseca/ChainWallet <https://github.com/Saulo-Fonseca/ChainWallet>\n\nThe community is testing the concept for a while. You can find discussions on this links:\n\nhttps://www.reddit.com/r/Bitcoin/comments/cya467/chainwallet_challenge_get_01_btc_if_you_solve_it <https://www.reddit.com/r/Bitcoin/comments/cya467/chainwallet_challenge_get_01_btc_if_you_solve_it>\n\nhttps://www.reddit.com/r/Bitcoin/comments/d9ltec/does_someone_know_how_to_submit_a_bip_for_bitcoin <https://www.reddit.com/r/Bitcoin/comments/d9ltec/does_someone_know_how_to_submit_a_bip_for_bitcoin>\n\nSaulo Fonseca\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/b7a50f18/attachment-0001.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2019-10-04T10:02:59",
                "message_text_only": "Since the user can't prove that they are using this technique, or\npetertodd's timelock encryption for that matter, an attacker has little\nincentive to stop physically attacking until they have a spendable UTXO.\n\nI believe you can get the same effect with on-chain timelocks, or\ndelete-the-bits plus a rangeproof and a zero-knowledge proof that the\nrangeproof corresponds to some secret that can be used to derive the\nexpected public key. I think Jeremy Rubin had an idea for such a proof.\n\nAlso, adam3us has described a similar thought here:\nhttps://bitcointalk.org/index.php?topic=311000.0\n\n- Bryan\n\nOn Fri, Oct 4, 2019, 4:43 AM Saulo Fonseca via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi everyone\n>\n> If you are a hodler, I like to propose the creation of a key stretching as\n> a new layer of protection over your current wallet.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191004/1473ba33/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "ChainWallet - A way to prevent loss of funds by physical violence",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bryan Bishop",
                "Saulo Fonseca"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6623
        }
    },
    {
        "title": "[bitcoin-dev] BIPable-idea: Consistent and better definition of the term 'address'",
        "thread_messages": [
            {
                "author": "Emil Engler",
                "date": "2019-10-05T21:57:48",
                "message_text_only": "Hello dear mailing list subscribers.\nBefore I'll explain my idea here, I need to define a term first\n\n'address':\nWhen I use the terms address, pubkey, etc., I mean the same: The Base58\nstring\n\nOk now let's get into it:\nAs you should know, sending bitcoins to an address more than once is a\nvery bad approach.\nIn my opinion the problem why so many people are still doing this is\nbecause of the term 'address' which is used in lots of wallets,\nimplementations, BIP 21 and so on. It is a design issue.\nWith the term 'address' most people identify things that are fixed and\ndon't change really often (e.g postal address, IP address [depends on\nprovider], Domain, E-Mail address, ...).\nBecause of this most people compare bitcoin addresses with e-mail\naddresses and use this address to send the recipient money multiple times.\n\nMy suggestion would be to change the term address in wallets, the URI\nscheme and so on to something of the following options by a\nInformational/Process BIP:\n\n* Payment Password\n* Transaction Password\n* ...\n\nThe guideline for the term should indicate that it is:\n* temporary\n* Something that identifies the recipient\n\nI've chosen 'password' because they can be used as a pseudonym to\nidentify a person.\nThis is already used in stuff like bank transfers where something like\nthe transaction id should be used as the purpose or at universities\nthere are student numbers.\nThe first is probably a better example because student numbers aren't\ntemporary.\n\nWhat do you think? Should I write a BIP for this or use another term?\nFeedback is most welcome :)\n\nGreetings,\nEmil Engler\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: pEpkey.asc\nType: application/pgp-keys\nSize: 3147 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/6875962d/attachment-0001.bin>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2019-10-06T11:32:33",
                "message_text_only": "On Saturday 05 October 2019 21:57:48 Emil Engler via bitcoin-dev wrote:\n> Hello dear mailing list subscribers.\n> Before I'll explain my idea here, I need to define a term first\n>\n> 'address':\n> When I use the terms address, pubkey, etc., I mean the same: The Base58\n> string\n\nBut a pubkey is not a Base58 string, and fundamentally different from an \naddress. An address identifies the recipient and the purpose of the payment; \na pubkey does not. The pubkey remains with the UTXO; an address does not.\n\n> Ok now let's get into it:\n> As you should know, sending bitcoins to an address more than once is a\n> very bad approach.\n> In my opinion the problem why so many people are still doing this is\n> because of the term 'address' which is used in lots of wallets,\n> implementations, BIP 21 and so on. It is a design issue.\n> With the term 'address' most people identify things that are fixed and\n> don't change really often (e.g postal address, IP address [depends on\n> provider], Domain, E-Mail address, ...).\n> Because of this most people compare bitcoin addresses with e-mail\n> addresses and use this address to send the recipient money multiple times.\n\nThat problem would require using a different term than \"address\" to address.\nA BIP is unlikely to do the job (though it may help).\n\n> My suggestion would be to change the term address in wallets, the URI\n> scheme and so on to something of the following options by a\n> Informational/Process BIP:\n>\n> * Payment Password\n> * Transaction Password\n> * ...\n\nNeither the address nor pubkey are a password...\n\nSome possible alternative terms would be \"invoice id\", \"payment token\", etc.\n\n> The guideline for the term should indicate that it is:\n> * temporary\n> * Something that identifies the recipient\n>\n> I've chosen 'password' because they can be used as a pseudonym to\n> identify a person.\n> This is already used in stuff like bank transfers where something like\n> the transaction id should be used as the purpose or at universities\n> there are student numbers.\n> The first is probably a better example because student numbers aren't\n> temporary.\n>\n> What do you think? Should I write a BIP for this or use another term?\n> Feedback is most welcome :)\n>\n> Greetings,\n> Emil Engler"
            },
            {
                "author": "Emil Engler",
                "date": "2019-10-06T16:06:27",
                "message_text_only": "> But a pubkey is not a Base58 string, and fundamentally different from an \n> address. An address identifies the recipient and the purpose of the payment; \n> a pubkey does not. The pubkey remains with the UTXO; an address does not.\n\nI don't know much about this topic in Bitcoin so far, but I meant the\naddress :)\n\n> That problem would require using a different term than \"address\" to address.\n\nI meant this the whole time, sorry for misunderstandings.\n\n> A BIP is unlikely to do the job (though it may help).\n\nWhy? They are specifications and implementations need them (even if some\ndo not implement everything).\nAlso BIP 21 would need an alternative/separate parameter with the new term.\n\n> Neither the address nor pubkey are a password...\n\nIn Germany at least (where I live), you need to provide a 'Password'\n(the german translation of thin word of course) in the purpose of the\ntransaction if you pay with a bank transfer. They are used as IDs but I\nagree, ID is definitely a better suffix.\n\n> Some possible alternative terms would be \"invoice id\", \"payment token\", etc.\n\nACK\n\nGreetings, Emil Engler\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: pEpkey.asc\nType: application/pgp-keys\nSize: 3147 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191006/5c829744/attachment.bin>"
            },
            {
                "author": "Chris Belcher",
                "date": "2019-10-09T19:32:13",
                "message_text_only": "This is an excellent idea and I hope something like this happens.\n\nI've had the idea of using an intermediate name to make the transition\neasier, for example \"Bitcoin address\" becomes \"Bitcoin invoice address\"\nwhich after 10 years becomes \"Bitcoin invoice\" (or \"Bitcoin invoice\").\n\n\"Invoice\" would also work well because Lightning uses the name invoice\nfor the object which is used to receive payments. So it's easy to\nimagine that the GUI text input presented to users can be called\n\"invoice\" and users can put both kinds of invoices there leaving the\nwallet to easily figure out whether it makes an on-chain transaction or\na Lightning Network transaction.\n\nChanging a commonly-used name like this could be very hard, but the\ngains in terms of privacy are immense.\n\nOn 05/10/2019 22:57, Emil Engler via bitcoin-dev wrote:\n> Hello dear mailing list subscribers.\n> Before I'll explain my idea here, I need to define a term first\n> \n> 'address':\n> When I use the terms address, pubkey, etc., I mean the same: The Base58\n> string\n> \n> Ok now let's get into it:\n> As you should know, sending bitcoins to an address more than once is a\n> very bad approach.\n> In my opinion the problem why so many people are still doing this is\n> because of the term 'address' which is used in lots of wallets,\n> implementations, BIP 21 and so on. It is a design issue.\n> With the term 'address' most people identify things that are fixed and\n> don't change really often (e.g postal address, IP address [depends on\n> provider], Domain, E-Mail address, ...).\n> Because of this most people compare bitcoin addresses with e-mail\n> addresses and use this address to send the recipient money multiple times.\n> \n> My suggestion would be to change the term address in wallets, the URI\n> scheme and so on to something of the following options by a\n> Informational/Process BIP:\n> \n> * Payment Password\n> * Transaction Password\n> * ...\n> \n> The guideline for the term should indicate that it is:\n> * temporary\n> * Something that identifies the recipient\n> \n> I've chosen 'password' because they can be used as a pseudonym to\n> identify a person.\n> This is already used in stuff like bank transfers where something like\n> the transaction id should be used as the purpose or at universities\n> there are student numbers.\n> The first is probably a better example because student numbers aren't\n> temporary.\n> \n> What do you think? Should I write a BIP for this or use another term?\n> Feedback is most welcome :)\n> \n> Greetings,\n> Emil Engler\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Emil Engler",
                "date": "2019-10-10T15:05:36",
                "message_text_only": "* Sorry if this mail was sent multiple times, my E-Mail client went crazy *\n\nThanks for all your feedback.\nI came to the decision to write a BIP for this, even if it might not be\nimplemented by many wallets, a standardization is never wrong and this\nwould be the first step in the correct direction for better on-chain\nprivacy.\n\nHowever currently we still need a good term for the 'address' replacement.\n\nThe current suggestions are:\n* Invoice ID\n* Payment Token\n* Bitcoin invoice (address)\n* Bitcoin invoice (path)\n\nBecause of the LN term invoice I really like the term 'Bitcoin Invoice'\nby Chris Belcher.\n\nSo how do find a consensus about these terms?\n\nGreetings\nEmil Engler\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: pEpkey.asc\nType: application/pgp-keys\nSize: 3147 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191010/5b380c47/attachment.bin>"
            },
            {
                "author": "Lloyd Fournier",
                "date": "2019-10-11T01:13:40",
                "message_text_only": "Hi Thread,\n\nThis may not be the most practical information, but there actually did\nexist an almost perfect analogy for Bitcoin addresses from the ancient\nworld: From wikipedia https://en.wikipedia.org/wiki/Bulla_(seal)\n\n\"Transactions for trading needed to be accounted for efficiently, so the\nclay tokens were placed in a clay ball (bulla), which helped with\ndishonesty and kept all the tokens together. In order to account for the\ntokens, the bulla would have to be crushed to reveal their content. This\nintroduced the idea of impressing the token onto the wet bulla before it\ndried, to insure trust that the tokens hadn't been tampered with and for\nanyone to know what exactly was in the bulla without having to break it.\"\n\nYou could only use the bulla once because it had to be destroyed in order\nto get the tokens out! I think there are even examples of bulla with a kind\nof \"signature\" on them (an imprint with the seal of a noble family etc).\n\n\"send me a Bitcoin bulla\" has a nice ring to it!\n\nSincerely,\n\nLL\n\n\n\n\n\nOn Fri, Oct 11, 2019 at 2:44 AM Emil Engler via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> * Sorry if this mail was sent multiple times, my E-Mail client went crazy *\n>\n> Thanks for all your feedback.\n> I came to the decision to write a BIP for this, even if it might not be\n> implemented by many wallets, a standardization is never wrong and this\n> would be the first step in the correct direction for better on-chain\n> privacy.\n>\n> However currently we still need a good term for the 'address' replacement.\n>\n> The current suggestions are:\n> * Invoice ID\n> * Payment Token\n> * Bitcoin invoice (address)\n> * Bitcoin invoice (path)\n>\n> Because of the LN term invoice I really like the term 'Bitcoin Invoice'\n> by Chris Belcher.\n>\n> So how do find a consensus about these terms?\n>\n> Greetings\n> Emil Engler\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191011/e2d43243/attachment-0001.html>"
            },
            {
                "author": "Karl-Johan Alm",
                "date": "2019-10-11T02:00:51",
                "message_text_only": "I've proposed bitcoin invoice for awhile now. See\nhttps://twitter.com/kallewoof/status/1165841566105079808\n\nI like bitcoin invoice address into bitcoin address as proposed by Chris.\n\n\nOn Fri, Oct 11, 2019 at 12:45 AM Emil Engler via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> * Sorry if this mail was sent multiple times, my E-Mail client went crazy *\n>\n> Thanks for all your feedback.\n> I came to the decision to write a BIP for this, even if it might not be\n> implemented by many wallets, a standardization is never wrong and this\n> would be the first step in the correct direction for better on-chain\n> privacy.\n>\n> However currently we still need a good term for the 'address' replacement.\n>\n> The current suggestions are:\n> * Invoice ID\n> * Payment Token\n> * Bitcoin invoice (address)\n> * Bitcoin invoice (path)\n>\n> Because of the LN term invoice I really like the term 'Bitcoin Invoice'\n> by Chris Belcher.\n>\n> So how do find a consensus about these terms?\n>\n> Greetings\n> Emil Engler\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Ariel Lorenzo-Luaces",
                "date": "2019-10-11T04:22:03",
                "message_text_only": "I would propose a term different than all the others mentioned so far.\n\nI propose Funding Codes.\n\nThe word funding can be used as a verb or a noun and this works for the nature of Bitcoin transactions. Funding can be seen as what someone needs to do with the code as well as referring to it as the code that has funding once bitcoins have been transfered to it \"give me a funding code\".\n\nThe word code is fitting since codes are what addresses ultimately describe, the signature script, a piece of code. When speaking about the lightning transaction graph it's easy to talk about transactions making bitcoins flow from code to code, each code different for a different purpose \"funding is sent from this code to that code\" and \"funding is kept in this code for 144 blocks\".\n\n- Payment tokens feel like they themselves hold the value and can be transfered but anyone can generate as many payment tokens as they desire. This name conflicts with other cryptocurrencies that focus their blockchain on payments and refer to their currency as tokens.\n\n- Invoices are problematic because they imply that they hold bitcoins and they specify an amount. However addresses don't specify any amounts while they themselves can be included inside a real invoice. I think it is wrong to imply that the \"thing\" we are sending money to is temporary, because it isn't. Lightning channels can stay open forever until closed but money doesn't stay in an invoice for any amount of time.\n\n- I actually prefer Addresses over both Payment Tokens and Invoices. It's actually very natural to send something to an address and an address can hold something for a long time. However addresses fall short due to people only having one. This makes them think that they have to memorize a bitcoin address. There are also all the other reasons people have mentioned.\n\nThe word code fits well in the divide between technical and non-technical people. To the layman a code is just a string of characters and that is what we can visually see and check and compare when one of these funding codes are transfered between two parties \"does your finding code end with xyz?\". To programmers code is something that can be executed which is exactly what addresses are. Especially ones with complicated logic and lots of conditions \"this funding code can only be unlocked by Alice or Bob plus Charlie or Dave after 1000 blocks\".\n\nFunding codes work even better when thinking about self executing smart contracts \"they are funded and running code with their funds\". Contracts don't make sense at all when it's an autonomous thing that didn't strike any contract with anyone. Contracts should only be referred to the transactions people send to funding codes or \"smart\" codes.\n\nAddresses also fail when transferring between two of your own wallets because it doesn't make sense for someone to have so many addresses but it does make sense for someone to have many codes.\n\nLightning already has \"funding addresses\" and \"funding transactions\". With schnorr and taproot coming it will be possible to create more complex situations and they all need funding codes so that funds can be sent to it and be locked up in the code (sigscript).\n\nOne criticism might be that funding codes sound like they are funding something which doesn't appear to be true. But indeed they are! Funding codes fund a situation, a setup. The common setup is that you can unlock them at any time. Other setups demand multi-party coordination. The funding code is what funds all these setups.\n\nFunding codes are also two words and three syllables which is great because using only one word is not descriptive enough and using four or more syllables is way too long. Having the second word \"code\" makes for easy abbreviation when the conversation is already about Bitcoin \"which code did you send them to?\"\n\nPeople will naturally use funding code and bitcoin code interchangeably. This is a good thing because bitcoin is a type of fund, so there is no contradiction. The more common term should still be funding code because there is more than one type of \"code\" in Bitcoin \n\nMost importantly funding codes sound good when spoken. This goes for both singular and plural.\n\n\"First a receiver must generate a funding code to have a sender fund it with their\u00a0 from their own funding code which had been previously funded\"\n\nCheers\nAriel Lorenzo-Luaces\n\n\n\nOn Oct 10, 2019, 7:20 PM, at 7:20 PM, Karl-Johan Alm via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>I've proposed bitcoin invoice for awhile now. See\n>https://twitter.com/kallewoof/status/1165841566105079808\n>\n>I like bitcoin invoice address into bitcoin address as proposed by\n>Chris.\n>\n>\n>On Fri, Oct 11, 2019 at 12:45 AM Emil Engler via bitcoin-dev\n><bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> * Sorry if this mail was sent multiple times, my E-Mail client went\n>crazy *\n>>\n>> Thanks for all your feedback.\n>> I came to the decision to write a BIP for this, even if it might not\n>be\n>> implemented by many wallets, a standardization is never wrong and\n>this\n>> would be the first step in the correct direction for better on-chain\n>> privacy.\n>>\n>> However currently we still need a good term for the 'address'\n>replacement.\n>>\n>> The current suggestions are:\n>> * Invoice ID\n>> * Payment Token\n>> * Bitcoin invoice (address)\n>> * Bitcoin invoice (path)\n>>\n>> Because of the LN term invoice I really like the term 'Bitcoin\n>Invoice'\n>> by Chris Belcher.\n>>\n>> So how do find a consensus about these terms?\n>>\n>> Greetings\n>> Emil Engler\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191010/25216e06/attachment.html>"
            },
            {
                "author": "Emil Engler",
                "date": "2019-10-11T21:03:45",
                "message_text_only": "> This may not be the most practical information, but there actually did exist an almost perfect analogy for Bitcoin addresses from the ancient world: From wikipedia https://en.wikipedia.org/wiki/Bulla_(seal)\n\nI personally do not like the term bulla, it might be a perfect analogy\nbut I personally don't believe that this term is well known. Tbh I\ndidn't knew what a 'bulla' was before.\n\n> I propose Funding Codes.\n\nI'm neutral on this. With code I associate something that is slightly\nmore permanent like a code to unlock your mobile phone or a code to\nunlock your bike if you know what I mean. These kind of codes change\nsometimes but not as often as a bitcoin address (should).\nI also agree that Payment Tokens might confuse with other currencies and\nblock chain.\n\n> \n> - Invoices are problematic because they imply that they hold bitcoins and they specify an amount. However addresses don't specify any amounts while they themselves can be included inside a real invoice. I think it is wrong to imply that the \"thing\" we are sending money to is temporary, because it isn't. Lightning channels can stay open forever until closed but money doesn't stay in an invoice for any amount of time.\n\nWhat is with 'Bitcoin Invoice Address'?\nThis is the best of both worlds because it implies the temporary factor\nwith 'invoice' and the way that you send something to something.\n\nAlso, this is more a personal opinion but I thunk that 'funding' implies\nmore to donate to something. I think this could lead to misunderstandings.\n\nTo summarize it up, here are the following suggested terms:\n* Invoice ID\n* Payment Token\n* Bitcoin invoice (address)\n* Bitcoin invoice (path)\n* Bulla\n* Funding code\n\nGreetings,\nEmil Engler\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: pEpkey.asc\nType: application/pgp-keys\nSize: 3147 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191011/5cac5674/attachment-0001.bin>"
            },
            {
                "author": "Marco Falke",
                "date": "2019-10-17T13:23:39",
                "message_text_only": "I also like the \"bitcoin invoice address\" term by Chris. Invoice is a\ncommon term and easily translatable into other languages.\n\nMarco"
            },
            {
                "author": "Emil Engler",
                "date": "2019-10-17T19:28:44",
                "message_text_only": "As the idea got positive feedback, I've written the BIP.\nThe draft is available here:\nhttps://github.com/bitcoin/bips/pull/856\n\nAm 17.10.19 um 15:23 schrieb Marco Falke via bitcoin-dev:\n> I also like the \"bitcoin invoice address\" term by Chris. Invoice is a\n> common term and easily translatable into other languages.\n> \n> Marco\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: pEpkey.asc\nType: application/pgp-keys\nSize: 3147 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191017/0b2934c5/attachment-0001.bin>"
            }
        ],
        "thread_summary": {
            "title": "BIPable-idea: Consistent and better definition of the term 'address'",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Belcher",
                "Emil Engler",
                "Ariel Lorenzo-Luaces",
                "Lloyd Fournier",
                "Luke Dashjr",
                "Karl-Johan Alm",
                "Marco Falke"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 21523
        }
    },
    {
        "title": "[bitcoin-dev] Human readable format for private keys",
        "thread_messages": [
            {
                "author": "JW Weatherman",
                "date": "2019-10-05T22:51:20",
                "message_text_only": "Hey Guys,\n\nI\u2019d like to propose a feature to bitcoin to solve the following problems:\n\n- When people read or write private keys it is very easy to mistake a letter or number.\n- When entering a private key a mistake isn\u2019t identified until the entire key is entered.\n- When an error is made in providing a private key the location of the error isn\u2019t indicated within the private key.\n- Private keys stored on paper can be lost if a single character is damaged or poorly transcribed.\n\nThe solution I\u2019m proposing has two parts.\n\nFirst provide an option to use to the NATO phonetic alphabet when displaying or entertaining private keys. To indicate lower case the word should not be capitalized. Capital letters and numbers should be capitalized.\n\nThe nato phonetic alphabet is a long-standing international standard (as international as the use of letters and numbers already used in base58) and has been designed to make each letter easily distinguishable when spoken and written.\n\nBy using whole words, that are easily distinguishable and from a very short word database (58 well known words that are either the English numbers or words that begin with the letter indicated) the likelihood of errors in recovery are reduced.\n\nThe second part of the solution is to insert checksum letters. If every 5th word is actually a checksum for the previous 4 words, you end up with 13 sentences such as:\n\nALFA tango THREE SIX bravo\n\nIn this case bravo is actually a checksum for the previous 4 words and can be calculated and verified as the private key is entered. If the user accidentally trumped BRAVO instead of bravo the checksum would immediately indicate an error within these 5 words (in most cases) making for a greatly improved user experience.\n\nAn additional side effect of this is that even if an entire word is lost on multiple lines, the  checksum would probably make guessing the correct words relatively easy.\n\nI realize some of these issues have been discussed in relation to bip39, but I hope this is more likely to be adopted by bitcoin core as it uses existing private keys, has no impact on keygen, does not require a standardized and well known word list for every language, and is essential just a display format that hopefully wouldn\u2019t require invasive code changes.\n\nThanks in advance for your feedback.\n\n-JW\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191005/ccdde54b/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Human readable format for private keys",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "JW Weatherman"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2508
        }
    },
    {
        "title": "[bitcoin-dev] Taproot updates",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2019-10-09T21:34:32",
                "message_text_only": "Hi all,\n\nI wanted to give an update on some of the changes we've made to the\nbip-schnorr/taproot/tapscript drafts following discussions on this\nlist:\n* The original post:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-May/016914.html\nand follow-ups\n* Using 2 or 4 byte indexes:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-June/017046.html\n* 32-byte public keys:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017247.html\n* Resource limits:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017306.html\n* P2SH support or not:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-September/017297.html).\n\nWe've made the following semantical changes to the proposal:\n* 32-byte public keys everywhere instead of 33-byte ones: dropping one\nbyte that provably does not contribute to security, while remaining\ncompatible with existing BIP32 and other key generation algorithms.\n* No more P2SH support: more efficient chain usage, no gratuitous\nfungibility loss from having 2 versions, no mode limited to 80-bit\nsecurity for non-interactive multiuser constructs; however senders\nwill need bech32 support to send to Taproot outputs.\n* 32-bit txin position and codesep position indexes instead of 16-bits ones.\n* Tagged hashes also in bip-schnorr: the signature and nonce\ngeneration now also use tagged hashes, rather than direct SHA256\n(previously tagged hashes were only used in bip-taproot and\nbip-tapscript)\n* Dropping the 10000 byte script limit and 201 non-push opcode limit:\nas no operations remain whose validation performance depends on the\nsize of scripts or number of executed opcodes, these limits serve no\npurpose, but complicate creation of Scripts.\n* Increased the limit on the depth of Merkle trees from 32 to 128: a\nlimit of 32 would necessitate suboptimal trees in some cases, but more\nthan 128 levels are only necessary when dealing with leaves that have\na chance of ~1/2^128 of being executed, which our security level\ntreats as impossible anyway.\n\nSee the updated documents:\n* https://github.com/sipa/bips/blob/bip-schnorr/bip-schnorr.mediawiki\n* https://github.com/sipa/bips/blob/bip-schnorr/bip-taproot.mediawiki\n* https://github.com/sipa/bips/blob/bip-schnorr/bip-tapscript.mediawiki\n\nIn addition, a lot of clarifications and rationales were added. The\nreference implementation on\nhttps://github.com/sipa/bitcoin/commits/taproot was also updated to\nreflect these changes, has a cleaner commit history now, and improved\ntests (though those can still use a lot of work).\n\nCheers,\n\n-- \nPieter"
            }
        ],
        "thread_summary": {
            "title": "Taproot updates",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pieter Wuille"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2584
        }
    },
    {
        "title": "[bitcoin-dev] Block Batch Filters for Light Clients",
        "thread_messages": [
            {
                "author": "Jonas Schnelli",
                "date": "2019-10-11T15:44:54",
                "message_text_only": "Hi Aleksey\n\n> BIP 157 unlike BIP 37 not allow apply filters to mempool and check zero confirmation transactions.\n> Light client that refused to use BIP 37 due to privacy leaks can process unconfirmed transactions only one way and this is loading the entire mempool transaction flow.\n> \n> https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki <https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki>\n\nInstead of using a per tx filter, would it be possible to allow retrieving a complete compact filter of the whole mempool (similar to BIP35)? Maybe using a maximum size of the filter (ordered by feerate).\nIn general, I guess the concerns are DOS and fingerprinting.\n\nMaybe DOS could be mitigated via a dynamic filter construction (append elements during the time between blocks, though unsure if possible).\nThe update-interval of a such filter could also be timebases rather than on every new tx in the mempool.\n\nUnsure about fingerprinting defence measures.\n\nI would expect the following process:\n* peer generates mempool filter\n* [timespan A (say 3 seconds)]\n* light client connects to peer\n* light client requests mempool-filter\n* peers sends mempool filter\n* light client processes filter for relevant txns\n* eventually, light client sends getdata of relevant txns\n\na) after the initial retrieve...\n* light client inspects all new txns (inv/getdata) received from peers from this point on (filterless unconfirmed tx detection)\n\nOf if a) is to bandwidth expansive, request the mempool filter again after a timeout\n\n/jonas\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191011/f88698ba/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Block Batch Filters for Light Clients",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jonas Schnelli"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1779
        }
    },
    {
        "title": "[bitcoin-dev] Block Batch Filters for Light Clients (Jonas Schnelli)",
        "thread_messages": [
            {
                "author": "admin at bitaps.com",
                "date": "2019-10-12T13:46:43",
                "message_text_only": "Hi Jonas,\n\nLet's review case when we create filters for part of mempool or whole mempool. \n\nAfter new block is mined we have to verify what transactions is confirmed from mempool. Mempool filter in design with set of transactions make impossible use it do block filter reconstruction because we have no mapping filter elements with transactions.\nThis means that we need to download block filter to check what exactly transactions are affected.\n\nWhich does not give us advantages in terms of traffic savings.\n\nThe idea of draft for mempool transactions filters is make possible to reconstruct correct block filters using unconfirmed tx filters and archive additional savings.\n\nAt this moment I  am working on Block Batch Filters implementation and have same changes which will be updated in drafts as soon as I complete all tests. Regarding unconfirmed TX filters, I came to the conclusion that the filter\nshould be the first 6 bytes of the address hash, this takes up a bit more space, but allows to construct block filter.\n\n\n\nWhat do you mean by  fingerprinting defence ?\n\nPlease let me know what, in your opinion, are the disadvantages of the per tx filter approach?\n\nWhy use whole/part mempool is better?\n\n\nWith regards Aleksey\n\n \n\n \n\n> 12 \u043e\u043a\u0442. 2019 \u0433., \u0432 5:40, bitcoin-dev-request at lists.linuxfoundation.org \u043d\u0430\u043f\u0438\u0441\u0430\u043b(\u0430):\n> \n> Re: Block Batch Filters for Light Clients (Jonas Schnelli)\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191012/2d5f3da2/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Block Batch Filters for Light Clients (Jonas Schnelli)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "admin at bitaps.com"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1578
        }
    },
    {
        "title": "[bitcoin-dev] Is Signet Bitcoin?",
        "thread_messages": [
            {
                "author": "Karl-Johan Alm",
                "date": "2019-10-15T02:54:28",
                "message_text_only": "Hello,\n\nThe pull request to the bips repository for Signet has stalled, as the\nmaintainer isn't sure Signet should have a BIP at all, i.e. \"is Signet\nBitcoin?\".\n\nMy argument is that Signet is indeed Bitcoin and should have a BIP, as\nthis facilitates the interoperability between different software in\nthe Bitcoin space.\n\nFeedback welcome, here or on the pull request itself:\nhttps://github.com/bitcoin/bips/pull/803"
            },
            {
                "author": "Jonathan Underwood",
                "date": "2019-10-15T04:38:29",
                "message_text_only": "I would also like to agree that Signet should be a BIP.\n\nProblem: Testnet is unreliable. *Testnet is used often for development of\nBitcoin*.\nProposal: To improve the dev environment for Bitcoin, let's create a new\nkind of testnet that is more reliable.\n\nI would also like to hear the logic behind \"Testnet is Bitcoin\" but \"Signet\nis not Bitcoin\"... both are not 100% compatible with mainnet consensus\nprocesses.\n\n-Jon\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191015/c00dcd1e/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2019-10-15T05:33:53",
                "message_text_only": "Indeed, Signet is no less (or more) Bitcoin than a seed format or BIP 32. It\u2019s \u201cnot Bitcoin\u201d but it\u2019s certainly \u201cinteroperability for how to build good testing for Bitcoin\u201d.\n\n> On Oct 14, 2019, at 19:55, Karl-Johan Alm via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeffHello,\n> \n> The pull request to the bips repository for Signet has stalled, as the\n> maintainer isn't sure Signet should have a BIP at all, i.e. \"is Signet\n> Bitcoin?\".\n> \n> My argument is that Signet is indeed Bitcoin and should have a BIP, as\n> this facilitates the interoperability between different software in\n> the Bitcoin space.\n> \n> Feedback welcome, here or on the pull request itself:\n> https://github.com/bitcoin/bips/pull/803\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Is Signet Bitcoin?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Matt Corallo",
                "Jonathan Underwood",
                "Karl-Johan Alm"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 1933
        }
    },
    {
        "title": "[bitcoin-dev] Removal of reject network messages from Bitcoin Core (BIP61)",
        "thread_messages": [
            {
                "author": "John Newbery",
                "date": "2019-10-16T16:43:53",
                "message_text_only": "Following discussion on this mailing list, support for BIP 61 REJECT\nmessages was not removed from Bitcoin Core in V0.19. The behaviour in that\nupcoming release is that REJECT messages are disabled by default and can be\nenabled using the `-enablebip61` command line option.\n\nSupport for REJECT messages will be removed entirely in Bitcoin Core V0.20,\nexpected for release in mid 2020. The PR to remove support was merged into\nBitcoin Core's master branch this week.\n\nAdoption of new Bitcoin Core versions across reachable nodes generally\ntakes several months. https://bitnodes.earn.com/dashboard/?days=365 shows\nthat although v0.18 was released in May 2019, there are still several\nhundred reachable nodes on V0.17, V0.16, V0.15 and earlier software.\nSoftware that currently use REJECT messages from public nodes for\ntroubleshooting issues therefore have plenty of time to transition to one\nof the methods listed by Marco in the email above.\n\nJohn\n\nOn Tue, Mar 5, 2019 at 10:28 PM Marco Falke via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Bitcoin Core may send \"reject\" messages as response to \"tx\", \"block\" or\n> \"version\" messages from a network peer when the message could not be\n> accepted.\n>\n> This feature is toggled by the `-enablebip61` command line option and has\n> been\n> disabled by default since Bitcoin Core version 0.18.0 (not yet released as\n> of\n> time of writing). Nodes on the network can not generally be trusted to send\n> valid (\"reject\") messages, so this should only ever be used when connected\n> to a\n> trusted node. At this time, I am not aware of any software that requires\n> this\n> feature, and I would like to remove if from Bitcoin Core to make the\n> codebase\n> slimmer, easier to understand and maintain. Let us know if your application\n> relies on this feature and you can not use any of the recommended\n> alternatives:\n>\n> * Testing or debugging of implementations of the Bitcoin P2P network\n> protocol\n>   should be done by inspecting the log messages that are produced by a\n> recent\n>   version of Bitcoin Core. Bitcoin Core logs debug messages\n>   (`-debug=<category>`) to a stream (`-printtoconsole`) or to a file\n>   (`-debuglogfile=<debug.log>`).\n>\n> * Testing the validity of a block can be achieved by specific RPCs:\n>   - `submitblock`\n>   - `getblocktemplate` with `'mode'` set to `'proposal'` for blocks with\n>     potentially invalid POW\n>\n> * Testing the validity of a transaction can be achieved by specific RPCs:\n>   - `sendrawtransaction`\n>   - `testmempoolaccept`\n>\n> * Wallets should not use the absence of \"reject\" messages to indicate a\n>   transaction has propagated the network, nor should wallets use \"reject\"\n>   messages to set transaction fees. Wallets should rather use fee\n> estimation\n>   to determine transaction fees and set replace-by-fee if desired. Thus,\n> they\n>   could wait until the transaction has confirmed (taking into account the\n> fee\n>   target they set (compare the RPC `estimatesmartfee`)) or listen for the\n>   transaction announcement by other network peers to check for propagation.\n>\n> I propose to remove \"reject\" messages from Bitcoin Core 0.19.0 unless\n> there are\n> valid concerns about its removal.\n>\n> Marco\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191016/4664e5b3/attachment.html>"
            },
            {
                "author": "Andreas Schildbach",
                "date": "2019-10-17T19:38:53",
                "message_text_only": "On 16/10/2019 18.43, John Newbery via bitcoin-dev wrote:\n\n> Following discussion on this mailing list, support for BIP 61 REJECT\n> messages was not removed from Bitcoin Core in V0.19. The behaviour in\n> that upcoming release is that REJECT messages are disabled by default\n> and can be enabled using the `-enablebip61` command line option.\n\nIs there a NODE_* bit we can use to pick peers that support this\n(useful!) feature?"
            },
            {
                "author": "Eric Voskuil",
                "date": "2019-10-17T20:16:47",
                "message_text_only": "As this is a P2P protocol change it should be exposed as a version increment (and a BIP), not just as a conditional service. If the intent is to retain this protocol indefinitely, exposing it conditionally, then a service bit would make sense, but it remains a protocol change.\n\nBIP61 is explicit:\n\n\u201cAll implementations of the P2P protocol version 70,002 and later should support the reject message.\u201c\n\ne\n\n> On Oct 17, 2019, at 12:54, Andreas Schildbach via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeffOn 16/10/2019 18.43, John Newbery via bitcoin-dev wrote:\n> \n>> Following discussion on this mailing list, support for BIP 61 REJECT\n>> messages was not removed from Bitcoin Core in V0.19. The behaviour in\n>> that upcoming release is that REJECT messages are disabled by default\n>> and can be enabled using the `-enablebip61` command line option.\n> \n> Is there a NODE_* bit we can use to pick peers that support this\n> (useful!) feature?\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "David A. Harding",
                "date": "2019-10-18T22:45:35",
                "message_text_only": "On Thu, Oct 17, 2019 at 01:16:47PM -0700, Eric Voskuil via bitcoin-dev wrote:\n> As this is a P2P protocol change it should be exposed as a version\n> increment (and a BIP) [...]\n>\n> BIP61 is explicit:\n> \n> \u201cAll implementations of the P2P protocol version 70,002 and later\n> should support the reject message.\u201c\n\nI don't think a new BIP or a version number increment is necessary.\n\n1. \"Should support\" isn't the same as \"must support\".  See\n   https://tools.ietf.org/html/rfc2119 ; by that reading,\n   implementations with protocol versions above 70,002 are not required\n   to support the reject message.\n\n2. If you don't implement a BIP, as Bitcoin Core explicitly doesn't any\n   more for BIP61[1], you're not bound by its conditions.\n\n-Dave\n\n[1] https://github.com/bitcoin/bitcoin/blob/master/doc/bips.md  \"BIP61\n[...] Support was removed in v0.20.0\""
            },
            {
                "author": "Eric Voskuil",
                "date": "2019-10-20T05:13:12",
                "message_text_only": "I agree, thanks.\n\nFWIW I\u2019ve never been a fan of the \u2018reject\u2019 message, or its implementation.\n\nhttps://github.com/bitcoin/bips/wiki/Comments:BIP-0061\n\ne\n\n> On Oct 18, 2019, at 18:46, David A. Harding <dave at dtrt.org> wrote:\n> \n> \ufeffOn Thu, Oct 17, 2019 at 01:16:47PM -0700, Eric Voskuil via bitcoin-dev wrote:\n>> As this is a P2P protocol change it should be exposed as a version\n>> increment (and a BIP) [...]\n>> \n>> BIP61 is explicit:\n>> \n>> \u201cAll implementations of the P2P protocol version 70,002 and later\n>> should support the reject message.\u201c\n> \n> I don't think a new BIP or a version number increment is necessary.\n> \n> 1. \"Should support\" isn't the same as \"must support\".  See\n>   https://tools.ietf.org/html/rfc2119 ; by that reading,\n>   implementations with protocol versions above 70,002 are not required\n>   to support the reject message.\n> \n> 2. If you don't implement a BIP, as Bitcoin Core explicitly doesn't any\n>   more for BIP61[1], you're not bound by its conditions.\n> \n> -Dave\n> \n> [1] https://github.com/bitcoin/bitcoin/blob/master/doc/bips.md  \"BIP61\n> [...] Support was removed in v0.20.0\"\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/d2498549/attachment-0001.html>"
            },
            {
                "author": "John Newbery",
                "date": "2019-10-18T20:53:23",
                "message_text_only": "> Is there a NODE_* bit we can use to pick peers that support this (useful!)\nfeature?\n\nNo. BIP 61 has no mechanism for advertising that a node will send REJECT\nmessages.\n\nOn Wed, Oct 16, 2019 at 12:43 PM John Newbery <john at johnnewbery.com> wrote:\n\n> Following discussion on this mailing list, support for BIP 61 REJECT\n> messages was not removed from Bitcoin Core in V0.19. The behaviour in that\n> upcoming release is that REJECT messages are disabled by default and can be\n> enabled using the `-enablebip61` command line option.\n>\n> Support for REJECT messages will be removed entirely in Bitcoin Core\n> V0.20, expected for release in mid 2020. The PR to remove support was\n> merged into Bitcoin Core's master branch this week.\n>\n> Adoption of new Bitcoin Core versions across reachable nodes generally\n> takes several months. https://bitnodes.earn.com/dashboard/?days=365 shows\n> that although v0.18 was released in May 2019, there are still several\n> hundred reachable nodes on V0.17, V0.16, V0.15 and earlier software.\n> Software that currently use REJECT messages from public nodes for\n> troubleshooting issues therefore have plenty of time to transition to one\n> of the methods listed by Marco in the email above.\n>\n> John\n>\n> On Tue, Mar 5, 2019 at 10:28 PM Marco Falke via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Bitcoin Core may send \"reject\" messages as response to \"tx\", \"block\" or\n>> \"version\" messages from a network peer when the message could not be\n>> accepted.\n>>\n>> This feature is toggled by the `-enablebip61` command line option and has\n>> been\n>> disabled by default since Bitcoin Core version 0.18.0 (not yet released\n>> as of\n>> time of writing). Nodes on the network can not generally be trusted to\n>> send\n>> valid (\"reject\") messages, so this should only ever be used when\n>> connected to a\n>> trusted node. At this time, I am not aware of any software that requires\n>> this\n>> feature, and I would like to remove if from Bitcoin Core to make the\n>> codebase\n>> slimmer, easier to understand and maintain. Let us know if your\n>> application\n>> relies on this feature and you can not use any of the recommended\n>> alternatives:\n>>\n>> * Testing or debugging of implementations of the Bitcoin P2P network\n>> protocol\n>>   should be done by inspecting the log messages that are produced by a\n>> recent\n>>   version of Bitcoin Core. Bitcoin Core logs debug messages\n>>   (`-debug=<category>`) to a stream (`-printtoconsole`) or to a file\n>>   (`-debuglogfile=<debug.log>`).\n>>\n>> * Testing the validity of a block can be achieved by specific RPCs:\n>>   - `submitblock`\n>>   - `getblocktemplate` with `'mode'` set to `'proposal'` for blocks with\n>>     potentially invalid POW\n>>\n>> * Testing the validity of a transaction can be achieved by specific RPCs:\n>>   - `sendrawtransaction`\n>>   - `testmempoolaccept`\n>>\n>> * Wallets should not use the absence of \"reject\" messages to indicate a\n>>   transaction has propagated the network, nor should wallets use \"reject\"\n>>   messages to set transaction fees. Wallets should rather use fee\n>> estimation\n>>   to determine transaction fees and set replace-by-fee if desired. Thus,\n>> they\n>>   could wait until the transaction has confirmed (taking into account the\n>> fee\n>>   target they set (compare the RPC `estimatesmartfee`)) or listen for the\n>>   transaction announcement by other network peers to check for\n>> propagation.\n>>\n>> I propose to remove \"reject\" messages from Bitcoin Core 0.19.0 unless\n>> there are\n>> valid concerns about its removal.\n>>\n>> Marco\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191018/947d583d/attachment-0001.html>"
            },
            {
                "author": "Andreas Schildbach",
                "date": "2019-10-21T08:44:16",
                "message_text_only": "I guess then the best way to discover nodes that have reject messages\nenabled is connecting/disconnecting to random nodes and send them\ninvalid transactions and keep the ones which reply with a reject message.\n\n\nOn 18/10/2019 22.53, John Newbery via bitcoin-dev wrote:\n>>\u00a0Is there a NODE_* bit we can use to pick peers that support this\n> (useful!) feature?\n> \n> No. BIP 61 has no mechanism for advertising that a node will send REJECT\n> messages.\n> \n> On Wed, Oct 16, 2019 at 12:43 PM John Newbery <john at johnnewbery.com\n> <mailto:john at johnnewbery.com>> wrote:\n> \n>     Following discussion on this mailing list, support for BIP 61 REJECT\n>     messages was not removed from Bitcoin Core in V0.19. The behaviour\n>     in that upcoming release is that REJECT messages are disabled by\n>     default and can be enabled using the `-enablebip61` command line option.\n> \n>     Support for REJECT messages will be removed entirely in Bitcoin Core\n>     V0.20, expected for release in mid 2020. The PR to remove support\n>     was merged into Bitcoin Core's master branch this week.\n> \n>     Adoption of new Bitcoin Core versions across reachable nodes\n>     generally takes several months.\n>     https://bitnodes.earn.com/dashboard/?days=365 shows that although\n>     v0.18 was released in May 2019, there are still several hundred\n>     reachable nodes on V0.17, V0.16, V0.15 and earlier software.\n>     Software that currently use REJECT messages from public nodes for\n>     troubleshooting issues therefore have plenty of time to transition\n>     to one of the methods listed by Marco in the email above.\n> \n>     John\n> \n>     On Tue, Mar 5, 2019 at 10:28 PM Marco Falke via bitcoin-dev\n>     <bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> \n>         Bitcoin Core may send \"reject\" messages as response to \"tx\",\n>         \"block\" or\n>         \"version\" messages from a network peer when the message could\n>         not be accepted.\n> \n>         This feature is toggled by the `-enablebip61` command line\n>         option and has been\n>         disabled by default since Bitcoin Core version 0.18.0 (not yet\n>         released as of\n>         time of writing). Nodes on the network can not generally be\n>         trusted to send\n>         valid (\"reject\") messages, so this should only ever be used when\n>         connected to a\n>         trusted node. At this time, I am not aware of any software that\n>         requires this\n>         feature, and I would like to remove if from Bitcoin Core to make\n>         the codebase\n>         slimmer, easier to understand and maintain. Let us know if your\n>         application\n>         relies on this feature and you can not use any of the\n>         recommended alternatives:\n> \n>         * Testing or debugging of implementations of the Bitcoin P2P\n>         network protocol\n>         \u00a0 should be done by inspecting the log messages that are\n>         produced by a recent\n>         \u00a0 version of Bitcoin Core. Bitcoin Core logs debug messages\n>         \u00a0 (`-debug=<category>`) to a stream (`-printtoconsole`) or to a file\n>         \u00a0 (`-debuglogfile=<debug.log>`).\n> \n>         * Testing the validity of a block can be achieved by specific RPCs:\n>         \u00a0 - `submitblock`\n>         \u00a0 - `getblocktemplate` with `'mode'` set to `'proposal'` for\n>         blocks with\n>         \u00a0 \u00a0 potentially invalid POW\n> \n>         * Testing the validity of a transaction can be achieved by\n>         specific RPCs:\n>         \u00a0 - `sendrawtransaction`\n>         \u00a0 - `testmempoolaccept`\n> \n>         * Wallets should not use the absence of \"reject\" messages to\n>         indicate a\n>         \u00a0 transaction has propagated the network, nor should wallets use\n>         \"reject\"\n>         \u00a0 messages to set transaction fees. Wallets should rather use\n>         fee estimation\n>         \u00a0 to determine transaction fees and set replace-by-fee if\n>         desired. Thus, they\n>         \u00a0 could wait until the transaction has confirmed (taking into\n>         account the fee\n>         \u00a0 target they set (compare the RPC `estimatesmartfee`)) or\n>         listen for the\n>         \u00a0 transaction announcement by other network peers to check for\n>         propagation.\n> \n>         I propose to remove \"reject\" messages from Bitcoin Core 0.19.0\n>         unless there are\n>         valid concerns about its removal.\n> \n>         Marco\n>         _______________________________________________\n>         bitcoin-dev mailing list\n>         bitcoin-dev at lists.linuxfoundation.org\n>         <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            }
        ],
        "thread_summary": {
            "title": "Removal of reject network messages from Bitcoin Core (BIP61)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Voskuil",
                "John Newbery",
                "David A. Harding",
                "Andreas Schildbach"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 16151
        }
    },
    {
        "title": "[bitcoin-dev] Co-Author for ''Redefinition of the term address\"",
        "thread_messages": [
            {
                "author": "Emil Engler",
                "date": "2019-10-17T20:30:41",
                "message_text_only": "Hi, I need a co-author for this BIP:\nhttps://github.com/bitcoin/bips/pull/856\n\nSee the reasons mentioned by Marco Falke\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: pEpkey.asc\nType: application/pgp-keys\nSize: 3147 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191017/94c19d3c/attachment-0001.bin>"
            }
        ],
        "thread_summary": {
            "title": "Co-Author for ''Redefinition of the term address\"",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Emil Engler"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 390
        }
    },
    {
        "title": "[bitcoin-dev] Trustless hash-price insurance contracts",
        "thread_messages": [
            {
                "author": "Lucas H",
                "date": "2019-10-18T22:01:54",
                "message_text_only": "Hi,\n\nThis is my first post to this list -- even though I did some tiny\ncontributions to bitcoin core I feel quite a beginner -- so if my idea is\nstupid, already known, or too off-topic, just let me know.\n\nTL;DR: a trustless contract that guarantees minimum profitability of a\nmining operation -- in case Bitcoin/hash price goes too low. It can be\ntrustless bc we can use the assumption that the price of hashing is low to\nunlock funds.\n\nThe problem:\n\nA miner invests in new mining equipment, but if the hash-rate goes up too\nmuch (the price he is paid for a hash goes down by too much) he will have a\nloss.\n\nSolution: trustless hash-price insurance contract (or can we call it an\noption to sell hashes at a given price?)\n\nAn insurer who believes that it's unlikely the price of a hash will go down\na lot negotiates a contract with the miner implemented as a Bitcoin\ntransaction:\n\nInputs: a deposit from the insurer and a premium payment by the miner\nOutput1: simply the premium payment to the insurer\nOutput2 -- that's the actual insurance\n  There are three OR'ed conditions for paying it:\n  A. After expiry date (in blocks) insurer can spend\n  B. Both miner and insurer can spend at any time by mutual agreement\n  C. Before expiry, miner can spend by providing **a pre-image that\nproduces a hash within certain difficulty constraints**\n\nThe thing that makes it a hash-price insurance (or option, pardon my lack\nof precise financial jargon), is that if hashing becomes cheap enough, it\nbecomes profitable to spend resources finding a suitable pre-image, rather\nthan mining Bitcoin.\nOf course, both parties can reach an agreement that doesn't require\nactually spending these resources -- so the miner can still mine Bitcoin\nand compensate for the lower-than-expected reward with part of the\ninsurance deposit.\nIf the price doesn't go down enough, the miner just mines Bitcoin and the\ninsurer gets his deposit back.\nIt's basically an instrument for guaranteeing a minimum profitability of\nthe mining operation.\n\nImplementation issues: unfortunately we can't do arithmetic comparison with\nlong integers >32bit in the script, so implementation of the difficulty\nrequirement needs to be hacky. I think we can use the hashes of one or more\npre-images with a given short length, and the miner has to provide the\nexact pre-images. The pre-images are chosen by the insurer, and we would\nneed a \"honesty\" deposit or other mechanism to punish the insurer if he\nchooses a hash that doesn't correspond to any short-length pre-image. I'm\nnot sure about this implementation though, maybe we actually need new\nopcodes.\n\nWhat do you guys think?\nThanks for reading it all! Hope it was worth your time!\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191018/02afb5ce/attachment.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2019-10-20T05:03:09",
                "message_text_only": "Hi Lucas,\n\nI would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it\u2019s not clear why a loss is assumed in the case of subsequently increasing hash rate. \n\nThe assumption of increasing hash rate implies an expectation of increasing return on investment.  There are certainly speculative errors, but a loss on new equipment implies *all miners* are operating at a loss, which is not a sustainable situation.\n\nIf any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.\n\nBest,\nEric\n\n> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeff\n> Hi,\n> \n> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.\n> \n> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.\n> \n> The problem:\n> \n> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.\n> \n> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)\n> \n> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:\n> \n> Inputs: a deposit from the insurer and a premium payment by the miner\n> Output1: simply the premium payment to the insurer\n> Output2 -- that's the actual insurance\n>   There are three OR'ed conditions for paying it:\n>   A. After expiry date (in blocks) insurer can spend\n>   B. Both miner and insurer can spend at any time by mutual agreement\n>   C. Before expiry, miner can spend by providing **a pre-image that produces a hash within certain difficulty constraints**\n> \n> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.\n> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.\n> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.\n> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.\n> \n> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a \"honesty\" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.\n> \n> What do you guys think?\n> Thanks for reading it all! Hope it was worth your time!\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "JW Weatherman",
                "date": "2019-10-20T14:10:55",
                "message_text_only": "I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.\n\nDepending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.\n\nAnd the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.\n\n-JW\n\n\n\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Lucas,\n>\n> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it\u2019s not clear why a loss is assumed in the case of subsequently increasing hash rate.\n>\n> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.\n>\n> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.\n>\n> Best,\n> Eric\n>\n> > On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> > Hi,\n> > This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.\n> > TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.\n> > The problem:\n> > A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.\n> > Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)\n> > An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:\n> > Inputs: a deposit from the insurer and a premium payment by the miner\n> > Output1: simply the premium payment to the insurer\n> > Output2 -- that's the actual insurance\n> > There are three OR'ed conditions for paying it:\n> > A. After expiry date (in blocks) insurer can spend\n> > B. Both miner and insurer can spend at any time by mutual agreement\n> > C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints\n> > The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.\n> > Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.\n> > If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.\n> > It's basically an instrument for guaranteeing a minimum profitability of the mining operation.\n> > Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a \"honesty\" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.\n> > What do you guys think?\n> > Thanks for reading it all! Hope it was worth your time!\n> >\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Eric Voskuil",
                "date": "2019-10-20T14:57:37",
                "message_text_only": "> On Oct 20, 2019, at 10:10, JW Weatherman <jw at mathbot.com> wrote:\n> \n> \ufeffI think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.\n\nThis is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.\n\nWhat is being assumed is a hash rate increase *without* a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.\n\n> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.\n> \n> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.\n> \n> -JW\n> \n> \n> \n> \n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>> Hi Lucas,\n>> \n>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it\u2019s not clear why a loss is assumed in the case of subsequently increasing hash rate.\n>> \n>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.\n>> \n>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.\n>> \n>> Best,\n>> Eric\n>> \n>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>>> Hi,\n>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.\n>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.\n>>> The problem:\n>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.\n>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)\n>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:\n>>> Inputs: a deposit from the insurer and a premium payment by the miner\n>>> Output1: simply the premium payment to the insurer\n>>> Output2 -- that's the actual insurance\n>>> There are three OR'ed conditions for paying it:\n>>> A. After expiry date (in blocks) insurer can spend\n>>> B. Both miner and insurer can spend at any time by mutual agreement\n>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints\n>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.\n>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.\n>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.\n>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.\n>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a \"honesty\" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.\n>>> What do you guys think?\n>>> Thanks for reading it all! Hope it was worth your time!\n>>> \n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> \n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n>"
            },
            {
                "author": "JW Weatherman",
                "date": "2019-10-20T16:10:53",
                "message_text_only": "Oh, I see your point.\n\nHowever the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).\n\nAn analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.\n\n-JW\n\n\n\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:\n\n>\n>\n> > On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:\n> > I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.\n>\n> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.\n>\n> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.\n>\n> > Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.\n> > And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.\n> > -JW\n> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> >\n> > > On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> > > Hi Lucas,\n> > > I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it\u2019s not clear why a loss is assumed in the case of subsequently increasing hash rate.\n> > > The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.\n> > > If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.\n> > > Best,\n> > > Eric\n> > >\n> > > > > On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n> > > > > Hi,\n> > > > > This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.\n> > > > > TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.\n> > > > > The problem:\n> > > > > A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.\n> > > > > Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)\n> > > > > An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:\n> > > > > Inputs: a deposit from the insurer and a premium payment by the miner\n> > > > > Output1: simply the premium payment to the insurer\n> > > > > Output2 -- that's the actual insurance\n> > > > > There are three OR'ed conditions for paying it:\n> > > > > A. After expiry date (in blocks) insurer can spend\n> > > > > B. Both miner and insurer can spend at any time by mutual agreement\n> > > > > C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints\n> > > > > The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.\n> > > > > Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.\n> > > > > If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.\n> > > > > It's basically an instrument for guaranteeing a minimum profitability of the mining operation.\n> > > > > Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a \"honesty\" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.\n> > > > > What do you guys think?\n> > > > > Thanks for reading it all! Hope it was worth your time!\n> > > >\n> > > > bitcoin-dev mailing list\n> > > > bitcoin-dev at lists.linuxfoundation.org\n> > > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > >\n> > > bitcoin-dev mailing list\n> > > bitcoin-dev at lists.linuxfoundation.org\n> > > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Eric Voskuil",
                "date": "2019-10-20T16:16:57",
                "message_text_only": "So we are talking about a miner insuring against his own inefficiency.\n\nFurthermore a disproportionate increase in hash rate is based on the expectation of higher future return (investment leads returns). So the insurance could end up paying out against realized profit.\n\nGenerally speaking, insuring investment is a zero sum game.\n\ne\n\n> On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:\n> \n> \ufeffOh, I see your point.\n> \n> However the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).\n> \n> An analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.\n> \n> -JW\n> \n> \n> \n> \n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:\n>> \n>> \n>> \n>>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:\n>>> I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.\n>> \n>> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.\n>> \n>> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.\n>> \n>>> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.\n>>> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.\n>>> -JW\n>>> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>>> \n>>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>>>> Hi Lucas,\n>>>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it\u2019s not clear why a loss is assumed in the case of subsequently increasing hash rate.\n>>>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.\n>>>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.\n>>>> Best,\n>>>> Eric\n>>>> \n>>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>>>>>> Hi,\n>>>>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.\n>>>>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.\n>>>>>> The problem:\n>>>>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.\n>>>>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)\n>>>>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:\n>>>>>> Inputs: a deposit from the insurer and a premium payment by the miner\n>>>>>> Output1: simply the premium payment to the insurer\n>>>>>> Output2 -- that's the actual insurance\n>>>>>> There are three OR'ed conditions for paying it:\n>>>>>> A. After expiry date (in blocks) insurer can spend\n>>>>>> B. Both miner and insurer can spend at any time by mutual agreement\n>>>>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints\n>>>>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.\n>>>>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.\n>>>>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.\n>>>>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.\n>>>>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a \"honesty\" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.\n>>>>>> What do you guys think?\n>>>>>> Thanks for reading it all! Hope it was worth your time!\n>>>>> \n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>> \n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n>"
            },
            {
                "author": "Lucas H",
                "date": "2019-10-20T19:45:49",
                "message_text_only": "Hi, guys.\n\nThanks a lot for taking the time to read and discuss my post.\n\nI definitely wasn't clear enough about the problem statement -- so let me\ntry to clarify my thinking.\n\nFirst, the main uncertainty the miner is trying to protect against isn't\nthe inefficiency of his new equipment, but how much new mining equipment is\nbeing deployed world-wide, which he can't know in advance (as the system is\npermissionless).\n\nSecond, there are two different metrics that can mean \"profitable\" that I\nthink are getting confused (probably my fault for lack of using the right\nterms).\n\n- Let's call it \"operational profitability\", and use \"P\" to denote it,\nwhere P = [bitcoin earned]/time - [operational cost of running\nequipment]/time.\n   Obviously if P < 0, the miner will just shut down his equipment.\n- Return on investment (ROI). A positive ROI requires not just that P > 0,\nbut that it is enough to compensate for the initial investment of buying or\nbuilding the equipment. As long as P > 0, a miner will keep his equipment\nrunning, even at a negative ROI, as the alternative would be an even worse\nnegative ROI. Sure he can sell it, but however buys it will also keep it\nrunning, otherwise the equipment is worthless.\n\nThe instrument I describe above protects against the scenario where P > 0,\nbut ROI < 0.\n(it's possible it could be useful in some cases to protect against P < 0,\nbut that's not my main motivator and isn't an assumption)\n\nIf too many miners are deploying too much new equipment at the same time,\nit's possible that your ROI becomes negative, while nobody shuts down their\nequipment and the difficulty still keeps going up. In fact, it is possible\nfor all miners to have negative ROI for a while without a reduction in\ndifficulty. Difficulty would only go down in this case at the end of life\nof these equipment, if there isn't a new wave of even more efficient\nequipment\nbeing adopted before that.\n\nLet's see a simplified scenario in which the insurance becomes useful. This\nis just one example, and other scenarios could also work.\n\n- Bitcoin price relatively constant, that is, it's not the main driver of P\nduring this period.\n- Approximately constant block rewards.\n- New equipment comes to market with much higher efficiency than all old\nequipment. So the old stock of old equipment becomes irrelevant after a\nshort while.\n- All miners decide to deploy new equipment, but none knows how much the\nothers are deploying, or when, or at what price or P.\n- Let's just assume P>0 for all miners using the new equipment.\n- Let's assume every unit of the new equipment runs at the same maximum\nhashrate it's capable of.\n\nLet's say miner A buys Na units of the new equipment and the total number\ndeployed by all miners is N.\n\nA's share of the block rewards will be Na / N.\n\nIf N is much higher than A's initial estimate, his ROI might well become\nnegative, and the insurance would help him prevent a loss.\n\nHope this makes the problem a bit clearer.\n\nThanks!\n@lucash-dev\n\nOn Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:\n\n> So we are talking about a miner insuring against his own inefficiency.\n>\n> Furthermore a disproportionate increase in hash rate is based on the\n> expectation of higher future return (investment leads returns). So the\n> insurance could end up paying out against realized profit.\n>\n> Generally speaking, insuring investment is a zero sum game.\n>\n> e\n>\n> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:\n> >\n> > \ufeffOh, I see your point.\n> >\n> > However the insurance contract would protect the miner even in that\n> case. A miner with great confidence that he is running optimal hardware and\n> has optimal electricity and labor costs probably wouldn't be interested in\n> purchasing insurance for a high price, but if it was cheap enough it would\n> still be worth it. And any potential new entrants on the edge of jumping in\n> would enter when they otherwise would not have because of the decreased\n> costs (decreased risk).\n> >\n> > An analogy would be car insurance. If you are an excellent driver you\n> wouldn't be willing to spend a ton of money to protect your car in the\n> event of an accident, but if it is cheap enough you would. And there may be\n> people that are unwilling to take the risk of a damaged car that refrain\n> from becoming drivers until insurance allows them to lower the worst case\n> scenario of a damaged car.\n> >\n> > -JW\n> >\n> >\n> >\n> >\n> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org>\n> wrote:\n> >>\n> >>\n> >>\n> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:\n> >>> I think the assumption is not that all miners are unprofitable, but\n> that a single miner could make an investment that becomes unprofitable if\n> the hash rate increases more than he expected.\n> >>\n> >> This is a restatement of the assumption I questioned. Hash rate\n> increase does not imply unprofitability. The new rig should be profitable.\n> >>\n> >> What is being assumed is a hash rate increase without a proportional\n> block reward value increase. In this case if the newest equipment is\n> unprofitable, all miners are unprofitable.\n> >>\n> >>> Depending on the cost of the offered insurance it would be prudent for\n> a miner to decrease his potential loss by buying insurance for this\n> possibility.\n> >>> And the existence of attractive insurance contracts would lower the\n> barrier to entry for new competitors in mining and this would increase\n> bitcoins security.\n> >>> -JW\n> >>> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> >>>\n> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev\n> bitcoin-dev at lists.linuxfoundation.org wrote:\n> >>>> Hi Lucas,\n> >>>> I would question the assumption inherent in the problem statement.\n> Setting aside variance discount, proximity premium, and questions of\n> relative efficiency, as these are presumably already considered by the\n> miner upon the purchase of new equipment, it\u2019s not clear why a loss is\n> assumed in the case of subsequently increasing hash rate.\n> >>>> The assumption of increasing hash rate implies an expectation of\n> increasing return on investment. There are certainly speculative errors,\n> but a loss on new equipment implies all miners are operating at a loss,\n> which is not a sustainable situation.\n> >>>> If any miner is profitable it is the miner with the new equipment,\n> and if he is not, hash rate will drop until he is. This drop is most likely\n> to be precipitated by older equipment going offline.\n> >>>> Best,\n> >>>> Eric\n> >>>>\n> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev\n> bitcoin-dev at lists.linuxfoundation.org wrote:\n> >>>>>> Hi,\n> >>>>>> This is my first post to this list -- even though I did some tiny\n> contributions to bitcoin core I feel quite a beginner -- so if my idea is\n> stupid, already known, or too off-topic, just let me know.\n> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability\n> of a mining operation -- in case Bitcoin/hash price goes too low. It can be\n> trustless bc we can use the assumption that the price of hashing is low to\n> unlock funds.\n> >>>>>> The problem:\n> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes\n> up too much (the price he is paid for a hash goes down by too much) he will\n> have a loss.\n> >>>>>> Solution: trustless hash-price insurance contract (or can we call\n> it an option to sell hashes at a given price?)\n> >>>>>> An insurer who believes that it's unlikely the price of a hash will\n> go down a lot negotiates a contract with the miner implemented as a Bitcoin\n> transaction:\n> >>>>>> Inputs: a deposit from the insurer and a premium payment by the\n> miner\n> >>>>>> Output1: simply the premium payment to the insurer\n> >>>>>> Output2 -- that's the actual insurance\n> >>>>>> There are three OR'ed conditions for paying it:\n> >>>>>> A. After expiry date (in blocks) insurer can spend\n> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement\n> >>>>>> C. Before expiry, miner can spend by providing a pre-image that\n> produces a hash within certain difficulty constraints\n> >>>>>> The thing that makes it a hash-price insurance (or option, pardon\n> my lack of precise financial jargon), is that if hashing becomes cheap\n> enough, it becomes profitable to spend resources finding a suitable\n> pre-image, rather than mining Bitcoin.\n> >>>>>> Of course, both parties can reach an agreement that doesn't require\n> actually spending these resources -- so the miner can still mine Bitcoin\n> and compensate for the lower-than-expected reward with part of the\n> insurance deposit.\n> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin\n> and the insurer gets his deposit back.\n> >>>>>> It's basically an instrument for guaranteeing a minimum\n> profitability of the mining operation.\n> >>>>>> Implementation issues: unfortunately we can't do arithmetic\n> comparison with long integers >32bit in the script, so implementation of\n> the difficulty requirement needs to be hacky. I think we can use the hashes\n> of one or more pre-images with a given short length, and the miner has to\n> provide the exact pre-images. The pre-images are chosen by the insurer, and\n> we would need a \"honesty\" deposit or other mechanism to punish the insurer\n> if he chooses a hash that doesn't correspond to any short-length pre-image.\n> I'm not sure about this implementation though, maybe we actually need new\n> opcodes.\n> >>>>>> What do you guys think?\n> >>>>>> Thanks for reading it all! Hope it was worth your time!\n> >>>>>\n> >>>>> bitcoin-dev mailing list\n> >>>>> bitcoin-dev at lists.linuxfoundation.org\n> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>>>\n> >>>> bitcoin-dev mailing list\n> >>>> bitcoin-dev at lists.linuxfoundation.org\n> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/46e65991/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2019-10-20T20:17:03",
                "message_text_only": "Hi Lucas,\n\nThis can all be inferred from the problem statement. In other words this doesn\u2019t change the assumptions behind my comments. However this is an unsupportable assumption:\n\n\u201cDifficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment being adopted before that.\u201d\n\nOperating at a loss would only be justifiable in the case of expected future returns, not due to sunk costs.\n\ne\n\n> On Oct 20, 2019, at 15:46, Lucas H <lucash.dev at gmail.com> wrote:\n> \n> \ufeff\n> Hi, guys.\n> \n> Thanks a lot for taking the time to read and discuss my post.\n> \n> I definitely wasn't clear enough about the problem statement -- so let me try to clarify my thinking.\n> \n> First, the main uncertainty the miner is trying to protect against isn't the inefficiency of his new equipment, but how much new mining equipment is being deployed world-wide, which he can't know in advance (as the system is permissionless).\n> \n> Second, there are two different metrics that can mean \"profitable\" that I think are getting confused (probably my fault for lack of using the right terms).\n> \n> - Let's call it \"operational profitability\", and use \"P\" to denote it, where P = [bitcoin earned]/time - [operational cost of running equipment]/time.\n>    Obviously if P < 0, the miner will just shut down his equipment.\n> - Return on investment (ROI). A positive ROI requires not just that P > 0, but that it is enough to compensate for the initial investment of buying or building the equipment. As long as P > 0, a miner will keep his equipment running, even at a negative ROI, as the alternative would be an even worse negative ROI. Sure he can sell it, but however buys it will also keep it running, otherwise the equipment is worthless.\n> \n> The instrument I describe above protects against the scenario where P > 0, but ROI < 0.\n> (it's possible it could be useful in some cases to protect against P < 0, but that's not my main motivator and isn't an assumption)\n> \n> If too many miners are deploying too much new equipment at the same time, it's possible that your ROI becomes negative, while nobody shuts down their equipment and the difficulty still keeps going up. In fact, it is possible for all miners to have negative ROI for a while without a reduction in difficulty. Difficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment\n> being adopted before that.\n> \n> Let's see a simplified scenario in which the insurance becomes useful. This is just one example, and other scenarios could also work.\n> \n> - Bitcoin price relatively constant, that is, it's not the main driver of P during this period.\n> - Approximately constant block rewards.\n> - New equipment comes to market with much higher efficiency than all old equipment. So the old stock of old equipment becomes irrelevant after a short while. \n> - All miners decide to deploy new equipment, but none knows how much the others are deploying, or when, or at what price or P. \n> - Let's just assume P>0 for all miners using the new equipment.\n> - Let's assume every unit of the new equipment runs at the same maximum hashrate it's capable of.\n> \n> Let's say miner A buys Na units of the new equipment and the total number deployed by all miners is N.\n> \n> A's share of the block rewards will be Na / N. \n> \n> If N is much higher than A's initial estimate, his ROI might well become negative, and the insurance would help him prevent a loss.\n> \n> Hope this makes the problem a bit clearer.\n> \n> Thanks!\n> @lucash-dev\n> \n>> On Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:\n>> So we are talking about a miner insuring against his own inefficiency.\n>> \n>> Furthermore a disproportionate increase in hash rate is based on the expectation of higher future return (investment leads returns). So the insurance could end up paying out against realized profit.\n>> \n>> Generally speaking, insuring investment is a zero sum game.\n>> \n>> e\n>> \n>> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:\n>> > \n>> > \ufeffOh, I see your point.\n>> > \n>> > However the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).\n>> > \n>> > An analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.\n>> > \n>> > -JW\n>> > \n>> > \n>> > \n>> > \n>> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:\n>> >> \n>> >> \n>> >> \n>> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:\n>> >>> I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.\n>> >> \n>> >> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.\n>> >> \n>> >> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.\n>> >> \n>> >>> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.\n>> >>> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.\n>> >>> -JW\n>> >>> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>> >>> \n>> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>> >>>> Hi Lucas,\n>> >>>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it\u2019s not clear why a loss is assumed in the case of subsequently increasing hash rate.\n>> >>>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.\n>> >>>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.\n>> >>>> Best,\n>> >>>> Eric\n>> >>>> \n>> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>> >>>>>> Hi,\n>> >>>>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.\n>> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.\n>> >>>>>> The problem:\n>> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.\n>> >>>>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)\n>> >>>>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:\n>> >>>>>> Inputs: a deposit from the insurer and a premium payment by the miner\n>> >>>>>> Output1: simply the premium payment to the insurer\n>> >>>>>> Output2 -- that's the actual insurance\n>> >>>>>> There are three OR'ed conditions for paying it:\n>> >>>>>> A. After expiry date (in blocks) insurer can spend\n>> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement\n>> >>>>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints\n>> >>>>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.\n>> >>>>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.\n>> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.\n>> >>>>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.\n>> >>>>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a \"honesty\" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.\n>> >>>>>> What do you guys think?\n>> >>>>>> Thanks for reading it all! Hope it was worth your time!\n>> >>>>> \n>> >>>>> bitcoin-dev mailing list\n>> >>>>> bitcoin-dev at lists.linuxfoundation.org\n>> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> >>>> \n>> >>>> bitcoin-dev mailing list\n>> >>>> bitcoin-dev at lists.linuxfoundation.org\n>> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> > \n>> > \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/1a643cc1/attachment-0001.html>"
            },
            {
                "author": "Lucas H",
                "date": "2019-10-20T21:33:24",
                "message_text_only": "Sorry, Eric, but I think you're completely missing the point.\n\nIt has nothing to do with sunken cost -- but the fact that the mining\nequipment is good for nothing else other than performing hashing operations.\nAs long as someone can get paid more than they spend to keep the equipment\nrunning, i.e. P>0,  it will keep running.\nYour argument only makes sense in an ASIC-free world.\n\nLet's assume you decide to just shut down your whole operation. In that\nscenario, it doesn't make sense *not* to sell your equipment, even at a\nloss. Just destroying it makes no economic sense: your loss would be much\nworse. So you'll sell it -- at a loss -- to someone who will buy it at a\nprice that will make *their* ROI>0 for keeping the equipment running --\nand the equipment *will* be again running, and *will* keep the hashrate\nhigh. Only consequence of you shutting down your operation is you taking a\nloss.\n\nEven if you sell it to someone who will run it exactly as efficiently as\nyou, or even at lower efficiency (as long as P>0), they'll just pay less\nfor the equipment than you did, their ROI will be >0 and you'll bear the\nloss. No drop in hashrate.\n\nHashrate can only respond to mining being unprofitable in the sense \"P\" --\nnot in the sense \"ROI\". But a miner can still go bankrupt even if P>0.\n\nPlease note that none of the above breaks the economic assumptions of the\nprotocol. The problem I'm talking about isn't a problem in the protocol,\nbut a problem for miners -- and it's the same as in many kinds of economic\nactivity.\n\nConsider investing in building an oil refinery -- if the price of the\nrefined products get lower than expected to pay for the capital, but still\nhigh enough to pay for operating costs, you'd rather keep it running (or\nsell to someone who will keep it running) than just sell the parts as scrap\nmetal. In that case you might want to protect yourself against the price of\nthe refined products going too low.\n\nOf course miners can (and maybe already do) hedge against these scenarios\nusing other kinds of instruments -- most likely facilitated by a trusted\n3rd party. I'm just interested in the possibility of a new, trustless\ninstrument.\n\n*Anyway* I'm far more interested in the technical feasibility of the\ncontract, given the economic assumptions, than it's economic practicality\nin the present.\n\n\n\n\n\nOn Sun, Oct 20, 2019 at 1:17 PM Eric Voskuil <eric at voskuil.org> wrote:\n\n> Hi Lucas,\n>\n> This can all be inferred from the problem statement. In other words this\n> doesn\u2019t change the assumptions behind my comments. However this is an\n> unsupportable assumption:\n>\n> \u201cDifficulty would only go down in this case at the end of life of these\n> equipment, if there isn't a new wave of even more efficient equipment being\n> adopted before that.\u201d\n>\n> Operating at a loss would only be justifiable in the case of expected\n> future returns, not due to sunk costs.\n>\n> e\n>\n> On Oct 20, 2019, at 15:46, Lucas H <lucash.dev at gmail.com> wrote:\n>\n> \ufeff\n> Hi, guys.\n>\n> Thanks a lot for taking the time to read and discuss my post.\n>\n> I definitely wasn't clear enough about the problem statement -- so let me\n> try to clarify my thinking.\n>\n> First, the main uncertainty the miner is trying to protect against isn't\n> the inefficiency of his new equipment, but how much new mining equipment is\n> being deployed world-wide, which he can't know in advance (as the system is\n> permissionless).\n>\n> Second, there are two different metrics that can mean \"profitable\" that I\n> think are getting confused (probably my fault for lack of using the right\n> terms).\n>\n> - Let's call it \"operational profitability\", and use \"P\" to denote it,\n> where P = [bitcoin earned]/time - [operational cost of running\n> equipment]/time.\n>    Obviously if P < 0, the miner will just shut down his equipment.\n> - Return on investment (ROI). A positive ROI requires not just that P > 0,\n> but that it is enough to compensate for the initial investment of buying or\n> building the equipment. As long as P > 0, a miner will keep his equipment\n> running, even at a negative ROI, as the alternative would be an even worse\n> negative ROI. Sure he can sell it, but however buys it will also keep it\n> running, otherwise the equipment is worthless.\n>\n> The instrument I describe above protects against the scenario where P > 0,\n> but ROI < 0.\n> (it's possible it could be useful in some cases to protect against P < 0,\n> but that's not my main motivator and isn't an assumption)\n>\n> If too many miners are deploying too much new equipment at the same time,\n> it's possible that your ROI becomes negative, while nobody shuts down their\n> equipment and the difficulty still keeps going up. In fact, it is possible\n> for all miners to have negative ROI for a while without a reduction in\n> difficulty. Difficulty would only go down in this case at the end of life\n> of these equipment, if there isn't a new wave of even more efficient\n> equipment\n> being adopted before that.\n>\n> Let's see a simplified scenario in which the insurance becomes useful.\n> This is just one example, and other scenarios could also work.\n>\n> - Bitcoin price relatively constant, that is, it's not the main driver of\n> P during this period.\n> - Approximately constant block rewards.\n> - New equipment comes to market with much higher efficiency than all old\n> equipment. So the old stock of old equipment becomes irrelevant after a\n> short while.\n> - All miners decide to deploy new equipment, but none knows how much the\n> others are deploying, or when, or at what price or P.\n> - Let's just assume P>0 for all miners using the new equipment.\n> - Let's assume every unit of the new equipment runs at the same maximum\n> hashrate it's capable of.\n>\n> Let's say miner A buys Na units of the new equipment and the total number\n> deployed by all miners is N.\n>\n> A's share of the block rewards will be Na / N.\n>\n> If N is much higher than A's initial estimate, his ROI might well become\n> negative, and the insurance would help him prevent a loss.\n>\n> Hope this makes the problem a bit clearer.\n>\n> Thanks!\n> @lucash-dev\n>\n> On Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:\n>\n>> So we are talking about a miner insuring against his own inefficiency.\n>>\n>> Furthermore a disproportionate increase in hash rate is based on the\n>> expectation of higher future return (investment leads returns). So the\n>> insurance could end up paying out against realized profit.\n>>\n>> Generally speaking, insuring investment is a zero sum game.\n>>\n>> e\n>>\n>> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:\n>> >\n>> > \ufeffOh, I see your point.\n>> >\n>> > However the insurance contract would protect the miner even in that\n>> case. A miner with great confidence that he is running optimal hardware and\n>> has optimal electricity and labor costs probably wouldn't be interested in\n>> purchasing insurance for a high price, but if it was cheap enough it would\n>> still be worth it. And any potential new entrants on the edge of jumping in\n>> would enter when they otherwise would not have because of the decreased\n>> costs (decreased risk).\n>> >\n>> > An analogy would be car insurance. If you are an excellent driver you\n>> wouldn't be willing to spend a ton of money to protect your car in the\n>> event of an accident, but if it is cheap enough you would. And there may be\n>> people that are unwilling to take the risk of a damaged car that refrain\n>> from becoming drivers until insurance allows them to lower the worst case\n>> scenario of a damaged car.\n>> >\n>> > -JW\n>> >\n>> >\n>> >\n>> >\n>> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org>\n>> wrote:\n>> >>\n>> >>\n>> >>\n>> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:\n>> >>> I think the assumption is not that all miners are unprofitable, but\n>> that a single miner could make an investment that becomes unprofitable if\n>> the hash rate increases more than he expected.\n>> >>\n>> >> This is a restatement of the assumption I questioned. Hash rate\n>> increase does not imply unprofitability. The new rig should be profitable.\n>> >>\n>> >> What is being assumed is a hash rate increase without a proportional\n>> block reward value increase. In this case if the newest equipment is\n>> unprofitable, all miners are unprofitable.\n>> >>\n>> >>> Depending on the cost of the offered insurance it would be prudent\n>> for a miner to decrease his potential loss by buying insurance for this\n>> possibility.\n>> >>> And the existence of attractive insurance contracts would lower the\n>> barrier to entry for new competitors in mining and this would increase\n>> bitcoins security.\n>> >>> -JW\n>> >>> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>> >>>\n>> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev\n>> bitcoin-dev at lists.linuxfoundation.org wrote:\n>> >>>> Hi Lucas,\n>> >>>> I would question the assumption inherent in the problem statement.\n>> Setting aside variance discount, proximity premium, and questions of\n>> relative efficiency, as these are presumably already considered by the\n>> miner upon the purchase of new equipment, it\u2019s not clear why a loss is\n>> assumed in the case of subsequently increasing hash rate.\n>> >>>> The assumption of increasing hash rate implies an expectation of\n>> increasing return on investment. There are certainly speculative errors,\n>> but a loss on new equipment implies all miners are operating at a loss,\n>> which is not a sustainable situation.\n>> >>>> If any miner is profitable it is the miner with the new equipment,\n>> and if he is not, hash rate will drop until he is. This drop is most likely\n>> to be precipitated by older equipment going offline.\n>> >>>> Best,\n>> >>>> Eric\n>> >>>>\n>> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev\n>> bitcoin-dev at lists.linuxfoundation.org wrote:\n>> >>>>>> Hi,\n>> >>>>>> This is my first post to this list -- even though I did some tiny\n>> contributions to bitcoin core I feel quite a beginner -- so if my idea is\n>> stupid, already known, or too off-topic, just let me know.\n>> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability\n>> of a mining operation -- in case Bitcoin/hash price goes too low. It can be\n>> trustless bc we can use the assumption that the price of hashing is low to\n>> unlock funds.\n>> >>>>>> The problem:\n>> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes\n>> up too much (the price he is paid for a hash goes down by too much) he will\n>> have a loss.\n>> >>>>>> Solution: trustless hash-price insurance contract (or can we call\n>> it an option to sell hashes at a given price?)\n>> >>>>>> An insurer who believes that it's unlikely the price of a hash\n>> will go down a lot negotiates a contract with the miner implemented as a\n>> Bitcoin transaction:\n>> >>>>>> Inputs: a deposit from the insurer and a premium payment by the\n>> miner\n>> >>>>>> Output1: simply the premium payment to the insurer\n>> >>>>>> Output2 -- that's the actual insurance\n>> >>>>>> There are three OR'ed conditions for paying it:\n>> >>>>>> A. After expiry date (in blocks) insurer can spend\n>> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement\n>> >>>>>> C. Before expiry, miner can spend by providing a pre-image that\n>> produces a hash within certain difficulty constraints\n>> >>>>>> The thing that makes it a hash-price insurance (or option, pardon\n>> my lack of precise financial jargon), is that if hashing becomes cheap\n>> enough, it becomes profitable to spend resources finding a suitable\n>> pre-image, rather than mining Bitcoin.\n>> >>>>>> Of course, both parties can reach an agreement that doesn't\n>> require actually spending these resources -- so the miner can still mine\n>> Bitcoin and compensate for the lower-than-expected reward with part of the\n>> insurance deposit.\n>> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin\n>> and the insurer gets his deposit back.\n>> >>>>>> It's basically an instrument for guaranteeing a minimum\n>> profitability of the mining operation.\n>> >>>>>> Implementation issues: unfortunately we can't do arithmetic\n>> comparison with long integers >32bit in the script, so implementation of\n>> the difficulty requirement needs to be hacky. I think we can use the hashes\n>> of one or more pre-images with a given short length, and the miner has to\n>> provide the exact pre-images. The pre-images are chosen by the insurer, and\n>> we would need a \"honesty\" deposit or other mechanism to punish the insurer\n>> if he chooses a hash that doesn't correspond to any short-length pre-image.\n>> I'm not sure about this implementation though, maybe we actually need new\n>> opcodes.\n>> >>>>>> What do you guys think?\n>> >>>>>> Thanks for reading it all! Hope it was worth your time!\n>> >>>>>\n>> >>>>> bitcoin-dev mailing list\n>> >>>>> bitcoin-dev at lists.linuxfoundation.org\n>> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> >>>>\n>> >>>> bitcoin-dev mailing list\n>> >>>> bitcoin-dev at lists.linuxfoundation.org\n>> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> >\n>> >\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/556d1bf2/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2019-10-21T03:34:12",
                "message_text_only": "Hi Lucas,\n\nYou are assuming that all miners operate at equal efficiency. The least efficient miners are expected to drop offline first. Even with identical hardware and operational efficiency, the necessary variance discount and proximity premium create a profitability spread. The relation between hash rate and reward value is not only predictable, it is easily observed.\n\nThere is an error in your sold hardware scenario. When equipment is sold at a loss, the remaining operating miners have a reduced capital cost, which means, despite higher hash rate, miners are profitable. The least efficient miners have written down their expected losses and hash rate becomes consistent with market returns despite being higher.\n\nWith respect to the contract, I don\u2019t yet see this working, but there are several gaps and I don\u2019t want to make assumptions. More detailed specification would be helpful. Even a full scenario with numbers and justifications would be something.\n\ne\n\n> On Oct 20, 2019, at 14:33, Lucas H <lucash.dev at gmail.com> wrote:\n> \n> Sorry, Eric, but I think you're completely missing the point.\n> \n> It has nothing to do with sunken cost -- but the fact that the mining equipment is good for nothing else other than performing hashing operations.\n> As long as someone can get paid more than they spend to keep the equipment running, i.e. P>0,  it will keep running.\n> Your argument only makes sense in an ASIC-free world.\n> \n> Let's assume you decide to just shut down your whole operation. In that scenario, it doesn't make sense *not* to sell your equipment, even at a loss. Just destroying it makes no economic sense: your loss would be much worse. So you'll sell it -- at a loss -- to someone who will buy it at a price that will make *their* ROI>0 for keeping the equipment running --  and the equipment *will* be again running, and *will* keep the hashrate high. Only consequence of you shutting down your operation is you taking a loss. \n> \n> Even if you sell it to someone who will run it exactly as efficiently as you, or even at lower efficiency (as long as P>0), they'll just pay less for the equipment than you did, their ROI will be >0 and you'll bear the loss. No drop in hashrate.\n> \n> Hashrate can only respond to mining being unprofitable in the sense \"P\" -- not in the sense \"ROI\". But a miner can still go bankrupt even if P>0.\n> \n> Please note that none of the above breaks the economic assumptions of the protocol. The problem I'm talking about isn't a problem in the protocol, but a problem for miners -- and it's the same as in many kinds of economic activity.\n> \n> Consider investing in building an oil refinery -- if the price of the refined products get lower than expected to pay for the capital, but still high enough to pay for operating costs, you'd rather keep it running (or sell to someone who will keep it running) than just sell the parts as scrap metal. In that case you might want to protect yourself against the price of the refined products going too low.\n> \n> Of course miners can (and maybe already do) hedge against these scenarios using other kinds of instruments -- most likely facilitated by a trusted 3rd party. I'm just interested in the possibility of a new, trustless instrument.\n> \n> *Anyway* I'm far more interested in the technical feasibility of the contract, given the economic assumptions, than it's economic practicality in the present.\n> \n> \n> \n> \n> \n>>> On Sun, Oct 20, 2019 at 1:17 PM Eric Voskuil <eric at voskuil.org> wrote:\n>>> Hi Lucas,\n>>> \n>>> This can all be inferred from the problem statement. In other words this doesn\u2019t change the assumptions behind my comments. However this is an unsupportable assumption:\n>>> \n>>> \u201cDifficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment being adopted before that.\u201d\n>>> \n>>> Operating at a loss would only be justifiable in the case of expected future returns, not due to sunk costs.\n>>> \n>>> e\n>>> \n>>>> On Oct 20, 2019, at 15:46, Lucas H <lucash.dev at gmail.com> wrote:\n>>>> \n>>> \ufeff\n>>> Hi, guys.\n>>> \n>>> Thanks a lot for taking the time to read and discuss my post.\n>>> \n>>> I definitely wasn't clear enough about the problem statement -- so let me try to clarify my thinking.\n>>> \n>>> First, the main uncertainty the miner is trying to protect against isn't the inefficiency of his new equipment, but how much new mining equipment is being deployed world-wide, which he can't know in advance (as the system is permissionless).\n>>> \n>>> Second, there are two different metrics that can mean \"profitable\" that I think are getting confused (probably my fault for lack of using the right terms).\n>>> \n>>> - Let's call it \"operational profitability\", and use \"P\" to denote it, where P = [bitcoin earned]/time - [operational cost of running equipment]/time.\n>>>    Obviously if P < 0, the miner will just shut down his equipment.\n>>> - Return on investment (ROI). A positive ROI requires not just that P > 0, but that it is enough to compensate for the initial investment of buying or building the equipment. As long as P > 0, a miner will keep his equipment running, even at a negative ROI, as the alternative would be an even worse negative ROI. Sure he can sell it, but however buys it will also keep it running, otherwise the equipment is worthless.\n>>> \n>>> The instrument I describe above protects against the scenario where P > 0, but ROI < 0.\n>>> (it's possible it could be useful in some cases to protect against P < 0, but that's not my main motivator and isn't an assumption)\n>>> \n>>> If too many miners are deploying too much new equipment at the same time, it's possible that your ROI becomes negative, while nobody shuts down their equipment and the difficulty still keeps going up. In fact, it is possible for all miners to have negative ROI for a while without a reduction in difficulty. Difficulty would only go down in this case at the end of life of these equipment, if there isn't a new wave of even more efficient equipment\n>>> being adopted before that.\n>>> \n>>> Let's see a simplified scenario in which the insurance becomes useful. This is just one example, and other scenarios could also work.\n>>> \n>>> - Bitcoin price relatively constant, that is, it's not the main driver of P during this period.\n>>> - Approximately constant block rewards.\n>>> - New equipment comes to market with much higher efficiency than all old equipment. So the old stock of old equipment becomes irrelevant after a short while. \n>>> - All miners decide to deploy new equipment, but none knows how much the others are deploying, or when, or at what price or P. \n>>> - Let's just assume P>0 for all miners using the new equipment.\n>>> - Let's assume every unit of the new equipment runs at the same maximum hashrate it's capable of.\n>>> \n>>> Let's say miner A buys Na units of the new equipment and the total number deployed by all miners is N.\n>>> \n>>> A's share of the block rewards will be Na / N. \n>>> \n>>> If N is much higher than A's initial estimate, his ROI might well become negative, and the insurance would help him prevent a loss.\n>>> \n>>> Hope this makes the problem a bit clearer.\n>>> \n>>> Thanks!\n>>> @lucash-dev\n>>> \n>>>> On Sun, Oct 20, 2019 at 9:16 AM Eric Voskuil <eric at voskuil.org> wrote:\n>>>> So we are talking about a miner insuring against his own inefficiency.\n>>>> \n>>>> Furthermore a disproportionate increase in hash rate is based on the expectation of higher future return (investment leads returns). So the insurance could end up paying out against realized profit.\n>>>> \n>>>> Generally speaking, insuring investment is a zero sum game.\n>>>> \n>>>> e\n>>>> \n>>>> > On Oct 20, 2019, at 12:10, JW Weatherman <jw at mathbot.com> wrote:\n>>>> > \n>>>> > \ufeffOh, I see your point.\n>>>> > \n>>>> > However the insurance contract would protect the miner even in that case. A miner with great confidence that he is running optimal hardware and has optimal electricity and labor costs probably wouldn't be interested in purchasing insurance for a high price, but if it was cheap enough it would still be worth it. And any potential new entrants on the edge of jumping in would enter when they otherwise would not have because of the decreased costs (decreased risk).\n>>>> > \n>>>> > An analogy would be car insurance. If you are an excellent driver you wouldn't be willing to spend a ton of money to protect your car in the event of an accident, but if it is cheap enough you would. And there may be people that are unwilling to take the risk of a damaged car that refrain from becoming drivers until insurance allows them to lower the worst case scenario of a damaged car.\n>>>> > \n>>>> > -JW\n>>>> > \n>>>> > \n>>>> > \n>>>> > \n>>>> > \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>>>> >> On Sunday, October 20, 2019 10:57 AM, Eric Voskuil <eric at voskuil.org> wrote:\n>>>> >> \n>>>> >> \n>>>> >> \n>>>> >>>> On Oct 20, 2019, at 10:10, JW Weatherman jw at mathbot.com wrote:\n>>>> >>> I think the assumption is not that all miners are unprofitable, but that a single miner could make an investment that becomes unprofitable if the hash rate increases more than he expected.\n>>>> >> \n>>>> >> This is a restatement of the assumption I questioned. Hash rate increase does not imply unprofitability. The new rig should be profitable.\n>>>> >> \n>>>> >> What is being assumed is a hash rate increase without a proportional block reward value increase. In this case if the newest equipment is unprofitable, all miners are unprofitable.\n>>>> >> \n>>>> >>> Depending on the cost of the offered insurance it would be prudent for a miner to decrease his potential loss by buying insurance for this possibility.\n>>>> >>> And the existence of attractive insurance contracts would lower the barrier to entry for new competitors in mining and this would increase bitcoins security.\n>>>> >>> -JW\n>>>> >>> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n>>>> >>> \n>>>> >>>> On Sunday, October 20, 2019 1:03 AM, Eric Voskuil via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>>>> >>>> Hi Lucas,\n>>>> >>>> I would question the assumption inherent in the problem statement. Setting aside variance discount, proximity premium, and questions of relative efficiency, as these are presumably already considered by the miner upon the purchase of new equipment, it\u2019s not clear why a loss is assumed in the case of subsequently increasing hash rate.\n>>>> >>>> The assumption of increasing hash rate implies an expectation of increasing return on investment. There are certainly speculative errors, but a loss on new equipment implies all miners are operating at a loss, which is not a sustainable situation.\n>>>> >>>> If any miner is profitable it is the miner with the new equipment, and if he is not, hash rate will drop until he is. This drop is most likely to be precipitated by older equipment going offline.\n>>>> >>>> Best,\n>>>> >>>> Eric\n>>>> >>>> \n>>>> >>>>>> On Oct 20, 2019, at 00:31, Lucas H via bitcoin-dev bitcoin-dev at lists.linuxfoundation.org wrote:\n>>>> >>>>>> Hi,\n>>>> >>>>>> This is my first post to this list -- even though I did some tiny contributions to bitcoin core I feel quite a beginner -- so if my idea is stupid, already known, or too off-topic, just let me know.\n>>>> >>>>>> TL;DR: a trustless contract that guarantees minimum profitability of a mining operation -- in case Bitcoin/hash price goes too low. It can be trustless bc we can use the assumption that the price of hashing is low to unlock funds.\n>>>> >>>>>> The problem:\n>>>> >>>>>> A miner invests in new mining equipment, but if the hash-rate goes up too much (the price he is paid for a hash goes down by too much) he will have a loss.\n>>>> >>>>>> Solution: trustless hash-price insurance contract (or can we call it an option to sell hashes at a given price?)\n>>>> >>>>>> An insurer who believes that it's unlikely the price of a hash will go down a lot negotiates a contract with the miner implemented as a Bitcoin transaction:\n>>>> >>>>>> Inputs: a deposit from the insurer and a premium payment by the miner\n>>>> >>>>>> Output1: simply the premium payment to the insurer\n>>>> >>>>>> Output2 -- that's the actual insurance\n>>>> >>>>>> There are three OR'ed conditions for paying it:\n>>>> >>>>>> A. After expiry date (in blocks) insurer can spend\n>>>> >>>>>> B. Both miner and insurer can spend at any time by mutual agreement\n>>>> >>>>>> C. Before expiry, miner can spend by providing a pre-image that produces a hash within certain difficulty constraints\n>>>> >>>>>> The thing that makes it a hash-price insurance (or option, pardon my lack of precise financial jargon), is that if hashing becomes cheap enough, it becomes profitable to spend resources finding a suitable pre-image, rather than mining Bitcoin.\n>>>> >>>>>> Of course, both parties can reach an agreement that doesn't require actually spending these resources -- so the miner can still mine Bitcoin and compensate for the lower-than-expected reward with part of the insurance deposit.\n>>>> >>>>>> If the price doesn't go down enough, the miner just mines Bitcoin and the insurer gets his deposit back.\n>>>> >>>>>> It's basically an instrument for guaranteeing a minimum profitability of the mining operation.\n>>>> >>>>>> Implementation issues: unfortunately we can't do arithmetic comparison with long integers >32bit in the script, so implementation of the difficulty requirement needs to be hacky. I think we can use the hashes of one or more pre-images with a given short length, and the miner has to provide the exact pre-images. The pre-images are chosen by the insurer, and we would need a \"honesty\" deposit or other mechanism to punish the insurer if he chooses a hash that doesn't correspond to any short-length pre-image. I'm not sure about this implementation though, maybe we actually need new opcodes.\n>>>> >>>>>> What do you guys think?\n>>>> >>>>>> Thanks for reading it all! Hope it was worth your time!\n>>>> >>>>> \n>>>> >>>>> bitcoin-dev mailing list\n>>>> >>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> >>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>> >>>> \n>>>> >>>> bitcoin-dev mailing list\n>>>> >>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> >>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>> > \n>>>> >\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191020/5b027844/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Trustless hash-price insurance contracts",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "JW Weatherman",
                "Lucas H",
                "Eric Voskuil"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 77878
        }
    },
    {
        "title": "[bitcoin-dev] Draft BIP for SNICKER",
        "thread_messages": [
            {
                "author": "SomberNight",
                "date": "2019-10-20T00:29:25",
                "message_text_only": "Hi all,\n\nwaxwing, ThomasV, and I recently had a discussion about implementing SNICKER in Electrum; specifically the \"Receiver\" role. To me, SNICKER is an interesting proposal, due to the non-interactivity and because it seems it would be easy to implement the \"Receiver\" role in a light wallet. If enough users are using wallets that implement the \"Receiver\" role, even if full nodes and specialised scripts are needed to run SNICKER as a \"Proposer\", then coinjoins via SNICKER could become fairly frequent on-chain, benefitting the whole ecosystem indirectly by breaking common chain-analysis assumptions even further.\n\nThe BIP (draft) describes how the Receiver can deterministically find all his outputs and reconstruct all corresponding private keys, just from the seed words and the blockchain.\nHowever what is not explicitly pointed out, and what I would like to point out in this mail, is that SNICKER breaks watch-only functionality.\n\nSee \"Receiver actions\" > \"Storage of Keys\" section (\"Re-derive from blockchain history\"). [0]\n\nSpecifically, the output address in the SNICKER transaction that pays to the \"Receiver\", is constructed from the pubkey `P_A + cG`, where `P_A` is a pubkey of \"Receiver\" (typically a leaf pubkey along some BIP32 path), and `c` is a tweak. This tweak was constructed such that `c = ECDH(Q, P_A)`, where `Q` is a pubkey of the \"Proposer\" that appears in the witness of the SNICKER tx.\n\nAs the referenced section [0] explains, the \"Receiver\" can restore from seed, and assuming he knows he needs to do extra scanning steps (e.g. via a seed version that signals SNICKER support), he can find and regain access to his SNICKER outputs. However, to calculate `c` he needs access to his private keys, as it is the ECDH of one of the Receiver's pubkeys and one of the Proposer's pubkeys.\n\nThis means the proposed scheme is fundamentally incompatible with watch-only wallets.\nNowadays many users expect being able to watch their addresses from an unsecure machine, or to be able to offline sign transactions. In the case of Electrum specifically, Electrum Personal Server (EPS) is also using xpubs to function. We've been exposing users to xpubs since the initial BIP32 implementation (and even predating BIP32, in the legacy Electrum HD scheme, there were already \"master public keys\").\n\nIt would seem that if we implemented SNICKER, users would have to make a choice, most likely during wallet creation time, whether they want to be able to use xpubs or to potentially participate in SNICKER coinjoins as a \"Receiver\" (and then encode the choice in the seed version). This choice seems rather difficult to communicate to users. Further, if SNICKER is not supported by the default choice then it is less likely to take off and hence less useful for the user; OTOH if xpubs are not supported by the default choice then existing user expectations are broken.\n\n(Note that I am using a loosened definition of xpub here. The pubkeys in SNICKER tx output scripts are not along any BIP32 derivation. The point here is whether they could somehow be identified deterministically without access to secret material.)\n\nUnfortunately it is not clear how the SNICKER scheme could be adjusted to \"fix\" this. Note that `c` needs to be known exactly by the two coinjoin-participants and no-one else; otherwise the anonymity set (of 2) is broken as:\n- which SNICKER output corresponds to the tweaked public key and hence to the Receiver, can then be identified (as soon as the output is spent and the pubkey appears on-chain), and\n- using subset-sum analysis the inputs and the outputs can be linked\nSNICKER assumes almost no communication between the two parties, so it seems difficult to find a sufficient construction for `c` such that it can be recreated by the Receiver if he only has an xpub (and access to the blockchain) as all pubkeys from the xpub that the Proposer would have access to are already public information visible on-chain.\n\nghost43\n\n\n[0] https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#Storage_of_Keys\n\n\n\n> Hello list,\n> Here is a link for a draft of a BIP for a different type of CoinJoin I've named 'SNICKER' = Simple Non-Interactive Coinjoin with Keys for Encryption Reused.\n>\n> https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79\n>\n> Purpose of writing this as a BIP:\n> There was some discussion on the Wasabi repo about this recently (https://github.com/zkSNACKs/Meta/issues/67) and it prompted me to do something I should have done way back when I came up with the idea in late '17: write a technical specification, because one of the main attractive points about this is that it isn't a hugely difficult task for a wallet developer to implement (especially: Receiver side), and it would only really have value if wallet developers did indeed implement it. To be specific, it requires ECDH (which is already available in libsecp256k1 anyway) and ECIES which is pretty easy to do (just ecdh and hmac, kinda).\n>\n> Plenty of uncertainty on the specs, in particular the specification for transactions, e.g. see 'Partially signed transactions' subsection, point 3). Also perhaps the encryption specs. I went with the exact algo already used by Electrum here, but it could be considered dubious (CBC).\n>\n> Thanks for any feedback.\n>\n> Adam Gibson / waxwing"
            },
            {
                "author": "David A. Harding",
                "date": "2019-10-21T00:06:08",
                "message_text_only": "On Sun, Oct 20, 2019 at 12:29:25AM +0000, SomberNight via bitcoin-dev wrote:\n> waxwing, ThomasV, and I recently had a discussion about implementing\n> SNICKER in Electrum; specifically the \"Receiver\" role. \n\nThat'd be awesome!\n\n> As the referenced section [0] explains, the \"Receiver\" can restore\n> from seed, and assuming he knows he needs to do extra scanning steps\n> (e.g. via a seed version that signals SNICKER support), he can find\n> and regain access to his SNICKER outputs. However, to calculate `c` he\n> needs access to his private keys, as it is the ECDH of one of the\n> Receiver's pubkeys and one of the Proposer's pubkeys.\n> \n> This means the proposed scheme is fundamentally incompatible with\n> watch-only wallets.\n> \n> [0] https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#Storage_of_Keys\n\nYour logic seems correct for the watching half of the wallet, but I\nthink it's ok to consider requiring interaction with the cold wallet.\nLet's look at the recovery procedure from the SNICKER documentation\nthat you kindly cited:\n\n    1. Derive all regular addresses normally (doable watch-only for\n    wallets using public BIP32 derivation)\n\n    2. Find all transactions spending an output for each of those\n    addresses.  Determine whether the spend looks like a SNICKER\n    coinjoin (e.g. \"two equal-[value] outputs\").  (doable watch-only)\n    \n    3. \"For each of those transactions, check, for each of the two equal\n    sized outputs, whether one destination address can be regenerated\n    from by taking c found in the method described above\" (not doable\n    watch only; requires private keys)\n\nI'd expect the set of candidate transactions produced in step #2 to be\npretty small and probably with no false positives for users not\nparticipating in SNICKER coinjoins or doing lots of payment batching.\nThat means, if any SNICKER candidates were found by a watch-only wallet,\nthey could be compactly bundled up and the user could be encouraged to\ncopy them to the corresponding cold wallet using the same means used for\nPSBTs (e.g. USB drive, QR codes, etc).  You wouldn't even need the whole\ntransactions, just the BIP32 index of the user's key, the pubkey of the\nsuspected proposer, and a checksum of the resultant address.\n\nThe cold wallet could then perform step #3 using its private keys and\nreturn a file/QRcode/whatever to the hot wallet telling it any shared\nsecrets it found.\n\nThis process may need to be repeated several times if an output created\nby one SNICKER round is spent in a subsequent SNICKER round.  This can be\naddressed by simply refusing to participate in chains of SNICKER\ntransactions or by refusing to participant in chains of SNICKERs more\nthan n long (requring a maximum n rounds of recovery).  It could also be\naddressed by the watching-only wallet looking ahead at the block chain a\nbit in order to grab SNICKER-like child and grandchild transactions of\nour SNICKER candidates and sending them also to the cold wallet for\nattempted shared secret recovery.\n\nThe SNICKER recovery process is, of course, only required for wallet\nrecovery and not normal wallet use, so I don't think a small amount of\nround-trip communication between the hot wallet and the cold wallet is\ntoo much to ask---especially since anyone using SNICKER with a\nwatching-only wallet must be regularly interacting with their cold\nwallet anyway to sign the coinjoins.\n\n-Dave"
            },
            {
                "author": "Riccardo Casatta",
                "date": "2019-10-21T11:00:26",
                "message_text_only": "The \"Receiver\" could immediately create a tx that spend the coinjoin\noutputs to bip32 keys,\nThe hard part is that he had to delay the broadcast otherwise he loose\nprivacy\n\nIl giorno lun 21 ott 2019 alle ore 02:08 David A. Harding via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> ha scritto:\n\n> On Sun, Oct 20, 2019 at 12:29:25AM +0000, SomberNight via bitcoin-dev\n> wrote:\n> > waxwing, ThomasV, and I recently had a discussion about implementing\n> > SNICKER in Electrum; specifically the \"Receiver\" role.\n>\n> That'd be awesome!\n>\n> > As the referenced section [0] explains, the \"Receiver\" can restore\n> > from seed, and assuming he knows he needs to do extra scanning steps\n> > (e.g. via a seed version that signals SNICKER support), he can find\n> > and regain access to his SNICKER outputs. However, to calculate `c` he\n> > needs access to his private keys, as it is the ECDH of one of the\n> > Receiver's pubkeys and one of the Proposer's pubkeys.\n> >\n> > This means the proposed scheme is fundamentally incompatible with\n> > watch-only wallets.\n> >\n> > [0]\n> https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#Storage_of_Keys\n>\n> Your logic seems correct for the watching half of the wallet, but I\n> think it's ok to consider requiring interaction with the cold wallet.\n> Let's look at the recovery procedure from the SNICKER documentation\n> that you kindly cited:\n>\n>     1. Derive all regular addresses normally (doable watch-only for\n>     wallets using public BIP32 derivation)\n>\n>     2. Find all transactions spending an output for each of those\n>     addresses.  Determine whether the spend looks like a SNICKER\n>     coinjoin (e.g. \"two equal-[value] outputs\").  (doable watch-only)\n>\n>     3. \"For each of those transactions, check, for each of the two equal\n>     sized outputs, whether one destination address can be regenerated\n>     from by taking c found in the method described above\" (not doable\n>     watch only; requires private keys)\n>\n> I'd expect the set of candidate transactions produced in step #2 to be\n> pretty small and probably with no false positives for users not\n> participating in SNICKER coinjoins or doing lots of payment batching.\n> That means, if any SNICKER candidates were found by a watch-only wallet,\n> they could be compactly bundled up and the user could be encouraged to\n> copy them to the corresponding cold wallet using the same means used for\n> PSBTs (e.g. USB drive, QR codes, etc).  You wouldn't even need the whole\n> transactions, just the BIP32 index of the user's key, the pubkey of the\n> suspected proposer, and a checksum of the resultant address.\n>\n> The cold wallet could then perform step #3 using its private keys and\n> return a file/QRcode/whatever to the hot wallet telling it any shared\n> secrets it found.\n>\n> This process may need to be repeated several times if an output created\n> by one SNICKER round is spent in a subsequent SNICKER round.  This can be\n> addressed by simply refusing to participate in chains of SNICKER\n> transactions or by refusing to participant in chains of SNICKERs more\n> than n long (requring a maximum n rounds of recovery).  It could also be\n> addressed by the watching-only wallet looking ahead at the block chain a\n> bit in order to grab SNICKER-like child and grandchild transactions of\n> our SNICKER candidates and sending them also to the cold wallet for\n> attempted shared secret recovery.\n>\n> The SNICKER recovery process is, of course, only required for wallet\n> recovery and not normal wallet use, so I don't think a small amount of\n> round-trip communication between the hot wallet and the cold wallet is\n> too much to ask---especially since anyone using SNICKER with a\n> watching-only wallet must be regularly interacting with their cold\n> wallet anyway to sign the coinjoins.\n>\n> -Dave\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nRiccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191021/51924e75/attachment.html>"
            },
            {
                "author": "SomberNight",
                "date": "2019-10-21T15:04:59",
                "message_text_only": "> The SNICKER recovery process is, of course, only required for wallet\nrecovery and not normal wallet use, so I don't think a small amount of\nround-trip communication between the hot wallet and the cold wallet is\ntoo much to ask---especially since anyone using SNICKER with a\nwatching-only wallet must be regularly interacting with their cold\nwallet anyway to sign the coinjoins.\n\nWhat you described only considers the \"initial setup\" of a watch-only wallet. There are many usecases for watch-only wallets. There doesn't even necessarily need to be any offline-signing involved. For example, consider a user who has a hot wallet on their laptop with xprv; and wants to watch their addresses using an xpub from their mobile. Or consider giving an xpub to an accountant. Or giving an xpub to your Electrum Personal Server (which is how it works).\n\nNote that all these usecases require \"on-going\" discovery of addresses, and so they would break.\n\nghost43\n\n(ps: Apologies Dave for the double-email; forgot to cc list originally)"
            },
            {
                "author": "AdamISZ",
                "date": "2019-10-22T13:21:00",
                "message_text_only": "Just to chime in on these points:\n\nMy discussions with ghost43 and ThomasV led me to the same conclusion, at least in general, for the whole watch-only issue:\n\nIt's necessary that the key tweak (`c` as per draft BIP) be known by Proposer (because has to add it to transaction before signing) and Receiver (to check ownership), but must not be known by anyone else (else Coinjoin function fails), hence it can't be publically derivable in any way but must require information secret to the two parties. This can be a pure random sent along with the encrypted proposal (the original concept), or based on such, or implicit via ECDH (arubi's suggestion, now in the draft, requiring each party to access their own secret key). So I reached the same conclusion: the classic watch-only use case of monitoring a wallet in real time with no privkey access is incompatible with this.\n\nIt's worth mentioning a nuance, however: distinguish two requirements: (1) to recover from zero information and (2) to monitor in real time as new SNICKER transactions arrive.\n\nFor (2) it's interesting to observe that the tweak `c` is not a money-controlling secret; it's only a privacy-controlling secret. If you imagined two wallets, one hot and one cold, with the second tracking the first but having a lower security requirement because cold, then the `c` values could be sent along from the hot to the cold, as they are created, without changing the cold's security model as they are not money-controlling private keys. They should still be encrypted of course, but that's largely a technical detail, if they were exposed it would only break the effect of the coinjoin outputs being indistinguishable.\n\nFor (1) the above does not apply; for there, we don't have anyone telling us what `c` values to look for, we have to somehow rederive, and to do that we need key access, so it reverts to the discussion above about whether it might be possible to interact with the cold wallet 'manually' so to speak.\n\nTo be clear, I don't think either of the above paragraphs describe things that are particularly likely to be implemented, but the hot/cold monitoring is at least feasible, if there were enough desire for it.\n\nAt the higher level, how important is this? I guess it just depends; there are similar problems (not identical, and perhaps more addressable?) in Lightning; importing keys is generally non-trivial; one can always sweep non-standard keys back into the HD tree, but clearly that is not really a solution in general; one can mark out wallets/seeds of this type as distinct; not all wallets need to have watch-only (phone wallets? small wallets? lower security?) one can prioritise spends of these coins. Etc.\n\nSome more general comments:\n\nNote Elichai's comment on the draft (repeated here for local convenience: https://gist.github.com/AdamISZ/2c13fb5819bd469ca318156e2cf25d79#gistcomment-3014924) about AES-GCM vs AES-CBC, any thoughts?\n\nI didn't discuss the security of the construction for a Receiver from a Proposer who should after all be assumed to be an attacker (except, I emphasised that PSBT parsing could be sensitive on this point); I hope it's clear to everyone that the construction Q = P + cG is only controllable by the owner of the discrete log of P (trivial reduction: if an attacker who knows c, can find the private key q of Q, he can derive the private key p of P as q - c, thus he is an ECDLP cracker).\n\nThanks for all the comments so far, it's been very useful.\n\nAdamISZ/waxwing/Adam Gibson\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Monday, October 21, 2019 4:04 PM, SomberNight via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > The SNICKER recovery process is, of course, only required for wallet\n>\n> recovery and not normal wallet use, so I don't think a small amount of\n> round-trip communication between the hot wallet and the cold wallet is\n> too much to ask---especially since anyone using SNICKER with a\n> watching-only wallet must be regularly interacting with their cold\n> wallet anyway to sign the coinjoins.\n>\n> What you described only considers the \"initial setup\" of a watch-only wallet. There are many usecases for watch-only wallets. There doesn't even necessarily need to be any offline-signing involved. For example, consider a user who has a hot wallet on their laptop with xprv; and wants to watch their addresses using an xpub from their mobile. Or consider giving an xpub to an accountant. Or giving an xpub to your Electrum Personal Server (which is how it works).\n>\n> Note that all these usecases require \"on-going\" discovery of addresses, and so they would break.\n>\n> ghost43\n>\n> (ps: Apologies Dave for the double-email; forgot to cc list originally)\n>\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Draft BIP for SNICKER",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "AdamISZ",
                "Riccardo Casatta",
                "David A. Harding",
                "SomberNight"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 18866
        }
    },
    {
        "title": "[bitcoin-dev] Sign up for Taproot BIP review by October 30",
        "thread_messages": [
            {
                "author": "Steve Lee",
                "date": "2019-10-23T17:44:49",
                "message_text_only": "Hello everyone,\n\nThe schnorr/taproot/tapscript BIPs are ready for review at this point, and\nwe want to get as much in-depth review from as broad a range of people as\nwe can before we go further on implementation/deployment. Reviewing the\nBIPs is hard in two ways: not many people are familiar with reviewing BIPs\nin the first place, and there are a lot of concepts involved in the three\nBIPs for people to get their heads around.\n\nThis is a proposal <https://github.com/ajtowns/taproot-review> for a\nstructured review period. The idea is that participants will be given some\nguidance/structure for going through the BIPs, and at the end should be\nable to either describe issues with the BIP drafts that warrant changes, or\nbe confident that they\u2019ve examined the proposals thoroughly enough to give\nan \u201cACK\u201d that the drafts should be formalised and move forwards into\nimplementation/deployment phases.\n\nBenefits of participating:\n* Deeply understand schnorr and taproot\n* Be a stakeholder in Bitcoin consensus development\n* Support/safeguard decentralisation of Bitcoin protocol development\n* Have fun!\n\nIf you are interested in participating, please sign up here\n<https://forms.gle/stnJvHQHREdkaxkS8 >.\n\nSpecial thanks to AJ Towns for doing most of the heavy lifting to get this\ngoing!\n\nSteve\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191023/e864c58e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Sign up for Taproot BIP review by October 30",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Steve Lee"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1475
        }
    },
    {
        "title": "[bitcoin-dev] Signaling support for addr relay (revision #1)",
        "thread_messages": [
            {
                "author": "Gleb Naumenko",
                "date": "2019-10-23T21:52:49",
                "message_text_only": "Hi,\n\n### Introduction\n\nI was recently looking into AddrMan and I realized that unlike with blocks (BIP152) and transactions (a node can opt-out via various mechanisms such as blocks-only or block-only-relay), address relay is under-specified.\n\nFor example, we had a discussion [1] on whether SPV nodes store/relay IP addresses. While it seems they don\u2019t do it currently in practice, in some cases they should if they want to be secure and reliable.\n\n### Motivation\n\nThis change would decouple addr relay considerations from light/full node/block-relay-only.\nThis would also allow us to easier analyze (in a scientific sense, not in a spying sense) and adjust address relay, which currently seems to have understudied properties and guarantees.\nIn practice, this may allow more efficient address relay (fewer messages and less time to relay a new address across all nodes) both immediately and potentially long-term.\n\n### Solution\n\nI want to suggest making explicit whether a node promises to participate in address relay by a) forwarding unsolicited messages (I work on a somewhat related issue in this PR [2]) , and, b) responding to GETADDR.\n\nIn my opinion, these 2 signals (a and b) should be viewed independently.\n\nObviously, these signals should not be relied upon and future protocol changes should assume they may represent lies.\nHowever, explicitly opting-out of relay addresses will help to improve non-adversarial address relay.\n\n### Implementation\n\nI see 2 ways to implement this:\n- 2 new service bits\n- per-link-direction negotiation: e.g., use BIP-155 (a new message sendaddrv2 is discussed here [3] and can be used to signal this)\n\nBoth of them can allow decoupling addr relay from node type, but they do have different trade-offs.\n\n#### Service bits\n\nHaving service bits makes sense only if nodes are going to make peering decisions based on it. (everything else might be achieved without introducing a service bit). It is not clear to me whether this makes sense in this context.\n\nThe fundamental problem with service bits is that they make a uniform \u201cpromise\u201d for all connections to a given node. E.g., if node X announces NODE_ADDR_FORWARD, all nodes in the network expect node X to forward addresses. (If the \u201cpromise\u201d is not strong, then additional negotiation is required anyway, so service bits do not solve the problem).\n\nIt\u2019s worth keeping in mind that all of the honest reachable full nodes nodes DO relay addresses, and we already won\u2019t connect to those nodes which don\u2019t (light clients). Service bits won\u2019t help here because the problem of connecting to non-addr-relaying full nodes does not exist.\nMaybe, if we think that a large fraction of reachable nodes might start completely disabling addr relay to all in the future, then it makes sense to have this service bit, to prevent nodes from accidentally connecting to these peers only and not learning addrs.\n\nIntuitively, it\u2019s also easier to shoot in the leg with the deployment of service bits (might make it easier for attacker to accumulate connections comparing to the case of victims choosing their peers uniformly at random without considering new service bit).\n\n#### Per-link-direction negotiation\n\nThis approach does not have the shortcomings mentioned above.\n\nIn addition, I think that having more flexibility (Per-link-direction negotiation) is better for the future of the protocol, where some nodes might want to opt-out of addr relay for a subset of their links.\n(A node might want to opt-out from addr relay for particular links due to privacy reasons because addr-relay currently leaks information and maybe we shouldn\u2019t relay transactions through the same links).\n\nAnd I think this future is much more likely to happen than a future where a significant fraction of reachable nodes disable addr relay to *everyone* and need to announce this to the network. Also, even if needed, this can be done with per-link-direction negotiation too, and handled by the peers accordingly.\n\nPer-link-direction negotiation also allows to decouple the behaviour from inbound/outbound type of connection (currently we do not respond to GETADDR from outbound). This logic seems not fundamental to me, but rather a temporary heuristic to prevent attacks, which might be changed in future.\n\n### Conclusion\n\nI think the solution fundamentally depends on the answer to:\n\u201cDo we believe that some of the future security advices for node operators would be to disable address relay to all (or most) of the links\u201d.\n\nIf yes, I think we should use service bits.\nIf no, I think we should use per-link-direction negotiation.\n\nIf the answer will change, we can also add a service bit later.\n\nAnyway, according to the current considerations I explained in this email, I\u2019d suggest extending BIP-155 with per-link-direction negotiation, but I\u2019m interested in the opinion of the community.\n\n### References\n\n1. Bitcoin core dev IRC meeting (http://www.erisian.com.au/bitcoin-core-dev/log-2019-10-17.html)\n2. p2p: Avoid forwarding ADDR messages to SPV nodes (https://github.com/bitcoin/bitcoin/pull/17194)\n3. BIP 155: addrv2 BIP proposal (https://github.com/bitcoin/bips/pull/766)\n\n\u2013 gleb\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191023/55c63626/attachment.html>"
            },
            {
                "author": "Joachim Str\u00f6mbergson",
                "date": "2019-10-24T08:01:49",
                "message_text_only": "> Anyway, according to the current considerations I explained in this email, I\u2019d suggest extending BIP-155 with per-link-direction negotiation, but I\u2019m interested in the opinion of the community.\n\nI don't have a strong opinion here but intuitively, it seems to me that per-link variant makes better sense as I currently can't imagine the future requirement for completely opting out.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191024/d08d6f66/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Signaling support for addr relay (revision #1)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Gleb Naumenko",
                "Joachim Str\u00f6mbergson"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5894
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction Issues in Contracting Applications (eg Lightning)",
        "thread_messages": [
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-24T13:49:09",
                "message_text_only": "Reviving this old thread now that the recently released RC for bitcoind\n0.19 includes the above mentioned carve-out rule.\n\nIn an attempt to pave the way for more robust CPFP of on-chain contracts\n(Lightning commitment transactions), the carve-out rule was added in\nhttps://github.com/bitcoin/bitcoin/pull/15681. However, having worked on an\nimplementation of a new commitment format for utilizing the Bring Your Own\nFees strategy using CPFP, I\u2019m wondering if the special case rule should\nhave been relaxed a bit, to avoid the need for adding a 1 CSV to all\noutputs (in case of Lightning this means HTLC scripts would need to be\nchanged to add the CSV delay).\n\nInstead, what about letting the rule be\n\nThe last transaction which is added to a package of dependent\ntransactions in the mempool must:\n  * Have no more than one unconfirmed parent.\n\nThis would of course allow adding a large transaction to each output of the\nunconfirmed parent, which in effect would allow an attacker to exceed the\nMAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is this a problem\nwith the current mempool acceptance code in bitcoind? I would imagine\nevicting transactions based on feerate when the max mempool size is met\nhandles this, but I\u2019m asking since it seems like there has been several\nchanges to the acceptance code and eviction policy since the limit was\nfirst introduced.\n\n- Johan\n\n\nOn Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au> wrote:\n\n> Matt Corallo <lf-lists at mattcorallo.com> writes:\n> >>> Thus, even if you imagine a steady-state mempool growth, unless the\n> >>> \"near the top of the mempool\" criteria is \"near the top of the next\n> >>> block\" (which is obviously *not* incentive-compatible)\n> >>\n> >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie. next\n> >> block, and assumed you'd only allow RBF if the old package wasn't in the\n> >> top and the replacement would be.  That seems incentive compatible; more\n> >> than the current scheme?\n> >\n> > My point was, because of block time variance, even that criteria doesn't\n> hold up. If you assume a steady flow of new transactions and one or two\n> blocks come in \"late\", suddenly \"top 4MWeight\" isn't likely to get\n> confirmed until a few blocks come in \"early\". Given block variance within a\n> 12 block window, this is a relatively likely scenario.\n>\n> [ Digging through old mail. ]\n>\n> Doesn't really matter.  Lightning close algorithm would be:\n>\n> 1.  Give bitcoind unileratal close.\n> 2.  Ask bitcoind what current expidited fee is (or survey your mempool).\n> 3.  Give bitcoind child \"push\" tx at that total feerate.\n> 4.  If next block doesn't contain unilateral close tx, goto 2.\n>\n> In this case, if you allow a simpified RBF where 'you can replace if\n> 1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3. old tx\n> isnt',\n> it works.\n>\n> It allows someone 100k of free tx spam, sure.  But it's simple.\n>\n> We could further restrict it by marking the unilateral close somehow to\n> say \"gonna be pushed\" and further limiting the child tx weight (say,\n> 5kSipa?) in that case.\n>\n> Cheers,\n> Rusty.\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191024/065e650f/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2019-10-24T21:25:14",
                "message_text_only": "I may be missing something, but I'm not sure how this changes anything?\n\nIf you have a commitment transaction, you always need at least, and\nexactly, one non-CSV output per party. The fact that there is a size\nlimitation on the transaction that spends for carve-out purposes only\neffects how many other inputs/outputs you can add, but somehow I doubt\nits ever going to be a large enough number to matter.\n\nMatt\n\nOn 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n> Reviving this old thread now that the recently released RC for bitcoind\n> 0.19 includes the above mentioned carve-out rule.\n> \n> In an attempt to pave the way for more robust CPFP of on-chain contracts\n> (Lightning commitment transactions), the carve-out rule was added in\n> https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on\n> an implementation of a new commitment format for utilizing the Bring\n> Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n> rule should have been relaxed a bit, to avoid the need for adding a 1\n> CSV to all outputs (in case of Lightning this means HTLC scripts would\n> need to be changed to add the CSV delay).\n> \n> Instead, what about letting the rule be\n> \n> The last transaction which is added to a package of dependent\n> transactions in the mempool must:\n> \u00a0 * Have no more than one unconfirmed parent.\n> \n> This would of course allow adding a large transaction to each output of\n> the unconfirmed parent, which in effect would allow an attacker to\n> exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n> this a problem with the current mempool acceptance code in bitcoind? I\n> would imagine evicting transactions based on feerate when the max\n> mempool size is met handles this, but I\u2019m asking since it seems like\n> there has been several changes to the acceptance code and eviction\n> policy since the limit was first introduced.\n> \n> - Johan\n> \n> \n> On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n> <mailto:rusty at rustcorp.com.au>> wrote:\n> \n>     Matt Corallo <lf-lists at mattcorallo.com\n>     <mailto:lf-lists at mattcorallo.com>> writes:\n>     >>> Thus, even if you imagine a steady-state mempool growth, unless the\n>     >>> \"near the top of the mempool\" criteria is \"near the top of the next\n>     >>> block\" (which is obviously *not* incentive-compatible)\n>     >>\n>     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie. next\n>     >> block, and assumed you'd only allow RBF if the old package wasn't\n>     in the\n>     >> top and the replacement would be.\u00a0 That seems incentive\n>     compatible; more\n>     >> than the current scheme?\n>     >\n>     > My point was, because of block time variance, even that criteria\n>     doesn't hold up. If you assume a steady flow of new transactions and\n>     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>     likely to get confirmed until a few blocks come in \"early\". Given\n>     block variance within a 12 block window, this is a relatively likely\n>     scenario.\n> \n>     [ Digging through old mail. ]\n> \n>     Doesn't really matter.\u00a0 Lightning close algorithm would be:\n> \n>     1.\u00a0 Give bitcoind unileratal close.\n>     2.\u00a0 Ask bitcoind what current expidited fee is (or survey your mempool).\n>     3.\u00a0 Give bitcoind child \"push\" tx at that total feerate.\n>     4.\u00a0 If next block doesn't contain unilateral close tx, goto 2.\n> \n>     In this case, if you allow a simpified RBF where 'you can replace if\n>     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>     old tx isnt',\n>     it works.\n> \n>     It allows someone 100k of free tx spam, sure.\u00a0 But it's simple.\n> \n>     We could further restrict it by marking the unilateral close somehow to\n>     say \"gonna be pushed\" and further limiting the child tx weight (say,\n>     5kSipa?) in that case.\n> \n>     Cheers,\n>     Rusty.\n>     _______________________________________________\n>     Lightning-dev mailing list\n>     Lightning-dev at lists.linuxfoundation.org\n>     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-25T07:05:15",
                "message_text_only": "It essentially changes the rule to always allow CPFP-ing the commitment as\nlong as there is an output available without any descendants. It changes\nthe commitment from \"you always need at least, and exactly, one non-CSV\noutput per party. \" to \"you always need at least one non-CSV output per\nparty. \"\n\nI realize these limits are there for a reason though, but I'm wondering if\ncould relax them. Also now that jeremyrubin has expressed problems with the\ncurrent mempool limits.\n\nOn Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> I may be missing something, but I'm not sure how this changes anything?\n>\n> If you have a commitment transaction, you always need at least, and\n> exactly, one non-CSV output per party. The fact that there is a size\n> limitation on the transaction that spends for carve-out purposes only\n> effects how many other inputs/outputs you can add, but somehow I doubt\n> its ever going to be a large enough number to matter.\n>\n> Matt\n>\n> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n> > Reviving this old thread now that the recently released RC for bitcoind\n> > 0.19 includes the above mentioned carve-out rule.\n> >\n> > In an attempt to pave the way for more robust CPFP of on-chain contracts\n> > (Lightning commitment transactions), the carve-out rule was added in\n> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on\n> > an implementation of a new commitment format for utilizing the Bring\n> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n> > rule should have been relaxed a bit, to avoid the need for adding a 1\n> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n> > need to be changed to add the CSV delay).\n> >\n> > Instead, what about letting the rule be\n> >\n> > The last transaction which is added to a package of dependent\n> > transactions in the mempool must:\n> >   * Have no more than one unconfirmed parent.\n> >\n> > This would of course allow adding a large transaction to each output of\n> > the unconfirmed parent, which in effect would allow an attacker to\n> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n> > this a problem with the current mempool acceptance code in bitcoind? I\n> > would imagine evicting transactions based on feerate when the max\n> > mempool size is met handles this, but I\u2019m asking since it seems like\n> > there has been several changes to the acceptance code and eviction\n> > policy since the limit was first introduced.\n> >\n> > - Johan\n> >\n> >\n> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n> > <mailto:rusty at rustcorp.com.au>> wrote:\n> >\n> >     Matt Corallo <lf-lists at mattcorallo.com\n> >     <mailto:lf-lists at mattcorallo.com>> writes:\n> >     >>> Thus, even if you imagine a steady-state mempool growth, unless\n> the\n> >     >>> \"near the top of the mempool\" criteria is \"near the top of the\n> next\n> >     >>> block\" (which is obviously *not* incentive-compatible)\n> >     >>\n> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie.\n> next\n> >     >> block, and assumed you'd only allow RBF if the old package wasn't\n> >     in the\n> >     >> top and the replacement would be.  That seems incentive\n> >     compatible; more\n> >     >> than the current scheme?\n> >     >\n> >     > My point was, because of block time variance, even that criteria\n> >     doesn't hold up. If you assume a steady flow of new transactions and\n> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n> >     likely to get confirmed until a few blocks come in \"early\". Given\n> >     block variance within a 12 block window, this is a relatively likely\n> >     scenario.\n> >\n> >     [ Digging through old mail. ]\n> >\n> >     Doesn't really matter.  Lightning close algorithm would be:\n> >\n> >     1.  Give bitcoind unileratal close.\n> >     2.  Ask bitcoind what current expidited fee is (or survey your\n> mempool).\n> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n> >\n> >     In this case, if you allow a simpified RBF where 'you can replace if\n> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n> >     old tx isnt',\n> >     it works.\n> >\n> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n> >\n> >     We could further restrict it by marking the unilateral close somehow\n> to\n> >     say \"gonna be pushed\" and further limiting the child tx weight (say,\n> >     5kSipa?) in that case.\n> >\n> >     Cheers,\n> >     Rusty.\n> >     _______________________________________________\n> >     Lightning-dev mailing list\n> >     Lightning-dev at lists.linuxfoundation.org\n> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191025/58a3d7b8/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2019-10-27T22:54:02",
                "message_text_only": "On Thu, Oct 24, 2019 at 03:49:09PM +0200, Johan Tor\u00e5s Halseth wrote:\n> [...] what about letting the rule be\n> \n> The last transaction which is added to a package of dependent\n> transactions in the mempool must:\n>   * Have no more than one unconfirmed parent.\n> [... subsequent email ...]\n> I realize these limits are there for a reason though, but I'm wondering if\n> we could relax them.\n\nJohan,\n\nI'm not sure any of the other replies to this thread addressed your\nrequest for a reason behind the limits related to your proposal, so I\nthought I'd point out that---subsequent to your posting here---a\ndocument[1] was added to the Bitcoin Core developer wiki that I think\ndescribes the risk of the approach you proposed:\n\n> Free relay attack:\n>\n> - Create a low feerate transaction T.\n>\n> - Send zillions of child transactions that are slightly higher feerate\n>   than T until mempool is full.\n>\n> - Create one small transaction with feerate just higher than T\u2019s, and\n>    watch T and all its children get evicted. Total fees in mempool drops\n>    dramatically!\n>\n> - Attacker just relayed (say) 300MB of data across the whole network\n>   but only pays small feerate on one small transaction.\n\nThe document goes on to describe at a high level how Bitcoin Core\nattempts to mitigate this problem as well as other ways it tries to\noptimize the mempool in order to maximize miner profit (and so ensure\nthat miners continue to use public transaction relay).\n\nI hope that's helpful to you and to others in both understanding the\ncurrent state and in thinking about ways in which it might be improved.\n\n-Dave\n\n[1] https://github.com/bitcoin-core/bitcoin-devwiki/wiki/Mempool-and-mining\n    Content adapted from slides by Suhas Daftuar, uploaded and formatted\n    by Gregory Sanders and Marco Falke."
            },
            {
                "author": "Matt Corallo",
                "date": "2019-10-25T17:30:41",
                "message_text_only": "I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now they stuff the package size on one of their spendable outlets until it is right at the limit, add one more on their other output (to meet the Carve-Out), and now Party B can\u2019t do anything.\n\n> On Oct 24, 2019, at 21:05, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n> \n> \ufeff\n> It essentially changes the rule to always allow CPFP-ing the commitment as long as there is an output available without any descendants. It changes the commitment from \"you always need at least, and exactly, one non-CSV output per party. \" to \"you always need at least one non-CSV output per party. \"\n> \n> I realize these limits are there for a reason though, but I'm wondering if could relax them. Also now that jeremyrubin has expressed problems with the current mempool limits.\n> \n>> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com> wrote:\n>> I may be missing something, but I'm not sure how this changes anything?\n>> \n>> If you have a commitment transaction, you always need at least, and\n>> exactly, one non-CSV output per party. The fact that there is a size\n>> limitation on the transaction that spends for carve-out purposes only\n>> effects how many other inputs/outputs you can add, but somehow I doubt\n>> its ever going to be a large enough number to matter.\n>> \n>> Matt\n>> \n>> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n>> > Reviving this old thread now that the recently released RC for bitcoind\n>> > 0.19 includes the above mentioned carve-out rule.\n>> > \n>> > In an attempt to pave the way for more robust CPFP of on-chain contracts\n>> > (Lightning commitment transactions), the carve-out rule was added in\n>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked on\n>> > an implementation of a new commitment format for utilizing the Bring\n>> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n>> > rule should have been relaxed a bit, to avoid the need for adding a 1\n>> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n>> > need to be changed to add the CSV delay).\n>> > \n>> > Instead, what about letting the rule be\n>> > \n>> > The last transaction which is added to a package of dependent\n>> > transactions in the mempool must:\n>> >   * Have no more than one unconfirmed parent.\n>> > \n>> > This would of course allow adding a large transaction to each output of\n>> > the unconfirmed parent, which in effect would allow an attacker to\n>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n>> > this a problem with the current mempool acceptance code in bitcoind? I\n>> > would imagine evicting transactions based on feerate when the max\n>> > mempool size is met handles this, but I\u2019m asking since it seems like\n>> > there has been several changes to the acceptance code and eviction\n>> > policy since the limit was first introduced.\n>> > \n>> > - Johan\n>> > \n>> > \n>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n>> > <mailto:rusty at rustcorp.com.au>> wrote:\n>> > \n>> >     Matt Corallo <lf-lists at mattcorallo.com\n>> >     <mailto:lf-lists at mattcorallo.com>> writes:\n>> >     >>> Thus, even if you imagine a steady-state mempool growth, unless the\n>> >     >>> \"near the top of the mempool\" criteria is \"near the top of the next\n>> >     >>> block\" (which is obviously *not* incentive-compatible)\n>> >     >>\n>> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie. next\n>> >     >> block, and assumed you'd only allow RBF if the old package wasn't\n>> >     in the\n>> >     >> top and the replacement would be.  That seems incentive\n>> >     compatible; more\n>> >     >> than the current scheme?\n>> >     >\n>> >     > My point was, because of block time variance, even that criteria\n>> >     doesn't hold up. If you assume a steady flow of new transactions and\n>> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>> >     likely to get confirmed until a few blocks come in \"early\". Given\n>> >     block variance within a 12 block window, this is a relatively likely\n>> >     scenario.\n>> > \n>> >     [ Digging through old mail. ]\n>> > \n>> >     Doesn't really matter.  Lightning close algorithm would be:\n>> > \n>> >     1.  Give bitcoind unileratal close.\n>> >     2.  Ask bitcoind what current expidited fee is (or survey your mempool).\n>> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n>> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n>> > \n>> >     In this case, if you allow a simpified RBF where 'you can replace if\n>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>> >     old tx isnt',\n>> >     it works.\n>> > \n>> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n>> > \n>> >     We could further restrict it by marking the unilateral close somehow to\n>> >     say \"gonna be pushed\" and further limiting the child tx weight (say,\n>> >     5kSipa?) in that case.\n>> > \n>> >     Cheers,\n>> >     Rusty.\n>> >     _______________________________________________\n>> >     Lightning-dev mailing list\n>> >     Lightning-dev at lists.linuxfoundation.org\n>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>> > \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191025/2221408c/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2019-10-27T19:13:09",
                "message_text_only": "Johan,\n\nThe issues with mempool limits for OP_SECURETHEBAG are related, but have\ndistinct solutions.\n\nThere are two main categories of mempool issues at stake. One is relay\ncost, the other is mempool walking.\n\nIn terms of relay cost, if an ancestor can be replaced, it will invalidate\nall it's children, meaning that no one paid for that broadcasting. This can\nbe fixed by appropriately assessing Replace By Fee update fees to\nencapsulate all descendants, but there are some tricky edge cases that make\nthis non-obvious to do.\n\nThe other issue is walking the mempool -- many of the algorithms we use in\nthe mempool can be N log N or N^2 in the number of descendants. (simple\nexample: an input chain of length N to a fan out of N outputs that are all\nspent, is O(N^2) to look up ancestors per-child, unless we're caching).\n\nThe other sort of walking issue is where the indegree or outdegree for a\ntransaction is high. Then when we are computing descendants or ancestors we\nwill need to visit it multiple times. To avoid re-expanding a node, we\ncurrently cache it with a set. This uses O(N) extra memory and makes O(N\nLog N) (we use std::set not unordered_set) comparisons.\n\nI just opened a PR which should help with some of the walking issues by\nallowing us to cheaply cache which nodes we've visited on a run. It makes a\nlot of previously O(N log N) stuff O(N) and doesn't allocate as much new\nmemory. See: https://github.com/bitcoin/bitcoin/pull/17268.\n\n\nNow, for OP_SECURETHEBAG we want a particular property that is very\ndifferent from with lightning htlcs (as is). We want that an unlimited\nnumber of child OP_SECURETHEBAG txns may extend from a confirmed\nOP_SECURETHEBAG, and then at the leaf nodes, we want the same rule as\nlightning (one dangling unconfirmed to permit channels).\n\nOP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a tree\nwhere they are individualized leaf nodes with a preceding CSV. Then, the\nabove fix would ensure each HTLC always has time to close properly as they\nwould have individualized lockpoints. This is desirable for some additional\nreasons and not for others, but it should \"work\".\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Oct 25, 2019 at 10:31 AM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n> I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now\n> they stuff the package size on one of their spendable outlets until it is\n> right at the limit, add one more on their other output (to meet the\n> Carve-Out), and now Party B can\u2019t do anything.\n>\n> On Oct 24, 2019, at 21:05, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n>\n> \ufeff\n> It essentially changes the rule to always allow CPFP-ing the commitment as\n> long as there is an output available without any descendants. It changes\n> the commitment from \"you always need at least, and exactly, one non-CSV\n> output per party. \" to \"you always need at least one non-CSV output per\n> party. \"\n>\n> I realize these limits are there for a reason though, but I'm wondering if\n> could relax them. Also now that jeremyrubin has expressed problems with the\n> current mempool limits.\n>\n> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>\n> wrote:\n>\n>> I may be missing something, but I'm not sure how this changes anything?\n>>\n>> If you have a commitment transaction, you always need at least, and\n>> exactly, one non-CSV output per party. The fact that there is a size\n>> limitation on the transaction that spends for carve-out purposes only\n>> effects how many other inputs/outputs you can add, but somehow I doubt\n>> its ever going to be a large enough number to matter.\n>>\n>> Matt\n>>\n>> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n>> > Reviving this old thread now that the recently released RC for bitcoind\n>> > 0.19 includes the above mentioned carve-out rule.\n>> >\n>> > In an attempt to pave the way for more robust CPFP of on-chain contracts\n>> > (Lightning commitment transactions), the carve-out rule was added in\n>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked\n>> on\n>> > an implementation of a new commitment format for utilizing the Bring\n>> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n>> > rule should have been relaxed a bit, to avoid the need for adding a 1\n>> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n>> > need to be changed to add the CSV delay).\n>> >\n>> > Instead, what about letting the rule be\n>> >\n>> > The last transaction which is added to a package of dependent\n>> > transactions in the mempool must:\n>> >   * Have no more than one unconfirmed parent.\n>> >\n>> > This would of course allow adding a large transaction to each output of\n>> > the unconfirmed parent, which in effect would allow an attacker to\n>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n>> > this a problem with the current mempool acceptance code in bitcoind? I\n>> > would imagine evicting transactions based on feerate when the max\n>> > mempool size is met handles this, but I\u2019m asking since it seems like\n>> > there has been several changes to the acceptance code and eviction\n>> > policy since the limit was first introduced.\n>> >\n>> > - Johan\n>> >\n>> >\n>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n>> > <mailto:rusty at rustcorp.com.au>> wrote:\n>> >\n>> >     Matt Corallo <lf-lists at mattcorallo.com\n>> >     <mailto:lf-lists at mattcorallo.com>> writes:\n>> >     >>> Thus, even if you imagine a steady-state mempool growth, unless\n>> the\n>> >     >>> \"near the top of the mempool\" criteria is \"near the top of the\n>> next\n>> >     >>> block\" (which is obviously *not* incentive-compatible)\n>> >     >>\n>> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie.\n>> next\n>> >     >> block, and assumed you'd only allow RBF if the old package wasn't\n>> >     in the\n>> >     >> top and the replacement would be.  That seems incentive\n>> >     compatible; more\n>> >     >> than the current scheme?\n>> >     >\n>> >     > My point was, because of block time variance, even that criteria\n>> >     doesn't hold up. If you assume a steady flow of new transactions and\n>> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>> >     likely to get confirmed until a few blocks come in \"early\". Given\n>> >     block variance within a 12 block window, this is a relatively likely\n>> >     scenario.\n>> >\n>> >     [ Digging through old mail. ]\n>> >\n>> >     Doesn't really matter.  Lightning close algorithm would be:\n>> >\n>> >     1.  Give bitcoind unileratal close.\n>> >     2.  Ask bitcoind what current expidited fee is (or survey your\n>> mempool).\n>> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n>> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n>> >\n>> >     In this case, if you allow a simpified RBF where 'you can replace if\n>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>> >     old tx isnt',\n>> >     it works.\n>> >\n>> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n>> >\n>> >     We could further restrict it by marking the unilateral close\n>> somehow to\n>> >     say \"gonna be pushed\" and further limiting the child tx weight (say,\n>> >     5kSipa?) in that case.\n>> >\n>> >     Cheers,\n>> >     Rusty.\n>> >     _______________________________________________\n>> >     Lightning-dev mailing list\n>> >     Lightning-dev at lists.linuxfoundation.org\n>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>> >\n>>\n> _______________________________________________\n> Lightning-dev mailing list\n> Lightning-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191027/184af597/attachment.html>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-28T09:45:39",
                "message_text_only": ">\n>\n> I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now\n> they stuff the package size on one of their spendable outlets until it is\n> right at the limit, add one more on their other output (to meet the\n> Carve-Out), and now Party B can\u2019t do anything.\n\nMatt: With the proposed change, party B would always be able to add a child\nto its output, regardless of what games party A is playing.\n\n\nThanks for the explanation, Jeremy!\n\n\n> In terms of relay cost, if an ancestor can be replaced, it will invalidate\n> all it's children, meaning that no one paid for that broadcasting. This can\n> be fixed by appropriately assessing Replace By Fee update fees to\n> encapsulate all descendants, but there are some tricky edge cases that make\n> this non-obvious to do.\n\n\nRelay cost is the obvious problem with just naively removing all limits.\nRelaxing the current rules by allowing to add a child to each output as\nlong as it has a single unconfirmed parent would still only allow free\nrelay of O(size of parent) extra data (which might not be that bad? Similar\nto the carve-out rule we could put limits on the child size). This would be\nenough for the current LN use case (increasing fee of commitment tx), but\nnot for OP_SECURETHEBAG I guess, as you need the tree of children, as you\nmention.\n\nI imagine walking the mempool wouldn't change much, as you would only have\none extra child per output. But here I'm just speculating, as I don't know\nthe code well enough know what the diff would look like.\n\n\n> OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a\n> tree where they are individualized leaf nodes with a preceding CSV. Then,\n> the above fix would ensure each HTLC always has time to close properly as\n> they would have individualized lockpoints. This is desirable for some\n> additional reasons and not for others, but it should \"work\".\n\n\nThis is interesting for an LN commitment! You could really hide every\noutput of the commitment within OP_STB, which could either allow bypassing\nthe fee-pinning attack entirely (if the output cannot be spent unconfirmed)\nor adding fees to the commitment using SIGHASH_SINGLE|ANYONECANPAY.\n\n- Johan\n\nOn Sun, Oct 27, 2019 at 8:13 PM Jeremy <jlrubin at mit.edu> wrote:\n\n> Johan,\n>\n> The issues with mempool limits for OP_SECURETHEBAG are related, but have\n> distinct solutions.\n>\n> There are two main categories of mempool issues at stake. One is relay\n> cost, the other is mempool walking.\n>\n> In terms of relay cost, if an ancestor can be replaced, it will invalidate\n> all it's children, meaning that no one paid for that broadcasting. This can\n> be fixed by appropriately assessing Replace By Fee update fees to\n> encapsulate all descendants, but there are some tricky edge cases that make\n> this non-obvious to do.\n>\n> The other issue is walking the mempool -- many of the algorithms we use in\n> the mempool can be N log N or N^2 in the number of descendants. (simple\n> example: an input chain of length N to a fan out of N outputs that are all\n> spent, is O(N^2) to look up ancestors per-child, unless we're caching).\n>\n> The other sort of walking issue is where the indegree or outdegree for a\n> transaction is high. Then when we are computing descendants or ancestors we\n> will need to visit it multiple times. To avoid re-expanding a node, we\n> currently cache it with a set. This uses O(N) extra memory and makes O(N\n> Log N) (we use std::set not unordered_set) comparisons.\n>\n> I just opened a PR which should help with some of the walking issues by\n> allowing us to cheaply cache which nodes we've visited on a run. It makes a\n> lot of previously O(N log N) stuff O(N) and doesn't allocate as much new\n> memory. See: https://github.com/bitcoin/bitcoin/pull/17268.\n>\n>\n> Now, for OP_SECURETHEBAG we want a particular property that is very\n> different from with lightning htlcs (as is). We want that an unlimited\n> number of child OP_SECURETHEBAG txns may extend from a confirmed\n> OP_SECURETHEBAG, and then at the leaf nodes, we want the same rule as\n> lightning (one dangling unconfirmed to permit channels).\n>\n> OP_SECURETHEBAG can help with the LN issue by putting all HTLCS into a\n> tree where they are individualized leaf nodes with a preceding CSV. Then,\n> the above fix would ensure each HTLC always has time to close properly as\n> they would have individualized lockpoints. This is desirable for some\n> additional reasons and not for others, but it should \"work\".\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n>\n> On Fri, Oct 25, 2019 at 10:31 AM Matt Corallo <lf-lists at mattcorallo.com>\n> wrote:\n>\n>> I don\u2019te see how? Let\u2019s imagine Party A has two spendable outputs, now\n>> they stuff the package size on one of their spendable outlets until it is\n>> right at the limit, add one more on their other output (to meet the\n>> Carve-Out), and now Party B can\u2019t do anything.\n>>\n>> On Oct 24, 2019, at 21:05, Johan Tor\u00e5s Halseth <johanth at gmail.com> wrote:\n>>\n>> \ufeff\n>> It essentially changes the rule to always allow CPFP-ing the commitment\n>> as long as there is an output available without any descendants. It changes\n>> the commitment from \"you always need at least, and exactly, one non-CSV\n>> output per party. \" to \"you always need at least one non-CSV output per\n>> party. \"\n>>\n>> I realize these limits are there for a reason though, but I'm wondering\n>> if could relax them. Also now that jeremyrubin has expressed problems with\n>> the current mempool limits.\n>>\n>> On Thu, Oct 24, 2019 at 11:25 PM Matt Corallo <lf-lists at mattcorallo.com>\n>> wrote:\n>>\n>>> I may be missing something, but I'm not sure how this changes anything?\n>>>\n>>> If you have a commitment transaction, you always need at least, and\n>>> exactly, one non-CSV output per party. The fact that there is a size\n>>> limitation on the transaction that spends for carve-out purposes only\n>>> effects how many other inputs/outputs you can add, but somehow I doubt\n>>> its ever going to be a large enough number to matter.\n>>>\n>>> Matt\n>>>\n>>> On 10/24/19 1:49 PM, Johan Tor\u00e5s Halseth wrote:\n>>> > Reviving this old thread now that the recently released RC for bitcoind\n>>> > 0.19 includes the above mentioned carve-out rule.\n>>> >\n>>> > In an attempt to pave the way for more robust CPFP of on-chain\n>>> contracts\n>>> > (Lightning commitment transactions), the carve-out rule was added in\n>>> > https://github.com/bitcoin/bitcoin/pull/15681. However, having worked\n>>> on\n>>> > an implementation of a new commitment format for utilizing the Bring\n>>> > Your Own Fees strategy using CPFP, I\u2019m wondering if the special case\n>>> > rule should have been relaxed a bit, to avoid the need for adding a 1\n>>> > CSV to all outputs (in case of Lightning this means HTLC scripts would\n>>> > need to be changed to add the CSV delay).\n>>> >\n>>> > Instead, what about letting the rule be\n>>> >\n>>> > The last transaction which is added to a package of dependent\n>>> > transactions in the mempool must:\n>>> >   * Have no more than one unconfirmed parent.\n>>> >\n>>> > This would of course allow adding a large transaction to each output of\n>>> > the unconfirmed parent, which in effect would allow an attacker to\n>>> > exceed the MAX_PACKAGE_VIRTUAL_SIZE limit in some cases. However, is\n>>> > this a problem with the current mempool acceptance code in bitcoind? I\n>>> > would imagine evicting transactions based on feerate when the max\n>>> > mempool size is met handles this, but I\u2019m asking since it seems like\n>>> > there has been several changes to the acceptance code and eviction\n>>> > policy since the limit was first introduced.\n>>> >\n>>> > - Johan\n>>> >\n>>> >\n>>> > On Wed, Feb 13, 2019 at 6:57 AM Rusty Russell <rusty at rustcorp.com.au\n>>> > <mailto:rusty at rustcorp.com.au>> wrote:\n>>> >\n>>> >     Matt Corallo <lf-lists at mattcorallo.com\n>>> >     <mailto:lf-lists at mattcorallo.com>> writes:\n>>> >     >>> Thus, even if you imagine a steady-state mempool growth,\n>>> unless the\n>>> >     >>> \"near the top of the mempool\" criteria is \"near the top of the\n>>> next\n>>> >     >>> block\" (which is obviously *not* incentive-compatible)\n>>> >     >>\n>>> >     >> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie.\n>>> next\n>>> >     >> block, and assumed you'd only allow RBF if the old package\n>>> wasn't\n>>> >     in the\n>>> >     >> top and the replacement would be.  That seems incentive\n>>> >     compatible; more\n>>> >     >> than the current scheme?\n>>> >     >\n>>> >     > My point was, because of block time variance, even that criteria\n>>> >     doesn't hold up. If you assume a steady flow of new transactions\n>>> and\n>>> >     one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't\n>>> >     likely to get confirmed until a few blocks come in \"early\". Given\n>>> >     block variance within a 12 block window, this is a relatively\n>>> likely\n>>> >     scenario.\n>>> >\n>>> >     [ Digging through old mail. ]\n>>> >\n>>> >     Doesn't really matter.  Lightning close algorithm would be:\n>>> >\n>>> >     1.  Give bitcoind unileratal close.\n>>> >     2.  Ask bitcoind what current expidited fee is (or survey your\n>>> mempool).\n>>> >     3.  Give bitcoind child \"push\" tx at that total feerate.\n>>> >     4.  If next block doesn't contain unilateral close tx, goto 2.\n>>> >\n>>> >     In this case, if you allow a simpified RBF where 'you can replace\n>>> if\n>>> >     1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3.\n>>> >     old tx isnt',\n>>> >     it works.\n>>> >\n>>> >     It allows someone 100k of free tx spam, sure.  But it's simple.\n>>> >\n>>> >     We could further restrict it by marking the unilateral close\n>>> somehow to\n>>> >     say \"gonna be pushed\" and further limiting the child tx weight\n>>> (say,\n>>> >     5kSipa?) in that case.\n>>> >\n>>> >     Cheers,\n>>> >     Rusty.\n>>> >     _______________________________________________\n>>> >     Lightning-dev mailing list\n>>> >     Lightning-dev at lists.linuxfoundation.org\n>>> >     <mailto:Lightning-dev at lists.linuxfoundation.org>\n>>> >     https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>> >\n>>>\n>> _______________________________________________\n>> Lightning-dev mailing list\n>> Lightning-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/lightning-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191028/92777d7b/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2019-10-28T17:14:38",
                "message_text_only": "On Mon, Oct 28, 2019 at 10:45:39AM +0100, Johan Tor\u00e5s Halseth wrote:\n> Relay cost is the obvious problem with just naively removing all limits.\n> Relaxing the current rules by allowing to add a child to each output as\n> long as it has a single unconfirmed parent would still only allow free\n> relay of O(size of parent) extra data (which might not be that bad? Similar\n> to the carve-out rule we could put limits on the child size). \n\nA parent transaction near the limit of 100,000 vbytes could have almost\n10,000 outputs paying OP_TRUE (10 vbytes per output).  If the children\nwere limited to 10,000 vbytes each (the current max carve-out size),\nthat allows relaying 100 mega-vbytes or nearly 400 MB data size (larger\nthan the default maximum mempool size in Bitcoin Core).\n\nAs Matt noted in discussion on #lightning-dev about this issue, it's\npossible to increase second-child carve-out to nth-child carve-out but\nwe'd need to be careful about choosing an appropriately low value for n.\n\nFor example, BOLT2 limits the number of HTLCs to 483 on each side of the\nchannel (so 966 + 2 outputs total), which means the worst case free\nrelay to support the current LN protocol would be approximately:\n\n    (100000 + 968 * 10000) * 4 = ~39 MB\n\nEven if the mempool was empty (as it sometimes is these days), it would\nonly cost an attacker about 1.5 BTC to fill it at the default minimum\nrelay feerate[1] so that they could execute this attack at the minimal\ncost per iteration of paying for a few hundred or a few thousand vbytes\nat slightly higher than the current mempool minimum fee.\n\nInstead, with the existing rules (including second-child carve-out),\nthey'd have to iterate (39 MB / 400 kB = ~100) times more often to\nachieve an equivalent waste of bandwidth, costing them proportionally\nmore in fees.\n\nSo, I think these rough numbers clearly back what Matt said about us\nbeing able to raise the limits a bit if we need to, but that we have to\nbe careful not to raise them so far that attackers can make it\nsignificantly more bandwidth expensive for people to run relaying full\nnodes.\n\n-Dave\n\n[1] Several developers are working on lowering the default minimum in\nBitcoin Core, which would of course make this attack proportionally\ncheaper."
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2019-10-30T07:22:53",
                "message_text_only": "On Mon, Oct 28, 2019 at 6:16 PM David A. Harding <dave at dtrt.org> wrote:\n\n> A parent transaction near the limit of 100,000 vbytes could have almost\n> 10,000 outputs paying OP_TRUE (10 vbytes per output).  If the children\n> were limited to 10,000 vbytes each (the current max carve-out size),\n> that allows relaying 100 mega-vbytes or nearly 400 MB data size (larger\n> than the default maximum mempool size in Bitcoin Core).\n>\n\nThanks, Dave, I wasn't aware the limits would allow this many outputs. And\nas your calculation shows, this opens up the potential for free relay of\nlarge amounts of data.\n\nWe could start special casing to only allow this for \"LN commitment-like\"\ntransactions, but this would be application specific changes, and your\ncalculation shows that even with the BOLT2 numbers there still exists cases\nwith a large number of children.\n\nWe are moving forward with adding a 1 block delay to all outputs to utilize\nthe current carve-out rule, and the changes aren't that bad. See Joost's\npost in \"[PATCH] First draft of option_simplfied_commitment\"\n\n- Johan\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191030/bc2a90f3/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "CPFP Carve-Out for Fee-Prediction Issues in Contracting Applications (eg Lightning)",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Jeremy",
                "Johan Tor\u00e5s Halseth",
                "David A. Harding",
                "Matt Corallo"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 42041
        }
    },
    {
        "title": "[bitcoin-dev] Transition to post-quantum",
        "thread_messages": [
            {
                "author": "Erik Aronesty",
                "date": "2019-10-24T15:34:14",
                "message_text_only": "- It would be hard to prove you have access to an x that can produce\nH(g^x) in a way that doesn't expose g^x and isn't one of those slow,\ninteractive bit-encryption algorithms.\n\n- Instead a simple scheme would publish a transaction to the\nblockchain that lists:\n     - pre-quantum signature\n     - hash of post-quantum address\n\n- Any future transactions would require both the pre *and*\npost-quantum signatures.\n\nThat scheme would need to be implemented sufficient number of years\nbefore quantum became a pressing issue, but it's super simple,\nspam-proof (requires fees), and flexible enough that it can change as\npost-quantum addressing improves.\n\nImagine there are 2 quantum addressing schemes in order of discovery.\n\n1. Soft-fork 1 accepts the first scheme and people begin publishing\nPRE/POST upgrades.\n2. Discovery is made that shows a second scheme has smaller\ntransactions and faster validation.\n3. Soft-fork 2 refuses to accept upgrades to the first scheme in\ntransactions beyond a certain block number in order to improve\nperformance.\n\n\n\n\n\n\nOn Thu, Feb 15, 2018 at 6:44 PM Tim Ruffing via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> On Thu, 2018-02-15 at 23:44 +0100, Natanael wrote:\n> > If your argument is that we publish the full transaction minus the\n> > public key and signatures, just committing to it, and then revealing\n> > that later (which means an attacker can't modify the transaction in\n> > advance in a way that produces a valid transaction);\n>\n> Almost. Actually we reveal the entire transaction later.\n>\n> >\n> > [...] while *NOT* allowing expiration makes it a trivial DoS target.\n> >\n> > Anybody can flood the miners with invalid transaction commitments. No\n> > miner can ever prune invalid commitments until a valid transaction is\n> > finalized which conflicts with the invalid commitments. You can't\n> > even rate limit it safely.\n>\n> Yes, that's certainly true. I mentioned that issue already.\n>\n> You can rate limit this: The only thing I see is that one can require\n> transaction fees even for commitments. That's super annoying, because\n> you need a second (PQ-)UTXO just to commit. But it's not impossible.\n>\n> You can call this impractical and this may well be true. But what will\n> be most practical in the future depends on many parameters that are\n> totally unclear at the moment, e.g., the efficiency of zero-knowledge\n> proof systems. Who knows?\n>\n> If you would like to use zero-knowledge proofs to recover an UTXO with\n> an P2PKH address, you need to prove in zero-knowledge that you know\n> some secret key x such that H(g^x)=addr. That seems plausible. But\n> P2PKH is by far the simplest example. For arbitrary scripts, this can\n> become pretty complex and nasty, even if our proof systems and machines\n> are fast enough.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Transition to post-quantum",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Erik Aronesty"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2972
        }
    },
    {
        "title": "[bitcoin-dev] Fwd: node-Tor is now open source in clear (and modular)",
        "thread_messages": [
            {
                "author": "Aymeric Vitte",
                "date": "2019-10-28T10:33:32",
                "message_text_only": "FYI, javascript implementation of the Tor protocol on server side and\ninside browsers\n\nNot related directly to bitcoin-dev but might be of some use one day to\nanonymize bitcoin apps (light wallets for example)\n\n\n-------- Message transf\u00e9r\u00e9 --------\nSujet\u00a0: \tnode-Tor is now open source in clear (and modular)\nDate\u00a0: \tThu, 24 Oct 2019 18:02:42 +0200\nDe\u00a0: \tAymeric Vitte <vitteaymeric at gmail.com>\nPour\u00a0: \ttor-talk at lists.torproject.org\n\n\n\nPlease see https://github.com/Ayms/node-Tor and http://peersm.com/peersm2\n\nThis is a javascript implementation of the Tor protocol on server side\n(nodejs) and inside browsers, please note that it is not intended to add\nnodes into the Tor network, neither to implement the Tor Browser\nfeatures, it is intended to build projects using the Tor protocol from\nthe browser and/or servers (most likely P2P projects), the Onion Proxy\nand Onion Router functions are available directly inside the browser\nwhich establishes circuits with other nodes understanding the Tor\nprotocol (so it's not a \"dumb\" proxy), but it can of course establish\ncircuits with the Tor network nodes (see\nhttps://github.com/Ayms/node-Tor#test-configuration-and-use) and act as\na Tor node\n\nIt is financed by NLnet via EU Horizon 2020 Next Generation Internet\nPrivacy & Trust Enhancing Technologies, now open source under a MIT\nlicense and we made it modular, it is fast (extensively tested when\nvideo streaming was there, especially with bittorrent or ORDB concept)\nand the total unminified code\n(https://github.com/Ayms/node-Tor/blob/master/html/browser.js) is only 1\nMB (so ~600 kB minified) which is quite small for what it does, this is\nnot a browser extension/module but pure js\n\nPossible next steps are to implement elliptic crypto and connections via\nWebRTC Snowflake (peersm2 above uses WebSockets a bit the way flashproxy\nwas working, ie implementing the ws interface on bridges side), as well\nas integrating it with \"Discover and move your coins by yourself\"\n(https://peersm.com/wallet) for anonymous blockchain search and\nanonymous sending of transactions from the browser\n\n-- \nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191028/1c6957c0/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd: node-Tor is now open source in clear (and modular)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Aymeric Vitte"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2932
        }
    },
    {
        "title": "[bitcoin-dev] Ring VS Schnorr signatures",
        "thread_messages": [
            {
                "author": "Casciano, Anthony",
                "date": "2019-10-28T14:12:49",
                "message_text_only": "In the world of architectural trade-offs, specifically: confidentiality versus base layer performance, and\nin the current regulatory environment, as global monetary affairs begin to get \"more real,\" I'm now leaning towards greater confidentiality rather than my earlier preference for performance.\n\nDo Ring sigs with Stealth addresses impede blockchain performance or do they mis-align with Bitcoin's longer term\ndev roadmap?\n\n\n~ TC\n\n________________________________________\nFrom: bitcoin-dev-bounces at lists.linuxfoundation.org <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of bitcoin-dev-request at lists.linuxfoundation.org <bitcoin-dev-request at lists.linuxfoundation.org>\nSent: Monday, October 28, 2019 8:00 AM\nTo: bitcoin-dev at lists.linuxfoundation.org\nSubject: bitcoin-dev Digest, Vol 53, Issue 41\n\nSend bitcoin-dev mailing list submissions to\n        bitcoin-dev at lists.linuxfoundation.org\n\nTo subscribe or unsubscribe via the World Wide Web, visit\n        https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\nor, via email, send a message with subject or body 'help' to\n        bitcoin-dev-request at lists.linuxfoundation.org\n\nYou can reach the person managing the list at\n        bitcoin-dev-owner at lists.linuxfoundation.org\n\nWhen replying, please edit your Subject line so it is more specific\nthan \"Re: Contents of bitcoin-dev digest...\"\n\n\nToday's Topics:\n\n   1. Fwd: node-Tor is now open source in clear (and    modular)\n      (Aymeric Vitte)\n\n\n----------------------------------------------------------------------\n\nMessage: 1\nDate: Mon, 28 Oct 2019 11:33:32 +0100\nFrom: Aymeric Vitte <vitteaymeric at gmail.com>\nTo: Bitcoin Dev <bitcoin-dev at lists.linuxfoundation.org>\nSubject: [bitcoin-dev] Fwd: node-Tor is now open source in clear (and\n        modular)\nMessage-ID: <70e77789-9fce-0d54-0aed-361035e79c1d at gmail.com>\nContent-Type: text/plain; charset=\"utf-8\"\n\nFYI, javascript implementation of the Tor protocol on server side and\ninside browsers\n\nNot related directly to bitcoin-dev but might be of some use one day to\nanonymize bitcoin apps (light wallets for example)\n\n\n-------- Message transf?r? --------\nSujet?:         node-Tor is now open source in clear (and modular)\nDate?:  Thu, 24 Oct 2019 18:02:42 +0200\nDe?:    Aymeric Vitte <vitteaymeric at gmail.com>\nPour?:  tor-talk at lists.torproject.org\n\n\n\nPlease see https://github.com/Ayms/node-Tor and http://peersm.com/peersm2\n\nThis is a javascript implementation of the Tor protocol on server side\n(nodejs) and inside browsers, please note that it is not intended to add\nnodes into the Tor network, neither to implement the Tor Browser\nfeatures, it is intended to build projects using the Tor protocol from\nthe browser and/or servers (most likely P2P projects), the Onion Proxy\nand Onion Router functions are available directly inside the browser\nwhich establishes circuits with other nodes understanding the Tor\nprotocol (so it's not a \"dumb\" proxy), but it can of course establish\ncircuits with the Tor network nodes (see\nhttps://github.com/Ayms/node-Tor#test-configuration-and-use) and act as\na Tor node\n\nIt is financed by NLnet via EU Horizon 2020 Next Generation Internet\nPrivacy & Trust Enhancing Technologies, now open source under a MIT\nlicense and we made it modular, it is fast (extensively tested when\nvideo streaming was there, especially with bittorrent or ORDB concept)\nand the total unminified code\n(https://github.com/Ayms/node-Tor/blob/master/html/browser.js) is only 1\nMB (so ~600 kB minified) which is quite small for what it does, this is\nnot a browser extension/module but pure js\n\nPossible next steps are to implement elliptic crypto and connections via\nWebRTC Snowflake (peersm2 above uses WebSockets a bit the way flashproxy\nwas working, ie implementing the ws interface on bridges side), as well\nas integrating it with \"Discover and move your coins by yourself\"\n(https://peersm.com/wallet) for anonymous blockchain search and\nanonymous sending of transactions from the browser\n\n--\nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20191028/1c6957c0/attachment-0001.html>\n\n------------------------------\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\nEnd of bitcoin-dev Digest, Vol 53, Issue 41\n*******************************************"
            }
        ],
        "thread_summary": {
            "title": "Ring VS Schnorr signatures",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Casciano, Anthony"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5158
        }
    }
]