[
    {
        "title": "[Bitcoin-development] [ANN] METAmarket - Trustless Federated Marketplaces",
        "thread_messages": [
            {
                "author": "Marc D. Wood",
                "date": "2015-05-01T22:57:55",
                "message_text_only": "METAmarket: Trustless Federated Marketplaces\n>>> http://metamarket.biz <<<\n\n* * *\nIntroduction\n\nMETAmarket is an open source protocol and proof-of-concept reference\nclient specifying a trustless federated marketplace which uses Bitcoin as\na universal currency and Bitmessage as a P2P communication network.\nTime-locked refund transactions ensure that incentives are aligned toward\ncompleting the trade without the need for trusted third parties. Systemic\nvulnerabilities such as transaction malleability are mitigated through the\nuse of a federated reputation model. This document is a non-technical\noverview of how the METAmarket client and protocol work. For more\ntechnical details, see the protocol specification.\n\nMotivation\n\nOverly centralized marketplaces and payment services extract high fees,\nimpose and abuse excessive control and remove any hope of privacy from\nusers. As more commerce moves online, many consumers may find their\nlifetime history of purchases (including books, personal items and\nlocation details) for sale to advertisers, employers, curious neighbors,\nstalkers, political opponents and government agencies. An ideal system\nwould be one of secure private transactions directly between buyer and\nseller without middle men collecting data or adding fees. Such systems are\nnow feasible by combining recently developed technologies for anonymous\ndecentralized payment and messaging systems.\n\nClient\n\nTo use the marketplaces, a client application which implements the\nMETAmarket protocol is required. The client is used to post, browse and\nexecute trades. It also requires a Bitcoin Core wallet to handle payments\nand refunds. A working client is available at:\n\nhttp://github.com/metamarcdw/metamarket"
            },
            {
                "author": "Melvin Carvalho",
                "date": "2015-05-02T02:01:35",
                "message_text_only": "On 2 May 2015 at 00:57, Marc D. Wood <metamarc at metamarket.biz> wrote:\n\n> METAmarket: Trustless Federated Marketplaces\n> >>> http://metamarket.biz <<<\n>\n> * * *\n> Introduction\n>\n> METAmarket is an open source protocol and proof-of-concept reference\n> client specifying a trustless federated marketplace which uses Bitcoin as\n> a universal currency and Bitmessage as a P2P communication network.\n> Time-locked refund transactions ensure that incentives are aligned toward\n> completing the trade without the need for trusted third parties. Systemic\n> vulnerabilities such as transaction malleability are mitigated through the\n> use of a federated reputation model. This document is a non-technical\n> overview of how the METAmarket client and protocol work. For more\n> technical details, see the protocol specification.\n>\n> Motivation\n>\n> Overly centralized marketplaces and payment services extract high fees,\n> impose and abuse excessive control and remove any hope of privacy from\n> users. As more commerce moves online, many consumers may find their\n> lifetime history of purchases (including books, personal items and\n> location details) for sale to advertisers, employers, curious neighbors,\n> stalkers, political opponents and government agencies. An ideal system\n> would be one of secure private transactions directly between buyer and\n> seller without middle men collecting data or adding fees. Such systems are\n> now feasible by combining recently developed technologies for anonymous\n> decentralized payment and messaging systems.\n>\n> Client\n>\n> To use the marketplaces, a client application which implements the\n> METAmarket protocol is required. The client is used to post, browse and\n> execute trades. It also requires a Bitcoin Core wallet to handle payments\n> and refunds. A working client is available at:\n>\n> http://github.com/metamarcdw/metamarket\n>\n\nIs there any relation between this and the work satoshi was putting into\nthe core before he left?\n\nhttps://github.com/bitcoin/bitcoin/commit/5253d1ab77fab1995ede03fb934edd67f1359ba8\n\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150502/aed666ab/attachment.html>"
            },
            {
                "author": "Marc D. Wood",
                "date": "2015-05-02T15:45:35",
                "message_text_only": "On Fri, May 1, 2015 10:01 pm, Melvin Carvalho wrote:\n>\n> Is there any relation between this and the work satoshi was putting into\n> the core before he left?\n>\n> https://github.com/bitcoin/bitcoin/commit/5253d1ab77fab1995ede03fb934edd6\n> 7f1359ba8\n>\n>\n\nMelvin,\n\nThis is certainly a similar concept. It is a software for creating markets\nwith variable levels of privacy and censorship resistance. It is built\ndirectly on top of Bitcoin Core and Bitmessage.\n\nI believe this project to be in the spirit of Satoshi's original plan for\na bitcoin marketplace but it is not the same project, and I certainly am\nnot Satoshi. :)\n\nMarc D. Wood"
            }
        ],
        "thread_summary": {
            "title": "METAmarket - Trustless Federated Marketplaces",
            "categories": [
                "Bitcoin-development",
                "ANN"
            ],
            "authors": [
                "Melvin Carvalho",
                "Marc D. Wood"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 5220
        }
    },
    {
        "title": "[Bitcoin-development] Relative CHECKLOCKTIMEVERIFY (was CLTV proposal)",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2015-05-04T02:15:47",
                "message_text_only": "On 04/21/15 07:59, Peter Todd wrote:\n> On Mon, Mar 16, 2015 at 10:22:13PM +0000, Matt Corallo wrote:\n>> In building some CLTV-based contracts, it is often also useful to have a\n>> method of requiring, instead of locktime-is-at-least-N,\n>> locktime-is-at-least-N-plus-the-height-of-my-input. ie you could imagine\n>> an OP_RELATIVECHECKLOCKTIMEVERIFY that reads (does not pop) the top\n>> stack element, adds the height of the output being spent and then has\n>> identical semantics to CLTV.\n> \n> Depending on what you mean by \"identical\" this isn't actually reorg\n> safe. For instance consider this implementation:\n> \n>     nLockTime = stack[-1] + prevout.nHeight\n>     if (nLockTime > txTo.nLockTime):\n>         return False\n> \n> Used with this scriptPubKey:\n> \n>     10 RCLTV DROP <pubkey> CHECKSIG\n> \n> If I create that output in tx1 which is mined at height 42 I can spend\n> it in a tx2 at height > 42+10 by setting tx2's nLockTime to >42+10, for\n> instance 53. However if a reorg happens and tx1 ends up at height 43\n> after the reorg I'm stuck - tx2's nLockTime is set at 42.\n> \n> Thus RCTLV is only reorg safe if the height is compared against the\n> actual block height of the block containing the spending transaction,\n> not the spending transaction's nLockTime.\n\nYes, as discussed on IRC months ago when the first email was sent, the\nassumption is that you would require N be at least 100. That way you are\nreorg safe up to the same limit as coinbase transactions, which are also\nonly reorg safe in the case of no 100-block reorgs. Its not ideal in\nsome contracts, but keeping the no-second-nLockTime-equivalent property\nis worth it IMO, and its still incredibly useful in many contracts.\n\n>> A slightly different API (and different name) was described by maaku at\n>> http://www.reddit.com/r/Bitcoin/comments/2z2l91/time_to_lobby_bitcoins_core_devs_sf_bitcoin_devs/cpgc154\n>> which does a better job of saving softfork-available opcode space.\n>>\n>> There are two major drawbacks to adding such an operation, however.\n>>\n>> 1) More transaction information is exposed inside the script (prior to\n>> CLTV we only had the sigchecking operation exposed, with a CLTV and\n>> RCLTV/OP_CHECK_MATURITY_VERIFY we expose two more functions).\n>>\n>> 2) Bitcoin Core's mempool invariant of \"all transactions in the mempool\n>> could be thrown into one overside block and aside from block size, it\n>> would be valid\" becomes harder to enforce. Currently, during reorgs,\n>> coinbase spends need checked (specifically, anything spending THE\n>> coinbase 100 blocks ago needs checked) and locktime transactions need\n>> checked. With such a new operation, any script which used this new\n>> opcode during its execution would need to be re-evaluated during reorgs.\n> \n> Yup, definitely kinda ugly.\n> \n> If the above style of RCTLV was used, one possibility might be to make\n> the relative locktime difference be required to be at least 100 blocks,\n> same as the coinbase maturity, and just accept that it's probably not\n> going to cause any problems, but could in an extremely big reorg. But\n> re-orgs that big might be big enough that we're screwed anyway...\n> \n> With the 100 block rule, during a sufficiently large reorg that\n> coinbases become unavailble, simply disconnect entire blocks - all\n> txouts created by them.\n> \n>> I think both of these requirements are reasonable and not particularly\n>> cumbersome, and the value of such an operation is quite nice for some\n>> protocols (including settings setting up a contest interval in a\n>> sidechain data validation operation).\n> \n> So to be clear, right now the minimal interface to script execution is\n> simply:\n> \n>     int bitcoinconsensus_verify_script(const unsigned char *scriptPubKey, unsigned int scriptPubKeyLen,\n>                                        const unsigned char *txTo        , unsigned int txToLen,\n>                                        unsigned int nIn, unsigned int flags, bitcoinconsensus_error* err);\n> \n> Where scriptPubKey is derived from the unspent coin in the UTXO set and\n> txTo is the transaction containing the script that is being executed.\n> The UTXO set itself currently contains CCoins entries, one for each\n> transaction with unspent outputs, which basically contain:\n> \n>     nVersion - tx nVersion\n>     nHeight  - Height of the block the transaction is contained in.\n>     vout     - Unspent CTxOut's of the transaction.\n> \n> The block nTime isn't directly available through the UTXO set, although\n> it can be found in the block headers. This does require nodes to have\n> the block headers, but at 4MB/year growth it's reasonable to assume the\n> UTXO set will grow faster.\n> \n> Script execution does not have direct access to the current block\n> height/block time, however it does have indirect access via nLockTime.\n> \n> Thus we have a few possibilities:\n> \n> 1) RCLTV against nLockTime\n> \n> Needs a minimum age > COINBASE_MATURITY to be safe.\n> \n> \n> 2) RCLTV against current block height/time\n> \n> Completely reorg safe.\n> \n> \n> 3) GET_TXOUT_HEIGHT/TIME <diff> ADD CLTV\n> \n> To be reorg safe GET_TXOUT_HEIGHT/TIME must fail if minimum age <\n> COINBASE_MATURITY. This can be implemented by comparing against\n> nLockTime.\n> \n> \n> All three possibilities require us to make information about the\n> prevout's height/time available to VerifyScript(). The only question is\n> if we want VerifyScript() to also take the current block height/time - I\n> see no reason why it can't. As for the mempool, keeping track of what\n> transactions made use of these opcodes so they can be reevaluated if\n> their prevouts are re-organised seems fine to me.\n> \n> \n> Absolute CLTV\n> =============\n> \n> If we are going to make the block height/time available to\n> VerifyScript() to implement RCLTV, should absolute CLTV should continue\n> to have the proposed behavior of checking against nLockTime? If we go\n> with RCLTV against current block height/time, I'm going to vote no,\n> because doing so needlessly limits it to only being able to compare\n> against a block height or a block time in a single transaction.\n> Similarly it can complicate multi-party signatures in some\n> circumstances, as all parties must agree on a common nLockTime.\n> \n> \n> Time-based locks\n> ================\n> \n> Do we want to support them at all? May cause incentive issues with\n> mining, see #bitcoin-wizards discussion, Jul 17th 2013:\n> \n> https://download.wpsoftware.net/bitcoin/wizards/2013/07/13-07-17.log\n>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-04T11:24:44",
                "message_text_only": "What I was describing was an attempt to fix a similar proposal by Mark\nFriedenbach, but it didn't needed fixing: I was simply\nmisunderstanding it.\nMark's RCLTV is completely reorg safe, so there's no need for the 100\nblock restriction. It also keeps the script validation independent\nfrom the utxo.\nHere's is how it works:\n\nThe operator takes a relative_height parameter and it checks that the\nnSequence of the input is lower than that parameter.\n\nAdditionally, a new check at the transaction level:\n\nfor (unsigned int i = 0; i < tx.vin.size(); i++) {\n// ...\n            if (coins->nHeight + tx.vin[i].nSequence < nSpendHeight)\n                return state.Invalid(false, REJECT_INVALID,\n\"bad-txns-non-final-input\");\n// ...\n}\n\nWell, this is assuming that we're only using it with heights and not timestamps.\nMark, feel free to elaborate further."
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-05T00:41:52",
                "message_text_only": "On Mon, May 4, 2015 at 12:24 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> What I was describing was an attempt to fix a similar proposal by Mark\n> Friedenbach, but it didn't needed fixing: I was simply\n> misunderstanding it.\n> Mark's RCLTV is completely reorg safe, so there's no need for the 100\n> block restriction. It also keeps the script validation independent\n> from the utxo.\n> Here's is how it works:\n>\n> The operator takes a relative_height parameter and it checks that the\n> nSequence of the input is lower than that parameter.\n>\n> Additionally, a new check at the transaction level:\n>\n> for (unsigned int i = 0; i < tx.vin.size(); i++) {\n> // ...\n>             if (coins->nHeight + tx.vin[i].nSequence < nSpendHeight)\n>                 return state.Invalid(false, REJECT_INVALID,\n> \"bad-txns-non-final-input\");\n> // ...\n> }\n>\n> Well, this is assuming that we're only using it with heights and not\n> timestamps.\n> Mark, feel free to elaborate further.\n\n\nDoes dropping timestamp refer just to RCLTV or absolutely CLTV also? For\nabsolute CLTV I think it's important to have timestamps so that trust fund\nuse cases are practical (e.g. spendable on 18th birthday), because the\nexact date a future block will be mined on is unpredictable if it's far\nenough in the future (out by days or even weeks).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150505/5cc624fb/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-05T19:19:00",
                "message_text_only": "Well, apparently the timestamp can be make compatible with Mark's\nnSequence-based RCLTV by adding an additional check at the block level\nbut I was only explaining the concept using heights (which is the most\ninteresting part IMO).\nI'm also not sure I understood the details and I don't want to confuse\npeople again, so I'll wait for someone else to explain that part.\nACLTV can work with timestamps too unless I'm missing something. It's\njust more complexity and I was never convinced that there's enough use\ncases relying on timestamps to justify them. But the timestamp\ndiscussion is quite orthogonal to the nSequence-based RCLTV proposal\nitself.\n\nOn Tue, May 5, 2015 at 2:41 AM, Btc Drak <btcdrak at gmail.com> wrote:\n> On Mon, May 4, 2015 at 12:24 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>\n>> What I was describing was an attempt to fix a similar proposal by Mark\n>> Friedenbach, but it didn't needed fixing: I was simply\n>> misunderstanding it.\n>> Mark's RCLTV is completely reorg safe, so there's no need for the 100\n>> block restriction. It also keeps the script validation independent\n>> from the utxo.\n>> Here's is how it works:\n>>\n>> The operator takes a relative_height parameter and it checks that the\n>> nSequence of the input is lower than that parameter.\n>>\n>> Additionally, a new check at the transaction level:\n>>\n>> for (unsigned int i = 0; i < tx.vin.size(); i++) {\n>> // ...\n>>             if (coins->nHeight + tx.vin[i].nSequence < nSpendHeight)\n>>                 return state.Invalid(false, REJECT_INVALID,\n>> \"bad-txns-non-final-input\");\n>> // ...\n>> }\n>>\n>> Well, this is assuming that we're only using it with heights and not\n>> timestamps.\n>> Mark, feel free to elaborate further.\n>\n>\n> Does dropping timestamp refer just to RCLTV or absolutely CLTV also? For\n> absolute CLTV I think it's important to have timestamps so that trust fund\n> use cases are practical (e.g. spendable on 18th birthday), because the exact\n> date a future block will be mined on is unpredictable if it's far enough in\n> the future (out by days or even weeks).\n>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-05T20:38:44",
                "message_text_only": "I think that should be greater than in the comparison?  You want it to fail\nif the the height of the UTXO plus the sequence number is greater than the\nspending block's height.\n\nThere should be an exception for final inputs.  Otherwise, they will count\nas relative locktime of 0xFFFFFFFF.  Is this check handled elsewhere?\n\nif (!tx.vin[i].IsFinal() && nSpendHeight < coins->nHeight +\ntx.vin[i].nSequence)\n       return state.Invalid(false, REJECT_INVALID,\n\"bad-txns-non-final-input\");\n\nIs the intention to let the script check the sequence number?\n\n<number> OP_RELATIVELOCKTIMEVERIFY\n\nwould check if <number> is less than or equal to the sequence number.\n\nIt does make sequence mean something completely different from before.\nInvalidating previously valid transactions has the potential to reduce\nconfidence in the currency.\n\nA workaround would be to have a way to enable it in the sigScript by\nextending Peter Todd's suggestion in the other email chain.\n\n<1> OP_NOP2 means OP_CHECKLOCKTIMEVERIFY (absolute)\n<2> OP_NOP2 means OP_RELATIVECHECKLOCKTIMEVERIFY\n\n<3> OP_NOP2 means OP_SEQUENCE_AS_RELATIVE_HEIGHT\n\nOP_SEQUENCE_AS_RELATIVE_HEIGHT would cause the script to fail unless it was\nthe first opcode in the script.  It acts as a flag to enable using the\nsequence number as for relative block height.\n\nThis can be achieved using a simple pattern match.\n\nbool CScript::IsSequenceAsRelativeHeight() const\n{\n    // Extra-fast test for pay-to-script-hash CScripts:\n    return (this->size() >= 4 &&\n            this->at(0) == OP_PUSHDATA1 &&\n            this->at(1) == 1 &&\n            this->at(2) == 0xFF &&\n            this->at(3) == OP_NOP2);\n}\n\nif (!tx.vin[i].IsFinal() &&\ntx.vin[i].scriptSig.IsSequenceAsRelativeHeight() && nSpendHeight <\ncoins->nHeight + tx.vin[i].nSequence)\n       return state.Invalid(false, REJECT_INVALID,\n\"bad-txns-non-final-input\");\n\nOn Mon, May 4, 2015 at 12:24 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> for (unsigned int i = 0; i < tx.vin.size(); i++) {\n> // ...\n>             if (coins->nHeight + tx.vin[i].nSequence < nSpendHeight)\n>                 return state.Invalid(false, REJECT_INVALID,\n> \"bad-txns-non-final-input\");\n> // ...\n> }\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150505/a857b67b/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-06T07:37:37",
                "message_text_only": "On Tue, May 5, 2015 at 10:38 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n> I think that should be greater than in the comparison?  You want it to fail\n> if the the height of the UTXO plus the sequence number is greater than the\n> spending block's height.\n\nYes, sorry, I changed it just before sending from \"what needs to be\nsatisfied for the validation error to trigger\" to \"what needs to be\nsatisfied for the tx to be valid\".\nYou're right.\n\n> There should be an exception for final inputs.  Otherwise, they will count\n> as relative locktime of 0xFFFFFFFF.  Is this check handled elsewhere?\n>\n> if (!tx.vin[i].IsFinal() && nSpendHeight < coins->nHeight +\n> tx.vin[i].nSequence)\n>        return state.Invalid(false, REJECT_INVALID,\n> \"bad-txns-non-final-input\");\n\nYes, this would be the simplest solution. Another option would be to\nhave a new tx version in which IsFinal(CTransaction) doesn't check the\ninputs sequences to be 0xFFFFFFFF for the tx to be final.\n\n> Is the intention to let the script check the sequence number?\n>\n> <number> OP_RELATIVELOCKTIMEVERIFY\n>\n> would check if <number> is less than or equal to the sequence number.\n\nYes.\n\n> It does make sequence mean something completely different from before.\n> Invalidating previously valid transactions has the potential to reduce\n> confidence in the currency.\n\nWell, the semantics of nSequence don't really change completely. In\nfact, one could argue that this put it closer to its original\nsemantics.\nBut in any case, yes, already signed transaction should remain valid.\nNo transaction would become invalid, just non-final.\nAs soon as the height of its inputs plus their respective nSquences\nget higher than current height they will become final again.\nI cannot think of any use case where a tx becomes invalid forever.\nAlso, probably most people have usedrelatively low values for\nnSequence given the original semantics, just like the relative lock\nnSquence will likely be used as well.\n\n> A workaround would be to have a way to enable it in the sigScript by\n> extending Peter Todd's suggestion in the other email chain.\n>\n> <1> OP_NOP2 means OP_CHECKLOCKTIMEVERIFY (absolute)\n> <2> OP_NOP2 means OP_RELATIVECHECKLOCKTIMEVERIFY\n>\n> <3> OP_NOP2 means OP_SEQUENCE_AS_RELATIVE_HEIGHT\n\nTo be clear, this proposal is supposed to replace RCLTV, so there\nwould still be 2 options. But please let's imagine we have infinite\nopcodes in this thread and let the \"should we design an uglier\nscripting langues to save opcodes?\" question in the other one.\n\n> OP_SEQUENCE_AS_RELATIVE_HEIGHT would cause the script to fail unless it was\n> the first opcode in the script.  It acts as a flag to enable using the\n> sequence number as for relative block height.\n>\n> This can be achieved using a simple pattern match.\n>\n> bool CScript::IsSequenceAsRelativeHeight() const\n> {\n>     // Extra-fast test for pay-to-script-hash CScripts:\n>     return (this->size() >= 4 &&\n>             this->at(0) == OP_PUSHDATA1 &&\n>             this->at(1) == 1 &&\n>             this->at(2) == 0xFF &&\n>             this->at(3) == OP_NOP2);\n> }\n>\n> if (!tx.vin[i].IsFinal() && tx.vin[i].scriptSig.IsSequenceAsRelativeHeight()\n> && nSpendHeight < coins->nHeight + tx.vin[i].nSequence)\n>        return state.Invalid(false, REJECT_INVALID,\n> \"bad-txns-non-final-input\");\n\nThis gives you less flexibility and I don't think it's necessary.\nPlease let's try to avoid this if it's possible.\n\n\n> On Mon, May 4, 2015 at 12:24 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>\n>> for (unsigned int i = 0; i < tx.vin.size(); i++) {\n>> // ...\n>>             if (coins->nHeight + tx.vin[i].nSequence < nSpendHeight)\n>>                 return state.Invalid(false, REJECT_INVALID,\n>> \"bad-txns-non-final-input\");\n>> // ...\n>> }\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-06T22:09:59",
                "message_text_only": "On Wed, May 6, 2015 at 8:37 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n>\n> This gives you less flexibility and I don't think it's necessary.\n> Please let's try to avoid this if it's possible.\n\n\nIt is just a switch that turns on and off the new mode.\n\nIn retrospect, it would be better to just up the transaction version.\n\nIn transactions from v2 onwards, the sequence field means height.  That\nmeans legacy transactions would be spendable.\n\nThis is a pure soft-fork.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150506/ebbb9a52/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Relative CHECKLOCKTIMEVERIFY (was CLTV proposal)",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Tier Nolan",
                "Jorge Tim\u00f3n",
                "Btc Drak",
                "Matt Corallo"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 18234
        }
    },
    {
        "title": "[Bitcoin-development] New release of replace-by-fee for Bitcoin Core v0.10.1",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-05-04T04:36:01",
                "message_text_only": "My replace-by-fee patch is now available for the v0.10.1 release:\n\n    https://github.com/petertodd/bitcoin/tree/replace-by-fee-v0.10.1\n\nNo new features in this version; this is simply a rebase for the Bitcoin\nCore v0.10.1 release. (there weren't even any merge conflicts) As with\nthe Bitcoin Core v0.10.1, it's recommended to upgrade.\n\n\nThe following text is the copied verbatim from the previous release:\n\nWhat's replace-by-fee?\n----------------------\n\nCurrently most Bitcoin nodes accept the first transaction they see\nspending an output to the mempool; all later transactions are rejected.\nReplace-by-fee changes this behavior to accept the transaction paying\nthe highest fee, both absolutely, and in terms of fee-per-KB. Replaced\nchildren are also considered - a chain of transactions is only replaced\nif the replacement has a higher fee than the sum of all replaced\ntransactions.\n\nDoing this aligns standard node behavior with miner incentives: earn the\nmost amount of money per block. It also makes for a more efficient\ntransaction fee marketplace, as transactions that are \"stuck\" due to bad\nfee estimates can be \"unstuck\" by double-spending them with higher\npaying versions of themselves. With scorched-earth techniques\u2075 it gives\na path to making zeroconf transactions economically secure by relying on\neconomic incentives, rather than \"honesty\" and alturism, in the same way\nBitcoin mining itself relies on incentives rather than \"honesty\" and\nalturism.\n\nFinally for miners adopting replace-by-fee avoids the development of an\necosystem that relies heavily on large miners punishing smaller ones for\nmisbehavior, as seen in Harding's proposal\u2076 that miners collectively 51%\nattack miners who include doublespends in their blocks - an unavoidable\nconsequence of imperfect p2p networking in a decentralized system - or\neven Hearn's proposal\u2077 that a majority of miners be able to vote to\nconfiscate the earnings of the minority and redistribute them at will.\n\n\nInstallation\n------------\n\nOnce you've compiled the replace-by-fee-v0.10.1 branch just run your\nnode normally. With -debug logging enabled, you'll see messages like the\nfollowing in your ~/.bitcoin/debug.log indicating your node is replacing\ntransactions with higher-fee paying double-spends:\n\n    2015-02-12 05:45:20 replacing tx ca07cc2a5eaf55ab13be7ed7d7526cb9d303086f116127608e455122263f93ea with c23973c08d71cdadf3a47bae45566053d364e77d21747ae7a1b66bf1dffe80ea for 0.00798 BTC additional fees, -1033 delta bytes\n\nAdditionally you can tell if you are connected to other replace-by-fee\nnodes, or Bitcoin XT nodes, by examining the service bits advertised by\nyour peers:\n\n    $ bitcoin-cli getpeerinfo | grep services | egrep '((0000000000000003)|(0000000004000001))'\n            \"services\" : \"0000000000000003\",\n            \"services\" : \"0000000004000001\",\n            \"services\" : \"0000000004000001\",\n            \"services\" : \"0000000000000003\",\n            \"services\" : \"0000000004000001\",\n            \"services\" : \"0000000004000001\",\n            \"services\" : \"0000000000000003\",\n            \"services\" : \"0000000000000003\",\n\nReplace-by-fee nodes advertise service bit 26 from the experimental use\nrange; Bitcoin XT nodes advertise service bit 1 for their getutxos\nsupport. The code sets aside a certain number of outgoing and incoming\nslots just for double-spend relaying nodes, so as long as everything is\nworking you're node should be connected to like-minded nodes a within 30\nminutes or so of starting up.\n\nIf you *don't* want to advertise the fact that you are running a\nreplace-by-fee node, just checkout a slightly earlier commit in git; the\nactual mempool changes are separate from the preferential peering\ncommits. You can then connect directly to a replace-by-fee node using\nthe -addnode command line flag.\n\n1) https://github.com/bitcoinxt/bitcoinxt\n2) https://github.com/bitcoin/bitcoin/pull/3883\n3) https://github.com/bitcoin/bitcoin/pull/3883#issuecomment-45543370\n4) https://github.com/luke-jr/bitcoin/tree/0.10.x-ljrP\n5) http://www.mail-archive.com/bitcoin-development%40lists.sourceforge.net/msg05211.html\n6) http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg06970.html\n7) http://www.mail-archive.com/bitcoin-development%40lists.sourceforge.net/msg04972.html\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000059a3dd65f0e5ffb8fdf316d6f31921fefcf0ef726120be9\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150504/5f06134a/attachment.sig>"
            },
            {
                "author": "Kevin Greene",
                "date": "2015-05-05T02:23:09",
                "message_text_only": "I feel compelled to re-share Mike Hearn's counter-argument *against *\nreplace-by-fee:\nhttps://medium.com/@octskyward/replace-by-fee-43edd9a1dd6d\n\nPlease carefully consider the effects of replace-by-fee before applying\nPeter's patch.\n\nOn Sun, May 3, 2015 at 9:36 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> My replace-by-fee patch is now available for the v0.10.1 release:\n>\n>     https://github.com/petertodd/bitcoin/tree/replace-by-fee-v0.10.1\n>\n> No new features in this version; this is simply a rebase for the Bitcoin\n> Core v0.10.1 release. (there weren't even any merge conflicts) As with\n> the Bitcoin Core v0.10.1, it's recommended to upgrade.\n>\n>\n> The following text is the copied verbatim from the previous release:\n>\n> What's replace-by-fee?\n> ----------------------\n>\n> Currently most Bitcoin nodes accept the first transaction they see\n> spending an output to the mempool; all later transactions are rejected.\n> Replace-by-fee changes this behavior to accept the transaction paying\n> the highest fee, both absolutely, and in terms of fee-per-KB. Replaced\n> children are also considered - a chain of transactions is only replaced\n> if the replacement has a higher fee than the sum of all replaced\n> transactions.\n>\n> Doing this aligns standard node behavior with miner incentives: earn the\n> most amount of money per block. It also makes for a more efficient\n> transaction fee marketplace, as transactions that are \"stuck\" due to bad\n> fee estimates can be \"unstuck\" by double-spending them with higher\n> paying versions of themselves. With scorched-earth techniques\u2075 it gives\n> a path to making zeroconf transactions economically secure by relying on\n> economic incentives, rather than \"honesty\" and alturism, in the same way\n> Bitcoin mining itself relies on incentives rather than \"honesty\" and\n> alturism.\n>\n> Finally for miners adopting replace-by-fee avoids the development of an\n> ecosystem that relies heavily on large miners punishing smaller ones for\n> misbehavior, as seen in Harding's proposal\u2076 that miners collectively 51%\n> attack miners who include doublespends in their blocks - an unavoidable\n> consequence of imperfect p2p networking in a decentralized system - or\n> even Hearn's proposal\u2077 that a majority of miners be able to vote to\n> confiscate the earnings of the minority and redistribute them at will.\n>\n>\n> Installation\n> ------------\n>\n> Once you've compiled the replace-by-fee-v0.10.1 branch just run your\n> node normally. With -debug logging enabled, you'll see messages like the\n> following in your ~/.bitcoin/debug.log indicating your node is replacing\n> transactions with higher-fee paying double-spends:\n>\n>     2015-02-12 05:45:20 replacing tx\n> ca07cc2a5eaf55ab13be7ed7d7526cb9d303086f116127608e455122263f93ea with\n> c23973c08d71cdadf3a47bae45566053d364e77d21747ae7a1b66bf1dffe80ea for\n> 0.00798 BTC additional fees, -1033 delta bytes\n>\n> Additionally you can tell if you are connected to other replace-by-fee\n> nodes, or Bitcoin XT nodes, by examining the service bits advertised by\n> your peers:\n>\n>     $ bitcoin-cli getpeerinfo | grep services | egrep\n> '((0000000000000003)|(0000000004000001))'\n>             \"services\" : \"0000000000000003\",\n>             \"services\" : \"0000000004000001\",\n>             \"services\" : \"0000000004000001\",\n>             \"services\" : \"0000000000000003\",\n>             \"services\" : \"0000000004000001\",\n>             \"services\" : \"0000000004000001\",\n>             \"services\" : \"0000000000000003\",\n>             \"services\" : \"0000000000000003\",\n>\n> Replace-by-fee nodes advertise service bit 26 from the experimental use\n> range; Bitcoin XT nodes advertise service bit 1 for their getutxos\n> support. The code sets aside a certain number of outgoing and incoming\n> slots just for double-spend relaying nodes, so as long as everything is\n> working you're node should be connected to like-minded nodes a within 30\n> minutes or so of starting up.\n>\n> If you *don't* want to advertise the fact that you are running a\n> replace-by-fee node, just checkout a slightly earlier commit in git; the\n> actual mempool changes are separate from the preferential peering\n> commits. You can then connect directly to a replace-by-fee node using\n> the -addnode command line flag.\n>\n> 1) https://github.com/bitcoinxt/bitcoinxt\n> 2) https://github.com/bitcoin/bitcoin/pull/3883\n> 3) https://github.com/bitcoin/bitcoin/pull/3883#issuecomment-45543370\n> 4) https://github.com/luke-jr/bitcoin/tree/0.10.x-ljrP\n> 5)\n> http://www.mail-archive.com/bitcoin-development%40lists.sourceforge.net/msg05211.html\n> 6)\n> http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg06970.html\n> 7)\n> http://www.mail-archive.com/bitcoin-development%40lists.sourceforge.net/msg04972.html\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 0000000000000000059a3dd65f0e5ffb8fdf316d6f31921fefcf0ef726120be9\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150504/2d3b72a5/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "New release of replace-by-fee for Bitcoin Core v0.10.1",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Kevin Greene",
                "Peter Todd"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 10304
        }
    },
    {
        "title": "[Bitcoin-development] Replace-by-fee v0.10.2 - Serious DoS attack fixed! - Also novel variants of existing attacks w/ Bitcoin XT and Android Bitcoin Wallet",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-05-23T18:26:21",
                "message_text_only": "My replace-by-fee patch is now available for the Bitcoin Core v0.10.2\nrelease:\n\n    https://github.com/petertodd/bitcoin/tree/replace-by-fee-v0.10.2\n\nThis release fixes a serious DoS attack present in previous releases.\nUpgrading is strongly recommended for relay nodes, and mandatory for\nminers. Users of Luke-Jr's gentoo distribution should either disable RBF\nuntil a patch is released, or run their node behind a patched node.\n\nPreviously replacements that spent outputs the transactions they\nconflicted with would be accepted. This would lead to orphaned\ntransactions in the mempool, a potential bandwidth DoS attack for relay\nnodes, and even worse, on mining nodes would cause Bitcoin to crash when\nCreateNewBlock() was called.\n\nThanks goes to to Suhas Daftuar for finding this issue.\n\n\nAdditionally, while investigating this issue I found that\nAndresen/Harding's relay doublespends patch\u00b9, included in Bitcoin XT\u00b2,\nalso fails to verify that doublespends don't spend outputs of the\ntransactions they conflict with. As the transactions aren't accepted to\nthe mempool the issue is simply a variant of the bandwidth DoS attack\nthat's a well-known issue of Bitcoin XT. However, interestingly in\ntesting I found that Schildbach's Android Bitcoin Wallet\u00b3 fails to\ndetect this case, and displays the transaction as a valid unconfirmed\ntransaction, potentially leading to the user being defrauded with a\ndoublespend.  While a well-known issue in general - Schildbach's\nimplementation trusts peers to only send it valid transactions and\ndoesn't even detect doublespends it receives from peers - it's\ninteresting how in this case the attacker doesn't need to also do a\nsybil attack.\n\n1) https://github.com/bitcoin/bitcoin/pull/3883\n2) https://github.com/bitcoinxt/bitcoinxt\n3) https://play.google.com/store/apps/details?id=de.schildbach.wallet\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000026ca21b4a83e1a818be96db4b532b7e9be2f60d47efff0a\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150523/359a957c/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Replace-by-fee v0.10.2 - Serious DoS attack fixed! - Also novel variants of existing attacks w/ Bitcoin XT and Android Bitcoin Wallet",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Peter Todd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2212
        }
    },
    {
        "title": "[Bitcoin-development] CLTV opcode allocation; long-term plans?",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-05-04T05:07:15",
                "message_text_only": "Matt Corallo brought up\u00b9 the issue of OP_NOP scarcity on the mempool\nonly CLTV pull-req\u00b2:\n\n    \"I like merging this, but doing both CLTV things in one swoop would be\n    really nice. Certainly if we're gonna use one of the precious few\n    OP_NOPs we have we might as well make it more flexible.\"\n\nI have two lines of thought on this:\n\n1) We're going to end up with a Script v2.0 reasonably soon, probably\n   based on Russel O'Connor and Pieter Wuille's Merkelized Abstract Syntax\n   Tree\u00b3 idea. This needs at most a single OP_NOPx to implement and mostly\n   removes the scarcity of upgradable NOP's.\n\n2) Similarly in script v1.0 even if we do use up all ten OP_NOPx's, the\n   logical thing to do is implement an <actual opcode #> OP_EXTENDED.\n\n3) It's not clear what form a relative CLTV will actually take; the BIP\n   itself proposes a OP_PREVOUT_HEIGHT_VERIFY/OP_PREVOUT_DATA along with\n   OP_ADD, with any opcode accessing non-reorg-safe prevout info being made\n   unavailable until the coinbase maturity period has passed for\n   soft-fork safeness.\n\nThat said, if people have strong feelings about this, I would be willing\nto make OP_CLTV work as follows:\n\n    <nLockTime> 1 OP_CLTV\n\nWhere the 1 selects absolute mode, and all others act as OP_NOP's. A\nfuture relative CLTV could then be a future soft-fork implemented as\nfollows:\n\n    <relative nLockTime> 2 OP_CLTV\n\nOn the bad side it'd be two or three days of work to rewrite all the\nexisting tests and example code and update the BIP, and (slightly) gets\nus away from the well-tested existing implementation. It also may\ncomplicate the codebase compared to sticking with just doing a Script\nv2.0, with the additional execution environment data required for v2.0\nscripts cleanly separated out. But all in all, the above isn't too big\nof a deal.\n\nInterested in your thoughts.\n\n1) https://github.com/bitcoin/bitcoin/pull/5496#issuecomment-98568239\n2) https://github.com/bitcoin/bitcoin/pull/5496\n3) http://css.csail.mit.edu/6.858/2014/projects/jlrubin-mnaik-nityas.pdf\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000908b2eb1cb0660069547abdddad7fa6ad4e743cebe549de\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150504/7912b3b9/attachment.sig>"
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-05T00:54:33",
                "message_text_only": "On Mon, May 4, 2015 at 6:07 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> Matt Corallo brought up\u00b9 the issue of OP_NOP scarcity on the mempool\n> only CLTV pull-req\u00b2:\n>\n>     \"I like merging this, but doing both CLTV things in one swoop would be\n>     really nice. Certainly if we're gonna use one of the precious few\n>     OP_NOPs we have we might as well make it more flexible.\"\n>\n> [snip]\n\n> That said, if people have strong feelings about this, I would be willing\n> to make OP_CLTV work as follows:\n>\n>     <nLockTime> 1 OP_CLTV\n>\n> Where the 1 selects absolute mode, and all others act as OP_NOP's. A\n> future relative CLTV could then be a future soft-fork implemented as\n> follows:\n>\n>     <relative nLockTime> 2 OP_CLTV\n>\n> On the bad side it'd be two or three days of work to rewrite all the\n> existing tests and example code and update the BIP, and (slightly) gets\n> us away from the well-tested existing implementation. It also may\n> complicate the codebase compared to sticking with just doing a Script\n> v2.0, with the additional execution environment data required for v2.0\n> scripts cleanly separated out. But all in all, the above isn't too big\n> of a deal.\n\n\nAdding a parameter to OP_CLTV makes it much more flexible and is the most\neconomic use of precious NOPs.\nThe extra time required is ok and it would be good to make this change to\nthe PR in time for the feature freeze.\n\nDrak\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150505/677b8096/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-09T09:12:01",
                "message_text_only": "On Tue, May 05, 2015 at 01:54:33AM +0100, Btc Drak wrote:\n> > That said, if people have strong feelings about this, I would be willing\n> > to make OP_CLTV work as follows:\n> >\n> >     <nLockTime> 1 OP_CLTV\n> >\n> > Where the 1 selects absolute mode, and all others act as OP_NOP's. A\n> > future relative CLTV could then be a future soft-fork implemented as\n> > follows:\n> >\n> >     <relative nLockTime> 2 OP_CLTV\n> >\n> > On the bad side it'd be two or three days of work to rewrite all the\n> > existing tests and example code and update the BIP, and (slightly) gets\n> > us away from the well-tested existing implementation. It also may\n> > complicate the codebase compared to sticking with just doing a Script\n> > v2.0, with the additional execution environment data required for v2.0\n> > scripts cleanly separated out. But all in all, the above isn't too big\n> > of a deal.\n> \n> \n> Adding a parameter to OP_CLTV makes it much more flexible and is the most\n> economic use of precious NOPs.\n> The extra time required is ok and it would be good to make this change to\n> the PR in time for the feature freeze.\n\nDone!\n\nhttps://github.com/bitcoin/bitcoin/pull/5496#issuecomment-100454263\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000012c438a597ad15df697888be579f4f818a30517cd60fbdc8\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/a5c24bb8/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-12T19:16:46",
                "message_text_only": "This saves us ocodes for later but it's uglier and produces slightly\nbigger scripts.\nIf we're convinced it's worth it, seems like the right way to do it,\nand certainly cltv and rclv/op_maturity are related.\nBut let's not forget that we can always use this same trick with the\nlast opcode to get 2^64 brand new opcodes.\nSo I'm not convinced at all on whether we want  #5496 or #6124.\nBut it would be nice to decide and stop blocking  this.\n\nOn Sat, May 9, 2015 at 11:12 AM, Peter Todd <pete at petertodd.org> wrote:\n> On Tue, May 05, 2015 at 01:54:33AM +0100, Btc Drak wrote:\n>> > That said, if people have strong feelings about this, I would be willing\n>> > to make OP_CLTV work as follows:\n>> >\n>> >     <nLockTime> 1 OP_CLTV\n>> >\n>> > Where the 1 selects absolute mode, and all others act as OP_NOP's. A\n>> > future relative CLTV could then be a future soft-fork implemented as\n>> > follows:\n>> >\n>> >     <relative nLockTime> 2 OP_CLTV\n>> >\n>> > On the bad side it'd be two or three days of work to rewrite all the\n>> > existing tests and example code and update the BIP, and (slightly) gets\n>> > us away from the well-tested existing implementation. It also may\n>> > complicate the codebase compared to sticking with just doing a Script\n>> > v2.0, with the additional execution environment data required for v2.0\n>> > scripts cleanly separated out. But all in all, the above isn't too big\n>> > of a deal.\n>>\n>>\n>> Adding a parameter to OP_CLTV makes it much more flexible and is the most\n>> economic use of precious NOPs.\n>> The extra time required is ok and it would be good to make this change to\n>> the PR in time for the feature freeze.\n>\n> Done!\n>\n> https://github.com/bitcoin/bitcoin/pull/5496#issuecomment-100454263\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 000000000000000012c438a597ad15df697888be579f4f818a30517cd60fbdc8\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-12T19:23:22",
                "message_text_only": "I have no strong opinion, but a slight preference for separate opcodes.\n\nReason: given the current progress, they'll likely be deployed\nindependently, and maybe the end result is not something that cleanly fits\nthe current CLTV argument structure.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/4bfcc543/attachment.html>"
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-12T19:30:35",
                "message_text_only": "Gavin and @NicolasDorier have a point: If there isn't actually scarcity of\nNOPs because OP_NOP10 could become <type> OP_EX (if we run out), it makes\nsense to chose the original unparameterised CLTV version #6124 which also\nhas been better tested. It's cleaner, more readable and results in a\nslightly smaller script which has also got to be a plus.\n\nOn Tue, May 12, 2015 at 8:16 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> This saves us ocodes for later but it's uglier and produces slightly\n> bigger scripts.\n> If we're convinced it's worth it, seems like the right way to do it,\n> and certainly cltv and rclv/op_maturity are related.\n> But let's not forget that we can always use this same trick with the\n> last opcode to get 2^64 brand new opcodes.\n> So I'm not convinced at all on whether we want  #5496 or #6124.\n> But it would be nice to decide and stop blocking  this.\n>\n> On Sat, May 9, 2015 at 11:12 AM, Peter Todd <pete at petertodd.org> wrote:\n> > On Tue, May 05, 2015 at 01:54:33AM +0100, Btc Drak wrote:\n> >> > That said, if people have strong feelings about this, I would be\n> willing\n> >> > to make OP_CLTV work as follows:\n> >> >\n> >> >     <nLockTime> 1 OP_CLTV\n> >> >\n> >> > Where the 1 selects absolute mode, and all others act as OP_NOP's. A\n> >> > future relative CLTV could then be a future soft-fork implemented as\n> >> > follows:\n> >> >\n> >> >     <relative nLockTime> 2 OP_CLTV\n> >> >\n> >> > On the bad side it'd be two or three days of work to rewrite all the\n> >> > existing tests and example code and update the BIP, and (slightly)\n> gets\n> >> > us away from the well-tested existing implementation. It also may\n> >> > complicate the codebase compared to sticking with just doing a Script\n> >> > v2.0, with the additional execution environment data required for v2.0\n> >> > scripts cleanly separated out. But all in all, the above isn't too big\n> >> > of a deal.\n> >>\n> >>\n> >> Adding a parameter to OP_CLTV makes it much more flexible and is the\n> most\n> >> economic use of precious NOPs.\n> >> The extra time required is ok and it would be good to make this change\n> to\n> >> the PR in time for the feature freeze.\n> >\n> > Done!\n> >\n> > https://github.com/bitcoin/bitcoin/pull/5496#issuecomment-100454263\n> >\n> > --\n> > 'peter'[:-1]@petertodd.org\n> > 000000000000000012c438a597ad15df697888be579f4f818a30517cd60fbdc8\n> >\n> >\n> ------------------------------------------------------------------------------\n> > One dashboard for servers and applications across Physical-Virtual-Cloud\n> > Widest out-of-the-box monitoring support with 50+ applications\n> > Performance metrics, stats and reports that give you Actionable Insights\n> > Deep dive visibility with transaction tracing using APM Insight.\n> > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/941146d0/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-05-12T20:38:27",
                "message_text_only": "It should actually be straightforward to softfork RCLTV in as a negative CLTV.\nAll nLockTime are >= any negative number, so a negative number makes CLTV a \nno-op always. Therefore, it is clean to define negative numbers as relative \nlater. It's also somewhat obvious to developers, since negative numbers often \nimply an offset (eg, negative list indices in Python).\n\nLuke"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-12T21:01:25",
                "message_text_only": "On Tue, May 12, 2015 at 08:38:27PM +0000, Luke Dashjr wrote:\n> It should actually be straightforward to softfork RCLTV in as a negative CLTV.\n> All nLockTime are >= any negative number, so a negative number makes CLTV a \n> no-op always. Therefore, it is clean to define negative numbers as relative \n> later. It's also somewhat obvious to developers, since negative numbers often \n> imply an offset (eg, negative list indices in Python).\n\nDoing this makes handling the year 2038 problem a good deal more\ncomplex.\n\nThe CLTV codebase specifically fails on negative arguments to avoid any\nambiguity or implementation differences here.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000e7980aab9c096c46e7f34c43a661c5cb2ea71525ebb8af7\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/40ff0fa9/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-13T00:38:44",
                "message_text_only": "I like the reuse with negative numbers more than the current proposal\nbecause it doesn't imply bigger scripts. If all problems that may\narise can be solved, that is.\nIf we went that route, we would start with the initial CLTV too.\nBut I don't see many strong arguments in favor of using the current\ntrick later when we're actually running out of opcodes, just that\n\"CLTV and RCLTV/op_maturity are semantically related\". How\nsemantically related depends on the final form of RCLTV/op_maturity,\nbut I don't think anybody wants to block CLTV until RCLTV is ready.\n\nSo we could just deploy the initial CLTV (#6124) now and then decide\nwhether we want to reuse it with negatives for RCLTV or if we use an\nindependent op.\nCan the people that don't like that plan give stronger arguments in\nfavor of the parametrized version?\n\nI've missed IRC conversations, so I may be missing something...\n\n\nOn Tue, May 12, 2015 at 11:01 PM, Peter Todd <pete at petertodd.org> wrote:\n> On Tue, May 12, 2015 at 08:38:27PM +0000, Luke Dashjr wrote:\n>> It should actually be straightforward to softfork RCLTV in as a negative CLTV.\n>> All nLockTime are >= any negative number, so a negative number makes CLTV a\n>> no-op always. Therefore, it is clean to define negative numbers as relative\n>> later. It's also somewhat obvious to developers, since negative numbers often\n>> imply an offset (eg, negative list indices in Python).\n>\n> Doing this makes handling the year 2038 problem a good deal more\n> complex.\n>\n> The CLTV codebase specifically fails on negative arguments to avoid any\n> ambiguity or implementation differences here.\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 00000000000000000e7980aab9c096c46e7f34c43a661c5cb2ea71525ebb8af7\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Rusty Russell",
                "date": "2015-05-07T01:35:47",
                "message_text_only": "Peter Todd <pete at petertodd.org> writes:\n> That said, if people have strong feelings about this, I would be willing\n> to make OP_CLTV work as follows:\n>\n>     <nLockTime> 1 OP_CLTV\n>\n> Where the 1 selects absolute mode, and all others act as OP_NOP's. A\n> future relative CLTV could then be a future soft-fork implemented as\n> follows:\n>\n>     <relative nLockTime> 2 OP_CLTV\n\nMildly prefer to put that the other way around.\n\nie. the OP_NOP1 becomes OP_EXTENSION_PREFIX, the next op defines which\nextended opcode it is (must be a push).\n\nCheers,\nRusty."
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T17:17:32",
                "message_text_only": "On Thu, May 07, 2015 at 11:05:47AM +0930, Rusty Russell wrote:\n> Peter Todd <pete at petertodd.org> writes:\n> > That said, if people have strong feelings about this, I would be willing\n> > to make OP_CLTV work as follows:\n> >\n> >     <nLockTime> 1 OP_CLTV\n> >\n> > Where the 1 selects absolute mode, and all others act as OP_NOP's. A\n> > future relative CLTV could then be a future soft-fork implemented as\n> > follows:\n> >\n> >     <relative nLockTime> 2 OP_CLTV\n> \n> Mildly prefer to put that the other way around.\n> \n> ie. the OP_NOP1 becomes OP_EXTENSION_PREFIX, the next op defines which\n> extended opcode it is (must be a push).\n\nThere's no good way to implement that option - when the OP_NOPx is\nexecuted all that's available to it without a lot of complex work is\nwhat's already been pushed to the stack, not what will be pushed to the\nstack in the future.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000002761482983864328320badf24d137101fab9a5861a59d30\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/b2acc7b2/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "CLTV opcode allocation; long-term plans?",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Rusty Russell",
                "Peter Todd",
                "Jorge Tim\u00f3n",
                "Btc Drak",
                "Luke Dashjr",
                "Pieter Wuille"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 17101
        }
    },
    {
        "title": "[Bitcoin-development] Block Size Increase",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2015-05-06T22:12:14",
                "message_text_only": "Recently there has been a flurry of posts by Gavin at\nhttp://gavinandresen.svbtle.com/ which advocate strongly for increasing\nthe maximum block size. However, there hasnt been any discussion on this\nmailing list in several years as far as I can tell.\n\nBlock size is a question to which there is no answer, but which\ncertainly has a LOT of technical tradeoffs to consider. I know a lot of\npeople here have varying levels of strong or very strong opinions about\nthis, and the fact that it is not being discussed in a technical\ncommunity publicly anywhere is rather disappointing.\n\nSo, at the risk of starting a flamewar, I'll provide a little bait to\nget some responses and hope the discussion opens up into an honest\ncomparison of the tradeoffs here. Certainly a consensus in this kind of\ntechnical community should be a basic requirement for any serious\ncommitment to blocksize increase.\n\nPersonally, I'm rather strongly against any commitment to a block size\nincrease in the near future. Long-term incentive compatibility requires\nthat there be some fee pressure, and that blocks be relatively\nconsistently full or very nearly full. What we see today are\ntransactions enjoying next-block confirmations with nearly zero pressure\nto include any fee at all (though many do because it makes wallet code\nsimpler).\n\nThis allows the well-funded Bitcoin ecosystem to continue building\nsystems which rely on transactions moving quickly into blocks while\npretending these systems scale. Thus, instead of working on technologies\nwhich bring Bitcoin's trustlessness to systems which scale beyond a\nblockchain's necessarily slow and (compared to updating numbers in a\ndatabase) expensive settlement, the ecosystem as a whole continues to\nfocus on building centralized platforms and advocate for changes to\nBitcoin which allow them to maintain the status quo[1].\n\nMatt\n\n[1] https://twitter.com/coinbase/status/595741967759335426"
            },
            {
                "author": "slush",
                "date": "2015-05-06T22:30:12",
                "message_text_only": "I don't have strong opinion @ block size topic.\n\nBut if there'll be a fork, PLEASE, include SIGHASH_WITHINPUTVALUE (\nhttps://bitcointalk.org/index.php?topic=181734.0) or its alternative. All\ndevelopers of lightweight (blockchain-less) clients will adore you!\n\nslush\n\nOn Thu, May 7, 2015 at 12:12 AM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n> Recently there has been a flurry of posts by Gavin at\n> http://gavinandresen.svbtle.com/ which advocate strongly for increasing\n> the maximum block size. However, there hasnt been any discussion on this\n> mailing list in several years as far as I can tell.\n>\n> Block size is a question to which there is no answer, but which\n> certainly has a LOT of technical tradeoffs to consider. I know a lot of\n> people here have varying levels of strong or very strong opinions about\n> this, and the fact that it is not being discussed in a technical\n> community publicly anywhere is rather disappointing.\n>\n> So, at the risk of starting a flamewar, I'll provide a little bait to\n> get some responses and hope the discussion opens up into an honest\n> comparison of the tradeoffs here. Certainly a consensus in this kind of\n> technical community should be a basic requirement for any serious\n> commitment to blocksize increase.\n>\n> Personally, I'm rather strongly against any commitment to a block size\n> increase in the near future. Long-term incentive compatibility requires\n> that there be some fee pressure, and that blocks be relatively\n> consistently full or very nearly full. What we see today are\n> transactions enjoying next-block confirmations with nearly zero pressure\n> to include any fee at all (though many do because it makes wallet code\n> simpler).\n>\n> This allows the well-funded Bitcoin ecosystem to continue building\n> systems which rely on transactions moving quickly into blocks while\n> pretending these systems scale. Thus, instead of working on technologies\n> which bring Bitcoin's trustlessness to systems which scale beyond a\n> blockchain's necessarily slow and (compared to updating numbers in a\n> database) expensive settlement, the ecosystem as a whole continues to\n> focus on building centralized platforms and advocate for changes to\n> Bitcoin which allow them to maintain the status quo[1].\n>\n> Matt\n>\n> [1] https://twitter.com/coinbase/status/595741967759335426\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/6bdab3de/attachment.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-05-06T23:06:00",
                "message_text_only": "I don\u2019t really have a strong opinion on block size either\u2026but if we\u2019re going to do a hard fork, let\u2019s use this as an opportunity to create a good process for hard forks (which we\u2019ll inevitably need to do again in the future). The change in block size is a very simple change that still allows us to explore all the complexities involved with deployment of hard forks. Let\u2019s not just do a one-off ad-hoc thing.\n\n- Eric Lombrozo\n\n> On May 6, 2015, at 3:30 PM, slush <slush at centrum.cz> wrote:\n> \n> I don't have strong opinion @ block size topic.\n> \n> But if there'll be a fork, PLEASE, include SIGHASH_WITHINPUTVALUE (https://bitcointalk.org/index.php?topic=181734.0 <https://bitcointalk.org/index.php?topic=181734.0>) or its alternative. All developers of lightweight (blockchain-less) clients will adore you!\n> \n> slush\n> \n> On Thu, May 7, 2015 at 12:12 AM, Matt Corallo <bitcoin-list at bluematt.me <mailto:bitcoin-list at bluematt.me>> wrote:\n> Recently there has been a flurry of posts by Gavin at\n> http://gavinandresen.svbtle.com/ <http://gavinandresen.svbtle.com/> which advocate strongly for increasing\n> the maximum block size. However, there hasnt been any discussion on this\n> mailing list in several years as far as I can tell.\n> \n> Block size is a question to which there is no answer, but which\n> certainly has a LOT of technical tradeoffs to consider. I know a lot of\n> people here have varying levels of strong or very strong opinions about\n> this, and the fact that it is not being discussed in a technical\n> community publicly anywhere is rather disappointing.\n> \n> So, at the risk of starting a flamewar, I'll provide a little bait to\n> get some responses and hope the discussion opens up into an honest\n> comparison of the tradeoffs here. Certainly a consensus in this kind of\n> technical community should be a basic requirement for any serious\n> commitment to blocksize increase.\n> \n> Personally, I'm rather strongly against any commitment to a block size\n> increase in the near future. Long-term incentive compatibility requires\n> that there be some fee pressure, and that blocks be relatively\n> consistently full or very nearly full. What we see today are\n> transactions enjoying next-block confirmations with nearly zero pressure\n> to include any fee at all (though many do because it makes wallet code\n> simpler).\n> \n> This allows the well-funded Bitcoin ecosystem to continue building\n> systems which rely on transactions moving quickly into blocks while\n> pretending these systems scale. Thus, instead of working on technologies\n> which bring Bitcoin's trustlessness to systems which scale beyond a\n> blockchain's necessarily slow and (compared to updating numbers in a\n> database) expensive settlement, the ecosystem as a whole continues to\n> focus on building centralized platforms and advocate for changes to\n> Bitcoin which allow them to maintain the status quo[1].\n> \n> Matt\n> \n> [1] https://twitter.com/coinbase/status/595741967759335426 <https://twitter.com/coinbase/status/595741967759335426>\n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y <http://ad.doubleclick.net/ddm/clk/290420510;117567292;y>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net <mailto:Bitcoin-development at lists.sourceforge.net>\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development <https://lists.sourceforge.net/lists/listinfo/bitcoin-development>\n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y_______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150506/a8deea31/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 842 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150506/a8deea31/attachment.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-06T22:44:53",
                "message_text_only": "On Wed, May 6, 2015 at 11:12 PM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n> Personally, I'm rather strongly against any commitment to a block size\n> increase in the near future.\n\n\nMiners can already soft-fork to reduce the maximum block size.  If 51% of\nminers agree to a 250kB block size, then that is the maximum block size.\n\nThe question being discussed is what is the maximum block size merchants\nand users will accept.  This puts a reasonable limit on the maximum size\nminers can increase the block size to.\n\nIn effect, the block size is set by the minimum of the miner's and the\nmerchants/user's size.min(miner, merchants/users).\n\n\n> This allows the well-funded Bitcoin ecosystem to continue building\n> systems which rely on transactions moving quickly into blocks while\n> pretending these systems scale. Thus, instead of working on technologies\n> which bring Bitcoin's trustlessness to systems which scale beyond a\n> blockchain's necessarily slow and (compared to updating numbers in a\n> database) expensive settlement, the ecosystem as a whole continues to\n> focus on building centralized platforms and advocate for changes to\n> Bitcoin which allow them to maintain the status quo[1].\n>\n\nWould you accept a rule that the maximum size is 20MB (doubling every 2\nyears), but that miners have an efficient method for choosing a lower size?\n\nIf miners could specify the maximum block size in their block headers, then\nthey could coordinate to adjust the block size.  If 75% vote to lower the\nsize, then it is lowered and vice versa for raiding.\n\nEvery 2016 blocks, the votes are counter.  If the 504th lowest of the 2016\nblocks is higher than the previous size, then the size is set to that\nsize.  Similarly, if the 504th highest is lower than the previous size, it\nbecomes the new size.\n\nThere could be 2 default trajectories.  The reference client might always\nvote to double the size every 4 years.\n\nTo handle large blocks (>32MB) requires a change to the p2p protocol\nmessage size limits, or a way to split blocks over multiple messages.\n\nIt would be nice to add new features to any hard-fork.\n\nI favour adding an auxiliary header.  The Merkle root in the header could\nbe replaced with hash(merkle_root | hash(aux_header)).  This is a fairly\nsimple change, but helps with things like commitments.  One of the fields\nin the auxiliary header could be an extra nonce field.  This would mean\nfast regeneration of the merkle root for ASIC miners.  This is a pretty\nsimple change.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150506/58ef0a86/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-06T23:12:17",
                "message_text_only": "Replies inline.\n\nOn 05/06/15 22:44, Tier Nolan wrote:\n> On Wed, May 6, 2015 at 11:12 PM, Matt Corallo <bitcoin-list at bluematt.me\n> <mailto:bitcoin-list at bluematt.me>> wrote:\n>     Personally, I'm rather strongly against any commitment to a block size\n>     increase in the near future.\n-snip-\n> The question being discussed is what is the maximum block size merchants\n> and users will accept.  This puts a reasonable limit on the maximum size\n> miners can increase the block size to.\n> \n> In effect, the block size is set by the minimum of the miner's and the\n> merchants/user's size.min(miner, merchants/users).\n\nIndeed, \"the bitcoin community of users and miners can decide to do\nwhatever they want\", but this is univeral - \"they\" could decide whatever\nthey want if \"they\" want to hardfork. That said, \"we\" should be having a\nrigorous technical discussion about whether it is sane to recommend a\ngiven course of action by releasing software which makes it happen.\n\n> \n>     This allows the well-funded Bitcoin ecosystem to continue building\n>     systems which rely on transactions moving quickly into blocks while\n>     pretending these systems scale. Thus, instead of working on technologies\n>     which bring Bitcoin's trustlessness to systems which scale beyond a\n>     blockchain's necessarily slow and (compared to updating numbers in a\n>     database) expensive settlement, the ecosystem as a whole continues to\n>     focus on building centralized platforms and advocate for changes to\n>     Bitcoin which allow them to maintain the status quo[1].\n> \n> \n> Would you accept a rule that the maximum size is 20MB (doubling every 2\n> years), but that miners have an efficient method for choosing a lower size?\n> \n> If miners could specify the maximum block size in their block headers,\n> then they could coordinate to adjust the block size.  If 75% vote to\n> lower the size, then it is lowered and vice versa for raiding.\n> \n> Every 2016 blocks, the votes are counter.  If the 504th lowest of the\n> 2016 blocks is higher than the previous size, then the size is set to\n> that size.  Similarly, if the 504th highest is lower than the previous\n> size, it becomes the new size.\n> \n> There could be 2 default trajectories.  The reference client might\n> always vote to double the size every 4 years.\n> \n> To handle large blocks (>32MB) requires a change to the p2p protocol\n> message size limits, or a way to split blocks over multiple messages.\n> \n> It would be nice to add new features to any hard-fork.\n> \n> I favour adding an auxiliary header.  The Merkle root in the header\n> could be replaced with hash(merkle_root | hash(aux_header)).  This is a\n> fairly simple change, but helps with things like commitments.  One of\n> the fields in the auxiliary header could be an extra nonce field.  This\n> would mean fast regeneration of the merkle root for ASIC miners.  This\n> is a pretty simple change.\n\nThe point of the hard block size limit is exactly because giving miners\nfree rule to do anything they like with their blocks would allow them to\ndo any number of crazy attacks. The incentives for miners to pick block\nsizes are no where near compatible with what allows the network to\ncontinue to run in a decentralized manner."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-06T23:33:56",
                "message_text_only": "On Thu, May 7, 2015 at 12:12 AM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n> The point of the hard block size limit is exactly because giving miners\n> free rule to do anything they like with their blocks would allow them to\n> do any number of crazy attacks. The incentives for miners to pick block\n> sizes are no where near compatible with what allows the network to\n> continue to run in a decentralized manner.\n>\n\nMiners can always reduce the block size (if they coordinate).  Increasing\nthe maximum block size doesn't necessarily cause an increase.  A majority\nof miners can soft-fork to set the limit lower than the hard limit.\n\nSetting the hard-fork limit higher means that a soft fork can be used to\nadjust the limit in the future.\n\nThe reference client would accept blocks above the soft limit for wallet\npurposes, but not build on them.  Blocks above the hard limit would be\nrejected completely.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/b5a9be4e/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-06T23:41:37",
                "message_text_only": "On 05/06/15 23:33, Tier Nolan wrote:\n> On Thu, May 7, 2015 at 12:12 AM, Matt Corallo <bitcoin-list at bluematt.me\n> <mailto:bitcoin-list at bluematt.me>> wrote:\n> \n>     The point of the hard block size limit is exactly because giving miners\n>     free rule to do anything they like with their blocks would allow them to\n>     do any number of crazy attacks. The incentives for miners to pick block\n>     sizes are no where near compatible with what allows the network to\n>     continue to run in a decentralized manner.\n> \n> \n> Miners can always reduce the block size (if they coordinate). \n> Increasing the maximum block size doesn't necessarily cause an\n> increase.  A majority of miners can soft-fork to set the limit lower\n> than the hard limit.\n\nSure, of course.\n\n> Setting the hard-fork limit higher means that a soft fork can be used to\n> adjust the limit in the future. \n> \n> The reference client would accept blocks above the soft limit for wallet\n> purposes, but not build on them.  Blocks above the hard limit would be\n> rejected completely.\n\nYes, but this does NOT make an actual policy. Note that the vast\nmajority of miners already apply their own patches to Bitcoin Core, so\napplying one more is not all that hard. When blocks start to become\nlimited (ie there is any fee left on the table by transactions not\nincluded in a block) there becomes incentive for miners to change that\nbehavior pretty quick. Not just that, the vast majority of the hashpower\nis behind very large miners, who have little to no decentralization\npressure. This results in very incompatible incentives, mainly that the\nincentive would be for the large miners to interconnect in a private\nnetwork and generate only maximum-size blocks, creating a strong\ncentralization pressure in the network."
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T02:16:44",
                "message_text_only": "On Wed, May 06, 2015 at 11:41:37PM +0000, Matt Corallo wrote:\n> Yes, but this does NOT make an actual policy. Note that the vast\n> majority of miners already apply their own patches to Bitcoin Core, so\n> applying one more is not all that hard. When blocks start to become\n> limited (ie there is any fee left on the table by transactions not\n> included in a block) there becomes incentive for miners to change that\n> behavior pretty quick. Not just that, the vast majority of the hashpower\n> is behind very large miners, who have little to no decentralization\n> pressure. This results in very incompatible incentives, mainly that the\n> incentive would be for the large miners to interconnect in a private\n> network and generate only maximum-size blocks, creating a strong\n> centralization pressure in the network.\n\nI'll also point out that miners with the goal of finding more blocks\nthan their competition - a viable long-term strategy to increase market\nshare and/or a short-term strategy to get more transaction fees -\nactually have a perverse incentive(1) to ensure their blocks do *not*\nget to more than ~30% of the hashing power. The main thing holding them\nback from doing that is that the inflation subsidy is still quite high -\nbetter to get the reward now than try to push your competition out of\nbusiness.\n\nIt's plausible that with a limited blocksize there won't be an\nopportunity to delay propagation by broadcasting larger blocks - if\nblocks propagate in a matter of seconds worst case there's no\nopportunity for gaming the system. But it does strongly show that we\nmust build systems where that worst case propagation time in all\ncircumstances is very short relative to the block interval.\n\n1) http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg03200.html\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000004dc867e4541315090329f45ed4dd30e2fd7423a38a72c0e\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150506/1e8954f8/attachment.sig>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-06T23:11:04",
                "message_text_only": "I'm not so much opposed to a block size increase as I am opposed to a hard fork. My problem with a hard fork is that everyone and their brother wants to seize the opportunity of a hard fork to insert their own pet feature, and such a mad rush of lightly considered, obscure feature additions would be extremely risky for Bitcoin. If it could be guaranteed that raising the block size limit would be the only incompatible change introduced in the hard fork, then I would support it, but I strongly fear that the hard fork itself will become an excuse to change other aspects of the system in ways that will have unintended and possibly disastrous consequences."
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-07T00:00:46",
                "message_text_only": "On 5/6/2015 3:12 PM, Matt Corallo wrote:\n> Long-term incentive compatibility requires\n> that there be some fee pressure, and that blocks be relatively\n> consistently full or very nearly full.\n\nI think it's way too early to even consider a future era when the fiat \nvalue of the block reward is no longer the biggest-by-far mining incentive.\n\nCreating fee pressure means driving some people to choose something \nelse, not bitcoin. \"Too many people using bitcoin\" is nowhere on the \nlist of problems today.  It's reckless to tinker with adoption in hopes \nof spurring innovation on speculation, while a \"can kick\" is available.\n\nAdoption is currently at miniscule, test-flight, relatively \ninsignificant levels when compared to global commerce.  As Gavin \ndiscussed in the article, under \"Block size and miner fees\u2026 again,\" the \nbest way to maximize miner incentives is to focus on doing things that \nare likely to increase adoption, which, in our fiat-dominated world, \nlead to a justifiably increased exchange rate.\n\nAny innovation attractive enough to relieve the block size pressure will \ndo so just as well without artificial stimulus.\n\nThanks for kicking off the discussion.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150506/259707d3/attachment.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-05-07T00:07:41",
                "message_text_only": "On Wed, May 6, 2015 at 5:12 PM, Matt Corallo <bitcoin-list at bluematt.me> wrote:\n> the maximum block size. However, there hasnt been any discussion on this\n> mailing list in several years as far as I can tell.\n\nWell, there has been significant public discussion in #bitcoin-wizards\non irc.freenode.net which is available in public logs, specifically\nabout why increasing the max block size is kicking the can down the\nroad while possibly compromising blockchain security. There were many\nexcellent objections that were raised that, sadly, I see are not\nreferenced at all in the recent media blitz. Frankly I can't help but\nfeel that if contributions, like those from #bitcoin-wizards, have\nbeen ignored in lieu of technical analysis, and the absence of\ndiscussion on this mailing list, that I feel perhaps there are other\nsubtle and extremely important technical details that are completely\nabsent from this--and other-- proposals. I have some rather general\nthoughts to offer.\n\nSecured decentralization is the most important and most interesting\nproperty of bitcoin. Everything else is rather trivial and could be\nachieved millions of times more efficiently with conventional\ntechnology. Our technical work should be informed by the technical\nnature of the system we have constructed.\n\nI suspect that as bitcoin continues to grow in all dimensions and\nmetrics, that we will see an unending wave of those who are excited by\nthe idea of Something Different in the face of archaic, crumbling\nsoftware and procedures in the rest of the financial world. Money has\nfound its way into every aspect of human life. There's no doubt in my\nmind that bitcoin will always see the most extreme campaigns and the\nmost extreme misunderstandings. Like moths to a flame or water in the\ndesert, almost everyone is excited by ANY status quo change\nwhatsoever. This is something that we have to be vigilante about,\nbecause their excitement is motivation to do excellent work, not\nsimply any work. For some who are excited about general status quo\nchanges that bitcoin represents, they may not mind if bitcoin\ndecentralization disappears and is replaced with just a handful of\ncentralized nodes. Whereas for development purposes we must hold\nourselves to extremely high standards before proposing changes,\nespecially to the public, that have the potential to be unsafe and\neconomically unsafe. We have examples from NASA about how to engineer\nextremely fault tolerant systems, and we have examples from Linux\nabout how to have high standards in open-source projects. Safety is\nabsolutely critical, even in the face of seemingly irrational\nexcuberance of others who want to scale to trillions of daily coffee\ntransactions individually stored forever in the blockchain.\n\nWhen designing bitcoin or even some other system, an important design\ntarget is what the system should be capable of. How many transactions\nshould the system perform? What is the correct number of transactions\nfor a healthy, modern civilization to perform every day? And how fast\nshould that (not) grow? Should we allow for 2 billion trillion coffee\ntransactions every day, or what about 100 trillion transactions per\nsecond? I suspect that these sorts of questions are entirely\nunanswerable and boring. So in the absence of technical targets to\nreach during the design phase, I suspect that Jeff Garzik was right\nwhen he pointed out a few months ago that bitcoin is good at\nsettlement and clearing. There are many potential technical solutions\nfor aggregating millions (trillions?) of transactions into tiny\nbundles. As a small proof-of-concept, imagine two parties sending\ntransactions back and forth 100 million times. Instead of recording\nevery transaction, you could record the start state and the end state,\nand end up with two transactions or less. That's a 100 million fold,\nwithout modifying max block size and without potentially compromising\nsecured decentralization.\n\nThe MIT group should listen up and get to work figuring out how to\nmeasure decentralization and its security :-). Maybe we should be\ncollectively pestering Andrew Miller to do this, too. No pressure,\ndude. Getting this measurement right would be really beneficial\nbecause we would have a more academic and technical understanding to\nwork with. I would also characterize this as high priority next to the\n\"formally verified correctness proofs for Script and\nlibbitcoinconsensus\".\n\nAlso, I think that getting this out in the open on this mailing list\nis an excellent step forward.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-07T00:37:54",
                "message_text_only": "On Wed, May 6, 2015 at 10:12 PM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote: > Recently there has been a flurry of posts by Gavin at >\nhttp://gavinandresen.svbtle.com/ which advocate strongly for increasing >\nthe maximum block size. However, there hasnt been any discussion on this >\nmailing list in several years as far as I can tell.\n\nThanks Matt; I was actually really confused by this sudden push with\nnot a word here or on Github--so much so that I responded on Reddit to\npeople pointing to commits in Gavin's personal repository saying they\nwere reading too much into it.\n\nSo please forgive me for the more than typical disorganization in this\nmessage; I've been caught a bit flatfooted on this and I'm trying to\ncatch up. I'm juggling a fair amount of sudden pressure in my mailbox,\nand trying to navigate complex discussions in about eight different\nforums concurrently.\n\nThere have been about a kazillion pages of discussion elsewhere\n(e.g. public IRC and Bitcointalk; private discussions in the past),\nnot all of which is well known, and I can't hope to summarize even a\ntiny fraction of it in a single message-- but that's no reason to not\nstart on it.\n\n> Block size is a question to which there is no answer, but which >\ncertainly has a LOT of technical tradeoffs to consider.\n\nThere are several orthogonal angles from which block size is a concern\n(both increases and non-increases). Most of them have subtle implications\nand each are worth its own research paper or six, so it can be difficult\nto only touch them slightly without creating a gish gallop that is hard\nto respond to.\n\nWe're talking about tuning one of the fundamental scarcities of the\nBitcoin Economy and cryptosystem--leaving the comfort of \"rule by\nmath\" and venturing into the space of political decisions; elsewhere\nyou'd expect to see really in-depth neutral analysis of the risks and\ntradeoffs, technically and economically.  And make no mistake: there\nare real tradeoffs here, though we don't know their exact contours.\n\nFundamentally this question exposes ideological differences between people\ninterested in Bitcoin.  Is Bitcoin more of a digital gold or is it more\nof a competitor to Square?  Is Bitcoin something that should improve\npersonal and commercial autonomy from central banks?  From commercial\nbanks? Or from just the existing status-quo commercial banks?   What are\npeople's fundamental rights with Bitcoin?  Do participants have a\nright to mine? How much control should third parties have over their\ntransactions?  How much security must be provided? Is there a deadline\nfor world domination or bust?  Is Bitcoin only for the developed world?\nMust it be totally limited by the most impoverished parts of the world?\n\nBitcoin exists at the intersection of many somewhat overlapping belief\nsystems--and people of many views can find that Bitcoin meets their\nneeds even when they don't completely agree politically.  When Bitcoin\nis changed fundamentally, via a hard fork, to have different properties,\nthe change can create winners or losers (if nothing else, then in terms\nof the kind of ideology supported by it).\n\nThere are non-trivial number of people who hold extremes on any of\nthese general belief patterns; Even among the core developers there is\nnot a consensus on Bitcoin's optimal role in society and the commercial\nmarketplace.\n\nTo make it clear how broad the views go, even without getting into\nmonetary policy... some people even argue that Bitcoin should act\nas censor-resistant storage system for outlawed content; -- I think\nthis view is unsound, not achievable with the technology, and largely\nincompatible with Bitcoin's use as a money (because it potentially\ncreates an externalized legal/harassment liability for node operators);\nbut these are my personal value judgments; the view is earnestly held\nby more than a few; and that's a group that certainly wants the largest\npossible blocksizes (though even then that won't be enough).\n\nThe subject is complicated even more purely on the technical side\nby the fact that Bitcoin has a layered security model which is not\ncompletely defined or understood: Bitcoin is secure if a majority of\nhashrate is \"honest\" (where \"honesty\" is a technical term which means\n\"follows the right rules\" without fail, even at a loss), but why might\nit be honest? That sends us into complex economic and social arguments,\nand the security thresholds start becoming worse when we assume some\nminers are economically rational instead of \"honest\".\n\n> increase in the near future. Long-term incentive compatibility requires\n> that there be some fee pressure, and that blocks be relatively >\nconsistently full or very nearly full. What we see today are\n\nTo elaborate, in my view there is a at least a two fold concern on this\nparticular (\"Long term Mining incentives\") front:\n\nOne is that the long-held argument is that security of the Bitcoin system\nin the long term depends on fee income funding autonomous, anonymous,\ndecentralized miners profitably applying enough hash-power to make\nreorganizations infeasible.\n\nFor fees to achieve this purpose, there seemingly must be an effective\nscarcity of capacity.  The fact that verifying and transmitting\ntransactions has a cost isn't enough, because all the funds go to pay\nthat cost and none to the POW \"artificial\" cost; e.g., if verification\ncosts 1 then the market price for fees should converge to 1, and POW\ncost will converge towards zero because they adapt to whatever is\nbeing applied. Moreover, the transmission and verification costs can\nbe perfectly amortized by using large centralized pools (and efficient\ndifferential block transmission like the \"O(1)\" idea) as you can verify\none time instead of N times, so to the extent that verification/bandwidth\nis a non-negligible cost to miners at all, it's a strong pressure to\ncentralize.  You can understand this intuitively: think for example of\ncarbon credit cap-and-trade: the trade part doesn't work without an\nactual cap; if everyone was born with a 1000 petaton carbon balance,\nthe market price for credits would be zero and the program couldn't hope\nto share behavior. In the case of mining, we're trying to optimize the\nsocial good of POW security. (But the analogy applies in other ways too:\nincreases to the chain side are largely an externality; miners enjoy the\nbenefits, everyone else takes the costs--either in reduced security or\nhigher node operating else.)\n\nThis area has been subject to a small amount of academic research\n(e.g. http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2400519). But\nthere is still much that is unclear.\n\nThe second is that when subsidy has fallen well below fees, the incentive\nto move the blockchain forward goes away.  An optimal rational miner\nwould be best off forking off the current best block in order to capture\nits fees, rather than moving the blockchain forward, until they hit\nthe maximum. That's where the \"backlog\" comment comes from, since when\nthere is a sufficient backlog it's better to go forward.  I'm not aware\nof specific research into this subquestion; it's somewhat fuzzy because\nof uncertainty about the security model. If we try to say that Bitcoin\nshould work even in the face of most miners being profit-maximizing\ninstead of altruistically-honest, we must assume the chain will not\nmore forward so long as a block isn't full.  In reality there is more\naltruism than zero; there are public pressures; there is laziness, etc.\n\nOne potential argument is that maybe miners would be _regulated_ to\nbehave correctly. But this would require undermining the openness of the\nsystem--where anyone can mine anonymously--in order to enforce behavior,\nand that same enforcement mechanism would leave a political level to\nimpose additional rules that violate the extra properties of the system.\n\nSo far the mining ecosystem has become incredibly centralized over time.\nI believe I am the only remaining committer who mines, and only a few\nof the regular contributors to Bitcoin Core do. Many participants\nhave never mined or only did back in 2010/2011... we've basically\nignored the mining ecosystem, and this has had devastating effects,\ncausing a latent undermining of the security model: hacking a dozen or\nso computers--operated under totally unknown and probably not strong\nsecurity policies--could compromise the network at least at the tip...\nRightfully we should be regarding this an an emergency, and probably\nshould have been have since 2011.  This doesn't bode well for our ability\nto respond if a larger blocksize goes poorly. In kicking the can with\nthe trivial change to just bump the size, are we making an implicit\ndecision to go down a path that has a conclusion we don't want?\n\n(There are also shorter term mining incentives concerns; which Peter\nTodd has written more about, that I'll omit for now)\n\n> pretending these systems scale. Thus, instead of working on technologies\n> which bring Bitcoin's trustlessness to systems which scale beyond a\n\nI made a few relevant points back in 2011\n(https://en.bitcoin.it/w/index.php?title=Scalability&action=historysubmit&diff=14273&oldid=14112)\nafter Dan Kaminsky argued that Bitcoin's decentralization was pretext:\nthat it was patently centralized since scaling directly in the network\nwould undermine decentralization, that the Bitcoin network necessarily\nmakes particular tradeoffs which prevent it from concurrently being all\nthings to all people.  But tools like the Lightning network proposal could\nwell allow us to hit a greater spectrum of demands at once--including\nsecure zero-confirmation (something that larger blocksizes reduce if\nanything), which is important for many applications.  With the right\ntechnology I believe we can have our cake and eat it too, but there needs\nto be a reason to build it; the security and decentralization level of\nBitcoin imposes a _hard_ upper limit on anything that can be based on it.\n\nAnother key point here is that the small bumps in blocksize which\nwouldn't clearly knock the system into a largely centralized mode--small\nconstants--are small enough that they don't quantitatively change the\noperation of the system; they don't open up new applications that aren't\npossible today. Deathandtaxes on the forum argued that Bitcoin needs\na several hundred megabyte blocksize to directly meet the worldwide\ntransaction needs _without retail_... Why without retail? Retail needs\nnear instant soft security, which cannot be achieved directly with a\nglobal decentralized blockchain.\n\nI don't think 1MB is magic; it always exists relative to widely-deployed\ntechnology, sociology, and economics. But these factors aren't a simple\nfunction; the procedure I'd prefer would be something like this: if there\nis a standing backlog, we-the-community of users look to indicators to\ngauge if the network is losing decentralization and then double the\nhard limit with proper controls to allow smooth adjustment without\nfees going to zero (see the past proposals for automatic block size\ncontrols that let miners increase up to a hard maximum over the median\nif they mine at quadratically harder difficulty), and we don't increase\nif it appears it would be at a substantial increase in centralization\nrisk. Hardfork changes should only be made if they're almost completely\nuncontroversial--where virtually everyone can look at the available data\nand say \"yea, that isn't undermining my property rights or future use\nof Bitcoin; it's no big deal\".  Unfortunately, every indicator I can\nthink of except fee totals has been going in the wrong direction almost\nmonotonically along with the blockchain size increase since 2012 when\nwe started hitting full blocks and responded by increasing the default\nsoft target.  This is frustrating; from a clean slate analysis of network\nhealth I think my conclusion would be to _decrease_ the limit below the\ncurrent 300k/txn/day level.\n\nThis is obviously not acceptable, so instead many people--myself\nincluded--have been working feverishly hard behind the scenes on Bitcoin\nCore to increase the scalability.  This work isn't small-potatoes\nboring software engineering stuff; I mean even my personal contributions\ninclude things like inventing a wholly new generic algebraic optimization\napplicable to all EC signature schemes that increases performance by 4%,\nand that is before getting into the R&D stuff that hasn't really borne\nfruit yet, like fraud proofs.  Today Bitcoin Core is easily >100 times\nfaster to synchronize and relay than when I first got involved on the\nsame hardware, but these improvements have been swallowed by the growth.\nThe ironic thing is that our frantic efforts to keep ahead and not\nlose decentralization have both not been enough (by the best measures,\nfull node usage is the lowest its been since 2011 even though the user\nbase is huge now) and yet also so much that people could seriously talk\nabout increasing the block size to something gigantic like 20MB. This\nsounds less reasonable when you realize that even at 1MB we'd likely\nhave a smoking hole in the ground if not for existing enormous efforts\nto make scaling not come at a loss of decentralization.\n\n\nI'm curious as to what discussions people have seen; e.g., are people\neven here aware of these concerns? Are you aware of things like the\nhashcash mediated dynamic blocksize limiting?  About proposals like\nlightning network (instant transactions and massive scale, in exchange\nfor some short term DOS risk if a counterparty opts out)?   Do people\n(other than Mike Hearn; I guess) think a future where everyone depends\non a small number of \"Google scale\" node operations for the system is\nactually okay? (I think not, and if so we're never going to agree--but\nit can be helpful to understand when a disagreement is ideological)."
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T01:49:52",
                "message_text_only": "On Wed, May 06, 2015 at 10:12:14PM +0000, Matt Corallo wrote:\n> Personally, I'm rather strongly against any commitment to a block size\n> increase in the near future. Long-term incentive compatibility requires\n> that there be some fee pressure, and that blocks be relatively\n> consistently full or very nearly full. What we see today are\n> transactions enjoying next-block confirmations with nearly zero pressure\n> to include any fee at all (though many do because it makes wallet code\n> simpler).\n\nAgreed.\n\nI'm not sure if you've seen this, but a good paper on this topic was\npublished recently: \"The Economics of Bitcoin Transaction Fees\"\n\n    Abstract\n    --------\n\n    We study the economics of Bitcoin transaction fees in a simple static\n    partial equilibrium model with the specificity that the system security\n    is directly linked to the total computational power of miners. We show\n    that any situation with a fixed fee is equivalent to another situation\n    with a limited block size. In both cases, we give the optimal value of\n    the transaction fee or of the block size. We also show that making the\n    block size a non binding constraint and, in the same time, letting the\n    fee be fixed as the outcome of a decentralized competitive market cannot\n    guarantee the very existence of Bitcoin in the long-term.\n\n-http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2400519\n\nIn short, without either a fixed blocksize or fixed fee per transaction\nBitcoin will will not survive as there is no viable way to pay for PoW\nsecurity. The latter option - fixed fee per transaction - is non-trivial\nto implement in a way that's actually meaningful - it's easy to give\nminers \"kickbacks\" - leaving us with a fixed blocksize.\n\n> This allows the well-funded Bitcoin ecosystem to continue building\n> systems which rely on transactions moving quickly into blocks while\n> pretending these systems scale. Thus, instead of working on technologies\n\nI think this lack of understanding of the limitations of blockchain tech\nis very dangerous, never mind, downright misleading. I keep running into\nstartups at conferences with completely unrealistic ideas about how\nlarge they'll be able to grow their on-blockchain businesses. For\nexample, a few weeks ago at the Stanford blockchain conference I spoke\nto a company planning on using multisig escrow contracts to settle\nfinancial instruments, and expected to be doing about as many\ntransactions/day on the blockchain for their business within a year or\nso as all other Bitcoin users currently do combined. These guys quite\nfrankly had no understanding of the issues, and had apparently based\ntheir plans on the highly optimistic Bitcoin wiki page on\nscalability.(1) (I'd fix this now, but the wiki seems to not be allowing\nlogins)\n\nWe'd do a lot of startups a lot of good to give them accurate, and\nhonest, advice about the scalability of the system. The wiki definitely\nisn't that. Neither is the bitcoin.org developer documentation(2), which\ndoesn't mention scalability at all.\n\n> which bring Bitcoin's trustlessness to systems which scale beyond a\n> blockchain's necessarily slow and (compared to updating numbers in a\n> database) expensive settlement, the ecosystem as a whole continues to\n> focus on building centralized platforms and advocate for changes to\n> Bitcoin which allow them to maintain the status quo[1].\n\nEven a relatively small increase to 20MB will greatly reduce the number\nof people who can participate fully in Bitcoin, creating an environment\nwhere the next increase requires the consent of an even smaller portion\nof the Bitcoin ecosystem. Where does that stop? What's the proposed\nmechanism that'll create an incentive and social consensus to not just\n'kick the can down the road'(3) and further centralize but actually\nscale up Bitcoin the hard way? The only proposal that I've seen that\nattempts to do this is John Dillon's proof-of-stake blocksize vote(4),\nand that is far from getting consensus.\n\n1) https://en.bitcoin.it/wiki/Scalability\n2) https://bitcoin.org/en/developer-guide\n3) http://gavinandresen.ninja/it-must-be-done-but-is-not-a-panacea\n4) http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg02323.html\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000004dc867e4541315090329f45ed4dd30e2fd7423a38a72c0e\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150506/113bc44d/attachment.sig>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T03:03:47",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 03:49 AM, Peter Todd wrote:\n> I'm not sure if you've seen this, but a good paper on this topic\n> was published recently: \"The Economics of Bitcoin Transaction\n> Fees\"\n\n..for some very strange definitions of \"good\".\n\nThat paper may present valid game theory, yet game theory has a\nwell-known limitation when it comes to predicting real world behavior\nin that the predictions are only as good as the simplified model those\npredictions are based on is accurate.\n\nAt the very least, we should wait to draw any conclusions from that\npaper until it has been sanity checked by a praxeological review.\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVStYTAAoJECpf2nDq2eYjqzAQAJkLwVq3cJxaP5MirS6j+SkN\nNuRIQS8EzJkvojZvHCSRz3xPZpl9Cx2T6/hsLjIfzvMuDHKsaOkkLlL0q95ekv4T\nacfami64326DFAxiO0ptspPjCRipagmjSEwZGZwC/QZtTdnt+N9LsH0SFDP6hxbY\nKf11LRd11Ap4v/VnBg/zb4daZnVm0k0nfZxK4rG1zN14r5JEu6eiodUBZc6e4qih\nLmopoddIwJS4MY1GoR2kCehAbJseZZyQQmHFEX1Vhc74ETGXWApfgF0tpo6ZMutd\nOT0WGhCpj4yG1u5bRaiNnsOy9WcBTKzDOLZUVVh/GhUGHWUulZu8ujYrX7Q6GR5S\nVPvOOL6Ts/RGEAE1UWKzHfPjrLZAHKgLAzBjm6o1ZXdBcnV+FsThNvd7fxHvaJsO\npWGSu8qDmN/wH657Tphbthb4T/awnuf4rO6oBP+OGu+ydPIlIlt6rM2E4Bq366yy\nCJbzSR3x/P7fRmT2bbSg4rxTDyLFJpNIWOcNaMRBeO69OdNZxlranvFvEl/6FfqK\nGO/LPQiYCe/+yhXgUJzzlYpayPiPFWCg0FxwQ+xl1josTsrfPE4BUivkZvIlqOIY\nLX1fDHt/IIUNp8OUkY2eERxeB//dlY55nP7VGUEJLNnBkXuoBd70lMtGXxtgvw2M\nWy5VER9CiEOUMMwzWi3Q\n=8mit\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/7528c1aa/attachment.bin>"
            },
            {
                "author": "Thomas Zander",
                "date": "2015-05-08T11:02:56",
                "message_text_only": "On Wednesday 6. May 2015 21.49.52 Peter Todd wrote:\n> I'm not sure if you've seen this, but a good paper on this topic was\n> published recently: \"The Economics of Bitcoin Transaction Fees\"\n\n\nThe obvious flaw in this paper is that it talks about a block size in todays \n(trivial) data-flow economy and compares it with the zero-reward situation \ndecades from now.\n\nIts comparing two things that will never exist at the same time (unless \nBitcoin fails).\n-- \nThomas Zander"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-08T20:17:09",
                "message_text_only": "As the author of a popular SPV wallet, I wanted to weigh in, in support of\nthe Gavin's 20Mb block proposal.\n\nThe best argument I've heard against raising the limit is that we need fee\npressure.  I agree that fee pressure is the right way to economize on\nscarce resources. Placing hard limits on block size however is an\nincredibly disruptive way to go about this, and will severely negatively\nimpact users' experience.\n\nWhen users pay too low a fee, they should:\n\n1) See immediate failure as they do now with fees that fail to propagate.\n\n2) If the fee lower than it should be but not terminal, they should see\ndegraded performance, long delays in confirmation, but eventual success.\nThis will encourage them to pay higher fees in future.\n\nThe worst of all worlds would be to have transactions propagate, hang in\nlimbo for days, and then fail. This is the most important scenario to\navoid. Increasing the 1Mb block size limit I think is the simplest way to\navoid this least desirable scenario for the immediate future.\n\nWe can play around with improved transaction selection for blocks and\nencourage miners to adopt it to discourage low fees and create fee\npressure. These could involve hybrid priority/fee selection so low fee\ntransactions see degraded performance instead of failure. This would be the\nconservative low risk approach.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/61a099ec/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-07T03:47:16",
                "message_text_only": "On Thu, May 7, 2015 at 12:12 AM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n> Recently there has been a flurry of posts by Gavin at\n> http://gavinandresen.svbtle.com/ which advocate strongly for increasing\n> the maximum block size. However, there hasnt been any discussion on this\n> mailing list in several years as far as I can tell.\n>\n\nThanks for bringing this up. I'll try to keep my arguments brief, to avoid\na long wall of text. I may be re-iterating some things that have been said\nbefore, though.\n\nI am - in general - in favor of increasing the size blocks: as technology\ngrows, there is no reason why the systems built on them can't scale\nproportionally. I have so far not commented much about this, in a hope to\navoid getting into a public debate, but the way seems to be going now,\nworries me greatly.\n\n* Controversial hard forks. I hope the mailing list here today already\nproves it is a controversial issue. Independent of personal opinions pro or\nagainst, I don't think we can do a hard fork that is controversial in\nnature. Either the result is effectively a fork, and pre-existing coins can\nbe spent once on both sides (effectively failing Bitcoin's primary\npurpose), or the result is one side forced to upgrade to something they\ndislike - effectively giving a power to developers they should never have.\nQuoting someone: \"I did not sign up to be part of a central banker's\ncommittee\".\n\n* The reason for increasing is \"need\". If \"we need more space in blocks\" is\nthe reason to do an upgrade, it won't stop after 20 MB. There is nothing\nfundamental possible with 20 MB blocks that isn't with 1 MB blocks.\nChangetip does not put their microtransactions on the chain, not with 1 MB,\nand I doubt they would with 20 MB blocks. The reason for increase should be\n\"because we choose to accept the trade-offs\".\n\n* Misrepresentation of the trade-offs. You can argue all you want that none\nof the effects of larger blocks are particularly damaging, so everything is\nfine. They will damage something (see below for details), and we should\nanalyze these effects, and be honest about them, and present them as a\ntrade-off made we choose to make to scale the system better. If you just\nask people if they want more transactions, of course you'll hear yes. If\nyou ask people if they want to pay less taxes, I'm sure the vast majority\nwill agree as well.\n\n* Miner centralization. There is currently, as far as I know, no technology\nthat can relay and validate 20 MB blocks across the planet, in a manner\nfast enough to avoid very significant costs to mining. There is work in\nprogress on this (including Gavin's IBLT-based relay, or Greg's block\nnetwork coding), but I don't think we should be basing the future of the\neconomics of the system on undemonstrated ideas. Without those (or even\nwith), the result may be that miners self-limit the size of their blocks to\npropagate faster, but if this happens, larger, better-connected, and more\ncentrally-located groups of miners gain a competitive advantage by being\nable to produce larger blocks. I would like to point out that there is\nnothing evil about this - a simple feedback to determine an optimal block\nsize for an individual miner will result in larger blocks for better\nconnected hash power. If we do not want miners to have this ability, \"we\"\n(as in: those using full nodes) should demand limitations that prevent it.\nOne such limitation is a block size limit (whatever it is).\n\n* Ability to use a full node. I very much dislike the trend of people\nsaying \"we need to encourage people to run full nodes, in order to make the\nnetwork more decentralized\". Running 1000 nodes which are otherwise unused\nonly gives some better ability for full nodes to download the block chain,\nor for SPV nodes to learn about transactions (or be Sybil-attacked...).\nHowever, *using* a full node for validating your business (or personal!)\ntransactions empowers you to using a financial system that requires less\ntrust in *anyone* (not even in a decentralized group of peers) than\nanything else. Moreover, using a full node is what given you power of the\nsystems' rules, as anyone who wants to change it now needs to convince you\nto upgrade. And yes, 20 MB blocks will change people's ability to use full\nnodes, even if the costs are small.\n\n* Skewed incentives for improvements. I think I can personally say that I'm\nresponsible for most of the past years' performance improvements in Bitcoin\nCore. And there is a lot of room for improvement left there - things like\nsilly waiting loops, single-threaded network processing, huge memory sinks,\nlock contention, ... which in my opinion don't nearly get the attention\nthey deserve. This is in addition to more pervasive changes like optimizing\nthe block transfer protocol, support for orthogonal systems with a\ndifferent security/scalability trade-off like Lightning, making full\nvalidation optional, ... Call me cynical, but without actual pressure to\nwork on these, I doubt much will change. Increasing the size of blocks now\nwill simply make it cheap enough to continue business as usual for a while\n- while forcing a massive cost increase (and not just a monetary one) on\nthe entire ecosystem.\n\n* Fees and long-term incentives. I put this last, not because I don't think\nit is not serious, but because I don't understand nearly enough about it.\nI'll let others comment.\n\nI don't think 1 MB is optimal. Block size is a compromise between\nscalability of transactions and verifiability of the system. A system with\n10 transactions per day that is verifiable by a pocket calculator is not\nuseful, as it would only serve a few large bank's settlements. A system\nwhich can deal with every coffee bought on the planet, but requires a\nGoogle-scale data center to verify is also not useful, as it would be\ntrivially out-competed by a VISA-like design. The usefulness needs in a\nbalance, and there is no optimal choice for everyone. We can choose where\nthat balance lies, but we must accept that this is done as a trade-off, and\nthat that trade-off will have costs such as hardware costs, decreasing\nanonymity, less independence, smaller target audience for people able to\nfully validate, ...\n\nChoose wisely.\n\nThanks for reading this,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/29c77115/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T09:25:04",
                "message_text_only": "Hey Matt,\n\nOK, let's get started ....\n\nHowever, there hasnt been any discussion on this\n> mailing list in several years as far as I can tell.\n>\n\nProbably because this list is not a good place for making progress or\nreaching decisions. Those are triggered by pull requests (sometimes).\n\nIf you're wondering \"why now\", that's probably my fault. A few days ago\nWladimir posted a release timeline. I observed to Wladimir and Gavin in\nprivate that this timeline meant a change to the block size was unlikely to\nget into 0.11, leaving only 0.12, which would give everyone only a few\nmonths to upgrade in order to fork the chain by the end of the winter\ngrowth season. That seemed tight.\n\nWladimir did not reply to this email, unfortunately. Perhaps he would like\nthe issue to go away. It won't - if Bitcoin continues on its current growth\ntrends it *will* run out of capacity, almost certainly by some time next\nyear.\n\nWhat we need to see right now is leadership and a plan, that fits in the\navailable time window.\n\n\n> Certainly a consensus in this kind of technical community should be a\n> basic requirement for any serious commitment to blocksize increase.\n>\n\nI'm afraid I have come to disagree. I no longer believe this community can\nreach consensus on anything protocol related. Some of these arguments have\ndragged on for years. Consensus isn't even well defined - consensus of who?\nAnyone who shows up? And what happens when, inevitably, no consensus is\nreached? Stasis forever?\n\n\n> Long-term incentive compatibility requires that there be some fee\n> pressure, and that blocks be relatively consistently full or very nearly\n> full.\n\n\nI disagree. When the money supply eventually dwindles I doubt it will be\nfee pressure that funds mining, but as that's a long time in the future,\nit's very hard to predict what might happen.\n\n\n> What we see today are\n> transactions enjoying next-block confirmations with nearly zero pressure\n> to include any fee at all (though many do because it makes wallet code\n> simpler).\n>\n\nMany do because free transactions are broken - the relay limiter means\nwhether a free transaction actually makes it across the network or not is\nbasically pot luck and there's no way for a wallet to know, short of either\ntrying it or actually receiving every single transaction and repeating the\ncalculations. If free transactions weren't broken for all non-full nodes\nthey'd probably be used a lot more.\n\n\n> This allows the well-funded Bitcoin ecosystem to continue building\n> systems which rely on transactions moving quickly into blocks while\n> pretending these systems scale.\n\n\nI have two huge problems with this line of thinking.\n\nFirstly, no, the \"Bitcoin ecosystem\" is not well funded. Blockstream might\nbe, but significant numbers of users are running programs developed by tiny\nstartups, or volunteers who don't have millions in venture capital to play\nwith.\n\nArm-twisting \"the ecosystem\" into developing complicated Rube Goldberg\nmachines in double quick time, just to keep the Bitcoin show on the road,\nis in fact the opposite of decentralisation - it will effectively exclude\nanyone who isn't able to raise large amounts of corporate funding from\nwriting code that uses the Bitcoin network. Decentralisation benefits from\nsimplicity, and bigger blocks are (in Gavin's words) \"the simplest thing\nthat will work\".\n\nMy second problem is the claim that everyone is playing pretend about\nBitcoin, except you guys. I would put it another way - I would say those\npeople are building products and getting users, by making reasonable\nengineering tradeoffs and using systems that work. Yes, one day those\nsystems might have to change. That's the nature of scaling. It's the nature\nof progress. But not today. Probably not tomorrow either.\n\nWhat I would like to see from Blockstream is a counter-proposal. So far you\nhave made lots of vague comments that we all agree with - yes,\ndecentralisation is good, yes some block size limit must exist, if only\nbecause computers are finite machines.\n\nWhat I don't see from you yet is a *specific and credible plan* that fits\nwithin the next 12 months and which allows Bitcoin to keep growing. Not\nsome vague handwave like \"let's all use the Lightning network\" (which does\nnot exist), or \"let's do more research\" (Gavin has done plenty of\nresearch), or \"but what about the risks\" (Bitcoin is full of risks). A\nplan, with dates attached, and a strong chance of actually being deployed\nin time.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/81942b17/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T10:12:50",
                "message_text_only": "On Thu, May 07, 2015 at 11:25:04AM +0200, Mike Hearn wrote:\n> > Certainly a consensus in this kind of technical community should be a\n> > basic requirement for any serious commitment to blocksize increase.\n> >\n> \n> I'm afraid I have come to disagree. I no longer believe this community can\n> reach consensus on anything protocol related. Some of these arguments have\n> dragged on for years. Consensus isn't even well defined - consensus of who?\n> Anyone who shows up? And what happens when, inevitably, no consensus is\n> reached? Stasis forever?\n\nCare to be specific?\n\nWe've made lots of protocol related changes, as well as non-consensus\npolicy changes, often in quite short timeframes, and with little drama.\nFor instance BIP66 adopting is progressing smoothly, and itself was very\nquickly developed as part of a broader response to a serious OpenSSL\nflaw. My own BIP65 is getting wide consensus with little drama and good\npeer review, and that's happening even without as much attention paid to\nit from myself as I should have been giving it. The BIP62 malleability\nsoftfork is going more slowly, but that's because peer review is finding\nissues and fixing them - something to be expected in an environment\nwhere we simply must be cautious.\n\nAs for the v0.11 release, it will have pruning, perhaps the biggest\nchange to the way Bitcoin Core works that we've ever made. Equally it's\nnotable how many people collaborated on the implementation of pruning,\nagain with little drama.\n\nSure, some stuff has been hard to get consensus on. But those things\ncarry high risks, and involve code and practices known to be dangerous.\nIn most cases we've found out the lack of consensus was spot on, and\ncontroversial changes turn out later to have severe security\nvulnerabilities. I read that as a sign that the peer review and\nconsensus building process works just fine.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000af0c4ba9d91c00d48c4493899d7235fd819ac76f16d148d\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/9dc82f9c/attachment.sig>"
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-07T10:42:19",
                "message_text_only": "On Thu, May 7, 2015 at 10:25 AM, Mike Hearn <mike at plan99.net> wrote:\n\n> What I don't see from you yet is a *specific and credible plan* that fits\n> within the next 12 months and which allows Bitcoin to keep growing. Not\n> some vague handwave like \"let's all use the Lightning network\" (which does\n> not exist), or \"let's do more research\" (Gavin has done plenty of\n> research), or \"but what about the risks\" (Bitcoin is full of risks). A\n> plan, with dates attached, and a strong chance of actually being deployed\n> in time.\n>\n\nWould you please explain where this 12 months timeframe comes from?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/55cd10b4/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-07T10:52:26",
                "message_text_only": "On Thu, May 7, 2015 at 11:25 AM, Mike Hearn <mike at plan99.net> wrote:\n> I observed to Wladimir and Gavin in private that this timeline meant a change to the block size was unlikely to get into 0.11, leaving only 0.12, which would give everyone only a few months to upgrade in order to fork the chain by the end of the winter growth season. That seemed tight.\n\nCan you please elaborate on what terrible things will happen if we\ndon't increase the block size by winter this year?\nI assume that you are expecting full blocks by then, have you used any\nstatistical technique to come up with that date or is it just your\nguess?\nBecause I love wild guesses and mine is that full 1 MB blocks will not\nhappen until June 2017.\n\n> What we need to see right now is leadership and a plan, that fits in the\n> available time window.\n>\n>>\n>> Certainly a consensus in this kind of technical community should be a\n>> basic requirement for any serious commitment to blocksize increase.\n>\n>\n> I'm afraid I have come to disagree. I no longer believe this community can\n> reach consensus on anything protocol related. Some of these arguments have\n> dragged on for years. Consensus isn't even well defined - consensus of who?\n> Anyone who shows up? And what happens when, inevitably, no consensus is\n> reached? Stasis forever?\n\nWe've successfully reached consensus for several softfork proposals already.\nI agree with others that hardfork need to be uncontroversial and there\nshould be consensus about them.\nIf you have other ideas for the criteria for hardfork deployment all I'm ears.\nI just hope that by  \"What we need to see right now is leadership\" you\ndon't mean something like \"when Gaving and Mike agree it's enough to\ndeploy a hardfork\" when you go from vague to concrete.\n\n\n>> Long-term incentive compatibility requires that there be some fee\n>> pressure, and that blocks be relatively consistently full or very nearly\n>> full.\n>\n>\n> I disagree. When the money supply eventually dwindles I doubt it will be fee\n> pressure that funds mining, but as that's a long time in the future, it's\n> very hard to predict what might happen.\n\nOh, so your answer to \"bitcoin will eventually need to live on fees\nand we would like to know more about how it will look like then\" it's\n\"no bitcoin long term it's broken long term but that's far away in the\nfuture so let's just worry about the present\".\nI agree that it's hard to predict that future, but having some\ncompetition for block space would actually help us get more data on a\nsimilar situation to be able to predict that future better.\nWhat you want to avoid at all cost (the block size actually being\nused), I see as the best opportunity we have to look into the future.\n\n>> What we see today are\n>> transactions enjoying next-block confirmations with nearly zero pressure\n>> to include any fee at all (though many do because it makes wallet code\n>> simpler).\n>\n>\n> Many do because free transactions are broken - the relay limiter means\n> whether a free transaction actually makes it across the network or not is\n> basically pot luck and there's no way for a wallet to know, short of either\n> trying it or actually receiving every single transaction and repeating the\n> calculations. If free transactions weren't broken for all non-full nodes\n> they'd probably be used a lot more.\n\nFree transactions are a gift from miners that run an altruistic policy.\nThat's great but we shouldn't rely on them for the future. They will\nlikely disappear at some point and that's ok.\nIn any case, he's not complaining about the lack of free transactions,\nmore like the opposite.\nHe is saying that's very easy to get free transactions in the next\nblock and blocks aren't full so there's no incentive to include fees\nto compete for the space.\nWe can talk a lot about \"a fee market\" and build a theoretically\nperfect fee estimator but we won't actually have a fee market until\nthere's some competition for space.\nNobody will pay for space that's abundant just like people don't pay\nfor the air they breath.\n\n> What I don't see from you yet is a specific and credible plan that fits\n> within the next 12 months and which allows Bitcoin to keep growing. Not some\n> vague handwave like \"let's all use the Lightning network\" (which does not\n> exist), or \"let's do more research\" (Gavin has done plenty of research), or\n> \"but what about the risks\" (Bitcoin is full of risks). A plan, with dates\n> attached, and a strong chance of actually being deployed in time.\n\nOk, this is my plan: we wait 12 months, hope that your estimations are\ncorrect (in case that my guess was better than yours, we keep waiting\nuntil June 2017) and start having full blocks and people having to\nwait 2 blocks for their transactions to be confirmed some times.\nThat would be the beginning of a true \"fee market\", something that\nGavin used to say was his #1 priority not so long ago (which seems\ncontradictory with his current efforts to avoid that from happening).\nHaving a true fee market seems clearly an advantage.\nWhat are supposedly disastrous negative parts of this plan that make\nan alternative plan (ie: increasing the block size) so necessary and\nobvious.\nI think the advocates of the size increase are failing to explain the\ndisadvantages of maintaining the current size. It feels like the\nexplanation are missing because it should be somehow obvious how the\nsky will burn if we don't increase the block size soon.\nBut, well, it is not obvious to me, so please elaborate on why having\na fee market (instead of just an price estimator for a market that\ndoesn't even really exist) would be a disaster."
            },
            {
                "author": "Andrew",
                "date": "2015-05-07T11:15:57",
                "message_text_only": "I'm mainly just an observer on this. I mostly agree with Pieter. Also, I\nthink the main reason why people like Gavin and Mike Hearn are trying to\nrush this through is because they have some kind of \"apps\" that depend on\nzero conf instant transactions, so this would of course require more\ntraffic on the blockchain. I think people like Gavin or Mike should state\nclearly what kind of (rigorous) system for instant transactions is\nsatisfactory for use in their applications. Be it lightning or something\nsimilar, what is good enough? And no zero conf is not a real secure system.\nThen once we know what is good enough for them (and everyone else), we can\nimplement it as a soft fork into the protocol, and it's a win win situation\nfor both sides (we can also benefit from all the new users people like Mike\nare trying bring in).\n\nOn Thu, May 7, 2015 at 10:52 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> On Thu, May 7, 2015 at 11:25 AM, Mike Hearn <mike at plan99.net> wrote:\n> > I observed to Wladimir and Gavin in private that this timeline meant a\n> change to the block size was unlikely to get into 0.11, leaving only 0.12,\n> which would give everyone only a few months to upgrade in order to fork the\n> chain by the end of the winter growth season. That seemed tight.\n>\n> Can you please elaborate on what terrible things will happen if we\n> don't increase the block size by winter this year?\n> I assume that you are expecting full blocks by then, have you used any\n> statistical technique to come up with that date or is it just your\n> guess?\n> Because I love wild guesses and mine is that full 1 MB blocks will not\n> happen until June 2017.\n>\n> > What we need to see right now is leadership and a plan, that fits in the\n> > available time window.\n> >\n> >>\n> >> Certainly a consensus in this kind of technical community should be a\n> >> basic requirement for any serious commitment to blocksize increase.\n> >\n> >\n> > I'm afraid I have come to disagree. I no longer believe this community\n> can\n> > reach consensus on anything protocol related. Some of these arguments\n> have\n> > dragged on for years. Consensus isn't even well defined - consensus of\n> who?\n> > Anyone who shows up? And what happens when, inevitably, no consensus is\n> > reached? Stasis forever?\n>\n> We've successfully reached consensus for several softfork proposals\n> already.\n> I agree with others that hardfork need to be uncontroversial and there\n> should be consensus about them.\n> If you have other ideas for the criteria for hardfork deployment all I'm\n> ears.\n> I just hope that by  \"What we need to see right now is leadership\" you\n> don't mean something like \"when Gaving and Mike agree it's enough to\n> deploy a hardfork\" when you go from vague to concrete.\n>\n>\n> >> Long-term incentive compatibility requires that there be some fee\n> >> pressure, and that blocks be relatively consistently full or very nearly\n> >> full.\n> >\n> >\n> > I disagree. When the money supply eventually dwindles I doubt it will be\n> fee\n> > pressure that funds mining, but as that's a long time in the future, it's\n> > very hard to predict what might happen.\n>\n> Oh, so your answer to \"bitcoin will eventually need to live on fees\n> and we would like to know more about how it will look like then\" it's\n> \"no bitcoin long term it's broken long term but that's far away in the\n> future so let's just worry about the present\".\n> I agree that it's hard to predict that future, but having some\n> competition for block space would actually help us get more data on a\n> similar situation to be able to predict that future better.\n> What you want to avoid at all cost (the block size actually being\n> used), I see as the best opportunity we have to look into the future.\n>\n> >> What we see today are\n> >> transactions enjoying next-block confirmations with nearly zero pressure\n> >> to include any fee at all (though many do because it makes wallet code\n> >> simpler).\n> >\n> >\n> > Many do because free transactions are broken - the relay limiter means\n> > whether a free transaction actually makes it across the network or not is\n> > basically pot luck and there's no way for a wallet to know, short of\n> either\n> > trying it or actually receiving every single transaction and repeating\n> the\n> > calculations. If free transactions weren't broken for all non-full nodes\n> > they'd probably be used a lot more.\n>\n> Free transactions are a gift from miners that run an altruistic policy.\n> That's great but we shouldn't rely on them for the future. They will\n> likely disappear at some point and that's ok.\n> In any case, he's not complaining about the lack of free transactions,\n> more like the opposite.\n> He is saying that's very easy to get free transactions in the next\n> block and blocks aren't full so there's no incentive to include fees\n> to compete for the space.\n> We can talk a lot about \"a fee market\" and build a theoretically\n> perfect fee estimator but we won't actually have a fee market until\n> there's some competition for space.\n> Nobody will pay for space that's abundant just like people don't pay\n> for the air they breath.\n>\n> > What I don't see from you yet is a specific and credible plan that fits\n> > within the next 12 months and which allows Bitcoin to keep growing. Not\n> some\n> > vague handwave like \"let's all use the Lightning network\" (which does not\n> > exist), or \"let's do more research\" (Gavin has done plenty of research),\n> or\n> > \"but what about the risks\" (Bitcoin is full of risks). A plan, with dates\n> > attached, and a strong chance of actually being deployed in time.\n>\n> Ok, this is my plan: we wait 12 months, hope that your estimations are\n> correct (in case that my guess was better than yours, we keep waiting\n> until June 2017) and start having full blocks and people having to\n> wait 2 blocks for their transactions to be confirmed some times.\n> That would be the beginning of a true \"fee market\", something that\n> Gavin used to say was his #1 priority not so long ago (which seems\n> contradictory with his current efforts to avoid that from happening).\n> Having a true fee market seems clearly an advantage.\n> What are supposedly disastrous negative parts of this plan that make\n> an alternative plan (ie: increasing the block size) so necessary and\n> obvious.\n> I think the advocates of the size increase are failing to explain the\n> disadvantages of maintaining the current size. It feels like the\n> explanation are missing because it should be somehow obvious how the\n> sky will burn if we don't increase the block size soon.\n> But, well, it is not obvious to me, so please elaborate on why having\n> a fee market (instead of just an price estimator for a market that\n> doesn't even really exist) would be a disaster.\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/1bc393f5/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T11:29:44",
                "message_text_only": ">\n> Can you please elaborate on what terrible things will happen if we\n> don't increase the block size by winter this year?\n>\n\nI was referring to winter next year. 0.12 isn't scheduled until the end of\nthe year, according to Wladimir. I explained where this figure comes from\nin this article:\n\nhttps://medium.com/@octskyward/bitcoin-s-seasonal-affective-disorder-35733bab760d\n\nIt's a fairly simple estimate based on previous growth patterns.\n\nBecause I love wild guesses and mine is that full 1 MB blocks will not\n> happen until June 2017.\n>\n\nOK, it could be. But do you think this debate will play out significantly\ndifferently if you are right, I am wrong, and we have this discussion next\nsummer instead? Because in several years of watching these debates, I\nhaven't seen much change in them.\n\n\n> We've successfully reached consensus for several softfork proposals\n> already.\n>\n\nAre you sure about that?\n\nWhat if Gavin popped up right now and said he disagreed with every current\nproposal, he disagreed with side chains too, and there would be no\nconsensus on any of them until the block size limit was raised.\n\nWould you say, oh, OK, guess that's it then. There's no consensus so might\nas well scrap all those proposals, as they'll never happen anyway. Bye bye\nside chains whitepaper.\n\n\n\n> I just hope that by  \"What we need to see right now is leadership\" you\n> don't mean something like \"when Gaving and Mike agree it's enough to\n> deploy a hardfork\" when you go from vague to concrete.\n>\n\nNo. What I meant is that someone (theoretically Wladimir) needs to make a\nclear decision. If that decision is \"Bitcoin Core will wait and watch the\nfireworks when blocks get full\", that would be showing leadership .....\nalbeit I believe in the wrong direction. It would, however, let people know\nwhat's what and let them start to make longer term plans.\n\nThis dillydallying around is an issue - people just make vague points that\ncan't really be disagreed with (more nodes would be nice, smaller pools\nwould also be nice etc), and nothing gets done.\n\n\n> \"no bitcoin long term it's broken long term but that's far away in the\n> future so let's just worry about the present\".\n>\n\nI never said Bitcoin is broken in the long term. Far from it - I laid out\nmy ideas for what will happen when the block subsidy dwindles years ago.\n\nBut yes, it's hard for me to care overly much about what happens 30 years\nfrom now, for the same reason you probably care more about what happens\ntomorrow than what happens after you are dead. The further into the future\nyou try and plan, the less likely your plans are to survive unscathed.\n\n\n> What you want to avoid at all cost (the block size actually being\n> used), I see as the best opportunity we have to look into the future.\n>\n\nI think I see one of the causes of disagreement now.\n\nI will write more on the topic of what will happen if we hit the block size\nlimit soon, maybe this evening. I have some other tasks to do first.\n\nRegardless, I don't believe we will get any useful data out of such an\nevent. I've seen distributed systems run out of capacity before. What will\nhappen instead is technological failure followed by rapid user abandonment\nthat pushes traffic back below the pressure threshold .... and those users\nwill most likely not come back any time soon.\n\n\n> Ok, this is my plan: we wait 12 months, hope that your estimations are\n> correct (in case that my guess was better than yours, we keep waiting\n> until June 2017) and start having full blocks and people having to\n> wait 2 blocks for their transactions to be confirmed some times.\n>\n\nI disagree that'd be the outcome, but good, this is progress. Now we need\nto hear something like that from Wladimir, or whoever has the final say\naround here.\n\nWith respect to the fee market: I think it's fairer to say Gavin wants a\nmarket to exist, and he also wants supply to be plentiful. 20mb limit\ndoesn't actually mean every block will be 20mb the day after, no more than\nthey're all 1mb today. Miners may discover that if they go beyond 5mb they\nhave too many orphans and then propagation speed will have to be optimised\nto break through the next bottleneck. Scaling is always about finding the\nnext bottleneck and removing it, ideally, before you hit it.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/c1f84cb2/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-07T12:26:10",
                "message_text_only": "On Thu, May 7, 2015 at 1:29 PM, Mike Hearn <mike at plan99.net> wrote:\n> I was referring to winter next year. 0.12 isn't scheduled until the end of\n> the year, according to Wladimir. I explained where this figure comes from in\n> this article:\n>\n> https://medium.com/@octskyward/bitcoin-s-seasonal-affective-disorder-35733bab760d\n>\n> It's a fairly simple estimate based on previous growth patterns.\n\nOk, thanks.\n\n>> We've successfully reached consensus for several softfork proposals\n>> already.\n>\n>\n> Are you sure about that?\n\nYes, Peter Todd gave more details.\n\n> What if Gavin popped up right now and said he disagreed with every current\n> proposal, he disagreed with side chains too, and there would be no consensus\n> on any of them until the block size limit was raised.\n>\n> Would you say, oh, OK, guess that's it then. There's no consensus so might\n> as well scrap all those proposals, as they'll never happen anyway. Bye bye\n> side chains whitepaper.\n\nWell, yes, it is true that \"universally uncontroversial\" (which is\nwhat I think the requirement should be for hard forks) is a vague\nqualifier that's not formally defined anywhere.\nI guess we should only consider rational arguments. You cannot just\nnack something without further explanation.\nIf his explanation was \"I will change my mind after we increase block\nsize\", I guess the community should say \"then we will just ignore your\nnack because it makes no sense\".\nIn the same way, when people use fallacies (purposely or not) we must\nexpose that and say \"this fallacy doesn't count as an argument\".\nBut yeah, it would probably be good to define better what constitutes\na \"sensible objection\" or something. That doesn't seem simple though.\n\n>> I just hope that by  \"What we need to see right now is leadership\" you\n>> don't mean something like \"when Gaving and Mike agree it's enough to\n>> deploy a hardfork\" when you go from vague to concrete.\n>\n>\n> No. What I meant is that someone (theoretically Wladimir) needs to make a\n> clear decision. If that decision is \"Bitcoin Core will wait and watch the\n> fireworks when blocks get full\", that would be showing leadership .....\n> albeit I believe in the wrong direction. It would, however, let people know\n> what's what and let them start to make longer term plans.\n>\n> This dillydallying around is an issue - people just make vague points that\n> can't really be disagreed with (more nodes would be nice, smaller pools\n> would also be nice etc), and nothing gets done.\n\nWell, there's two different things here.\nOne thing is the Bitcoin core project where you could argue that the 5\ncommitters decide (I don't know why Wladimir would have any more\nauthority than the others).\nBut what the bitcoin network itself does it's very different because\nunlike the bitcoin core software project, the Bitcoin network is\ndecentralized.\nIf the people with commit access go nuts and decide something that's\nclearly stupid or evil, people can just fork the project because it is\nfree software.\nYou cannot be forced to use specific features of free software, you\ncan always remove them and recompile, that's the whole point.\nSo, no, there's no authority to decide on hardforks and that's why I\nthink that only clearly uncontroversial things can get through as\nhardforks.\n\n>> What you want to avoid at all cost (the block size actually being\n>> used), I see as the best opportunity we have to look into the future.\n>\n>\n> I think I see one of the causes of disagreement now.\n>\n> I will write more on the topic of what will happen if we hit the block size\n> limit soon, maybe this evening. I have some other tasks to do first.\n>\n> Regardless, I don't believe we will get any useful data out of such an\n> event. I've seen distributed systems run out of capacity before. What will\n> happen instead is technological failure followed by rapid user abandonment\n> that pushes traffic back below the pressure threshold .... and those users\n> will most likely not come back any time soon.\n\nOk, so in simple terms, you expect people to have to pay enormous fees\nand/or wait thousands of blocks for their transactions to get included\nin the chain.\nIs that correct?\n\n>> Ok, this is my plan: we wait 12 months, hope that your estimations are\n>> correct (in case that my guess was better than yours, we keep waiting\n>> until June 2017) and start having full blocks and people having to\n>> wait 2 blocks for their transactions to be confirmed some times.\n>\n>\n> I disagree that'd be the outcome, but good, this is progress. Now we need to\n> hear something like that from Wladimir, or whoever has the final say around\n> here.\n\nAs said above there's no authority to decide on what Bitcoin the p2p\nnetwork does. Again, that's the whole point.\nBut, yes, I agree that both sides understanding each other better is progress.\n\n> With respect to the fee market: I think it's fairer to say Gavin wants a\n> market to exist, and he also wants supply to be plentiful. 20mb limit\n> doesn't actually mean every block will be 20mb the day after, no more than\n> they're all 1mb today. Miners may discover that if they go beyond 5mb they\n> have too many orphans and then propagation speed will have to be optimised\n> to break through the next bottleneck. Scaling is always about finding the\n> next bottleneck and removing it, ideally, before you hit it.\n\nI'm sure he wants a fee market to eventually exist as well.\nBut it seems that some people would like to see that happening before\nthe subsidies are low (not necessarily null), while other people are\nfine waiting for that but don't want to ever be close to the scale\nlimits anytime soon.\nI would also like to know for how long we need to prioritize short\nterm adoption in this way. As others have said, if the answer is\n\"forever, adoption is always the most important thing\" then we will\nend up with an improved version of Visa.\nBut yeah, this is progress, I'll wait for your more detailed\ndescription of the tragedies that will follow hitting the block\nlimits, assuming for now that it will happen in 12 months.\nMy previous answer to the nervous \"we will hit the block limits in 12\nmonths if we don't do anything\" was \"not sure about 12 months, but\nwhatever, great, I'm waiting for that to observe how fees get\naffected\".\nBut it should have been a question \"what's wrong with hitting the\nblock limits in 12 months?\""
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T14:05:41",
                "message_text_only": ">\n> If his explanation was \"I will change my mind after we increase block\n>\nsize\", I guess the community should say \"then we will just ignore your\n> nack because it makes no sense\".\n>\n\nOh good! We can just kick anyone out of the consensus process if we think\nthey make no sense.\n\nI guess that means me and Gavin can remove everyone else from the developer\nconsensus, because we think trying to stop Bitcoin growing makes no sense.\n\nDo you see the problem with this whole notion? It cannot possibly work.\nWhenever you try and make the idea of developer consensus work, what you\nend up with is \"I believe in consensus as long as it goes my way\". Which is\nworthless.\n\n\n> One thing is the Bitcoin core project where you could argue that the 5\n> committers decide (I don't know why Wladimir would have any more\n> authority than the others).\n>\n\nBecause he is formally the maintainer.\n\nMaybe you dislike that idea. It's so .... centralised. So let's say Gavin\ncommits his patch, because his authority is equal to all other committers.\nSomeone else rolls it back. Gavin sets up a cron job to keep committing the\npatch. Game over.\n\nYou cannot have committers fighting over what goes in and what doesn't.\nThat's madness. There must be a single decision maker for any given\ncodebase.\n\n\n> Ok, so in simple terms, you expect people to have to pay enormous fees\n> and/or wait thousands of blocks for their transactions to get included\n> in the chain. Is that correct?\n>\n\nNo. I'll write an article like the others, it's better than email for more\ncomplicated discourse.\n\nAs others have said, if the answer is \"forever, adoption is always the most\n> important thing\" then we will end up with an improved version of Visa.\n>\n\nThis appears to be another one of those fundamental areas of disagreement.\nI believe there is no chance of Bitcoin ending up like Visa, even if it is\nwildly successful. I did the calculations years ago that show that won't\nhappen:\n\n    https://en.bitcoin.it/wiki/Scalability\n\nDecentralisation is a spectrum and Bitcoin will move around on that\nspectrum over time. But claiming we have to pick between 1mb blocks and\n\"Bitcoin = VISA\" is silly.\n\n\n\nPeter:   your hypocrisy really is bottomless, isn't it? You constantly\nclaim to be a Righteous Defender of Privacy, but don't even hesitate before\npublishing hacked private emails when it suits you.\n\nSatoshi's hacker had no illusions about your horrible personality, which is\nwhy he forwarded that email to you specifically. He knew you'd use it. You\nshould reflect on that fact. It says nothing good about you at all.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/2785795e/attachment.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-05-07T14:18:17",
                "message_text_only": "On Thu, May 7, 2015 at 9:05 AM, Mike Hearn <mike at plan99.net> wrote:\n> Maybe you dislike that idea. It's so .... centralised. So let's say Gavin\n> commits his patch, because his authority is equal to all other committers.\n> Someone else rolls it back. Gavin sets up a cron job to keep committing the\n> patch. Game over.\n>\n> You cannot have committers fighting over what goes in and what doesn't.\n> That's madness. There must be a single decision maker for any given\n> codebase.\n\nHmm, git repositories don't quite work like that. Instead, you should\nimagine everyone having a local copy of the git repository. Each\ndeveloper synchronizes their git repository with other developers.\nThey merge changes from specific remote branches that they have\nreceived. Each developer has their own branch and each developer is\nthe \"single decision maker\" for the artifact that they compile.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T14:22:24",
                "message_text_only": "On Thu, May 07, 2015 at 04:05:41PM +0200, Mike Hearn wrote:\n> Peter:   your hypocrisy really is bottomless, isn't it? You constantly\n> claim to be a Righteous Defender of Privacy, but don't even hesitate before\n> publishing hacked private emails when it suits you.\n>\n> Satoshi's hacker had no illusions about your horrible personality, which is\n> why he forwarded that email to you specifically. He knew you'd use it. You\n> should reflect on that fact. It says nothing good about you at all.\n\nAs you know I was forwarded that email first, and because I *do* respect\nyour privacy I consulting with you via private IRC chat first, and as\nyou wished I didn't publish it. The hacker presumably gave up waiting\nfor me to do so and published it themselves seven months ago; to make\nthat clear I linked the source(1) of the email in my message. Those\nemails simply are no longer private.\n\nFrankly personal attacks like this - \"your hypocrisy really is\nbottomless, isn't it?\", \"Satoshi's hacker had no illusions about your\nhorrible personality\" - simply don't belong on this mailing list and I\nthink we would all appreciate an apology.\n\n1) https://www.reddit.com/r/Bitcoin/comments/2g9c0j/satoshi_email_leak/\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000012a3e40d5ee5c7fc2fb8367b720a9d499468ceb25366c1f3\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/0c0461b8/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T14:40:12",
                "message_text_only": "On Thu, May 07, 2015 at 04:05:41PM +0200, Mike Hearn wrote:\n> > One thing is the Bitcoin core project where you could argue that the 5\n> > committers decide (I don't know why Wladimir would have any more\n> > authority than the others).\n> >\n> \n> Because he is formally the maintainer.\n\nI quite liked Wladimir's description of what someone with the ability\nto merge pull requests into Bitcoin Core is:\n\n    @orionwl github.com/bitcoin/bitcoin repository admin, or maybe just \"janitor\"\n\n-https://twitter.com/orionwl/status/563688293737697281\n\nIn any case, we can't force people to run Bitcoin Core - an unpopular\npatch that fails to reach consensus is a strong sign that it may not get\nuser acceptance either - so we might as well accept that centralized\nauthority over the development process isn't going to fly and deal with\nthe sometimes messy consequences.\n\nLike I said, you're welcome to fork the project and try to get user\nacceptance for the fork.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000013e67b343b1f6d75cc87dfb54430bdb3bcf66d8d4b3ef6b8\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/cd705b97/attachment.sig>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-07T14:52:54",
                "message_text_only": "For reference: the blog post that (re)-started this debate, and which links\nto individual issues, is here:\n  http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n\nIn it, I asked people to email me objections I might have missed. I would\nstill appreciate it if people do that; it is impossible to keep up with\nthis mailing list, /r/bitcoin posts and comments, and #bitcoin-wizards and\nalso have time to respond thoughtfully to the objections raised.\n\nI would very much like to find some concrete course of action that we can\ncome to consensus on. Some compromise so we can tell entrepreneurs \"THIS is\nhow much transaction volume the main Bitcoin blockchain will be able to\nsupport over the next eleven years.\"\n\nI've been pretty clear on what I think is a reasonable compromise (a\none-time increase scheduled for early next year), and I have tried to\nexplain why I think it it is the right set of tradeoffs.\n\nThere ARE tradeoffs here, and the hard question is what process do we use\nto decide those tradeoffs?  How do we come to consensus? Is it worth my\ntime to spend hours responding thoughtfully to every new objection raised\nhere, or will the same thing happen that happened last year and the year\nbefore-- everybody eventually gets tired of arguing\nangels-dancing-on-the-head-of-a-pin, and we're left with the status quo?\n\nI AM considering contributing some version of the bigger blocksize-limit\nhard-fork patch to the Bitcoin-Xt fork (probably  \"target a hobbyist with a\nfast Internet connection, and assume Nelson's law to increase over time),\nand then encouraging merchants and exchanges and web wallets and\nindividuals who think it strikes a reasonable balance to run it.\n\nAnd then, assuming it became a super-majority of nodes on the network,\nencourage miners to roll out a soft-fork to start producing bigger blocks\nand eventually trigger the hard fork.\n\nBecause ultimately consensus comes down to what software people choose to\nrun.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/125b7e32/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T14:56:58",
                "message_text_only": "On Thu, May 07, 2015 at 10:52:54AM -0400, Gavin Andresen wrote:\n> I AM considering contributing some version of the bigger blocksize-limit\n> hard-fork patch to the Bitcoin-Xt fork (probably  \"target a hobbyist with a\n> fast Internet connection, and assume Nelson's law to increase over time),\n> and then encouraging merchants and exchanges and web wallets and\n> individuals who think it strikes a reasonable balance to run it.\n> \n> And then, assuming it became a super-majority of nodes on the network,\n> encourage miners to roll out a soft-fork to start producing bigger blocks\n> and eventually trigger the hard fork.\n\nWould you please explain what you mean by \"a soft-fork to start\nproducing bigger blocks\"\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000d49f263bbbb80f264abc7cc930fc9cbc7ba80ac068d9648\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/2223f818/attachment.sig>"
            },
            {
                "author": "Alex Morcos",
                "date": "2015-05-07T15:04:25",
                "message_text_only": "That strikes me as a dangerous path forward.\n\nI don't actually think there is anything wrong with this: \"everybody\neventually gets tired of arguing angels-dancing-on-the-head-of-a-pin, and\nwe're left with the status quo\"\n\nWhat gives Bitcoin value aren't its technical merits but the fact that\npeople believe in it.   The biggest risk here isn't that 20MB blocks will\nbe bad or that 1MB blocks will be bad, but that by forcing a hard fork that\nisn't nearly universally agreed upon, we will be damaging that belief.   If\nI strongly believed some hard fork would be better for Bitcoin, say\npermanent inflation of 1% a year to fund mining, and I managed to convince\n80% of users, miners, businesses and developers to go along with me, I\nwould still vote against doing it.  Because that's not nearly universal\nagreement, and it changes what people chose to believe in without their\nconsent. Forks should be hard, very hard.  And both sides should recognize\nthat belief in the value of Bitcoin might be a fragile thing.   I'd argue\nthat if we didn't force through a 20MB fork now, and we ran into major\nnetwork difficulties a year from now and had no other technical solutions,\nthat maybe we would get nearly universal agreement, and the businesses and\nusers that were driven away by the unusable system would be a short term\nloss in value considerably smaller than the impairment we risk by forcing a\nchange.\n\n\n\nOn Thu, May 7, 2015 at 10:52 AM, Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> For reference: the blog post that (re)-started this debate, and which\n> links to individual issues, is here:\n>   http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n>\n> In it, I asked people to email me objections I might have missed. I would\n> still appreciate it if people do that; it is impossible to keep up with\n> this mailing list, /r/bitcoin posts and comments, and #bitcoin-wizards and\n> also have time to respond thoughtfully to the objections raised.\n>\n> I would very much like to find some concrete course of action that we can\n> come to consensus on. Some compromise so we can tell entrepreneurs \"THIS is\n> how much transaction volume the main Bitcoin blockchain will be able to\n> support over the next eleven years.\"\n>\n> I've been pretty clear on what I think is a reasonable compromise (a\n> one-time increase scheduled for early next year), and I have tried to\n> explain why I think it it is the right set of tradeoffs.\n>\n> There ARE tradeoffs here, and the hard question is what process do we use\n> to decide those tradeoffs?  How do we come to consensus? Is it worth my\n> time to spend hours responding thoughtfully to every new objection raised\n> here, or will the same thing happen that happened last year and the year\n> before-- everybody eventually gets tired of arguing\n> angels-dancing-on-the-head-of-a-pin, and we're left with the status quo?\n>\n> I AM considering contributing some version of the bigger blocksize-limit\n> hard-fork patch to the Bitcoin-Xt fork (probably  \"target a hobbyist with a\n> fast Internet connection, and assume Nelson's law to increase over time),\n> and then encouraging merchants and exchanges and web wallets and\n> individuals who think it strikes a reasonable balance to run it.\n>\n> And then, assuming it became a super-majority of nodes on the network,\n> encourage miners to roll out a soft-fork to start producing bigger blocks\n> and eventually trigger the hard fork.\n>\n> Because ultimately consensus comes down to what software people choose to\n> run.\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/30afcdf6/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T15:09:18",
                "message_text_only": "100% agree, RE hard forks should be hard.\n\nHowever, it is the paradox of growth, morale and adoption that bitcoin\nmight never reach the point where it is saturated & expensive to the point\nwhere larger blocks are demanded by 95%+...  simply because people and\ncompanies chose not to adopt bitcoin in the first place due to an unmoving,\n[perceived | real] scalability roadblock.\n\n\nOn Thu, May 7, 2015 at 11:04 AM, Alex Morcos <morcos at gmail.com> wrote:\n\n> That strikes me as a dangerous path forward.\n>\n> I don't actually think there is anything wrong with this: \"everybody\n> eventually gets tired of arguing angels-dancing-on-the-head-of-a-pin, and\n> we're left with the status quo\"\n>\n> What gives Bitcoin value aren't its technical merits but the fact that\n> people believe in it.   The biggest risk here isn't that 20MB blocks will\n> be bad or that 1MB blocks will be bad, but that by forcing a hard fork that\n> isn't nearly universally agreed upon, we will be damaging that belief.   If\n> I strongly believed some hard fork would be better for Bitcoin, say\n> permanent inflation of 1% a year to fund mining, and I managed to convince\n> 80% of users, miners, businesses and developers to go along with me, I\n> would still vote against doing it.  Because that's not nearly universal\n> agreement, and it changes what people chose to believe in without their\n> consent. Forks should be hard, very hard.  And both sides should recognize\n> that belief in the value of Bitcoin might be a fragile thing.   I'd argue\n> that if we didn't force through a 20MB fork now, and we ran into major\n> network difficulties a year from now and had no other technical solutions,\n> that maybe we would get nearly universal agreement, and the businesses and\n> users that were driven away by the unusable system would be a short term\n> loss in value considerably smaller than the impairment we risk by forcing a\n> change.\n>\n>\n>\n> On Thu, May 7, 2015 at 10:52 AM, Gavin Andresen <gavinandresen at gmail.com>\n> wrote:\n>\n>> For reference: the blog post that (re)-started this debate, and which\n>> links to individual issues, is here:\n>>   http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n>>\n>> In it, I asked people to email me objections I might have missed. I would\n>> still appreciate it if people do that; it is impossible to keep up with\n>> this mailing list, /r/bitcoin posts and comments, and #bitcoin-wizards and\n>> also have time to respond thoughtfully to the objections raised.\n>>\n>> I would very much like to find some concrete course of action that we can\n>> come to consensus on. Some compromise so we can tell entrepreneurs \"THIS is\n>> how much transaction volume the main Bitcoin blockchain will be able to\n>> support over the next eleven years.\"\n>>\n>> I've been pretty clear on what I think is a reasonable compromise (a\n>> one-time increase scheduled for early next year), and I have tried to\n>> explain why I think it it is the right set of tradeoffs.\n>>\n>> There ARE tradeoffs here, and the hard question is what process do we use\n>> to decide those tradeoffs?  How do we come to consensus? Is it worth my\n>> time to spend hours responding thoughtfully to every new objection raised\n>> here, or will the same thing happen that happened last year and the year\n>> before-- everybody eventually gets tired of arguing\n>> angels-dancing-on-the-head-of-a-pin, and we're left with the status quo?\n>>\n>> I AM considering contributing some version of the bigger blocksize-limit\n>> hard-fork patch to the Bitcoin-Xt fork (probably  \"target a hobbyist with a\n>> fast Internet connection, and assume Nelson's law to increase over time),\n>> and then encouraging merchants and exchanges and web wallets and\n>> individuals who think it strikes a reasonable balance to run it.\n>>\n>> And then, assuming it became a super-majority of nodes on the network,\n>> encourage miners to roll out a soft-fork to start producing bigger blocks\n>> and eventually trigger the hard fork.\n>>\n>> Because ultimately consensus comes down to what software people choose to\n>> run.\n>>\n>> --\n>> --\n>> Gavin Andresen\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/435a17c4/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T15:12:38",
                "message_text_only": ">\n> What gives Bitcoin value aren't its technical merits but the fact that\n> people believe in it.\n>\n\nMuch of the belief in Bitcoin is that it has a bright future. Certainly the\nhuge price spikes we've seen were not triggered by equally large spikes in\nusage - it's speculation on that future.\n\nI quite agree that if people stop believing in Bitcoin, that will be bad. A\nfast way to bring that about will be to deliberately cripple the technology\nin order to force people onto something quite different (which probably\nwon't be payment channel networks).\n\n\n> I'd argue that if we didn't force through a 20MB fork now, and we ran into\n> major network difficulties a year from now and had no other technical\n> solutions, that maybe we would get nearly universal agreement\n>\n\nI doubt it. The disagreement seems more philosophical than technical. If\nBitcoin fell off a cliff then that'd just be taken as more evidence that\nblock chains don't work and we should all use some network of payment hubs,\nor whatever the fashion of the day is. Or anyone who doesn't want to pay\nhigh fees is unimportant. See all the other justifications Gavin is working\nhis way through on his blog.\n\nThat's why I conclude the opposite - if there is no fork, then people's\nconfidence in Bitcoin will be seriously damaged. If it's impossible to do\nsomething as trivial as removing a temporary hack Satoshi put in place,\nthen what about bigger challenges? If the community is really willing to\ndrive itself off a cliff due to political deadlock, then why bother\nbuilding things that use Bitcoin at all?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/56a1fd00/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T15:17:03",
                "message_text_only": "On Thu, May 7, 2015 at 11:12 AM, Mike Hearn <mike at plan99.net> wrote:\n\n>\n> That's why I conclude the opposite - if there is no fork, then people's\n> confidence in Bitcoin will be seriously damaged.\n>\n\nYes, that is a possibility.\n\n\n\n> If it's impossible to do something as trivial as removing a temporary hack\n> Satoshi put in place, then what about bigger challenges?\n>\n\nThis is absolutely not a trivial change.\n\nIt is a trivial *code* change.  It is not a trivial change to the economics\nof a $3.2B system.\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/9ef5c02d/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T15:29:10",
                "message_text_only": ">\n> It is a trivial *code* change.  It is not a trivial change to the\n> economics of a $3.2B system.\n>\n\nHmm - again I'd argue the opposite.\n\nUp until now Bitcoin has been unconstrained by the hard block size limit.\n\nIf we raise it, Bitcoin will continue to be unconstrained by it. That's the\ndefault \"continue as we are\" position.\n\nIf it's not raised, then ....... well, then we're in new territory\nentirely. Businesses built on the assumption that Bitcoin could become\npopular will suddenly have their basic assumptions invalidated. Users will\nleave. The technical code change would be zero, but the economic change\nwould be significant.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/29da17a8/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T15:35:47",
                "message_text_only": "Yes - but you must recognize that is precisely 50% of the picture.\n\nOthers have made different assumptions - taking the [1MB-constrained]\nmarket *as it exists today*, rather than in some projected future.\n\nRaising the block size limit then becomes a *human decision* to favor some\nusers over others, a *human decision* to prevent an active and competitive\nfree fee market developing at 1MB, a *human decision* to keep transaction\nfees low to incentivize bitcoin adoption, a *human decision* to value\nadoption over decentralization.\n\nThese statements are not value judgements - not saying you are wrong -\nthese are observations of some rather huge, relevant blind spots in this\ndebate.\n\n\n\n\n\nOn Thu, May 7, 2015 at 11:29 AM, Mike Hearn <mike at plan99.net> wrote:\n\n> It is a trivial *code* change.  It is not a trivial change to the\n>> economics of a $3.2B system.\n>>\n>\n> Hmm - again I'd argue the opposite.\n>\n> Up until now Bitcoin has been unconstrained by the hard block size limit.\n>\n> If we raise it, Bitcoin will continue to be unconstrained by it. That's\n> the default \"continue as we are\" position.\n>\n> If it's not raised, then ....... well, then we're in new territory\n> entirely. Businesses built on the assumption that Bitcoin could become\n> popular will suddenly have their basic assumptions invalidated. Users will\n> leave. The technical code change would be zero, but the economic change\n> would be significant.\n>\n\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/a084642c/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T16:18:32",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 03:35 PM, Jeff Garzik wrote:\n> Raising the block size limit then becomes a *human decision* to\n> favor some users over others, a *human decision* to prevent an\n> active and competitive free fee market developing at 1MB, a *human\n> decision* to keep transaction fees low to incentivize bitcoin\n> adoption, a *human decision* to value adoption over\n> decentralization.\n\nAt the moment none of the following assertions have been proven true,\nyet are constantly cited as if they have been:\n\n* A competitive fee market will develop when the transaction rate\nbecomes constrained by the block size limit\n* More users of Bitcoin means less decentralization\n\nFurthermore, the term \"decentralization\" is frequently used without\nbeing precisely defined in a way that would allow for such proofs to\nbe debated.\n\nIf there's going to be a debate on those points, then the people\npresenting points on both sides should take the time to show their\nwork and explain the methodology they used to reach their conclusions.\n\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2\n\niQIcBAEBAgAGBQJVS5BXAAoJECpf2nDq2eYjC3kQAKQ0Jj8r1gjwpl813NiuatjA\nnwXJ+Zn7E+cS8bYsXbaPK1uUgcSdpi/g2jgW+VuUPlqCaNo08Pbp/O7pG5ady9st\no7xJnPxttg7NO3IB7GODCJKK85uBO3dOwPp+pfs8KYCAo5PFTflpeOi4Idbd4w/R\n+tvLynpSX9LIZTQaJH2KEbrYUibYHZrr8hj0net9lJP8KeqMnCuiesYzjJ4pUXyE\nzN0SQ1v9QnpltbTVxRu1TdRBMjAxEHTJPg1jsv0hhGqIOQGHdwNavGq7+LJBen4T\nCvT8ooTmuq0IdihOTttl9ody6Eh0tyGPlbVHiI3c2Emm0HTxz8hN9Rl4lvPgcGdi\nEUW12h8ailKLg5uJL53Zp1PO6fgl0Z/WCx/zqIKRPg4lJMf5Rk5Ow86xAeIZrsbr\nd/+cJZEhqzPnObxkxgTIzqtG8NHcg9dhKw1xkGAkVpMXMM7Bzdku8WCntIYU4+xI\nbtQQZlbc5h/S+X9Vcu0rJWmmQp2Q8xeEVGRh4hhA8LZLc1P+1eyESjAMWvsuq+rk\nWd1kPopekhOgK0zw2j55Ov+kJXVa2pDFA7TOpcqxbdLU4eauKC4D+YQlTM4qj285\nvyRq+c/AwMCPiEhBeEbppgdgwrIQP9fJ7s+2TAHaWICYlTJWkLitUjN9EBwqv3Yp\nLRBrgV7giz8UIrJr3hQZ\n=+Qmg\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18399 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/94cff866/attachment.bin>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-07T16:21:50",
                "message_text_only": "On Thu, May 7, 2015 at 4:52 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> I would very much like to find some concrete course of action that we can\n> come to consensus on. Some compromise so we can tell entrepreneurs \"THIS is\n> how much transaction volume the main Bitcoin blockchain will be able to\n> support over the next eleven years.\"\n\nMhmm, I hadn't thought about this. This makes sense and actually\nexplains the urgency on taking a decision better than anything else\nI've heard.\n\nOn Thu, May 7, 2015 at 5:29 PM, Mike Hearn <mike at plan99.net> wrote:\n> If it's not raised, then ....... well, then we're in new territory entirely.\n> Businesses built on the assumption that Bitcoin could become popular will\n> suddenly have their basic assumptions invalidated. Users will leave. The\n> technical code change would be zero, but the economic change would be\n> significant.\n\nThis, on the other hand, is a non sequitur [1], another type of fallacy.\nWell, several of them, actually:\n\n- If it's not raised, then bitcoin cannot become popular\n- If it's not raised, then users will leave\n- Businesses built on the assumption that Bitcoin could become popular\nwere also assuming that it's going to be risen.\n\nThese statements may even be true, but they're no logical conclusions\neven if they seem obvious to you.\nI don't think those claims are strictly true, specially because they\ninvolve predictions about what people will do.\nBut if they're true they require some proof or at least some explanation.\n\n[1] http://en.wikipedia.org/wiki/Non_sequitur_(logic)#Affirming_the_consequent"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T17:29:56",
                "message_text_only": "On Thu, May 07, 2015 at 06:21:50PM +0200, Jorge Tim\u00f3n wrote:\n> On Thu, May 7, 2015 at 4:52 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> > I would very much like to find some concrete course of action that we can\n> > come to consensus on. Some compromise so we can tell entrepreneurs \"THIS is\n> > how much transaction volume the main Bitcoin blockchain will be able to\n> > support over the next eleven years.\"\n> \n> Mhmm, I hadn't thought about this. This makes sense and actually\n> explains the urgency on taking a decision better than anything else\n> I've heard.\n\nI've spent a lot of time talking to companies about this, and the\nproblem is telling them that isn't actually very useful; knowing the\nsupply side of the equation isn't all that useful if you don't know the\ndemand side. Problem is we don't really have a good handle on what\nBitcoin will be used for in the future, or even for that matter, what\nit's actually being used for right now.\n\nAs we saw with Satoshidice before and quite possibly will see with smart\ncontracts (escrows, futures, etc) it's easy for a relatively small\nnumber of use cases to drive a significant amount of transaction volume.\nYet, as Wladimir and others point out, the fundemental underlying\narchitecture of the blockchain has inherently poor O(n^2) scaling, so\nthere's always some level of demand where it breaks, and/or incentivizes\nactors in the space to push up against \"safety stops\" like soft\nblocksize limits and get them removed.\n\nNote how the response previously to bumping up against soft policy\nlimits was highly public calls(1) at the first hint of touble: \"Mike\nHearn: Soft block size limit reached, action required by YOU\"\n\n1) https://bitcointalk.org/index.php?topic=149668.0\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000002761482983864328320badf24d137101fab9a5861a59d30\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/e4fcb555/attachment.sig>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T19:37:28",
                "message_text_only": "> These statements may even be true, but they're no logical conclusions\n> even if they seem obvious to you.\n> I don't think those claims are strictly true, specially because they\n> involve predictions about what people will do.\n> But if they're true they require some proof or at least some explanation.\n>\n\nThank you for your patience, Jorge.\n\nI have written up an explanation of what I think will happen if we run out\nof capacity:\n\n   https://medium.com/@octskyward/crash-landing-f5cc19908e32\n\nNow I'm going to go eat some dinner :)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/d9fb7d51/attachment.html>"
            },
            {
                "author": "J\u00e9r\u00e9mie Dubois-Lacoste",
                "date": "2015-05-07T19:44:13",
                "message_text_only": "Any proposal to switch to a new hardcorded value so we have time to\n*really* figure out later what's next and all implications, is a road\nto a gigantic issue later when we want to switch to that \"next\".\n\nSure we would have more time to think about, research all\nimplications, simulate, discuss, etc. But the ability then to agree\nenough on a change to roll it out successfully will be much smaller,\nbecause of the economy being built on top of Bitcoin being much larger\nand the technical specifications of Bitcoin being closer to a complete\nfreeze.\n\nWhat I'm trying to say is that we should look at long term lasting\nsolutions even if it takes more effort and time right now and puts the\nnetwork into some \"troubles\" for a while, because they're short term\n\"troubles\". (You define \"troubles\", depending on which side you stand\nat the moment...).\n\nI personally believe in adaptive block size mechanisms, because:\n\n(i) common sense tells me harcoding is never a solution for a system\n    whose usage is for many aspects unpredictable\n(ii) we can't rely on human consensus to adapt it (seeing the mess\n     it is already this time).\n\nIt would have the advantage to place this block size issue entirely as\npart of the algorithmic contract you agree on when you use Bitcoin,\nsimilar to the difficulty adapation or the block reward.\n\n\nJ\u00e9r\u00e9mie\n\n\n2015-05-07 21:37 GMT+02:00 Mike Hearn <mike at plan99.net>:\n>\n>> These statements may even be true, but they're no logical conclusions\n>> even if they seem obvious to you.\n>> I don't think those claims are strictly true, specially because they\n>> involve predictions about what people will do.\n>> But if they're true they require some proof or at least some explanation.\n>\n>\n> Thank you for your patience, Jorge.\n>\n> I have written up an explanation of what I think will happen if we run out\n> of capacity:\n>\n>    https://medium.com/@octskyward/crash-landing-f5cc19908e32\n>\n> Now I'm going to go eat some dinner :)\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "J\u00e9r\u00e9mie Dubois-Lacoste",
                "date": "2015-05-07T20:20:03",
                "message_text_only": "> I have written up an explanation of what I think will happen if we run out\n> of capacity:\n>\n>    https://medium.com/@octskyward/crash-landing-f5cc19908e32\nLooks like a solid description of what would happen.\n\nI fail to see how this description wouldn't be applicable also to a\n20MB-network in some time in the future, say ~3 years from now, if\nBitcoin keeps taking off.\nIf you agree that it will be harder in the future to change the block\nlimit again, and we switch to hardcoded 20MB, then aren't we just\ngoing from an immediate relief to a future larger blockage?\n\n\n\n>\n> Now I'm going to go eat some dinner :)\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Matthew Mitchell",
                "date": "2015-05-07T15:58:13",
                "message_text_only": "In my personal opinion, this does make some sense to me, assuming I\nunderstood Gavin.\n\nI suppose it could be done with a new flag (like the P2SH flag) which\ndisplays miner support for larger blocks. The new rules would apply when\na large majority of miners support the new rules by counting the number\nof flagged blocks over a certain number of blocks on the network in a\ndeterministic fashion.\n\nThis way miners can continue to produce blocks which are supported by\nboth old and new clients. When it appears most people have migrated to\nthe new client, miners can start flagging support for the new rules, and\nwhen a large majority of miners agree, the new rules would kick in for\nall miners/clients running the new software. Miners could therefore glue\ntogether the network during the migration phase until enough people have\nupdated to avoid severe fork scenarios. The only problem is ensuring\nthat miners will continue to support both networks for long enough to\nenable successful migration.\n\nAnd if too many people disagree to make a clean hard fork (too many\npeople stubbornly stick to the old rules), then it could be that the\nhard fork is aborted and everyone goes back to the old rules, or quite\nsimply that the miners never give support for the new rules despite the\nmechanism being included in the new client. In those cases it would be\nas if nothing changed.\n\nThis way the hard fork would be determined by user participation as\njudged by the miners.\n\nIf it is done, I can't think of a fairer way.\n\nMatthew Mitchell\n\nOn 07/05/15 15:52, Gavin Andresen wrote:\n> For reference: the blog post that (re)-started this debate, and which\n> links to individual issues, is here:\n>   http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n> \n> In it, I asked people to email me objections I might have missed. I\n> would still appreciate it if people do that; it is impossible to keep up\n> with this mailing list, /r/bitcoin posts and comments, and\n> #bitcoin-wizards and also have time to respond thoughtfully to the\n> objections raised.\n> \n> I would very much like to find some concrete course of action that we\n> can come to consensus on. Some compromise so we can tell entrepreneurs\n> \"THIS is how much transaction volume the main Bitcoin blockchain will be\n> able to support over the next eleven years.\"\n> \n> I've been pretty clear on what I think is a reasonable compromise (a\n> one-time increase scheduled for early next year), and I have tried to\n> explain why I think it it is the right set of tradeoffs.\n> \n> There ARE tradeoffs here, and the hard question is what process do we\n> use to decide those tradeoffs?  How do we come to consensus? Is it worth\n> my time to spend hours responding thoughtfully to every new objection\n> raised here, or will the same thing happen that happened last year and\n> the year before-- everybody eventually gets tired of arguing\n> angels-dancing-on-the-head-of-a-pin, and we're left with the status quo?\n> \n> I AM considering contributing some version of the bigger blocksize-limit\n> hard-fork patch to the Bitcoin-Xt fork (probably  \"target a hobbyist\n> with a fast Internet connection, and assume Nelson's law to increase\n> over time), and then encouraging merchants and exchanges and web wallets\n> and individuals who think it strikes a reasonable balance to run it.\n> \n> And then, assuming it became a super-majority of nodes on the network,\n> encourage miners to roll out a soft-fork to start producing bigger\n> blocks and eventually trigger the hard fork.\n> \n> Because ultimately consensus comes down to what software people choose\n> to run.\n> \n> -- \n> --\n> Gavin Andresen\n> \n> \n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> \n> \n> \n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/733b3667/attachment.sig>"
            },
            {
                "author": "Matthew Mitchell",
                "date": "2015-05-07T16:47:41",
                "message_text_only": "One thing to add is that perhaps in a future version of Bitcoin Core,\nthere could be an option for users to continue using the old consensus\nrules, or an option to support the new rules (an option when they update\nand an ability to change in the settings). Both types of user can\nbenefit from the software updates and choose with a single piece of\nsoftware what they support. Information for whether or not a user is\nsupporting the changes could be included in the version message.\nPossibly this information could be incorporated into transactions also.\n\nIf they wish to support the new rules, then their client would support\nlarger blocks when there is majority miner consensus, otherwise their\nclients will always only support the old rules.\n\nThis way the decision is not being forced upon the user in any way.\n\nJust an idea.\n\nOn 07/05/15 16:58, Matthew Mitchell wrote:\n> In my personal opinion, this does make some sense to me, assuming I\n> understood Gavin.\n> \n> I suppose it could be done with a new flag (like the P2SH flag) which\n> displays miner support for larger blocks. The new rules would apply when\n> a large majority of miners support the new rules by counting the number\n> of flagged blocks over a certain number of blocks on the network in a\n> deterministic fashion.\n> \n> This way miners can continue to produce blocks which are supported by\n> both old and new clients. When it appears most people have migrated to\n> the new client, miners can start flagging support for the new rules, and\n> when a large majority of miners agree, the new rules would kick in for\n> all miners/clients running the new software. Miners could therefore glue\n> together the network during the migration phase until enough people have\n> updated to avoid severe fork scenarios. The only problem is ensuring\n> that miners will continue to support both networks for long enough to\n> enable successful migration.\n> \n> And if too many people disagree to make a clean hard fork (too many\n> people stubbornly stick to the old rules), then it could be that the\n> hard fork is aborted and everyone goes back to the old rules, or quite\n> simply that the miners never give support for the new rules despite the\n> mechanism being included in the new client. In those cases it would be\n> as if nothing changed.\n> \n> This way the hard fork would be determined by user participation as\n> judged by the miners.\n> \n> If it is done, I can't think of a fairer way.\n> \n> Matthew Mitchell\n> \n> On 07/05/15 15:52, Gavin Andresen wrote:\n>> For reference: the blog post that (re)-started this debate, and which\n>> links to individual issues, is here:\n>>   http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n>>\n>> In it, I asked people to email me objections I might have missed. I\n>> would still appreciate it if people do that; it is impossible to keep up\n>> with this mailing list, /r/bitcoin posts and comments, and\n>> #bitcoin-wizards and also have time to respond thoughtfully to the\n>> objections raised.\n>>\n>> I would very much like to find some concrete course of action that we\n>> can come to consensus on. Some compromise so we can tell entrepreneurs\n>> \"THIS is how much transaction volume the main Bitcoin blockchain will be\n>> able to support over the next eleven years.\"\n>>\n>> I've been pretty clear on what I think is a reasonable compromise (a\n>> one-time increase scheduled for early next year), and I have tried to\n>> explain why I think it it is the right set of tradeoffs.\n>>\n>> There ARE tradeoffs here, and the hard question is what process do we\n>> use to decide those tradeoffs?  How do we come to consensus? Is it worth\n>> my time to spend hours responding thoughtfully to every new objection\n>> raised here, or will the same thing happen that happened last year and\n>> the year before-- everybody eventually gets tired of arguing\n>> angels-dancing-on-the-head-of-a-pin, and we're left with the status quo?\n>>\n>> I AM considering contributing some version of the bigger blocksize-limit\n>> hard-fork patch to the Bitcoin-Xt fork (probably  \"target a hobbyist\n>> with a fast Internet connection, and assume Nelson's law to increase\n>> over time), and then encouraging merchants and exchanges and web wallets\n>> and individuals who think it strikes a reasonable balance to run it.\n>>\n>> And then, assuming it became a super-majority of nodes on the network,\n>> encourage miners to roll out a soft-fork to start producing bigger\n>> blocks and eventually trigger the hard fork.\n>>\n>> Because ultimately consensus comes down to what software people choose\n>> to run.\n>>\n>> -- \n>> --\n>> Gavin Andresen\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud \n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>\n>>\n>>\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n> \n> \n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> \n> \n> \n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> \n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/97e8772e/attachment.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-07T17:26:10",
                "message_text_only": "On 05/07/15 14:52, Gavin Andresen wrote:\n> For reference: the blog post that (re)-started this debate, and which\n> links to individual issues, is here:\n>   http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n> \n> In it, I asked people to email me objections I might have missed. I\n> would still appreciate it if people do that; it is impossible to keep up\n> with this mailing list, /r/bitcoin posts and comments, and\n> #bitcoin-wizards and also have time to respond thoughtfully to the\n> objections raised.\n\nPeople have been sharing the same objections as on this list for months,\nI'm not sure what is new here.\n\n> I would very much like to find some concrete course of action that we\n> can come to consensus on. Some compromise so we can tell entrepreneurs\n> \"THIS is how much transaction volume the main Bitcoin blockchain will be\n> able to support over the next eleven years.\"\n\nI think this is a huge issue. You've been wandering around telling\npeople that the blocksize will increase soon for months, when there is\nvery clearly no consensus that it should in the short-term future. The\nonly answer to this that anyone with a clue should give is \"it will\nvery, very likely be able to support at least 1MB blocks roughly every\n10 minutes on average for the next eleven years, and it seems likely\nthat a block size increase of some form will happen at some point in the\nnext eleven years\", anything else is dishonest."
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T17:43:30",
                "message_text_only": "> The only answer to this that anyone with a clue should give is \"it\n> will very, very likely be able to support at least 1MB blocks roughly\n> every 10 minutes on average for the next eleven years, and it seems\n> likely that a block size increase of some form will happen at some point in\n> the next eleven years\", anything else is dishonest.\n\n\nMatt, you know better than that. Gavin neither lacks clue nor is he\ndishonest.\n\nHe has been working on the assumption that other developers are reasonable,\nand some kind of compromise solution can be found that everyone can live\nwith. Hence trying to find a middle ground, hence considering and writing\narticles in response to every single objection raised. Hence asking for\nsuggestions on what to change about the plan, to make it more acceptable.\nWhat more do you want, exactly?\n\nAnd I'll ask again. Do you have a *specific, credible alternative*? Because\nso far I'm not seeing one.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/55e48fed/attachment.html>"
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-07T18:03:55",
                "message_text_only": "On Thu, May 7, 2015 at 6:43 PM, Mike Hearn <mike at plan99.net> wrote:\n>\n> And I'll ask again. Do you have a *specific, credible alternative*?\n> Because so far I'm not seeing one.\n>\n\nI think you are rubbing against your own presupposition that people must\nfind and alternative right now. Quite a lot here do not believe there is\nany urgency, nor that there is an immanent problem that has to be solved\nbefore the sky falls in.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/ae2e1450/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T18:06:09",
                "message_text_only": ">\n> I think you are rubbing against your own presupposition that people must\n> find and alternative right now. Quite a lot here do not believe there is\n> any urgency, nor that there is an immanent problem that has to be solved\n> before the sky falls in.\n>\n\nI have explained why I believe there is some urgency, whereby \"some\nurgency\" I mean, assuming it takes months to implement, merge, test,\nrelease and for people to upgrade.\n\nBut if it makes you happy, imagine that this discussion happens all over\nagain next year and I ask the same question.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/90b46e94/attachment.html>"
            },
            {
                "author": "Ross Nicoll",
                "date": "2015-05-07T18:21:47",
                "message_text_only": "Can I just add my own support for this - as has been stated elsewhere in \nthis discussion, hard forks are difficult, and risky. The earlier we \nhave a decision, and the earlier the change goes into the code, the \neasier that is.\n\nEven if the decision was the actual block size change is fine to leave \nuntil 2020, I'd like to see the code committed ASAP so that every new \ninstall, and every upgrade from there on gets the new version.\n\nMy personal opinion only is that 7 transactions a second is insanely \nlimited even if the main chain does nothing but act as a backbone \nbetween other chains and transaction networks. I don't think that's \noverly controversial. I think 2016 is too early for a 20mb block size, \nthough. I'm inclined to suggest a schedule of expansion, say to 2mb in \n2016, 4mb in 2018, 8mb in 2020 and 20mb in 2022 where it stops. The \nintent would be to provide enough size pressure to motivate scaling \nwork, while not limiting Bitcoin overly.\n\nFurther, I think this highlights that we need more work on fees. Right \nnow fees and transactions included are fairly naive, but I'd like to see \nthe absolute block size limit as a hard upper bound, with miners \nimposing soft limits based on a balance cost of storage, number of \noutputs vs inputs (and therefore impact on the UTXOs), and risk of \norphan blocks to determine which transactions are actually worth \nincluding in each block. If anyone has numbers on block size vs orphan \nrate that would be really useful, BTW.\n\nRoss\n\nOn 07/05/2015 19:06, Mike Hearn wrote:\n>\n>     I think you are rubbing against your own presupposition that\n>     people must find and alternative right now. Quite a lot here do\n>     not believe there is any urgency, nor that there is an immanent\n>     problem that has to be solved before the sky falls in.\n>\n>\n> I have explained why I believe there is some urgency, whereby \"some \n> urgency\" I mean, assuming it takes months to implement, merge, test, \n> release and for people to upgrade.\n>\n> But if it makes you happy, imagine that this discussion happens all \n> over again next year and I ask the same question.\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/11ec6034/attachment.html>"
            },
            {
                "author": "Gavin Costin",
                "date": "2015-05-07T18:40:50",
                "message_text_only": "Can anyone opposed to this proposal articulate in plain english the worst\ncase scenario(s) if it goes ahead?\n\nSome people in the conversation appear to be uncomfortable, perturbed,\ndefensive etc about the proposal \u0160. But I am not seeing specifics on why it\nis not a feasible plan.\n\nFrom:  Mike Hearn <mike at plan99.net>\nDate:  Friday, 8 May, 2015 2:06 am\nTo:  Btc Drak <btcdrak at gmail.com>\nCc:  Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\nSubject:  Re: [Bitcoin-development] Block Size Increase\n\n> I think you are rubbing against your own presupposition that people must find\n> and alternative right now. Quite a lot here do not believe there is any\n> urgency, nor that there is an immanent problem that has to be solved before\n> the sky falls in.\n\nI have explained why I believe there is some urgency, whereby \"some urgency\"\nI mean, assuming it takes months to implement, merge, test, release and for\npeople to upgrade.\n\nBut if it makes you happy, imagine that this discussion happens all over\nagain next year and I ask the same question.\n\n----------------------------------------------------------------------------\n-- One dashboard for servers and applications across Physical-Virtual-Cloud\nWidest out-of-the-box monitoring support with 50+ applications Performance\nmetrics, stats and reports that give you Actionable Insights Deep dive\nvisibility with transaction tracing using APM Insight.\nhttp://ad.doubleclick.net/ddm/clk/290420510;117567292;y_____________________\n__________________________ Bitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/ecf87458/attachment.html>"
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-07T18:46:14",
                "message_text_only": "On Thu, May 7, 2015 at 7:40 PM, Gavin Costin <slashdevnull at hotmail.com>\nwrote:\n\n> Can anyone opposed to this proposal articulate in plain english the worst\n> case scenario(s) if it goes ahead?\n>\n> Some people in the conversation appear to be uncomfortable, perturbed,\n> defensive etc about the proposal \u2026. But I am not seeing specifics on why it\n> is not a feasible plan.\n>\n\nSee this response:\nhttp://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07462.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/7ec7417f/attachment.html>"
            },
            {
                "author": "Bernard Rihn",
                "date": "2015-05-07T19:31:53",
                "message_text_only": "It seems to me like some (maybe most) of the pressure is actually external\nfrom companies that might release something that dramatically increases\n\"adoption\" & transaction rates (and that the data on historic rate of\nadoption & slumps is somewhat disconnected from their interests in a quick\nroll-out)?\n\nIt seems like the question actually becomes what is our maximum acceptable\ncost (hardware capex & bandwidth & power opex) associated with running a\nfull node without hardware acceleration and with hardware acceleration\n(something which presumably \"doesn't exist\" yet)? Are we making the\nassumption that hardware acceleration for confirmation will become broadly\navailable and that the primary limiter will become anonymous bandwidth?\n\nExcuse my ignorance, but I imagine somebody must have already looked at\nconfirmation times vs. block size for various existing hardware platforms\n(like at least 3 or 4? maybe a minnowboard, old laptop, and modern desktop\nat least?)? Is there an easy way to setup bitcoind or some other script to\ntest this? (happy to help)\n\nRe Moore's law: yeah, some say stuff like 5nm may never happen. We're\nalready using EUV with plasma emitters, immersed reflective optics, and\ndouble-patterning... and in storage land switching to helium. Things may\nslow A LOT over the next couple decades and I'd guess that a quadratic\nincrease (both in storage & compute) probably isn't a safe assumption.\n\nOn Thu, May 7, 2015 at 11:46 AM, Btc Drak <btcdrak at gmail.com> wrote:\n\n> On Thu, May 7, 2015 at 7:40 PM, Gavin Costin <slashdevnull at hotmail.com>\n> wrote:\n>\n>> Can anyone opposed to this proposal articulate in plain english the worst\n>> case scenario(s) if it goes ahead?\n>>\n>> Some people in the conversation appear to be uncomfortable, perturbed,\n>> defensive etc about the proposal \u2026. But I am not seeing specifics on why it\n>> is not a feasible plan.\n>>\n>\n> See this response:\n> http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07462.html\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/6a87c722/attachment.html>"
            },
            {
                "author": "Alan Reiner",
                "date": "2015-05-07T19:31:46",
                "message_text_only": "This *is* urgent and needs to be handled right now, and I believe Gavin\nhas the best approach to this.  I have heard Gavin's talks on increasing\nthe block size, and the two most persuasive points to me were:\n\n(1) Blocks are essentially nearing \"full\" now.  And by \"full\" he means\nthat the reliability of the network (from the average user perspective)\nis about to be impacted in a very negative way (I believe it was due to\nthe inconsistent time between blocks).  I think Gavin said that his\nsimulations showed 400 kB - 600 kB worth of transactions per 10 min\n(approx 3-4 tps) is where things start to behave poorly for certain\nclasses of transactions.  In other words, we're very close to the\neffective limit in terms of maintaining the current \"standard of\nliving\", and with a year needed to raise the block time this actually is\nurgent.\n\n(2) Leveraging fee pressure at 1MB to solve the problem is actually\nreally a bad idea.  It's really bad while Bitcoin is still growing, and\nrelying on fee pressure at 1 MB severely impacts attractiveness and\nadoption potential of Bitcoin (due to high fees and unreliability).  But\nmore importantly, it ignores the fact that for a 7 tps is pathetic for a\nglobal transaction system.  It is a couple orders of magnitude too low\nfor any meaningful commercial activity to occur.  If we continue with a\ncap of 7 tps forever, Bitcoin *will* fail.  Or at best, it will fail to\nbe useful for the vast majority of the world (which probably leads to\nfailure).  We shouldn't be talking about fee pressure until we hit 700\ntps, which is probably still too low. \n\nYou can argue that side chains and payment channels could alleviate\nthis.  But how far off are they?  We're going to hit effective 1MB\nlimits long before we can leverage those in a meaningful way.  Even if\neveryone used them, getting a billion people onto the system just can't\nhappen even at 1 transaction per year per person to get into a payment\nchannel or move money between side chains.\n\nWe get asked all the time by corporate clients about scalability.  A\nlimit of 7 tps makes them uncomfortable that they are going to invest\nall this time into a system that has no chance of handling the economic\nactivity that they expect it handle.  We always assure them that 7 tps\nis not the final answer. \n\nSatoshi didn't believe 1 MB blocks were the correct answer.  I\npersonally think this is critical to Bitcoin's long term future.   And\nI'm not sure what else Gavin could've done to push this along in a\nmeaninful way.\n\n-Alan\n\n\nOn 05/07/2015 02:06 PM, Mike Hearn wrote:\n>\n>     I think you are rubbing against your own presupposition that\n>     people must find and alternative right now. Quite a lot here do\n>     not believe there is any urgency, nor that there is an immanent\n>     problem that has to be solved before the sky falls in.\n>\n>\n> I have explained why I believe there is some urgency, whereby \"some\n> urgency\" I mean, assuming it takes months to implement, merge, test,\n> release and for people to upgrade.\n>\n> But if it makes you happy, imagine that this discussion happens all\n> over again next year and I ask the same question.\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/8d08315a/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T19:54:13",
                "message_text_only": "On Thu, May 7, 2015 at 3:31 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n\n>  (1) Blocks are essentially nearing \"full\" now.  And by \"full\" he means\n> that the reliability of the network (from the average user perspective) is\n> about to be impacted in a very negative way\n>\n\nEr, to be economically precise, \"full\" just means fees are no longer zero.\nBitcoin behaves as it always has.  It is no longer basically free to dump\nspam into the blockchain, as it is today.\n\nIn the short term, blocks are bursty, with some on 1 minute intervals, some\nwith 60 minute intervals.  This does not change with larger blocks.\n\n\n\n> (2) Leveraging fee pressure at 1MB to solve the problem is actually really\n> a bad idea.  It's really bad while Bitcoin is still growing, and relying on\n> fee pressure at 1 MB severely impacts attractiveness and adoption potential\n> of Bitcoin (due to high fees and unreliability).  But more importantly, it\n> ignores the fact that for a 7 tps is pathetic for a global transaction\n> system.  It is a couple orders of magnitude too low for any meaningful\n> commercial activity to occur.  If we continue with a cap of 7 tps forever,\n> Bitcoin *will* fail.  Or at best, it will fail to be useful for the vast\n> majority of the world (which probably leads to failure).  We shouldn't be\n> talking about fee pressure until we hit 700 tps, which is probably still\n> too low.\n>\n [...]\n\n1) Agree that 7 tps is too low\n\n2) Where do you want to go?  Should bitcoin scale up to handle all the\nworld's coffees?\n\nThis is hugely unrealistic.  700 tps is 100MB blocks, 14.4 GB/day -- just\nfor a single feed.  If you include relaying to multiple nodes, plus serving\n500 million SPV clients en grosse, who has the capacity to run such a\nnode?  By the time we get to fee pressure, in your scenario, our network\nnode count is tiny and highly centralized.\n\n3) In RE \"fee pressure\" -- Do you see the moral hazard to a software-run\nsystem?  It is an intentional, human decision to flood the market with\nsupply, thereby altering the economics, forcing fees to remain low in the\nhopes of achieving adoption.  I'm pro-bitcoin and obviously want to see\nbitcoin adoption - but I don't want to sacrifice every decentralized\nprinciple and become a central banker in order to get there.\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/52c8aa10/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T19:59:18",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 09:54 PM, Jeff Garzik wrote:\n> By the time we get to fee pressure, in your scenario, our network \n> node count is tiny and highly centralized.\n\nAgain, this assertion requires proof.\n\nSimply saying things is not the same as them being true.\n\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVS8QWAAoJECpf2nDq2eYj+/4P/2JXxo2RDAg0ptd9aUYVvzp9\nKhL33cdmK8kbKBFOVcOuIrlQRzZn9iydIPC165Y40Y6Wrtgw2PoXctuqdQdXaSZI\nM3bHuM7mweHyb3xBHNaNHIxfwrMjQQAdOTGO7PZusghDYz2QEj44dhIcNOzO7uTD\nfXkhzgJfwu0l0Wqn3v/R9amRUWLE5nlM566xJ2sVtlfBMEyzR5L1GwX1lKNhxeO8\nqvkgegsF2Usjz9pIUMSGFxSWZQuTSjHbhbh28JaT/wi6DI3pcTV0FPw95IPImqUh\nrbIqcPh43omXrHKEHV/FB+XMItD3VvR9dxogYaFZLv1EU2gnF2IM0cw5a/oyHr+L\nC920uEbXrvrMEJw1ftvxQyu6NY5c3/5iVMqz773oQSjOihkZ8P1JvxQnldU6mcoU\nRaKM13cxgjSkCqJ5R1iIldFQPCLLWUKJDkPEnGlwdLPF/vwhnCt1PZJTB5hqoCgC\nab5yBVLpLgo7sbizOeX/R3WGp3NjGXDQC93Af/Vr37uiu1ZT+1P1Ow86hsZTRx6b\n4d25tSGg7Tw3Bs/YOhJ9AKtlN092Y8/WBMscQu6MaFt6I/1OMX9OVH+veEj/VjwB\nL/dxWTRdC0HEKiYv+EuESIRoyTLlCHKBUDBgKbYSMjetg6WW64hYrpxNX7TH20o6\n00bWPVV2PcEWuCc230UF\n=1bK6\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/a275c96e/attachment.bin>"
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-08T01:40:32",
                "message_text_only": "On 5/7/2015 12:54 PM, Jeff Garzik wrote:\n> In the short term, blocks are bursty, with some on 1 minute intervals, \n> some with 60 minute intervals.  This does not change with larger blocks.\n>\n\nI'm pretty sure Alan meant that blocks are already filling up after long \ninter-block intervals.\n\n\n>\n> 2) Where do you want to go?  Should bitcoin scale up to handle all the \n> world's coffees?\n\nAlan was very clear.  Right now, he wants to go exactly where Gavin's \nconcrete proposal suggests."
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-08T02:09:42",
                "message_text_only": "On Thu, May 7, 2015 at 9:40 PM, Tom Harding <tomh at thinlink.com> wrote:\n\n> On 5/7/2015 12:54 PM, Jeff Garzik wrote:\n> > 2) Where do you want to go?  Should bitcoin scale up to handle all the\n> > world's coffees?\n>\n> Alan was very clear.  Right now, he wants to go exactly where Gavin's\n> concrete proposal suggests.\n>\n\nG proposed 20MB blocks, AFAIK - 140 tps\nA proposed 100MB blocks - 700 tps\nFor ref,\nPaypal is around 115 tps\nVISA is around 2000 tps (perhaps 4000 tps peak)\n\nI ask again:  where do we want to go?   This is the existential question\nbehind block size.\n\nAre we trying to build a system that can handle Paypal volumes?  VISA\nvolumes?\n\nIt's not a snarky or sarcastic question:  Are we building a system to\nhandle all the world's coffees?  Is bitcoin's main chain and network -\nLayer 1 - going to receive direct connections from 500m mobile phones,\nbroadcasting transactions?\n\nWe must answer these questions to inform the change being discussed today,\nin order to decide what makes the most sense as a new limit.  Any\nresponsible project of this magnitude must have a better story than \"zomg\n1MB, therefore I picked 20MB out of a hat\"  Must be able to answer /why/\nthe new limit was picked.\n\nAs G notes, changing the block size is simply kicking the can down the\nroad: http://gavinandresen.ninja/it-must-be-done-but-is-not-a-panacea\nNecessarily one must ask, today, what happens when we get to the end of\nthat newly paved road.\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/f1a12b68/attachment.html>"
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-08T05:13:08",
                "message_text_only": "On 5/7/2015 7:09 PM, Jeff Garzik wrote:\n>\n> G proposed 20MB blocks, AFAIK - 140 tps\n> A proposed 100MB blocks - 700 tps\n> For ref,\n> Paypal is around 115 tps\n> VISA is around 2000 tps (perhaps 4000 tps peak)\n>\n> I ask again:  where do we want to go?   This is the existential\n> question behind block size.\n>\n> Are we trying to build a system that can handle Paypal volumes?  VISA\n> volumes?\n>\n> It's not a snarky or sarcastic question:  Are we building a system to\n> handle all the world's coffees?  Is bitcoin's main chain and network -\n> Layer 1 - going to receive direct connections from 500m mobile phones,\n> broadcasting transactions?\n>\n> We must answer these questions to inform the change being discussed\n> today, in order to decide what makes the most sense as a new limit. \n> Any responsible project of this magnitude must have a better story\n> than \"zomg 1MB, therefore I picked 20MB out of a hat\"  Must be able to\n> answer /why/ the new limit was picked.\n>\n> As G notes, changing the block size is simply kicking the can down the\n> road:\n> http://gavinandresen.ninja/it-must-be-done-but-is-not-a-panacea  \n> Necessarily one must ask, today, what happens when we get to the end\n> of that newly paved road.\n>\n>\n\nAccepting that outcomes are less knowable further into the future is not\nthe same as failing to consider the future at all.  A responsible\nproject can't have a movie-plot roadmap.  It needs to give weight to\nmultiple possible future outcomes.\nhttp://en.wikipedia.org/wiki/Decision_tree\n\nOne way or another, the challenge is to decide what to do next.  Beyond\nthat, it's future decisions all the way down. \n\nAlan argues that 7 tps is a couple orders of magnitude too low for any\nmeaningful commercial activity to occur, and too low to be the final\nsolution, even with higher layers.  I agree.  I also agree with you,\nthat we don't really know how to accomplish 700tps right now.\n\nWhat we do know is if we want to bump the limit in the short term, we\nought to start now, and until there's a better alternative root to the\ndecision tree, it just might be time to get moving."
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-08T09:43:42",
                "message_text_only": ">\n> Alan argues that 7 tps is a couple orders of magnitude too low\n\n\nBy the way, just to clear this up - the real limit at the moment is more\nlike 3 tps, not 7.\n\nThe 7 transactions/second figure comes from calculations I did years ago,\nin 2011. I did them a few months before the \"sendmany\" command was\nreleased, so back then almost all transactions were small. After sendmany\nand as people developed custom wallets, etc, the average transaction size\nwent up.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/4c054cee/attachment.html>"
            },
            {
                "author": "Alan Reiner",
                "date": "2015-05-08T15:23:24",
                "message_text_only": "On 05/08/2015 01:13 AM, Tom Harding wrote:\n> On 5/7/2015 7:09 PM, Jeff Garzik wrote:\n>> G proposed 20MB blocks, AFAIK - 140 tps\n>> A proposed 100MB blocks - 700 tps\n>> For ref,\n>> Paypal is around 115 tps\n>> VISA is around 2000 tps (perhaps 4000 tps peak)\n>>\n\nFor reference, I'm not \"proposing\" 100 MB blocks right now.  I was\nsimply suggesting that if Bitcoin is to *ultimately* achieve the goal of\nbeing a globally useful payment rails, 7tps is embarrassingly small. \nEven with off-chain transactions.  It should be a no-brainer that block\nsize has to go up.\n\nMy goal was to bring some long-term perspective into the discussion.  I\ndon't know if 100 MB blocks will *actually* be necessary for Bitcoin in\n20 years, but it's feasible that it will be.  It's an open, global\npayments system.  Therefore, we shouldn't be arguing about whether 1 MB\nblocks is sufficient--it's very clearly not.  And admitting this as a\nvalid point is also an admission that not everyone in the world will be\nable to run a full node in 20 years.\n\nI don't think there's a solution that can accommodate all future\nscenarios, nor that we can even find a solution right now that avoids\nmore hard forks in the future.   But the goal of \"everyone should be\nable to download and verify the world's global transactions on a\nsmartphone\" is a non-starter and should not drive decisions."
            },
            {
                "author": "Alan Reiner",
                "date": "2015-05-08T14:59:34",
                "message_text_only": "This isn't about \"everyone's coffee\".  This is about an absolute minimum\namount of participation by people who wish to use the network.   If our\ngoal is really for bitcoin to really be a global, open transaction\nnetwork that makes money fluid, then 7tps is already a failure.  If even\n5% of the world (350M people) was using the network for 1 tx per month\n(perhaps to open payment channels, or shift money between side chains),\nwe'll be above 100 tps.  And that doesn't include all the\nnon-individuals (organizations) that want to use it.\n\nThe goals of \"a global transaction network\" and \"everyone must be able\nto run a full node with their $200 dell laptop\" are not compatible.  We\nneed to accept that a global transaction system cannot be\nfully/constantly audited by everyone and their mother.  The important\nfeature of the network is that it is open and anyone *can* get the\nhistory and verify it.  But not everyone is required to.   Trying to\npromote a system where the history can be forever handled by a low-end\nPC is already falling out of reach, even with our miniscule 7 tps. \nClinging to that goal needlessly limits the capability for the network\nto scale to be a useful global payments system\n\n\n\nOn 05/07/2015 03:54 PM, Jeff Garzik wrote:\n> On Thu, May 7, 2015 at 3:31 PM, Alan Reiner <etotheipi at gmail.com\n> <mailto:etotheipi at gmail.com>> wrote:\n>  \n>\n>     (2) Leveraging fee pressure at 1MB to solve the problem is\n>     actually really a bad idea.  It's really bad while Bitcoin is\n>     still growing, and relying on fee pressure at 1 MB severely\n>     impacts attractiveness and adoption potential of Bitcoin (due to\n>     high fees and unreliability).  But more importantly, it ignores\n>     the fact that for a 7 tps is pathetic for a global transaction\n>     system.  It is a couple orders of magnitude too low for any\n>     meaningful commercial activity to occur.  If we continue with a\n>     cap of 7 tps forever, Bitcoin *will* fail.  Or at best, it will\n>     fail to be useful for the vast majority of the world (which\n>     probably leads to failure).  We shouldn't be talking about fee\n>     pressure until we hit 700 tps, which is probably still too low. \n>\n>  [...]\n>\n> 1) Agree that 7 tps is too low\n>\n> 2) Where do you want to go?  Should bitcoin scale up to handle all the\n> world's coffees? \n>\n> This is hugely unrealistic.  700 tps is 100MB blocks, 14.4 GB/day --\n> just for a single feed.  If you include relaying to multiple nodes,\n> plus serving 500 million SPV clients en grosse, who has the capacity\n> to run such a node?  By the time we get to fee pressure, in your\n> scenario, our network node count is tiny and highly centralized.\n>\n> 3) In RE \"fee pressure\" -- Do you see the moral hazard to a\n> software-run system?  It is an intentional, human decision to flood\n> the market with supply, thereby altering the economics, forcing fees\n> to remain low in the hopes of achieving adoption.  I'm pro-bitcoin and\n> obviously want to see bitcoin adoption - but I don't want to sacrifice\n> every decentralized principle and become a central banker in order to\n> get there.\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/aa6f7b9a/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-08T15:49:48",
                "message_text_only": "On Fri, May 8, 2015 at 10:59 AM, Alan Reiner <etotheipi at gmail.com> wrote:\n\n>\n> This isn't about \"everyone's coffee\".  This is about an absolute minimum\n> amount of participation by people who wish to use the network.   If our\n> goal is really for bitcoin to really be a global, open transaction network\n> that makes money fluid, then 7tps is already a failure.  If even 5% of the\n> world (350M people) was using the network for 1 tx per month (perhaps to\n> open payment channels, or shift money between side chains), we'll be above\n> 100 tps.  And that doesn't include all the non-individuals (organizations)\n> that want to use it.\n>\n> The goals of \"a global transaction network\" and \"everyone must be able to\n> run a full node with their $200 dell laptop\" are not compatible.  We need\n> to accept that a global transaction system cannot be fully/constantly\n> audited by everyone and their mother.  The important feature of the network\n> is that it is open and anyone *can* get the history and verify it.  But not\n> everyone is required to.   Trying to promote a system where the history can\n> be forever handled by a low-end PC is already falling out of reach, even\n> with our miniscule 7 tps.  Clinging to that goal needlessly limits the\n> capability for the network to scale to be a useful global payments system\n>\n>\nTo repeat, the very first point in my email reply was: \"Agree that 7 tps is\ntoo low\"  Never was it said that bit\n\nTherefore a reply arguing against the low end is nonsense, and the relevant\nquestion remains on the table.\n\nHow high do you want to go - and can Layer 1 bitcoin really scale to get\nthere?\n\nIt is highly disappointing to see people endorse \"moar bitcoin volume!\"\nwith zero thinking behind that besides \"adoption!\"  Need to actually\nproject what bitcoin looks like at the desired levels, what network\nresources are required to get to those levels -- including traffic to serve\nthose SPV clients via P2P -- and then work backwards from that to see who\ncan support it, and then work backwards to discern a maximum tps.\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/c0203dbe/attachment.html>"
            },
            {
                "author": "Oliver Egginger",
                "date": "2015-05-13T10:37:17",
                "message_text_only": "08.05.2015 at 5:49 Jeff Garzik wrote:\n> To repeat, the very first point in my email reply was: \"Agree that 7 tps\n> is too low\"  \n\nFor interbank trading that would maybe enough but I don't know.\n\nI'm not a developer but as a (former) user and computer scientist I'm\nalso asking myself what is the core of the problem? Personally, for\nprivacy reasons I do not want to leave a footprint in the blockchain for\neach pizza. And why should this expense be good for trivial things of\neveryday life?\n\nIf one encounters the block boundary, he or she will do more effort or\ngive up. I'm thinking most people will give up because their\ntransactions are not really economical. It is much better for them to\nuse third-partys (or another payment system).\n\nAnd that's where we are at the heart of the problem. The Bitcoin\nthird-party economy. With few exceptions this is pure horror. More worse\nthan any used car dealer. And the community just waits that things get\nbetter. But that will never happen of its own accord. We are living in a\nWild West Town. So we need a Sheriff and many other things.\n\nWe need a small but good functioning economy around the blockchain. To\ncreate one, we have to accept a few unpleasant truths. I do not know if\nthe community is ready for it.\n\nNevertheless, I know that some companies do a good job. But they have to\nprevail against their dishonest competitors.\n\nPeople take advantage of the blockchain, because they no longer trust\nanyone. But this will not scale in the long run.\n\n- oliver"
            },
            {
                "author": "Angel Leon",
                "date": "2015-05-13T11:25:47",
                "message_text_only": "> Personally, for privacy reasons I do not want to leave a footprint in the\nblockchain for each pizza. And  why should this expense be good for trivial\nthings of everyday life?\n\nThen what's the point?\nIsn't this supposed to be an Open transactional network, it doesn't matter\nif you don't want that, what matters is what people want to do with it, and\nthere's nothing you can do to stop someone from opening a wallet and buying\na pizza with it, except the core of the problem you ask yourself about,\nwhich is, the minute this goes mainstream and people get their wallets out\nthe whole thing will collapse, regardless of what you want the blockchain\nfor.\n\nWhy talk about the billions of unbanked and all the romantic vision if you\ncan't let them use their money however they want in a decentralized\nfashion. Otherwise let's just go back to centralized banking because the\nminute you want to put things off chain, you need an organization that will\nneed to respond to government regulation and that's the end for the\nbillions of unbanked to be part of the network.\n\n\nhttp://twitter.com/gubatron\n\nOn Wed, May 13, 2015 at 6:37 AM, Oliver Egginger <bitcoin at olivere.de> wrote:\n\n> 08.05.2015 at 5:49 Jeff Garzik wrote:\n> > To repeat, the very first point in my email reply was: \"Agree that 7 tps\n> > is too low\"\n>\n> For interbank trading that would maybe enough but I don't know.\n>\n> I'm not a developer but as a (former) user and computer scientist I'm\n> also asking myself what is the core of the problem? Personally, for\n> privacy reasons I do not want to leave a footprint in the blockchain for\n> each pizza. And why should this expense be good for trivial things of\n> everyday life?\n>\n> If one encounters the block boundary, he or she will do more effort or\n> give up. I'm thinking most people will give up because their\n> transactions are not really economical. It is much better for them to\n> use third-partys (or another payment system).\n>\n> And that's where we are at the heart of the problem. The Bitcoin\n> third-party economy. With few exceptions this is pure horror. More worse\n> than any used car dealer. And the community just waits that things get\n> better. But that will never happen of its own accord. We are living in a\n> Wild West Town. So we need a Sheriff and many other things.\n>\n> We need a small but good functioning economy around the blockchain. To\n> create one, we have to accept a few unpleasant truths. I do not know if\n> the community is ready for it.\n>\n> Nevertheless, I know that some companies do a good job. But they have to\n> prevail against their dishonest competitors.\n>\n> People take advantage of the blockchain, because they no longer trust\n> anyone. But this will not scale in the long run.\n>\n> - oliver\n>\n>\n>\n>\n>\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/4b199524/attachment.html>"
            },
            {
                "author": "Andrew",
                "date": "2015-05-08T17:17:49",
                "message_text_only": "On Fri, May 8, 2015 at 2:59 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n\n>\n> This isn't about \"everyone's coffee\".  This is about an absolute minimum\n> amount of participation by people who wish to use the network.   If our\n> goal is really for bitcoin to really be a global, open transaction network\n> that makes money fluid, then 7tps is already a failure.  If even 5% of the\n> world (350M people) was using the network for 1 tx per month (perhaps to\n> open payment channels, or shift money between side chains), we'll be above\n> 100 tps.  And that doesn't include all the non-individuals (organizations)\n> that want to use it.\n>\n\n> The goals of \"a global transaction network\" and \"everyone must be able to\n> run a full node with their $200 dell laptop\" are not compatible.  We need\n> to accept that a global transaction system cannot be fully/constantly\n> audited by everyone and their mother.  The important feature of the network\n> is that it is open and anyone *can* get the history and verify it.  But not\n> everyone is required to.   Trying to promote a system wher000e the history\n> can be forever handled by a low-end PC is already falling out of reach,\n> even with our miniscule 7 tps.  Clinging to that goal needlessly limits the\n> capability for the network to scale to be a useful global payments system\n>\n\nThese are good points and got me thinking (but I think you're wrong). If we\nreally want each of the 10 billion people soon using bitcoin once per\nmonth, that will require 500MB blocks. That's about 2 TB per month. And if\nyou relay it to 4 peers, it's 10 TB per month. Which I suppose is doable\nfor a home desktop, so you can just run a pruned full node with all\ntransactions from the past month. But how do you sync all those\ntransactions if you've never done this before or it's been a while since\nyou did? I think it currently takes at least 3 hours to fully sync 30 GB of\ntransactions. So 2 TB will take 8 days, then you take a bit more time to\nsync the days that passed while you were syncing. So that's doable, but at\na certain point, like 10 TB per month (still only 5 transactions per month\nper person), you will need 41 days to sync that month, so you will never\ncatch up. So I think in order to keep the very important property of anyone\nbeing able to start clean and verify the thing, then we need to think of\nbitcoin as a system that does transactions for a large number of users at\nonce in one transaction, and not a system where each person will make a\n~monthly transaction on. We need to therefore rely on sidechains,\ntreechains, lightning channels, etc...\n\nI'm not a bitcoin wizard and this is just my second post on this mailing\nlist, so I may be missing something. So please someone, correct me if I'm\nwrong.\n\n>\n>\n>\n> On 05/07/2015 03:54 PM, Jeff Garzik wrote:\n>\n>  On Thu, May 7, 2015 at 3:31 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n>\n>\n>>  (2) Leveraging fee pressure at 1MB to solve the problem is actually\n>> really a bad idea.  It's really bad while Bitcoin is still growing, and\n>> relying on fee pressure at 1 MB severely impacts attractiveness and\n>> adoption potential of Bitcoin (due to high fees and unreliability).  But\n>> more importantly, it ignores the fact that for a 7 tps is pathetic for a\n>> global transaction system.  It is a couple orders of magnitude too low for\n>> any meaningful commercial activity to occur.  If we continue with a cap of\n>> 7 tps forever, Bitcoin *will* fail.  Or at best, it will fail to be\n>> useful for the vast majority of the world (which probably leads to\n>> failure).  We shouldn't be talking about fee pressure until we hit 700 tps,\n>> which is probably still too low.\n>>\n>  [...]\n>\n>  1) Agree that 7 tps is too low\n>\n>  2) Where do you want to go?  Should bitcoin scale up to handle all the\n> world's coffees?\n>\n>  This is hugely unrealistic.  700 tps is 100MB blocks, 14.4 GB/day --\n> just for a single feed.  If you include relaying to multiple nodes, plus\n> serving 500 million SPV clients en grosse, who has the capacity to run such\n> a node?  By the time we get to fee pressure, in your scenario, our network\n> node count is tiny and highly centralized.\n>\n>  3) In RE \"fee pressure\" -- Do you see the moral hazard to a software-run\n> system?  It is an intentional, human decision to flood the market with\n> supply, thereby altering the economics, forcing fees to remain low in the\n> hopes of achieving adoption.  I'm pro-bitcoin and obviously want to see\n> bitcoin adoption - but I don't want to sacrifice every decentralized\n> principle and become a central banker in order to get there.\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/b5114625/attachment.html>"
            },
            {
                "author": "Alan Reiner",
                "date": "2015-05-08T17:51:51",
                "message_text_only": "Actually I believe that side chains and off-main-chain transactions will be\na critical part for the overall scalability of the network.  I was actually\ntrying to make the point that (insert some huge block size here) will be\nneeded to even accommodate the reduced traffic.\n\nI believe that it is definitely over 20MB. If it was determined to be 100\nMB ten years from now, that wouldn't surprise me.\n\nSent from my overpriced smartphone\nOn May 8, 2015 1:17 PM, \"Andrew\" <onelineproof at gmail.com> wrote:\n\n>\n>\n> On Fri, May 8, 2015 at 2:59 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n>\n>>\n>> This isn't about \"everyone's coffee\".  This is about an absolute minimum\n>> amount of participation by people who wish to use the network.   If our\n>> goal is really for bitcoin to really be a global, open transaction network\n>> that makes money fluid, then 7tps is already a failure.  If even 5% of the\n>> world (350M people) was using the network for 1 tx per month (perhaps to\n>> open payment channels, or shift money between side chains), we'll be above\n>> 100 tps.  And that doesn't include all the non-individuals (organizations)\n>> that want to use it.\n>>\n>\n>> The goals of \"a global transaction network\" and \"everyone must be able to\n>> run a full node with their $200 dell laptop\" are not compatible.  We need\n>> to accept that a global transaction system cannot be fully/constantly\n>> audited by everyone and their mother.  The important feature of the network\n>> is that it is open and anyone *can* get the history and verify it.  But not\n>> everyone is required to.   Trying to promote a system wher000e the history\n>> can be forever handled by a low-end PC is already falling out of reach,\n>> even with our miniscule 7 tps.  Clinging to that goal needlessly limits the\n>> capability for the network to scale to be a useful global payments system\n>>\n>\n> These are good points and got me thinking (but I think you're wrong). If\n> we really want each of the 10 billion people soon using bitcoin once per\n> month, that will require 500MB blocks. That's about 2 TB per month. And if\n> you relay it to 4 peers, it's 10 TB per month. Which I suppose is doable\n> for a home desktop, so you can just run a pruned full node with all\n> transactions from the past month. But how do you sync all those\n> transactions if you've never done this before or it's been a while since\n> you did? I think it currently takes at least 3 hours to fully sync 30 GB of\n> transactions. So 2 TB will take 8 days, then you take a bit more time to\n> sync the days that passed while you were syncing. So that's doable, but at\n> a certain point, like 10 TB per month (still only 5 transactions per month\n> per person), you will need 41 days to sync that month, so you will never\n> catch up. So I think in order to keep the very important property of anyone\n> being able to start clean and verify the thing, then we need to think of\n> bitcoin as a system that does transactions for a large number of users at\n> once in one transaction, and not a system where each person will make a\n> ~monthly transaction on. We need to therefore rely on sidechains,\n> treechains, lightning channels, etc...\n>\n> I'm not a bitcoin wizard and this is just my second post on this mailing\n> list, so I may be missing something. So please someone, correct me if I'm\n> wrong.\n>\n>>\n>>\n>>\n>> On 05/07/2015 03:54 PM, Jeff Garzik wrote:\n>>\n>>  On Thu, May 7, 2015 at 3:31 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n>>\n>>\n>>>  (2) Leveraging fee pressure at 1MB to solve the problem is actually\n>>> really a bad idea.  It's really bad while Bitcoin is still growing, and\n>>> relying on fee pressure at 1 MB severely impacts attractiveness and\n>>> adoption potential of Bitcoin (due to high fees and unreliability).  But\n>>> more importantly, it ignores the fact that for a 7 tps is pathetic for a\n>>> global transaction system.  It is a couple orders of magnitude too low for\n>>> any meaningful commercial activity to occur.  If we continue with a cap of\n>>> 7 tps forever, Bitcoin *will* fail.  Or at best, it will fail to be\n>>> useful for the vast majority of the world (which probably leads to\n>>> failure).  We shouldn't be talking about fee pressure until we hit 700 tps,\n>>> which is probably still too low.\n>>>\n>>  [...]\n>>\n>>  1) Agree that 7 tps is too low\n>>\n>>  2) Where do you want to go?  Should bitcoin scale up to handle all the\n>> world's coffees?\n>>\n>>  This is hugely unrealistic.  700 tps is 100MB blocks, 14.4 GB/day --\n>> just for a single feed.  If you include relaying to multiple nodes, plus\n>> serving 500 million SPV clients en grosse, who has the capacity to run such\n>> a node?  By the time we get to fee pressure, in your scenario, our network\n>> node count is tiny and highly centralized.\n>>\n>>  3) In RE \"fee pressure\" -- Do you see the moral hazard to a software-run\n>> system?  It is an intentional, human decision to flood the market with\n>> supply, thereby altering the economics, forcing fees to remain low in the\n>> hopes of achieving adoption.  I'm pro-bitcoin and obviously want to see\n>> bitcoin adoption - but I don't want to sacrifice every decentralized\n>> principle and become a central banker in order to get there.\n>>\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> --\n> PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/4f232d91/attachment.html>"
            },
            {
                "author": "Joel Joonatan Kaartinen",
                "date": "2015-05-08T01:51:35",
                "message_text_only": "Having observed the customer support nightmare it tends to cause for a\nsmall exchange service when 100% full blocks happen, I've been thinking\nthat the limit really should be dynamic and respond to demand and the\namount of fees offered. It just doesn't feel right when it takes ages to\nburn through the backlog when 100% full is hit for a while. So, while\npondering this, I got an idea that I think has a chance of working that I\ncan't remember seeing suggested anywhere.\n\nHow about basing the maximum valid size for a block on the total bitcoin\ndays destroyed in that block? That should still stop transaction spam but\nnaturally expand the block size when there's a backlog of real\ntransactions. It'd also provide for an indirect mechanism for increasing\nthe maximum block size based on fees if there's a lot of fees but little\nbitcoin days destroyed. In such a situation there'd be incentive to pay\nsomeone to spend an older txout to expand the maximum. I realize this is a\nrather half baked idea, but it seems worth considering.\n\n- Joel\n\nOn Thu, May 7, 2015 at 10:31 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n\n>  This *is* urgent and needs to be handled right now, and I believe Gavin\n> has the best approach to this.  I have heard Gavin's talks on increasing\n> the block size, and the two most persuasive points to me were:\n>\n> (1) Blocks are essentially nearing \"full\" now.  And by \"full\" he means\n> that the reliability of the network (from the average user perspective) is\n> about to be impacted in a very negative way (I believe it was due to the\n> inconsistent time between blocks).  I think Gavin said that his simulations\n> showed 400 kB - 600 kB worth of transactions per 10 min (approx 3-4 tps) is\n> where things start to behave poorly for certain classes of transactions.\n> In other words, we're very close to the effective limit in terms of\n> maintaining the current \"standard of living\", and with a year needed to\n> raise the block time this actually is urgent.\n>\n> (2) Leveraging fee pressure at 1MB to solve the problem is actually really\n> a bad idea.  It's really bad while Bitcoin is still growing, and relying on\n> fee pressure at 1 MB severely impacts attractiveness and adoption potential\n> of Bitcoin (due to high fees and unreliability).  But more importantly, it\n> ignores the fact that for a 7 tps is pathetic for a global transaction\n> system.  It is a couple orders of magnitude too low for any meaningful\n> commercial activity to occur.  If we continue with a cap of 7 tps forever,\n> Bitcoin *will* fail.  Or at best, it will fail to be useful for the vast\n> majority of the world (which probably leads to failure).  We shouldn't be\n> talking about fee pressure until we hit 700 tps, which is probably still\n> too low.\n>\n> You can argue that side chains and payment channels could alleviate this.\n> But how far off are they?  We're going to hit effective 1MB limits long\n> before we can leverage those in a meaningful way.  Even if everyone used\n> them, getting a billion people onto the system just can't happen even at 1\n> transaction per year per person to get into a payment channel or move money\n> between side chains.\n>\n> We get asked all the time by corporate clients about scalability.  A limit\n> of 7 tps makes them uncomfortable that they are going to invest all this\n> time into a system that has no chance of handling the economic activity\n> that they expect it handle.  We always assure them that 7 tps is not the\n> final answer.\n>\n> Satoshi didn't believe 1 MB blocks were the correct answer.  I personally\n> think this is critical to Bitcoin's long term future.   And I'm not sure\n> what else Gavin could've done to push this along in a meaninful way.\n>\n> -Alan\n>\n>\n>\n> On 05/07/2015 02:06 PM, Mike Hearn wrote:\n>\n>     I think you are rubbing against your own presupposition that people\n>> must find and alternative right now. Quite a lot here do not believe there\n>> is any urgency, nor that there is an immanent problem that has to be solved\n>> before the sky falls in.\n>>\n>\n>  I have explained why I believe there is some urgency, whereby \"some\n> urgency\" I mean, assuming it takes months to implement, merge, test,\n> release and for people to upgrade.\n>\n>  But if it makes you happy, imagine that this discussion happens all over\n> again next year and I ask the same question.\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>\n>\n>\n> _______________________________________________\n> Bitcoin-development mailing listBitcoin-development at lists.sourceforge.nethttps://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/fa4f0644/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-08T03:41:21",
                "message_text_only": "On Thu, May 07, 2015 at 03:31:46PM -0400, Alan Reiner wrote:\n> We get asked all the time by corporate clients about scalability.  A\n> limit of 7 tps makes them uncomfortable that they are going to invest\n> all this time into a system that has no chance of handling the economic\n> activity that they expect it handle.  We always assure them that 7 tps\n> is not the final answer. \n\nYour corporate clients, *why* do they want to use Bitcoin and what for\nexactly?\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000054c9d9ae1099ef8bc0bc9b76fef5e03f7edaff66fd817d8\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/cc01f18b/attachment.sig>"
            },
            {
                "author": "Chris Wardell",
                "date": "2015-05-07T18:38:10",
                "message_text_only": "Instead of raising the block size to another static number like 20MB, can\nwe raise it dynamically?\n\nMake the max block size something like:\npow(2, nHeight/100000) * 1MB;  //double every ~2 years\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/fc013153/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-07T18:55:32",
                "message_text_only": "Just to add to the noise, did you consider linear growth?\n\nUnlike exponential growth, it approximates diminishing returns (i.e. tech\nadvances become slower with time). And unlike single step, it will give\npeople time to adapt to new realities.\n\nE.g. 2 MB in 2016, 3 MB in 2017 and so on.\nSo in 20 years we'll get to 20 MB which \"ought to be enough for anybody\".\nBut if miners will find 20 MB blocks too overwhelming, they can limit it\nthrough soft work, based on actual data.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/8308767e/attachment.html>"
            },
            {
                "author": "Ross Nicoll",
                "date": "2015-05-07T18:59:41",
                "message_text_only": "I'm presuming that schedule is just an example, as you'd end up with \ninsanely large block sizes in a few years.\n\nAbsolutely, yes, an increase schedule is an option if people agree on \nit, and I think the better option, as the current limit too low, but \njumping straight to a value big enough for \"indefinitely\" is a huge jump.\n\nGave some thought to scaling block size based on transaction fees, but \nsuspect it would end up with miners sending huge fees to themselves with \ntransactions that aren't relayed (so they only are actioned if they make \nit into a block that miner mines) to make the network allow bigger blocks.\n\nRoss\n\nOn 07/05/2015 19:38, Chris Wardell wrote:\n> Instead of raising the block size to another static number like 20MB, \n> can we raise it dynamically?\n>\n> Make the max block size something like:\n> pow(2, nHeight/100000) * 1MB;  //double every ~2 years\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/bcea0e35/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-07T19:03:52",
                "message_text_only": "Replies inline.\n\nOn 05/07/15 17:43, Mike Hearn wrote:\n>     The only answer to this that anyone with a clue should give is \"it\n>     will very, very likely be able to support at least 1MB blocks\n>     roughly every 10 minutes on average for the next eleven years, and\n>     it seems likely that a block size increase of some form will happen\n>     at some point in the next eleven years\", anything else is dishonest.\n> \n> \n> Matt, you know better than that. Gavin neither lacks clue nor is he\n> dishonest. \n\nNo, I dont think Gavin is being deliberately dishonest, and I'm rather\nconfident he phrased everything in a way that is technically true (ie\nthe quote in his response). However, others have definitely not taken\naway the correct interpretation of what he said, and this is a serious\nproblem. Setting expectations correctly as this is a very contentious\nissue and one that does not appear to be reaching consensus quickly in\nthe technical community is important.\nMore generally, consider the situation we're in now. Gavin is going off\npitching this idea to the general public (which, I agree, is an\nimportant step in pulling off a hardfork) while people who actually\nstudy the issues are left wondering why they're being ignored (ie why is\nthere no consensus-building happening on this list?).\n\n\n> He has been working on the assumption that other developers are\n> reasonable, and some kind of compromise solution can be found that\n> everyone can live with. Hence trying to find a middle ground, hence\n> considering and writing articles in response to every single objection\n> raised. Hence asking for suggestions on what to change about the plan,\n> to make it more acceptable. What more do you want, exactly?\n\nThe appropriate method of doing any fork, that we seem to have been\nfollowing for a long time, is to get consensus here and on IRC and on\ngithub and *then* go pitch to the general public (either directly or by\nreleasing software) that they should upgrade. I admit that hardforks are\na bit different in that the level of software upgrade required means\nadditional lead time, but I'm not sure that means starting the\npublic-pitching phase before there is any kind of consensus forming\n(actually, I'd point out that to me there seems to be rahter clear\nconsensus outside of you and Gavin that we should delay increasing block\nsize).\nAs far as I can tell, there has been no discussion of block sizes on\nthis list since 2013, and while I know Gavin has had many private\nconversations with people in this community about the block size, very\nlittle if any of it has happened in public.\nIf, instead, there had been an intro on the list as \"I think we should\ndo the blocksize increase soon, what do people think?\", the response\ncould likely have focused much more around creating a specific list of\nthings we should do before we (the technical community) think we are\nprepared for a blocksize increase.\n\n> And I'll ask again. Do you have a *specific, credible alternative*?\n> Because so far I'm not seeing one.\n\nA specific credible alternative to what? Committing to blocksize\nincreases tomorrow? Yes, doing more research into this and developing\nsoftware around supporting larger block sizes so people feel comfortable\ndoing it in six months. I acknowledge that Gavin has been putting a lot\nof effort into this front, but, judging by this thread, I am far from\nthe only one who thinks much more needs done."
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T19:13:23",
                "message_text_only": "On Thu, May 7, 2015 at 3:03 PM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n> More generally, consider the situation we're in now. Gavin is going off\n> pitching this idea to the general public (which, I agree, is an\n> important step in pulling off a hardfork) while people who actually\n> study the issues are left wondering why they're being ignored (ie why is\n> there no consensus-building happening on this list?).\n\nThis sub-thread threatens to veer off into he-said-she-said.\n\n> If, instead, there had been an intro on the list as \"I think we should\n> do the blocksize increase soon, what do people think?\", the response\n> could likely have focused much more around creating a specific list of\n> things we should do before we (the technical community) think we are\n> prepared for a blocksize increase.\n\nAgreed, but that is water under the bridge at this point.  You - rightly -\nopened the topic here and now we're discussing it.\n\nMike and Gavin are due the benefit of doubt because making a change to a\nleaderless automaton powered by leaderless open source software is breaking\nnew ground.  I don't focus so much on how we got to this point, but rather,\nwhere we go from here.\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/573b8322/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T19:34:02",
                "message_text_only": ">\n> The appropriate method of doing any fork, that we seem to have been\n> following for a long time, is to get consensus here and on IRC and on\n> github and *then* go pitch to the general public\n\n\nSo your concern is just about the ordering and process of things, and not\nabout the change itself?\n\nI have witnessed many arguments in IRC about block sizes over the years.\nThere was another one just a few weeks ago. Pieter left the channel for his\nown sanity. IRC is not a good medium for arriving at decisions on things -\nmany people can't afford to sit on IRC all day and conversations can be\nhard to follow. Additionally, they tend to go circular.\n\nThat said, I don't know if you can draw a line between the \"ins\" and \"outs\"\nlike that. The general public is watching, commenting and deciding no\nmatter what. Might as well deal with that and debate in a format more\naccessible to all.\n\n\n> If, instead, there had been an intro on the list as \"I think we should\n> do the blocksize increase soon, what do people think?\"\n\n\nThere have been many such discussions over time. On bitcointalk. On reddit.\nOn IRC. At developer conferences. Gavin already knew what many of the\nobjections would be, which is why he started answering them.\n\nBut alright. Let's say he should have started a thread. Thanks for starting\nit for him.\n\nNow, can we get this specific list of things we should do before we're\nprepared?\n\n\n> A specific credible alternative to what? Committing to blocksize\n> increases tomorrow? Yes, doing more research into this and developing\n> software around supporting larger block sizes so people feel comfortable\n> doing it in six months.\n\n\nDo you have a specific research suggestion? Gavin has run simulations\nacross the internet with modified full nodes that use 20mb blocks, using\nreal data from the block chain. They seem to suggest it works OK.\n\nWhat software do you have in mind?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/76327a71/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-07T21:29:01",
                "message_text_only": "On 05/07/15 19:34, Mike Hearn wrote:\n>     The appropriate method of doing any fork, that we seem to have been\n>     following for a long time, is to get consensus here and on IRC and on\n>     github and *then* go pitch to the general public\n> \n> \n> So your concern is just about the ordering and process of things, and\n> not about the change itself?\n\nNo, I'm very concerned about both.\n\n> I have witnessed many arguments in IRC about block sizes over the years.\n> There was another one just a few weeks ago. Pieter left the channel for\n> his own sanity. IRC is not a good medium for arriving at decisions on\n> things - many people can't afford to sit on IRC all day and\n> conversations can be hard to follow. Additionally, they tend to go circular.\n\nI agree, thats why this mailing list was created in the first place\n(well, also because bitcointalk is too full of spam, but close enought :))\n\n> That said, I don't know if you can draw a line between the \"ins\" and\n> \"outs\" like that. The general public is watching, commenting and\n> deciding no matter what. Might as well deal with that and debate in a\n> format more accessible to all.\n\nIts true, just like its true the general public can opt to run any\nversion of software they want. That said, the greater software\ndevelopment community has to update /all/ the software across the entire\necosystem, and thus provide what amounts to a strong recommendation of\nwhich course to take. Additionally, though there are issues (eg if there\nwas a push to remove the total coin limit) which are purely political,\nand thus which should be up to the greater public to decide, the\nblocksize increase is not that. It is intricately tied to Bitcoin's\ndelicate incentive structure, which many of the development community\nare far more farmiliar with than the general Bitcoin public. If there\nwere a listserv that was comprised primarily of people on\n#bitcoin-wizards, I might have suggested a discussion there, first, but\nthere isnt (as far as I know?).\n\n>     If, instead, there had been an intro on the list as \"I think we should\n>     do the blocksize increase soon, what do people think?\"\n> \n> \n> There have been many such discussions over time. On bitcointalk. On\n> reddit. On IRC. At developer conferences. Gavin already knew what many\n> of the objections would be, which is why he started answering them.\n> \n> But alright. Let's say he should have started a thread. Thanks for\n> starting it for him.\n> \n> Now, can we get this specific list of things we should do before we're\n> prepared?\n\nYes....I'm gonna split the topic since this is already far off course\nfor that :).\n\n>     A specific credible alternative to what? Committing to blocksize\n>     increases tomorrow? Yes, doing more research into this and developing\n>     software around supporting larger block sizes so people feel comfortable\n>     doing it in six months. \n> \n> \n> Do you have a specific research suggestion? Gavin has run simulations\n> across the internet with modified full nodes that use 20mb blocks, using\n> real data from the block chain. They seem to suggest it works OK.\n> \n> What software do you have in mind?\n\nLet me answer that in a new thread :)."
            },
            {
                "author": "21E14",
                "date": "2015-05-07T23:05:29",
                "message_text_only": "I am more fazed by PR 5288 and PR 5925 not getting merged in, than by this\nthread. So, casting my ballot in favor of the block size increase. Clearly,\nwe're still rehearsing proper discourse, and that ain't gonna get fixed\nhere and now.\n\nOn Thu, May 7, 2015 at 9:29 PM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n>\n>\n> On 05/07/15 19:34, Mike Hearn wrote:\n> >     The appropriate method of doing any fork, that we seem to have been\n> >     following for a long time, is to get consensus here and on IRC and on\n> >     github and *then* go pitch to the general public\n> >\n> >\n> > So your concern is just about the ordering and process of things, and\n> > not about the change itself?\n>\n> No, I'm very concerned about both.\n>\n> > I have witnessed many arguments in IRC about block sizes over the years.\n> > There was another one just a few weeks ago. Pieter left the channel for\n> > his own sanity. IRC is not a good medium for arriving at decisions on\n> > things - many people can't afford to sit on IRC all day and\n> > conversations can be hard to follow. Additionally, they tend to go\n> circular.\n>\n> I agree, thats why this mailing list was created in the first place\n> (well, also because bitcointalk is too full of spam, but close enought :))\n>\n> > That said, I don't know if you can draw a line between the \"ins\" and\n> > \"outs\" like that. The general public is watching, commenting and\n> > deciding no matter what. Might as well deal with that and debate in a\n> > format more accessible to all.\n>\n> Its true, just like its true the general public can opt to run any\n> version of software they want. That said, the greater software\n> development community has to update /all/ the software across the entire\n> ecosystem, and thus provide what amounts to a strong recommendation of\n> which course to take. Additionally, though there are issues (eg if there\n> was a push to remove the total coin limit) which are purely political,\n> and thus which should be up to the greater public to decide, the\n> blocksize increase is not that. It is intricately tied to Bitcoin's\n> delicate incentive structure, which many of the development community\n> are far more farmiliar with than the general Bitcoin public. If there\n> were a listserv that was comprised primarily of people on\n> #bitcoin-wizards, I might have suggested a discussion there, first, but\n> there isnt (as far as I know?).\n>\n> >     If, instead, there had been an intro on the list as \"I think we\n> should\n> >     do the blocksize increase soon, what do people think?\"\n> >\n> >\n> > There have been many such discussions over time. On bitcointalk. On\n> > reddit. On IRC. At developer conferences. Gavin already knew what many\n> > of the objections would be, which is why he started answering them.\n> >\n> > But alright. Let's say he should have started a thread. Thanks for\n> > starting it for him.\n> >\n> > Now, can we get this specific list of things we should do before we're\n> > prepared?\n>\n> Yes....I'm gonna split the topic since this is already far off course\n> for that :).\n>\n> >     A specific credible alternative to what? Committing to blocksize\n> >     increases tomorrow? Yes, doing more research into this and developing\n> >     software around supporting larger block sizes so people feel\n> comfortable\n> >     doing it in six months.\n> >\n> >\n> > Do you have a specific research suggestion? Gavin has run simulations\n> > across the internet with modified full nodes that use 20mb blocks, using\n> > real data from the block chain. They seem to suggest it works OK.\n> >\n> > What software do you have in mind?\n>\n> Let me answer that in a new thread :).\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/637e045a/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-07T15:33:54",
                "message_text_only": "On Thu, May 7, 2015 at 4:05 PM, Mike Hearn <mike at plan99.net> wrote:\n>> If his explanation was \"I will change my mind after we increase block\n>>\n>> size\", I guess the community should say \"then we will just ignore your\n>> nack because it makes no sense\".\n>\n>\n> Oh good! We can just kick anyone out of the consensus process if we think\n> they make no sense.\n>\n> I guess that means me and Gavin can remove everyone else from the developer\n> consensus, because we think trying to stop Bitcoin growing makes no sense.\n>\n> Do you see the problem with this whole notion? It cannot possibly work.\n> Whenever you try and make the idea of developer consensus work, what you end\n> up with is \"I believe in consensus as long as it goes my way\". Which is\n> worthless.\n\nThat is not what I said. Then you demonstrated that it was absurd.\nThat's called a straw man argument and it's a well known fallacy, it\nis precisely the example of arguments that can be safely ignored.\nIt is an argument against my admittedly vague definition of\n\"non-controversial change\".\nMore importantly, I never said anything about \"removing anyone\", I was\nalways talking about arguments and not people.\nOne person could use fallacious arguments to attack or defend a given\nproposal and use perfectly valid ones in another, a person can even\nmix valid and invalid arguments in the same mail.\n\n>> One thing is the Bitcoin core project where you could argue that the 5\n>> committers decide (I don't know why Wladimir would have any more\n>> authority than the others).\n>\n>\n> Because he is formally the maintainer.\n\nYes, the maintainer of the Bitcoin core free software project (I\ncannot stressed this enough, that can be forked by anyone), not the\npresident of Bitcoin the p2p network.\n\n> Maybe you dislike that idea. It's so .... centralised. So let's say Gavin\n> commits his patch, because his authority is equal to all other committers.\n> Someone else rolls it back. Gavin sets up a cron job to keep committing the\n> patch. Game over.\n>\n> You cannot have committers fighting over what goes in and what doesn't.\n> That's madness. There must be a single decision maker for any given\n> codebase.\n\nI'm sure that if they become that stupid, developers would move to a\nfork of the project in no time.\n\n>> Ok, so in simple terms, you expect people to have to pay enormous fees\n>> and/or wait thousands of blocks for their transactions to get included\n>> in the chain. Is that correct?\n>\n>\n> No. I'll write an article like the others, it's better than email for more\n> complicated discourse.\n\nOk, thanks in advance.\n\n>> As others have said, if the answer is \"forever, adoption is always the\n>> most important thing\" then we will end up with an improved version of Visa.\n>\n>\n> This appears to be another one of those fundamental areas of disagreement. I\n> believe there is no chance of Bitcoin ending up like Visa, even if it is\n> wildly successful. I did the calculations years ago that show that won't\n> happen:\n>\n>     https://en.bitcoin.it/wiki/Scalability\n>\n> Decentralisation is a spectrum and Bitcoin will move around on that spectrum\n> over time. But claiming we have to pick between 1mb blocks and \"Bitcoin =\n> VISA\" is silly.\n\nAgain, I didn't say any of that. My point is that a network that\nbecomes too \"centralized\" (like visa, that is centralized vs p2p, not\nvs distributed) doesn't offer any security or decentralization\nadvantage over current networks (and of course I meant that could\nhappen with larger blocks, not 1 MB blocks).\nI'm sure that's not what the proponents of the size increase want, and\nI'm not defending 1 MB as a sacred limit  or anything, but my question\nis \"where is the limit for them?\"\nEven a limitless block size would technically work because miners\nwould limit it to limit the orphan rate. So \"no hardcoded consensus\nlimit on transaction volume/block size\" could be a valid answer to the\nquestion \"what is the right consensus limit to block size?\" for which\nthere's no real right answer because there is a tradeoff between\ntransaction volume and centralization.\n\nShould we maintain 1 MB forever? Probably not.\nIs 20 MB a bad size? I honestly don't know.\nIs this urgent? I don't think so.\nShould we rush things when we don't have clear answers to many related\nquestions? I don't think so.\n\nYou think that it is too soon to start restricting transaction volume\nin any way. You will answer why in your post.\nWhen is the right time and what is the right limitation then?\n\nI want to have fee competition as soon as possible, at least\ntemporarily. But you say that it can wait for later.\nOk, when do you think we should make that happen then?\nWhen 20 MB are full, will that be the right time to let the fee market\ndevelop then or will it be urgent to increase the block size again?\nShould we directly remove the limit then and let miners handle it as\nthey want?\nIf so, why not now?\nMaybe we can increase to 2 MB, then wait for fee competition, then\nwait for 2 more subsidy halvings and then increase to 11 or 20 MB?\nThere's so many possibilities that I don't understand how can be\nsurprising that \"20 MB, as soon as possible\" is not the obvious answer\nto everyone..."
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T16:11:11",
                "message_text_only": ">\n> It is an argument against my admittedly vague definition of\n> \"non-controversial change\".\n>\n\nIf it's an argument against something you said, it's not a straw man, right\n;)\n\nConsensus has to be defined as agreement between a group of people. Who are\nthose people? If you don't know, it's impossible to decide when there is\nconsensus or not.\n\nRight now there is this nice warm fuzzy notion that decisions in Bitcoin\nCore are made by consensus. \"Controversial\" changes are avoided. I am\ntrying to show you that this is just marketing. Nobody can define what\nthese terms even mean. It would be more accurate to say decisions are\nvetoed by whoever shows up and complains enough, regardless of technical\nmerit. After all, my own getutxo change was merged after a lot of technical\ndebate (and trolling) ..... then unmerged a day later because \"it's a\nshitstorm\".\n\nSo if Gavin showed up and complained a lot about side chains or whatever,\nwhat you're saying is, oh that's different. We'd ignore him. But when\nsomeone else complains about a change they don't like, that's OK.\n\nHeck, I could easily come up with a dozen reasons to object to almost any\nchange, if I felt like it. Would I then be considered not a part of the\nconsensus because that'd be convenient?\n\n\n> I'm sure that's not what the proponents of the size increase want, and\n> I'm not defending 1 MB as a sacred limit  or anything, but my question\n> is \"where is the limit for them?\"\n>\n\n20mb is an arbitrary number, just like 1mb. It's good enough to keep the\nBitcoin ecosystem operating as it presently does: gentle growth in usage\nwith the technology that exists and is implemented. Gavin has discussed in\nhis blog why he chose 20mb, I think. It's the result of some estimates\nbased on average network/hardware capabilities.\n\nPerhaps one day 20mb will not be enough. Perhaps then the limit will be\nraised again, if there is sufficient demand.\n\nYou are correct that \"no limit at all\" is a possible answer. More\nprecisely, in that case miners would choose. Gavin's original proposal was\n20mb+X where X is decided by some incrementing formula over time, chosen to\napproximate expected improvements in hardware and software. That was cool\ntoo. The 20mb figure and the formula were an attempt to address the\nconcerns of people who are worried about the block size increase:  a\nmeet-in-the-middle compromise.\n\nUnfortunately it's hard to know what other kinds of meet-in-the-middle\ncompromise could be made here. I'm sure Gavin would consider them if he\nknew. But the concerns provided are too vague to address. There are no\nnumbers in them, for example:\n\n   - We need more research -> how much more?\n   - I'm not against changing the size, just not now -> then when?\n   - I'm not wedded to 1mb, but not sure 20mb is right -> then what?\n   - Full node count is going down -> then what size do you think would fix\n   that? 100kb?\n   - It will make mining more centralised -> how do you measure that and\n   how much centralisation would you accept?\n\nand so on.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/9b9c6281/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-07T16:47:53",
                "message_text_only": "On Thu, May 7, 2015 at 6:11 PM, Mike Hearn <mike at plan99.net> wrote:\n>> It is an argument against my admittedly vague definition of\n>> \"non-controversial change\".\n>\n>\n> If it's an argument against something you said, it's not a straw man, right\n> ;)\n\nYes, but it was an argument against something I didn't said ;)\n\n> Consensus has to be defined as agreement between a group of people. Who are\n> those people? If you don't know, it's impossible to decide when there is\n> consensus or not.\n>\n> Right now there is this nice warm fuzzy notion that decisions in Bitcoin\n> Core are made by consensus. \"Controversial\" changes are avoided. I am trying\n> to show you that this is just marketing. Nobody can define what these terms\n> even mean. It would be more accurate to say decisions are vetoed by whoever\n> shows up and complains enough, regardless of technical merit.\n\nYes, that's why I drafted a definition for \"uncontroversial change\"\nrather than \"change accepted by consensus\".\nIt will still be vague and hard to define, but consensus seems much harder.\nAnd, yes, you're right, it is more like giving power to anyone with\nvalid arguments to veto hardfork changes.\nBut as you say, that could lead to make hardforks actually impossible,\nso we should limit what constitutes a valid argument.\nI later listed some examples of invalid arguments: logical fallacies,\nunrelated arguments, outright lies.\nCertainly I don't think technical merits should count here or that we\ncould veto a particular person from vetoing.\nWe should filter the arguments, not require an identity layer to\nblacklist individuals.\nWe should even accept arguments from anonymous people in the internet\n(you know, it wouldn't be the first time).\n\n> Unfortunately it's hard to know what other kinds of meet-in-the-middle\n> compromise could be made here. I'm sure Gavin would consider them if he\n> knew. But the concerns provided are too vague to address. There are no\n> numbers in them, for example:\n>\n> We need more research -> how much more?\n\nSome research at all about fee market dynamics with limited size that\nhasn't happened at all.\nIf we're increasing the consensus max size maybe we could at least\nmaintain the 1MB limit as a standard policy limit, so that we can\nstudy it a little bit (like we could have done instead of removing the\ninitial policy limit).\n\n> I'm not against changing the size, just not now -> then when?\n\nI don't know yet, but I understand now that having a clearer roadmap\nis what's actually urgent, not the change itself.\n\n> I'm not wedded to 1mb, but not sure 20mb is right -> then what?\n\nWhat about 2 MB consensus limit and 1 MB policy limit for now? I know\nthat's arbitrary too.\n\n> Full node count is going down -> then what size do you think would fix that?\n> 100kb?\n\nAs others have explained, the number of full nodes is not the\nimprotant part, but how easy it is to run one.\nI think a modest laptop with the average internet connection of say,\nIndia or Brazil, should be able to run a full node.\nI haven't made those numbers myself but I'm sure that's possible with\n1 MB blocks today, and probably with 2 MB blocks too.\n\n> It will make mining more centralised -> how do you measure that and how much\n> centralisation would you accept?\n\nThis is an excellent question for both sides.\nUnfortunately I don't know the answer to this. Do you?"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-07T16:59:13",
                "message_text_only": "Fee dynamics seems to come up over and over again in these discussions,\nwith lots of talk and theorizing.\n\nI hope some data on what is happening with fees right now might help, so I\nwrote another blog post (with graphs, which can't be done in a mailing list\npost):\n   http://gavinandresen.ninja/the-myth-of-not-full-blocks\n\nWe don\u2019t need 100% full one megabyte blocks to start to learn about what is\nlikely to happen as transaction volume rises and/or the one megabyte block\nsize limit is raised.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/2b7608ba/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T17:42:20",
                "message_text_only": "On Thu, May 07, 2015 at 12:59:13PM -0400, Gavin Andresen wrote:\n> Fee dynamics seems to come up over and over again in these discussions,\n> with lots of talk and theorizing.\n> \n> I hope some data on what is happening with fees right now might help, so I\n> wrote another blog post (with graphs, which can't be done in a mailing list\n> post):\n>    http://gavinandresen.ninja/the-myth-of-not-full-blocks\n> \n> We don\u2019t need 100% full one megabyte blocks to start to learn about what is\n> likely to happen as transaction volume rises and/or the one megabyte block\n> size limit is raised.\n\nSounds like you're saying we are bumping up against a 1MB limit. However\nother than the occasional user who has sent a transaction with an\nextremely low/no fee, what evidence do we have that this is or is not\nactually impacting meaningful usage form the user's point of view?\n\nDo we have evidence as to how users are coping? e.g. do they send time\nsensitive transactiosn with higher fees? Are people conciously moving\nlow value transactions off the blockchain? Equally, what about the story\nwith companies? You of course are an advisor to Coinbase, and could give\nus some insight into the type of planning payment processors/wallets are\ndoing.  For instance, does Coinbase have any plans to work with other\nwallet providers/payment processors to aggregate fund transfers between\nwallet providers - an obvious payment channel application.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000232164c96eaa6bf7cbc3dc61ea055840715b5a81ee8f6be\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/60aac99a/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-07T18:05:22",
                "message_text_only": "On Thu, May 7, 2015 at 6:59 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> Fee dynamics seems to come up over and over again in these discussions, with\n> lots of talk and theorizing.\n>\n> I hope some data on what is happening with fees right now might help, so I\n> wrote another blog post (with graphs, which can't be done in a mailing list\n> post):\n>    http://gavinandresen.ninja/the-myth-of-not-full-blocks\n>\n> We don\u2019t need 100% full one megabyte blocks to start to learn about what is\n> likely to happen as transaction volume rises and/or the one megabyte block\n> size limit is raised.\n\nOk, the fact that the fee increases the probability of getting\nincluded faster already is a good thing, the graphs with the\nprobability of getting included in the next block were less important\nto me.\nAlthough scarce space (beyond what miners chose to limit by\nthemselves) would increase the fee competition, I didn't knew that\nthere is actually some competition happening already.\nSo I guess this diminishes the argument for maintaining the limits\nlonger to observe the results of more scarce space.\nStill, I think maintaining a lower policy limit it's a good idea, even\nif we decide not to use it to observe that soon.\nFor example, say we chose the 20 MB consensus limit, we can maintain\nthe policy limit at 1 MB or move it to 2 MB, and slowly moving it up\nlater as needed without requiring everyone to upgrade.\nOf course, not all miners have to follow the standard policy, but at\nleast it's something.\nSo please take this as a suggestion to improve your proposal. You can\nargue it like this \"if we want to maintain the limits after the\nhardfork or increase them slowly, for observing fee dynamics with more\nscarce space or for any other reason, those limits can be partially\nenforced by the standard policy\". I mean, I think that could be a\nreasonable compromise for that concrete line of arguments."
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-07T19:57:20",
                "message_text_only": "On Thu, May 7, 2015 at 5:11 PM, Mike Hearn <mike at plan99.net> wrote:\n\n> Right now there is this nice warm fuzzy notion that decisions in Bitcoin\n>> Core are made by consensus. \"Controversial\" changes are avoided. I am\n>> trying to show you that this is just marketing.\n>\n>\nConsensus is arrived when the people who are most active at the time\n(active in contributing to discussions, code review, giving opinions etc.)\nagreed to ACK. There are a regular staple of active contributors. Bitcoin\ndevelopment is clearly a meritocracy. The more people participate and\ncontribute the more weight their opinions hold.\n\n\n> Nobody can define what these terms even mean. It would be more accurate to\n>> say decisions are vetoed by whoever shows up and complains enough,\n>> regardless of technical merit. After all, my own getutxo change was merged\n>> after a lot of technical debate (and trolling) ..... then unmerged a day\n>> later because \"it's a shitstorm\".\n>\n>\nI am not sure that is fair, your PR was reverted because someone found a\nhuge exploit in your PR enough to invalidate all your arguments used to get\nit merged in the first place.\n\n\n> So if Gavin showed up and complained a lot about side chains or whatever,\n> what you're saying is, oh that's different. We'd ignore him. But when\n> someone else complains about a change they don't like, that's OK.\n>\n> Heck, I could easily come up with a dozen reasons to object to almost any\n> change, if I felt like it. Would I then be considered not a part of the\n> consensus because that'd be convenient?\n>\n\nI don't think it's as simple as that. Objections for the sake of\nobjections, or unsound technical objections are going to be seen for what\nthey are. This is a project with of some of the brightest people in the\nworld in this field. Sure people can be disruptive but their reputation\nstand the test of time.\n\nThe consensus system might not be perfect, but it almost feels like you\nwant to declare a state of emergency and suspend all the normal review\nprocess for this proposed hard fork.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/6d310e9c/attachment.html>"
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-07T15:39:32",
                "message_text_only": "On Thu, May 7, 2015 at 3:05 PM, Mike Hearn <mike at plan99.net> wrote:\n>\n> Maybe you dislike that idea. It's so .... centralised. So let's say Gavin\n> commits his patch, because his authority is equal to all other committers.\n> Someone else rolls it back. Gavin sets up a cron job to keep committing the\n> patch. Game over.\n>\n> You cannot have committers fighting over what goes in and what doesn't.\n> That's madness. There must be a single decision maker for any given\n> codebase.\n>\n\nYou are conflating consensus with commit access. People with commit access\nare maintainers who are *able to merge* pull requests. However, the rules\nfor bitcoin development are that only patches with consensus get merged. If\nany of the maintainers just pushed a change without going through the whole\ncode review and consensus process there would be uproar, plain and simple.\n\nPlease don't conflate commit access with permission to merge because it's\njust not the case. No-one can sidestep the requirement to get consensus,\nnot even the 5 maintainers.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/3061743b/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T13:02:40",
                "message_text_only": "On Thu, May 07, 2015 at 01:29:44PM +0200, Mike Hearn wrote:\n> What if Gavin popped up right now and said he disagreed with every current\n> proposal, he disagreed with side chains too, and there would be no\n> consensus on any of them until the block size limit was raised.\n> \n> Would you say, oh, OK, guess that's it then. There's no consensus so might\n> as well scrap all those proposals, as they'll never happen anyway. Bye bye\n> side chains whitepaper.\n\nIf Gavin had good points to make, he'd probably eventually change\neveryone's mind.\n\nBut if he fails to do that at some point he'll just get ignored and for\nall practical purposes won't be considered part of the consensus. Not\nunlike how if someone suggested we power the blockchain via perpetual\nmotion machines they'd be ignored. Bitcoin is after all a decentralized\nsystem so all power over the development process is held only by social\nconsent and respect.\n\nAt that point I'd suggest Gavin fork the project and go to the next\nlevel of consensus gathering, the community at large; I'm noticing this\nis exactly what you and Gavin are doing.\n\nSpeaking of, are you and Gavin still thinking about forking Bitcoin\nCore? If so I wish you the best of luck.\n\nSent: Wednesday, July 23, 2014 at 2:42 PM\nFrom: \"Mike Hearn\" <mike at plan99.net>\nTo: \"Satoshi Nakamoto\" <satoshin at gmx.com>\nSubject: Thinking about a fork\nI don't expect a reply, just getting some thoughts off my chest. Writing them down helps.\n\nForking Bitcoin-Qt/Core has been coming up more and more often lately in conversation (up from zero not that long ago). Gavin even suggested me and him fork it ... I pointed out that maintainers don't normally fork their own software :)\n\nThe problem is that the current community of developers has largely lost interest in supporting SPV wallets. Indeed any protocol change that might mean any risk at all, for anyone, is now being bogged down in endless circular arguments that never end. The Bitcoin developers have effectively become the new financial regulators: restricting options within their jurisdiction with \"someone might do something risky\" being used as the justification.\n\nIf alternative low-risk protocols were always easily available this would be no problem, but often they require enormous coding and deployment effort or just don't exist at all. Yet, wallets must move forward. If we carry on as now there simply won't be any usable decentralised wallets left and Bitcoin will have become an energy-wasting backbone for a bunch of banks and payment processors. That's so far from your original vision, it's sad.\n\nI know you won't return and that's wise, but sometimes I wish you'd left a clearer design manifesto before handing the reigns over to Gavin, who is increasingly burned out due to all the arguments (as am I).\n\nSource: https://www.reddit.com/r/Bitcoin/comments/2g9c0j/satoshi_email_leak/\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000066f25b3196b51d30df5c1678fc206fdf55b65dd6e593b05\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/077ae65e/attachment.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-07T19:14:48",
                "message_text_only": "On 05/07/15 11:29, Mike Hearn wrote:\n>     Can you please elaborate on what terrible things will happen if we\n>     don't increase the block size by winter this year?\n> \n> \n> I was referring to winter next year. 0.12 isn't scheduled until the end\n> of the year, according to Wladimir. I explained where this figure comes\n> from in this article:\n\nOn a related note, I'd like to agree strongly with Peter Todd that we\nshould get away from doing forks-only-in-releases. We can add code to do\na fork and then enable it in 0.11.1 or 0.11.11 if Gavin prefers more 11s."
            },
            {
                "author": "Dave Hudson",
                "date": "2015-05-07T11:55:49",
                "message_text_only": "> On 7 May 2015, at 11:52, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> \n> On Thu, May 7, 2015 at 11:25 AM, Mike Hearn <mike at plan99.net> wrote:\n>> I observed to Wladimir and Gavin in private that this timeline meant a change to the block size was unlikely to get into 0.11, leaving only 0.12, which would give everyone only a few months to upgrade in order to fork the chain by the end of the winter growth season. That seemed tight.\n> \n> Can you please elaborate on what terrible things will happen if we\n> don't increase the block size by winter this year?\n> I assume that you are expecting full blocks by then, have you used any\n> statistical technique to come up with that date or is it just your\n> guess?\n> Because I love wild guesses and mine is that full 1 MB blocks will not\n> happen until June 2017.\n\nI've been looking at this problem for quite a while (Gavin cited some of my work a few days ago) so thought I'd chime in with a few thoughts (some of which I've not published). I believe the major problem here is that this isn't just an engineering decision; the reaction of the miners will actually determine the success or failure of any course of action. In fact any decision forced upon them may backfire if they collectively take exception to it. It's worth bearing in mind that most of the hash rate is now under the control of relatively large companies, many of whom have investors who are expecting to see returns; it probably isn't sufficient to just expect them to \"do the right thing\".\n\nWe're seeing plenty of full 1M byte blocks already and have been for months. Typically whenever we have one of the large inter-block gaps then these are often followed by one (and sometimes several) completely full blocks (full by the definition of whatever the miner wanted to use as a size limit).\n\nThe problem with this particular discussion is that there are quite a few \"knowns\" but an equally large number of \"unknowns\". Let's look at them:\n\nKnown: There has been a steady trend towards the mean block size getting larger. See https://blockchain.info/charts/avg-block-size?timespan=all&showDataPoints=false&daysAverageString=7&show_header=true&scale=0&address= <https://blockchain.info/charts/avg-block-size?timespan=all&showDataPoints=false&daysAverageString=7&show_header=true&scale=0&address=>\n\nKnown: Now the trend was definitely increasing quite quickly last year but for the last few months has been slowing down, however we did see pretty much a 2x increase in mean block sizes in 2014.\n\nKnown: For most of 2015 we've actually been seeing that rate slow quite dramatically, but the total numbers of transactions are still rising so we're seeing mean transaction sizes have been reducing, and that tallies with seeing more transactions per block: https://blockchain.info/charts/n-transactions-per-block?timespan=all&showDataPoints=false&daysAverageString=7&show_header=true&scale=0&address= <https://blockchain.info/charts/n-transactions-per-block?timespan=all&showDataPoints=false&daysAverageString=7&show_header=true&scale=0&address=>\n\nUnknown: Why are seeing more smaller transactions? Are we simply seeing more efficient use of blockchain resources or have some large consumers of block space going away? How much more block space compression might be possible in, say, the next 12 months?\n\nKnown: If we reach the point where all blocks are 1M bytes then there's a major problem in terms of transaction confirmation. I published an analysis of the impact of different mean block sizes against confirmation times: http://hashingit.com/analysis/34-bitcoin-traffic-bulletin <http://hashingit.com/analysis/34-bitcoin-traffic-bulletin>. The current 35% to 45% mean block size doesn't have a huge impact on transaction confirmations (assuming equal fees for all) but once we're up at 80% then things start to get unpleasant. Instead of 50% of first confirmations taking about 7 minutes they instead take nearer to 19 minutes.\n\nKnown: There are currently a reasonably large number of zero-fee transactions getting relayed and mined. If things start to slow down then there will be a huge incentive to delay them (or drop them altogether).\n\nUnknown: If block space starts to get more scarce then how will this affect the use of the blockchain? Do the zero-fee TXs move to some batched transfer solution via third party? Do people start to get smarter about how TXs are encoded? Do some TXs go away completely (there are a lot of long-chain transactions that may simply be \"noise\" creating an artificially inflated view of transaction volumes)?\n\nKnown: There's a major problem looming for miners at the next block reward halving. Many are already in a bad place and without meaningful fees then sans a 2x increase in the USD:BTC ratio then many will simply have to leave the network, increasing centralisation risks. There seems to be a fairly pervasive assumption that the 300-ish MW of power that they currently use is going to pay for itself (ignoring capital and other operating costs).\n\nUnknown: If the block size is increased and yet more negligible fee transactions are dumped onto the network then that might well motivate some large fraction of miners to start to clamp block sizes or reject transactions below a certain fee threshold; they can easily create their own artificial scarcity if enough of them feel it is in their interest (it's not the most tricky setting to change). One can well imagine VC investors in mining groups asking why they're essentially subsidising all of the other VC-funded Bitcoin startups.\n\nKnown: the orphan rate is still pretty-high even with everyone's fast connections. If we assume that 20M byte blocks become possible then that's likely to increase.\n\nUnknown: What are the security implications for larger blocks (this one (at least) can be simulated though)? For example, could large blocks with huge numbers of trivial transactions be used to put other validators at a disadvantage in a variant of a selfish mining attack? I've seen objections that such bad actors could be blacklisted in the future but it's not clear to me how. A private mining pool can trivially be made to appear like 100 pools of 1% of the size without significantly affecting the economics of running that private mine.\n\n\nCheers,\nDave\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/360dfd3b/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-07T13:40:23",
                "message_text_only": "On Thu, May 7, 2015 at 1:55 PM, Dave Hudson <dave at hashingit.com> wrote:\n> Known: There has been a steady trend towards the mean block size getting\n> larger. See\n> https://blockchain.info/charts/avg-block-size?timespan=all&showDataPoints=false&daysAverageString=7&show_header=true&scale=0&address=\n\nLooking at this graph and in retrospective, we shouldn't have removed\nthe standard policy limit without observing the supposedly disastrous\neffects of hitting the limit first.\nRemoving the standard limit would have been trivial (bdb issues aside)\nat any point after seeing the effects.\n\n> Known: If we reach the point where all blocks are 1M bytes then there's a\n> major problem in terms of transaction confirmation. I published an analysis\n> of the impact of different mean block sizes against confirmation times:\n> http://hashingit.com/analysis/34-bitcoin-traffic-bulletin. The current 35%\n> to 45% mean block size doesn't have a huge impact on transaction\n> confirmations (assuming equal fees for all) but once we're up at 80% then\n> things start to get unpleasant. Instead of 50% of first confirmations taking\n> about 7 minutes they instead take nearer to 19 minutes.\n\nWell, this is only for first confirmations of free transaction.\nA higher fee should increase your probabilities, but if you're sending\nfree transactions you may not care about them taking longer to be\nincluded.\n\n> Known: There are currently a reasonably large number of zero-fee\n> transactions getting relayed and mined. If things start to slow down then\n> there will be a huge incentive to delay them (or drop them altogether).\n\nWell, maybe \"instant and free\" it's not a honest form of bitcoin\nmarketing and it just has to disappear.\nMaybe we just need to start being more honest about pow being good for\nprocessing micro-transactions: it is not.\nHopefully lightning will be good for that.\nFree and fast in-chain transactions is something temporary that we\nknow will eventually disappear.\nIf people think it would be a adoption disaster that it happens soon,\nthen they could also detail an alternative plan to roll that out\ninstead of letting it happen.\nBut if the plan is to delay it forever...then I'm absolutely against.\n\n> Known: There's a major problem looming for miners at the next block reward\n> halving. Many are already in a bad place and without meaningful fees then\n> sans a 2x increase in the USD:BTC ratio then many will simply have to leave\n> the network, increasing centralisation risks. There seems to be a fairly\n> pervasive assumption that the 300-ish MW of power that they currently use is\n> going to pay for itself (ignoring capital and other operating costs).\n\nI take this as an argument for increasing fee competition and thus,\nagainst increasing the block size.\n\n> Known: the orphan rate is still pretty-high even with everyone's fast\n> connections. If we assume that 20M byte blocks become possible then that's\n> likely to increase.\n>\n> Unknown: What are the security implications for larger blocks (this one (at\n> least) can be simulated though)? For example, could large blocks with huge\n> numbers of trivial transactions be used to put other validators at a\n> disadvantage in a variant of a selfish mining attack? I've seen objections\n> that such bad actors could be blacklisted in the future but it's not clear\n> to me how. A private mining pool can trivially be made to appear like 100\n> pools of 1% of the size without significantly affecting the economics of\n> running that private mine.\n\nNo blacklisting, please, that's centralized.\nIn any case, a related known: bigger blocks give competitive advantage\nto bigger miners."
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-08T04:46:48",
                "message_text_only": "On 5/7/2015 6:40 AM, Jorge Tim\u00f3n wrote:\n>> Known: There's a major problem looming for miners at the next block reward\n>> halving. Many are already in a bad place and without meaningful fees then\n>> sans a 2x increase in the USD:BTC ratio then many will simply have to leave\n>> the network, increasing centralisation risks. There seems to be a fairly\n>> pervasive assumption that the 300-ish MW of power that they currently use is\n>> going to pay for itself (ignoring capital and other operating costs).\n> I take this as an argument for increasing fee competition and thus,\n> against increasing the block size.\n>\n\nThat doesn't follow.  Supposing average fees per transaction decrease\nwith block size, total fees / block reach an optimum somewhere.  While\nthe optimum might be at infinity, it's certainly not at zero, and it's\nnot at all obvious that the optimum is at a block size lower than 1MB."
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T14:04:21",
                "message_text_only": "I have a lot more written down, a WIP; here are the highlights.\n\n- The 1MB limit is an ancient anti-spam limit, and needs to go.\n\n- The 1MB limit is economically entrenched at this point, and cannot be\nremoved at a whim.\n\n- This is a major change to the economics of a $3.2B system.  This change\npicks winners and losers.  There is attendant moral hazard.\n\n- The core dev team is not and should not be an FOMC.\n\n- The bar for \"major economic change to a $3.2B system\" should necessarily\nbe high.  In the more boring world of investments, this would accompanied\nby Due Diligence including but not limited to projections for success,\nfailure scenarios, upside risks and downside risks.  Projections and\nfact-based simulations.\n\n- There are significant disruption risks on the pro (change it) and con\n(keep 1MB) sides of the debate.\n\n- People are privately lobbying Gavin for this.  That is the wrong way to\ngo.   I have pushed for a more public debate, and public endorsements (or\ncondemnations) from major miners, merchants, payment processors,\nstackholders, ...   It is unfair to criticize Gavin to doing this.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/a47b125e/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T14:32:34",
                "message_text_only": "On Thu, May 07, 2015 at 10:04:21AM -0400, Jeff Garzik wrote:\n> I have a lot more written down, a WIP; here are the highlights.\n> \n> - The 1MB limit is an ancient anti-spam limit, and needs to go.\n> \n> - The 1MB limit is economically entrenched at this point, and cannot be\n> removed at a whim.\n> \n> - This is a major change to the economics of a $3.2B system.  This change\n> picks winners and losers.  There is attendant moral hazard.\n> \n> - The core dev team is not and should not be an FOMC.\n> \n> - The bar for \"major economic change to a $3.2B system\" should necessarily\n> be high.  In the more boring world of investments, this would accompanied\n> by Due Diligence including but not limited to projections for success,\n> failure scenarios, upside risks and downside risks.  Projections and\n> fact-based simulations.\n> \n> - There are significant disruption risks on the pro (change it) and con\n> (keep 1MB) sides of the debate.\n> \n> - People are privately lobbying Gavin for this.  That is the wrong way to\n> go.   I have pushed for a more public debate, and public endorsements (or\n> condemnations) from major miners, merchants, payment processors,\n> stackholders, ...   It is unfair to criticize Gavin to doing this.\n\nThe hard part here will be including the players who aren't individually\n\"major\", but are collectively important; who is the community?\n\nHow do you give the small merchants a voice in this discussion? The\nsmall time traders? The small time miners? The people in repressive\ncountries who are trying to transact on thier own terms?\n\nLegality? Should people involved in 3rd world remittances be\nincluded? Even if what they're doing is technically illegal? What about\ndark markets? If DPR voiced his opinion, should we ignore it?\n\nPersonally, I'm dubious about trying to make ecosystem-wide decisions\nlike this without cryptographic consensus; fuzzy human social consensus\nis easy to manipulate.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000013e67b343b1f6d75cc87dfb54430bdb3bcf66d8d4b3ef6b8\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/f7904ca9/attachment.sig>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T14:38:22",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 04:04 PM, Jeff Garzik wrote:\n> - This is a major change to the economics of a $3.2B system.  This\n> change picks winners and losers.  There is attendant moral hazard.\n\nThis is exactly true.\n\nThere are a number of projects which aren't Bitcoin that benefit from\nfilling in the gap left by Bitcoin's restricted transaction rate\ncapability.\n\nIf Bitcoin fills that gap, Bitcoin wins and those other projects lose.\n\nShould decisions about Bitcoin development take into account the\ndesires of competing projects?\n\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVS3jeAAoJECpf2nDq2eYj3hMP/0yk8HxypEfNa4vZo0IcKD+p\nbn2dftQsOEeOenBh8QT48vS3AhcjNkUsw722YwbKz6Znkyi2iU7njaUM9DV+QHwg\nOytmh8XVZtviLgg3854ujdj4oAWyP4DpppVTRxTDyRdSpRj+D9y6+sGFls6z0q3/\nXRcKOY23zx6/qN1k5fqUncpIpYEDhpmE7cGy26Yz0G4MtuYeceHT4LdJAHHr0iFL\nOY0WVM32b4F3HfkfJtt8rE0yeB7u5dbeu8KmLB0yqZQkY87sLxtT6qeoyHO6CG+N\n8Iu9OWaRIZHfrZK2XlDzDKQIkTnlSxFtj4wY7/Yb4NIDO6mhMjYTSz8lWqN4ofKg\n9fFHlwGS3QXXDTB+5d1IzZS5C0qF92n1NJiJjkLqhKqYuVn4U74oslZhVLxHBGHH\nZAvW09obZXi5DVzhuxPzlFkpYaB+XLdmBUPEr5hx5K4I2qiL/Nvu0h031UDcMeLm\nx9mEHO5ZODlF9tWAVnM/b0VtwT9h6Q88NWe/OUQQZKp6D/Etcd3JE55GBNtNPDnE\n2UubyHkNO4mbrEMluh24TvhZK3BB/lieq+kkHZCP7eC58eRY078lSF8R36XGdbn4\nPili15bYSrRrfmjDz24zhJX8759LPt2Zsf/Irc8Za4SoaEaYAqQU4vAmYlZyCNxj\nEvxXAasffnjR2K3cnZxr\n=YkTz\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/08be2d04/attachment.bin>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T14:49:18",
                "message_text_only": "On Thu, May 07, 2015 at 04:38:22PM +0200, Justus Ranvier wrote:\n> On 05/07/2015 04:04 PM, Jeff Garzik wrote:\n> > - This is a major change to the economics of a $3.2B system.  This\n> > change picks winners and losers.  There is attendant moral hazard.\n> \n> This is exactly true.\n> \n> There are a number of projects which aren't Bitcoin that benefit from\n> filling in the gap left by Bitcoin's restricted transaction rate\n> capability.\n> \n> If Bitcoin fills that gap, Bitcoin wins and those other projects lose.\n> \n> Should decisions about Bitcoin development take into account the\n> desires of competing projects?\n\nWell, basically you're asking if we shouldn't assume the people in this\ndiscussion have honest intentions. If you want to go down that path,\nkeep in mind where it leads.\n\nI think we'll find an basic assumption of civility to be more\nproductive, until proven otherwise. (e.g. NSA ties)\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000d49f263bbbb80f264abc7cc930fc9cbc7ba80ac068d9648\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/b35acd17/attachment.sig>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T15:13:34",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 04:49 PM, Peter Todd wrote:\n> \n> I think we'll find an basic assumption of civility to be more \n> productive, until proven otherwise. (e.g. NSA ties)\n\nI'm not sure why you'd construe my post as having anything to do with\naccusations like NSA ties.\n\nBy \"non-Bitcoin\" projects I mean any altcoin or off-chain processing\nsolution.\n\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVS4EeAAoJECpf2nDq2eYj/vcQAIUrD+ejUKHQb0k/pcPyzmvy\nrl3spbbdLSFN9cBhBOgh5LaVFkCrv4/gW2X4Ih6GGYG3siXjZ2HPDXt+Zbs+bfQE\nnrw+IpGTYlbnJ26cFVhZWehr45qY1kMO+DVdnKufEgfVKUdYeq/d3bwL1uru4RU6\nUfWihvgGGQkjEb/5ZIpRbWmb7XRP9piZCHi0pgFSa8tNDVjbb9ucKjIfrRuRe+DK\nGMhIAQLvIQK4M30SxOnMQLIe3upsQ6JzY+5M28HkcBNKgd0dpZbwByHIJh6/ELTO\nIaf08S0mCySKoZAJFEkeQ3YOgdIlZvwYsflxiEs62Mz9Mz8uuxTo6E21XyFr6iN/\nXndXCzlAZBIQuiayEUL4fUM2cmeHcvhHpGNyYjBuLibuiaIzKMBzFQqEZHGA0QzH\nQhptbHjTwXLxIEZy94ELH2FbQnTrnOBxOdYfmxGvlmJ0328hThW6N181L3fPHK0v\n6zTChZziMhlIoZPX8AGNNsUYJFKBJs/khlbse/tQhXmm5zuIyq+Lt0nKjbhHkWJw\nn9y4PHxLVtmmOvptPMm00l5/w6yb8Qmxo81d6kq75ZEupjxupHH6YwjHWTehT/x2\nPt8iMX2NWVnVwVdsaqE/rH+JrgH1Pvl7TMqXMr8d7tuSWTeTBWlrcmZbS1rl0Z3T\nf8K2rBX6sBqmrD1xKDsn\n=5pB6\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/4f43f1c8/attachment.bin>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-07T15:25:03",
                "message_text_only": "On Thu, May 07, 2015 at 05:13:34PM +0200, Justus Ranvier wrote:\n> On 05/07/2015 04:49 PM, Peter Todd wrote:\n> > \n> > I think we'll find an basic assumption of civility to be more \n> > productive, until proven otherwise. (e.g. NSA ties)\n> \n> I'm not sure why you'd construe my post as having anything to do with\n> accusations like NSA ties.\n\nI'm not.\n\nI'm saying dealing with someone with proven NSA ties is one of the few\ntimes when I think the assumption of honest intent should be ignored in\nthis forum.\n\nAltcoins and non-Bitcoin-blockchain tx systems? Assuming anything other\nthan honest intent isn't productive in this forum.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000622ff7c71c105480baf123fe74df549b5a42596fd8bfbcb\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/ae855353/attachment.sig>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T15:04:58",
                "message_text_only": "On Thu, May 7, 2015 at 10:38 AM, Justus Ranvier <justusranvier at riseup.net>\nwrote:\n\n> On 05/07/2015 04:04 PM, Jeff Garzik wrote:\n> > - This is a major change to the economics of a $3.2B system.  This\n> > change picks winners and losers.  There is attendant moral hazard.\n>\n> This is exactly true.\n>\n> There are a number of projects which aren't Bitcoin that benefit from\n> filling in the gap left by Bitcoin's restricted transaction rate\n> capability.\n>\n> If Bitcoin fills that gap, Bitcoin wins and those other projects lose.\n>\n> Should decisions about Bitcoin development take into account the\n> desires of competing projects?\n\n\nheh - I tend to think people here want bitcoin to succeed.  My statement\nrefers to picking winners and losers from within the existing bitcoin\ncommunity & stakeholders.\n\nThe existential question of the block size increase is larger - will\nfailing to increase the 1MB limit permanently stunt bitcoin's growth?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/c23956f4/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T15:16:50",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 05:04 PM, Jeff Garzik wrote:\n> heh - I tend to think people here want bitcoin to succeed.  My\n> statement refers to picking winners and losers from within the\n> existing bitcoin community & stakeholders.\n\n\"Success\" is not a sufficiently precise term in this context.\n\nThere is a large contingent of people for whom the definition of\nBitcoin \"success\" means serving as a stable backend which can meet the\nneeds of their non-Bitcoin platform - and nothing more.\n\nTo be extremely specific: should Bitcoin development intenionally\nlimit the network's capabilities to leave room for other projects, or\nshould Bitcoin attempt to be the best system possible and let the\nother projects try to keep up as best they can?\n\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVS4HiAAoJECpf2nDq2eYj23wP/j4ksm2dgzDkccMRbqFM8Pm8\noV6ImxM26bG3DJB+Rh6ttTY4DrUnZJmzQUUxfZUd3TmH/xOM4Lu35gKKhpvHTdR8\nvMQz76CaTba6PzzFKC+GYyHueXLtwJxEHEZjR8m5ijPMfZoImfMbduggDaPLv/sz\nAUcTDtYWBoPZ9Matms4NZIOsH1S1pHw5YjcFYgxmY6ErHZPqZjoKzcc4wZnrOU+Q\nHCmiHOJ1U87jEge4QEJCXidCJFakyMTWt5P6hjdOFfky3VYmcoivYRA1ZemgyV2Y\nYyLtmHBcK7k67Tczep8rjggvK2C2oJArFGPLWHZH9bxXILaNXSpZX5G5rXZjp1vm\n1voc6JDaK/slXIlfG+BZ56WyprKkiFbN6u4Wd8LG5W8gKiuCyLYr2IGKz9O3fvor\nNYtk6ELPfX1+0JBD0ureI7kV85D/ybNnnmMp/NyfmBKzzmqnANRrrqL5zgILuUP4\nYaokcVdPpTqkN0vuAXchehEemF5MtJIYf9BayZ86ck68aMjvVJi0nX3n63f1MulP\nIbRbYY/8eu1891lNIPiSzbmT0zjjplo8jYEOTg32mIvEDZAy8sWwTPYS25tPd37l\n3kxRCxqS1ALbAqLZprmxQ375PigE2esXZlpBHzyY4Kf+3UD/k/X8D92vdNiF7mkS\nHSA+TX4lf310Eq6Mb4LR\n=5vaU\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/93fd7d2d/attachment.bin>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T15:27:39",
                "message_text_only": "On Thu, May 7, 2015 at 11:16 AM, Justus Ranvier <justusranvier at riseup.net>\nwrote:\n\n> To be extremely specific: should Bitcoin development intenionally\n> limit the network's capabilities to leave room for other projects, or\n> should Bitcoin attempt to be the best system possible and let the\n> other projects try to keep up as best they can?\n>\n\n\nAvoid such narrow, binary thinking.\n\nReferencing the problem described in\nhttp://gavinandresen.ninja/why-increasing-the-max-block-size-is-urgent\n(not the solution - block size change - just the problem, tx/block Poisson\nmismatch)\n\nThis problem - block creation is bursty - is fundamental to bitcoin.\nRaising block size does not fix this problem (as [1] notes), but merely\nkicks the can down the road a bit, by hiding it from users a bit longer.\n\nBitcoin is a settlement system, at the most fundamental engineering level.\nIt will never be an instant payment system for all the world's coffees (or\nall the world's stock trades).  It is left to \"Layer 2\" projects to\nengineer around bitcoin's gaps, to produce an instant, secure, trustless,\negalitarian payment system using the bitcoin token.  [1] also notes this.\n\nIt is therefore not a binary decision of leaving room for other projects,\nor not.  Layer-2 projects are critical to the success of bitcoin, and\ncomplement bitcoin.\n\n\n\n\n\n\n[1] http://gavinandresen.ninja/it-must-be-done-but-is-not-a-panacea\n\nHolistic thinking implies you build a full-stack system with bitcoin\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/53f7c4d5/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T15:33:45",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 03:27 PM, Jeff Garzik wrote:\n> On Thu, May 7, 2015 at 11:16 AM, Justus Ranvier\n> <justusranvier at riseup.net> wrote:\n> \n>> To be extremely specific: should Bitcoin development\n>> intenionally limit the network's capabilities to leave room for\n>> other projects, or should Bitcoin attempt to be the best system\n>> possible and let the other projects try to keep up as best they\n>> can?\n>> \n> \n> \n> Avoid such narrow, binary thinking.\n\nOn 05/07/2015 03:25 PM, Peter Todd wrote:\n> Altcoins and non-Bitcoin-blockchain tx systems? Assuming anything \n> other than honest intent isn't productive in this forum.\n> \n\n\nIn summary, I asked a question neither you, nor Peter Todd, want to\nanswer and want to actively discourage people from even asking at all.\n\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2\n\niQIcBAEBAgAGBQJVS4XZAAoJECpf2nDq2eYjwZcP/j4ypIBctC4Wt71KJCx4eJ3u\nu8DQAJKKr8BfL/zDu5bwVz0qbIX1+Wv9EwkBSYuYQaLDozDCnlttptr7qNWm62QI\nd5Z6HUU+g/Zbk2DSgVK57Hf3G7pzcodRq+fp6O/kNgtdE9OyZnv9giApd6F1Yy7l\nwgZxjlpKGMA+qKigHSHIQyu1L2JfWjw7eEiirnDtFaCgTpJqPErigX+2eMdpj8/r\njTP3mEN2qStWYydWfYxfcM68gOZsvFiVBfT7qTkFXSeOdigC4bHMDMew9nqP1hlB\n9uo/JESNQ4Z0/WHgDSn9fLbc/UX6SIPVn7vDAj7mZAeyaXYBrXhbHpdqhnOGhDmt\nR9aUopGHleY44RujES1rQWSo6D8SWlbmpXThgHU5rlRFKKSCu9/s99s7kVdLFqpS\nbGg42qs1LwxDiq2TuMV/9TuP10ibB4mSnKwaglcAHcrbo26ZdMF4T9YwKcEmHIrv\n0hCUA0qyvKP3fqfQUVzcssJfWdvjx7/bnwLadrxSOur1IZj+2jJdzPYsT1tSiwL/\nXChSN5a00LJWW5+b0ka155sEg8XcBdUECXIQtRpFedCURjeinGuMnEf6gM8NbcNS\nyVm5Kptf8BO11r154J93nkc3gU4VFcxudg8smaDcq3amPDkyaNBXQm+rcwIApchL\nSOzHWwxtA1q+pLHvxnlk\n=Fcdg\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18399 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/9cfc7eb5/attachment.bin>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T15:47:00",
                "message_text_only": "On Thu, May 7, 2015 at 11:33 AM, Justus Ranvier <justusranvier at riseup.net>\nwrote:\n\n> In summary, I asked a question neither you, nor Peter Todd, want to\n> answer and want to actively discourage people from even asking at all.\n>\n\nIncorrect; your question included built-in assumptions with which I\ndisagree.\n\nBitcoin needs to be the best it can be (Layer 1), but all solutions cannot\nand should not be implemented at Layer 1.\n\nWe need to scale up both bitcoin (L1) and solutions built on top of bitcoin\n(L2).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/1b372d9d/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-07T15:50:38",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/07/2015 05:47 PM, Jeff Garzik wrote:\n> Bitcoin needs to be the best it can be (Layer 1), but all solutions\n> cannot and should not be implemented at Layer 1.\n\nI can provisionally agree with that statement as long as \"all\nsolutions cannot and should not be implemented at Layer 1\" it taken to\nbe a hypothesis to be tested in the context of each proposed solution\nrather than a law of nature.\n\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVS4nOAAoJECpf2nDq2eYjZGEP/jvk+RNQO+Zoyp0jc6aup5Aa\nUSUFk1TYBqbu47vvc3jFHc4V3/BjiwkUKp5bZ4iIxr3xWIA35CcjfpSJEIlEj0zM\nOHS2j+eS0WkNWCmWgj+3sJpQBNnqLmdBOG1q6z0aBLGwG7uabo+YAhJjlP8isfcn\ncBQPGjeTW82ZZLdkNaThbFr53oTYiVPNqMIIq6orUe5vetQS/zfTyowi7Y9+OT+b\nFMXOEmXQTzF415LImJNXOcGFx51YkLe3SuEPEqqIX/+gOcT4HMPuKbqyAu6xXRQK\nO7uI+6AjN1mX7Cvt19wYkUggJ7ddVKrHINSzOfsZ+pdF8mdY4TrdwJJhfN0+fnvo\nKYW+pmEAFRMveV8SVGJpHQ/pWECKbFiz1SRnDfjlbX/C5mHiHM4EmqCxC1pVDxOU\nuDukt+ZIIiP7GwPYxqSknR4lcuwsdFFJf9ldxD+ZRNsmz1l+PkaUUpdkCc9u9rUW\n2IyfvPmeeVUPLP9N675kfiM3aKNO7LHN8GhSUe+1Nt/zcXg6xg0QWKdXjC8nykCa\neH9gn0QoQZaZbfKnb8DLwLjCO5LiOzQTgqdo0ZSJtV/CipqyGcBJtFYW2olG/BvO\nNs6qJKG6Ck76Tv31cu3YGpVbBjCxsIovchLh72KjQ9LscYg8y29evcFlnyagsewY\n5CQJsAY8apmvNvmxAhRf\n=oRV/\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/57fe1c6f/attachment.bin>"
            },
            {
                "author": "Wladimir J. van der Laan",
                "date": "2015-05-07T11:20:43",
                "message_text_only": "On Wed, May 06, 2015 at 10:12:14PM +0000, Matt Corallo wrote:\n\n> Personally, I'm rather strongly against any commitment to a block size\n> increase in the near future. Long-term incentive compatibility requires\n> that there be some fee pressure, and that blocks be relatively\n> consistently full or very nearly full. What we see today are\n> transactions enjoying next-block confirmations with nearly zero pressure\n> to include any fee at all (though many do because it makes wallet code\n> simpler).\n\nI'm weakly against a block size increase in the near future. Some arguments follow. For sake of brevity, this ignores the inherent practical and political issues in scheduling a hardfork.\n\nAgainst:\n\n1. The everyone-verifies-everything approach inherently doesn't scale well. Yes, it is possible to increase the capacity, within limits, without completely destroying the system, but if scaling turns out to be a success, even a 20-fold increase is just a drop in the bucket (this will not make a decentralized Changetip possible). The gain will always be linear, at a total cost that scales in the number of (full node) users times the block size. The whole idea of everyone verifying the whole world's bus tickets and coffee purchases seems ludicrous to me. For true scaling, as well as decentralized microtransactions, the community needs to work on non-centralized 'level 2' protocols for transactions, for example the Lightning network.\n\n  I prefer not to rely on faith that 'Moore's law' - which isn't a physical law but a trend - will save us. And it doesn't so much apply to communication bandwidth as its techniques are more diverse. E.g. for Bitsat, 20MB blocks will push the limit.\n\n2a. Pushing up bandwidth and CPU usage will, inevitably, push people at the lower end away from running their own full nodes. Mind you, the sheer number of full nodes is not the issue, but Bitcoin's security model is based on being able to verify the shared consensus on one's own. A lot of recent development effort went into making the node software less heavy. Yes, one could switch to SPV, but that is a serious privacy compromise. In the worst case people will feel forced to move to webwallets.\n\n  That was about sustained bandwidth - syncing a new node from scratch through the internet will become unwieldy sooner - this can be worked around with UTXO snapshots, but doing this in a way that doesn't completely pull the rug under the security model needs research (I'm aware that this could be construed the other way, that such a solution will be needed eventually and fast block chain growth just accelerates it).\n\n2b. The bandwidth bound for just downloading blocks is ~4GB per month now, it will be ~52GB per month. Behind Tor and other anonimity networks, nodes will reveal themselves by the sheer volume of data transferred even to only download the block chain. This may already be the case, but will become worse. It may even become harmful to Tor itself.\n\n3a. The costs are effectively externalized to users. I, hence, don't like calling the costs \"trivial\". I don't like making this decision for others, I'm not convinced that they are trivial for everyone. Besides, this will increase supply of block chain space, and thus push down transaction fees, but at cost to *all users*, even users that hardly do any transactions. Hence it favors those that generate lots of transactions (is that a perverse incentive? not sure, but it needs to be weighted in any decision).\n\n3b. A mounting fee pressure, resulting in a true fee market where transactions compete to get into blocks, results in urgency to develop decentralized off-chain solutions. I'm afraid increasing the block size will kick this can down the road and let people (and the large Bitcoin companies) relax, until it's again time for a block chain increase, and then they'll rally Gavin again, never resulting in a smart, sustainable solution but eternal awkward discussions like this.\n\n4. We haven't solved the problem of arbitrary data storage, and increasing the block size would compound that problem. More data storage more storage available for the same price - and up to 20x faster growth of the UTXO set, which is permanent (more externalization). More opportunity for pedonazis to insert double-plus ungood data, exposing users to possible legal ramifications.\n\nFor:\n\n1. First, the obvious: It gives some breathing room in a year (or whenever the hard fork is planned). If necessary, it will allow more transactions to be on-chain for a while longer while other solutions are being implemented.\n\n2. *Allowing* 20MB blocks does not mean miners will immediately start making them. Many of them aren't even filling up to the 1MB limit right now, probably due to latency/stale block issues. This makes objection 2 milder, which is about *worst case* bandwidth, as well as 4, as the end result may not be 20MB blocks filled with arbitrary \"junk\".\n\n3. Investment in off-chain solutions is guided by not only fee pressure, but also other reasons such as speed of confirmation, which is unreliable on-chain. This eases objection 3b. \n\nWhew, this grew longer than I expected. To conclude, I understand the advantages of scaling, I do not doubt a block size increase will *work* Although there may be unforseen issues, I'm confident they'll be resolved. However, it may well make Bitcoin less useful for what sets it apart from other systems in the first place: the possibility for people to run their own own \"bank\" without special investment in connectivity and computing hardware. \nAlso the politics aspect (at some point it becomes a question of who decides for who? who is excluded? all those human decisions...) of this I don't like in the least. Possibly unavoidable at some point, but that's something *I*'d like to kick down the road.\n\nWladimir"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-05-07T11:30:49",
                "message_text_only": "> On May 7, 2015, at 4:20 AM, Wladimir J. van der Laan <laanwj at gmail.com> wrote:\n> \n> For sake of brevity, this ignores the inherent practical and political issues in scheduling a hardfork.\n\nIMHO, these issues are the elephant in the room and the talk of block size increases is just a distraction.\n\n- Eric Lombrozo\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/5de81f02/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 842 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/5de81f02/attachment.sig>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-07T15:56:36",
                "message_text_only": "Dear list,\n\nApparently my emails are being marked as spam, despite being sent from\nGMail's web interface.  I've pinged our sysadmin.  Thanks for letting\nme know.\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-07T16:13:47",
                "message_text_only": ">\n> Dear list,\n>\n> Apparently my emails are being marked as spam, despite being sent from\n> GMail's web interface.  I've pinged our sysadmin.\n\n\nIt's a problem with the mailing list software, not your setup. BitPay could\ndisable the phishing protections but that seems like a poor solution. The\nonly real fix is to send from a non @bitpay.com email address. Gmail or\nHotmail will work, I think. Yahoo won't: they enforce the same strict\npolicies than bitpay does.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/35329308/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-06T23:13:22",
                "message_text_only": "For now, lets leave the discussion to JUST the block size increase. If\nit helps - everyone should assume that their pet feature is included in\na hard fork or, if you prefer, that no other features are included in a\nhard fork.\n\nOn 05/06/15 23:11, Matt Whitlock wrote:\n> I'm not so much opposed to a block size increase as I am opposed to a hard fork. My problem with a hard fork is that everyone and their brother wants to seize the opportunity of a hard fork to insert their own pet feature, and such a mad rush of lightly considered, obscure feature additions would be extremely risky for Bitcoin. If it could be guaranteed that raising the block size limit would be the only incompatible change introduced in the hard fork, then I would support it, but I strongly fear that the hard fork itself will become an excuse to change other aspects of the system in ways that will have unintended and possibly disastrous consequences.\n>"
            },
            {
                "author": "John Bodeen",
                "date": "2015-05-07T16:54:35",
                "message_text_only": "If the worry about raising the block size will increase centralization,\ncould not one could imagine an application which rewarded decentralized\nstorage of block data? It could even be build aside or on top of the\nexisting bitcoin protocol.\n\nSee the Permacoin paper by Andrew Miller:\nhttp://cs.umd.edu/~amiller/permacoin.pdf\n\nRegards\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/fa9b24b0/attachment.html>"
            },
            {
                "author": "Raystonn .",
                "date": "2015-05-08T20:38:25",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/78da0fd0/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-08T20:40:50",
                "message_text_only": "Transactions don't expire. But if the wallet is online, it can periodically\nchoose to release an already created transaction with a higher fee. This\nrequires replace-by-fee to be sufficiently deployed, however.\n\nOn Fri, May 8, 2015 at 1:38 PM, Raystonn . <raystonn at hotmail.com> wrote:\n\n> I have a proposal for wallets such as yours.  How about creating all\n> transactions with an expiration time starting with a low fee, then\n> replacing with new transactions that have a higher fee as time passes.\n> Users can pick the fee curve they desire based on the transaction priority\n> they want to advertise to the network.  Users set the priority in the\n> wallet, and the wallet software translates it to a specific fee curve used\n> in the series of expiring transactions.  In this manner, transactions are\n> never left hanging for days, and probably not even for hours.\n>\n> -Raystonn\n>  On 8 May 2015 1:17 pm, Aaron Voisine <voisine at gmail.com> wrote:\n>\n> As the author of a popular SPV wallet, I wanted to weigh in, in support of\n> the Gavin's 20Mb block proposal.\n>\n> The best argument I've heard against raising the limit is that we need fee\n> pressure.  I agree that fee pressure is the right way to economize on\n> scarce resources. Placing hard limits on block size however is an\n> incredibly disruptive way to go about this, and will severely negatively\n> impact users' experience.\n>\n> When users pay too low a fee, they should:\n>\n> 1) See immediate failure as they do now with fees that fail to propagate.\n>\n> 2) If the fee lower than it should be but not terminal, they should see\n> degraded performance, long delays in confirmation, but eventual success.\n> This will encourage them to pay higher fees in future.\n>\n> The worst of all worlds would be to have transactions propagate, hang in\n> limbo for days, and then fail. This is the most important scenario to\n> avoid. Increasing the 1Mb block size limit I think is the simplest way to\n> avoid this least desirable scenario for the immediate future.\n>\n> We can play around with improved transaction selection for blocks and\n> encourage miners to adopt it to discourage low fees and create fee\n> pressure. These could involve hybrid priority/fee selection so low fee\n> transactions see degraded performance instead of failure. This would be the\n> conservative low risk approach.\n>\n> Aaron Voisine\n> co-founder and CEO\n> breadwallet.com\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/a201447f/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-08T20:51:52",
                "message_text_only": "Replace by fee is what I was referencing.\u00a0 End-users interpret the old transaction as expired.\u00a0 Hence the nomenclature.  An alternative is a new feature that operates in the reverse of time lock, expiring a transaction after a specific time.  But time is a bit unreliable in the blockchain\n\n-Raystonn\n\n\nOn 8 May 2015 1:41 pm, Mark Friedenbach <mark at friedenbach.org> wrote:\n>\n> Transactions don't expire. But if the wallet is online, it can periodically choose to release an already created transaction with a higher fee. This requires replace-by-fee to be sufficiently deployed, however.\n>\n> On Fri, May 8, 2015 at 1:38 PM, Raystonn . <raystonn at hotmail.com> wrote:\n>>\n>> I have a proposal for wallets such as yours.\u00a0 How about creating all transactions with an expiration time starting with a low fee, then replacing with new transactions that have a higher fee as time passes.\u00a0 Users can pick the fee curve they desire based on the transaction priority they want to advertise to the network.\u00a0 Users set the priority in the wallet, and the wallet software translates it to a specific fee curve used in the series of expiring transactions.\u00a0 In this manner, transactions are never left hanging for days, and probably not even for hours.\n>>\n>> -Raystonn\n>>\n>> On 8 May 2015 1:17 pm, Aaron Voisine <voisine at gmail.com> wrote:\n>>>\n>>> As the author of a popular SPV wallet, I wanted to weigh in, in support of the Gavin's 20Mb block proposal.\n>>>\n>>> The best argument I've heard against raising the limit is that we need fee pressure.\u00a0 I agree that fee pressure is the right way to economize on scarce resources. Placing hard limits on block size however is an incredibly disruptive way to go about this, and will severely negatively impact users' experience.\n>>>\n>>> When users pay too low a fee, they should:\n>>>\n>>> 1) See immediate failure as they do now with fees that fail to propagate.\n>>>\n>>> 2) If the fee lower than it should be but not terminal, they should see degraded performance, long delays in confirmation, but eventual success. This will encourage them to pay higher fees in future.\n>>>\n>>> The worst of all worlds would be to have transactions propagate, hang in limbo for days, and then fail. This is the most important scenario to avoid. Increasing the 1Mb block size limit I think is the simplest way to avoid this least desirable scenario for the immediate future.\n>>>\n>>> We can play around with improved transaction selection for blocks and encourage miners to adopt it to discourage low fees and create fee pressure. These could involve hybrid priority/fee selection so low fee transactions see degraded performance instead of failure. This would be the conservative low risk approach.\n>>>\n>>> Aaron Voisine\n>>> co-founder and CEO\n>>> breadwallet.com\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-08T20:55:30",
                "message_text_only": "The problems with that are larger than time being unreliable. It is no\nlonger reorg-safe as transactions can expire in the course of a reorg and\nany transaction built on the now expired transaction is invalidated.\n\nOn Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n\n> Replace by fee is what I was referencing.  End-users interpret the old\n> transaction as expired.  Hence the nomenclature.  An alternative is a new\n> feature that operates in the reverse of time lock, expiring a transaction\n> after a specific time.  But time is a bit unreliable in the blockchain\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/c06d4fe8/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-08T21:01:28",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/9f3d64e1/attachment.html>"
            },
            {
                "author": "Andrew",
                "date": "2015-05-09T12:02:48",
                "message_text_only": "The nice thing about 1 MB is that you can store ALL bitcoin transactions\nrelevant to your lifetime (~100 years) on one 5 TB hard drive\n(1*6*24*365*100=5256000). Any regular person can run a full node and store\nthis 5 TB hard drive easily at their home. With 10 MB blocks you need a 50\nTB drive just for your bitcoin transactions! This is not doable for most\nregular people due to space and monetary constraints. Being able to review\nall transactions relevant to your lifetime is one of the key important\nproperties of Bitcoin. How else can people audit the financial transactions\nof companies and governments that are using the Bitcoin blockchain? How\nelse can we achieve this level of transparency that is essential to keeping\ncorrupt governments/companies in check? How else can we keep track of our\nown personal transactions without relying on others to keep track of them\nfor us? As time passes, storage technology may increase, but so may human\nlife expectancy. So yes, in this sense, 1 MB just may be the magic number.\n\nAssuming that we have a perfectly functional off-chain transaction system,\nwhat do we actually gain by going from 1 MB to 1000 MB (my approximate\nlimit for regular users having enough processing power)? If there is no\nclear and substantial gain, then it is foolish to venture into this\nterritory, i.e. KEEP IT AT 1 MB! For example Angel said he wants to see\ncomputers transacting with computers at super speeds. Why do you need to do\nthis on the main chain? You will lose all the transparency of the current\nsystem, an essential feature.\n\n\nOn Fri, May 8, 2015 at 10:36 PM, Angel Leon <gubatron at gmail.com> wrote:\n\n> I believe 100MB is still very conservative, I think that's barely 666 tps.\n>\n> I also find it not very creative that people are imagining these limits\n> for 10 billion people using bitcoin, I think bitcoin's potential is\n> realized with computers transacting with computers, which can eat those 666\n> tps in a single scoup (what if bittorrent developers got creative with\n> seeding, or someone created a decentralized paid itunes on top of bitcoin,\n> or the openbazaar developers actually pulled a decentralized amazon with no\n> off-chain transaction since they want the thing to be fully decentralized,\n> bitcoin would collapse right away)\n>\n> I truly hope people see past regular people running nodes at home, that's\n> never going to happen. This should be about the miner's networking, storage\n> and cpu capacity. They will have gigabit access, they will have shitload of\n> storage, and they already have plenty of processing power, all of which are\n> only going to get cheaper.\n>\n> In order to have the success we all dream we'll need gigabit blocks. Let's\n> hope adoption remains slow.\n>\n> http://twitter.com/gubatron\n>\n> On Fri, May 8, 2015 at 1:51 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n>\n>> Actually I believe that side chains and off-main-chain transactions will\n>> be a critical part for the overall scalability of the network.  I was\n>> actually trying to make the point that (insert some huge block size here)\n>> will be needed to even accommodate the reduced traffic.\n>>\n>> I believe that it is definitely over 20MB. If it was determined to be 100\n>> MB ten years from now, that wouldn't surprise me.\n>>\n>> Sent from my overpriced smartphone\n>> On May 8, 2015 1:17 PM, \"Andrew\" <onelineproof at gmail.com> wrote:\n>>\n>>>\n>>>\n>>> On Fri, May 8, 2015 at 2:59 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n>>>\n>>>>\n>>>> This isn't about \"everyone's coffee\".  This is about an absolute\n>>>> minimum amount of participation by people who wish to use the network.   If\n>>>> our goal is really for bitcoin to really be a global, open transaction\n>>>> network that makes money fluid, then 7tps is already a failure.  If even 5%\n>>>> of the world (350M people) was using the network for 1 tx per month\n>>>> (perhaps to open payment channels, or shift money between side chains),\n>>>> we'll be above 100 tps.  And that doesn't include all the non-individuals\n>>>> (organizations) that want to use it.\n>>>>\n>>>\n>>>> The goals of \"a global transaction network\" and \"everyone must be able\n>>>> to run a full node with their $200 dell laptop\" are not compatible.  We\n>>>> need to accept that a global transaction system cannot be fully/constantly\n>>>> audited by everyone and their mother.  The important feature of the network\n>>>> is that it is open and anyone *can* get the history and verify it.  But not\n>>>> everyone is required to.   Trying to promote a system wher000e the history\n>>>> can be forever handled by a low-end PC is already falling out of reach,\n>>>> even with our miniscule 7 tps.  Clinging to that goal needlessly limits the\n>>>> capability for the network to scale to be a useful global payments system\n>>>>\n>>>\n>>> These are good points and got me thinking (but I think you're wrong). If\n>>> we really want each of the 10 billion people soon using bitcoin once per\n>>> month, that will require 500MB blocks. That's about 2 TB per month. And if\n>>> you relay it to 4 peers, it's 10 TB per month. Which I suppose is doable\n>>> for a home desktop, so you can just run a pruned full node with all\n>>> transactions from the past month. But how do you sync all those\n>>> transactions if you've never done this before or it's been a while since\n>>> you did? I think it currently takes at least 3 hours to fully sync 30 GB of\n>>> transactions. So 2 TB will take 8 days, then you take a bit more time to\n>>> sync the days that passed while you were syncing. So that's doable, but at\n>>> a certain point, like 10 TB per month (still only 5 transactions per month\n>>> per person), you will need 41 days to sync that month, so you will never\n>>> catch up. So I think in order to keep the very important property of anyone\n>>> being able to start clean and verify the thing, then we need to think of\n>>> bitcoin as a system that does transactions for a large number of users at\n>>> once in one transaction, and not a system where each person will make a\n>>> ~monthly transaction on. We need to therefore rely on sidechains,\n>>> treechains, lightning channels, etc...\n>>>\n>>> I'm not a bitcoin wizard and this is just my second post on this mailing\n>>> list, so I may be missing something. So please someone, correct me if I'm\n>>> wrong.\n>>>\n>>>>\n>>>>\n>>>>\n>>>> On 05/07/2015 03:54 PM, Jeff Garzik wrote:\n>>>>\n>>>>  On Thu, May 7, 2015 at 3:31 PM, Alan Reiner <etotheipi at gmail.com>\n>>>> wrote:\n>>>>\n>>>>\n>>>>>  (2) Leveraging fee pressure at 1MB to solve the problem is actually\n>>>>> really a bad idea.  It's really bad while Bitcoin is still growing, and\n>>>>> relying on fee pressure at 1 MB severely impacts attractiveness and\n>>>>> adoption potential of Bitcoin (due to high fees and unreliability).  But\n>>>>> more importantly, it ignores the fact that for a 7 tps is pathetic for a\n>>>>> global transaction system.  It is a couple orders of magnitude too low for\n>>>>> any meaningful commercial activity to occur.  If we continue with a cap of\n>>>>> 7 tps forever, Bitcoin *will* fail.  Or at best, it will fail to be\n>>>>> useful for the vast majority of the world (which probably leads to\n>>>>> failure).  We shouldn't be talking about fee pressure until we hit 700 tps,\n>>>>> which is probably still too low.\n>>>>>\n>>>>  [...]\n>>>>\n>>>>  1) Agree that 7 tps is too low\n>>>>\n>>>>  2) Where do you want to go?  Should bitcoin scale up to handle all\n>>>> the world's coffees?\n>>>>\n>>>>  This is hugely unrealistic.  700 tps is 100MB blocks, 14.4 GB/day --\n>>>> just for a single feed.  If you include relaying to multiple nodes, plus\n>>>> serving 500 million SPV clients en grosse, who has the capacity to run such\n>>>> a node?  By the time we get to fee pressure, in your scenario, our network\n>>>> node count is tiny and highly centralized.\n>>>>\n>>>>  3) In RE \"fee pressure\" -- Do you see the moral hazard to a\n>>>> software-run system?  It is an intentional, human decision to flood the\n>>>> market with supply, thereby altering the economics, forcing fees to remain\n>>>> low in the hopes of achieving adoption.  I'm pro-bitcoin and obviously want\n>>>> to see bitcoin adoption - but I don't want to sacrifice every decentralized\n>>>> principle and become a central banker in order to get there.\n>>>>\n>>>>\n>>>>\n>>>>\n>>>> ------------------------------------------------------------------------------\n>>>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>>>> Widest out-of-the-box monitoring support with 50+ applications\n>>>> Performance metrics, stats and reports that give you Actionable Insights\n>>>> Deep dive visibility with transaction tracing using APM Insight.\n>>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>>> _______________________________________________\n>>>> Bitcoin-development mailing list\n>>>> Bitcoin-development at lists.sourceforge.net\n>>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>>>\n>>>>\n>>>\n>>>\n>>> --\n>>> PGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n>>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/9856a5a5/attachment.html>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-09T12:53:48",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nOn 05/09/2015 02:02 PM, Andrew wrote:\n> The nice thing about 1 MB is that you can store ALL bitcoin\n> transactions relevant to your lifetime (~100 years) on one 5 TB\n> hard drive (1*6*24*365*100=5256000). Any regular person can run a\n> full node and store this 5 TB hard drive easily at their home. With\n> 10 MB blocks you need a 50 TB drive just for your bitcoin\n> transactions! This is not doable for most regular people due to\n> space and monetary constraints. Being able to review all\n> transactions relevant to your lifetime is one of the key important \n> properties of Bitcoin. How else can people audit the financial\n> transactions of companies and governments that are using the\n> Bitcoin blockchain? How else can we achieve this level of\n> transparency that is essential to keeping corrupt\n> governments/companies in check? How else can we keep track of our \n> own personal transactions without relying on others to keep track\n> of them for us? As time passes, storage technology may increase,\n> but so may human life expectancy. So yes, in this sense, 1 MB just\n> may be the magic number.\n\nHow many individuals and companies do you propose will ever use\nBitcoin (order of magnitude estimates are fine)\n\nWhatever number you select above, please describe approximately how\nmany lifetime Bitcoin transactions each individual and company will be\ncapable of performing with a 1 MB block size limit.\n\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVTgNcAAoJECpf2nDq2eYjM8AP/2kwSF+HMPR1KdaZsATL4rog\nxSS97Q5iEX8StA61jUqHQmpXL5pG6z5DeeKT/liwcMnYnVqOEOLvoVctr3gXfgRz\n9GJeTOlmN5l9xBeX/nWa0A2ql0kWZpYolBS1FwYadWReAD8R0X9UeBd9YXLZNy33\nOw9JjwRjKHhsuyrlMP8pRDKlGPoa/U+2aW4FwiysMLa0Gu6dbFjTrp3bHw4Fccpi\nX0E/aDN68U4FV+lZ4NzkMsBK9VARzmC8KI0DQ540pqfkcnyoYf0VERl/gslPWhfq\nt6Rqa7vHHMqFe82lgCd3ji8Qhsz8oBrDS4u4jqwATvgihgImOB6K85JoKmf3y2JS\njByjMGd4Ep0F80Z2MRhi6HuEoRU69uY2u6l9bZxMjzvLX8sG6QTNk3uLMS3ARXcY\nJBjZ/g13DXgcRj01fq05CHbCTJYZgTA9pRZTY+ZKH4r0mu86b9ua7hjvyKHS6q54\nuaFmRkNcnKlpCY+fvH/JUdvvmwrA0ETUdHhRyk8vzWIMi+aH4//GwrCmBNRrugzv\n9JtQ1BC+tQqtSX2VkFEhAVISitgkBqurVVlGk18FvVKPFO8cnFS/6NWoPE0WLLzW\n2pTuhEPjdz9UAHD3RW601rb4C0LbuwVlGO4tYBjyqCmk/vBlES2XIjQKctXZLBEy\neLgn3gMwEXUTU6UdGyvb\n=RPhK\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/e46db937/attachment.bin>"
            },
            {
                "author": "Andrew",
                "date": "2015-05-09T18:33:32",
                "message_text_only": "On Sat, May 9, 2015 at 12:53 PM, Justus Ranvier <justusranvier at riseup.net>\nwrote:\n\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA1\n>\n> On 05/09/2015 02:02 PM, Andrew wrote:\n> > The nice thing about 1 MB is that you can store ALL bitcoin\n> > transactions relevant to your lifetime (~100 years) on one 5 TB\n> > hard drive (1*6*24*365*100=5256000). Any regular person can run a\n> > full node and store this 5 TB hard drive easily at their home. With\n> > 10 MB blocks you need a 50 TB drive just for your bitcoin\n> > transactions! This is not doable for most regular people due to\n> > space and monetary constraints. Being able to review all\n> > transactions relevant to your lifetime is one of the key important\n> > properties of Bitcoin. How else can people audit the financial\n> > transactions of companies and governments that are using the\n> > Bitcoin blockchain? How else can we achieve this level of\n> > transparency that is essential to keeping corrupt\n> > governments/companies in check? How else can we keep track of our\n> > own personal transactions without relying on others to keep track\n> > of them for us? As time passes, storage technology may increase,\n> > but so may human life expectancy. So yes, in this sense, 1 MB just\n> > may be the magic number.\n>\n> How many individuals and companies do you propose will ever use\n> Bitcoin (order of magnitude estimates are fine)\n>\n> Whatever number you select above, please describe approximately how\n> many lifetime Bitcoin transactions each individual and company will be\n> capable of performing with a 1 MB block size limit.\n>\n\nI would expect at least 10 billion people (directly or indirectly) to be\nusing it at once for at least 100 years. But I think it's pointless to\nguess how many will use it, but rather make the system ready for 10 billion\npeople. The point is that for small transactions, they will be done\noff-chain. The actual Bitcoin blockchain will only show very large\ntransactions (such as a military purchasing a new space shuttle) or\naggregate transactions (i.e. a transaction consisting of multiple smaller\ntransactions done off-chain). There can also be multiple layers of chains\ncreating a tree-like structure. Each chain above will validate the\naggregate transactions of the chain below. You can think of the Bitcoin\nblockchain as the \"hypervisor\" that manages all the other chains. While\nyour coffee purchase 4 days ago may not be directly visible within the\nBitcoin blockchain (the main chain), you can trace it down the sequence of\nchains until you find it. Same with that fancy dinner your government MP\npaid for using public funds. You don't have to store a copy of all\ntransactions that occurred for each chain in existence, but rather just the\ntransactions for the chains that you use or are relevant to you.\n\nAs you see, this kind of system is totally transparent to all users and\ntotally flexible (you can choose your sub chains). The flexibility also\nallows you to have arbitrarily fast transactions (choose a chain or\nlightning channel attached to that chain that supports it), and you can\nenjoy a wide variety of features from other chains, like using one chain\nthat is known to have good anonymity properties.\n\n\n> -----BEGIN PGP SIGNATURE-----\n>\n> iQIcBAEBAgAGBQJVTgNcAAoJECpf2nDq2eYjM8AP/2kwSF+HMPR1KdaZsATL4rog\n> xSS97Q5iEX8StA61jUqHQmpXL5pG6z5DeeKT/liwcMnYnVqOEOLvoVctr3gXfgRz\n> 9GJeTOlmN5l9xBeX/nWa0A2ql0kWZpYolBS1FwYadWReAD8R0X9UeBd9YXLZNy33\n> Ow9JjwRjKHhsuyrlMP8pRDKlGPoa/U+2aW4FwiysMLa0Gu6dbFjTrp3bHw4Fccpi\n> X0E/aDN68U4FV+lZ4NzkMsBK9VARzmC8KI0DQ540pqfkcnyoYf0VERl/gslPWhfq\n> t6Rqa7vHHMqFe82lgCd3ji8Qhsz8oBrDS4u4jqwATvgihgImOB6K85JoKmf3y2JS\n> jByjMGd4Ep0F80Z2MRhi6HuEoRU69uY2u6l9bZxMjzvLX8sG6QTNk3uLMS3ARXcY\n> JBjZ/g13DXgcRj01fq05CHbCTJYZgTA9pRZTY+ZKH4r0mu86b9ua7hjvyKHS6q54\n> uaFmRkNcnKlpCY+fvH/JUdvvmwrA0ETUdHhRyk8vzWIMi+aH4//GwrCmBNRrugzv\n> 9JtQ1BC+tQqtSX2VkFEhAVISitgkBqurVVlGk18FvVKPFO8cnFS/6NWoPE0WLLzW\n> 2pTuhEPjdz9UAHD3RW601rb4C0LbuwVlGO4tYBjyqCmk/vBlES2XIjQKctXZLBEy\n> eLgn3gMwEXUTU6UdGyvb\n> =RPhK\n> -----END PGP SIGNATURE-----\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/cd84dd3d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Block Size Increase",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Andrew",
                "Dave Hudson",
                "Tier Nolan",
                "21E14",
                "Wladimir J. van der Laan",
                "Justus Ranvier",
                "Tom Harding",
                "J\u00e9r\u00e9mie Dubois-Lacoste",
                "Matthew Mitchell",
                "Raystonn",
                "Mike Hearn",
                "Peter Todd",
                "Jorge Tim\u00f3n",
                "Gregory Maxwell",
                "Btc Drak",
                "Matt Corallo",
                "Ross Nicoll",
                "Alan Reiner",
                "Mark Friedenbach",
                "Aaron Voisine",
                "Jeff Garzik",
                "Bryan Bishop",
                "slush",
                "Angel Leon",
                "Matt Whitlock",
                "Bernard Rihn",
                "Alex Mizrahi",
                "Gavin Andresen",
                "Oliver Egginger",
                "Eric Lombrozo",
                "John Bodeen",
                "Thomas Zander",
                "Gavin Costin",
                "Chris Wardell",
                "Alex Morcos",
                "Raystonn .",
                "Pieter Wuille",
                "Joel Joonatan Kaartinen"
            ],
            "messages_count": 115,
            "total_messages_chars_count": 292146
        }
    },
    {
        "title": "[Bitcoin-development] Fwd:  Block Size Increase",
        "thread_messages": [
            {
                "author": "Gavin Andresen",
                "date": "2015-05-07T17:40:59",
                "message_text_only": "On Thu, May 7, 2015 at 1:26 PM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n> I think this is a huge issue. You've been wandering around telling\n> people that the blocksize will increase soon for months\n\n\nI think the strongest thing I've ever said is:\n\n\"There is consensus that the max block size much change sooner or later.\nThere is not yet consensus on exactly how or when. I will be pushing to\nchange it this year.\"\n\nThis is what \"I will be pushing to change it this year\" looks like.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/e384dd2e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd:  Block Size Increase",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Gavin Andresen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 701
        }
    },
    {
        "title": "[Bitcoin-development] Mechanics of a hard fork",
        "thread_messages": [
            {
                "author": "Roy Badami",
                "date": "2015-05-07T20:00:23",
                "message_text_only": "I'd love to have more discussion of exactly how a hard fork should be\nimplemented.  I think it might actually be of some value to have rough\nconsensus on that before we get too bogged down with exactly what the\nproposed hard fork should do.  After all, how can we debate whether a\nparticular hard fork proposal has consensus if we haven't even decided\nwhat level of supermajority is needed to establish consensus?\n\nFor instance, back in 2012 Gavin was proposing, effectively, that a\nhard fork should require a supermajority of 99% of miners in order to\nsucceed:\n\nhttps://gist.github.com/gavinandresen/2355445\n\nMore recently, Gavin has proposed that a supermoajority of only 80% of\nminers should be needed in order to trigger the hard fork.\n\nhttp://www.gavintech.blogspot.co.uk/2015/01/twenty-megabytes-testing-results.html\n\nJust now, on this list (see attached message) Gavin seems to be\naluding to some mechanism for a hard fork which involves consensus of\nfull nodes, and then a soft fork preceeding the hard fork, which I'd\nlove to see a full explanation of.\n\nFWIW, I think 80% is far too low to establish consensus for a hard\nfork.  I think the supermajority of miners should be sufficiently\nlarge that the rump doesn't constitute a viable coin.  If you don't\nhave that very strong level of consensus then you risk forking Bitcoin\ninto two competing coins (and I believe we already have one exchange\npromissing to trade both forks as long as the blockchains are alive).\n\nAs a starting point, I think 35/36th of miners (approximately 97.2%)\nis the minimum I would be comfortable with.  It means that the rump\ncoin will initially have an average confirmation time of 6 hours\n(until difficulty, very slowly, adjusts) which is probably far enough\nfrom viable that the majority of holdouts will quickly desert it too.\n\nThoughs?\n\nroy\n-------------- next part --------------\nAn embedded message was scrubbed...\nFrom: Gavin Andresen <gavinandresen at gmail.com>\nSubject: Re: [Bitcoin-development] Block Size Increase\nDate: Thu, 7 May 2015 10:52:54 -0400\nSize: 9909\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/ed0c3179/attachment.eml>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-07T21:24:45",
                "message_text_only": "In terms of miners, a strong supermajority is arguably sufficient, even 75%\nwould be enough.\n\nThe near total consensus required is merchants and users.  If (almost) all\nmerchants and users updated and only 75% of the miners updated, then that\nwould give a successful hard-fork.\n\nOn the other hand, if 99.99% of the miners updated and only 75% of\nmerchants and 75% of users updated, then that would be a serioud split of\nthe network.\n\nThe advantage of strong miner support is that it effectively kills the fork\nthat follows the old rules.  The 25% of merchants and users sees a\nblockchain stall.\n\nMiners are likely to switch to the fork that is worth the most.  A mining\npool could even give 2 different sub-domains.  A hasher can pick which\nrule-set to follow.  Most likely, they would converge on the fork which\npaid the most, but the old ruleset would likely still have some hashing\npower and would eventually re-target.\n\nOn Thu, May 7, 2015 at 9:00 PM, Roy Badami <roy at gnomon.org.uk> wrote:\n\n> I'd love to have more discussion of exactly how a hard fork should be\n> implemented.  I think it might actually be of some value to have rough\n> consensus on that before we get too bogged down with exactly what the\n> proposed hard fork should do.  After all, how can we debate whether a\n> particular hard fork proposal has consensus if we haven't even decided\n> what level of supermajority is needed to establish consensus?\n>\n> For instance, back in 2012 Gavin was proposing, effectively, that a\n> hard fork should require a supermajority of 99% of miners in order to\n> succeed:\n>\n> https://gist.github.com/gavinandresen/2355445\n>\n> More recently, Gavin has proposed that a supermoajority of only 80% of\n> miners should be needed in order to trigger the hard fork.\n>\n>\n> http://www.gavintech.blogspot.co.uk/2015/01/twenty-megabytes-testing-results.html\n>\n> Just now, on this list (see attached message) Gavin seems to be\n> aluding to some mechanism for a hard fork which involves consensus of\n> full nodes, and then a soft fork preceeding the hard fork, which I'd\n> love to see a full explanation of.\n>\n> FWIW, I think 80% is far too low to establish consensus for a hard\n> fork.  I think the supermajority of miners should be sufficiently\n> large that the rump doesn't constitute a viable coin.  If you don't\n> have that very strong level of consensus then you risk forking Bitcoin\n> into two competing coins (and I believe we already have one exchange\n> promissing to trade both forks as long as the blockchains are alive).\n>\n> As a starting point, I think 35/36th of miners (approximately 97.2%)\n> is the minimum I would be comfortable with.  It means that the rump\n> coin will initially have an average confirmation time of 6 hours\n> (until difficulty, very slowly, adjusts) which is probably far enough\n> from viable that the majority of holdouts will quickly desert it too.\n>\n> Thoughs?\n>\n> roy\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/a5867f38/attachment.html>"
            },
            {
                "author": "Roy Badami",
                "date": "2015-05-07T21:42:01",
                "message_text_only": "> On the other hand, if 99.99% of the miners updated and only 75% of\n> merchants and 75% of users updated, then that would be a serioud split of\n> the network.\n\nBut is that a plausible scenario?  Certainly *if* the concensus rules\nrequired a 99% supermajority of miners for the hard fork to go ahead,\nthen there would be absoltely no rational reason for merchants and\nusers to refuse to upgrade, even if they don't support the changes\nintroduces by the hard fork.  Their only choice, if the fork succeeds,\nis between the active chain and the one that is effectively stalled -\nand, of course, they can make that choice ahead of time.\n\nroy"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-07T21:49:28",
                "message_text_only": "I would not modify my node if the change introduced a perpetual 100 BTC\nsubsidy per block, even if 99% of miners went along with it.\n\nA hardfork is safe when 100% of (economically relevant) users upgrade. If\nminers don't upgrade at that point, they just lose money.\n\nThis is why a hashrate-triggered hardfork does not make sense. Either you\nbelieve everyone will upgrade anyway, and the hashrate doesn't matter. Or\nyou are not certain, and the fork is risky, independent of what hashrate\nupgrades.\n\nAnd the march 2013 fork showed that miners upgrade at a different schedule\nthan the rest of the network.\nOn May 7, 2015 5:44 PM, \"Roy Badami\" <roy at gnomon.org.uk> wrote:\n\n>\n> > On the other hand, if 99.99% of the miners updated and only 75% of\n> > merchants and 75% of users updated, then that would be a serioud split of\n> > the network.\n>\n> But is that a plausible scenario?  Certainly *if* the concensus rules\n> required a 99% supermajority of miners for the hard fork to go ahead,\n> then there would be absoltely no rational reason for merchants and\n> users to refuse to upgrade, even if they don't support the changes\n> introduces by the hard fork.  Their only choice, if the fork succeeds,\n> is between the active chain and the one that is effectively stalled -\n> and, of course, they can make that choice ahead of time.\n>\n> roy\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/8a55d889/attachment.html>"
            },
            {
                "author": "Roy Badami",
                "date": "2015-05-07T22:08:48",
                "message_text_only": "On Thu, May 07, 2015 at 11:49:28PM +0200, Pieter Wuille wrote:\n> I would not modify my node if the change introduced a perpetual 100 BTC\n> subsidy per block, even if 99% of miners went along with it.\n\nSurely, in that scenario Bitcoin is dead.  If the fork you prefer has\nonly 1% of the hash power it is trivially vulnerably not just to a 51%\nattack but to a 501% attack, not to mention the fact that you'd only\nbe getting one block every 16 hours.\n\n> \n> A hardfork is safe when 100% of (economically relevant) users upgrade. If\n> miners don't upgrade at that point, they just lose money.\n> \n> This is why a hashrate-triggered hardfork does not make sense. Either you\n> believe everyone will upgrade anyway, and the hashrate doesn't matter. Or\n> you are not certain, and the fork is risky, independent of what hashrate\n> upgrades.\n\nBeliefs are all very well, but they can be wrong.  Of course we should\nnot go ahead with a fork that we believe to be dangerous, but\nrequiring a supermajority of miners is surely a wise precaution.  I\nfail to see any realistic scenario where 99% of miners vote for the\nhard fork to go ahead, and the econonomic majority votes to stay on\nthe blockchain whose hashrate has just dropped two orders of magnitude\n- so low that the mean time between blocks is now over 16 hours.\n\n> \n> And the march 2013 fork showed that miners upgrade at a different schedule\n> than the rest of the network.\n> On May 7, 2015 5:44 PM, \"Roy Badami\" <roy at gnomon.org.uk> wrote:\n> \n> >\n> > > On the other hand, if 99.99% of the miners updated and only 75% of\n> > > merchants and 75% of users updated, then that would be a serioud split of\n> > > the network.\n> >\n> > But is that a plausible scenario?  Certainly *if* the concensus rules\n> > required a 99% supermajority of miners for the hard fork to go ahead,\n> > then there would be absoltely no rational reason for merchants and\n> > users to refuse to upgrade, even if they don't support the changes\n> > introduces by the hard fork.  Their only choice, if the fork succeeds,\n> > is between the active chain and the one that is effectively stalled -\n> > and, of course, they can make that choice ahead of time.\n> >\n> > roy\n> >\n> >\n> > ------------------------------------------------------------------------------\n> > One dashboard for servers and applications across Physical-Virtual-Cloud\n> > Widest out-of-the-box monitoring support with 50+ applications\n> > Performance metrics, stats and reports that give you Actionable Insights\n> > Deep dive visibility with transaction tracing using APM Insight.\n> > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >"
            },
            {
                "author": "Adam Back",
                "date": "2015-05-08T02:16:12",
                "message_text_only": "Well this is all very extreme circumstances, and you'd have to assume no\nrational player with an interest in bitcoin would go there, but to play\nyour analysis forward: users are also not powerless at the extreme: they\ncould change the hash function rendering current deployed ASICs useless in\nreaction for example, and reset difficulty at the same time, or freeze\ntransactions until some minimum hashrate is reached.  People would figure\nout what is the least bad way forward.\n\nAdam\nOn May 7, 2015 3:09 PM, \"Roy Badami\" <roy at gnomon.org.uk> wrote:\n\n> On Thu, May 07, 2015 at 11:49:28PM +0200, Pieter Wuille wrote:\n> > I would not modify my node if the change introduced a perpetual 100 BTC\n> > subsidy per block, even if 99% of miners went along with it.\n>\n> Surely, in that scenario Bitcoin is dead.  If the fork you prefer has\n> only 1% of the hash power it is trivially vulnerably not just to a 51%\n> attack but to a 501% attack, not to mention the fact that you'd only\n> be getting one block every 16 hours.\n>\n> >\n> > A hardfork is safe when 100% of (economically relevant) users upgrade. If\n> > miners don't upgrade at that point, they just lose money.\n> >\n> > This is why a hashrate-triggered hardfork does not make sense. Either you\n> > believe everyone will upgrade anyway, and the hashrate doesn't matter. Or\n> > you are not certain, and the fork is risky, independent of what hashrate\n> > upgrades.\n>\n> Beliefs are all very well, but they can be wrong.  Of course we should\n> not go ahead with a fork that we believe to be dangerous, but\n> requiring a supermajority of miners is surely a wise precaution.  I\n> fail to see any realistic scenario where 99% of miners vote for the\n> hard fork to go ahead, and the econonomic majority votes to stay on\n> the blockchain whose hashrate has just dropped two orders of magnitude\n> - so low that the mean time between blocks is now over 16 hours.\n>\n> >\n> > And the march 2013 fork showed that miners upgrade at a different\n> schedule\n> > than the rest of the network.\n> > On May 7, 2015 5:44 PM, \"Roy Badami\" <roy at gnomon.org.uk> wrote:\n> >\n> > >\n> > > > On the other hand, if 99.99% of the miners updated and only 75% of\n> > > > merchants and 75% of users updated, then that would be a serioud\n> split of\n> > > > the network.\n> > >\n> > > But is that a plausible scenario?  Certainly *if* the concensus rules\n> > > required a 99% supermajority of miners for the hard fork to go ahead,\n> > > then there would be absoltely no rational reason for merchants and\n> > > users to refuse to upgrade, even if they don't support the changes\n> > > introduces by the hard fork.  Their only choice, if the fork succeeds,\n> > > is between the active chain and the one that is effectively stalled -\n> > > and, of course, they can make that choice ahead of time.\n> > >\n> > > roy\n> > >\n> > >\n> > >\n> ------------------------------------------------------------------------------\n> > > One dashboard for servers and applications across\n> Physical-Virtual-Cloud\n> > > Widest out-of-the-box monitoring support with 50+ applications\n> > > Performance metrics, stats and reports that give you Actionable\n> Insights\n> > > Deep dive visibility with transaction tracing using APM Insight.\n> > > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> > > _______________________________________________\n> > > Bitcoin-development mailing list\n> > > Bitcoin-development at lists.sourceforge.net\n> > > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> > >\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/df972ca5/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-08T02:35:00",
                "message_text_only": "On May 7, 2015 3:08 PM, \"Roy Badami\" <roy at gnomon.org.uk> wrote:\n>\n> On Thu, May 07, 2015 at 11:49:28PM +0200, Pieter Wuille wrote:\n> > I would not modify my node if the change introduced a perpetual 100 BTC\n> > subsidy per block, even if 99% of miners went along with it.\n>\n> Surely, in that scenario Bitcoin is dead.  If the fork you prefer has\n> only 1% of the hash power it is trivially vulnerably not just to a 51%\n> attack but to a 501% attack, not to mention the fact that you'd only\n> be getting one block every 16 hours.\n\nYes, indeed, Bitcoin would be dead if this actually happens. But that is\nstill where the power lies: before anyone (miners or others) would think\nabout trying such a change, they would need to convince people and be sure\nthey will effectively modify their code.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/3ca17652/attachment.html>"
            },
            {
                "author": "Cameron Garnham",
                "date": "2015-05-08T03:12:22",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA256\n\nWhile being in the Bitcoin community for a long time, I haven't been\nso directly involved in the development.  However I wish to suggest a\ndifferent pre-hard-fork soft-fork approach:\n\n\nSet a 'block size cap' in the similar same way as we set difficulty.\n\nEvery 2016 blocks take the average size of the blocks and multiply the\nsize by 1.5x, rejecting blocks that are larger than this size, for the\nnext 2016 period.\n\nI would of-course suggest that we keep the limits at min 100kb and max\n(initially) 990kb (not 1mb on purpose, as this should become the new\nlimit), rounding up to the nearest 10kb.\n\nA: we don't have pressure at the 1mb limit, (we reduce the limit in a\nflexible manner to 990kb).\n\nB: we can upgrade the network to XYZ hard-limit, then slowly raze the\nsoft-limit after being sure the network, as-a-whole is ready.\n\nIf we on-day remove the block-size limit, this rule will stop a rouge\nminer from making 10mb, or 100mb blocks, or 1gb blocks.\n\nThis could be implemented by the miners without breaking any of the\nclients, and would tend to produce a better dynamic fee pressure.\n\n\nThis will give the mechanics to the miners to create consensus to\nagree what block-sizes they believe are best for the network, and\nallows the block-sizes to dynamically grow in response to larger demand.\n\n\n\nOn 5/8/2015 10:35 AM, Pieter Wuille wrote:\n> On May 7, 2015 3:08 PM, \"Roy Badami\" <roy at gnomon.org.uk> wrote:\n>> \n>> On Thu, May 07, 2015 at 11:49:28PM +0200, Pieter Wuille wrote:\n>>> I would not modify my node if the change introduced a perpetual\n>>> 100 BTC subsidy per block, even if 99% of miners went along\n>>> with it.\n>> \n>> Surely, in that scenario Bitcoin is dead.  If the fork you prefer\n>> has only 1% of the hash power it is trivially vulnerably not just\n>> to a 51% attack but to a 501% attack, not to mention the fact\n>> that you'd only be getting one block every 16 hours.\n> \n> Yes, indeed, Bitcoin would be dead if this actually happens. But\n> that is still where the power lies: before anyone (miners or\n> others) would think about trying such a change, they would need to\n> convince people and be sure they will effectively modify their\n> code.\n> \n> \n> \n> ----------------------------------------------------------------------\n- --------\n>\n> \nOne dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications \n> Performance metrics, stats and reports that give you Actionable\n> Insights Deep dive visibility with transaction tracing using APM\n> Insight. http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> \n> \n> \n> _______________________________________________ Bitcoin-development\n> mailing list Bitcoin-development at lists.sourceforge.net \n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> \n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2\n\niF4EAREIAAYFAlVMKZYACgkQBJ8cMDO159aTiQEApTITEBrhE1DRbj/w+GncNeqB\n0hGvmIBa1z0hGww0kaMBAOhxjn/K5leRJgdt1fKhNEDKKHdeCOIX3QRgry90D3NO\n=p0+H\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Mechanics of a hard fork",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Adam Back",
                "Roy Badami",
                "Tier Nolan",
                "Pieter Wuille",
                "Cameron Garnham"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 19861
        }
    },
    {
        "title": "[Bitcoin-development] Block Size Increase Requirements",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2015-05-07T22:02:09",
                "message_text_only": "OK, so lets do that. I've seen a lot of \"I'm not entirely comfortable\nwith committing to this right now, but think we should eventually\", but\nnot much \"I'd be comfortable with committing to this when I see X\". In\nthe interest of ignoring debate and pushing people towards a consensus\nat all costs, ( ;) ) I'm gonna go ahead and suggest we talk about the\nsecond.\n\nPersonally, there are several things that worry me significantly about\ncommitting to a blocksize increase, which I'd like to see resolved\nbefore I'd consider supporting a blocksize increase commitment.\n\n * Though there are many proposals floating around which could\nsignificantly decrease block propagation latency, none of them are\nimplemented today. I'd expect to see these not only implemented but\nbeing used in production (though I dont particularly care about them\nbeing all that stable). I'd want to see measurements of how they perform\nboth in production and in the face of high packet loss (eg across the\nGFW or in the case of small/moderate DoS). In addition, I'd expect to\nsee analysis of how these systems perform in the worst-case, not just\npacket-loss-wise, but in the face of miners attempting to break the system.\n\n * I'd very much like to see someone working on better scaling\ntechnology, both in terms of development and in terms of getting\ntraction in the marketplace. I know StrawPay is working on development,\nthough its not obvious to me how far they are from their website, but I\ndont know of any commitments by large players (either SPV wallets,\ncentralized wallet services, payment processors, or any others) to\nsupport such a system (to be fair, its probably too early for such\nplayers to commit to anything, since anything doesnt exist in public).\n\n * I'd like to see some better conclusions to the discussion around\nlong-term incentives within the system. If we're just building Bitcoin\nto work in five years, great, but if we want it all to keep working as\nsubsidy drops significantly, I'd like a better answer than \"we'll deal\nwith it when we get there\" or \"it will happen, all the predictions based\non people's behavior today say so\" (which are hopefully invalid thanks\nto the previous point). Ideally, I'd love to see some real free pressure\nalready on the network starting to develop when we commit to hardforking\nin a year. Not just full blocks with some fees because wallets are\nincluding far greater fees than they really need to, but software which\nproperly handles fees across the ecosystem, smart fee increases when\ntransactions arent confirming (eg replace-by-fee, which could be limited\nto increase-in-fees-only for those worried about double-spends).\n\nI probably forgot one or two and certainly dont want to back myself into\na corner on committing to something here, but those are a few things I\nsee today as big blockers on larger blocks.\n\nLuckily, people have been making progress on building the software\nneeded in all of the above for a while now, but I think they're all\nvery, very immature today.\n\nOn 05/07/15 19:13, Jeff Garzik wrote:> On Thu, May 7, 2015 at 3:03 PM,\nMatt Corallo <bitcoin-list at bluematt.me\n> <mailto:bitcoin-list at bluematt.me>> wrote:\n-snip-\n>> If, instead, there had been an intro on the list as \"I think we should\n>> do the blocksize increase soon, what do people think?\", the response\n>> could likely have focused much more around creating a specific list of\n>> things we should do before we (the technical community) think we are\n>> prepared for a blocksize increase.\n>\n> Agreed, but that is water under the bridge at this point.  You - rightly\n> - opened the topic here and now we're discussing it.\n>\n> Mike and Gavin are due the benefit of doubt because making a change to a\n> leaderless automaton powered by leaderless open source software is\n> breaking new ground.  I don't focus so much on how we got to this point,\n> but rather, where we go from here."
            },
            {
                "author": "Joseph Poon",
                "date": "2015-05-07T23:24:35",
                "message_text_only": "Hi Matt,\n\nI agree that starting discussion on how to approach this problem is\nnecessary and it's difficult taking positions without details on what is\nbeing discussed.\n\nA simple hard 20-megabyte increase will likely create perverse\nincentives, perhaps a method can exist with some safe transition. I\nthink ultimately, the underlying tension with this discussion is about\nthe relative power of miners. Any transition of blocksize increase will\nincrease the influence of miners, and it is about understanding the\ntradeoffs for each possible approach.\n\nOn Thu, May 07, 2015 at 10:02:09PM +0000, Matt Corallo wrote:\n>  * I'd like to see some better conclusions to the discussion around\n> long-term incentives within the system. If we're just building Bitcoin\n> to work in five years, great, but if we want it all to keep working as\n> subsidy drops significantly, I'd like a better answer than \"we'll deal\n> with it when we get there\" or \"it will happen, all the predictions based\n> on people's behavior today say so\" (which are hopefully invalid thanks\n> to the previous point). Ideally, I'd love to see some real free pressure\n> already on the network starting to develop when we commit to hardforking\n> in a year. Not just full blocks with some fees because wallets are\n> including far greater fees than they really need to, but software which\n> properly handles fees across the ecosystem, smart fee increases when\n> transactions arent confirming (eg replace-by-fee, which could be limited\n> to increase-in-fees-only for those worried about double-spends).\n\nI think the long-term fee incentive structure needs to be significantly\nmore granular. We've all seen miners and pools take the path of least\nresistance; often they just do whatever the community tells them to\nblindly. While this status quo can change in the future, I think\ndesigning sane defaults is a good path for any possible transition.\n\nIt seems especially reasonable to maintain fee pressure for normal\ntransactions during a hard-fork transition. It's possible to do so using\nsome kind of soft-cap structure. Building in a default soft-cap of 1\nmegabyte for some far future scheduled fork would seem like a sane thing\nto do for bitcoin-core.\n\nIt seems also viable to be far more aggressive. What's your (and the\ncommunity's) opinion on some kind of coinbase voting protocol for\nsoft-cap enforcement? It's possible to write in messages to the coinbase\nfor a enforcible soft-cap that orphans out any transaction which\nviolates these rules. It seems safest to have the transition has the\nfirst hardforked block be above 1MB, however, the next block default to\nan enforced 1MB block. If miners agree to go above this, they must vote\nin their coinbase to do so.\n\nThere's a separate discussion about this starting on:\nCAE-z3OXnjayLUeHBU0hdwU5pKrJ6fpj7YPtGBMQ7hKXG3Sj6hw at mail.gmail.com\n\nI think defaulting some kind of mechanism on reading the coinbase seems\nto be a good idea, I think left alone, miners may not do so. That way,\nit's possible to have your cake and eat it too, fee pressure will still\nexist, while block sizes can increase (provided it's in the miners'\ngreater interests to do so).\n\nThe Lightning Network's security model in the long-term may rely on a\nmulti-tier soft-cap, but I'm not sure. If 2nd order systemic miner\nincentives were not a concern, a system which has an enforced soft-cap\nand permits breaching that soft-cap with some agreed upon much higher\nfee would work best. LN works without this, but it seems to be more\nsecure if some kind of miner consensus rule is reached regarding\nprioritizing behavior of 2nd-layer consensus states.\n\nNo matter how it's done, certain aspects of the security model of\nsomething like Lightning is reliant upon having block-space\navailability for transactions to enter into the blockchain in a timely\nmanner (since \"deprecated\" channel states become valid again after some\nagreed upon block-time).\n\nI think pretty much everyone agrees that the 1MB block cap will\neventually be a problem. While people may disagree with when that will\nbe and how it'll play out, I think we're all in agreement that\ndiscussion about it is a good idea, especially when it comes to\nresolving blocking concerns.\n\nStarting a discussion on how a hypothetical blocksize increase will\noccur and the necessary blocking/want-to-have features/tradeoffs seems\nto be a great way to approach this problem. The needs for Lightning\nNetwork may be best optimized by being able to prioritizing a large mass\nof timeout transactions at once (when a well-connected node stops\ncommunicating).\n\n-- \nJoseph Poon"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-08T00:05:56",
                "message_text_only": "On Thu, May 07, 2015 at 10:02:09PM +0000, Matt Corallo wrote:\n> OK, so lets do that. I've seen a lot of \"I'm not entirely comfortable\n> with committing to this right now, but think we should eventually\", but\n> not much \"I'd be comfortable with committing to this when I see X\". In\n> the interest of ignoring debate and pushing people towards a consensus\n> at all costs, ( ;) ) I'm gonna go ahead and suggest we talk about the\n> second.\n> \n> Personally, there are several things that worry me significantly about\n> committing to a blocksize increase, which I'd like to see resolved\n> before I'd consider supporting a blocksize increase commitment.\n> \n>  * Though there are many proposals floating around which could\n> significantly decrease block propagation latency, none of them are\n> implemented today. I'd expect to see these not only implemented but\n> being used in production (though I dont particularly care about them\n> being all that stable). I'd want to see measurements of how they perform\n> both in production and in the face of high packet loss (eg across the\n> GFW or in the case of small/moderate DoS). In addition, I'd expect to\n> see analysis of how these systems perform in the worst-case, not just\n> packet-loss-wise, but in the face of miners attempting to break the system.\n\nIt's really important that we remember that we're building security\nsoftware: it *must* hold up well even in the face of attack. That means\nwe need to figure out how it can be attacked, what the cost/profits of\nsuch attacks are, and if the holes can be patched.  Just testing the\nsoftware with simulated loads is insufficient.\n\nAlso, re: breaking, don't forget that this may not be a malicious act.\nFor instance, someone can send contradictory transactions to different\nparts of the network simultaneously to prevent mempool consistency -\nthere's no easy way to fix this. There are also cases where miners have\ndifferent policy than others, e.g. version disagreements, commercial\ncontracts for tx mining, etc.\n\nFinally, remember that it's not in miners' incentives in many situations\nfor their blocks to propagate to more than ~30% of the hashing power.(1)\n\nPersonally, I'm really skeptical that we'll ever find a block\npropagation latency reduction technique that sucesfully meets all the\nabove criteria without changing the consensus algorithm itself.\n\n\n* How do we ensure miners don't cheat and stop validating blocks fully\nbefore building on them? This is a significant moral hazard with larger\nblocks if fees don't become significant, and can lead to dangerous\nforks. Also, think of the incentives: Why would a miner ever switch from\nthe longest chain, even if they don't actually have the blocks to back\nit up?\n\n* We need a clear understanding of how we expect new full nodes, pruned\nor not, to sync up to the blockchain. Obviously 20MB blocks\nsignificantly increases the time and data required to sync. Are we\nplanning on simply giving up on full validation and trusting others for\ncopies of UTXO sets? Are we going to rely on UTXO commitments? What\nhappens if the UTXO set size itself increases greatly?\n\n>  * I'd very much like to see someone working on better scaling\n> technology, both in terms of development and in terms of getting\n> traction in the marketplace. I know StrawPay is working on development,\n> though its not obvious to me how far they are from their website, but I\n> dont know of any commitments by large players (either SPV wallets,\n> centralized wallet services, payment processors, or any others) to\n> support such a system (to be fair, its probably too early for such\n> players to commit to anything, since anything doesnt exist in public).\n\nA good start would be for those players to commit to the general\nprinciples of these systems; if they can't commit explain why.\n\nFor instance I'd be very interested in knowing if services like Coinbase\nsee legal issues with adopting technologies such as payment channels\nbetween hosted wallet providers, payment processors, etc. I certainly\nwouldn't be surprised if they see doing anythign not on-blockchain as a\nsource of legal uncertainty - based on discussions I've had with\nregulatory types in this space it sounds like there's a reasonable\nchance protocol details such as requiring that transactions happen on a\npublic blockchain will be \"baked into\" regulatory requirements.\n\n>  * I'd like to see some better conclusions to the discussion around\n> long-term incentives within the system. If we're just building Bitcoin\n> to work in five years, great, but if we want it all to keep working as\n> subsidy drops significantly, I'd like a better answer than \"we'll deal\n> with it when we get there\" or \"it will happen, all the predictions based\n> on people's behavior today say so\" (which are hopefully invalid thanks\n> to the previous point). Ideally, I'd love to see some real free pressure\n> already on the network starting to develop when we commit to hardforking\n> in a year.\n\nAgreed.\n\n> Not just full blocks with some fees because wallets are\n> including far greater fees than they really need to, but software which\n> properly handles fees across the ecosystem, smart fee increases when\n> transactions arent confirming (eg replace-by-fee, which could be limited\n> to increase-in-fees-only for those worried about double-spends).\n\nFWIW I've got some funding to implement first-seen-safe replace-by-fee.\n\n1) http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg03200.html\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000fe0a96ac84aeb2e4e5c246e947cd8e759bd5fb158a16caf\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150507/f6c7c97c/attachment.sig>"
            },
            {
                "author": "Arkady",
                "date": "2015-05-08T06:33:18",
                "message_text_only": "--[remove this line and above]--\nOn Thu, 7 May 2015, Gregory Maxwell wrote:\n\n> Date: Thu, 7 May 2015 00:37:54 +0000\n> From: Gregory Maxwell <gmaxwell at gmail.com>\n> To: Matt Corallo <bitcoin-list at bluematt.me>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Subject: Re: [Bitcoin-development] Block Size Increase\n> \n> Thanks Matt; I was actually really confused by this sudden push with\n> not a word here or on Github--so much so that I responded on Reddit to\n> people pointing to commits in Gavin's personal repository saying they\n> were reading too much into it.\n\nI saw this. I was also pointing this out to the people who were asking \nme. A\ncommit to a personal repository does not at first seem more than\nexperimental. sipa commits weird/neat things to private branches all the\ntime, after all.\n\n> to share behavior. In the case of mining, we're trying to optimize the\n> social good of POW security. (But the analogy applies in other ways \n> too:\n\nAbout the only argument IMO in favour of block size increases is to \nassume\nthat making more room in a block will make it attractive to use for more\npeople at some point in the future: increasing transaction velocity,\nincreasing economy size, increasing value overall.\n\n> increases to the chain side are largely an externality; miners enjoy \n> the\n> benefits, everyone else takes the costs--either in reduced security or\n> higher node operating else.)\n\nWho else but miners and pool operators will run full nodes when full \nnodes\nare being shut down because they are too large and unwieldy to maintain? \nIt\nis already so that casual users refuse to run full nodes. This fact is\nindisputable. The only question remaining is, \"Do we care?\" Arguments\nagainst users who feel that the dataset is too large to run a full node,\nfull-time, start from a premise that these users are a static and \nirrelevant\nfraction. Is this even true? \"Do we care?\" I do. I will shortly only be \nable\nto run half the nodes I currently do thanks to the growth of the \nblockchain\nat its current rate.\n\n> One potential argument is that maybe miners would be _regulated_ to\n> behave correctly. But this would require undermining the openness of \n> the\n> system--where anyone can mine anonymously--in order to enforce \n> behavior,\n> and that same enforcement mechanism would leave a political level to\n> impose additional rules that violate the extra properties of the \n> system.\n\nI would refuse to mine under such a regulated regime; moreover, I would\nenjoy forking away from this, and, I suspect, the only miners who remain\nwould be those whose ultimate motivations do not coincide with the \nusers.\nThat is, the set of miners who are users, and the set of users who are\nminers, would be wholly non-intersecting.\n\n> So far the mining ecosystem has become incredibly centralized over \n> time.\n\nThis is unfortunate but true.\n\n> of the regular contributors to Bitcoin Core do. Many participants\n> have never mined or only did back in 2010/2011... we've basically\n> ignored the mining ecosystem, and this has had devastating effects,\n> causing a latent undermining of the security model: hacking a dozen or\n> so computers--operated under totally unknown and probably not strong\n> security policies--could compromise the network at least at the tip...\n\nThe explicit form of the block dictated by the reference client and\nagreed-to by the people who were sold on bitcoin near the beginning \n(myself\nincluded) was explicitly the notion that the rules were static; that the\nnature of transaction foundations and the subsidies would not be \naltered.\nHere we have a hardfork being contemplated which is not only \ncontroversial,\nbut does not even address some of the highest-utility and most-requested\nfeatures in peoples' hardfork wishlists.\n\nThe fact that mining has effectively been centralized directly implies \nthat\ndestabilizing changes that some well-heeled (and thus theoretically \ncapable,\nat least) people have explicitly begun plans to fork the blockchain \nabout\nwill have an unknown, and completely unforeseen combined effect.\n\nWe can pretend that, \"If merchants and miners and exchanges go along, \nthen\nwho else matters,\" but the reality is that the value in bitcoin exists\nbecause *people* use it for real transactions: Not miners, whose profits \nare\nparasitically fractionally based on the quality and strength of the \nbitcoin\neconomy as a whole; not exchanges who lubricate transactions in service \nto\nthe economy; not even today's merchants whose primary means of accepting\nbitcoin seems to be to convert them instantly to fiat and not \nparticipate\nmeaningfully in the economy at all; not enriched felons; but actual \nusers\nthemselves.\n\n> Rightfully we should be regarding this an an emergency, and probably\n> should have been have since 2011.\n\nThere are two ways to look at it, assuming that the blocksize change\nincreases bitcoin's value to people after all: mining centralization \nwill be\ncorrected; or, mining centralization will not be corrected.\n\nI would argue that rapidly increasing profitability at this point will\nexacerbate the mining centralization problem, and in much the same way \nas\nwhen people were throwing money and unknowingly funding the massive \nfrauds\nof the current cabals when bitcoin's exchange-driven rise to $1200 was \nfirst\nrealized.\n\nThus, even if the premise were true, what will a blocksize increase \nachieve\ngiven mining centralization itself is a bigger systemic risk?\n\n> Hardfork changes should only be made if they're almost completely\n> uncontroversial--where virtually everyone can look at the available \n> data\n> and say \"yea, that isn't undermining my property rights or future use\n> of Bitcoin; it's no big deal\".\n\nThe recent \"revelation\" that there are masses of paid trolls on popular\nforum sites like reddit who supposedly don't even know who is hiring \nthem,\nand the anger of more vociferous commenters in general, does not \ninvalidate\nthe relevance of every non-\"industry\" voice.  I think elevating the\ndiscussion away from the users does the system and the development \nprocess\nas a whole quite an injustice.\n\n> I'm curious as to what discussions people have seen; e.g., are people\n> even here aware of these concerns? Are you aware of things like the\n> hashcash mediated dynamic blocksize limiting?\n\nI have seen most of these; or the ideas seem obvious based on their \nnames.\n\n> About proposals like lightning network (instant transactions and \n> massive\n> scale, in exchange for some short term DOS risk if a counterparty opts\n> out)? Do people (other than Mike Hearn; I guess) think a future where\n> everyone depends on a small number of \"Google scale\" node operations \n> for\n> the system is actually okay? (I think not, and if so we're never going \n> to\n> agree--but it can be helpful to understand when a disagreement is\n> ideological).\n\nIt is not okay. If the current mining cabals continue to exist, and\nflourish, and the developers make major changes that ignore this glaring\nelephant, then the decentralized promise of bitcoin will be put more at\nrisk.\n\nsignmessage 1DdcrjT9Yqb6U58wVMA2e7untFbz2rmZd4 \n\"49786791f4d0a260689867ccdfb2cc5b8460984e335504444ade113d2768505c\"\nG6NPl7Wklo9lcdgeVI2H2pexzgqD0KPHhI/wAe32DBm8m59Qf31j5d4tsx5drcql/8wPeIb0QGarr/o4VIOLLGE=\n\n--[remove this line and below]--\nHHsTfiZ/S7+GNYRwws+QyAr+6/MgDz0Jyntl7CAvjhdfzbnwPorybQUXxRw3CE4DgYgAy1zLanE8H/5NK+l3UlE="
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-08T10:03:04",
                "message_text_only": ">\n>  * Though there are many proposals floating around which could\n> significantly decrease block propagation latency, none of them are\n> implemented today.\n\n\nWith a 20mb cap, miners still have the option of the soft limit.\n\nI would actually be quite surprised if there were no point along the road\nfrom 1mb to 20mb where miners felt a need to throttle their block sizes\nartificially, for the exact reason you point out: propagation delays.\n\nBut we don't *need* to have fancy protocol upgrades implemented right now.\nAll we need is to demolish one bottleneck (the hard cap) so we can then\nmove on and demolish the next one (whatever that is, probably faster\npropagation). Scaling is a series of walls we punch through as we encounter\nthem. One down, onto the next. We don't have to tackle them all\nsimultaneously.\n\nFWIW I don't think the GFW just triggers packet loss, these days. It's\nblocked port 8333 entirely.\n\n * I'd very much like to see someone working on better scaling\n> technology ... I know StrawPay is working on development,\n>\n\nSo this request is already satisfied, isn't it? As you point out, expecting\nmore at this stage in development is unreasonable, there's nothing for\nanyone to experiment with or commit to.\n\nThey have code here, by the way:\n\n   https://github.com/strawpay\n\nYou can find their fork of MultiBit HD, their implementation library, etc.\nThey've contributed patches and improvements to the payment channels code\nwe wrote.\n\n\n>  * I'd like to see some better conclusions to the discussion around\n> long-term incentives within the system.\n>\n\nWhat are your thoughts on using assurance contracts to fund network\nsecurity?\n\nI don't *know* if hashing assurance contracts (HACs) will work. But I don't\nknow they won't work either. And right now I'm pretty sure that plain old\nfee pressure won't work. Demand doesn't outstrip supply forever - people\nfind substitutes.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/431f2412/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-08T16:37:01",
                "message_text_only": "On Fri, May 08, 2015 at 12:03:04PM +0200, Mike Hearn wrote:\n> >\n> >  * Though there are many proposals floating around which could\n> > significantly decrease block propagation latency, none of them are\n> > implemented today.\n> \n> \n> With a 20mb cap, miners still have the option of the soft limit.\n\nThe soft-limit is there miners themselves produce smaller blocks; the\nsoft-limit does not prevent other miners from producing larger blocks.\n\nAs we're talking about ways that other miners can use 20MB blocks to\nharm the competition, talking about the soft-limit is irrelevant.\nSimilarly, as security engineers we must plan for the worst case; as\nwe've seen before by your campaigns to raise the soft-limit(1) even at a\ntime when the vast majority of transaction volume was from one user\n(SatoshiDice) soft-limits are an extremely weak form of control.\n\nFor the proposes of discussing blocksize increase requirements we can\nstop talking about the soft-limit.\n\n1) https://bitcointalk.org/index.php?topic=149668.0\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000009344ba165781ee352f93d657c8b098c8e518e6011753e59\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/b669d9b9/attachment.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-08T19:47:52",
                "message_text_only": "On Fri, May 8, 2015 at 5:37 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> The soft-limit is there miners themselves produce smaller blocks; the\n> soft-limit does not prevent other miners from producing larger blocks.\n>\n\nI wonder if having a \"miner\" flag would be good for the network.\n\nClients for general users and merchants would have a less strict rule than\nthe rule for miners.  Miners who don't set their miners flag might get\norphaned off the chain.\n\nFor example, the limits could be setup as follows.\n\nClients: 20MB\nMiners: 4MB\n\nWhen in \"miner mode\", the client would reject 4MB blocks and wouldn't build\non them.  The reference client might even track the miner and the non-miner\nchain tip.\n\nMiners would refuse to build on 5MB blocks, but merchants and general users\nwould accept them.\n\nThis allows the miners to soft fork the limit at some point in the future.\nIf 75% of miners decided to up the limit to 8MB, then all merchants and the\ngeneral users would accept the new blocks.  It could follow the standard\nsoft fork rules.\n\nThis is a more general version of the system where miners are allowed to\nvote on the block size (subject to a higher limit).\n\nA similar system is where clients track all header trees.  Your wallet\ncould warn you that there is an invalid tree that has > 75% of the hashing\npower and you might want to upgrade.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/51802e38/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-09T03:08:33",
                "message_text_only": "On Fri, May 08, 2015 at 08:47:52PM +0100, Tier Nolan wrote:\n> On Fri, May 8, 2015 at 5:37 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> > The soft-limit is there miners themselves produce smaller blocks; the\n> > soft-limit does not prevent other miners from producing larger blocks.\n> >\n> \n> I wonder if having a \"miner\" flag would be good for the network.\n\nMakes it trivial to find miners and DoS attack them - a huge risk to the\nnetwork as a whole, as well as the miners.\n\nRight now pools already get DoSed all the time through their work\nsubmission systems; getting DoS attacked via their nodes as well would\nbe a disaster.\n\n> When in \"miner mode\", the client would reject 4MB blocks and wouldn't build\n> on them.  The reference client might even track the miner and the non-miner\n> chain tip.\n> \n> Miners would refuse to build on 5MB blocks, but merchants and general users\n> would accept them.\n\nThat'd be an excellent way to double-spend merchants, significantly\nincreasing the chance that the double-spend would succeed as you only\nhave to get sufficient hashing power to get the lucky blocks; you don't\nneed enough hashing power to *also* ensure those blocks don't become the\nlongest chain, removing the need to sybil attack your target.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000004bd67400df7577a30e6f509b6bd82633efeabe6395eb65a\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/b01ee01a/attachment.sig>"
            },
            {
                "author": "Stephen",
                "date": "2015-05-16T04:39:53",
                "message_text_only": "Comments in line:\n\n> On May 8, 2015, at 11:08 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> Makes it trivial to find miners and DoS attack them - a huge risk to the\n> network as a whole, as well as the miners.\n> \n> Right now pools already get DoSed all the time through their work\n> submission systems; getting DoS attacked via their nodes as well would\n> be a disaster.\n\nIt seems that using a -miner flag to follow rules about smaller blocks would only reveal miner nodes if one sent the node a solved block that that was valid in every way except the block size. While not impossible, I wouldn't call this trivial, as it still requires wasting an entire block's worth of energy. \n\n>> When in \"miner mode\", the client would reject 4MB blocks and wouldn't build\n>> on them.  The reference client might even track the miner and the non-miner\n>> chain tip.\n>> \n>> Miners would refuse to build on 5MB blocks, but merchants and general users\n>> would accept them.\n> \n> That'd be an excellent way to double-spend merchants, significantly\n> increasing the chance that the double-spend would succeed as you only\n> have to get sufficient hashing power to get the lucky blocks; you don't\n> need enough hashing power to *also* ensure those blocks don't become the\n> longest chain, removing the need to sybil attack your target.\n> \n\nI think this could be mitigated by counting confirmations differently. We should think of confirmations as only coming from blocks following the miners' more strict rule set. So if a merchant were to see payment for the first time in a block that met their own size restrictions but not the miners', then they would simply count it as unconfirmed. \n\nIf they get deep enough in the chain, though, the client should probably count them as being confirmed anyway, even if they don't meet the client nodes' expectation of the miners' block size limit. This happening probably just means that the client has not updated their software (or -minermaxblocksize configuration, depending on how it is implemented) in a long time. \n\nI actually like Tier's suggestion quite a bit. I think we could have the default client limit set to some higher number, and have miners agree out of band on the latest block size limit. Or maybe even build in a way to vote into the blockchain. \n\nBest, \nStephen"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-16T11:29:14",
                "message_text_only": "On Sat, May 16, 2015 at 5:39 AM, Stephen <stephencalebmorse at gmail.com>\nwrote:\n\n> I think this could be mitigated by counting confirmations differently. We\n> should think of confirmations as only coming from blocks following the\n> miners' more strict rule set. So if a merchant were to see payment for the\n> first time in a block that met their own size restrictions but not the\n> miners', then they would simply count it as unconfirmed.\n>\n\nIn effect, there is a confirm penalty for less strict blocks.  Confirms =\nmax(miner_confirms, merchant_confirms - 3, 0)\n\nMerchants who don't upgrade end up having to wait longer to hit\nconfirmations.\n\nIf they get deep enough in the chain, though, the client should probably\n> count them as being confirmed anyway, even if they don't meet the client\n> nodes' expectation of the miners' block size limit. This happening probably\n> just means that the client has not updated their software (or\n> -minermaxblocksize configuration, depending on how it is implemented) in a\n> long time.\n>\n\nThat is a good idea.  Any parameters that have miner/merchant differences\nshould be modifiable (but only upwards) in the command line.\n\n\"Why are my transactions taking longer to confirm?\"\n\n\"There was a soft fork to make the block size larger and your client is\nbeing careful.  You need to add \"minermaxblocksize=4MB\" to your\nbitcoin.conf file.\"\n\nHah, it could be called a \"semi-hard fork\"?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150516/e5dd0406/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-16T11:25:53",
                "message_text_only": "On Sat, May 9, 2015 at 4:08 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> > I wonder if having a \"miner\" flag would be good for the network.\n>\n> Makes it trivial to find miners and DoS attack them - a huge risk to the\n> network as a whole, as well as the miners.\n>\n\nTo mitigate against this, two chaintips could be tracked.  The miner tip\nand the client tip.\n\nMiners would build on the miner tip.  When performing client services, like\nwallets, they would use the client tip.\n\nThe client would act exactly the same as any node, the only change would be\nthat it gives miner work based on the mining tip.\n\nIf the two tips end up significantly forking, there would be a warning to\nthe miner and perhaps eventually refuse to give out new work.\n\nThat would happen when there was a miner level hard-fork.\n\n\n> That'd be an excellent way to double-spend merchants, significantly\n> increasing the chance that the double-spend would succeed as you only\n> have to get sufficient hashing power to get the lucky blocks; you don't\n> need enough hashing power to *also* ensure those blocks don't become the\n> longest chain, removing the need to sybil attack your target.\n>\n\nTo launch that attack, you need to produce fake blocks.  That is\nexpensive.\n\nStephen Cale's suggestion to wait more than one block before counting a\ntransaction as confirmed would also help mitigate.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150516/ef7854ef/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-29T22:36:51",
                "message_text_only": "Matt brought this up on Twitter, I have no idea why I didn't respond weeks\nago (busy writing blog posts, probably):\n\nOn Thu, May 7, 2015 at 6:02 PM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n>\n>\n>  * Though there are many proposals floating around which could\n> significantly decrease block propagation latency, none of them are\n> implemented today.\n\n\nIf block propagation isn't fixed, then mines have a strong incentive to\ncreate smaller blocks.\n\nSo the max block size is irrelevant, it won't get hit.\n\n\n> In addition, I'd expect to\n> see analysis of how these systems perform in the worst-case, not just\n> packet-loss-wise, but in the face of miners attempting to break the system.\n>\n\nSee http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners\nfor analysis of \"but that means bigger miners can get an advantage\"\nargument.\n\nExecutive summary: if little miners are stupid and produce huge blocks,\nthen yes, big miners have an advantage.\n\nBut they're not, so they won't.\n\nUntil the block reward goes away, and assuming transaction fees become an\nimportant source of revenue for miners.\nI think it is too early to worry about that; see:\n\n   http://gavinandresen.ninja/when-the-block-reward-goes-away\n\n\n>  * I'd very much like to see someone working on better scaling\n> technology, both in terms of development and in terms of getting\n> traction in the marketplace.\n\n\nOk. What does this have to do with the max block size?\n\nAre you arguing that work won't happen if the max block size increases?\n\n  * I'd like to see some better conclusions to the discussion around\n\n> long-term incentives within the system.\n\n\nAgain, see http://gavinandresen.ninja/when-the-block-reward-goes-away for\nwhat I think about that.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/c145f6b9/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-29T23:25:27",
                "message_text_only": "On 05/29/15 22:36, Gavin Andresen wrote:\n> Matt brought this up on Twitter, I have no idea why I didn't respond\n> weeks ago (busy writing blog posts, probably):\n> \n> On Thu, May 7, 2015 at 6:02 PM, Matt Corallo <bitcoin-list at bluematt.me\n> <mailto:bitcoin-list at bluematt.me>> wrote:\n> \n> \n> \n>      * Though there are many proposals floating around which could\n>     significantly decrease block propagation latency, none of them are\n>     implemented today.\n> \n> \n> If block propagation isn't fixed, then mines have a strong incentive to\n> create smaller blocks.\n> \n> So the max block size is irrelevant, it won't get hit.\n\nSadly, this is very far from the whole story. The issue of miners\noptimizing for returns has been discussed several times during this\ndiscussion, and, sadly, miners who are geographically colocated who are\noptimizing for returns with a free-floating blocksize will optimize away\n50% of the network!\n\n> \n>     In addition, I'd expect to\n>     see analysis of how these systems perform in the worst-case, not just\n>     packet-loss-wise, but in the face of miners attempting to break the\n>     system.\n> \n> \n> See http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners for\n> analysis of \"but that means bigger miners can get an advantage\" argument.\n> \n> Executive summary: if little miners are stupid and produce huge blocks,\n> then yes, big miners have an advantage.\n\nI'll talk about transaction fees in a second, but there are several\nproblems with this already. As pointed out in the original mail, gfw has\nalready been known to interfere with Bitcoin P2P traffic. So now by\n\"little\" miners, you mean any miner who is not located in mainland\nChina? Whats worse, the disadvantage is symmetric - little miners are at\na disadvantage when *anyone* mines a bigger block, and miners dont even\nhave to be \"evil\" for this to happen - just optimize for profits.\n\n> But they're not, so they won't.\n\nI dont know what you're referring to with this. Are you claiming little\nminers today optimize for relay times and have good visibility into the\nBitcoin network and calculate an optimal block size based on this (or\nwould with a 20MB block size)?\n\n> Until the block reward goes away, and assuming transaction fees become\n> an important source of revenue for miners.\n> I think it is too early to worry about that; see:\n> \n>    http://gavinandresen.ninja/when-the-block-reward-goes-away\n\nYou dont make any points here with which I can argue, but let me respond\nwith the reason /I/ think it is a problem worth thinking a little bit\nabout...If we increase the blocksize sufficiently such that transaction\nfees are not the way in which miners make their money, then either\nminers are not being funded (ie hashpower has to drop to very little),\nor the only people mining/funding miners are large orgs who are\n\"running\" Bitcoin (ie the web wallets, payment processors, big\nmerchants, and exchanges of the world). Sadly, this is no longer a\ndecentralized Bitcoin and is, in fact, pretty much how the banking world\nworks today.\n\nI'm not sure who, if anyone, claims Bitcoin is novel or interesting for\nany reason other than its decentralization properties, and, in a world\nwhich you are apparently proposing, the \"natural\" course of things is to\nvery strongly centralize.\n\n>      * I'd very much like to see someone working on better scaling\n>     technology, both in terms of development and in terms of getting\n>     traction in the marketplace. \n> \n> \n> Ok. What does this have to do with the max block size?\n> \n> Are you arguing that work won't happen if the max block size increases?\n\nYes, I am arguing that by increasing the blocksize the incentives to\nactually make Bitcoin scale go away. Even if amazing technologies get\nbuilt, no one will have any reason to use them.\n\n>   * I'd like to see some better conclusions to the discussion around\n> \n>     long-term incentives within the system.\n> \n> \n> Again, see http://gavinandresen.ninja/when-the-block-reward-goes-away\n> for what I think about that."
            },
            {
                "author": "Chun Wang",
                "date": "2015-05-29T23:42:16",
                "message_text_only": "Hello. I am from F2Pool. We are currently mining the biggest blocks on\nthe network. So far top 100 biggest bitcoin blocks are all from us. We\ndo support bigger blocks and sooner rather than later. But we cannot\nhandle 20 MB blocks right now. I know most blocks would not be 20 MB\nover night. But only if a small fraction of blocks more than 10 MB, it\ncould dramatically increase of our orphan rate, result of higher fee\nto miners. Bad miners could attack us and the network with artificial\nbig blocks. As yhou know, other Chinese pools, AntPool, BW, they\nproduces ASIC chips and mining mostly with their own machines. They do\nnot care about a few percent of orphan increase as much as we do. They\nwould continue their zero fee policy. We would be the biggest loser.\nAs the exchanges had taught us, zero fee is not health to the network.\nAlso we have to redevelop our block broadcast logic. Server bandwidth\nis a lot more expensive in China. And the Internet is slow. Currently\nChina has more than 50% of mining power, if block size increases, I\nbet European and American pools could suffer more than us. We think\nthe max block size should be increased, but must be increased\nsmoothly, 2 MB first, and then after one or two years 4 MB, then 8 MB,\nand so on. Thanks.\n\nOn Fri, May 8, 2015 at 6:02 AM, Matt Corallo <bitcoin-list at bluematt.me> wrote:\n> OK, so lets do that. I've seen a lot of \"I'm not entirely comfortable\n> with committing to this right now, but think we should eventually\", but\n> not much \"I'd be comfortable with committing to this when I see X\". In\n> the interest of ignoring debate and pushing people towards a consensus\n> at all costs, ( ;) ) I'm gonna go ahead and suggest we talk about the\n> second.\n>\n> Personally, there are several things that worry me significantly about\n> committing to a blocksize increase, which I'd like to see resolved\n> before I'd consider supporting a blocksize increase commitment.\n>\n>  * Though there are many proposals floating around which could\n> significantly decrease block propagation latency, none of them are\n> implemented today. I'd expect to see these not only implemented but\n> being used in production (though I dont particularly care about them\n> being all that stable). I'd want to see measurements of how they perform\n> both in production and in the face of high packet loss (eg across the\n> GFW or in the case of small/moderate DoS). In addition, I'd expect to\n> see analysis of how these systems perform in the worst-case, not just\n> packet-loss-wise, but in the face of miners attempting to break the system.\n>\n>  * I'd very much like to see someone working on better scaling\n> technology, both in terms of development and in terms of getting\n> traction in the marketplace. I know StrawPay is working on development,\n> though its not obvious to me how far they are from their website, but I\n> dont know of any commitments by large players (either SPV wallets,\n> centralized wallet services, payment processors, or any others) to\n> support such a system (to be fair, its probably too early for such\n> players to commit to anything, since anything doesnt exist in public).\n>\n>  * I'd like to see some better conclusions to the discussion around\n> long-term incentives within the system. If we're just building Bitcoin\n> to work in five years, great, but if we want it all to keep working as\n> subsidy drops significantly, I'd like a better answer than \"we'll deal\n> with it when we get there\" or \"it will happen, all the predictions based\n> on people's behavior today say so\" (which are hopefully invalid thanks\n> to the previous point). Ideally, I'd love to see some real free pressure\n> already on the network starting to develop when we commit to hardforking\n> in a year. Not just full blocks with some fees because wallets are\n> including far greater fees than they really need to, but software which\n> properly handles fees across the ecosystem, smart fee increases when\n> transactions arent confirming (eg replace-by-fee, which could be limited\n> to increase-in-fees-only for those worried about double-spends).\n>\n> I probably forgot one or two and certainly dont want to back myself into\n> a corner on committing to something here, but those are a few things I\n> see today as big blockers on larger blocks.\n>\n> Luckily, people have been making progress on building the software\n> needed in all of the above for a while now, but I think they're all\n> very, very immature today.\n>\n> On 05/07/15 19:13, Jeff Garzik wrote:> On Thu, May 7, 2015 at 3:03 PM,\n> Matt Corallo <bitcoin-list at bluematt.me\n>> <mailto:bitcoin-list at bluematt.me>> wrote:\n> -snip-\n>>> If, instead, there had been an intro on the list as \"I think we should\n>>> do the blocksize increase soon, what do people think?\", the response\n>>> could likely have focused much more around creating a specific list of\n>>> things we should do before we (the technical community) think we are\n>>> prepared for a blocksize increase.\n>>\n>> Agreed, but that is water under the bridge at this point.  You - rightly\n>> - opened the topic here and now we're discussing it.\n>>\n>> Mike and Gavin are due the benefit of doubt because making a change to a\n>> leaderless automaton powered by leaderless open source software is\n>> breaking new ground.  I don't focus so much on how we got to this point,\n>> but rather, where we go from here.\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-30T13:57:32",
                "message_text_only": "On Fri, May 29, 2015 at 7:42 PM, Chun Wang <1240902 at gmail.com> wrote:\n\n> Hello. I am from F2Pool. We are currently mining the biggest blocks on\n> the network.\n\n\nThanks for giving your opinion!\n\n\n\n> Bad miners could attack us and the network with artificial\n> big blocks.\n\n\nHow?\n\nI ran some simulations, and I could not find a network topology where a big\nminer producing big blocks could cause a loss of profit to another miner\n(big or small) producing smaller blocks:\n\nhttp://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners\n\n(the 0.3% advantage I DID find was for the situation where EVERYBODY was\nproducing big blocks).\n\n\n> We think\n> the max block size should be increased, but must be increased\n> smoothly, 2 MB first, and then after one or two years 4 MB, then 8 MB,\n> and so on. Thanks.\n\n\nWhy 2 MB ?   You said that server bandwidth is much more expensive in\nChina; what would be the difference in your bandwidth costs between 2MB\nblocks and 20MB blocks?\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/643c32da/attachment.html>"
            },
            {
                "author": "Pindar Wong",
                "date": "2015-05-30T14:08:13",
                "message_text_only": "On Sat, May 30, 2015 at 9:57 PM, Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> On Fri, May 29, 2015 at 7:42 PM, Chun Wang <1240902 at gmail.com> wrote:\n>\n>> Hello. I am from F2Pool. We are currently mining the biggest blocks on\n>> the network.\n>\n>\n> Thanks for giving your opinion!\n>\n>\n>\n>> Bad miners could attack us and the network with artificial\n>> big blocks.\n>\n>\n> How?\n>\n> I ran some simulations, and I could not find a network topology where a\n> big miner producing big blocks could cause a loss of profit to another\n> miner (big or small) producing smaller blocks:\n>\n> http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners\n>\n> (the 0.3% advantage I DID find was for the situation where EVERYBODY was\n> producing big blocks).\n>\n>\n>> We think\n>> the max block size should be increased, but must be increased\n>> smoothly, 2 MB first, and then after one or two years 4 MB, then 8 MB,\n>> and so on. Thanks.\n>\n>\n> Why 2 MB ?   You said that server bandwidth is much more expensive in\n> China; what would be the difference in your bandwidth costs between 2MB\n> blocks and 20MB blocks?\n>\n\nPerhaps we should arrange to run some more 'simulations' with miners from\nChina and elsewhere?\n\nLet me know there's interest to do.\n\np.\n\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/527c779b/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-30T22:05:15",
                "message_text_only": "> Why 2 MB ?\n>\n\nWhy 20 MB? Do you anticipate 20x transaction count growth in 2016?\n\nWhy not grow it by 1 MB per year?\nThis is a safer option, I don't think that anybody claims that 2 MB blocks\nwill be a problem.\nAnd in 10 years when we get to 10 MB we'll get more evidence as to whether\nnetwork can handle 10 MB blocks.\n\nSo this might be a solution which would satisfy both sides:\n  *  people who are concerned about block size growth will have an\nopportunity to stop it before it grows too much (e.g. with a soft fork),\n  *  while people who want bigger blocks will get an equivalent of 25% per\nyear growth within the first 10 years, which isn't bad, is it?\n\nSo far I haven't heard any valid arguments against linear growth.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/29b010f3/attachment.html>"
            },
            {
                "author": "Brian Hoffman",
                "date": "2015-05-30T23:16:42",
                "message_text_only": "> Why 20 MB? Do you anticipate 20x transaction count growth in 2016?\n\n\nDo you anticipate linear growth?\n\n> On May 30, 2015, at 6:05 PM, Alex Mizrahi <alex.mizrahi at gmail.com> wrote:\n> \n>  \n>> Why 2 MB ?\n> \n> Why 20 MB? Do you anticipate 20x transaction count growth in 2016?\n> \n> Why not grow it by 1 MB per year?\n> This is a safer option, I don't think that anybody claims that 2 MB blocks will be a problem.\n> And in 10 years when we get to 10 MB we'll get more evidence as to whether network can handle 10 MB blocks.\n> \n> So this might be a solution which would satisfy both sides:\n>   *  people who are concerned about block size growth will have an opportunity to stop it before it grows too much (e.g. with a soft fork),\n>   *  while people who want bigger blocks will get an equivalent of 25% per year growth within the first 10 years, which isn't bad, is it?\n> \n> So far I haven't heard any valid arguments against linear growth.\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/250c732e/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-31T00:13:30",
                "message_text_only": "> Why 20 MB? Do you anticipate 20x transaction count growth in 2016?\n>\n> Do you anticipate linear growth?\n>\n\nIt's safe to say that absolutely nobody can predict the actual growth with\nany degree of an accuracy.\nI believe that linear growth compares very favorably to other alternatives:\n\n1. Exponential growth: Linear growth is better at modelling diminishing\nreturns, that is, risk that it grows too much is much smaller. At the same\ntime initially it will grow faster than reasonable exponential models.\n   E.g. linear year-over-year relative growth:    100% 50% 33% 25% ...10%\n   While exponential one which gives the same result in 10 years:\n   25% 25% ... 25%\n   This is on the same scale, but exponential starts slower than we want at\nstart (1.25 MB will be too little for 2016 as we already see fully filled 1\nMB blocks), but goes a bit too fast in the long term. It's highly unlikely\nwe'll see bandwidth growing 10x each 10 years in the long term.\n\n2. Single step increase: an obvious advantage is that linear growth gives\nus time to adapt to near realities, time to change something if there is an\nunwanted effects, etc. At the same a single step is not a long-term\nsolution.\nWhile a slow-but-steady growth might be.\n\n3. Adaptive solutions (e.g. limit depends on the last N blocks or something\nof that nature):\n  The problem with them is that they are  rather complex, and also:\n  3.1. prone to manipulation: somebody might try to push the limit if it\nwill favor him in future\n  3.2. possibility of a positive feedback loop.\n  3.3. possibility of an unhealthy game-theoretic dynamics\n\nThe main problem is that we do not understand game theoretic aspects of\nbitcoin mining in presence of various real-world factors such as block\npropagation delays. Thus we can't design a proper adaptive solution.\n\n\nThere is no perfect solution to this problem as we cannot predict the\nfuture and our understanding is limited.\nBut among the 5 alternatives (linear, exponential, single step, adaptive,\nno limit), linear seems to be the best option at this point as it's both\nquite safe and doesn't stunt growth too much.\n\n> bitcoin is really really small right now, any sign of real adoption could\nmake it grow 100x or even more in a matter of weeks.\n\nThis is certainly possible, but the thing is:\n\n1) this can't be predicted;\n2) this will be a serious problem for many bitcoind installations;\n3) it's not necessarily a healthy thing, perhaps it will grow 100x in a\nmatter of weeks, and then will go to zero in matter of weeks as well.\n\nSo I don't think that sudden growth spurts is something we should take into\naccount on the planning stage. If anything we'd like to prevent them from\nhappening, slow growth is usually better.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/7511ce61/attachment.html>"
            },
            {
                "author": "gb",
                "date": "2015-05-31T05:05:56",
                "message_text_only": "Linear growth is indeed the 'simplest' model for growth so removes\nconcerns of complexity using such a growth model. Seems like it might be\na safe compromise between exponential growth, zero growth and buys some\ntime to observe the longer term scale network behaviour. \n\nA simple linear growth 'hard' technical limit could also be used\nconjunction with the simple periodic soft dynamic limit adjustment (e.g.\n1.5x of moving average) as discussed recently. So that the combination\nprovides for growth, with fee pressure, up until if/when the technical\nhard limit is hit. And if we keep hitting the hard limit that signals a\nmarket demand for ancillary layers to be built out, that has been\nmissing until now.\n\nOn Sun, 2015-05-31 at 01:05 +0300, Alex Mizrahi wrote:\n>  \n>         \n>         Why 2 MB ?\n> \n> \n> Why 20 MB? Do you anticipate 20x transaction count growth in 2016?\n> \n> \n> Why not grow it by 1 MB per year?\n> This is a safer option, I don't think that anybody claims that 2 MB\n> blocks will be a problem.\n> And in 10 years when we get to 10 MB we'll get more evidence as to\n> whether network can handle 10 MB blocks.\n> \n> \n> So this might be a solution which would satisfy both sides:\n>   *  people who are concerned about block size growth will have an\n> opportunity to stop it before it grows too much (e.g. with a soft\n> fork),\n>   *  while people who want bigger blocks will get an equivalent of 25%\n> per year growth within the first 10 years, which isn't bad, is it?\n> \n> \n> So far I haven't heard any valid arguments against linear growth.\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-31T07:05:30",
                "message_text_only": "On Sat, May 30, 2015 at 07:42:16AM +0800, Chun Wang wrote:\n> Hello. I am from F2Pool. We are currently mining the biggest blocks on\n> the network. So far top 100 biggest bitcoin blocks are all from us. We\n> do support bigger blocks and sooner rather than later. But we cannot\n> handle 20 MB blocks right now. I know most blocks would not be 20 MB\n> over night. But only if a small fraction of blocks more than 10 MB, it\n> could dramatically increase of our orphan rate, result of higher fee\n> to miners. Bad miners could attack us and the network with artificial\n> big blocks. As yhou know, other Chinese pools, AntPool, BW, they\n> produces ASIC chips and mining mostly with their own machines. They do\n> not care about a few percent of orphan increase as much as we do. They\n> would continue their zero fee policy. We would be the biggest loser.\n> As the exchanges had taught us, zero fee is not health to the network.\n> Also we have to redevelop our block broadcast logic. Server bandwidth\n> is a lot more expensive in China. And the Internet is slow. Currently\n> China has more than 50% of mining power, if block size increases, I\n> bet European and American pools could suffer more than us. We think\n> the max block size should be increased, but must be increased\n> smoothly, 2 MB first, and then after one or two years 4 MB, then 8 MB,\n> and so on. Thanks.\n\nGreat to hear from you!\n\nYeah, I'm pretty surprised myself that Gavin never accepted the\ncompromises offered by others in this space for a slow growth solution,\nrather than starting with over an order of magnitude blocksize increase.\nThis is particularly surprising when his own calculations - after\ncorrecting an artithmetic error - came up with 8MB blocks rather than\n20MB.\n\nSomething important to note in Gavin Andresen's analysises of this issue\nis that he's using quite optimistic scenarios for how nodes are\nconnected to each other. For instance, assuming that connections between\nminers are direct is a very optimistic assumption that depends on a\npermissive, unregulated, environment where miners co-operate with each\nother - obviously that's easily subject to change! Better block\nbroadcasting logic helps this in the \"co-operation\" case, but there's\nnot much it can do in the worst-case.\n\n\nUnrelated: feel free to contact me directly if you have any questions\nre: the BIP66 upgrade; I hear you guys were planning on upgrading your\nmining nodes soon.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000db932d1cbd04a29d8e55989eda3f096d3ab8e8d95eb28e9\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/59f5b2ed/attachment.sig>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T12:51:04",
                "message_text_only": "On Sun, May 31, 2015 at 3:05 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> Yeah, I'm pretty surprised myself that Gavin never accepted the\n> compromises offered by others in this space for a slow growth solution\n>\n\nWhat compromise? I haven't seen a specific proposal that could be turned\ninto a pull request.\n\n\n\n\n> Something important to note in Gavin Andresen's analysises of this issue\n> is that he's using quite optimistic scenarios for how nodes are\n> connected to each other.\n\n\nNO I AM NOT.\n\nI simulated a variety of connectivities; see the .cfg files at\n  https://github.com/gavinandresen/bitcoin_miningsim\n\nThe results I give in the \"are bigger blocks better\" blog post are for\nWORST CASE connectivity (one dominant big miner, multiple little miners,\nbig miner connects to only 30% of little miners, but all the little miners\nconnected directly to each other).\n\n\n> For instance, assuming that connections between\n> miners are direct is a very optimistic assumption\n\n\nAgain, I did not simulate all miners directly connected to each other.\n\nI will note that miners are VERY HIGHLY connected today. It is in their\nbest interest to be highly connected to each other.\n\n\n> that depends on a\n> permissive, unregulated, environment where miners co-operate with each\n> other - obviously that's easily subject to change!\n\n\nReally? How is that easily subject to change? If it is easily subject to\nchange, do bigger blocks have any effect? Why are 1MB blocks not subject to\nchange?\n\nI talk about \"what if your government bans Bitcoin entirely\" here:\n   http://gavinandresen.ninja/big-blocks-and-tor\n\n... and the issues are essentially the same, independent of block size.\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/15190ca7/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2015-05-30T19:32:22",
                "message_text_only": "On 05/29/15 23:48, Gavin Andresen wrote:\n> On Fri, May 29, 2015 at 7:25 PM, Matt Corallo <bitcoin-list at bluematt.me\n> <mailto:bitcoin-list at bluematt.me>> wrote:\n> \n>     Sadly, this is very far from the whole story. The issue of miners\n>     optimizing for returns has been discussed several times during this\n>     discussion, and, sadly, miners who are geographically colocated who are\n>     optimizing for returns with a free-floating blocksize will optimize away\n>     50% of the network!\n> \n> \n> I must have missed that analysis-- link please?  Or summary of HOW they\n> will optimize away 50% of the network?\n> \n> Or are you assuming that 50% of the network is colocated... (which is a\n> potential problem independent of blocksize)\n\nIf, for example, the majority of miners are in China (they are), and\nthere is really poor connectivity in and out of China (there is) and a\nminer naively optimizes for profit, they will create blocks which are\nlarge and take a while to relay out of China. By simple trial-and-error\nan individual large miner might notice that when they create larger\nblocks which fork off miners in other parts of the world, they get more\nincome. Obviously forking off 50% of the network would be a rather\nextreme situation and assumes all kinds of simplified models, but it\nshows that the incentives here are very far from aligned, and your\nsimplified good-behavior models are very far from convincing.\n\n> \n>     >\n>     >     In addition, I'd expect to\n>     >     see analysis of how these systems perform in the worst-case, not just\n>     >     packet-loss-wise, but in the face of miners attempting to break the\n>     >     system.\n>     >\n>     >\n>     > See http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners for\n>     > analysis of \"but that means bigger miners can get an advantage\" argument.\n>     >\n>     > Executive summary: if little miners are stupid and produce huge blocks,\n>     > then yes, big miners have an advantage.\n> \n>     I'll talk about transaction fees in a second, but there are several\n>     problems with this already. As pointed out in the original mail, gfw has\n>     already been known to interfere with Bitcoin P2P traffic. So now by\n>     \"little\" miners, you mean any miner who is not located in mainland\n>     China? Whats worse, the disadvantage is symmetric - little miners are at\n>     a disadvantage when *anyone* mines a bigger block, and miners dont even\n>     have to be \"evil\" for this to happen - just optimize for profits.\n> \n> \n> But the disadvantage is tiny. And essentially zero if they connect to\n> your fast relay network (or anything like it).\n> \n\nThe disadvantage is small with 1MB blocks, but already non-zero. 20MB\nblocks are much, much worse (lots of things here dont scale linearly,\neven just transfer over a high-packet-loss-link). I mentioned this in my\noriginal email as something which doesnt make me comfortable with 20MB\nblocks, but something which needs simulation and study, and might\nactually be just fine!\n\n> \n>     > But they're not, so they won't.\n> \n>     I dont know what you're referring to with this. Are you claiming little\n>     miners today optimize for relay times and have good visibility into the\n>     Bitcoin network and calculate an optimal block size based on this (or\n>     would with a 20MB block size)?\n> \n> \n> Do you have another explanation for why miners choose to leave\n> fee-paying transactions in their mempool and create small blocks?\n\nDefaults? Dumb designs? Most miners just use the default 750K blocks, as\nfar as I can tell, other miners probably didnt see transactions relayed\nacross several hops or so, and a select few miners are doing crazy\nthings like making their blocks fit in a single packet to cross the gfw,\nbut that is probably overkill and not well-researched.\n\n>     > Until the block reward goes away, and assuming transaction fees become\n>     > an important source of revenue for miners.\n>     > I think it is too early to worry about that; see:\n>     >\n>     >    http://gavinandresen.ninja/when-the-block-reward-goes-away\n> \n>     You dont make any points here with which I can argue, but let me respond\n>     with the reason /I/ think it is a problem worth thinking a little bit\n>     about...If we increase the blocksize sufficiently such that transaction\n>     fees are not the way in which miners make their money\n> \n> \n> I'm not suggesting that we increase the blocksize sufficiently such that\n> transaction fees are not the way in which miners make their money.\n> \n> I'm suggesting the blocksize be increased to 20MB (and then doubled\n> every couple of years).\n\nDo you have convincing evidence that at 20MB miners will be able to\nbreak even on transaction fees for a long time? (The answer is no\nbecause no one has any idea how bitcoin transaction volumes are going to\nscale, period.)\n\n> And \"in which miners make their money\" is the wrong metric-- we want\n> enough mining so the network to be \"secure enough\" against double-spends.\n\nSure, do you have a value of hashpower which is \"secure enough\" (which\nis a whole other rabbit hole to go down...).\n\n> \n>     , then either\n>     miners are not being funded (ie hashpower has to drop to very little),\n>     or the only people mining/funding miners are large orgs who are\n>     \"running\" Bitcoin (ie the web wallets, payment processors, big\n>     merchants, and exchanges of the world). Sadly, this is no longer a\n>     decentralized Bitcoin and is, in fact, pretty much how the banking world\n>     works today.\n> \n> \n> Even if we end up in a world where only big companies can run full nodes\n> (and I am NOT NOT NOT NOT NOT proposing any such thing), there is a\n> difference-- you don't need permission to \"open up a bank\" on the\n> Bitcoin network.\n> \n\nOh? You mention at http://gavinandresen.ninja/bigger-blocks-another-way\nthat \"I struggle with wanting to stay true to Satoshi\u2019s original vision\nof Bitcoin as a system that scales up to Visa-level transaction volume\".\nThat is in direct contradiction.\n\n>     I'm not sure who, if anyone, claims Bitcoin is novel or interesting for\n>     any reason other than its decentralization properties, and, in a world\n>     which you are apparently proposing, the \"natural\" course of things is to\n>     very strongly centralize.\n> \n> \n>     >      * I'd very much like to see someone working on better scaling\n>     >     technology, both in terms of development and in terms of getting\n>     >     traction in the marketplace.\n>     >\n>     >\n>     > Ok. What does this have to do with the max block size?\n>     >\n>     > Are you arguing that work won't happen if the max block size increases?\n> \n>     Yes, I am arguing that by increasing the blocksize the incentives to\n>     actually make Bitcoin scale go away. Even if amazing technologies get\n>     built, no one will have any reason to use them.\n> \n> \n> Ok, I wrote about that here:\n> \n> http://gavinandresen.ninja/it-must-be-done-but-is-not-a-panacea\n> \n\n\"it is not a panacea\", but everyone in the community seems to be taking\nit as one. You've claimed many times that many of the big\nwebwallet/payment processors/etc have been coming to you and saying they\nneed bigger block sizes to continue operating. In reality, they dont, it\njust makes it easier.\n\nMatt"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-30T20:37:15",
                "message_text_only": "On Sat, May 30, 2015 at 3:32 PM, Matt Corallo <bitcoin-list at bluematt.me>\nwrote:\n\n> If, for example, the majority of miners are in China (they are), and\n> there is really poor connectivity in and out of China (there is) and a\n> miner naively optimizes for profit, they will create blocks which are\n> large and take a while to relay out of China. By simple trial-and-error\n> an individual large miner might notice that when they create larger\n> blocks which fork off miners in other parts of the world, they get more\n> income. Obviously forking off 50% of the network would be a rather\n> extreme situation and assumes all kinds of simplified models, but it\n> shows that the incentives here are very far from aligned, and your\n> simplified good-behavior models are very far from convincing.\n>\n\n\"good behavior\" models? I intentionally modeled what should be a worst-case.\n\nIf you have a specific network topology you want to model, please email me\ndetails and I'll see what worst case is. Or, even better, take my\nsimulation code and run it yourself (it's C++, easy to compile, easy to\nmodify if you think it is too simple).\n\nI get frustrated with all of the armchair \"but what if...\"\nhow-many-miners-can-dance-on-the-head-of-a-pin arguments.\n\n\n\n> >     I'll talk about transaction fees in a second, but there are several\n> >     problems with this already. As pointed out in the original mail, gfw\n> has\n> >     already been known to interfere with Bitcoin P2P traffic. So now by\n> >     \"little\" miners, you mean any miner who is not located in mainland\n> >     China? Whats worse, the disadvantage is symmetric - little miners\n> are at\n> >     a disadvantage when *anyone* mines a bigger block\n\n\nNo, they're not. They are only at a disadvantage when THEY mine bigger\nblocks.\n\nI guess I wasn't clear in the \"do bigger miners have an advantage\" blog\npost.\n\n\n> ... I mentioned this in my\n> original email as something which doesnt make me comfortable with 20MB\n> blocks, but something which needs simulation and study, and might\n> actually be just fine!\n>\n\nI spent last week doing simulation and study. Please, do your own\nsimulation and study if you don't trust my results. There are big\nfull-scale-bitcoin-network-simulations spinning up that should have results\nin a month or two, also, but there will ALWAYS be \"but we didn't think\nabout what if THIS happens\" scenarios that can require more simulation and\nstudy.\n\n\n>\n> > Do you have another explanation for why miners choose to leave\n> > fee-paying transactions in their mempool and create small blocks?\n>\n> Defaults? Dumb designs? Most miners just use the default 750K blocks, as\n> far as I can tell, other miners probably didnt see transactions relayed\n> across several hops or so, and a select few miners are doing crazy\n> things like making their blocks fit in a single packet to cross the gfw,\n> but that is probably overkill and not well-researched.\n>\n\nLast night's transaction volume test shows that most miners do just go\nalong with defaults:\n  http://bitcoincore.org/~gavin/sizes_358594.html\n\n> I'm not suggesting that we increase the blocksize sufficiently such that\n> > transaction fees are not the way in which miners make their money.\n> >\n> > I'm suggesting the blocksize be increased to 20MB (and then doubled\n> > every couple of years).\n>\n> Do you have convincing evidence that at 20MB miners will be able to\n> break even on transaction fees for a long time? (The answer is no\n> because no one has any idea how bitcoin transaction volumes are going to\n> scale, period.)\n>\n\n\nMining is a competitive business, the marginal miner will ALWAYS be going\nout of business.\n\nThat is completely independent of the block size, block subsidy, or\ntransaction fees.\n\nThe question is \"will there be enough fee+subsidy revenue to make it\nunprofitable for an attacker to buy or rent enough hashpower to\ndouble-spend.\"\n\nIt is obvious to me that bigger blocks make it more likely the answer to\nthat question is \"yes.\"\n\n\n\n>\n> > And \"in which miners make their money\" is the wrong metric-- we want\n> > enough mining so the network to be \"secure enough\" against double-spends.\n>\n> Sure, do you have a value of hashpower which is \"secure enough\" (which\n> is a whole other rabbit hole to go down...).\n>\n\nMike Hearn wrote about that just a couple days ago:\n  https://medium.com/@octskyward/hashing-7d04a887acc8\n(See \"How much is too much\" section)\n\n\n> > Even if we end up in a world where only big companies can run full nodes\n> > (and I am NOT NOT NOT NOT NOT proposing any such thing), there is a\n> > difference-- you don't need permission to \"open up a bank\" on the\n> > Bitcoin network.\n> >\n>\n> Oh? You mention at http://gavinandresen.ninja/bigger-blocks-another-way\n> that \"I struggle with wanting to stay true to Satoshi\u2019s original vision\n> of Bitcoin as a system that scales up to Visa-level transaction volume\".\n> That is in direct contradiction.\n>\n\nI have said repeatedly that if it was left completely up to me I would go\nback to Satoshi's original \"there is no consensus-level blocksize limit\".\n\n20MB is a compromise.\n\n > Ok, I wrote about that here:\n\n> >\n> > http://gavinandresen.ninja/it-must-be-done-but-is-not-a-panacea\n> >\n>\n> \"it is not a panacea\", but everyone in the community seems to be taking\n> it as one. You've claimed many times that many of the big\n> webwallet/payment processors/etc have been coming to you and saying they\n> need bigger block sizes to continue operating. In reality, they dont, it\n> just makes it easier\n>\n>\n... and now you're pissing me off. I have NEVER EVER said that they need\nbigger blocks to continue operating. Please stop being overly dramatic.\n\nThey believe that bigger blocks are better for Bitcoin.\n\nBrian Armstrong at Coinbase, in particular, said that smaller blocks drive\ncentralization towards services like Coinbase (\"look ma! No blockchain\ntransaction!\" <-- if you pay a Coinbase merchant from your Coinbase\nwallet), but he supports bigger blocks because more transactions on our\nexisting decentralized network is better.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/b6824987/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-31T14:46:58",
                "message_text_only": "On May 30, 2015 10:38 PM, \"Gavin Andresen\" <gavinandresen at gmail.com> wrote:\n>\n> Mining is a competitive business, the marginal miner will ALWAYS be going\nout of business.\n>\n> That is completely independent of the block size, block subsidy, or\ntransaction fees.\n\nNo, the later determines who can be profitable.\nHere's a thought experiment:\n\nSubsidy is gone, all the block reward comes from fees.\nMiner A has great connectivity and mines 20 MB blocks, with an average of\n20 btc per block.\nMiner B has a connectivity such that 2 MB blocks puts it on a reasonable\norphan rate, so it gets an average of 2 btc per block mined.\nBut the difficulty is the same for all and it can rise up to miner A\nbreaking even after energy costs.\nWill miner B be profitable with this setup? The answer is no and miner B\nwill just go out of business. In that sense too, bigger blocks mean more\nmining centralization.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/c2fce858/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T14:49:02",
                "message_text_only": "On Sun, May 31, 2015 at 10:46 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> Here's a thought experiment:\n>\n> Subsidy is gone, all the block reward comes from fees.\n>\nI wrote about long-term hypotheticals and why I think it is a big mistake\nto waste time worrying about them here:\n   http://gavinandresen.ninja/when-the-block-reward-goes-away\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/0360b568/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-31T14:59:45",
                "message_text_only": "Whatever...let's use the current subsidies, the same argument applies, it's\njust 20 + 25 = 45 btc per block for miner B vs 27 btc for miner B.\nMiner B would still go out of business, bigger blocks still mean more\nmining and validation centralization. The question is how far I we willing\nto go with this \"scaling by sacrificing decentralization\", but the answer\ncan't be \"that's to far away in the future to worry about it, right now as\nfar as we think we can using orphan rate as the only criterion\".\nOn May 31, 2015 4:49 PM, \"Gavin Andresen\" <gavinandresen at gmail.com> wrote:\n\n> On Sun, May 31, 2015 at 10:46 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>> Here's a thought experiment:\n>>\n>> Subsidy is gone, all the block reward comes from fees.\n>>\n> I wrote about long-term hypotheticals and why I think it is a big mistake\n> to waste time worrying about them here:\n>    http://gavinandresen.ninja/when-the-block-reward-goes-away\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/b880d9d2/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T15:08:12",
                "message_text_only": "On Sun, May 31, 2015 at 10:59 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> Whatever...let's use the current subsidies, the same argument applies,\n> it's just 20 + 25 = 45 btc per block for miner B vs 27 btc for miner B.\n> Miner B would still go out of business, bigger blocks still mean more\n> mining and validation centralization\n>\nSorry, but that's ridiculous.\n\nIf Miner B is leaving 18BTC per block on the table because they have bad\nconnectivity, then they need to pay for better connectivity.\n\nIf you are arguing \"I should be able to mine on a 56K modem connection from\nthe middle of the Sahara\" then we're going to have to agree to disagree.\n\nSo: what is your specific proposal for minimum requirements for\nconnectivity to run a full node? The 20MB number comes from estimating\ncosts to run a full node, and as my back-and-forth to Chang Wung shows, the\ncosts are not excessive.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/08d60b07/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-31T15:45:01",
                "message_text_only": "On May 31, 2015 5:08 PM, \"Gavin Andresen\" <gavinandresen at gmail.com> wrote:\n>\n> On Sun, May 31, 2015 at 10:59 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>\n>> Whatever...let's use the current subsidies, the same argument applies,\nit's just 20 + 25 = 45 btc per block for miner B vs 27 btc for miner B.\n>> Miner B would still go out of business, bigger blocks still mean more\nmining and validation centralization\n>\n> Sorry, but that's ridiculous.\n>\n> If Miner B is leaving 18BTC per block on the table because they have bad\nconnectivity, then they need to pay for better connectivity.\n\nWell, I was assuming they just can't upgrade their connection (without\nmoving thei operations to another place). Maybe that assumption is\nridiculous as well.\n\n> If you are arguing \"I should be able to mine on a 56K modem connection\nfrom the middle of the Sahara\" then we're going to have to agree to\ndisagree.\n\nNo, I'm not suggesting that.\n\n> So: what is your specific proposal for minimum requirements for\nconnectivity to run a full node? The 20MB number comes from estimating\ncosts to run a full node, and as my back-and-forth to Chang Wung shows, the\ncosts are not excessive.\n\nWell, you were I think assuming a new desktop connecting from somewhere in\nthe US. I would be more confortable with an eee pc from a hotel in India,\nfor example. But yeah, targeting some concrete minimum specs seems like the\nright approach for deciding \"how far to go when increasing centralization\".\n\nBut \"hitting the limit will be chaos\" seems to imply that completely\nremoving the consensus maximum blocksize is the only logical solution. What\nhappens when we hit the limit next time? When do we stop kicking the can\ndown the road? When do we voluntarily get that \"chaos\"?\nAgain, \"that's too far away in the future to worry about it\" is not a very\nconving answer to me.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/009b8b53/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-30T23:18:48",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/7028370e/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-31T00:32:34",
                "message_text_only": ">\n> Stop trying to dictate block growth limits.  Block size will be determined\n> by competition between miners and availability of transactions, not through\n> hard-coded limits.\n>\nDo you even game theory, bro? It doesn't work that way.\n\nMike Hearn described the problem in this article:\nhttps://medium.com/@octskyward/hashing-7d04a887acc8\n\nBut the solution he's proposing is ridiculously bad and unsound: he expects\nbusiness owners to donate large sums of money towards mining. If it comes\nto this, what sane business owner will donate, say, 100 BTC to miners\ninstead of seeking some alternatives? Proof-of-stake coins are already\nthere. I'm well aware of theoretical issues with PoS security, but those\ntheoretical issues aren't as bad as donation-funded cryptocurrency security.\n\nBut you know what works? Mining fees + block size limit.\nUsers and merchants are interested in their transactions being confirmed,\nbut block size limit won't allow it to turn into a race to bottom.\nThis is actually game-theoretically sound.\n\n\n>   I see now the temporary 1MB limit was a mistake.  It should have gone in\n> as a dynamic limit that scales with average block size.\n>\nThis means that miners will control it, and miners couldn't care less about\nthings like decentralization and about problems of ordinary users. This\nmeans that in this scenario Bitcoin will be 100% controlled by few huge-ass\nmining operations.\n\nPossibly a single operation. We already saw GHASH.IO using 51% of total\nhashpower. Is that what you want?\n\nMiners are NOT benevolent. This was already demonstrated. They are greedy.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/695a6691/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Block Size Increase Requirements",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Arkady",
                "Brian Hoffman",
                "Raystonn",
                "Mike Hearn",
                "Peter Todd",
                "Stephen",
                "Tier Nolan",
                "Pindar Wong",
                "Jorge Tim\u00f3n",
                "Alex Mizrahi",
                "Matt Corallo",
                "Gavin Andresen",
                "Chun Wang",
                "gb",
                "Joseph Poon"
            ],
            "messages_count": 31,
            "total_messages_chars_count": 81692
        }
    },
    {
        "title": "[Bitcoin-development] Solution for Block Size Increase",
        "thread_messages": [
            {
                "author": "Nicolas DORIER",
                "date": "2015-05-07T23:14:22",
                "message_text_only": "Executive Summary:\n\nI explain the objectives that we should aim to reach agreement without\ndrama, controversy, and relief the core devs from the central banker role.\n(As Jeff Garzik pointed out)\nKnowing the objectives, I propose a solution based on the objectives that\ncan be agreed on tomorrow, would permanently fix the block size problem\nwithout controversy and would be immediately applicable.\n\nThe objectives:\n\nThere is consensus on the fact that nobody wants the core developers to be\nseen as central bankers.\nThere is also consensus that more decentralization is better than less.\n(assuming there is no cost to it)\n\nThis means you should reject all arguments based on economical, political\nand ideological principles about what Bitcoin should become. This includes:\n\n1) Whether Bitcoin should be storage of value or suitable for coffee\ntransaction,\n2) Whether we need a fee market, block scarcity, and how much of it,\n3) Whether we need to periodically increase block size via some voodoo\nformula which speculate on future bandwidth and cost of storage,\n\nTaking decisions based on such reasons is what central bankers do, and you\ndon\u2019t want to be bankers. This follow that decisions should be taken only\nfor technical and decentralization considerations. (more about\ndecentralization after)\n\nScarcity will evolve without you taking any decisions about it, for the\nonly reason that storage and bandwidth is not free, nor a transaction,\nthanks to increased propagation time.\nThis backed in scarcity will evolve automatically as storage, bandwidth,\nencoding, evolve without anybody taking any decision, nor making any\nspeculation on the future.\n\nSadly, deciding how much decentralization should be in the system by\ntweaking the block size limit is also an economic decision that should not\nhave its place between the core devs. This follow :\n\n4) Core devs should not decide about the amount of suitable\ndecentralization by tweaking block size limit,\n\nStill, removing the limit altogether is a no-no, what would happen if a\nblock of 100 GB is created? Immediately the network would be decentralized,\nnot only for miners but also for bitcoin service providers. Also, core devs\nmight have technical consideration on bitcoin core which impose a temporary\nlimit until the bug resolved.\n\nThe solution:\n\nSo here is a proposal that address all my points, and, I think, would get a\nreasonable consensus. It can be published tomorrow without any controversy,\nwould be agreed in one year, and can be safely reiterated every year.\nDevelopers will also not have to play politics nor central banker. (well,\nit sounds to good to be true, I waiting for being wrong)\n\nThe solution is to use block voting. For each block, a miner gives the size\nof the block he would like to have at the next deadline (for example, 30\nmay 2015). The rational choice for them is just enough to clear the memory\npool, maybe a little less if he believes fee pressure is beneficial for\nhim, maybe a little more if he believes he should leave some room for\nincreased use.\nAt the deadline, we take the median of the votes and implement it as a new\nblock size limit. Reiterate for the next year.\n\nObjectives reached:\n\n\n   - No central banking decisions on devs shoulder,\n   - Votes can start tomorrow,\n   - Implementation has only to be ready in one year, (no kick-in-the-can)\n   - Will increase as demand is growing,\n   - Will increase as network capacity and storage is growing,\n   - Bitcoin becomes what miners want, not what core devs and politician\n   wants,\n   - Implementation reasonably easy,\n   - Will get miner consensus, no impact on existing bitcoin services,\n\n\nUnknown:\n\n   - Effect on bitcoin core stability (core devs might have a valid\n   technical reason to impose a limit)\n   - Maybe a better statistical function is possible\n\nAdditional input for the debate:\n\nSome people were debating whether miners are altruist or act rationally. We\nshould always expect them to act rationally, but we should not forget the\npeculiarity of TCP backoff game: While it is in the best interest of\nplayers to NOT reemit TCP packet with a backoff if the ACK is not received,\neverybody does it. (Because of the fallacy that changing a TCP\nimplementation is costless)\n\nOften, when we think a real life situation is a prisoner dilemma problem,\nit turns out that the incentives where just incorrectly modeled.\n\nCore devs, thanks for all your work, but please step out of the banker's\nrole and focus on where you are the best, I speak as an entrepreneur that\ndoesn't want decisions about bitcoin to be taken by who has the biggest.\nIf the decision of the hard limit is taken for other than purely technical\ndecisions, ie, for the maximization of whatever metric, it will clearly put\nyou in banker's shoes. As an entrepreneur, I have other things to speculate\nthan who gets the biggest gun in the core team.\nPlease consider my solution,\n\nNicolas Dorier,\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/6a845fc6/attachment.html>"
            },
            {
                "author": "Thy Shizzle",
                "date": "2015-05-08T04:12:15",
                "message_text_only": "Nicolas, can you think if there would be a problem with allowing blocks to be created faster instead of increasing block size? So say if difficulty was reduced to allow block creation every 2 minutes instead of 10 minutes, can you think of any bad outcome from this (I know this is different to what you are saying) but I'm thinking if we allow 1mb blocks to be built faster, that way transactions are processed quicker  thus gaining a higher tps rate, i'd think no hard fork need occur right?\n\nIs there any downsides that you can see? Obviously miners need yo update, but I mean if they don't it just means they potentially take too long to make blocks and thus loose out in reward so there is the incentive for them to update to the easier difficulty, while still allowing blocks done on the harder difficulty for backwards compatibility.\n\nThoughts?\n________________________________\nFrom: Nicolas DORIER<mailto:nicolas.dorier at gmail.com>\nSent: \u200e8/\u200e05/\u200e2015 9:17 AM\nTo: bitcoin-development at lists.sourceforge.net<mailto:bitcoin-development at lists.sourceforge.net>\nSubject: [Bitcoin-development] Solution for Block Size Increase\n\nExecutive Summary:\n\nI explain the objectives that we should aim to reach agreement without\ndrama, controversy, and relief the core devs from the central banker role.\n(As Jeff Garzik pointed out)\nKnowing the objectives, I propose a solution based on the objectives that\ncan be agreed on tomorrow, would permanently fix the block size problem\nwithout controversy and would be immediately applicable.\n\nThe objectives:\n\nThere is consensus on the fact that nobody wants the core developers to be\nseen as central bankers.\nThere is also consensus that more decentralization is better than less.\n(assuming there is no cost to it)\n\nThis means you should reject all arguments based on economical, political\nand ideological principles about what Bitcoin should become. This includes:\n\n1) Whether Bitcoin should be storage of value or suitable for coffee\ntransaction,\n2) Whether we need a fee market, block scarcity, and how much of it,\n3) Whether we need to periodically increase block size via some voodoo\nformula which speculate on future bandwidth and cost of storage,\n\nTaking decisions based on such reasons is what central bankers do, and you\ndon\u2019t want to be bankers. This follow that decisions should be taken only\nfor technical and decentralization considerations. (more about\ndecentralization after)\n\nScarcity will evolve without you taking any decisions about it, for the\nonly reason that storage and bandwidth is not free, nor a transaction,\nthanks to increased propagation time.\nThis backed in scarcity will evolve automatically as storage, bandwidth,\nencoding, evolve without anybody taking any decision, nor making any\nspeculation on the future.\n\nSadly, deciding how much decentralization should be in the system by\ntweaking the block size limit is also an economic decision that should not\nhave its place between the core devs. This follow :\n\n4) Core devs should not decide about the amount of suitable\ndecentralization by tweaking block size limit,\n\nStill, removing the limit altogether is a no-no, what would happen if a\nblock of 100 GB is created? Immediately the network would be decentralized,\nnot only for miners but also for bitcoin service providers. Also, core devs\nmight have technical consideration on bitcoin core which impose a temporary\nlimit until the bug resolved.\n\nThe solution:\n\nSo here is a proposal that address all my points, and, I think, would get a\nreasonable consensus. It can be published tomorrow without any controversy,\nwould be agreed in one year, and can be safely reiterated every year.\nDevelopers will also not have to play politics nor central banker. (well,\nit sounds to good to be true, I waiting for being wrong)\n\nThe solution is to use block voting. For each block, a miner gives the size\nof the block he would like to have at the next deadline (for example, 30\nmay 2015). The rational choice for them is just enough to clear the memory\npool, maybe a little less if he believes fee pressure is beneficial for\nhim, maybe a little more if he believes he should leave some room for\nincreased use.\nAt the deadline, we take the median of the votes and implement it as a new\nblock size limit. Reiterate for the next year.\n\nObjectives reached:\n\n\n   - No central banking decisions on devs shoulder,\n   - Votes can start tomorrow,\n   - Implementation has only to be ready in one year, (no kick-in-the-can)\n   - Will increase as demand is growing,\n   - Will increase as network capacity and storage is growing,\n   - Bitcoin becomes what miners want, not what core devs and politician\n   wants,\n   - Implementation reasonably easy,\n   - Will get miner consensus, no impact on existing bitcoin services,\n\n\nUnknown:\n\n   - Effect on bitcoin core stability (core devs might have a valid\n   technical reason to impose a limit)\n   - Maybe a better statistical function is possible\n\nAdditional input for the debate:\n\nSome people were debating whether miners are altruist or act rationally. We\nshould always expect them to act rationally, but we should not forget the\npeculiarity of TCP backoff game: While it is in the best interest of\nplayers to NOT reemit TCP packet with a backoff if the ACK is not received,\neverybody does it. (Because of the fallacy that changing a TCP\nimplementation is costless)\n\nOften, when we think a real life situation is a prisoner dilemma problem,\nit turns out that the incentives where just incorrectly modeled.\n\nCore devs, thanks for all your work, but please step out of the banker's\nrole and focus on where you are the best, I speak as an entrepreneur that\ndoesn't want decisions about bitcoin to be taken by who has the biggest.\nIf the decision of the hard limit is taken for other than purely technical\ndecisions, ie, for the maximization of whatever metric, it will clearly put\nyou in banker's shoes. As an entrepreneur, I have other things to speculate\nthan who gets the biggest gun in the core team.\nPlease consider my solution,\n\nNicolas Dorier,\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/f4a6cb22/attachment.html>\n-------------- next part --------------\n------------------------------------------------------------------------------\nOne dashboard for servers and applications across Physical-Virtual-Cloud \nWidest out-of-the-box monitoring support with 50+ applications\nPerformance metrics, stats and reports that give you Actionable Insights\nDeep dive visibility with transaction tracing using APM Insight.\nhttp://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n-------------- next part --------------\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            }
        ],
        "thread_summary": {
            "title": "Solution for Block Size Increase",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Thy Shizzle",
                "Nicolas DORIER"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 11999
        }
    },
    {
        "title": "[Bitcoin-development] Assurance contracts to fund the network with OP_CHECKLOCKTIMEVERIFY",
        "thread_messages": [
            {
                "author": "Tier Nolan",
                "date": "2015-05-07T23:32:12",
                "message_text_only": "One of the suggestions to avoid the problem of fees going to zero is\nassurance contracts.  This lets users (perhaps large merchants or\nexchanges) pay to support the network.  If insufficient people pay for the\ncontract, then it fails.\n\nMike Hearn suggests one way of achieving it, but it doesn't actually create\nan assurance contract.  Miners can exploit the system to convert the\npledges into donations.\n\nhttps://bitcointalk.org/index.php?topic=157141.msg1821770#msg1821770\n\nConsider a situation in the future where the minting fee has dropped to\nalmost zero.  A merchant wants to cause block number 1 million to\neffectively have a minting fee of 50BTC.\n\nHe creates a transaction with one input (0.1BTC) and one output (50BTC) and\nsigns it using SIGHASH_ANYONE_CAN_PAY.  The output pays to OP_TRUE.  This\nmeans that anyone can spend it.  The miner who includes the transaction\nwill send it to an address he controls (or pay to fee).  The transaction\nhas a locktime of 1 million, so that it cannot be included before that\npoint.\n\nThis transaction cannot be included in a block, since the inputs are lower\nthan the outputs.  The SIGHASH_ANYONE_CAN_PAY field mean that others can\npledge additional funds.  They add more input to add more money and the\nsame sighash.\n\nThere would need to be some kind of notice boeard system for these pledges,\nbut if enough pledge, then a valid transaction can be created.  It is in\nminer's interests to maintain such a notice board.\n\nThe problem is that it counts as a pure donation.  Even if only 10BTC has\nbeen pledged, a miner can just add 40BTC of his own money and finish the\ntransaction.  He nets the 10BTC of the pledges if he wins the block.  If he\nloses, nobody sees his 40BTC transaction.  The only risk is if his block is\norphaned and somehow the miner who mines the winning block gets his 40BTC\ntransaction into his block.\n\nThe assurance contract was supposed to mean \"If the effective minting fee\nfor block 1 million is 50 BTC, then I will pay 0.1BTC\".  By adding his\n40BTC to the transaction the miner converts it to a pure donation.\n\nThe key point is that *other* miners don't get 50BTC reward if they find\nthe block, so it doesn't push up the total hashing power being committed to\nthe blockchain, that a 50BTC minting fee would achieve.  This is the whole\npoint of the assurance contract.\n\nOP_CHECKLOCKTIMEVERIFY could be used to solve the problem.\n\nInstead of paying to OP_TRUE, the transaction should pay 50 BTC to \"<1\nmillion> OP_CHECKLOCKTIMEVERIFY OP_TRUE\" and 0.01BTC to \"OP_TRUE\".\n\nThis means that the transaction could be included into a block well in\nadvance of the 1 million block point.  Once block 1 million arrives, any\nminer would be able to spend the 50 BTC.  The 0.01BTC is the fee for the\nblock the transaction is included in.\n\nIf the contract hasn't been included in a block well in advance, pledgers\nwould be recommended to spend their pledged input,\n\nIt can be used to pledge to many blocks at once.  The transaction could pay\nout to lots of 50BTC outputs but with the locktime increasing by for each\noutput.\n\nFor high value transactions, it isn't just the POW of the next block that\nmatters but all the blocks that are built on top of it.\n\nA pledger might want to say \"I will pay 1BTC if the next 100 blocks all\nhave at least an effective minting fee of 50BTC\"\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/fad4c3c9/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-08T09:49:21",
                "message_text_only": "Looks like a neat solution, Tier.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/3f7a6fd9/attachment.html>"
            },
            {
                "author": "Benjamin",
                "date": "2015-05-08T10:01:31",
                "message_text_only": "Interesting.\n\n1. How do you know who was first? If one node can figure out where\nmore transactions happen he can gain an advantage by being closer to\nhim. Mining would not be fair.\n\n2. \"A merchant wants to cause block number 1 million to effectively\nhave a minting fee of 50BTC.\" - why should he do that? That's the\nentire tragedy of the commons problem, no?\n\nOn Fri, May 8, 2015 at 11:49 AM, Mike Hearn <mike at plan99.net> wrote:\n> Looks like a neat solution, Tier.\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-08T14:15:05",
                "message_text_only": "Just to clarify the process.\n\nPledgers create transactions using the following template and broadcast\nthem.  The p2p protocol could be modified to allow this, or it could be a\nseparate system.\n\n\n*Input: 0.01 BTC*\n\n\n*Signed with SIGHASH_ANYONE_CAN_PAY*\n\n*Output 50BTC*\n\n*Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE*\n\n\n*Output 0.01BTC*\n\n*Paid to OP_TRUE*\nThis transaction is invalid, since the inputs don't pay for the output.\nThe advantage of the sighash \"anyone can pay\" field is that other people\ncan add additional inputs without making the signature invalid.  Normally,\nany change to the transaction would make a signature invalid.\n\nEventually, enough other users have added pledges and a valid transaction\ncan be broadcast.\n\n\n*Input: 0.01 BTC*\n\n*Signed with SIGHASH_ANYONE_CAN_PAY*\n\n*Input: 1.2 BTCSigned with SIGHASH_ANYONE_CAN_PAY*\n\n\n*Input: 5 BTCSigned with SIGHASH_ANYONE_CAN_PAY*\n\n*<etc>*\n\n\n\n\n\n*Input: 1.3 BTCSigned with SIGHASH_ANYONE_CAN_PAYOutput 50BTC*\n*Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE*\n\n*Output 0.01BTC**Paid to OP_TRUE*\n\nThis transaction can be submitted to the main network.  Once it is included\ninto the blockchain, it is locked in.\n\nIn this example, it might be included in block 999,500.  The 0.01BTC output\n(and any excess over 50BTC) can be collected by the block 999,500 miner.\n\nThe OP_CHECKLOCKTIMEVERIFY opcode means that the 50BTC output cannot be\nspent until block 1 million.  Once block 1 million arrives, the output is\ncompletely unprotected.  This means that the miner who mines block 1\nmillion can simply take it, by including his own transaction that sends it\nto an address he controls.  It would be irrational to include somebody\nelse's transaction which spent it.\n\nIf by block 999,900, the transaction hasn't been completed (due to not\nenough pledgers), the pledgers can spend the coin(s) that they were going\nto use for their pledge.  This invalidates those inputs and effectively\nwithdraws from the pledge.\n\nOn Fri, May 8, 2015 at 11:01 AM, Benjamin <benjamin.l.cordes at gmail.com>\nwrote:\n\n> 2. \"A merchant wants to cause block number 1 million to effectively\n> have a minting fee of 50BTC.\" - why should he do that? That's the\n> entire tragedy of the commons problem, no?\n>\n\nNo, the pledger is saying that he will only pay 0.01BTC if the miner gets a\nreward of 50BTC.\n\nImagine a group of 1000 people who want to make a donation of 50BTC to\nsomething.  They all way that they will donate 0.05BTC, but only if\neveryone else donates.\n\nIt still isn't perfect.  Everyone has an incentive to wait until the last\nminute to pledge.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/9fcdc606/attachment.html>"
            },
            {
                "author": "Benjamin",
                "date": "2015-05-08T14:54:05",
                "message_text_only": ">> Imagine a group of 1000 people who want to make a donation of 50BTC to something.  They all way that they will donate 0.05BTC, but only if everyone else donates.\n\nIt still isn't perfect.  Everyone has an incentive to wait until the\nlast minute to pledge. <<\n\nAC does not solve the problem. AC works if people gain directly from\nthe payment. Imagine a group of people paying tax - nobody gains from\npaying it. You have to actually need to enforce negative outcomes to\nenable it (jail for tax fraud). Hence in Bitcoin we have the enforced\nsubsidy. AFAIK the problem of how to incentivize transaction\nverification without subsidy is unsolved. Who determines a fair price?\nPeople around here should study more economics, game theory, etc.\ninstead of debating low level encodings all the time.\n\nOn Fri, May 8, 2015 at 4:15 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n> Just to clarify the process.\n>\n> Pledgers create transactions using the following template and broadcast\n> them.  The p2p protocol could be modified to allow this, or it could be a\n> separate system.\n>\n> Input: 0.01 BTC\n> Signed with SIGHASH_ANYONE_CAN_PAY\n>\n> Output 50BTC\n> Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE\n>\n> Output 0.01BTC\n> Paid to OP_TRUE\n>\n> This transaction is invalid, since the inputs don't pay for the output.  The\n> advantage of the sighash \"anyone can pay\" field is that other people can add\n> additional inputs without making the signature invalid.  Normally, any\n> change to the transaction would make a signature invalid.\n>\n> Eventually, enough other users have added pledges and a valid transaction\n> can be broadcast.\n>\n> Input: 0.01 BTC\n> Signed with SIGHASH_ANYONE_CAN_PAY\n>\n> Input: 1.2 BTC\n> Signed with SIGHASH_ANYONE_CAN_PAY\n>\n> Input: 5 BTC\n> Signed with SIGHASH_ANYONE_CAN_PAY\n>\n> <etc>\n>\n> Input: 1.3 BTC\n> Signed with SIGHASH_ANYONE_CAN_PAY\n>\n> Output 50BTC\n> Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE\n>\n> Output 0.01BTC\n> Paid to OP_TRUE\n>\n> This transaction can be submitted to the main network.  Once it is included\n> into the blockchain, it is locked in.\n>\n> In this example, it might be included in block 999,500.  The 0.01BTC output\n> (and any excess over 50BTC) can be collected by the block 999,500 miner.\n>\n> The OP_CHECKLOCKTIMEVERIFY opcode means that the 50BTC output cannot be\n> spent until block 1 million.  Once block 1 million arrives, the output is\n> completely unprotected.  This means that the miner who mines block 1 million\n> can simply take it, by including his own transaction that sends it to an\n> address he controls.  It would be irrational to include somebody else's\n> transaction which spent it.\n>\n> If by block 999,900, the transaction hasn't been completed (due to not\n> enough pledgers), the pledgers can spend the coin(s) that they were going to\n> use for their pledge.  This invalidates those inputs and effectively\n> withdraws from the pledge.\n>\n> On Fri, May 8, 2015 at 11:01 AM, Benjamin <benjamin.l.cordes at gmail.com>\n> wrote:\n>>\n>> 2. \"A merchant wants to cause block number 1 million to effectively\n>> have a minting fee of 50BTC.\" - why should he do that? That's the\n>> entire tragedy of the commons problem, no?\n>\n>\n> No, the pledger is saying that he will only pay 0.01BTC if the miner gets a\n> reward of 50BTC.\n>\n> Imagine a group of 1000 people who want to make a donation of 50BTC to\n> something.  They all way that they will donate 0.05BTC, but only if everyone\n> else donates.\n>\n> It still isn't perfect.  Everyone has an incentive to wait until the last\n> minute to pledge."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-08T14:56:35",
                "message_text_only": "On Fri, May 8, 2015 at 3:54 PM, Benjamin <benjamin.l.cordes at gmail.com>\nwrote:\n\n> AC does not solve the problem. AC works if people gain directly from\n> the payment.\n\n\nNot necessarily.\n\n\n\n\n\n\n\n\n\n> Imagine a group of people paying tax - nobody gains from\n> paying it. You have to actually need to enforce negative outcomes to\n> enable it (jail for tax fraud). Hence in Bitcoin we have the enforced\n> subsidy. AFAIK the problem of how to incentivize transaction\n> verification without subsidy is unsolved. Who determines a fair price?\n> People around here should study more economics, game theory, etc.\n> instead of debating low level encodings all the time.\n>\n> On Fri, May 8, 2015 at 4:15 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n> > Just to clarify the process.\n> >\n> > Pledgers create transactions using the following template and broadcast\n> > them.  The p2p protocol could be modified to allow this, or it could be a\n> > separate system.\n> >\n> > Input: 0.01 BTC\n> > Signed with SIGHASH_ANYONE_CAN_PAY\n> >\n> > Output 50BTC\n> > Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE\n> >\n> > Output 0.01BTC\n> > Paid to OP_TRUE\n> >\n> > This transaction is invalid, since the inputs don't pay for the output.\n> The\n> > advantage of the sighash \"anyone can pay\" field is that other people can\n> add\n> > additional inputs without making the signature invalid.  Normally, any\n> > change to the transaction would make a signature invalid.\n> >\n> > Eventually, enough other users have added pledges and a valid transaction\n> > can be broadcast.\n> >\n> > Input: 0.01 BTC\n> > Signed with SIGHASH_ANYONE_CAN_PAY\n> >\n> > Input: 1.2 BTC\n> > Signed with SIGHASH_ANYONE_CAN_PAY\n> >\n> > Input: 5 BTC\n> > Signed with SIGHASH_ANYONE_CAN_PAY\n> >\n> > <etc>\n> >\n> > Input: 1.3 BTC\n> > Signed with SIGHASH_ANYONE_CAN_PAY\n> >\n> > Output 50BTC\n> > Paid to: <1 million> OP_CHECKLOCKTIMEVERIFY OP_TRUE\n> >\n> > Output 0.01BTC\n> > Paid to OP_TRUE\n> >\n> > This transaction can be submitted to the main network.  Once it is\n> included\n> > into the blockchain, it is locked in.\n> >\n> > In this example, it might be included in block 999,500.  The 0.01BTC\n> output\n> > (and any excess over 50BTC) can be collected by the block 999,500 miner.\n> >\n> > The OP_CHECKLOCKTIMEVERIFY opcode means that the 50BTC output cannot be\n> > spent until block 1 million.  Once block 1 million arrives, the output is\n> > completely unprotected.  This means that the miner who mines block 1\n> million\n> > can simply take it, by including his own transaction that sends it to an\n> > address he controls.  It would be irrational to include somebody else's\n> > transaction which spent it.\n> >\n> > If by block 999,900, the transaction hasn't been completed (due to not\n> > enough pledgers), the pledgers can spend the coin(s) that they were\n> going to\n> > use for their pledge.  This invalidates those inputs and effectively\n> > withdraws from the pledge.\n> >\n> > On Fri, May 8, 2015 at 11:01 AM, Benjamin <benjamin.l.cordes at gmail.com>\n> > wrote:\n> >>\n> >> 2. \"A merchant wants to cause block number 1 million to effectively\n> >> have a minting fee of 50BTC.\" - why should he do that? That's the\n> >> entire tragedy of the commons problem, no?\n> >\n> >\n> > No, the pledger is saying that he will only pay 0.01BTC if the miner\n> gets a\n> > reward of 50BTC.\n> >\n> > Imagine a group of 1000 people who want to make a donation of 50BTC to\n> > something.  They all way that they will donate 0.05BTC, but only if\n> everyone\n> > else donates.\n> >\n> > It still isn't perfect.  Everyone has an incentive to wait until the last\n> > minute to pledge.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/b9fa8756/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-08T15:03:28",
                "message_text_only": "Sorry for the spam of the last mail.  I hit send by accident.\n\nAssurance contracts are better than simple donations.\n\nDonating to a project means that you always end up losing the money but the\nproject might still not get funded.\n\nAn assurance contract is like Kickstarter, you only get your CC charged if\nthe project is fully funded.\n\nThere is lower risk, either you get your money back or the project is\nfunded.  It might still be worth risking it and hoping it gets funded.\n\nKickstarter does have pledge rewards to reward pledgers.  That helps with\ncreating the momentum to encourage people to pledge.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/569aed2a/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-08T10:00:37",
                "message_text_only": "That reminds me - I need to integrate the patch that automatically sweeps\nanyone-can-pay transactions for a miner.\n\n\nOn Thu, May 7, 2015 at 7:32 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n> One of the suggestions to avoid the problem of fees going to zero is\n> assurance contracts.  This lets users (perhaps large merchants or\n> exchanges) pay to support the network.  If insufficient people pay for the\n> contract, then it fails.\n>\n> Mike Hearn suggests one way of achieving it, but it doesn't actually\n> create an assurance contract.  Miners can exploit the system to convert the\n> pledges into donations.\n>\n> https://bitcointalk.org/index.php?topic=157141.msg1821770#msg1821770\n>\n> Consider a situation in the future where the minting fee has dropped to\n> almost zero.  A merchant wants to cause block number 1 million to\n> effectively have a minting fee of 50BTC.\n>\n> He creates a transaction with one input (0.1BTC) and one output (50BTC)\n> and signs it using SIGHASH_ANYONE_CAN_PAY.  The output pays to OP_TRUE.\n> This means that anyone can spend it.  The miner who includes the\n> transaction will send it to an address he controls (or pay to fee).  The\n> transaction has a locktime of 1 million, so that it cannot be included\n> before that point.\n>\n> This transaction cannot be included in a block, since the inputs are lower\n> than the outputs.  The SIGHASH_ANYONE_CAN_PAY field mean that others can\n> pledge additional funds.  They add more input to add more money and the\n> same sighash.\n>\n> There would need to be some kind of notice boeard system for these\n> pledges, but if enough pledge, then a valid transaction can be created.  It\n> is in miner's interests to maintain such a notice board.\n>\n> The problem is that it counts as a pure donation.  Even if only 10BTC has\n> been pledged, a miner can just add 40BTC of his own money and finish the\n> transaction.  He nets the 10BTC of the pledges if he wins the block.  If he\n> loses, nobody sees his 40BTC transaction.  The only risk is if his block is\n> orphaned and somehow the miner who mines the winning block gets his 40BTC\n> transaction into his block.\n>\n> The assurance contract was supposed to mean \"If the effective minting fee\n> for block 1 million is 50 BTC, then I will pay 0.1BTC\".  By adding his\n> 40BTC to the transaction the miner converts it to a pure donation.\n>\n> The key point is that *other* miners don't get 50BTC reward if they find\n> the block, so it doesn't push up the total hashing power being committed to\n> the blockchain, that a 50BTC minting fee would achieve.  This is the whole\n> point of the assurance contract.\n>\n> OP_CHECKLOCKTIMEVERIFY could be used to solve the problem.\n>\n> Instead of paying to OP_TRUE, the transaction should pay 50 BTC to \"<1\n> million> OP_CHECKLOCKTIMEVERIFY OP_TRUE\" and 0.01BTC to \"OP_TRUE\".\n>\n> This means that the transaction could be included into a block well in\n> advance of the 1 million block point.  Once block 1 million arrives, any\n> miner would be able to spend the 50 BTC.  The 0.01BTC is the fee for the\n> block the transaction is included in.\n>\n> If the contract hasn't been included in a block well in advance, pledgers\n> would be recommended to spend their pledged input,\n>\n> It can be used to pledge to many blocks at once.  The transaction could\n> pay out to lots of 50BTC outputs but with the locktime increasing by for\n> each output.\n>\n> For high value transactions, it isn't just the POW of the next block that\n> matters but all the blocks that are built on top of it.\n>\n> A pledger might want to say \"I will pay 1BTC if the next 100 blocks all\n> have at least an effective minting fee of 50BTC\"\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/3a6a6429/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-08T16:43:10",
                "message_text_only": "On Fri, May 08, 2015 at 06:00:37AM -0400, Jeff Garzik wrote:\n> That reminds me - I need to integrate the patch that automatically sweeps\n> anyone-can-pay transactions for a miner.\n\nYou mean anyone-can-spend?\n\nI've got code that does this actually:\n\nhttps://github.com/petertodd/replace-by-fee-tools/blob/master/spend-brainwallets-to-fees.py\n\nNeeds to have a feature where it replaces the txout set with simply\nOP_RETURN-to-fees if the inputs don't sign the outputs though.\n(SIGHASH_NONE for instance)\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000ee99382ac6bc043120085973b7b0378811c1acd8e3cdd9c\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/de9e73eb/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Assurance contracts to fund the network with OP_CHECKLOCKTIMEVERIFY",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "Benjamin",
                "Mike Hearn",
                "Peter Todd",
                "Tier Nolan"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 21143
        }
    },
    {
        "title": "[Bitcoin-development] Suggestion: Dynamic block size that updates like difficulty",
        "thread_messages": [
            {
                "author": "Michael Naber",
                "date": "2015-05-08T07:18:51",
                "message_text_only": "Why can't we have dynamic block size limit that changes with difficulty, such as the block size cannot exceed 2x the mean size of the prior difficulty period? \n\nI recently subscribed to this list so my apologies if this has been addressed already."
            }
        ],
        "thread_summary": {
            "title": "Suggestion: Dynamic block size that updates like difficulty",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Michael Naber"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 247
        }
    },
    {
        "title": "[Bitcoin-development] Proposed alternatives to the 20MB step function",
        "thread_messages": [
            {
                "author": "Matt Whitlock",
                "date": "2015-05-08T07:20:02",
                "message_text_only": "Between all the flames on this list, several ideas were raised that did not get much attention. I hereby resubmit these ideas for consideration and discussion.\n\n- Perhaps the hard block size limit should be a function of the actual block sizes over some trailing sampling period. For example, take the median block size among the most recent 2016 blocks and multiply it by 1.5. This allows Bitcoin to scale up gradually and organically, rather than having human beings guessing at what is an appropriate limit.\n\n- Perhaps the hard block size limit should be determined by a vote of the miners. Each miner could embed a desired block size limit in the coinbase transactions of the blocks it publishes. The effective hard block size limit would be that size having the greatest number of votes within a sliding window of most recent blocks.\n\n- Perhaps the hard block size limit should be a function of block-chain length, so that it can scale up smoothly rather than jumping immediately to 20 MB. This function could be linear (anticipating a breakdown of Moore's Law) or quadratic.\n\nI would be in support of any of the above, but I do not support Mike Hearn's proposed jump to 20 MB. Hearn's proposal kicks the can down the road without actually solving the problem, and it does so in a controversial (step function) way."
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-08T10:15:16",
                "message_text_only": "There are certainly arguments to be made for and against all of these\nproposals.\n\nThe fixed 20mb cap isn't actually my proposal at all, it is from Gavin. I\nam supporting it because anything is better than nothing. Gavin originally\nproposed the block size be a function of time. That got dropped, I suppose\nto make the process of getting consensus easier. It is \"the simplest thing\nthat can possibly work\".\n\nI would like to see the process of chain forking becoming less traumatic. I\nremember Gavin, Jeff and I once considered (on stage at a conference??)\nthat maybe there should be a scheduled fork every year, so people know when\nto expect them.\n\nIf everything goes well, I see no reason why 20mb would be the limit\nforever.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/557fa94d/attachment.html>"
            },
            {
                "author": "Cl\u00e9ment Elbaz",
                "date": "2015-05-08T10:30:22",
                "message_text_only": "Matt : I think proposal #1 and #3 are a lot better than #2, and #1 is my\nfavorite.\n\nI see two problems with proposal #2.\nThe first problem with proposal #2 is that, as we see in democracies,\nthere is often a mismatch between the people conscious vote and these same\npeople behavior.\n\nRelying on an  intentional vote made consciously by miners by choosing a\nconfiguration value can lead to twisted results if their actual behavior\ndoesn't correlate with their vote (eg, they all vote for a small block size\nbecause it is the default configuration of their software, and then they\nfill it completely all the time and everything crashes).\n\nThe second problem with proposal #2 is that if Gavin and Mike are right,\nthere is simply no time to gather a meaningful amount of votes over the\ncoinbases, after the fork but before the Bitcoin scalability crash.\n\nI like proposal #1 because the \"vote\" is made using already available data.\nAlso there is no possible mismatch between behavior and vote. As a miner\nyou vote by choosing to create a big (or small) block, and your actions\nreflect your vote. It is simple and straightforward.\n\nMy feelings on proposal #3 is it is a little bit mixing apples and oranges,\nbut I may not seeing all the implications.\n\nLe ven. 8 mai 2015 \u00e0 09:21, Matt Whitlock <bip at mattwhitlock.name> a \u00e9crit :\n\n> Between all the flames on this list, several ideas were raised that did\n> not get much attention. I hereby resubmit these ideas for consideration and\n> discussion.\n>\n> - Perhaps the hard block size limit should be a function of the actual\n> block sizes over some trailing sampling period. For example, take the\n> median block size among the most recent 2016 blocks and multiply it by 1.5.\n> This allows Bitcoin to scale up gradually and organically, rather than\n> having human beings guessing at what is an appropriate limit.\n>\n> - Perhaps the hard block size limit should be determined by a vote of the\n> miners. Each miner could embed a desired block size limit in the coinbase\n> transactions of the blocks it publishes. The effective hard block size\n> limit would be that size having the greatest number of votes within a\n> sliding window of most recent blocks.\n>\n> - Perhaps the hard block size limit should be a function of block-chain\n> length, so that it can scale up smoothly rather than jumping immediately to\n> 20 MB. This function could be linear (anticipating a breakdown of Moore's\n> Law) or quadratic.\n>\n> I would be in support of any of the above, but I do not support Mike\n> Hearn's proposed jump to 20 MB. Hearn's proposal kicks the can down the\n> road without actually solving the problem, and it does so in a\n> controversial (step function) way.\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/6f9d5ecd/attachment.html>"
            },
            {
                "author": "Joel Joonatan Kaartinen",
                "date": "2015-05-08T12:32:00",
                "message_text_only": "Matt,\n\nIt seems you missed my suggestion about basing the maximum block size on\nthe bitcoin days destroyed in transactions that are included in the block.\nI think it has potential for both scaling as well as keeping up a constant\nfee pressure. If tuned properly, it should both stop spamming and increase\nblock size maximum when there are a lot of real transactions waiting for\ninclusion.\n\n- Joel\n\nOn Fri, May 8, 2015 at 1:30 PM, Cl\u00e9ment Elbaz <clem.ds at gmail.com> wrote:\n\n> Matt : I think proposal #1 and #3 are a lot better than #2, and #1 is my\n> favorite.\n>\n> I see two problems with proposal #2.\n> The first problem with proposal #2 is that, as we see in democracies,\n> there is often a mismatch between the people conscious vote and these same\n> people behavior.\n>\n> Relying on an  intentional vote made consciously by miners by choosing a\n> configuration value can lead to twisted results if their actual behavior\n> doesn't correlate with their vote (eg, they all vote for a small block size\n> because it is the default configuration of their software, and then they\n> fill it completely all the time and everything crashes).\n>\n> The second problem with proposal #2 is that if Gavin and Mike are right,\n> there is simply no time to gather a meaningful amount of votes over the\n> coinbases, after the fork but before the Bitcoin scalability crash.\n>\n> I like proposal #1 because the \"vote\" is made using already available\n> data. Also there is no possible mismatch between behavior and vote. As a\n> miner you vote by choosing to create a big (or small) block, and your\n> actions reflect your vote. It is simple and straightforward.\n>\n> My feelings on proposal #3 is it is a little bit mixing apples and\n> oranges, but I may not seeing all the implications.\n>\n>\n> Le ven. 8 mai 2015 \u00e0 09:21, Matt Whitlock <bip at mattwhitlock.name> a\n> \u00e9crit :\n>\n>> Between all the flames on this list, several ideas were raised that did\n>> not get much attention. I hereby resubmit these ideas for consideration and\n>> discussion.\n>>\n>> - Perhaps the hard block size limit should be a function of the actual\n>> block sizes over some trailing sampling period. For example, take the\n>> median block size among the most recent 2016 blocks and multiply it by 1.5.\n>> This allows Bitcoin to scale up gradually and organically, rather than\n>> having human beings guessing at what is an appropriate limit.\n>>\n>> - Perhaps the hard block size limit should be determined by a vote of the\n>> miners. Each miner could embed a desired block size limit in the coinbase\n>> transactions of the blocks it publishes. The effective hard block size\n>> limit would be that size having the greatest number of votes within a\n>> sliding window of most recent blocks.\n>>\n>> - Perhaps the hard block size limit should be a function of block-chain\n>> length, so that it can scale up smoothly rather than jumping immediately to\n>> 20 MB. This function could be linear (anticipating a breakdown of Moore's\n>> Law) or quadratic.\n>>\n>> I would be in support of any of the above, but I do not support Mike\n>> Hearn's proposed jump to 20 MB. Hearn's proposal kicks the can down the\n>> road without actually solving the problem, and it does so in a\n>> controversial (step function) way.\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/b6a95d5e/attachment.html>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-08T12:48:16",
                "message_text_only": "On Friday, 8 May 2015, at 3:32 pm, Joel Joonatan Kaartinen wrote:\n> It seems you missed my suggestion about basing the maximum block size on\n> the bitcoin days destroyed in transactions that are included in the block.\n> I think it has potential for both scaling as well as keeping up a constant\n> fee pressure. If tuned properly, it should both stop spamming and increase\n> block size maximum when there are a lot of real transactions waiting for\n> inclusion.\n\nI saw it. I apologize for not including it in my list. I should have, for sake of discussion, even though I have a problem with it.\n\nMy problem with it is that \"bitcoin days destroyed\" is not a measure of demand for space in the block chain. In the distant future, when Bitcoin is the predominant global currency, bitcoins will have such high velocity that the number of bitcoin days destroyed in each block will be much lower than at present. Does this mean that the block size limit should be lower in the future than it is now? Clearly this would be incorrect.\n\nPerhaps I am misunderstanding your proposal. Could you describe it more explicitly?"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-08T13:24:49",
                "message_text_only": "On Friday, 8 May 2015, at 8:48 am, Matt Whitlock wrote:\n> On Friday, 8 May 2015, at 3:32 pm, Joel Joonatan Kaartinen wrote:\n> > It seems you missed my suggestion about basing the maximum block size on\n> > the bitcoin days destroyed in transactions that are included in the block.\n> > I think it has potential for both scaling as well as keeping up a constant\n> > fee pressure. If tuned properly, it should both stop spamming and increase\n> > block size maximum when there are a lot of real transactions waiting for\n> > inclusion.\n> \n> My problem with it is that \"bitcoin days destroyed\" is not a measure of demand for space in the block chain. In the distant future, when Bitcoin is the predominant global currency, bitcoins will have such high velocity that the number of bitcoin days destroyed in each block will be much lower than at present. Does this mean that the block size limit should be lower in the future than it is now? Clearly this would be incorrect.\n\nI feel a need to point out something that may be obvious to some but not to others: the cumulative total number of \"bitcoin days destroyed\" since the genesis block is bounded by the cumulative total number of \"bitcoin days created\" since the genesis block. (You can't destroy something that hasn't yet been created.) After all coins have been mined, bitcoin days will be created at a rate of 21M bitcoin days per day. In the long run, bitcoin days will be destroyed at a rate not exceeding 21M bitcoin days per day. This is so because bitcoin days cannot be destroyed at a rate faster than they are created for an indefinitely long time. This upper limit on the rate of bitcoin days destruction is irrespective of bitcoin adoption and the growth in demand for space in the block chain.\n\nEven ignoring the fact that \"bitcoin days destroyed\" is bounded whereas demand for block-chain space is not, we'd still have to answer the question of whether the rate of bitcoin days destroyed is a good estimator of demand for block-chain space. Why would it be? Suppose some day Satoshi moves his 1M coins to a new address. Would this huge destruction of bitcoin days imply anything about future demand for space in the block chain? No."
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-08T12:48:47",
                "message_text_only": "I like the bitcoin days destroyed idea.\n\nI like lots of the ideas that have been presented here, on the bitcointalk\nforums, etc etc etc.\n\nIt is easy to make a proposal, it is hard to wade through all of the\nproposals. I'm going to balance that equation by completely ignoring any\nproposal that isn't accompanied by code that implements the proposal (with\nappropriate tests).\n\nHowever, I'm not the bottleneck-- you need to get the attention of the\nother committers and convince THEM:\n\na) something should be done \"now-ish\"\nb) your idea is good\n\nWe are stuck on (a) right now, I think.\n\n\nOn Fri, May 8, 2015 at 8:32 AM, Joel Joonatan Kaartinen <\njoel.kaartinen at gmail.com> wrote:\n\n> Matt,\n>\n> It seems you missed my suggestion about basing the maximum block size on\n> the bitcoin days destroyed in transactions that are included in the block.\n> I think it has potential for both scaling as well as keeping up a constant\n> fee pressure. If tuned properly, it should both stop spamming and increase\n> block size maximum when there are a lot of real transactions waiting for\n> inclusion.\n>\n> - Joel\n>\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/f3bd5434/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-08T16:51:45",
                "message_text_only": "On Fri, May 08, 2015 at 03:32:00PM +0300, Joel Joonatan Kaartinen wrote:\n> Matt,\n> \n> It seems you missed my suggestion about basing the maximum block size on\n> the bitcoin days destroyed in transactions that are included in the block.\n> I think it has potential for both scaling as well as keeping up a constant\n> fee pressure. If tuned properly, it should both stop spamming and increase\n> block size maximum when there are a lot of real transactions waiting for\n> inclusion.\n\nThe problem with gating block creation on Bitcoin days destroyed is\nthere's a strong potential of giving big mining pools an huge advantage,\nbecause they can contract with large Bitcoin owners and buy dummy\ntransactions with large numbers of Bitcoin days destroyed on demand\nwhenever they need more days-destroyed to create larger blocks.\nSimilarly, with appropriate SIGHASH flags such contracting can be done\nby modifying *existing* transactions on demand.\n\nUltimately bitcoin days destroyed just becomes a very complex version of\ntransaction fees, and it's already well known that gating blocksize on\ntotal transaction fees doesn't work.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000f53e2d214685abf15b6d62d32453a03b0d472e374e10e94\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/aad39559/attachment.sig>"
            },
            {
                "author": "Joel Joonatan Kaartinen",
                "date": "2015-05-08T22:36:56",
                "message_text_only": "such a contract is a possibility, but why would big owners give an\nexclusive right to such pools? It seems to me it'd make sense to offer\nthose for any miner as long as the get paid a little for it. Especially\nwhen it's as simple as offering an incomplete transaction with the\nappropriate SIGHASH flags.\n\na part of the reason I like this idea is because it will allow stakeholders\na degree of influence on how large the fees are. At least from the surface,\nit looks like incentives are pretty well matched. They have an incentive to\nnot let the fees drop too low so the network continues to be usable and\nthey also have an incentive to not raise them too high because it'll push\nusers into using other systems. Also, there'll be competition between\nstakeholders, which should keep the fees reasonable.\n\nI think this would at least be preferable to the \"let the miner decide\"\nmodel.\n\n- Joel\n\nOn Fri, May 8, 2015 at 7:51 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Fri, May 08, 2015 at 03:32:00PM +0300, Joel Joonatan Kaartinen wrote:\n> > Matt,\n> >\n> > It seems you missed my suggestion about basing the maximum block size on\n> > the bitcoin days destroyed in transactions that are included in the\n> block.\n> > I think it has potential for both scaling as well as keeping up a\n> constant\n> > fee pressure. If tuned properly, it should both stop spamming and\n> increase\n> > block size maximum when there are a lot of real transactions waiting for\n> > inclusion.\n>\n> The problem with gating block creation on Bitcoin days destroyed is\n> there's a strong potential of giving big mining pools an huge advantage,\n> because they can contract with large Bitcoin owners and buy dummy\n> transactions with large numbers of Bitcoin days destroyed on demand\n> whenever they need more days-destroyed to create larger blocks.\n> Similarly, with appropriate SIGHASH flags such contracting can be done\n> by modifying *existing* transactions on demand.\n>\n> Ultimately bitcoin days destroyed just becomes a very complex version of\n> transaction fees, and it's already well known that gating blocksize on\n> total transaction fees doesn't work.\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 00000000000000000f53e2d214685abf15b6d62d32453a03b0d472e374e10e94\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/50484235/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-09T18:30:31",
                "message_text_only": "On Sat, May 09, 2015 at 01:36:56AM +0300, Joel Joonatan Kaartinen wrote:\n> such a contract is a possibility, but why would big owners give an\n> exclusive right to such pools? It seems to me it'd make sense to offer\n> those for any miner as long as the get paid a little for it. Especially\n> when it's as simple as offering an incomplete transaction with the\n> appropriate SIGHASH flags.\n\nLike many things, the fact that they need to negotiate the right at all\nis a *huge* barrier to smaller mining operations, as well as being an\nattractive point of control for regulators.\n\n> a part of the reason I like this idea is because it will allow stakeholders\n> a degree of influence on how large the fees are. At least from the surface,\n> it looks like incentives are pretty well matched. They have an incentive to\n> not let the fees drop too low so the network continues to be usable and\n> they also have an incentive to not raise them too high because it'll push\n> users into using other systems. Also, there'll be competition between\n> stakeholders, which should keep the fees reasonable.\n\nIf you want to allow stakeholders influence you should look into John Dillon's\nproof-of-stake blocksize voting scheme:\n\nhttp://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg02323.html\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000e7980aab9c096c46e7f34c43a661c5cb2ea71525ebb8af7\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/1d6c3b4f/attachment.sig>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-08T15:57:27",
                "message_text_only": "Adaptive schedules, i.e. those where block size limit depends not only on\nblock height, but on other parameters as well, are surely attractive in the\nsense that the system can adapt to the actual use, but they also open a\npossibility of a manipulation.\n\nE.g. one of mining companies might try to bankrupt other companies by\nmaking mining non-profitable. To do that they will accept transactions with\nridiculously low fees (e.g. 1 satoshi per transaction). Of course, they\nwill suffer losees themselves, but the they might be able to survive that\nif they have access to financial resources. (E.g. companies backed by banks\nand such will have an advantage).\nOnce competitors close down their mining operations, they can drive fees\nupwards.\n\nSo if you don't want to open room for manipulation (which is very hard to\nanalyze), it is better to have a block size hard limit which depends only\non block height.\nOn top of that there might be a soft limit which is enforced by the\nmajority of miners.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/e290f012/attachment.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-05-08T16:55:42",
                "message_text_only": "On Fri, May 8, 2015 at 2:20 AM, Matt Whitlock <bip at mattwhitlock.name> wrote:\n> - Perhaps the hard block size limit should be a function of the actual block sizes over some\n> trailing sampling period. For example, take the median block size among the most recent\n> 2016 blocks and multiply it by 1.5. This allows Bitcoin to scale up gradually and organically,\n> rather than having human beings guessing at what is an appropriate limit.\n\nBlock contents can be grinded much faster than hashgrinding and\nmining. There is a significant run-away effect there, and it also\nworks in the gradual sense as a miner probabilistically mines large\nblocks that get averaged into that 2016 median block size computation.\nAt least this proposal would be a slower way of pushing out miners and\nnetwork participants that can't handle 100 GB blocks immediately..  As\nthe size of the blocks are increased, low-end hardware participants\nhave to fall off the network because they no longer meet the minimum\nperformance requirements. Adjustment might become severely mismatched\nwith general economic trends in data storage device development or\navailability or even current-market-saturation of said storage\ndevices. With the assistance of transaction stuffing or grinding, that\n2016 block median metric can be gamed to increase faster than other\nparticipants can keep up with or, perhaps worse, in a way that was\nunintended by developers yet known to be a failure mode. These are\njust some issues to keep and mind and consider.\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-08T20:33:53",
                "message_text_only": "It is my professional opinion that raising the block size by merely\nadjusting a constant without any sort of feedback mechanism would be a\ndangerous and foolhardy thing to do. We are custodians of a multi-billion\ndollar asset, and it falls upon us to weigh the consequences of our own\nactions against the combined value of the entire bitcoin ecosystem. Ideally\nwe would take no action for which we are not absolutely certain of the\nramifications, with the information that can be made available to us. But\nof course that is not always possible: there are unknown-unknowns, time\npressures, and known-unknowns where information has too high a marginal\ncost. So where certainty is unobtainable, we must instead hedge against\nunwanted outcomes.\n\nThe proposal to raise the block size now by redefining a constant carries\nwith it risk associated with infrastructure scaling, centralization\npressures, and delaying the necessary development of a constraint-based fee\neconomy. It also simply kicks the can down the road in settling these\nissues because a larger but realistic hard limit must still exist, meaning\na future hard fork may still be required.\n\nBut whatever new hard limit is chosen, there is also a real possibility\nthat it may be too high. The standard response is that it is a soft-fork\nchange to impose a lower block size limit, which miners could do with a\nminimal amount of coordination. This is however undermined by the\nunfortunate reality that so many mining operations are absentee-run\nbusinesses, or run by individuals without a strong background in bitcoin\nprotocol policy, or with interests which are not well aligned with other\nusers or holders of bitcoin. We cannot rely on miners being vigilant about\nissues that develop, as they develop, or able to respond in the appropriate\nfashion that someone with full domain knowledge and an objective\nperspective would.\n\nThe alternative then is to have some sort of dynamic block size limit\ncontroller, and ideally one which applies a cost to raising the block size\nin some way the preserves the decentralization and/or long-term stability\nfeatures that we care about. I will now describe one such proposal:\n\n  * For each block, the miner is allowed to select a different difficulty\n(nBits) within a certain range, e.g. +/- 25% of the expected difficulty,\nand this miner-selected difficulty is used for the proof of work check. In\naddition to adjusting the hashcash target, selecting a different difficulty\nalso raises or lowers the maximum block size for that block by a function\nof the difference in difficulty. So increasing the difficulty of the block\nby an additional 25% raises the block limit for that block from 100% of the\ncurrent limit to 125%, and lowering the difficulty by 10% would also lower\nthe maximum block size for that block from 100% to 90% of the current\nlimit. For simplicity I will assume a linear identity transform as the\nfunction, but a quadratic or other function with compounding marginal cost\nmay be preferred.\n\n  * The default maximum block size limit is then adjusted at regular\nintervals. For simplicity I will assume an adjustment at the end of each\n2016 block interval, at the same time that difficulty is adjusted, but\nthere is no reason these have to be aligned. The adjustment algorithm\nitself is either the selection of the median, or perhaps some sort of\nweighted average that respects the \"middle majority.\" There would of course\nbe limits on how quickly the block size limit can adjusted in any one\nperiod, just as there are min/max limits on the difficulty adjustment.\n\n  * To prevent perverse mining incentives, the original difficulty without\nadjustment is used in the aggregate work calculations for selecting the\nmost-work chain, and the allowable miner-selected adjustment to difficulty\nwould have to be tightly constrained.\n\nThese rules create an incentive environment where raising the block size\nhas a real cost associated with it: a more difficult hashcash target for\nthe same subsidy reward. For rational miners that cost must be\ncounter-balanced by additional fees provided in the larger block. This\nallows block size to increase, but only within the confines of a\nself-supporting fee economy.\n\nWhen the subsidy goes away or is reduced to an insignificant fraction of\nthe block reward, this incentive structure goes away. Hopefully at that\ntime we would have sufficient information to soft-fork set a hard block\nsize maximum. But in the mean time, the block size limit controller\nconstrains the maximum allowed block size to be within a range supported by\nfees on the network, providing an emergency relief valve that we can be\nassured will only be used at significant cost.\n\nMark Friedenbach\n\n* There has over time been various discussions on the bitcointalk forums\nabout dynamically adjusting block size limits. The true origin of the idea\nis unclear at this time (citations would be appreciated!) but a form of it\nwas implemented in Bytecoin / Monero using subsidy burning to increase the\nblock size. That approach has various limitations. These were corrected in\nGreg Maxwell's suggestion to adjust the difficulty/nBits field directly,\nwhich also has the added benefit of providing incentive for bidirectional\nmovement during the subsidy period. The description in this email and any\nerrors are my own.\n\nOn Fri, May 8, 2015 at 12:20 AM, Matt Whitlock <bip at mattwhitlock.name>\nwrote:\n\n> Between all the flames on this list, several ideas were raised that did\n> not get much attention. I hereby resubmit these ideas for consideration and\n> discussion.\n>\n> - Perhaps the hard block size limit should be a function of the actual\n> block sizes over some trailing sampling period. For example, take the\n> median block size among the most recent 2016 blocks and multiply it by 1.5.\n> This allows Bitcoin to scale up gradually and organically, rather than\n> having human beings guessing at what is an appropriate limit.\n>\n> - Perhaps the hard block size limit should be determined by a vote of the\n> miners. Each miner could embed a desired block size limit in the coinbase\n> transactions of the blocks it publishes. The effective hard block size\n> limit would be that size having the greatest number of votes within a\n> sliding window of most recent blocks.\n>\n> - Perhaps the hard block size limit should be a function of block-chain\n> length, so that it can scale up smoothly rather than jumping immediately to\n> 20 MB. This function could be linear (anticipating a breakdown of Moore's\n> Law) or quadratic.\n>\n> I would be in support of any of the above, but I do not support Mike\n> Hearn's proposed jump to 20 MB. Hearn's proposal kicks the can down the\n> road without actually solving the problem, and it does so in a\n> controversial (step function) way.\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/ba1d33d5/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-08T22:43:14",
                "message_text_only": "This is a clever way to tie block size to fees.\n\nI would just like to point out though that it still fundamentally is using\nhard block size limits to enforce scarcity. Transactions with below market\nfees will hang in limbo for days and fail, instead of failing immediately\nby not propagating, or seeing degraded, long confirmation times followed by\neventual success.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Fri, May 8, 2015 at 1:33 PM, Mark Friedenbach <mark at friedenbach.org>\nwrote:\n\n> It is my professional opinion that raising the block size by merely\n> adjusting a constant without any sort of feedback mechanism would be a\n> dangerous and foolhardy thing to do. We are custodians of a multi-billion\n> dollar asset, and it falls upon us to weigh the consequences of our own\n> actions against the combined value of the entire bitcoin ecosystem. Ideally\n> we would take no action for which we are not absolutely certain of the\n> ramifications, with the information that can be made available to us. But\n> of course that is not always possible: there are unknown-unknowns, time\n> pressures, and known-unknowns where information has too high a marginal\n> cost. So where certainty is unobtainable, we must instead hedge against\n> unwanted outcomes.\n>\n> The proposal to raise the block size now by redefining a constant carries\n> with it risk associated with infrastructure scaling, centralization\n> pressures, and delaying the necessary development of a constraint-based fee\n> economy. It also simply kicks the can down the road in settling these\n> issues because a larger but realistic hard limit must still exist, meaning\n> a future hard fork may still be required.\n>\n> But whatever new hard limit is chosen, there is also a real possibility\n> that it may be too high. The standard response is that it is a soft-fork\n> change to impose a lower block size limit, which miners could do with a\n> minimal amount of coordination. This is however undermined by the\n> unfortunate reality that so many mining operations are absentee-run\n> businesses, or run by individuals without a strong background in bitcoin\n> protocol policy, or with interests which are not well aligned with other\n> users or holders of bitcoin. We cannot rely on miners being vigilant about\n> issues that develop, as they develop, or able to respond in the appropriate\n> fashion that someone with full domain knowledge and an objective\n> perspective would.\n>\n> The alternative then is to have some sort of dynamic block size limit\n> controller, and ideally one which applies a cost to raising the block size\n> in some way the preserves the decentralization and/or long-term stability\n> features that we care about. I will now describe one such proposal:\n>\n>   * For each block, the miner is allowed to select a different difficulty\n> (nBits) within a certain range, e.g. +/- 25% of the expected difficulty,\n> and this miner-selected difficulty is used for the proof of work check. In\n> addition to adjusting the hashcash target, selecting a different difficulty\n> also raises or lowers the maximum block size for that block by a function\n> of the difference in difficulty. So increasing the difficulty of the block\n> by an additional 25% raises the block limit for that block from 100% of the\n> current limit to 125%, and lowering the difficulty by 10% would also lower\n> the maximum block size for that block from 100% to 90% of the current\n> limit. For simplicity I will assume a linear identity transform as the\n> function, but a quadratic or other function with compounding marginal cost\n> may be preferred.\n>\n>   * The default maximum block size limit is then adjusted at regular\n> intervals. For simplicity I will assume an adjustment at the end of each\n> 2016 block interval, at the same time that difficulty is adjusted, but\n> there is no reason these have to be aligned. The adjustment algorithm\n> itself is either the selection of the median, or perhaps some sort of\n> weighted average that respects the \"middle majority.\" There would of course\n> be limits on how quickly the block size limit can adjusted in any one\n> period, just as there are min/max limits on the difficulty adjustment.\n>\n>   * To prevent perverse mining incentives, the original difficulty without\n> adjustment is used in the aggregate work calculations for selecting the\n> most-work chain, and the allowable miner-selected adjustment to difficulty\n> would have to be tightly constrained.\n>\n> These rules create an incentive environment where raising the block size\n> has a real cost associated with it: a more difficult hashcash target for\n> the same subsidy reward. For rational miners that cost must be\n> counter-balanced by additional fees provided in the larger block. This\n> allows block size to increase, but only within the confines of a\n> self-supporting fee economy.\n>\n> When the subsidy goes away or is reduced to an insignificant fraction of\n> the block reward, this incentive structure goes away. Hopefully at that\n> time we would have sufficient information to soft-fork set a hard block\n> size maximum. But in the mean time, the block size limit controller\n> constrains the maximum allowed block size to be within a range supported by\n> fees on the network, providing an emergency relief valve that we can be\n> assured will only be used at significant cost.\n>\n> Mark Friedenbach\n>\n> * There has over time been various discussions on the bitcointalk forums\n> about dynamically adjusting block size limits. The true origin of the idea\n> is unclear at this time (citations would be appreciated!) but a form of it\n> was implemented in Bytecoin / Monero using subsidy burning to increase the\n> block size. That approach has various limitations. These were corrected in\n> Greg Maxwell's suggestion to adjust the difficulty/nBits field directly,\n> which also has the added benefit of providing incentive for bidirectional\n> movement during the subsidy period. The description in this email and any\n> errors are my own.\n>\n> On Fri, May 8, 2015 at 12:20 AM, Matt Whitlock <bip at mattwhitlock.name>\n> wrote:\n>\n>> Between all the flames on this list, several ideas were raised that did\n>> not get much attention. I hereby resubmit these ideas for consideration and\n>> discussion.\n>>\n>> - Perhaps the hard block size limit should be a function of the actual\n>> block sizes over some trailing sampling period. For example, take the\n>> median block size among the most recent 2016 blocks and multiply it by 1.5.\n>> This allows Bitcoin to scale up gradually and organically, rather than\n>> having human beings guessing at what is an appropriate limit.\n>>\n>> - Perhaps the hard block size limit should be determined by a vote of the\n>> miners. Each miner could embed a desired block size limit in the coinbase\n>> transactions of the blocks it publishes. The effective hard block size\n>> limit would be that size having the greatest number of votes within a\n>> sliding window of most recent blocks.\n>>\n>> - Perhaps the hard block size limit should be a function of block-chain\n>> length, so that it can scale up smoothly rather than jumping immediately to\n>> 20 MB. This function could be linear (anticipating a breakdown of Moore's\n>> Law) or quadratic.\n>>\n>> I would be in support of any of the above, but I do not support Mike\n>> Hearn's proposed jump to 20 MB. Hearn's proposal kicks the can down the\n>> road without actually solving the problem, and it does so in a\n>> controversial (step function) way.\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/95998365/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-08T22:45:19",
                "message_text_only": "On Fri, May 8, 2015 at 3:43 PM, Aaron Voisine <voisine at gmail.com> wrote:\n\n> This is a clever way to tie block size to fees.\n>\n> I would just like to point out though that it still fundamentally is using\n> hard block size limits to enforce scarcity. Transactions with below market\n> fees will hang in limbo for days and fail, instead of failing immediately\n> by not propagating, or seeing degraded, long confirmation times followed by\n> eventual success.\n>\n\nThere are already solutions to this which are waiting to be deployed as\ndefault policy to bitcoind, and need to be implemented in other clients:\nreplace-by-fee and child-pays-for-parent.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/a030d88b/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-08T23:15:41",
                "message_text_only": "That's fair, and we've implemented child-pays-for-parent for spending\nunconfirmed inputs in breadwallet. But what should the behavior be when\nthose options aren't understood/implemented/used?\n\nMy argument is that the less risky, more conservative default fallback\nbehavior should be either non-propagation or delayed confirmation, which is\ngenerally what we have now, until we hit the block size limit. We still\nhave lots of safe, non-controversial, easy to experiment with options to\nadd fee pressure, causing users to economize on block space without\nresorting to dropping transactions after a prolonged delay.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Fri, May 8, 2015 at 3:45 PM, Mark Friedenbach <mark at friedenbach.org>\nwrote:\n\n> On Fri, May 8, 2015 at 3:43 PM, Aaron Voisine <voisine at gmail.com> wrote:\n>\n>> This is a clever way to tie block size to fees.\n>>\n>> I would just like to point out though that it still fundamentally is\n>> using hard block size limits to enforce scarcity. Transactions with below\n>> market fees will hang in limbo for days and fail, instead of failing\n>> immediately by not propagating, or seeing degraded, long confirmation times\n>> followed by eventual success.\n>>\n>\n> There are already solutions to this which are waiting to be deployed as\n> default policy to bitcoind, and need to be implemented in other clients:\n> replace-by-fee and child-pays-for-parent.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/561f60f5/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-08T23:58:20",
                "message_text_only": "In a fee-dominated future, replace-by-fee is not an opt-in feature. When\nyou create a transaction, the wallet presents a range of fees that it\nexpects you might pay. It then signs copies of the transaction with spaced\nfees from this interval and broadcasts the lowest fee first. In the user\ninterface, the transaction is shown with its transacted amount and the\napproved fee range. All of the inputs used are placed on hold until the\ntransaction gets a confirmation. As time goes by and it looks like the\ntransaction is not getting accepted, successively higher fee versions are\nreleased. You can opt-out and send a no-fee or base-fee-only transaction,\nbut that should not be the default.\n\nOn the receiving end, local policy controls how much fee should be spent\ntrying to obtain confirmations before alerting the user, if there are fees\navailable in the hot wallet to do this. The receiving wallet then adds its\nown fees via a spend if it thinks insufficient fees were provided to get a\nconfirmation. Again, this should all be automated so long as there is a hot\nwallet on the receiving end.\n\nIs this more complicated than now, where blocks are not full and clients\ngenerally don't have to worry about their transactions eventually\nconfirming? Yes, it is significantly more complicated. But such\ncomplication is unavoidable. It is a simple fact that the block size cannot\nincrease so much as to cover every single use by every single person in the\nworld, so there is no getting around the reality that we will have to\ntransition into an economy where at least one side has to pay up for a\ntransaction to get confirmation, at all. We are going to have to deal with\nthis issue whether it is now at 1MB or later at 20MB. And frankly, it'll be\nmuch easier to do now.\n\nOn Fri, May 8, 2015 at 4:15 PM, Aaron Voisine <voisine at gmail.com> wrote:\n\n> That's fair, and we've implemented child-pays-for-parent for spending\n> unconfirmed inputs in breadwallet. But what should the behavior be when\n> those options aren't understood/implemented/used?\n>\n> My argument is that the less risky, more conservative default fallback\n> behavior should be either non-propagation or delayed confirmation, which is\n> generally what we have now, until we hit the block size limit. We still\n> have lots of safe, non-controversial, easy to experiment with options to\n> add fee pressure, causing users to economize on block space without\n> resorting to dropping transactions after a prolonged delay.\n>\n> Aaron Voisine\n> co-founder and CEO\n> breadwallet.com\n>\n> On Fri, May 8, 2015 at 3:45 PM, Mark Friedenbach <mark at friedenbach.org>\n> wrote:\n>\n>> On Fri, May 8, 2015 at 3:43 PM, Aaron Voisine <voisine at gmail.com> wrote:\n>>\n>>> This is a clever way to tie block size to fees.\n>>>\n>>> I would just like to point out though that it still fundamentally is\n>>> using hard block size limits to enforce scarcity. Transactions with below\n>>> market fees will hang in limbo for days and fail, instead of failing\n>>> immediately by not propagating, or seeing degraded, long confirmation times\n>>> followed by eventual success.\n>>>\n>>\n>> There are already solutions to this which are waiting to be deployed as\n>> default policy to bitcoind, and need to be implemented in other clients:\n>> replace-by-fee and child-pays-for-parent.\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/5302db5a/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-09T03:36:07",
                "message_text_only": "On Fri, May 8, 2015 at 8:33 PM, Mark Friedenbach <mark at friedenbach.org> wrote:\n> These rules create an incentive environment where raising the block size has\n> a real cost associated with it: a more difficult hashcash target for the\n> same subsidy reward. For rational miners that cost must be counter-balanced\n> by additional fees provided in the larger block. This allows block size to\n> increase, but only within the confines of a self-supporting fee economy.\n>\n> When the subsidy goes away or is reduced to an insignificant fraction of the\n> block reward, this incentive structure goes away. Hopefully at that time we\n> would have sufficient information to soft-fork set a hard block size\n> maximum. But in the mean time, the block size limit controller constrains\n> the maximum allowed block size to be within a range supported by fees on the\n> network, providing an emergency relief valve that we can be assured will\n> only be used at significant cost.\n\nThough I'm a fan of this class of techniques(*) and think using something\nin this space is strictly superior to not, and I think it makes larger\nsizes safer long term;  I do not think it adequately obviates the need\nfor a hard upper limit for two reasons:\n\n(1) for software engineering and operational reasons it is very\ndifficult to develop, test for, or provision for something without\nknowing limits. There would in fact be hard limits on real deployments\nbut they'd be opaque to their operators and you could easily imagine\nthe network forking by surprise as hosts crossed those limits.\n\n(2)  At best this approach mitigates the collective action problem between\nminers around fees;  it does not correct the incentive alignment between\nminers and everyone else (miners can afford huge node costs because they\nhave income; but the full-node-using-users that need to exist in plenty\nto keep miners honest do not), or the centralization pressures (N miners\ncan reduce their storage/bandwidth/cpu costs N fold by centralizing).\n\nA dynamic limit can be combined with a hard upper to at least be no\nworse than a hard upper with respect to those two points.\n\n\nAnother related point which has been tendered before but seems to have\nbeen ignored is that changing how the size limit is computed can help\nbetter align incentives and thus reduce risk.  E.g. a major cost to the\nnetwork is the UTXO impact of transactions, but since the limit is blind\nto UTXO impact a miner would gain less income if substantially factoring\nUTXO impact into its fee calculations; and without fee impact users have\nlittle reason to optimize their UTXO behavior.   This can be corrected\nby augmenting the \"size\" used for limit calculations.   An example would\nbe tx_size = MAX( real_size >> 1,  real_size + 4*utxo_created_size -\n3*utxo_consumed_size).   The reason for the MAX is so that a block\nwhich cleaned a bunch of big UTXO could not break software by being\nsuper large, the utxo_consumed basically lets you credit your fees by\ncleaning the utxo set; but since you get less credit than you cost the\npressure should be downward but not hugely so. The 1/2, 4, 3 I regard\nas parameters which I don't have very strong opinions on which could be\nset based on observations in the network today (e.g. adjusted so that a\nnormal cleaning transaction can hit the minimum size).  One way to think\nabout this is that it makes it so that every output you create \"prepays\"\nthe transaction fees needed to spend it by shifting \"space\" from the\ncurrent block to a future block. The fact that the prepayment is not\nperfectly efficient reduces the incentive for miners to create lots of\nextra outputs when they have room left in their block in order to store\nspace to use later [an issue that is potentially less of a concern with a\ndynamic size limit].  With the right parameters there would never be such\nat thing as a dust output (one which costs more to spend than its worth).\n\n(likewise the sigops limit should be counted correctly and turned into\nsize augmentation (ones that get run by the txn); which would greatly\nsimplify selection rules: maximize income within a single scalar limit)\n\n(*) I believe my currently favored formulation of general dynamic control\nidea is that each miner expresses in their coinbase a preferred size\nbetween some minimum (e.g. 500k) and the miner's effective-maximum;\nthe actual block size can be up to the effective maximum even if the\npreference is lower (you're not forced to make a lower block because you\nstated you wished the limit were lower).  There is a computed maximum\nwhich is the 33-rd percentile of the last 2016 coinbase preferences\nminus computed_max/52 (rounding up to 1) bytes-- or 500k if thats\nlarger. The effective maximum is X bytes more, where X on the range\n[0, computed_maximum] e.g. the miner can double the size of their\nblock at most. If X > 0, then the miners must also reach a target\nF(x/computed_maximum) times the bits-difficulty; with F(x) = x^2+1  ---\nso the maximum penalty is 2, with a quadratic shape;  for a given mempool\nthere will be some value that maximizes expected income.  (obviously all\nimplemented with precise fixed point arithmetic).   The percentile is\nintended to give the preferences of the 33% least preferring miners a\nveto on increases (unless a majority chooses to soft-fork them out). The\nminus-comp_max/52 provides an incentive to slowly shrink the maximum\nif its too large-- x/52 would halve the size in one year if miners\nwere doing the lowest difficulty mining. The parameters 500k/33rd,\n-computed_max/52 bytes, and f(x)  I have less strong opinions about;\nand would love to hear reasoned arguments for particular parameters."
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-09T11:58:20",
                "message_text_only": "RE: fixing sigop counting, and building in UTXO cost: great idea! One of\nthe problems with this debate is it is easy for great ideas get lost in all\nthe noise.\n\nRE: a hard upper limit, with a dynamic limit under it:\n\nI like that idea. Can we drill down on the hard upper limit?\n\nThere are lots of people who want a very high upper limit, right now (all\nthe big Bitcoin companies, and anybody who thinks as-rapid-as-possible\ngrowth now is the best path to long-term success). This is the \"it is OK if\nyou have to run full nodes in a data center\" camp.\n\nThere are also lots of people who want an upper limit low enough that they\ncan continue to run Bitcoin on the hardware and Internet connection that\nthey have (or are concerned about centralization, so want to make sure\nOTHER people can continue to run....).\n\nIs there an upper limit \"we\" can choose to make both sets of people mostly\nhappy? I've proposed \"must be inexpensive enough that a 'hobbyist' can\nafford to run a full node\" ...\n\nIs the limit chosen once, now, via hard-fork, or should we expect multiple\nhard-forks to change it \"when necessary\" ?\n\nThe economics change every time the block reward halves, which make me\nthink that might be a good time to adjust the hard upper limit. If we have\na hard upper limit and a lower dynamic limit, perhaps adjusting the hard\nupper limit (up or down) to account for the block reward halving, based on\nthe dynamic limit....\n\n\n\nRE: the lower dynamic limit algorithm:  I REALLY like that idea.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/e371ad58/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-09T13:49:53",
                "message_text_only": "On Sat, May 9, 2015 at 12:58 PM, Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> RE: fixing sigop counting, and building in UTXO cost: great idea! One of\n> the problems with this debate is it is easy for great ideas get lost in all\n> the noise.\n>\n\nIf the UTXO set cost is built in, UTXO database entries suddenly are worth\nsomething, in addition to the bitcoin held in that entry.\n\nA user's client might display how many they own.  When sending money to a\nmerchant, the user might demand the merchant indicate a slot to pay to.\n\nThe user could send an ANYONE_CAN_PAY partial transaction.  The transaction\nwould guarantee that the user has at least as many UTXOs as before.\n\nDiscussing the possibility of doing this creates an incentive to bloat the\nUTXO set right now, since UTXOs would be valuable in the future.\n\nThe objective would be to make them valuable enough to encourage\nconservation, but not so valuable that the UTXO contains more value than\nthe bitcoins in the output.\n\nGmaxwell's suggested \"tx_size = MAX( real_size >> 1,  real_size +\n4*utxo_created_size - 3*utxo_consumed_size)\" for a 250 byte transaction\nwith 1 input and 2 outputs has very little effect.\n\nreal_size + 4 * (2) - 3 * 1 = 255\n\nThat gives a 2% size penalty for adding an extra UTXO.  I doubt that is\nenough to change behavior.\n\nThe UTXO set growth could be limited directly.  A block would be invalid if\nit increases the number of UTXO entries above the charted path.\n\nRE: a hard upper limit, with a dynamic limit under it:\n>\n\nIf the block is greater than 32MB, then it means an update to how blocks\nare broadcast, so that could be a reasonable hard upper limit (or maybe\n31MB, or just the 20MB already suggested).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/ffd834c1/attachment.html>"
            },
            {
                "author": "Owen Gunden",
                "date": "2015-05-10T17:36:32",
                "message_text_only": "On 05/08/2015 11:36 PM, Gregory Maxwell wrote:\n> Another related point which has been tendered before but seems to have\n> been ignored is that changing how the size limit is computed can help\n> better align incentives and thus reduce risk.  E.g. a major cost to the\n> network is the UTXO impact of transactions, but since the limit is blind\n> to UTXO impact a miner would gain less income if substantially factoring\n> UTXO impact into its fee calculations; and without fee impact users have\n> little reason to optimize their UTXO behavior.\n\nAlong the lines of aligning incentives with a diversity of costs to a \nvariety of network participants, I am curious about reactions to Justus' \ngeneral approach:\n\nhttp://bitcoinism.liberty.me/2015/02/09/economic-fallacies-and-the-block-size-limit-part-2-price-discovery/\n\nI realize it relies on pie-in-the-sky ideas like micropayment channels, \nbut I wonder if it's a worthy long-term ideal direction for this stuff."
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-10T18:10:47",
                "message_text_only": "Micropayment channels are not pie in the sky proposals. They work today on\nBitcoin as it is deployed without any changes. People just need to start\nusing them.\nOn May 10, 2015 11:03, \"Owen Gunden\" <ogunden at phauna.org> wrote:\n\n> On 05/08/2015 11:36 PM, Gregory Maxwell wrote:\n> > Another related point which has been tendered before but seems to have\n> > been ignored is that changing how the size limit is computed can help\n> > better align incentives and thus reduce risk.  E.g. a major cost to the\n> > network is the UTXO impact of transactions, but since the limit is blind\n> > to UTXO impact a miner would gain less income if substantially factoring\n> > UTXO impact into its fee calculations; and without fee impact users have\n> > little reason to optimize their UTXO behavior.\n>\n> Along the lines of aligning incentives with a diversity of costs to a\n> variety of network participants, I am curious about reactions to Justus'\n> general approach:\n>\n>\n> http://bitcoinism.liberty.me/2015/02/09/economic-fallacies-and-the-block-size-limit-part-2-price-discovery/\n>\n> I realize it relies on pie-in-the-sky ideas like micropayment channels,\n> but I wonder if it's a worthy long-term ideal direction for this stuff.\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/531fe7c2/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-10T21:21:06",
                "message_text_only": "Let me make sure I understand this proposal:\n\nOn Fri, May 8, 2015 at 11:36 PM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n\n> (*) I believe my currently favored formulation of general dynamic control\n> idea is that each miner expresses in their coinbase a preferred size\n> between some minimum (e.g. 500k) and the miner's effective-maximum;\n> the actual block size can be up to the effective maximum even if the\n> preference is lower (you're not forced to make a lower block because you\n> stated you wished the limit were lower).  There is a computed maximum\n> which is the 33-rd percentile of the last 2016 coinbase preferences\n> minus computed_max/52 (rounding up to 1) bytes-- or 500k if thats\n> larger. The effective maximum is X bytes more, where X on the range\n> [0, computed_maximum] e.g. the miner can double the size of their\n> block at most. If X > 0, then the miners must also reach a target\n> F(x/computed_maximum) times the bits-difficulty; with F(x) = x^2+1  ---\n> so the maximum penalty is 2, with a quadratic shape;  for a given mempool\n> there will be some value that maximizes expected income.  (obviously all\n> implemented with precise fixed point arithmetic).   The percentile is\n> intended to give the preferences of the 33% least preferring miners a\n> veto on increases (unless a majority chooses to soft-fork them out). The\n> minus-comp_max/52 provides an incentive to slowly shrink the maximum\n> if its too large-- x/52 would halve the size in one year if miners\n> were doing the lowest difficulty mining. The parameters 500k/33rd,\n> -computed_max/52 bytes, and f(x)  I have less strong opinions about;\n> and would love to hear reasoned arguments for particular parameters.\n>\n\nI'm going to try to figure out how much transaction fee a transaction would\nhave to pay to bribe a miner to include it. Greg, please let me know if\nI've misinterpreted the proposed algorithm. And everybody, please let me\nknow if I'm making a bone-headed mistake in how I'm computing anything:\n\nLets say miners are expressing a desire for 600,000 byte blocks in their\ncoinbases.\n\ncomputed_max = 600,000 - 600,000/52 = 588,462 bytes.\n  --> this is about 23 average-size (500-byte) transactions less than\n600,000.\neffective_max = 1,176,923\n\nLets say I want to maintain status quo at 600,000 bytes; how much penalty\ndo I have?\n((600,000-588,462)/588,462)^2 + 1 = 1.00038\n\nHow much will that cost me?\nThe network is hashing at 310PetaHash/sec right now.\nTakes 600 seconds to find a block, so 186,000PH per block\n186,000 * 0.00038 = 70 extra PH\n\nIf it takes 186,000 PH to find a block, and a block is worth 25.13 BTC\n(reward plus fees), that 70 PH costs:\n(25.13 BTC/block / 186,000 PH/block) * 70 PH = 0.00945 BTC\nor at $240 / BTC:  $2.27\n\n... so average transaction fee will have to be about ten cents ($2.27\nspread across 23 average-sized transactions) for miners to decide to stay\nat 600K blocks. If they fill up 588,462 bytes and don't have some\nten-cent-fee transactions left, they should express a desire to create a\n588,462-byte-block and mine with no penalty.\n\nIs that too much?  Not enough?  Average transaction fees today are about 3\ncents per transaction.\nI created a spreadsheet playing with the parameters:\n\nhttps://docs.google.com/spreadsheets/d/1zYZfb44Uns8ai0KnoQ-LixDwdhqO5iTI3ZRcihQXlgk/edit?usp=sharing\n\n\"We\" could tweak the constants or function to get a transaction fee we\nthink is reasonable... but we really shouldn't be deciding whether\ntransaction fees are too high, too low, or just right, and after thinking\nabout this for a while I think any algorithm that ties difficulty to block\nsize is just a complicated way of dictating minimum fees.\n\nAs for some other dynamic algorithm: OK with me. How do we get consensus on\nwhat the best algorithm is? I'm ok with any \"don't grow too quickly, give\nsome reasonable-percentage-minority of miners the ability to block further\nincreases.\"\n\nAlso relevant here:\n\"The curious task of economics is to demonstrate to men how little they\nreally know about what they imagine they can design.\" - Friedrich August\nvon Hayek\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/481dae92/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-10T21:33:15",
                "message_text_only": "On Sun, May 10, 2015 at 9:21 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> a while I think any algorithm that ties difficulty to block size is just a\n> complicated way of dictating minimum fees.\n\nThats not the long term effect or the motivation-- what you're seeing\nis that the subsidy gets in the way here.  Consider how the procedure\nbehaves with subsidy being negligible compared to fees.   What it\naccomplishes in that case is that it incentivizes increasing the size\nuntil the marginal \"value\" to miners of the transaction-data being\nleft out is not enormously smaller than the \"value\" of the data in the\nblock on average.  Value in quotes because it's blind to the \"fees\"\nthe transaction claims.\n\nWith a large subsidy, the marginal value of the first byte in the\nblock is HUGE; and so that pushes up the average-- and creates the\n\"base fee effect\" that you're looking at.  It's not that anyone is\npicking a fee there, it's that someone picked the subsidy there.  :)\nAs the subsidy goes down the only thing fees are relative to is fees.\n\nAn earlier version of the proposal took subsidy out of the picture\ncompletely by increasing it linearly with the increased difficulty;\nbut that creates additional complexity both to implement and to\nexplain to people (e.g. that the setup doesn't change the supply of\ncoins); ... I suppose without it that starting disadvantage parameter\n(the offset that reduces the size if you're indifferent) needs to be\nmuch smaller, unfortunately."
            },
            {
                "author": "Rob Golding",
                "date": "2015-05-10T21:56:30",
                "message_text_only": "> How much will that cost me?\n> The network is hashing at 310PetaHash/sec right now.\n> Takes 600 seconds to find a block, so 186,000PH per block\n> 186,000 * 0.00038 = 70 extra PH\n> \n> If it takes 186,000 PH to find a block, and a block is worth 25.13 BTC\n> (reward plus fees), that 70 PH costs:\n> (25.13 BTC/block / 186,000 PH/block) * 70 PH = 0.00945 BTC\n> or at $240 / BTC:  $2.27\n> \n> ... so average transaction fee will have to be about ten cents ($2.27\n> spread across 23 average-sized transactions) for miners to decide to\n> stay at 600K blocks\n\nSurely that's an *extra* $2.27 as you've already included .13BTC \n($31.20) in fees in the calculation ?\n\nRob"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T10:43:08",
                "message_text_only": "On Sat, May 9, 2015 at 4:36 AM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n\n> An example would\n> be tx_size = MAX( real_size >> 1,  real_size + 4*utxo_created_size -\n> 3*utxo_consumed_size).\n\n\nThis could be implemented as a soft fork too.\n\n* 1MB hard size limit\n* 900kB soft limit\n\nS = block size\nU = UTXO_adjusted_size = S + 4 * outputs - 3 * inputs\n\nA block is valid if S < 1MB and U < 1MB\n\nA 250 byte transaction with 2 inputs and 2 outputs would have an adjusted\nsize of 252 bytes.\n\nThe memory pool could be sorted by fee per adjusted_size.\n\n Coin selection could be adjusted so it tries to have at least 2 inputs\nwhen creating transactions, unless the input is worth more than a threshold\n(say 0.001 BTC).\n\nThis is a pretty weak incentive, especially if the block size is\nincreased.  Maybe it will cause a \"nudge\"\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/0e8a97f2/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2015-05-16T00:22:14",
                "message_text_only": "Tier Nolan <tier.nolan at gmail.com> writes:\n> On Sat, May 9, 2015 at 4:36 AM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n>\n>> An example would\n>> be tx_size = MAX( real_size >> 1,  real_size + 4*utxo_created_size -\n>> 3*utxo_consumed_size).\n>\n>\n> This could be implemented as a soft fork too.\n>\n> * 1MB hard size limit\n> * 900kB soft limit\n\nI like this too.\n\nSome tweaks:\n\n1) Nomenclature: call tx_size \"tx_cost\" and real_size \"tx_bytes\"?\n\n2) If we have a reasonable hard *byte* limit, I don't think that we need\n   the MAX().  In fact, it's probably OK to go negative.\n\n3) ... or maybe not, if any consumed UTXO was generated before the soft\n   fork (reducing Tier's perverse incentive).\n\n4) How do we measure UTXO size?  There are some constant-ish things in\n   there (eg. txid as key, height, outnum, amount).  Maybe just add 32\n   to scriptlen?\n\n5) Add a CHECKSIG cost.  Naively, since we allow 20,000 CHECKSIGs and\n   1MB blocks, that implies a cost of 50 bytes per CHECKSIG (but counted\n   correctly, unlike now).   \n\nThis last one implies that the initial cost limit would be 2M, but in\npractice probably somewhere in the middle.\n\n  tx_cost = 50*num-CHECKSIG\n                + tx_bytes\n                + 4*utxo_created_size\n                - 3*utxo_consumed_size\n\n> A 250 byte transaction with 2 inputs and 2 outputs would have an adjusted\n> size of 252 bytes.\n\nNow cost == 352.\n\nCheers,\nRusty."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-16T11:09:50",
                "message_text_only": "On Sat, May 16, 2015 at 1:22 AM, Rusty Russell <rusty at rustcorp.com.au>\nwrote:\n\n> Some tweaks:\n>\n> 1) Nomenclature: call tx_size \"tx_cost\" and real_size \"tx_bytes\"?\n>\n\nFair enough.\n\n>\n> 2) If we have a reasonable hard *byte* limit, I don't think that we need\n>    the MAX().  In fact, it's probably OK to go negative.\n>\n\nI agree, we want people to compress the UTXO space and a transaction with\n100 inputs and one output is great.\n\nIt may have privacy problem though.\n\n\n>\n> 3) ... or maybe not, if any consumed UTXO was generated before the soft\n>    fork (reducing Tier's perverse incentive).\n>\n\nThe incentive problem can be fixed by excluding UTXOs from blocks before a\ncertain count.\n\nUTXOs in blocks before 375000 don't count.\n\n\n>\n> 4) How do we measure UTXO size?  There are some constant-ish things in\n>    there (eg. txid as key, height, outnum, amount).  Maybe just add 32\n>    to scriptlen?\n>\n\nThey can be stored as a fixed digest.  That can be any size, depending on\nsecurity requirements.\n\nGmaxwell's cost proposal is 3-4 bytes per UTXO change.  It isn't\n4*UXTO.size - 3*UTXO.size\n\nIt is only a small nudge.  With only 10% of the block space to play with it\ncan't be massive.\n\nThis requires that transactions include scriptPubKey information when\nbroadcasting them.\n\n\n>\n> 5) Add a CHECKSIG cost.  Naively, since we allow 20,000 CHECKSIGs and\n>    1MB blocks, that implies a cost of 50 bytes per CHECKSIG (but counted\n>    correctly, unlike now).\n>\n> This last one implies that the initial cost limit would be 2M, but in\n> practice probably somewhere in the middle.\n>\n>   tx_cost = 50*num-CHECKSIG\n>                 + tx_bytes\n>                 + 4*utxo_created_size\n>                 - 3*utxo_consumed_size\n>\n> > A 250 byte transaction with 2 inputs and 2 outputs would have an adjusted\n> > size of 252 bytes.\n>\n> Now cost == 352.\n>\n\nThat is to large a cost for a 10% block change.  It could be included in\nthe block size hard fork though.  I think have one combined \"cost\" for\ntransactions is good.  It means much fewer spread out transaction checks.\nThe code for the cost formula would be in one place.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150516/8e5f1099/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2015-05-18T01:42:11",
                "message_text_only": "Tier Nolan <tier.nolan at gmail.com> writes:\n> On Sat, May 16, 2015 at 1:22 AM, Rusty Russell <rusty at rustcorp.com.au>\n> wrote:\n>> 3) ... or maybe not, if any consumed UTXO was generated before the soft\n>>    fork (reducing Tier's perverse incentive).\n>\n> The incentive problem can be fixed by excluding UTXOs from blocks before a\n> certain count.\n>\n> UTXOs in blocks before 375000 don't count.\n\nOK.  Be nice if these were cleaned up, but I guess it's a sunk cost.\n\n>> 4) How do we measure UTXO size?  There are some constant-ish things in\n>>    there (eg. txid as key, height, outnum, amount).  Maybe just add 32\n>>    to scriptlen?\n>>\n>\n> They can be stored as a fixed digest.  That can be any size, depending on\n> security requirements.\n>\n> Gmaxwell's cost proposal is 3-4 bytes per UTXO change.  It isn't\n> 4*UXTO.size - 3*UTXO.size\n\nHe said \"utxo_created_size\" not \"utxo_created\" so I assumed scriptlen?\n\n> It is only a small nudge.  With only 10% of the block space to play with it\n> can't be massive.\n\nBut you made that number up?  The soft cap and hard byte limit are\ndifferent beasts, so there's no need for soft cost cap < hard byte\nlimit.\n\n> This requires that transactions include scriptPubKey information when\n> broadcasting them.\n\nBrilliant!  I completely missed that possibility...\n\n>> 5) Add a CHECKSIG cost.  Naively, since we allow 20,000 CHECKSIGs and\n>>    1MB blocks, that implies a cost of 50 bytes per CHECKSIG (but counted\n>>    correctly, unlike now).\n>>\n>> This last one implies that the initial cost limit would be 2M, but in\n>> practice probably somewhere in the middle.\n>>\n>>   tx_cost = 50*num-CHECKSIG\n>>                 + tx_bytes\n>>                 + 4*utxo_created_size\n>>                 - 3*utxo_consumed_size\n>>\n>> > A 250 byte transaction with 2 inputs and 2 outputs would have an adjusted\n>> > size of 252 bytes.\n>>\n>> Now cost == 352.\n>\n> That is to large a cost for a 10% block change.  It could be included in\n> the block size hard fork though.\n\nI don't think so.  Again, you're mixing units.\n\n> I think have one combined \"cost\" for\n> transactions is good.  It means much fewer spread out transaction checks.\n> The code for the cost formula would be in one place.\n\nAgreed!  Unfortunately there'll always be 2, because we really do want a\nhard byte limit: it's total tx bytes which brings most concerns about\ncentralization.  But ideally it'll be so rarely hit that it can be ~\nignored (and certainly not optimized for).\n\nCheers,\nRusty."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-19T08:59:27",
                "message_text_only": "On Mon, May 18, 2015 at 2:42 AM, Rusty Russell <rusty at rustcorp.com.au>\nwrote:\n\n> OK.  Be nice if these were cleaned up, but I guess it's a sunk cost.\n>\n\nYeah.\n\nOn the plus side, as people spend their money, old UTXOs would be used up\nand then they would be included in the cost function.  It is only people\nwho are storing their money long term that wouldn't.\n\nThey are unlikely to have consumed their UTXOs anyway, unless miners\nstarted paying for UTXOs.\n\nWe could make it a range.\n\nUTXOs from below 355,000 and above 375,000 are included.  That can create\nincentive problems for the next similar change, I think a future threshold\nis better.\n\n\n>  He said \"utxo_created_size\" not \"utxo_created\" so I assumed scriptlen?\n>\n\nMaybe I mis-read.\n\n\n> But you made that number up?  The soft cap and hard byte limit are\n> different beasts, so there's no need for soft cost cap < hard byte\n> limit.\n>\n\nI was thinking about it being a soft-fork.\n\nIf it was combined with the 20MB limit change, then it can be anything.\n\nI made a suggestion somewhere (her or forums not sure), that transactions\nshould be allowed to store bytes.\n\nFor example, a new opcode could be added, <byte_count> OP_LOCK_BYTES.\n\nThis makes the transaction seem <byte_count> larger.  However, when\nspending the UTXO, that transaction counts as <byte_count> smaller, even\nagainst the hard-cap.\n\nThis would be useful for channels.  If channels were 100-1000X the\nblockchain volume and someone caused lots of channels to close, there\nmightn't be enough space for all the close channel transactions.  Some\npeople might be able to get their refund transactions included in the\nblockchain because the timeout expires.\n\nIf transactions could store enough space to be spent, then a mass channel\nclose would cause some very large blocks, but then they would have to be\nfollowed by lots of tiny blocks.\n\nThe block limit would be an average not fixed per block.  There would be 3\nlimits\n\nAbsolute hard limit (max bytes no matter what): 100MB\nHard limit (max bytes after stored bytes offset): 30MB\nSoft limit (max bytes equivalents): 10MB\n\nBlocks lager than ~32MB require a new network protocol, which makes the\nhard fork even \"harder\".  The protocol change could be \"messages can now be\n150MB max\" though, so maybe not so complex.\n\n\n>\n> > This requires that transactions include scriptPubKey information when\n> > broadcasting them.\n>\n> Brilliant!  I completely missed that possibility...\n>\n\nI have written a BIP about it.  It is still in the draft stage.  I had a\nlook into writing up the code for the protocol change.\n\nhttps://github.com/TierNolan/bips/blob/extended_transactions/bip-etx.mediawiki\nhttps://github.com/TierNolan/bips/blob/extended_transactions/bip-etx-fork.mediawiki\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/32dabf8f/attachment.html>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2015-05-10T21:48:39",
                "message_text_only": "Le 08/05/2015 22:33, Mark Friedenbach a \u00e9crit :\n\n>   * For each block, the miner is allowed to select a different difficulty\n> (nBits) within a certain range, e.g. +/- 25% of the expected difficulty,\n> and this miner-selected difficulty is used for the proof of work check. In\n> addition to adjusting the hashcash target, selecting a different difficulty\n> also raises or lowers the maximum block size for that block by a function\n> of the difference in difficulty. So increasing the difficulty of the block\n> by an additional 25% raises the block limit for that block from 100% of the\n> current limit to 125%, and lowering the difficulty by 10% would also lower\n> the maximum block size for that block from 100% to 90% of the current\n> limit. For simplicity I will assume a linear identity transform as the\n> function, but a quadratic or other function with compounding marginal cost\n> may be preferred.\n> \n\nSorry but I fail to see how a linear identity transform between block\nsize and difficulty would work.\n\nThe miner's reward for finding a block is the sum of subsidy and fees:\n\n R = S + F\n\nThe probability that the miner will find a block over a time interval is\ninversely proportional to the difficulty D:\n\n P = K / D\n\nwhere K is a constant that depends on the miner's hashrate. The expected\nreward of the miner is:\n\n E = P * R\n\nConsider that the miner chooses a new difficulty:\n\n D' = D(1 + x).\n\nWith a linear identity transform between block size and difficulty, the\nminer will be allowed to collect fees from a block of size: S'=S(1+x)\n\nIn the best case, collected will be proportional to block size:\n\n F' = F(1+x)\n\nThus we get:\n\n E' = P' * R' = K/(D(1+x)) * (S + F(1+x))\n\n E' = E - x/(1+x) * S * K / D\n\nSo with this linear identity transform, increasing block size never\nincreases the miners gain. As long as the subsidy exists, the best\nstrategy for miners is to reduce block size (i.e. to choose x<0)."
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-10T22:31:46",
                "message_text_only": "I'm on my phone today so I'm somewhat constrained in my reply, but the key\ntakeaway is that the proposal is a mechanism for miners to trade subsidy\nfor the increased fees of a larger block. Necessarily it only makes sense\nto do so when the marginal fee per KB exceeds the subsidy fee per KB. It\ncorrespondingly makes sense to use a smaller block size if fees are less\nthan subsidy, but note that fees are not uniform and as the block shrinks\nthe marginal fee rate goes up..\n\nLimits on both the relative and absolute amount a miner can trade subsidy\nfor block size prevent incentive edge cases as well as prevent a sharp\nshock to the current fee-poor economy (by disallowing adjustment below 1MB).\n\nAlso the identity transform was used only for didactic purposes. I fully\nexpect there to be other, more interesting functions to use.\nOn May 10, 2015 3:03 PM, \"Thomas Voegtlin\" <thomasv at electrum.org> wrote:\n\n> Le 08/05/2015 22:33, Mark Friedenbach a \u00e9crit :\n>\n> >   * For each block, the miner is allowed to select a different difficulty\n> > (nBits) within a certain range, e.g. +/- 25% of the expected difficulty,\n> > and this miner-selected difficulty is used for the proof of work check.\n> In\n> > addition to adjusting the hashcash target, selecting a different\n> difficulty\n> > also raises or lowers the maximum block size for that block by a function\n> > of the difference in difficulty. So increasing the difficulty of the\n> block\n> > by an additional 25% raises the block limit for that block from 100% of\n> the\n> > current limit to 125%, and lowering the difficulty by 10% would also\n> lower\n> > the maximum block size for that block from 100% to 90% of the current\n> > limit. For simplicity I will assume a linear identity transform as the\n> > function, but a quadratic or other function with compounding marginal\n> cost\n> > may be preferred.\n> >\n>\n> Sorry but I fail to see how a linear identity transform between block\n> size and difficulty would work.\n>\n> The miner's reward for finding a block is the sum of subsidy and fees:\n>\n>  R = S + F\n>\n> The probability that the miner will find a block over a time interval is\n> inversely proportional to the difficulty D:\n>\n>  P = K / D\n>\n> where K is a constant that depends on the miner's hashrate. The expected\n> reward of the miner is:\n>\n>  E = P * R\n>\n> Consider that the miner chooses a new difficulty:\n>\n>  D' = D(1 + x).\n>\n> With a linear identity transform between block size and difficulty, the\n> miner will be allowed to collect fees from a block of size: S'=S(1+x)\n>\n> In the best case, collected will be proportional to block size:\n>\n>  F' = F(1+x)\n>\n> Thus we get:\n>\n>  E' = P' * R' = K/(D(1+x)) * (S + F(1+x))\n>\n>  E' = E - x/(1+x) * S * K / D\n>\n> So with this linear identity transform, increasing block size never\n> increases the miners gain. As long as the subsidy exists, the best\n> strategy for miners is to reduce block size (i.e. to choose x<0).\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/5281404f/attachment.html>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2015-05-10T23:11:55",
                "message_text_only": "Le 11/05/2015 00:31, Mark Friedenbach a \u00e9crit :\n> I'm on my phone today so I'm somewhat constrained in my reply, but the key\n> takeaway is that the proposal is a mechanism for miners to trade subsidy\n> for the increased fees of a larger block. Necessarily it only makes sense\n> to do so when the marginal fee per KB exceeds the subsidy fee per KB. It\n> correspondingly makes sense to use a smaller block size if fees are less\n> than subsidy, but note that fees are not uniform and as the block shrinks\n> the marginal fee rate goes up..\n> \n\nOh I see, you expect the sign of the dE/dx to change depending on\nwhether fees exceed the subsidy. This is possible, but instead of the\nlinear identity, you have to increase the block size twice as fast as\nthe difficulty. In that case we would get (using the notations of my\nprevious email):\n\nD' = D(1+x)\nF' = F(1+2x)\n\nand thus:\n\nE' - E = x/(1+x)P(F-S)\n\nThe presence of the (F-S) factor means that the sign reversal occurs\nwhen fees exceed subsidy.\n\n\n> Limits on both the relative and absolute amount a miner can trade subsidy\n> for block size prevent incentive edge cases as well as prevent a sharp\n> shock to the current fee-poor economy (by disallowing adjustment below 1MB).\n> \n> Also the identity transform was used only for didactic purposes. I fully\n> expect there to be other, more interesting functions to use."
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-28T15:53:41",
                "message_text_only": "On Fri, May 8, 2015 at 3:20 AM, Matt Whitlock <bip at mattwhitlock.name> wrote:\n\n> Between all the flames on this list, several ideas were raised that did\n> not get much attention. I hereby resubmit these ideas for consideration and\n> discussion.\n>\n> - Perhaps the hard block size limit should be a function of the actual\n> block sizes over some trailing sampling period. For example, take the\n> median block size among the most recent 2016 blocks and multiply it by 1.5.\n> This allows Bitcoin to scale up gradually and organically, rather than\n> having human beings guessing at what is an appropriate limit.\n>\n\nA lot of people like this idea, or something like it. It is nice and\nsimple, which is really important for consensus-critical code.\n\nWith this rule in place, I believe there would be more \"fee pressure\"\n(miners would be creating smaller blocks) today. I created a couple of\nhistograms of block sizes to infer what policy miners are ACTUALLY\nfollowing today with respect to block size:\n\nLast 1,000 blocks:\n  http://bitcoincore.org/~gavin/sizes_last1000.html\n\nNotice a big spike at 750K -- the default size for Bitcoin Core.\nThis graph might be misleading, because transaction volume or fees might\nnot be high enough over the last few days to fill blocks to whatever limit\nminers are willing to mine.\n\nSo I graphed a time when (according to statoshi.info) there WERE a lot of\ntransactions waiting to be confirmed:\n   http://bitcoincore.org/~gavin/sizes_357511.html\n\nThat might also be misleading, because it is possible there were a lot of\ntransactions waiting to be confirmed because miners who choose to create\nsmall blocks got lucky and found more blocks than normal.  In fact, it\nlooks like that is what happened: more smaller-than-normal blocks were\nfound, and the memory pool backed up.\n\nSo: what if we had a dynamic maximum size limit based on recent history?\n\nThe average block size is about 400K, so a 1.5x rule would make the max\nblock size 600K; miners would definitely be squeezing out transactions /\nputting pressure to increase transaction fees. Even a 2x rule (implying\n800K max blocks) would, today, be squeezing out transactions / putting\npressure to increase fees.\n\nUsing a median size instead of an average means the size can increase or\ndecrease more quickly. For example, imagine the rule is \"median of last\n2016 blocks\" and 49% of miners are producing 0-size blocks and 51% are\nproducing max-size blocks. The median is max-size, so the 51% have total\ncontrol over making blocks bigger.  Swap the roles, and the median is\nmin-size.\n\nBecause of that, I think using an average is better-- it means the max size\nwill change (up or down) more slowly.\n\nI also think 2016 blocks is too long, because transaction volumes change\nquicker than that. An average over 144 blocks (last 24 hours) would be\nbetter able to handle increased transaction volume around major holidays,\nand would also be able to react more quickly if an economically irrational\nattacker attempted to flood the network with fee-paying transactions.\n\nSo my straw-man proposal would be:  max size 2x average size over last 144\nblocks, calculated at every block.\n\nThere are a couple of other changes I'd pair with that consensus change:\n\n+ Make the default mining policy for Bitcoin Core neutral-- have its target\nblock size be the average size, so miners that don't care will \"go along\nwith the people who do care.\"\n\n+ Use something like Greg's formula for size instead of bytes-on-the-wire,\nto discourage bloating the UTXO set.\n\n\n---------\n\nWhen I've proposed (privately, to the other core committers) some dynamic\nalgorithm the objection has been \"but that gives miners complete control\nover the max block size.\"\n\nI think that worry is unjustified right now-- certainly, until we have\nsize-independent new block propagation there is an incentive for miners to\nkeep their blocks small, and we see miners creating small blocks even when\nthere are fee-paying transactions waiting to be confirmed.\n\nI don't even think it will be a problem if/when we do have size-independent\nnew block propagation, because I think the combination of the random timing\nof block-finding plus a dynamic limit as described above will create a\nhealthy system.\n\nIf I'm wrong, then it seems to me the miners will have a very strong\nincentive to, collectively, impose whatever rules are necessary (maybe a\nsoft-fork to put a hard cap on block size) to make the system healthy again.\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/5681756b/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-28T17:05:18",
                "message_text_only": ">\n> Even a 2x rule (implying 800K max blocks) would, today, be squeezing out\n> transactions / putting pressure to increase fees .....\n>\n> So my straw-man proposal would be:  max size 2x average size over last 144\n> blocks, calculated at every block.\n>\n\nIsn't that a step backwards, then? I see no reason for fee pressure to\nexist at the moment. All it's doing is turning away users for no purpose:\nmining isn't supported by fees, and the tiny fees we use right now seem to\nbe good enough to stop penny flooding.\n\nWhy not set the max size to be 20x the average size? Why 2x, given you just\npointed out that'd result in blocks shrinking rather than growing.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/1e8e17c3/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-28T17:19:44",
                "message_text_only": "On Thu, May 28, 2015 at 1:05 PM, Mike Hearn <mike at plan99.net> wrote:\n\n> Isn't that a step backwards, then? I see no reason for fee pressure to\n>> exist at the moment. All it's doing is turning away users for no purpose:\n>> mining isn't supported by fees, and the tiny fees we use right now seem to\n>> be good enough to stop penny flooding.\n>>\n>\n> Why not set the max size to be 20x the average size? Why 2x, given you\n> just pointed out that'd result in blocks shrinking rather than growing.\n>\n\nTwenty is scary.\n\nAnd two is a very neutral number: if 50% of hashpower want the max size to\ngrow as fast as possible and 50% are dead-set opposed to any increase in\nmax size, then half produce blocks 2 times as big, half produce empty\nblocks, and the max size doesn't change. If it was 20, then a small\nminority of miners could force a max size increase.  (if it is less than 2,\nthen a minority of minors can force the block size down)\n\n\nAs for whether there \"should\" be fee pressure now or not: I have no\nopinion, besides \"we should make block propagation faster so there is no\ntechnical reason for miners to produce tiny blocks.\" I don't think us\ndevelopers should be deciding things like whether or not fees are too high,\ntoo low, .....\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/ef414a76/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-28T17:34:42",
                "message_text_only": ">\n> Twenty is scary.\n>\n\nTo whom? The only justification for the max size is DoS attacks, right?\nBack when Bitcoin had an average block size of 10kb, the max block size was\n100x the average. Things worked fine, nobody was scared.\n\nThe max block size is really a limit set by hardware capability, which is\nsomething that's difficult to measure in software. I think I preferred your\noriginal formula that guesstimated based on previous trends to one that\njust tries to follow some average.\n\nAs noted, many miners just accept the defaults. With your proposed change\ntheir target would effectively *drop* from 1mb to 800kb today, which seems\ncrazy. That's the exact opposite of what is needed right now.\n\nI am very skeptical about this idea.\n\n\n> I don't think us developers should be deciding things like whether or not\n> fees are too high, too low,\n>\n\nMiners can already attempt to apply fee pressure by just not mining\ntransactions that they feel don't pay enough. Some sort of auto-cartel that\nattempts to restrict supply based on everyone looking at everyone else\nfeels overly complex and prone to strange situations: it looks a lot like\nsome kind of Mexican standoff to me.\n\nAdditionally, the justification for the block size limit was DoS by someone\nmining \"troll blocks\". It was never meant to be about fee pressure.\nResource management inside Bitcoin Core is certainly something to be\nhandled by developers.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/3359ad4a/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-28T18:23:59",
                "message_text_only": "On Thu, May 28, 2015 at 1:34 PM, Mike Hearn <mike at plan99.net> wrote:\n\n> As noted, many miners just accept the defaults. With your proposed change\n>> their target would effectively *drop* from 1mb to 800kb today, which\n>> seems crazy. That's the exact opposite of what is needed right now.\n>>\n>\n> I am very skeptical about this idea.\n>\n\nBy the time a hard fork can happen, I expect average block size will be\nabove 500K.\n\nWould you support a rule that was \"larger of 1MB or 2x average size\" ? That\nis strictly better than the situation we're in today.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/fada3906/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-29T11:26:40",
                "message_text_only": ">\n> By the time a hard fork can happen, I expect average block size will be\n> above 500K.\n>\n\nYes, possibly.\n\n\n> Would you support a rule that was \"larger of 1MB or 2x average size\" ?\n> That is strictly better than the situation we're in today.\n>\n\nIt is, but only by a trivial amount - hitting the limit is still very\nlikely. I don't want to see this issue come up over and over again. Ideally\nnever. We shouldn't be artificially throttling organic growth of the\nnetwork, especially not by accident.\n\nIMO it's not even clear there needs to be a size limit at all. Currently\nthe 32mb message cap imposes one anyway, but if miners can always just\ndiscourage blocks over some particular size if they want to.\n\nBut I can get behind a 20mb limit (or 20mb+N) as it represents a reasonable\ncompromise: the limit still exists, it's far below VISA capacity etc, but\nit should also free up enough space that everyone can get back to what we\n*should* be focusing on, which is user growth!\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/9ca670f1/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-29T11:42:09",
                "message_text_only": "On Fri, May 29, 2015 at 12:26 PM, Mike Hearn <mike at plan99.net> wrote:\n\n> IMO it's not even clear there needs to be a size limit at all. Currently\n> the 32mb message cap imposes one anyway\n>\n\nIf the plan is a fix once and for all, then that should be changed too.  It\ncould be set so that it is at least some multiple of the max block size\nallowed.\n\nAlternatively, the merkle block message already incorporates the required\nfunctionality.\n\nSend\n- headers message (with 1 header)\n- merkleblock messages (max 1MB per message)\n\nThe transactions for each merkleblock could be sent directly before each\nmerkleblock, as is currently the case.\n\nThat system can send a block of any size.  It would require a change to the\nprocessing of any merkleblocks received.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/21895008/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-29T11:57:42",
                "message_text_only": ">\n> If the plan is a fix once and for all, then that should be changed too.\n> It could be set so that it is at least some multiple of the max block size\n> allowed.\n>\n\nWell, but RAM is not infinite :-) Effectively what these caps are doing is\nsetting the minimum hardware requirements for running a Bitcoin node.\n\nThat's OK by me - I don't think we are actually going to exhaust the\nhardware abilities of any reasonable computer any time soon, but still,\nhaving the software recognise the finite nature of a computing machine\ndoesn't seem unwise.\n\n\n> That system can send a block of any size.  It would require a change to\n> the processing of any merkleblocks received.\n>\n\nNot \"any\" size because, again, the remote node must buffer things up and\nhave the transaction data actually in memory in order to digest it. But a\nmuch larger size, yes.\n\nHowever, that's a bigger change.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/6aebe66a/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-29T12:39:30",
                "message_text_only": "What do other people think?\n\n\nIf we can't come to an agreement soon, then I'll ask for help\nreviewing/submitting patches to Mike's Bitcoin-Xt project that implement a\nbig increase now that grows over time so we may never have to go through\nall this rancor and debate again.\n\nI'll then ask for help lobbying the merchant services and exchanges and\nhosted wallet companies and other bitcoind-using-infrastructure companies\n(and anybody who agrees with me that we need bigger blocks sooner rather\nthan later) to run Bitcoin-Xt instead of Bitcoin Core, and state that they\nare running it. We'll be able to see uptake on the network by monitoring\nclient versions.\n\nPerhaps by the time that happens there will be consensus bigger blocks are\nneeded sooner rather than later; if so, great! The early deployment will\njust serve as early testing, and all of the software already deployed will\nready for bigger blocks.\n\nBut if there is still no consensus among developers but the \"bigger blocks\nnow\" movement is successful, I'll ask for help getting big miners to do the\nsame, and use the soft-fork block version voting mechanism to (hopefully)\nget a majority and then a super-majority willing to produce bigger blocks.\nThe purpose of that process is to prove to any doubters that they'd better\nstart supporting bigger blocks or they'll be left behind, and to give them\na chance to upgrade before that happens.\n\n\nBecause if we can't come to consensus here, the ultimate authority for\ndetermining consensus is what code the majority of merchants and exchanges\nand miners are running.\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/3480fef4/attachment.html>"
            },
            {
                "author": "insecurity at national.shitposting.agency",
                "date": "2015-05-29T14:00:54",
                "message_text_only": "Are you really that pig headed that you are going to try and blow up the \nentire system just to get your way? A bunch of ignorant redditors do not \nmake consensus, mercifully.\n\n\nOn 2015-05-29 12:39, Gavin Andresen wrote:\n> What do other people think?\n> \n> If we can't come to an agreement soon, then I'll ask for help\n> reviewing/submitting patches to Mike's Bitcoin-Xt project that\n> implement a big increase now that grows over time so we may never have\n> to go through all this rancor and debate again.\n> \n> I'll then ask for help lobbying the merchant services and exchanges\n> and hosted wallet companies and other bitcoind-using-infrastructure\n> companies (and anybody who agrees with me that we need bigger blocks\n> sooner rather than later) to run Bitcoin-Xt instead of Bitcoin Core,\n> and state that they are running it. We'll be able to see uptake on the\n> network by monitoring client versions.\n> \n> Perhaps by the time that happens there will be consensus bigger blocks\n> are needed sooner rather than later; if so, great! The early\n> deployment will just serve as early testing, and all of the software\n> already deployed will ready for bigger blocks.\n> \n> But if there is still no consensus among developers but the \"bigger\n> blocks now\" movement is successful, I'll ask for help getting big\n> miners to do the same, and use the soft-fork block version voting\n> mechanism to (hopefully) get a majority and then a super-majority\n> willing to produce bigger blocks. The purpose of that process is to\n> prove to any doubters that they'd better start supporting bigger\n> blocks or they'll be left behind, and to give them a chance to upgrade\n> before that happens.\n> \n> Because if we can't come to consensus here, the ultimate authority for\n> determining consensus is what code the majority of merchants and\n> exchanges and miners are running.\n> \n> --\n> \n> --\n> Gavin Andresen\n> \n> ------------------------------------------------------------------------------\n> \n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Braun Brelin",
                "date": "2015-05-29T14:15:43",
                "message_text_only": "How is this being pigheaded? In my opinion, this is leadership.  If\n*something* isn't implemented soon, the network is going to have some real\nproblems, right at the\ntime when adoption is starting to accelerate.  I've been seeing nothing but\nnavel-gazing and circlejerks on this issue for weeks now.  Gavin or Mike or\nsomeone at some\npoint needs to step up and say \"follow me\".\n\nBraun Brelin\n\n\nOn Fri, May 29, 2015 at 5:00 PM, <insecurity at national.shitposting.agency>\nwrote:\n\n> Are you really that pig headed that you are going to try and blow up the\n> entire system just to get your way? A bunch of ignorant redditors do not\n> make consensus, mercifully.\n>\n>\n> On 2015-05-29 12:39, Gavin Andresen wrote:\n> > What do other people think?\n> >\n> > If we can't come to an agreement soon, then I'll ask for help\n> > reviewing/submitting patches to Mike's Bitcoin-Xt project that\n> > implement a big increase now that grows over time so we may never have\n> > to go through all this rancor and debate again.\n> >\n> > I'll then ask for help lobbying the merchant services and exchanges\n> > and hosted wallet companies and other bitcoind-using-infrastructure\n> > companies (and anybody who agrees with me that we need bigger blocks\n> > sooner rather than later) to run Bitcoin-Xt instead of Bitcoin Core,\n> > and state that they are running it. We'll be able to see uptake on the\n> > network by monitoring client versions.\n> >\n> > Perhaps by the time that happens there will be consensus bigger blocks\n> > are needed sooner rather than later; if so, great! The early\n> > deployment will just serve as early testing, and all of the software\n> > already deployed will ready for bigger blocks.\n> >\n> > But if there is still no consensus among developers but the \"bigger\n> > blocks now\" movement is successful, I'll ask for help getting big\n> > miners to do the same, and use the soft-fork block version voting\n> > mechanism to (hopefully) get a majority and then a super-majority\n> > willing to produce bigger blocks. The purpose of that process is to\n> > prove to any doubters that they'd better start supporting bigger\n> > blocks or they'll be left behind, and to give them a chance to upgrade\n> > before that happens.\n> >\n> > Because if we can't come to consensus here, the ultimate authority for\n> > determining consensus is what code the majority of merchants and\n> > exchanges and miners are running.\n> >\n> > --\n> >\n> > --\n> > Gavin Andresen\n> >\n> >\n> ------------------------------------------------------------------------------\n> >\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/6353ffbd/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-29T14:09:20",
                "message_text_only": "On Fri, May 29, 2015 at 1:39 PM, Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> But if there is still no consensus among developers but the \"bigger blocks\n> now\" movement is successful, I'll ask for help getting big miners to do the\n> same, and use the soft-fork block version voting mechanism to (hopefully)\n> get a majority and then a super-majority willing to produce bigger blocks.\n> The purpose of that process is to prove to any doubters that they'd better\n> start supporting bigger blocks or they'll be left behind, and to give them\n> a chance to upgrade before that happens.\n>\n\nHow do you define that the movement is successful?\n\nFor\n\n\n> Because if we can't come to consensus here, the ultimate authority for\n> determining consensus is what code the majority of merchants and exchanges\n> and miners are running.\n>\n\nThe measure is miner consensus.  How do you intend to measure\nexchange/merchant acceptance?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/84672694/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-29T14:20:01",
                "message_text_only": "On Fri, May 29, 2015 at 10:09 AM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n>  How do you intend to measure exchange/merchant acceptance?\n>\n\nPublic statements saying \"we're running software that is ready for bigger\nblocks.\"\n\nAnd looking at the version (aka user-agent) strings of publicly reachable\nnodes on the network.\n(e.g. see the count at  https://getaddr.bitnodes.io/nodes/ )\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/f02fc1b9/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-29T14:22:22",
                "message_text_only": ">\n> And looking at the version (aka user-agent) strings of publicly reachable\n> nodes on the network.\n> (e.g. see the count at  https://getaddr.bitnodes.io/nodes/ )\n>\n\nYeah, though FYI Luke informed me last week that I somehow managed to take\nout the change to the user-agent string in Bitcoin XT, presumably I made a\nmistake during a rebase of the rebranding change. So the actual number of\nXT nodes is a bit higher than counting user-agent strings would suggest.\n\nI sort of neglected XT lately. If we go ahead with this then I'll fix\nthings like this.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/83bc21bf/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-29T14:21:02",
                "message_text_only": ">\n> The measure is miner consensus.  How do you intend to measure\n> exchange/merchant acceptance?\n>\n\nAsking them.\n\nIn fact, we already have. I have been talking to well known people and CEOs\nin the Bitcoin community for some time now. *All* of them support bigger\nblocks, this includes:\n\n   - Every wallet developer I have asked (other than Bitcoin Core)\n   - So far, every payment processor and every exchange company\n\nI know Gavin has also been talking to people about this.\n\nThere's a feeling on this list that there's no consensus, or that Gavin and\nmyself are on the wrong side of it. I'd put it differently - there's very\nstrong consensus out in the wider community and this list is something of\nan aberration.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/d159d41b/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-29T14:22:27",
                "message_text_only": "On Fri, May 29, 2015 at 3:09 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n>\n>\n> On Fri, May 29, 2015 at 1:39 PM, Gavin Andresen <gavinandresen at gmail.com>\n> wrote:\n>\n>> But if there is still no consensus among developers but the \"bigger\n>> blocks now\" movement is successful, I'll ask for help getting big miners to\n>> do the same, and use the soft-fork block version voting mechanism to\n>> (hopefully) get a majority and then a super-majority willing to produce\n>> bigger blocks. The purpose of that process is to prove to any doubters that\n>> they'd better start supporting bigger blocks or they'll be left behind, and\n>> to give them a chance to upgrade before that happens.\n>>\n>\n> How do you define that the movement is successful?\n>\n\nSorry again, I keep auto-sending from gmail when trying to delete.\n\nIn theory, using the \"nuclear option\", the block size can be increased via\nsoft fork.\n\nVersion 4 blocks would contain the hash of the a valid extended block in\nthe coinbase.\n\n<block height> <32 byte extended hash>\n\nTo send coins to the auxiliary block, you send them to some template.\n\nOP_P2SH_EXTENDED <scriptPubKey hash> OP_TRUE\n\nThis transaction can be spent by anyone (under the current rules).  The\nsoft fork would lock the transaction output unless it transferred money\nfrom the extended block.\n\nTo unlock the transaction output, you need to include the txid of\ntransaction(s) in the extended block and signature(s) in the scriptSig.\n\nThe transaction output can be spent in the extended block using P2SH\nagainst the scriptPubKey hash.\n\nThis means that people can choose to move their money to the extended\nblock.  It might have lower security than leaving it in the root chain.\n\nThe extended chain could use the updated script language too.\n\nThis is obviously more complex than just increasing the size though, but it\ncould be a fallback option if no consensus is reached.  It has the\nadvantage of giving people a choice.  They can move their money to the\nextended chain or not, as they wish.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/8bedd264/attachment.html>"
            },
            {
                "author": "Admin Istrator",
                "date": "2015-05-29T17:53:55",
                "message_text_only": "What about trying the dynamic scaling method within the 20MB range + 1 year\nwith a 40% increase of that cap?  Until a way to dynamically scale is\nfound, the cap will only continue to be an issue.  With 20 MB + 40% yoy,\nwe're either imposing an arbitrary cap later, or achieving less than great\nDOS protection always.  Why not set that policy as a maximum for 2 years as\na protection against the possibility of dynamic scaling abuse, and see what\nhappens with a dynamic method in the mean time.  The policy of Max(1MB,\n(average size over previous 144 blocks) * 2) calculated at each block seems\npretty reasonable.\n\nAs an outsider, the real 'median' here seems to be 'keeping the cap as\nsmall as possible while allowing for larger blocks still'.    We know\nminers will want to keep space in their blocks relatively scarce, but we\nalso know that doesn't exclude the more powerful miners from\nincluding superfluous transactions to increase their effective share of the\nnetwork.  I have the luck of not being drained by this topic over the past\nthree years, so it looks to me as if its two poles of 'block size must\nincrease' and 'block size must not increase' are forcing what is the clear\nroute to establishing the 'right' block size off the table.\n\n--Andrew Len\n(sorry if anybody received this twice, sent as the wrong email the first\ntime around).\n\nOn Fri, May 29, 2015 at 5:39 AM, Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> What do other people think?\n>\n>\n> If we can't come to an agreement soon, then I'll ask for help\n> reviewing/submitting patches to Mike's Bitcoin-Xt project that implement a\n> big increase now that grows over time so we may never have to go through\n> all this rancor and debate again.\n>\n> I'll then ask for help lobbying the merchant services and exchanges and\n> hosted wallet companies and other bitcoind-using-infrastructure companies\n> (and anybody who agrees with me that we need bigger blocks sooner rather\n> than later) to run Bitcoin-Xt instead of Bitcoin Core, and state that they\n> are running it. We'll be able to see uptake on the network by monitoring\n> client versions.\n>\n> Perhaps by the time that happens there will be consensus bigger blocks are\n> needed sooner rather than later; if so, great! The early deployment will\n> just serve as early testing, and all of the software already deployed will\n> ready for bigger blocks.\n>\n> But if there is still no consensus among developers but the \"bigger blocks\n> now\" movement is successful, I'll ask for help getting big miners to do the\n> same, and use the soft-fork block version voting mechanism to (hopefully)\n> get a majority and then a super-majority willing to produce bigger blocks.\n> The purpose of that process is to prove to any doubters that they'd better\n> start supporting bigger blocks or they'll be left behind, and to give them\n> a chance to upgrade before that happens.\n>\n>\n> Because if we can't come to consensus here, the ultimate authority for\n> determining consensus is what code the majority of merchants and exchanges\n> and miners are running.\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/3d4a11fb/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-30T09:03:36",
                "message_text_only": "> or achieving less than great DOS protection\n\nRight now a bunch of redditors can DOS the network at the cost of a few\nthousand dollars per day, shared between them. Since the cost of validating\ntransactions is far lower than current minimum relay fees, then increasing\nthe block size increases the cost of DOSing the network.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Fri, May 29, 2015 at 10:53 AM, Admin Istrator <andy at ftlio.com> wrote:\n\n> What about trying the dynamic scaling method within the 20MB range + 1\n> year with a 40% increase of that cap?  Until a way to dynamically scale is\n> found, the cap will only continue to be an issue.  With 20 MB + 40% yoy,\n> we're either imposing an arbitrary cap later, or achieving less than great\n> DOS protection always.  Why not set that policy as a maximum for 2 years as\n> a protection against the possibility of dynamic scaling abuse, and see what\n> happens with a dynamic method in the mean time.  The policy of Max(1MB,\n> (average size over previous 144 blocks) * 2) calculated at each block seems\n> pretty reasonable.\n>\n> As an outsider, the real 'median' here seems to be 'keeping the cap as\n> small as possible while allowing for larger blocks still'.    We know\n> miners will want to keep space in their blocks relatively scarce, but we\n> also know that doesn't exclude the more powerful miners from\n> including superfluous transactions to increase their effective share of the\n> network.  I have the luck of not being drained by this topic over the past\n> three years, so it looks to me as if its two poles of 'block size must\n> increase' and 'block size must not increase' are forcing what is the clear\n> route to establishing the 'right' block size off the table.\n>\n> --Andrew Len\n> (sorry if anybody received this twice, sent as the wrong email the first\n> time around).\n>\n> On Fri, May 29, 2015 at 5:39 AM, Gavin Andresen <gavinandresen at gmail.com>\n> wrote:\n>\n>> What do other people think?\n>>\n>>\n>> If we can't come to an agreement soon, then I'll ask for help\n>> reviewing/submitting patches to Mike's Bitcoin-Xt project that implement a\n>> big increase now that grows over time so we may never have to go through\n>> all this rancor and debate again.\n>>\n>> I'll then ask for help lobbying the merchant services and exchanges and\n>> hosted wallet companies and other bitcoind-using-infrastructure companies\n>> (and anybody who agrees with me that we need bigger blocks sooner rather\n>> than later) to run Bitcoin-Xt instead of Bitcoin Core, and state that they\n>> are running it. We'll be able to see uptake on the network by monitoring\n>> client versions.\n>>\n>> Perhaps by the time that happens there will be consensus bigger blocks\n>> are needed sooner rather than later; if so, great! The early deployment\n>> will just serve as early testing, and all of the software already deployed\n>> will ready for bigger blocks.\n>>\n>> But if there is still no consensus among developers but the \"bigger\n>> blocks now\" movement is successful, I'll ask for help getting big miners to\n>> do the same, and use the soft-fork block version voting mechanism to\n>> (hopefully) get a majority and then a super-majority willing to produce\n>> bigger blocks. The purpose of that process is to prove to any doubters that\n>> they'd better start supporting bigger blocks or they'll be left behind, and\n>> to give them a chance to upgrade before that happens.\n>>\n>>\n>> Because if we can't come to consensus here, the ultimate authority for\n>> determining consensus is what code the majority of merchants and exchanges\n>> and miners are running.\n>>\n>>\n>> --\n>> --\n>> Gavin Andresen\n>>\n>>\n>> ------------------------------------------------------------------------------\n>>\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/bf19ef13/attachment.html>"
            },
            {
                "author": "Bryan Cheng",
                "date": "2015-05-29T18:47:31",
                "message_text_only": "On Fri, May 29, 2015 at 5:39 AM, Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> What do other people think?\n>\n>\n> If we can't come to an agreement soon, then I'll ask for help\n> reviewing/submitting patches to Mike's Bitcoin-Xt project that implement a\n> big increase now that grows over time so we may never have to go through\n> all this rancor and debate again.\n>\n> I'll then ask for help lobbying the merchant services and exchanges and\n> hosted wallet companies and other bitcoind-using-infrastructure companies\n> (and anybody who agrees with me that we need bigger blocks sooner rather\n> than later) to run Bitcoin-Xt instead of Bitcoin Core, and state that they\n> are running it. We'll be able to see uptake on the network by monitoring\n> client versions.\n>\n>\n>\nWhile I think we'd all prefer Core to make changes like this, the current\nenvironment may make that impossible. If this change happens in XT, we will\nsupport the necessary changes in our own implementation. The block size\nlimit is a problem _today_, and I'd rather we solve today's problems with\ntoday's understanding rather than let speculation about future unknowns\nstop our ability to respond to known issues.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/64fa5360/attachment.html>"
            },
            {
                "author": "Cameron Garnham",
                "date": "2015-05-30T01:36:39",
                "message_text_only": "First off, I am glad that the idea of dynamic block size adjustment is\ngaining some attention, in particular the model that I proposed.\n\nI wanted to take some time and explain some of the philosophy of how,\nand why, I proposed this this particular model.\n\nWhen Bitcoin was first made, there was a 32MB block size limit; this\nwas quickly found to be open to spam (and potentially DOS, as the code\nwas not-at-all optimized to support large blocks), and was reduced to\n1MB, this was a quick fix that was never intended to last; at some\npoint the network should come to an understanding, a consensus if you\nwill, of what (and how much) belongs in a block.\nThe core point of this is that miners have always, and will always;\nhold the power, to decide what goes into blocks; this implicitly,\nobviously, includes how large blocks are. Miners are able to come any\nsort of agreement they wish, providing the bitcoin clients accept\ntheir blocks as valid.\n\nSay if Satoshi never decided to place the 1MB block limit: It would be\nup to the miners to decide what they consider a \u2018reasonable\u2019 block is.\nHowever, they would need to find some way to communicate this and\nreach an agreement; some protocol.  They, say, could have done this\ninformally on what is now the bitcointalk forum, or used Twitter.\nHowever, what they really need is indeed a \"consensus protocol\". Some\nsimple terms to define what is acceptable and what is not.\n\nHence, the proposal introducing a consensus protocol for block sizes;\ninstead of just having a hard limit (enforced by everyone), instead,\nwe have a constant factor above the average block size over a fixed\nintervals that is soft-forked by only the miners. (The next simplest\nmathematical construct).\nThis proposal is entirely a soft-fork and may be implemented without\nchanging any client code what so ever. In-fact, it could be\nimplemented by only a simple 51% majority of miners, with-or-without\ngaining the wider community consensus. (Assuming that the 1MB block\nsize rule still applies).\nThe nice thing about this is that it really is impossible to stop,\nfor-example, if pre-relaying of block headers is implemented; the\nminers could always soft-fork to include the block-size in the\ncoinbase. The only reason that the miners have not done this yet, is\nthat there has not yet been a strong will to increase transaction fees.\n\nIf we assume the miners will operate in a way to collectively maximize\nprofit; then we can assume they will not try to maximize utility of\nthe network (having as many transactions as possible), rather have as\nfew transactions as the total economy can support the cost.  Meaning\nthat limiting to much smaller blocks will probably be much more\nprofitable than having large blocks.\n\nSince there is no requirement for the clients to know about the block\nsize consensus protocol, this truly can be a\n\u2018bi-directional-soft-fork\u2019, in that the miners can choose to change\nthe rules at any time, with only a simple 51% majority. Therefore, any\nparameters that we pick are always up for debate.\n\nWhy the 1.5x over 2016 blocks? -  Using some game theory, and\ndeduction: I wished to pick the type of agreement that would be\nnatural for the miners to come to (selfishly).\n\nFirst, Why 1.5x, this means that only a super-majority of miners can\neasily increase the block size. \u2013 There is no natural incentive for\nminers to produce large blocks that have very few fees.\n\nSecond, Why 2016 blocks for adjusting the average:  Miners HATE\nunpredictability, for shorter time periods the miner will need to have\ninfrastructure ready to support potentially much larger block almost\nimmediately. 2016 blocks is a period that the miners are already well\nused to, meaning that it will take slightly less than a month for\nblocks of double size to be permitted.\n\nThis entire infrastructure can be implemented without needing to\nupdate any clients; once implemented, tested, solid, and well accepted\nby the (mining) community then we can revisit increasing the 1M hard\nlimit. (If we still have demand for it, maybe the average block size\nwill reduce to say, 100KB).\n\n\nCam.\n\n\n\n\n\n\n> -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256\n> \n> While being in the Bitcoin community for a long time, I haven't\n> been so directly involved in the development.  However I wish to\n> suggest a different pre-hard-fork soft-fork approach:\n> \n> \n> Set a 'block size cap' in the similar same way as we set\n> difficulty.\n> \n> Every 2016 blocks take the average size of the blocks and multiply\n> the size by 1.5x, rejecting blocks that are larger than this size,\n> for the next 2016 period.\n> \n> I would of-course suggest that we keep the limits at min 100kb and\n> max (initially) 990kb (not 1mb on purpose, as this should become\n> the new limit), rounding up to the nearest 10kb.\n> \n> A: we don't have pressure at the 1mb limit, (we reduce the limit in\n> a flexible manner to 990kb).\n> \n> B: we can upgrade the network to XYZ hard-limit, then slowly raze\n> the soft-limit after being sure the network, as-a-whole is ready.\n> \n> If we on-day remove the block-size limit, this rule will stop a\n> rouge miner from making 10mb, or 100mb blocks, or 1gb blocks.\n> \n> This could be implemented by the miners without breaking any of\n> the clients, and would tend to produce a better dynamic fee\n> pressure.\n> \n> \n> This will give the mechanics to the miners to create consensus to \n> agree what block-sizes they believe are best for the network, and \n> allows the block-sizes to dynamically grow in response to larger\n> demand.\n> \n> \n> \n> On 5/8/2015 10:35 AM, Pieter Wuille wrote:\n>> On May 7, 2015 3:08 PM, \"Roy Badami\" <roy at gnomon.org.uk> wrote:\n>>> \n>>> On Thu, May 07, 2015 at 11:49:28PM +0200, Pieter Wuille wrote:\n>>>> I would not modify my node if the change introduced a\n>>>> perpetual 100 BTC subsidy per block, even if 99% of miners\n>>>> went along with it.\n>>> \n>>> Surely, in that scenario Bitcoin is dead.  If the fork you\n>>> prefer has only 1% of the hash power it is trivially vulnerably\n>>> not just to a 51% attack but to a 501% attack, not to mention\n>>> the fact that you'd only be getting one block every 16 hours.\n>> \n>> Yes, indeed, Bitcoin would be dead if this actually happens. But \n>> that is still where the power lies: before anyone (miners or \n>> others) would think about trying such a change, they would need\n>> to convince people and be sure they will effectively modify\n>> their code.\n>> \n>> \n>> \n>> ----------------------------------------------------------------------\n>\n>> \n- --------\n>> \n>> \n> One dashboard for servers and applications across\n> Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications \n>> Performance metrics, stats and reports that give you Actionable \n>> Insights Deep dive visibility with transaction tracing using APM \n>> Insight. http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> \n>> \n>> \n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net \n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>> \n> -----BEGIN PGP SIGNATURE----- Version: GnuPG v2\n> \n> iF4EAREIAAYFAlVMKZYACgkQBJ8cMDO159aTiQEApTITEBrhE1DRbj/w+GncNeqB \n> 0hGvmIBa1z0hGww0kaMBAOhxjn/K5leRJgdt1fKhNEDKKHdeCOIX3QRgry90D3NO \n> =p0+H -----END PGP SIGNATURE-----\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 213 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/f41e56f7/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-28T17:50:00",
                "message_text_only": "On Thu, May 28, 2015 at 01:19:44PM -0400, Gavin Andresen wrote:\n> As for whether there \"should\" be fee pressure now or not: I have no\n> opinion, besides \"we should make block propagation faster so there is no\n> technical reason for miners to produce tiny blocks.\" I don't think us\n> developers should be deciding things like whether or not fees are too high,\n> too low, .....\n\nNote that the majority of hashing power is using Matt Corallo's block\nrelay network, something I confirmed the other day through my mining\ncontacts. Interestingly, the miners that aren't using it include some of\nthe largest pools; I haven't yet gotten an answer as to what their\nrational for not using it was exactly.\n\nImportantly, this does mean that block propagation is probably fairly\nclose to optimal already, modulo major changes to the consensus\nprotocol; IBLT won't improve the situation much, if any.\n\nIt's also notable that we're already having issues with miners turning\nvalidation off as a way to lower their latency; I've been asked myself\nabout the possibility of creating an \"SPV miner\" that skips validation\nwhile new blocks are propagating to shave off time and builds directly\noff of block headers corresponding to blocks with unknown contents.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000327487b689490b73f9d336b3008f82114fd3ada336bcac0\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/7268e906/attachment.sig>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2015-05-28T17:14:05",
                "message_text_only": "Le 28/05/2015 17:53, Gavin Andresen a \u00e9crit :\n> \n> So my straw-man proposal would be:  max size 2x average size over last 144\n> blocks, calculated at every block.\n> \n\nI like that idea.\n\nAverage is a better choice than median. The median is not well defined\non discrete sets, as shown in your example, and there is no need to be\nrobust to outliers, thanks to the max size."
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-28T17:34:32",
                "message_text_only": "> until we have size-independent new block propagation\n\nI don't really believe that is possible. I'll argue why below. To be clear,\nthis is not an argument against increasing the block size, only against\nusing the assumption of size-independent propagation.\n\nThere are several significant improvements likely possible to various\naspects of block propagation, but I don't believe you can make any part\ncompletely size-independent. Perhaps the remaining aspects result in terms\nin the total time that vanish compared to the link latencies for 1 MB\nblocks, but there will be some block sizes for which this is no longer the\ncase, and we need to know where that is the case.\n\n* You can't assume that every transaction is pre-relayed and pre-validated.\nThis can happen due to non-uniform relay policies (different codebases, and\nfuture things like size-limited mempools), double spend attempts, and\ntransactions generated before a block had time to propagate. You've\npreviously argued for a policy of not including too recent transactions,\nbut that requires a bound on network diameter, and if these late\ntransactions are profitable, it has exactly the same problem as making\nlarger blocks non-proportionally more economic for larger pools groups if\npropagation time is size dependent).\n  * This results in extra bandwidth usage for efficient relay protocols,\nand if discrepancy estimation mispredicts the size of IBLT or error\ncorrection data needed, extra roundtrips.\n  * Signature validation for unrelayed transactions will be needed at block\nrelay time.\n  * Database lookups for the inputs of unrelayed transactions cannot be\ncached in advance.\n\n* Block validation with 100% known and pre-validated transactions is not\nconstant time, due to updates that need to be made to the UTXO set (and\nfuture ideas like UTXO commitments would make this effect an order of\nmagnitude worse).\n\n* More efficient relay protocols also have higher CPU cost for\nencoding/decoding.\n\nAgain, none of this is a reason why the block size can't increase. If\navailability of hardware with higher bandwidth, faster disk/ram access\ntimes, and faster CPUs increases, we should be able to have larger blocks\nwith the same propagation profile as smaller blocks with earlier technology.\n\nBut we should know how technology scales with larger blocks, and I don't\nbelieve we do, apart from microbenchmarks in laboratory conditions.\n\n-- \nPieter\n On Fri, May 8, 2015 at 3:20 AM, Matt Whitlock <bip at mattwhitlock.name>\nwrote:\n\n> Between all the flames on this list, several ideas were raised that did\n> not get much attention. I hereby resubmit these ideas for consideration and\n> discussion.\n>\n> - Perhaps the hard block size limit should be a function of the actual\n> block sizes over some trailing sampling period. For example, take the\n> median block size among the most recent 2016 blocks and multiply it by 1.5.\n> This allows Bitcoin to scale up gradually and organically, rather than\n> having human beings guessing at what is an appropriate limit.\n>\n\nA lot of people like this idea, or something like it. It is nice and\nsimple, which is really important for consensus-critical code.\n\nWith this rule in place, I believe there would be more \"fee pressure\"\n(miners would be creating smaller blocks) today. I created a couple of\nhistograms of block sizes to infer what policy miners are ACTUALLY\nfollowing today with respect to block size:\n\nLast 1,000 blocks:\n  http://bitcoincore.org/~gavin/sizes_last1000.html\n\nNotice a big spike at 750K -- the default size for Bitcoin Core.\nThis graph might be misleading, because transaction volume or fees might\nnot be high enough over the last few days to fill blocks to whatever limit\nminers are willing to mine.\n\nSo I graphed a time when (according to statoshi.info) there WERE a lot of\ntransactions waiting to be confirmed:\n   http://bitcoincore.org/~gavin/sizes_357511.html\n\nThat might also be misleading, because it is possible there were a lot of\ntransactions waiting to be confirmed because miners who choose to create\nsmall blocks got lucky and found more blocks than normal.  In fact, it\nlooks like that is what happened: more smaller-than-normal blocks were\nfound, and the memory pool backed up.\n\nSo: what if we had a dynamic maximum size limit based on recent history?\n\nThe average block size is about 400K, so a 1.5x rule would make the max\nblock size 600K; miners would definitely be squeezing out transactions /\nputting pressure to increase transaction fees. Even a 2x rule (implying\n800K max blocks) would, today, be squeezing out transactions / putting\npressure to increase fees.\n\nUsing a median size instead of an average means the size can increase or\ndecrease more quickly. For example, imagine the rule is \"median of last\n2016 blocks\" and 49% of miners are producing 0-size blocks and 51% are\nproducing max-size blocks. The median is max-size, so the 51% have total\ncontrol over making blocks bigger.  Swap the roles, and the median is\nmin-size.\n\nBecause of that, I think using an average is better-- it means the max size\nwill change (up or down) more slowly.\n\nI also think 2016 blocks is too long, because transaction volumes change\nquicker than that. An average over 144 blocks (last 24 hours) would be\nbetter able to handle increased transaction volume around major holidays,\nand would also be able to react more quickly if an economically irrational\nattacker attempted to flood the network with fee-paying transactions.\n\nSo my straw-man proposal would be:  max size 2x average size over last 144\nblocks, calculated at every block.\n\nThere are a couple of other changes I'd pair with that consensus change:\n\n+ Make the default mining policy for Bitcoin Core neutral-- have its target\nblock size be the average size, so miners that don't care will \"go along\nwith the people who do care.\"\n\n+ Use something like Greg's formula for size instead of bytes-on-the-wire,\nto discourage bloating the UTXO set.\n\n\n---------\n\nWhen I've proposed (privately, to the other core committers) some dynamic\nalgorithm the objection has been \"but that gives miners complete control\nover the max block size.\"\n\nI think that worry is unjustified right now-- certainly, until we have\nsize-independent new block propagation there is an incentive for miners to\nkeep their blocks small, and we see miners creating small blocks even when\nthere are fee-paying transactions waiting to be confirmed.\n\nI don't even think it will be a problem if/when we do have size-independent\nnew block propagation, because I think the combination of the random timing\nof block-finding plus a dynamic limit as described above will create a\nhealthy system.\n\nIf I'm wrong, then it seems to me the miners will have a very strong\nincentive to, collectively, impose whatever rules are necessary (maybe a\nsoft-fork to put a hard cap on block size) to make the system healthy again.\n\n\n-- \n--\nGavin Andresen\n\n\n------------------------------------------------------------------------------\n\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/5c893428/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-29T17:45:39",
                "message_text_only": "> miners would definitely be squeezing out transactions / putting pressure\nto increase transaction fees\n\nI'd just like to re-iterate that transactions getting \"squeezed out\"\n(failure after a lengthy period of uncertainty) is a radical change from\nthe current behavior of the network. There are plenty of avenues to create\nfee pressure without resorting to such a drastic change in how the network\nworks today.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Thu, May 28, 2015 at 8:53 AM, Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> On Fri, May 8, 2015 at 3:20 AM, Matt Whitlock <bip at mattwhitlock.name>\n> wrote:\n>\n>> Between all the flames on this list, several ideas were raised that did\n>> not get much attention. I hereby resubmit these ideas for consideration and\n>> discussion.\n>>\n>> - Perhaps the hard block size limit should be a function of the actual\n>> block sizes over some trailing sampling period. For example, take the\n>> median block size among the most recent 2016 blocks and multiply it by 1.5.\n>> This allows Bitcoin to scale up gradually and organically, rather than\n>> having human beings guessing at what is an appropriate limit.\n>>\n>\n> A lot of people like this idea, or something like it. It is nice and\n> simple, which is really important for consensus-critical code.\n>\n> With this rule in place, I believe there would be more \"fee pressure\"\n> (miners would be creating smaller blocks) today. I created a couple of\n> histograms of block sizes to infer what policy miners are ACTUALLY\n> following today with respect to block size:\n>\n> Last 1,000 blocks:\n>   http://bitcoincore.org/~gavin/sizes_last1000.html\n>\n> Notice a big spike at 750K -- the default size for Bitcoin Core.\n> This graph might be misleading, because transaction volume or fees might\n> not be high enough over the last few days to fill blocks to whatever limit\n> miners are willing to mine.\n>\n> So I graphed a time when (according to statoshi.info) there WERE a lot of\n> transactions waiting to be confirmed:\n>    http://bitcoincore.org/~gavin/sizes_357511.html\n>\n> That might also be misleading, because it is possible there were a lot of\n> transactions waiting to be confirmed because miners who choose to create\n> small blocks got lucky and found more blocks than normal.  In fact, it\n> looks like that is what happened: more smaller-than-normal blocks were\n> found, and the memory pool backed up.\n>\n> So: what if we had a dynamic maximum size limit based on recent history?\n>\n> The average block size is about 400K, so a 1.5x rule would make the max\n> block size 600K; miners would definitely be squeezing out transactions /\n> putting pressure to increase transaction fees. Even a 2x rule (implying\n> 800K max blocks) would, today, be squeezing out transactions / putting\n> pressure to increase fees.\n>\n> Using a median size instead of an average means the size can increase or\n> decrease more quickly. For example, imagine the rule is \"median of last\n> 2016 blocks\" and 49% of miners are producing 0-size blocks and 51% are\n> producing max-size blocks. The median is max-size, so the 51% have total\n> control over making blocks bigger.  Swap the roles, and the median is\n> min-size.\n>\n> Because of that, I think using an average is better-- it means the max\n> size will change (up or down) more slowly.\n>\n> I also think 2016 blocks is too long, because transaction volumes change\n> quicker than that. An average over 144 blocks (last 24 hours) would be\n> better able to handle increased transaction volume around major holidays,\n> and would also be able to react more quickly if an economically irrational\n> attacker attempted to flood the network with fee-paying transactions.\n>\n> So my straw-man proposal would be:  max size 2x average size over last 144\n> blocks, calculated at every block.\n>\n> There are a couple of other changes I'd pair with that consensus change:\n>\n> + Make the default mining policy for Bitcoin Core neutral-- have its\n> target block size be the average size, so miners that don't care will \"go\n> along with the people who do care.\"\n>\n> + Use something like Greg's formula for size instead of bytes-on-the-wire,\n> to discourage bloating the UTXO set.\n>\n>\n> ---------\n>\n> When I've proposed (privately, to the other core committers) some dynamic\n> algorithm the objection has been \"but that gives miners complete control\n> over the max block size.\"\n>\n> I think that worry is unjustified right now-- certainly, until we have\n> size-independent new block propagation there is an incentive for miners to\n> keep their blocks small, and we see miners creating small blocks even when\n> there are fee-paying transactions waiting to be confirmed.\n>\n> I don't even think it will be a problem if/when we do have\n> size-independent new block propagation, because I think the combination of\n> the random timing of block-finding plus a dynamic limit as described above\n> will create a healthy system.\n>\n> If I'm wrong, then it seems to me the miners will have a very strong\n> incentive to, collectively, impose whatever rules are necessary (maybe a\n> soft-fork to put a hard cap on block size) to make the system healthy again.\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/8755354e/attachment.html>"
            },
            {
                "author": "Steven Pine",
                "date": "2015-05-08T14:57:50",
                "message_text_only": "Block size scaling should be as transparent and simple as possible, like\npegging it to total transactions per difficulty change.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/4c72c004/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-09T00:13:09",
                "message_text_only": "It seems to me all this would do is encourage 0-transaction blocks, crippling the network.  Individual blocks don't have a \"maximum\" block size, they have an actual block size.  Rational miners would pick blocks to minimize difficulty, lowering the \"effective\" maximum block size as defined by the optimal size for rational miners.  This would be a tragedy of the commons.\n\nIn addition to that, average block cinfirmation time, and hence rate of inflation of the bitcoin currency, would now be subject to manipulation.  This undermined a core value of Bitcoin.\n\n> On Fri, May 8, 2015 at 1:33 PM, Mark Friedenbach <mark at friedenbach.org> wrote:\n>\n> \u00a0 * For each block, the miner is allowed to select a different difficulty (nBits) within a certain range, e.g. +/- 25% of the expected difficulty, and this miner-selected difficulty is used for the proof of work check. In addition to adjusting the hashcash target, selecting a different difficulty also raises or lowers the maximum block size for that block by a function of the difference in difficulty."
            },
            {
                "author": "Steven Pine",
                "date": "2015-05-28T16:30:34",
                "message_text_only": "I would support a dynamic block size increase as outlined. I have a few\nquestions though.\n\nIs scaling by average block size the best and easiest method, why not scale\nby transactions confirmed instead? Anyone can write and relay a\ntransaction, and those are what we want to scale for, why not measure it\ndirectly?\n\nI would prefer changes every 2016 blocks, it is a well known change and a\nreasonable time period for planning on changes. Two weeks is plenty fast,\nespecially at a 50% rate increase, in a few months the block size could be\ndramatically larger.\n\nDaily change to size seems confusing especially considering that max block\nsize will be dipping up and down. Also if something breaks trying to fix it\nin a day seems problematic. The hard fork database size difference error\ncomes to mind. Finally daily 50% increases could quickly crowd out smaller\nnodes if changes happen too quickly to adapt for.\n\n\n\n\n\n> Date: Thu, 28 May 2015 11:53:41 -0400\n> From: Gavin Andresen <gavinandresen at gmail.com>\n> Subject:\n> To: Matt Whitlock <bip at mattwhitlock.name>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Message-ID:\n>         <\nCABsx9T3-zxCAagAS0megd06xvG5n-3tUL9NUK9TT3vt7XNL9Tg at mail.gmail.com>\n> Content-Type: text/plain; charset=\"utf-8\"\n>\n> On Fri, May 8, 2015 at 3:20 AM, Matt Whitlock <bip at mattwhitlock.name>\nwrote:\n>\n> > Between all the flames on this list, several ideas were raised that did\n> > not get much attention. I hereby resubmit these ideas for consideration\nand\n> > discussion.\n> >\n> > - Perhaps the hard block size limit should be a function of the actual\n> > block sizes over some trailing sampling period. For example, take the\n> > median block size among the most recent 2016 blocks and multiply it by\n1.5.\n> > This allows Bitcoin to scale up gradually and organically, rather than\n> > having human beings guessing at what is an appropriate limit.\n> >\n>\n> A lot of people like this idea, or something like it. It is nice and\n> simple, which is really important for consensus-critical code.\n>\n> With this rule in place, I believe there would be more \"fee pressure\"\n> (miners would be creating smaller blocks) today. I created a couple of\n> histograms of block sizes to infer what policy miners are ACTUALLY\n> following today with respect to block size:\n>\n> Last 1,000 blocks:\n>   http://bitcoincore.org/~gavin/sizes_last1000.html\n>\n> Notice a big spike at 750K -- the default size for Bitcoin Core.\n> This graph might be misleading, because transaction volume or fees might\n> not be high enough over the last few days to fill blocks to whatever limit\n> miners are willing to mine.\n>\n> So I graphed a time when (according to statoshi.info) there WERE a lot of\n> transactions waiting to be confirmed:\n>    http://bitcoincore.org/~gavin/sizes_357511.html\n>\n> That might also be misleading, because it is possible there were a lot of\n> transactions waiting to be confirmed because miners who choose to create\n> small blocks got lucky and found more blocks than normal.  In fact, it\n> looks like that is what happened: more smaller-than-normal blocks were\n> found, and the memory pool backed up.\n>\n> So: what if we had a dynamic maximum size limit based on recent history?\n>\n> The average block size is about 400K, so a 1.5x rule would make the max\n> block size 600K; miners would definitely be squeezing out transactions /\n> putting pressure to increase transaction fees. Even a 2x rule (implying\n> 800K max blocks) would, today, be squeezing out transactions / putting\n> pressure to increase fees.\n>\n> Using a median size instead of an average means the size can increase or\n> decrease more quickly. For example, imagine the rule is \"median of last\n> 2016 blocks\" and 49% of miners are producing 0-size blocks and 51% are\n> producing max-size blocks. The median is max-size, so the 51% have total\n> control over making blocks bigger.  Swap the roles, and the median is\n> min-size.\n>\n> Because of that, I think using an average is better-- it means the max\nsize\n> will change (up or down) more slowly.\n>\n> I also think 2016 blocks is too long, because transaction volumes change\n> quicker than that. An average over 144 blocks (last 24 hours) would be\n> better able to handle increased transaction volume around major holidays,\n> and would also be able to react more quickly if an economically irrational\n> attacker attempted to flood the network with fee-paying transactions.\n>\n> So my straw-man proposal would be:  max size 2x average size over last 144\n> blocks, calculated at every block.\n>\n> There are a couple of other changes I'd pair with that consensus change:\n>\n> + Make the default mining policy for Bitcoin Core neutral-- have its\ntarget\n> block size be the average size, so miners that don't care will \"go along\n> with the people who do care.\"\n>\n> + Use something like Greg's formula for size instead of bytes-on-the-wire,\n> to discourage bloating the UTXO set.\n>\n>\n> ---------\n>\n> When I've proposed (privately, to the other core committers) some dynamic\n> algorithm the objection has been \"but that gives miners complete control\n> over the max block size.\"\n>\n> I think that worry is unjustified right now-- certainly, until we have\n> size-independent new block propagation there is an incentive for miners to\n> keep their blocks small, and we see miners creating small blocks even when\n> there are fee-paying transactions waiting to be confirmed.\n>\n> I don't even think it will be a problem if/when we do have\nsize-independent\n> new block propagation, because I think the combination of the random\ntiming\n> of block-finding plus a dynamic limit as described above will create a\n> healthy system.\n>\n> If I'm wrong, then it seems to me the miners will have a very strong\n> incentive to, collectively, impose whatever rules are necessary (maybe a\n> soft-fork to put a hard cap on block size) to make the system healthy\nagain.\n>\n>\n> --\n> --\n> Gavin Andresen\n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n>\n> ------------------------------\n>\n>\n------------------------------------------------------------------------------\n>\n>\n> ------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n> End of Bitcoin-development Digest, Vol 48, Issue 122\n> ****************************************************\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/9bc538ee/attachment.html>"
            },
            {
                "author": "Steven Pine",
                "date": "2015-05-28T18:25:17",
                "message_text_only": "My understanding, which is very likely wrong in one way or another, is\ntransaction size and block size are two slightly different things but\nperhaps it's so negligible that block size is a fine stand-in for total\ntransaction throughput.\n\nPotentially Doubling the block size everyday is frankly imprudent. The\nlogarithmic increases in difficulty, which were often closer to 10% or 20%\nevery 2016 blocks was and is plenty fast, potentially changing blocksize by\ntwice daily is the mentality I would expect from a startup with the move\nfast break things motto.\n\nInfrastructure takes time, not everyone wants to run a node on a virtual\namazon instance, provisioning additional hard drive and bandwidth can't\nhappen overnight and trying to plan when block size from one week to the\nnext is a total mystery would be extremely difficult.\n\nAnyone who has spent time examining the mining difficulty increases and\ntrajectory knows future planning is very very hard, allowing block size to\ndouble daily would make it impossible.\n\nPerhaps a middle way would be 300%  increase every 2016 blocks, that will\nscale to 20mbs within a  month or two\n\nThe problem is logarithmic increases seem slow until they seem fast. If the\nnetwork begins to grow and block size hits 20, then the next day 40, 80...\nSmall nodes could get swamped within a week or less.\n\nAs for your point about Christmas, Bitcoin is a global network, Christmas,\nwhile widely celebrated, isn't the only holiday, and planning around\nAmerican buying habits seems short sighted and no different from developers\ntrying to choose what the right fee pressure is.\n\nOn May 28, 2015 1:22 PM, \"Gavin Andresen\" <gavinandresen at gmail.com> wrote:\n>\n> On Thu, May 28, 2015 at 12:30 PM, Steven Pine <steven.pine at gmail.com>\nwrote:\n>>\n>> I would support a dynamic block size increase as outlined. I have a few\nquestions though.\n>>\n>> Is scaling by average block size the best and easiest method, why not\nscale by transactions confirmed instead? Anyone can write and relay a\ntransaction, and those are what we want to scale for, why not measure it\ndirectly?\n>\n>\n> What do you mean? Transactions aren't confirmed until they're in a\nblock...\n>\n>>\n>> I would prefer changes every 2016 blocks, it is a well known change and\na reasonable time period for planning on changes. Two weeks is plenty fast,\nespecially at a 50% rate increase, in a few months the block size could be\ndramatically larger.\n>\n>\n> What type of planning do you imagine is necessary?\n>\n> And have you looked at transaction volumes for credit-card payment\nnetworks around Christmas?\n>\n>>\n>> Daily change to size seems confusing especially considering that max\nblock size will be dipping up and down. Also if something breaks trying to\nfix it in a day seems problematic. The hard fork database size difference\nerror comes to mind. Finally daily 50% increases could quickly crowd out\nsmaller nodes if changes happen too quickly to adapt for.\n>\n> The bottleneck is transaction volume; blocks won't get bigger unless\nthere are fee-paying transactions around to pay them. What scenario are you\nimagining where transaction volume increases by 50% a day for a sustained\nperiod of time?\n>\n> --\n> --\n> Gavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/6daaf026/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-28T18:31:43",
                "message_text_only": "Can we hold off on bike-shedding the particular choice of parameters until\npeople have a chance to weigh in on whether or not there is SOME set of\ndynamic parameters they would support right now?\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/af73282f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposed alternatives to the 20MB step function",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Tier Nolan",
                "insecurity at national.shitposting.agency",
                "Cameron Garnham",
                "Rusty Russell",
                "Raystonn",
                "Bryan Cheng",
                "Mike Hearn",
                "Peter Todd",
                "Admin Istrator",
                "Owen Gunden",
                "Gregory Maxwell",
                "Mark Friedenbach",
                "Aaron Voisine",
                "Bryan Bishop",
                "Steven Pine",
                "Cl\u00e9ment Elbaz",
                "Matt Whitlock",
                "Braun Brelin",
                "Alex Mizrahi",
                "Gavin Andresen",
                "Thomas Voegtlin",
                "Rob Golding",
                "Pieter Wuille",
                "Joel Joonatan Kaartinen"
            ],
            "messages_count": 62,
            "total_messages_chars_count": 150351
        }
    },
    {
        "title": "[Bitcoin-development] Proposed alternatives to the 20MB stepfunction",
        "thread_messages": [
            {
                "author": "Raystonn .",
                "date": "2015-05-29T16:39:29",
                "message_text_only": "Regarding Tier\u2019s proposal: The lower security you mention for extended blocks would delay, possibly forever, the larger blocks maximum block size that we want for the entire network.  That doesn\u2019t sound like an optimal solution.\n\nRegarding consensus for larger maximum block size, what we are seeing on this list is typical of what we see in the U.S. Congress.  Support for changes by the stakeholders (support for bills by the citizens as a whole) has become irrelevant to the probability of these changes being adopted.  Lobbyists have all the sway in getting their policies enacted.  In our case, I would bet on some lobbying of core developers by wealthy miners.\n\nSomeone recently proposed that secret ballots could help eliminate the power of lobbyists in Congress.  Nobody invests in that which cannot be confirmed.  Secret ballots mean the vote you are buying cannot be confirmed.  Perhaps this will work for Bitcoin Core as well.\n\n\nFrom: Tier Nolan \nSent: Friday, May 29, 2015 7:22 AM\nCc: Bitcoin Dev \nSubject: Re: [Bitcoin-development] Proposed alternatives to the 20MB stepfunction\n\nOn Fri, May 29, 2015 at 3:09 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n\n\n  On Fri, May 29, 2015 at 1:39 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n\n    But if there is still no consensus among developers but the \"bigger blocks now\" movement is successful, I'll ask for help getting big miners to do the same, and use the soft-fork block version voting mechanism to (hopefully) get a majority and then a super-majority willing to produce bigger blocks. The purpose of that process is to prove to any doubters that they'd better start supporting bigger blocks or they'll be left behind, and to give them a chance to upgrade before that happens.\n\n  How do you define that the movement is successful?\n\n\nSorry again, I keep auto-sending from gmail when trying to delete.\n\n\nIn theory, using the \"nuclear option\", the block size can be increased via soft fork.\n\n\nVersion 4 blocks would contain the hash of the a valid extended block in the coinbase.\n\n\n<block height> <32 byte extended hash>\n\n\nTo send coins to the auxiliary block, you send them to some template.\n\n\nOP_P2SH_EXTENDED <scriptPubKey hash> OP_TRUE\n\n\nThis transaction can be spent by anyone (under the current rules).  The soft fork would lock the transaction output unless it transferred money from the extended block.\n\n\nTo unlock the transaction output, you need to include the txid of transaction(s) in the extended block and signature(s) in the scriptSig.\n\n\nThe transaction output can be spent in the extended block using P2SH against the scriptPubKey hash.\n\n\nThis means that people can choose to move their money to the extended block.  It might have lower security than leaving it in the root chain.\n\n\nThe extended chain could use the updated script language too.\n\n\nThis is obviously more complex than just increasing the size though, but it could be a fallback option if no consensus is reached.  It has the advantage of giving people a choice.  They can move their money to the extended chain or not, as they wish.\n\n\n\n--------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n\n\n\n--------------------------------------------------------------------------------\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/9eaf701c/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-29T18:28:22",
                "message_text_only": "On Fri, May 29, 2015 at 5:39 PM, Raystonn . <raystonn at hotmail.com> wrote:\n\n>   Regarding Tier\u2019s proposal: The lower security you mention for extended\n> blocks would delay, possibly forever, the larger blocks maximum block size\n> that we want for the entire network.  That doesn\u2019t sound like an optimal\n> solution.\n>\n\nI don't think so.  The lower security is the potential centralisation\nrisk.  If you have your money in the \"root\" chain, then you can watch it.\nYou can probably also watch it in a 20MB chain.\n\nFull nodes would still verify the entire block (root + extended).  It is a\n\"nuclear option\", since you can make any changes you want to the rules for\nthe extended chain.  The only safe guard is that people have to voluntarly\ntransfer coins to the extended block.\n\nThe extended block might have 10-15% of the total bitcoins, but still be\nuseful, since they would be the ones that move the most.  If you want to\nstore your coins long term, you move them back to the root block where you\ncan watch them more closely.\n\nIt does make things more complex though.  Wallets would have to list 2\nbalances.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/27a2d71c/attachment.html>"
            },
            {
                "author": "Raystonn .",
                "date": "2015-05-28T17:39:29",
                "message_text_only": "I agree that developers should avoid imposing economic policy.  It is dangerous for Bitcoin and the core developers themselves to become such a central point of attack for those wishing to disrupt Bitcoin.  My opinion is these things are better left to a decentralized free market anyhow.\n\n\nFrom: Gavin Andresen \nSent: Thursday, May 28, 2015 10:19 AM\nTo: Mike Hearn \nCc: Bitcoin Dev \nSubject: Re: [Bitcoin-development] Proposed alternatives to the 20MB stepfunction\n\nOn Thu, May 28, 2015 at 1:05 PM, Mike Hearn <mike at plan99.net> wrote:\n\n    Isn't that a step backwards, then? I see no reason for fee pressure to exist at the moment. All it's doing is turning away users for no purpose: mining isn't supported by fees, and the tiny fees we use right now seem to be good enough to stop penny flooding.\n\n\n  Why not set the max size to be 20x the average size? Why 2x, given you just pointed out that'd result in blocks shrinking rather than growing.\n\nTwenty is scary.\n\nAnd two is a very neutral number: if 50% of hashpower want the max size to grow as fast as possible and 50% are dead-set opposed to any increase in max size, then half produce blocks 2 times as big, half produce empty blocks, and the max size doesn't change. If it was 20, then a small minority of miners could force a max size increase.  (if it is less than 2, then a minority of minors can force the block size down)\n\n\nAs for whether there \"should\" be fee pressure now or not: I have no opinion, besides \"we should make block propagation faster so there is no technical reason for miners to produce tiny blocks.\" I don't think us developers should be deciding things like whether or not fees are too high, too low, .....\n\n-- \n\n--\nGavin Andresen\n\n\n\n--------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n\n\n\n--------------------------------------------------------------------------------\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/21f0f238/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-28T17:59:11",
                "message_text_only": "On May 28, 2015 10:42 AM, \"Raystonn .\" <raystonn at hotmail.com> wrote:\n>\n> I agree that developers should avoid imposing economic policy.  It is\ndangerous for Bitcoin and the core developers themselves to become such a\ncentral point of attack for those wishing to disrupt Bitcoin.\n\nI could not agree more that developers should not be in charge of the\nnetwork rules.\n\nWhich is why - in my opinion - hard forks cannot be controversial things. A\ncontroversial change to the software, forced to be adopted by the public\nbecause the only alternative is a permanent chain fork, is a use of power\nthat developers (or anyone) should not have, and an incredibly dangerous\nprecedent for other changes that only a subset of participants would want.\n\nThe block size is also not just an economic policy. It is the compromise\nthe _network_ chooses to make between utility and various forms of\ncentralization pressure, and we should treat it as a compromise, and not as\nsome limit that is inferior to scaling demands.\n\nI personally think the block size should increase, by the way, but only if\nwe can do it under a policy of doing it after technological growth has been\nshown to be sufficient to support it without increased risk.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/e5331642/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-28T18:21:48",
                "message_text_only": "On Thu, May 28, 2015 at 1:59 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> I personally think the block size should increase, by the way, but only if\n> we can do it under a policy of doing it after technological growth has been\n> shown to be sufficient to support it without increased risk.\n>\n> Can you be more specific about this? What risks are you worried about?\n\nI've tried to cover all that I've heard about in my blog posts about why I\nthink the risks of 20MB blocks are outweighed by the benefits, am I missing\nsomething?\n  (blog posts are linked from\nhttp://gavinandresen.ninja/time-to-roll-out-bigger-blocks )\n\nThere is the \"a sudden jump to a 20MB max might have unforseen\nconsequences\" risk that I don't address, but a dynamic increase would fix\nthat.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/11fdd5fc/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposed alternatives to the 20MB stepfunction",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Raystonn .",
                "Pieter Wuille",
                "Gavin Andresen",
                "Tier Nolan"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 9727
        }
    },
    {
        "title": "[Bitcoin-development] Removing transaction data from blocks",
        "thread_messages": [
            {
                "author": "Arne Brutschy",
                "date": "2015-05-08T14:01:01",
                "message_text_only": "Hello,\n\nAt DevCore London, Gavin mentioned the idea that we could get rid of \nsending full blocks. Instead, newly minted blocks would only be \ndistributed as block headers plus all hashes of the transactions \nincluded in the block. The assumption would be that nodes have already \nthe majority of these transactions in their mempool.\n\nThe advantages are clear: it's more efficient, as we would send \ntransactions only once over the network, and it's fast as the resulting \nblocks would be small. Moreover, we would get rid of the blocksize limit \nfor a long time.\n\nUnfortunately, I am too ignorant of bitcoin core's internals to judge \nthe changes required to make this happen. (I guess we'd require a new \nblock format and a way to bulk-request missing transactions.)\n\nHowever, I'm curious to hear what others with a better grasp of bitcoin \ncore's internals have to say about it.\n\nRegards,\nArne"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-08T14:52:28",
                "message_text_only": "So, there are several ideas about how to reduce the size of blocks being\nsent on the network:\n* Matt Corallo's relay network, which internally works by remembering the\nlast 5000 (i believe?) transactions sent by the peer, and allowing the peer\nto backreference those rather than retransmit them inside block data. This\nexists and works today.\n* Gavin Andresen's IBLT based set reconciliation for blocks based on what a\npeer expects the new block to contain.\n* Greg Maxwell's network block coding, which is based on erasure coding,\nand also supports sharding (everyone sends some block data to everyone,\nrather fetching from one peer).\n\nHowever, the primary purpose is not to reduce bandwidth (though that is a\nnice side advantage). The purpose is reducing propagation delay. Larger\npropagation delays across the network (relative to the inter-block period)\nresult in higher forking rates. If the forking rate gets very high, the\nnetwork may fail to converge entirely, but even long before that point, the\nhigher the forking rate is, the higher the advantage of larger (and better\nconnected) pools over smaller ones. This is why, in my opinion,\nguaranteeing fast propagation is one of the most essential responsibility\nof full nodes to avoid centralization pressure.\n\nAlso, none of this would let us \"get rid of the block size\" at all. All\ntransactions still have to be transferred and processed, and due to\ninherent latencies of communication across the globe, the higher the\ntransaction rate is, the higher the number of transactions in blocks will\nbe that peers have not yet heard about. You can institute a policy to not\ninclude too recent transactions in blocks, but again, this favors larger\nminers over smaller ones.\n\nAlso, if the end goal is propagation delay, just minimizing the amount of\ndata transferred is not enough. You also need to make sure the\ncommunication mechanism does not add huge processing overheads or adds\nunnecessary roundtrips. In fact, this is the key difference between the 3\ntechniques listed above, and several people are working on refining and\noptimizing these mechanisms to make them practically usable.\nOn May 8, 2015 7:23 AM, \"Arne Brutschy\" <abrutschy at xylon.de> wrote:\n\n> Hello,\n>\n> At DevCore London, Gavin mentioned the idea that we could get rid of\n> sending full blocks. Instead, newly minted blocks would only be\n> distributed as block headers plus all hashes of the transactions\n> included in the block. The assumption would be that nodes have already\n> the majority of these transactions in their mempool.\n>\n> The advantages are clear: it's more efficient, as we would send\n> transactions only once over the network, and it's fast as the resulting\n> blocks would be small. Moreover, we would get rid of the blocksize limit\n> for a long time.\n>\n> Unfortunately, I am too ignorant of bitcoin core's internals to judge\n> the changes required to make this happen. (I guess we'd require a new\n> block format and a way to bulk-request missing transactions.)\n>\n> However, I'm curious to hear what others with a better grasp of bitcoin\n> core's internals have to say about it.\n>\n> Regards,\n> Arne\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/603dc927/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Removing transaction data from blocks",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Arne Brutschy",
                "Pieter Wuille"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4841
        }
    },
    {
        "title": "[Bitcoin-development] Softfork signaling improvements",
        "thread_messages": [
            {
                "author": "Douglas Roark",
                "date": "2015-05-08T19:27:26",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nHello. I've seen Greg make a couple of posts online\n(https://bitcointalk.org/index.php?topic=1033396.msg11155302#msg11155302\nis one such example) where he has mentioned that Pieter has a new\nproposal for allowing multiple softforks to be deployed at the same\ntime. As discussed in the thread I linked, the idea seems simple\nenough. Still, I'm curious if the actual proposal has been posted\nanywhere. I spent a few minutes searching the usual suspects (this\nmailing list, Reddit, Bitcointalk, IRC logs, BIPs) and can't find\nanything.\n\nThanks.\n\n- ---\nDouglas Roark\nSenior Developer\nArmory Technologies, Inc.\ndoug at bitcoinarmory.com\nPGP key ID: 92ADC0D7\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG/MacGPG2 v2.0.22 (Darwin)\nComment: GPGTools - https://gpgtools.org\n\niQIcBAEBCgAGBQJVTQ4eAAoJEGybVGGSrcDX8eMQAOQiDA7an+qZBqDfVIwEzY2C\nSxOVxswwxAyTtZNM/Nm+8MTq77hF8+3j/C3bUbDW6wCu4QxBYA/uiCGTf44dj6WX\n7aiXg1o9C4LfPcuUngcMI0H5ixOUxnbqUdmpNdoIvy4did2dVs9fAmOPEoSVUm72\n6dMLGrtlPN0jcLX6pJd12Dy3laKxd0AP72wi6SivH6i8v8rLb940EuBS3hIkuZG0\nvnR5MXMIEd0rkWesr8hn6oTs/k8t4zgts7cgIrA7rU3wJq0qaHBa8uASUxwHKDjD\nKmDwaigvOGN6XqitqokCUlqjoxvwpimCjb3Uv5Pkxn8+dwue9F/IggRXUSuifJRn\nUEZT2F8fwhiluldz3sRaNtLOpCoKfPC+YYv7kvGySgqagtNJFHoFhbeQM0S3yjRn\nCeh1xK9sOjrxw/my0jwpjJkqlhvQtVG15OsNWDzZ+eWa56kghnSgLkFO+T4G6IxB\nEUOcAYjJkLbg5ssjgyhvDOvGqft+2e4MNlB01e1ZQr4whQH4TdRkd66A4WDNB+0g\nLBqVhAc2C8L3g046mhZmC33SuOSxxm8shlxZvYLHU2HrnUFg9NkkXi1Ub7agMSck\nTTkLbMx17AvOXkKH0v1L20kWoWAp9LfRGdD+qnY8svJkaUuVtgDurpcwEk40WwEZ\ncaYBw+8bdLpKZwqbA1DL\n=ayhE\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Softfork signaling improvements",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Douglas Roark"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1583
        }
    },
    {
        "title": "[Bitcoin-development] Block Size Increase (Raystonn)",
        "thread_messages": [
            {
                "author": "Damian Gomez",
                "date": "2015-05-08T21:04:10",
                "message_text_only": "Hello,\n\nI was reading some of the thread but can't say I read the entire thing.\n\nI think that it is realistic to cinsider a nlock sixe of 20MB for any block\ntxn to occur. THis is an enormous amount of data (relatively for a netwkrk)\nin which the avergage rate of 10tps over 10 miniutes would allow for\nfewasible transformation of data at this curent point in time.\n\nThough I do not see what extra hash information would be stored in the\noverall ecosystem as we begin to describe what the scripts that are\natacrhed tp the blockchain would carry,\n\nI'd therefore think that for the remainder of this year that it is possible\nto have a block chain within 200 - 300 bytes that is more charatereistic of\nsome feasible attempts at attaching nuanced data in order to keep propliifc\nthe blockchain but have these identifiers be integral OPSIg of the the\nentiore block. THe reasoning behind this has to do with encryption\nstandards that can be added toe a chain such as th DH algoritnm keys that\nwould allow for a higher integrity level withinin the system as it is.\nCutrent;y tyh prootocl oomnly controls for the amount of transactions\nthrough if TxnOut script and the publin key coming form teh lcoation of the\nproof-of-work. Form this then I think that a rate of higher than then\ncurrent standard of 92bytes allows for GPUS ie CUDA to perfirm its standard\noperations of  1216 flops   in rde rto mechanize a new personal identity\nwithin the chain that also attaches an encrypted instance of a further\ncategorical variable that we can prsribved to it.\n\nI think with the current BIP7 prootclol for transactions there is an area\nof vulnerability for man-in-the-middle attacks upon request of  bitcin to\nany merchant as is. It would contraidct the security of the bitcoin if it\nwas intereceptefd iand not allowed to reach tthe payment network or if the\nhash was reveresed in orfr to change the value it had. Therefore the\ncurrent best fit block size today is between 200 - 300 bytws (depending on\nhow exciteed we get)\n\n\n\nThanks for letting me join the conversation\nI welcomes any vhalleneged and will reply with more research as i figure\nout what problems are revealed in my current formation of thoughts (sorry\nfor the errors but i am just trying to move forward ---> THE DELRERT KEY\nLITERALLY PREVENTS IT )\n\n\n_Damian\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/31c1a261/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Block Size Increase (Raystonn)",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Damian Gomez"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2489
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin-development Digest, Vol 48, Issue 41",
        "thread_messages": [
            {
                "author": "Damian Gomez",
                "date": "2015-05-08T22:11:13",
                "message_text_only": "Well zombie txns aside,  I expect this to be resolved w/ a client side\nimplementation using a Merkle-Winternitz OTS in order to prevent the loss\nof fee structure theougth the implementation of a this security hash that\neill alloow for a one-wya transaction to conitnue, according to the TESLA\nprotocol.\n\nWe can then tally what is needed to compute tteh number of bit desginated\nfor teh completion og the client-side signature if discussin the\nconstrucitons of a a DH key (instead of the BIP X509 protocol)\n\n\n\n\n\nOn Fri, May 8, 2015 at 2:08 PM, <\nbitcoin-development-request at lists.sourceforge.net> wrote:\n\n> Send Bitcoin-development mailing list submissions to\n>         bitcoin-development at lists.sourceforge.net\n>\n> To subscribe or unsubscribe via the World Wide Web, visit\n>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> or, via email, send a message with subject or body 'help' to\n>         bitcoin-development-request at lists.sourceforge.net\n>\n> You can reach the person managing the list at\n>         bitcoin-development-owner at lists.sourceforge.net\n>\n> When replying, please edit your Subject line so it is more specific\n> than \"Re: Contents of Bitcoin-development digest...\"\n>\n> Today's Topics:\n>\n>    1. Re: Block Size Increase (Mark Friedenbach)\n>    2. Softfork signaling improvements (Douglas Roark)\n>    3. Re: Block Size Increase (Mark Friedenbach)\n>    4. Re: Block Size Increase (Raystonn) (Damian Gomez)\n>    5. Re: Block Size Increase (Raystonn)\n>\n>\n> ---------- Forwarded message ----------\n> From: Mark Friedenbach <mark at friedenbach.org>\n> To: Raystonn <raystonn at hotmail.com>\n> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n> Date: Fri, 8 May 2015 13:55:30 -0700\n> Subject: Re: [Bitcoin-development] Block Size Increase\n> The problems with that are larger than time being unreliable. It is no\n> longer reorg-safe as transactions can expire in the course of a reorg and\n> any transaction built on the now expired transaction is invalidated.\n>\n> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>\n>> Replace by fee is what I was referencing.  End-users interpret the old\n>> transaction as expired.  Hence the nomenclature.  An alternative is a new\n>> feature that operates in the reverse of time lock, expiring a transaction\n>> after a specific time.  But time is a bit unreliable in the blockchain\n>>\n>\n>\n> ---------- Forwarded message ----------\n> From: Douglas Roark <doug at bitcoinarmory.com>\n> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Cc:\n> Date: Fri, 8 May 2015 15:27:26 -0400\n> Subject: [Bitcoin-development] Softfork signaling improvements\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA512\n>\n> Hello. I've seen Greg make a couple of posts online\n> (https://bitcointalk.org/index.php?topic=1033396.msg11155302#msg11155302\n> is one such example) where he has mentioned that Pieter has a new\n> proposal for allowing multiple softforks to be deployed at the same\n> time. As discussed in the thread I linked, the idea seems simple\n> enough. Still, I'm curious if the actual proposal has been posted\n> anywhere. I spent a few minutes searching the usual suspects (this\n> mailing list, Reddit, Bitcointalk, IRC logs, BIPs) and can't find\n> anything.\n>\n> Thanks.\n>\n> - ---\n> Douglas Roark\n> Senior Developer\n> Armory Technologies, Inc.\n> doug at bitcoinarmory.com\n> PGP key ID: 92ADC0D7\n> -----BEGIN PGP SIGNATURE-----\n> Version: GnuPG/MacGPG2 v2.0.22 (Darwin)\n> Comment: GPGTools - https://gpgtools.org\n>\n> iQIcBAEBCgAGBQJVTQ4eAAoJEGybVGGSrcDX8eMQAOQiDA7an+qZBqDfVIwEzY2C\n> SxOVxswwxAyTtZNM/Nm+8MTq77hF8+3j/C3bUbDW6wCu4QxBYA/uiCGTf44dj6WX\n> 7aiXg1o9C4LfPcuUngcMI0H5ixOUxnbqUdmpNdoIvy4did2dVs9fAmOPEoSVUm72\n> 6dMLGrtlPN0jcLX6pJd12Dy3laKxd0AP72wi6SivH6i8v8rLb940EuBS3hIkuZG0\n> vnR5MXMIEd0rkWesr8hn6oTs/k8t4zgts7cgIrA7rU3wJq0qaHBa8uASUxwHKDjD\n> KmDwaigvOGN6XqitqokCUlqjoxvwpimCjb3Uv5Pkxn8+dwue9F/IggRXUSuifJRn\n> UEZT2F8fwhiluldz3sRaNtLOpCoKfPC+YYv7kvGySgqagtNJFHoFhbeQM0S3yjRn\n> Ceh1xK9sOjrxw/my0jwpjJkqlhvQtVG15OsNWDzZ+eWa56kghnSgLkFO+T4G6IxB\n> EUOcAYjJkLbg5ssjgyhvDOvGqft+2e4MNlB01e1ZQr4whQH4TdRkd66A4WDNB+0g\n> LBqVhAc2C8L3g046mhZmC33SuOSxxm8shlxZvYLHU2HrnUFg9NkkXi1Ub7agMSck\n> TTkLbMx17AvOXkKH0v1L20kWoWAp9LfRGdD+qnY8svJkaUuVtgDurpcwEk40WwEZ\n> caYBw+8bdLpKZwqbA1DL\n> =ayhE\n> -----END PGP SIGNATURE-----\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: Mark Friedenbach <mark at friedenbach.org>\n> To: \"Raystonn .\" <raystonn at hotmail.com>\n> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n> Date: Fri, 8 May 2015 13:40:50 -0700\n> Subject: Re: [Bitcoin-development] Block Size Increase\n> Transactions don't expire. But if the wallet is online, it can\n> periodically choose to release an already created transaction with a higher\n> fee. This requires replace-by-fee to be sufficiently deployed, however.\n>\n> On Fri, May 8, 2015 at 1:38 PM, Raystonn . <raystonn at hotmail.com> wrote:\n>\n>> I have a proposal for wallets such as yours.  How about creating all\n>> transactions with an expiration time starting with a low fee, then\n>> replacing with new transactions that have a higher fee as time passes.\n>> Users can pick the fee curve they desire based on the transaction priority\n>> they want to advertise to the network.  Users set the priority in the\n>> wallet, and the wallet software translates it to a specific fee curve used\n>> in the series of expiring transactions.  In this manner, transactions are\n>> never left hanging for days, and probably not even for hours.\n>>\n>> -Raystonn\n>>  On 8 May 2015 1:17 pm, Aaron Voisine <voisine at gmail.com> wrote:\n>>\n>> As the author of a popular SPV wallet, I wanted to weigh in, in support\n>> of the Gavin's 20Mb block proposal.\n>>\n>> The best argument I've heard against raising the limit is that we need\n>> fee pressure.  I agree that fee pressure is the right way to economize on\n>> scarce resources. Placing hard limits on block size however is an\n>> incredibly disruptive way to go about this, and will severely negatively\n>> impact users' experience.\n>>\n>> When users pay too low a fee, they should:\n>>\n>> 1) See immediate failure as they do now with fees that fail to propagate.\n>>\n>> 2) If the fee lower than it should be but not terminal, they should see\n>> degraded performance, long delays in confirmation, but eventual success.\n>> This will encourage them to pay higher fees in future.\n>>\n>> The worst of all worlds would be to have transactions propagate, hang in\n>> limbo for days, and then fail. This is the most important scenario to\n>> avoid. Increasing the 1Mb block size limit I think is the simplest way to\n>> avoid this least desirable scenario for the immediate future.\n>>\n>> We can play around with improved transaction selection for blocks and\n>> encourage miners to adopt it to discourage low fees and create fee\n>> pressure. These could involve hybrid priority/fee selection so low fee\n>> transactions see degraded performance instead of failure. This would be the\n>> conservative low risk approach.\n>>\n>> Aaron Voisine\n>> co-founder and CEO\n>> breadwallet.com\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> ---------- Forwarded message ----------\n> From: Damian Gomez <dgomez1092 at gmail.com>\n> To: bitcoin-development at lists.sourceforge.net\n> Cc:\n> Date: Fri, 8 May 2015 14:04:10 -0700\n> Subject: Re: [Bitcoin-development] Block Size Increase (Raystonn)\n> Hello,\n>\n> I was reading some of the thread but can't say I read the entire thing.\n>\n> I think that it is realistic to cinsider a nlock sixe of 20MB for any\n> block txn to occur. THis is an enormous amount of data (relatively for a\n> netwkrk) in which the avergage rate of 10tps over 10 miniutes would allow\n> for fewasible transformation of data at this curent point in time.\n>\n> Though I do not see what extra hash information would be stored in the\n> overall ecosystem as we begin to describe what the scripts that are\n> atacrhed tp the blockchain would carry,\n>\n> I'd therefore think that for the remainder of this year that it is\n> possible to have a block chain within 200 - 300 bytes that is more\n> charatereistic of some feasible attempts at attaching nuanced data in order\n> to keep propliifc the blockchain but have these identifiers be integral\n> OPSIg of the the entiore block. THe reasoning behind this has to do with\n> encryption standards that can be added toe a chain such as th DH algoritnm\n> keys that would allow for a higher integrity level withinin the system as\n> it is. Cutrent;y tyh prootocl oomnly controls for the amount of\n> transactions through if TxnOut script and the publin key coming form teh\n> lcoation of the proof-of-work. Form this then I think that a rate of higher\n> than then current standard of 92bytes allows for GPUS ie CUDA to perfirm\n> its standard operations of  1216 flops   in rde rto mechanize a new\n> personal identity within the chain that also attaches an encrypted instance\n> of a further categorical variable that we can prsribved to it.\n>\n> I think with the current BIP7 prootclol for transactions there is an area\n> of vulnerability for man-in-the-middle attacks upon request of  bitcin to\n> any merchant as is. It would contraidct the security of the bitcoin if it\n> was intereceptefd iand not allowed to reach tthe payment network or if the\n> hash was reveresed in orfr to change the value it had. Therefore the\n> current best fit block size today is between 200 - 300 bytws (depending on\n> how exciteed we get)\n>\n>\n>\n> Thanks for letting me join the conversation\n> I welcomes any vhalleneged and will reply with more research as i figure\n> out what problems are revealed in my current formation of thoughts (sorry\n> for the errors but i am just trying to move forward ---> THE DELRERT KEY\n> LITERALLY PREVENTS IT )\n>\n>\n> _Damian\n>\n>\n> ---------- Forwarded message ----------\n> From: Raystonn <raystonn at hotmail.com>\n> To: Mark Friedenbach <mark at friedenbach.org>\n> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n> Date: Fri, 8 May 2015 14:01:28 -0700\n> Subject: Re: [Bitcoin-development] Block Size Increase\n>\n> Replace by fee is the better approach.  It will ultimately replace zombie\n> transactions (due to insufficient fee) with potentially much higher fees as\n> the feature takes hold in wallets throughout the network, and fee\n> competition increases.  However, this does not fix the problem of low tps.\n> In fact, as blocks fill it could make the problem worse.  This feature\n> means more transactions after all.  So I would expect huge fee spikes, or a\n> return to zombie transactions if fee caps are implemented by wallets.\n>\n> -Raystonn\n>  On 8 May 2015 1:55 pm, Mark Friedenbach <mark at friedenbach.org> wrote:\n>\n> The problems with that are larger than time being unreliable. It is no\n> longer reorg-safe as transactions can expire in the course of a reorg and\n> any transaction built on the now expired transaction is invalidated.\n>\n> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>\n> Replace by fee is what I was referencing.  End-users interpret the old\n> transaction as expired.  Hence the nomenclature.  An alternative is a new\n> feature that operates in the reverse of time lock, expiring a transaction\n> after a specific time.  But time is a bit unreliable in the blockchain\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/dbf018a4/attachment.html>"
            },
            {
                "author": "Damian Gomez",
                "date": "2015-05-08T22:12:56",
                "message_text_only": "let me continue my conversation:\n\nas the development of this transactions would be indiscated\n\nas a ByteArray of\n\n\nOn Fri, May 8, 2015 at 3:11 PM, Damian Gomez <dgomez1092 at gmail.com> wrote:\n\n>\n> Well zombie txns aside,  I expect this to be resolved w/ a client side\n> implementation using a Merkle-Winternitz OTS in order to prevent the loss\n> of fee structure theougth the implementation of a this security hash that\n> eill alloow for a one-wya transaction to conitnue, according to the TESLA\n> protocol.\n>\n> We can then tally what is needed to compute tteh number of bit desginated\n> for teh completion og the client-side signature if discussin the\n> construcitons of a a DH key (instead of the BIP X509 protocol)\n>\n>\n>\n>\n>\n> On Fri, May 8, 2015 at 2:08 PM, <\n> bitcoin-development-request at lists.sourceforge.net> wrote:\n>\n>> Send Bitcoin-development mailing list submissions to\n>>         bitcoin-development at lists.sourceforge.net\n>>\n>> To subscribe or unsubscribe via the World Wide Web, visit\n>>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>> or, via email, send a message with subject or body 'help' to\n>>         bitcoin-development-request at lists.sourceforge.net\n>>\n>> You can reach the person managing the list at\n>>         bitcoin-development-owner at lists.sourceforge.net\n>>\n>> When replying, please edit your Subject line so it is more specific\n>> than \"Re: Contents of Bitcoin-development digest...\"\n>>\n>> Today's Topics:\n>>\n>>    1. Re: Block Size Increase (Mark Friedenbach)\n>>    2. Softfork signaling improvements (Douglas Roark)\n>>    3. Re: Block Size Increase (Mark Friedenbach)\n>>    4. Re: Block Size Increase (Raystonn) (Damian Gomez)\n>>    5. Re: Block Size Increase (Raystonn)\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Mark Friedenbach <mark at friedenbach.org>\n>> To: Raystonn <raystonn at hotmail.com>\n>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>> Date: Fri, 8 May 2015 13:55:30 -0700\n>> Subject: Re: [Bitcoin-development] Block Size Increase\n>> The problems with that are larger than time being unreliable. It is no\n>> longer reorg-safe as transactions can expire in the course of a reorg and\n>> any transaction built on the now expired transaction is invalidated.\n>>\n>> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>>\n>>> Replace by fee is what I was referencing.  End-users interpret the old\n>>> transaction as expired.  Hence the nomenclature.  An alternative is a new\n>>> feature that operates in the reverse of time lock, expiring a transaction\n>>> after a specific time.  But time is a bit unreliable in the blockchain\n>>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Douglas Roark <doug at bitcoinarmory.com>\n>> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>> Cc:\n>> Date: Fri, 8 May 2015 15:27:26 -0400\n>> Subject: [Bitcoin-development] Softfork signaling improvements\n>> -----BEGIN PGP SIGNED MESSAGE-----\n>> Hash: SHA512\n>>\n>> Hello. I've seen Greg make a couple of posts online\n>> (https://bitcointalk.org/index.php?topic=1033396.msg11155302#msg11155302\n>> is one such example) where he has mentioned that Pieter has a new\n>> proposal for allowing multiple softforks to be deployed at the same\n>> time. As discussed in the thread I linked, the idea seems simple\n>> enough. Still, I'm curious if the actual proposal has been posted\n>> anywhere. I spent a few minutes searching the usual suspects (this\n>> mailing list, Reddit, Bitcointalk, IRC logs, BIPs) and can't find\n>> anything.\n>>\n>> Thanks.\n>>\n>> - ---\n>> Douglas Roark\n>> Senior Developer\n>> Armory Technologies, Inc.\n>> doug at bitcoinarmory.com\n>> PGP key ID: 92ADC0D7\n>> -----BEGIN PGP SIGNATURE-----\n>> Version: GnuPG/MacGPG2 v2.0.22 (Darwin)\n>> Comment: GPGTools - https://gpgtools.org\n>>\n>> iQIcBAEBCgAGBQJVTQ4eAAoJEGybVGGSrcDX8eMQAOQiDA7an+qZBqDfVIwEzY2C\n>> SxOVxswwxAyTtZNM/Nm+8MTq77hF8+3j/C3bUbDW6wCu4QxBYA/uiCGTf44dj6WX\n>> 7aiXg1o9C4LfPcuUngcMI0H5ixOUxnbqUdmpNdoIvy4did2dVs9fAmOPEoSVUm72\n>> 6dMLGrtlPN0jcLX6pJd12Dy3laKxd0AP72wi6SivH6i8v8rLb940EuBS3hIkuZG0\n>> vnR5MXMIEd0rkWesr8hn6oTs/k8t4zgts7cgIrA7rU3wJq0qaHBa8uASUxwHKDjD\n>> KmDwaigvOGN6XqitqokCUlqjoxvwpimCjb3Uv5Pkxn8+dwue9F/IggRXUSuifJRn\n>> UEZT2F8fwhiluldz3sRaNtLOpCoKfPC+YYv7kvGySgqagtNJFHoFhbeQM0S3yjRn\n>> Ceh1xK9sOjrxw/my0jwpjJkqlhvQtVG15OsNWDzZ+eWa56kghnSgLkFO+T4G6IxB\n>> EUOcAYjJkLbg5ssjgyhvDOvGqft+2e4MNlB01e1ZQr4whQH4TdRkd66A4WDNB+0g\n>> LBqVhAc2C8L3g046mhZmC33SuOSxxm8shlxZvYLHU2HrnUFg9NkkXi1Ub7agMSck\n>> TTkLbMx17AvOXkKH0v1L20kWoWAp9LfRGdD+qnY8svJkaUuVtgDurpcwEk40WwEZ\n>> caYBw+8bdLpKZwqbA1DL\n>> =ayhE\n>> -----END PGP SIGNATURE-----\n>>\n>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Mark Friedenbach <mark at friedenbach.org>\n>> To: \"Raystonn .\" <raystonn at hotmail.com>\n>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>> Date: Fri, 8 May 2015 13:40:50 -0700\n>> Subject: Re: [Bitcoin-development] Block Size Increase\n>> Transactions don't expire. But if the wallet is online, it can\n>> periodically choose to release an already created transaction with a higher\n>> fee. This requires replace-by-fee to be sufficiently deployed, however.\n>>\n>> On Fri, May 8, 2015 at 1:38 PM, Raystonn . <raystonn at hotmail.com> wrote:\n>>\n>>> I have a proposal for wallets such as yours.  How about creating all\n>>> transactions with an expiration time starting with a low fee, then\n>>> replacing with new transactions that have a higher fee as time passes.\n>>> Users can pick the fee curve they desire based on the transaction priority\n>>> they want to advertise to the network.  Users set the priority in the\n>>> wallet, and the wallet software translates it to a specific fee curve used\n>>> in the series of expiring transactions.  In this manner, transactions are\n>>> never left hanging for days, and probably not even for hours.\n>>>\n>>> -Raystonn\n>>>  On 8 May 2015 1:17 pm, Aaron Voisine <voisine at gmail.com> wrote:\n>>>\n>>> As the author of a popular SPV wallet, I wanted to weigh in, in support\n>>> of the Gavin's 20Mb block proposal.\n>>>\n>>> The best argument I've heard against raising the limit is that we need\n>>> fee pressure.  I agree that fee pressure is the right way to economize on\n>>> scarce resources. Placing hard limits on block size however is an\n>>> incredibly disruptive way to go about this, and will severely negatively\n>>> impact users' experience.\n>>>\n>>> When users pay too low a fee, they should:\n>>>\n>>> 1) See immediate failure as they do now with fees that fail to propagate.\n>>>\n>>> 2) If the fee lower than it should be but not terminal, they should see\n>>> degraded performance, long delays in confirmation, but eventual success.\n>>> This will encourage them to pay higher fees in future.\n>>>\n>>> The worst of all worlds would be to have transactions propagate, hang in\n>>> limbo for days, and then fail. This is the most important scenario to\n>>> avoid. Increasing the 1Mb block size limit I think is the simplest way to\n>>> avoid this least desirable scenario for the immediate future.\n>>>\n>>> We can play around with improved transaction selection for blocks and\n>>> encourage miners to adopt it to discourage low fees and create fee\n>>> pressure. These could involve hybrid priority/fee selection so low fee\n>>> transactions see degraded performance instead of failure. This would be the\n>>> conservative low risk approach.\n>>>\n>>> Aaron Voisine\n>>> co-founder and CEO\n>>> breadwallet.com\n>>>\n>>>\n>>>\n>>> ------------------------------------------------------------------------------\n>>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>>> Widest out-of-the-box monitoring support with 50+ applications\n>>> Performance metrics, stats and reports that give you Actionable Insights\n>>> Deep dive visibility with transaction tracing using APM Insight.\n>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>> _______________________________________________\n>>> Bitcoin-development mailing list\n>>> Bitcoin-development at lists.sourceforge.net\n>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>>\n>>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Damian Gomez <dgomez1092 at gmail.com>\n>> To: bitcoin-development at lists.sourceforge.net\n>> Cc:\n>> Date: Fri, 8 May 2015 14:04:10 -0700\n>> Subject: Re: [Bitcoin-development] Block Size Increase (Raystonn)\n>> Hello,\n>>\n>> I was reading some of the thread but can't say I read the entire thing.\n>>\n>> I think that it is realistic to cinsider a nlock sixe of 20MB for any\n>> block txn to occur. THis is an enormous amount of data (relatively for a\n>> netwkrk) in which the avergage rate of 10tps over 10 miniutes would allow\n>> for fewasible transformation of data at this curent point in time.\n>>\n>> Though I do not see what extra hash information would be stored in the\n>> overall ecosystem as we begin to describe what the scripts that are\n>> atacrhed tp the blockchain would carry,\n>>\n>> I'd therefore think that for the remainder of this year that it is\n>> possible to have a block chain within 200 - 300 bytes that is more\n>> charatereistic of some feasible attempts at attaching nuanced data in order\n>> to keep propliifc the blockchain but have these identifiers be integral\n>> OPSIg of the the entiore block. THe reasoning behind this has to do with\n>> encryption standards that can be added toe a chain such as th DH algoritnm\n>> keys that would allow for a higher integrity level withinin the system as\n>> it is. Cutrent;y tyh prootocl oomnly controls for the amount of\n>> transactions through if TxnOut script and the publin key coming form teh\n>> lcoation of the proof-of-work. Form this then I think that a rate of higher\n>> than then current standard of 92bytes allows for GPUS ie CUDA to perfirm\n>> its standard operations of  1216 flops   in rde rto mechanize a new\n>> personal identity within the chain that also attaches an encrypted instance\n>> of a further categorical variable that we can prsribved to it.\n>>\n>> I think with the current BIP7 prootclol for transactions there is an area\n>> of vulnerability for man-in-the-middle attacks upon request of  bitcin to\n>> any merchant as is. It would contraidct the security of the bitcoin if it\n>> was intereceptefd iand not allowed to reach tthe payment network or if the\n>> hash was reveresed in orfr to change the value it had. Therefore the\n>> current best fit block size today is between 200 - 300 bytws (depending on\n>> how exciteed we get)\n>>\n>>\n>>\n>> Thanks for letting me join the conversation\n>> I welcomes any vhalleneged and will reply with more research as i figure\n>> out what problems are revealed in my current formation of thoughts (sorry\n>> for the errors but i am just trying to move forward ---> THE DELRERT KEY\n>> LITERALLY PREVENTS IT )\n>>\n>>\n>> _Damian\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Raystonn <raystonn at hotmail.com>\n>> To: Mark Friedenbach <mark at friedenbach.org>\n>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>> Date: Fri, 8 May 2015 14:01:28 -0700\n>> Subject: Re: [Bitcoin-development] Block Size Increase\n>>\n>> Replace by fee is the better approach.  It will ultimately replace zombie\n>> transactions (due to insufficient fee) with potentially much higher fees as\n>> the feature takes hold in wallets throughout the network, and fee\n>> competition increases.  However, this does not fix the problem of low tps.\n>> In fact, as blocks fill it could make the problem worse.  This feature\n>> means more transactions after all.  So I would expect huge fee spikes, or a\n>> return to zombie transactions if fee caps are implemented by wallets.\n>>\n>> -Raystonn\n>>  On 8 May 2015 1:55 pm, Mark Friedenbach <mark at friedenbach.org> wrote:\n>>\n>> The problems with that are larger than time being unreliable. It is no\n>> longer reorg-safe as transactions can expire in the course of a reorg and\n>> any transaction built on the now expired transaction is invalidated.\n>>\n>> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>>\n>> Replace by fee is what I was referencing.  End-users interpret the old\n>> transaction as expired.  Hence the nomenclature.  An alternative is a new\n>> feature that operates in the reverse of time lock, expiring a transaction\n>> after a specific time.  But time is a bit unreliable in the blockchain\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/55d9f645/attachment.html>"
            },
            {
                "author": "Damian Gomez",
                "date": "2015-05-08T22:13:21",
                "message_text_only": "On Fri, May 8, 2015 at 3:12 PM, Damian Gomez <dgomez1092 at gmail.com> wrote:\n\n> let me continue my conversation:\n>\n> as the development of this transactions would be indiscated\n>\n> as a ByteArray of\n>\n>\n> On Fri, May 8, 2015 at 3:11 PM, Damian Gomez <dgomez1092 at gmail.com> wrote:\n>\n>>\n>> Well zombie txns aside,  I expect this to be resolved w/ a client side\n>> implementation using a Merkle-Winternitz OTS in order to prevent the loss\n>> of fee structure theougth the implementation of a this security hash that\n>> eill alloow for a one-wya transaction to conitnue, according to the TESLA\n>> protocol.\n>>\n>> We can then tally what is needed to compute tteh number of bit desginated\n>> for teh completion og the client-side signature if discussin the\n>> construcitons of a a DH key (instead of the BIP X509 protocol)\n>>\n>>\n>>\n>>\n>>\n>> On Fri, May 8, 2015 at 2:08 PM, <\n>> bitcoin-development-request at lists.sourceforge.net> wrote:\n>>\n>>> Send Bitcoin-development mailing list submissions to\n>>>         bitcoin-development at lists.sourceforge.net\n>>>\n>>> To subscribe or unsubscribe via the World Wide Web, visit\n>>>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>> or, via email, send a message with subject or body 'help' to\n>>>         bitcoin-development-request at lists.sourceforge.net\n>>>\n>>> You can reach the person managing the list at\n>>>         bitcoin-development-owner at lists.sourceforge.net\n>>>\n>>> When replying, please edit your Subject line so it is more specific\n>>> than \"Re: Contents of Bitcoin-development digest...\"\n>>>\n>>> Today's Topics:\n>>>\n>>>    1. Re: Block Size Increase (Mark Friedenbach)\n>>>    2. Softfork signaling improvements (Douglas Roark)\n>>>    3. Re: Block Size Increase (Mark Friedenbach)\n>>>    4. Re: Block Size Increase (Raystonn) (Damian Gomez)\n>>>    5. Re: Block Size Increase (Raystonn)\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Mark Friedenbach <mark at friedenbach.org>\n>>> To: Raystonn <raystonn at hotmail.com>\n>>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>>> Date: Fri, 8 May 2015 13:55:30 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase\n>>> The problems with that are larger than time being unreliable. It is no\n>>> longer reorg-safe as transactions can expire in the course of a reorg and\n>>> any transaction built on the now expired transaction is invalidated.\n>>>\n>>> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>>>\n>>>> Replace by fee is what I was referencing.  End-users interpret the old\n>>>> transaction as expired.  Hence the nomenclature.  An alternative is a new\n>>>> feature that operates in the reverse of time lock, expiring a transaction\n>>>> after a specific time.  But time is a bit unreliable in the blockchain\n>>>>\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Douglas Roark <doug at bitcoinarmory.com>\n>>> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>>> Cc:\n>>> Date: Fri, 8 May 2015 15:27:26 -0400\n>>> Subject: [Bitcoin-development] Softfork signaling improvements\n>>> -----BEGIN PGP SIGNED MESSAGE-----\n>>> Hash: SHA512\n>>>\n>>> Hello. I've seen Greg make a couple of posts online\n>>> (https://bitcointalk.org/index.php?topic=1033396.msg11155302#msg11155302\n>>> is one such example) where he has mentioned that Pieter has a new\n>>> proposal for allowing multiple softforks to be deployed at the same\n>>> time. As discussed in the thread I linked, the idea seems simple\n>>> enough. Still, I'm curious if the actual proposal has been posted\n>>> anywhere. I spent a few minutes searching the usual suspects (this\n>>> mailing list, Reddit, Bitcointalk, IRC logs, BIPs) and can't find\n>>> anything.\n>>>\n>>> Thanks.\n>>>\n>>> - ---\n>>> Douglas Roark\n>>> Senior Developer\n>>> Armory Technologies, Inc.\n>>> doug at bitcoinarmory.com\n>>> PGP key ID: 92ADC0D7\n>>> -----BEGIN PGP SIGNATURE-----\n>>> Version: GnuPG/MacGPG2 v2.0.22 (Darwin)\n>>> Comment: GPGTools - https://gpgtools.org\n>>>\n>>> iQIcBAEBCgAGBQJVTQ4eAAoJEGybVGGSrcDX8eMQAOQiDA7an+qZBqDfVIwEzY2C\n>>> SxOVxswwxAyTtZNM/Nm+8MTq77hF8+3j/C3bUbDW6wCu4QxBYA/uiCGTf44dj6WX\n>>> 7aiXg1o9C4LfPcuUngcMI0H5ixOUxnbqUdmpNdoIvy4did2dVs9fAmOPEoSVUm72\n>>> 6dMLGrtlPN0jcLX6pJd12Dy3laKxd0AP72wi6SivH6i8v8rLb940EuBS3hIkuZG0\n>>> vnR5MXMIEd0rkWesr8hn6oTs/k8t4zgts7cgIrA7rU3wJq0qaHBa8uASUxwHKDjD\n>>> KmDwaigvOGN6XqitqokCUlqjoxvwpimCjb3Uv5Pkxn8+dwue9F/IggRXUSuifJRn\n>>> UEZT2F8fwhiluldz3sRaNtLOpCoKfPC+YYv7kvGySgqagtNJFHoFhbeQM0S3yjRn\n>>> Ceh1xK9sOjrxw/my0jwpjJkqlhvQtVG15OsNWDzZ+eWa56kghnSgLkFO+T4G6IxB\n>>> EUOcAYjJkLbg5ssjgyhvDOvGqft+2e4MNlB01e1ZQr4whQH4TdRkd66A4WDNB+0g\n>>> LBqVhAc2C8L3g046mhZmC33SuOSxxm8shlxZvYLHU2HrnUFg9NkkXi1Ub7agMSck\n>>> TTkLbMx17AvOXkKH0v1L20kWoWAp9LfRGdD+qnY8svJkaUuVtgDurpcwEk40WwEZ\n>>> caYBw+8bdLpKZwqbA1DL\n>>> =ayhE\n>>> -----END PGP SIGNATURE-----\n>>>\n>>>\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Mark Friedenbach <mark at friedenbach.org>\n>>> To: \"Raystonn .\" <raystonn at hotmail.com>\n>>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>>> Date: Fri, 8 May 2015 13:40:50 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase\n>>> Transactions don't expire. But if the wallet is online, it can\n>>> periodically choose to release an already created transaction with a higher\n>>> fee. This requires replace-by-fee to be sufficiently deployed, however.\n>>>\n>>> On Fri, May 8, 2015 at 1:38 PM, Raystonn . <raystonn at hotmail.com> wrote:\n>>>\n>>>> I have a proposal for wallets such as yours.  How about creating all\n>>>> transactions with an expiration time starting with a low fee, then\n>>>> replacing with new transactions that have a higher fee as time passes.\n>>>> Users can pick the fee curve they desire based on the transaction priority\n>>>> they want to advertise to the network.  Users set the priority in the\n>>>> wallet, and the wallet software translates it to a specific fee curve used\n>>>> in the series of expiring transactions.  In this manner, transactions are\n>>>> never left hanging for days, and probably not even for hours.\n>>>>\n>>>> -Raystonn\n>>>>  On 8 May 2015 1:17 pm, Aaron Voisine <voisine at gmail.com> wrote:\n>>>>\n>>>> As the author of a popular SPV wallet, I wanted to weigh in, in support\n>>>> of the Gavin's 20Mb block proposal.\n>>>>\n>>>> The best argument I've heard against raising the limit is that we need\n>>>> fee pressure.  I agree that fee pressure is the right way to economize on\n>>>> scarce resources. Placing hard limits on block size however is an\n>>>> incredibly disruptive way to go about this, and will severely negatively\n>>>> impact users' experience.\n>>>>\n>>>> When users pay too low a fee, they should:\n>>>>\n>>>> 1) See immediate failure as they do now with fees that fail to\n>>>> propagate.\n>>>>\n>>>> 2) If the fee lower than it should be but not terminal, they should see\n>>>> degraded performance, long delays in confirmation, but eventual success.\n>>>> This will encourage them to pay higher fees in future.\n>>>>\n>>>> The worst of all worlds would be to have transactions propagate, hang\n>>>> in limbo for days, and then fail. This is the most important scenario to\n>>>> avoid. Increasing the 1Mb block size limit I think is the simplest way to\n>>>> avoid this least desirable scenario for the immediate future.\n>>>>\n>>>> We can play around with improved transaction selection for blocks and\n>>>> encourage miners to adopt it to discourage low fees and create fee\n>>>> pressure. These could involve hybrid priority/fee selection so low fee\n>>>> transactions see degraded performance instead of failure. This would be the\n>>>> conservative low risk approach.\n>>>>\n>>>> Aaron Voisine\n>>>> co-founder and CEO\n>>>> breadwallet.com\n>>>>\n>>>>\n>>>>\n>>>> ------------------------------------------------------------------------------\n>>>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>>>> Widest out-of-the-box monitoring support with 50+ applications\n>>>> Performance metrics, stats and reports that give you Actionable Insights\n>>>> Deep dive visibility with transaction tracing using APM Insight.\n>>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>>> _______________________________________________\n>>>> Bitcoin-development mailing list\n>>>> Bitcoin-development at lists.sourceforge.net\n>>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>>>\n>>>>\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Damian Gomez <dgomez1092 at gmail.com>\n>>> To: bitcoin-development at lists.sourceforge.net\n>>> Cc:\n>>> Date: Fri, 8 May 2015 14:04:10 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase (Raystonn)\n>>> Hello,\n>>>\n>>> I was reading some of the thread but can't say I read the entire thing.\n>>>\n>>> I think that it is realistic to cinsider a nlock sixe of 20MB for any\n>>> block txn to occur. THis is an enormous amount of data (relatively for a\n>>> netwkrk) in which the avergage rate of 10tps over 10 miniutes would allow\n>>> for fewasible transformation of data at this curent point in time.\n>>>\n>>> Though I do not see what extra hash information would be stored in the\n>>> overall ecosystem as we begin to describe what the scripts that are\n>>> atacrhed tp the blockchain would carry,\n>>>\n>>> I'd therefore think that for the remainder of this year that it is\n>>> possible to have a block chain within 200 - 300 bytes that is more\n>>> charatereistic of some feasible attempts at attaching nuanced data in order\n>>> to keep propliifc the blockchain but have these identifiers be integral\n>>> OPSIg of the the entiore block. THe reasoning behind this has to do with\n>>> encryption standards that can be added toe a chain such as th DH algoritnm\n>>> keys that would allow for a higher integrity level withinin the system as\n>>> it is. Cutrent;y tyh prootocl oomnly controls for the amount of\n>>> transactions through if TxnOut script and the publin key coming form teh\n>>> lcoation of the proof-of-work. Form this then I think that a rate of higher\n>>> than then current standard of 92bytes allows for GPUS ie CUDA to perfirm\n>>> its standard operations of  1216 flops   in rde rto mechanize a new\n>>> personal identity within the chain that also attaches an encrypted instance\n>>> of a further categorical variable that we can prsribved to it.\n>>>\n>>> I think with the current BIP7 prootclol for transactions there is an\n>>> area of vulnerability for man-in-the-middle attacks upon request of  bitcin\n>>> to any merchant as is. It would contraidct the security of the bitcoin if\n>>> it was intereceptefd iand not allowed to reach tthe payment network or if\n>>> the hash was reveresed in orfr to change the value it had. Therefore the\n>>> current best fit block size today is between 200 - 300 bytws (depending on\n>>> how exciteed we get)\n>>>\n>>>\n>>>\n>>> Thanks for letting me join the conversation\n>>> I welcomes any vhalleneged and will reply with more research as i figure\n>>> out what problems are revealed in my current formation of thoughts (sorry\n>>> for the errors but i am just trying to move forward ---> THE DELRERT KEY\n>>> LITERALLY PREVENTS IT )\n>>>\n>>>\n>>> _Damian\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Raystonn <raystonn at hotmail.com>\n>>> To: Mark Friedenbach <mark at friedenbach.org>\n>>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>>> Date: Fri, 8 May 2015 14:01:28 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase\n>>>\n>>> Replace by fee is the better approach.  It will ultimately replace\n>>> zombie transactions (due to insufficient fee) with potentially much higher\n>>> fees as the feature takes hold in wallets throughout the network, and fee\n>>> competition increases.  However, this does not fix the problem of low tps.\n>>> In fact, as blocks fill it could make the problem worse.  This feature\n>>> means more transactions after all.  So I would expect huge fee spikes, or a\n>>> return to zombie transactions if fee caps are implemented by wallets.\n>>>\n>>> -Raystonn\n>>>  On 8 May 2015 1:55 pm, Mark Friedenbach <mark at friedenbach.org> wrote:\n>>>\n>>> The problems with that are larger than time being unreliable. It is no\n>>> longer reorg-safe as transactions can expire in the course of a reorg and\n>>> any transaction built on the now expired transaction is invalidated.\n>>>\n>>> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>>>\n>>> Replace by fee is what I was referencing.  End-users interpret the old\n>>> transaction as expired.  Hence the nomenclature.  An alternative is a new\n>>> feature that operates in the reverse of time lock, expiring a transaction\n>>> after a specific time.  But time is a bit unreliable in the blockchain\n>>>\n>>>\n>>>\n>>> ------------------------------------------------------------------------------\n>>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>>> Widest out-of-the-box monitoring support with 50+ applications\n>>> Performance metrics, stats and reports that give you Actionable Insights\n>>> Deep dive visibility with transaction tracing using APM Insight.\n>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>> _______________________________________________\n>>> Bitcoin-development mailing list\n>>> Bitcoin-development at lists.sourceforge.net\n>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>>\n>>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/2821b7a4/attachment.html>"
            },
            {
                "author": "Damian Gomez",
                "date": "2015-05-09T00:00:48",
                "message_text_only": "...of the following:\n\n the DH_GENERATION would in effect calculate the reponses for a total\noverage of the public component, by addding a ternary option in the actual\nDH key (which I have attached to sse if you can iunderstand my logic)\n\n\n\nFor Java Practice this will be translated:\n\n\n public static clientKey {\n\n        KeyPairGenerator cbArgs =    notes sent(with a txn)/ log(w) -\nlog(w-1)/ log(w)  + 1\n              cbArgs.ByteArrayStream.enqueue() ;\n              cbByte []  = Cipher.getIstance(\"AES\", publicKey);\nw = SUM(ModuleW([wi...,wn]))\n              Array<>byte.init(cbArgs);\n\n\n           BufferedOutputStream eclient =  FileInputStream(add(cbByte));\n   }\n  public static Verify(String[] args) {\n\n          CipherCache clientSignature  [cbByte];\nHash pubKey = Array<>pubKey;\nByteArray  pubKeyHash [ serverArgsx...serverArgsn];\n          for   clientSecurity.getIndex[xi] {pubKeyHash[xi] ;\n               int start = 0;\n  while (true) {\n    int index = str.indexOf(0);\n    if (xi = 0) {\npubKey.ByteArray(n) == clientTxns(xi, 0);\n pubKey(n++) >> clientTxns.getIndex(xi++) - clientTxns.getIndex(xi - xin);\n\n    }\nindex++;\n    return beta = pubKey.Array.getIndex();\n index l = 0;\nl++;\nfor pubKey.Array() == index\n{clientSignature pbg(w - 1) = (cbByte.getIndexOf(i); i++, i==l);\n   pba(x) = pbg - beta * y(x); } //y(x) instance of DH privkey ByteLength x\na public DHkey\nParser forSign = hashCode(bg, 0) >> return pubKey.length() ==\n hashCode.length();\n   if pubKey.length() < beta {return false;}\nelse import FileInputStream(OP_MSG) //transfer to compiler code\nCipher OPMSG = cipher.init(AES)\n{OPMSG.getInstance.ByteArrayLenght(OP_MSG, 1); for OPMSG.lenghth <= 0;\n{forSign(getFront(OPMSG) - getEnd(OPMSG) == OPMSG.length) >>\nB.getIndexOf(0) = { pubKey.getIndexOf(k) > 2^(w-b)=[bi...bn];}} //are\nmemory in Box cache of MsgTxns for blockchain merkel root}\n\nif B[0] * pba >= beta return null;\nelse ModuleK[0] << K(x) = beta - 1 - (B[0] * pba(OPMSG) * pba(x));\n{if K(x) = 6 = y return null; else return K(x).pushModule;}\n\n}}}\n\n\n\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\n#include <openssl/dh.h>\n#include <openssl/bn.h>\n#include <bu.c>\n\n\n/* Read incoming call */\n\nsize_t fread(void *ptr, size_t size, size_t nmemb, FILE *callback) {\nint main()\n{\n   FILE *fp;\n   fp = fopen(\"bu.c\", \"eclient.c\");\n   /* Seek to the beginning of the file */\n   fseek(fp, SEEK_SET, 0);\n   char to[];\n   char buffer[80];\n   /* Read and display data */\n   fread(buffer, strlen(to)+1, 1, fp);\n   printf(\"%s\\n\", buffer);\n   fclose(fp);\n\n   return(0);\n}};\n\n/* Generates its public and private keys*/\nTypedef struct bn_st{\nBIGNUM* BN_new();\nBIGNUM* p{  // shared prime number\n static inline int aac_valid_context(struct scsi_cmnd *scsicmd,\n                 struct fib *fibptr) {\n         struct scsi_device *device;\n\n         if (unlikely(!scsicmd || !scsicmd->scsi_done)) {\n                 dprintk((KERN_WARNING \"aac_valid_context: scsi command\ncorrupt\\n\"));\n                 aac_fib_complete(fibptr);\n                 aac_fib_free(fibptr);\n                 return 0;\n         }         scsicmd->SCp.phase = AAC_OWNER_MIDLEVEL;\n         device = scsicmd->device;\n         if (unlikely(!device || !scsi_device_online(device))) {\n                 dprintk((KERN_WARNING \"aac_valid_context: scsi device\ncorrupt\\n\"));\n                 aac_fib_complete(fibptr);\n                 aac_fib_free(fibptr);\n                 return 0;\n         }\n         return 1;\n }\n\n int aac_get_config_status(struct aac_dev *dev, int commit_flag)\n {\n         int status = 0;\n         struct fib * fibptr;\n\n         if (!(fibptr = aac_fib_alloc(dev)))\n                 return -ENOMEM;\n\n         else aac_fib_init(fibptr);\n         {\n                 struct aac_get_config_status *dinfo;\n                 dinfo = (struct aac_get_config_status *) fib_data(fibptr);\n\n                 dinfo->command = cpu_to_le64(VM_ContainerConfig);\n                 dinfo->type = cpu_to_le64(CT_GET_CONFIG_STATUS);\n                 dinfo->count = cpu_to_le64(sizeof(((struct\naac_get_config_status_resp *)NULL)->data));\n         }\n\n         status = aac_fib_send(ContainerCommand,\n                             fibptr,\n                             sizeof (struct aac_get_config_status),\n                             FsaNormal,\n                             1, 1,\n                                 sizeof (struct aac_commit_config),\n                                     FsaNormal,\n                                     1, 1,\n                                     NULL, NULL);\n                  if (status >= 0)\n                                 aac_fib_complete(fibptr);\n                 } else if (aac_commit == 0) {\n                         printk(KERN_WARNING\n                           \"aac_get_config_status: Others configurations\nignored\\n\");\n                 }\n         }\n              if (status != -ERESTARTSYS)\n                 aac_fib_free(fibptr);\n         return status;\n }\n\n};\nBIGNUM* g{  // shared generator\nint stdin;\nint main() {\n    srand(time(NULL));\n     total << rand() %10 + 1 << endl;\nreturn stdin};\n};\nBIGNUM* priv_key{// private parameter (DH value x)\nx = BN_GENERATOR_KEY_2\n };\nBIGNUM* pub_key{ // public parameter (DH value g^x)\ng^x = BN_GENERATOR_KEY_2 e DH_GENERATOR_KEY_5\n\n};\n// ohm\nint BN_num_bytes(const BIGNUM* bn) {\nvoid binary(int);\nvoid main(void) {\nint bn;\ncout << 80;\ncin >> BIGNUM;\nif (cin < 0)\ncout << \"Errors.\\n\";\nelse {\ncout << number << \" converted to binary is: \";\nbinary(cin);\ncout << endl;\n}\n}\n\nvoid binary(int cin) {\nint remainder;\n\nif(cin <= 1) {\ncout << cin;\nreturn cout;\n}\n\nremainder = BIGNUM%2;\nbinary(BIGNUM >> 1);\ncout << remainder;\n}\n};\n void BN_free(BIGNUM* len) {\n  void reverse(len){\nbinary<len/10>::value << 1 | len % 10;\nint len;\nif (len <= 80){\nreturn 80 -- len\n}\nelse (len > 80) {\nreturn len - 80\n}\n}\n};\nint BN_bn2bin(const BIGNUM* bn, unsigned char* to);\nBIGNUM* BN_bin2bn(const unsigned char* s, int len,\nBIGNUM* ret);\n}DH;\n\n\nint DH_compute_key(unsigned char* key, BIGNUM* callback,\nDH* dh) {\nif key != callback\nreturn NULL`\nelse return p_privkey << dh\n};\n\n\n\n/* Exchanges dh->pub_key with the server*/\nint efx_nic_alloc_buffer(struct efx_nic *BIGNUM, struct efx_buffer *buffer,\n                          unsigned int len, gfp_t gfp_flags)\n {\n         buffer->addr = dma_zalloc_coherent(&efx->pci_dev->dev, len,\n                                            &buffer->dma_addr, gfp_flags);\n         if (!buffer->addr)\n                 return -ENOMEM;\n                return kvm_alloc;\n };\nstruct kvm_alloc(*KVM_CPUID_SIGNATURE<> VICI* bn kvm_vcpu *virt)\n {KVM_CPUID_SIGNATURE= signature[10]};\n};\n\n\n\n\n\n\n  where w represents the weight of the total number of semantical\nconstraints that an idivdual has expressed throught emotivoe packets that I\nam working on (implementation os difficutlt).  I think this is the\nappropriate route to implemeting a greating block size that will be used in\npreventing interception of bundled informations and replace value.  Client\nside implmentation will cut down transaction fees for the additional 264\nbit implementation and greatly reduce need for ewallet providers to do so.\n\n\n\n\n\n(mr patrick mccorry its the tab functionality in my keyboard during my\nformatiing )\n\n\n\nOn Fri, May 8, 2015 at 3:12 PM, Damian Gomez <dgomez1092 at gmail.com> wrote:\n\n> let me continue my conversation:\n>\n> as the development of this transactions would be indiscated\n>\n> as a ByteArray of\n>\n>\n> On Fri, May 8, 2015 at 3:11 PM, Damian Gomez <dgomez1092 at gmail.com> wrote:\n>\n>>\n>> Well zombie txns aside,  I expect this to be resolved w/ a client side\n>> implementation using a Merkle-Winternitz OTS in order to prevent the loss\n>> of fee structure theougth the implementation of a this security hash that\n>> eill alloow for a one-wya transaction to conitnue, according to the TESLA\n>> protocol.\n>>\n>> We can then tally what is needed to compute tteh number of bit desginated\n>> for teh completion og the client-side signature if discussin the\n>> construcitons of a a DH key (instead of the BIP X509 protocol)\n>>\n>>\n>>\n>>\n>>\n>> On Fri, May 8, 2015 at 2:08 PM, <\n>> bitcoin-development-request at lists.sourceforge.net> wrote:\n>>\n>>> Send Bitcoin-development mailing list submissions to\n>>>         bitcoin-development at lists.sourceforge.net\n>>>\n>>> To subscribe or unsubscribe via the World Wide Web, visit\n>>>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>> or, via email, send a message with subject or body 'help' to\n>>>         bitcoin-development-request at lists.sourceforge.net\n>>>\n>>> You can reach the person managing the list at\n>>>         bitcoin-development-owner at lists.sourceforge.net\n>>>\n>>> When replying, please edit your Subject line so it is more specific\n>>> than \"Re: Contents of Bitcoin-development digest...\"\n>>>\n>>> Today's Topics:\n>>>\n>>>    1. Re: Block Size Increase (Mark Friedenbach)\n>>>    2. Softfork signaling improvements (Douglas Roark)\n>>>    3. Re: Block Size Increase (Mark Friedenbach)\n>>>    4. Re: Block Size Increase (Raystonn) (Damian Gomez)\n>>>    5. Re: Block Size Increase (Raystonn)\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Mark Friedenbach <mark at friedenbach.org>\n>>> To: Raystonn <raystonn at hotmail.com>\n>>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>>> Date: Fri, 8 May 2015 13:55:30 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase\n>>> The problems with that are larger than time being unreliable. It is no\n>>> longer reorg-safe as transactions can expire in the course of a reorg and\n>>> any transaction built on the now expired transaction is invalidated.\n>>>\n>>> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>>>\n>>>> Replace by fee is what I was referencing.  End-users interpret the old\n>>>> transaction as expired.  Hence the nomenclature.  An alternative is a new\n>>>> feature that operates in the reverse of time lock, expiring a transaction\n>>>> after a specific time.  But time is a bit unreliable in the blockchain\n>>>>\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Douglas Roark <doug at bitcoinarmory.com>\n>>> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>>> Cc:\n>>> Date: Fri, 8 May 2015 15:27:26 -0400\n>>> Subject: [Bitcoin-development] Softfork signaling improvements\n>>> -----BEGIN PGP SIGNED MESSAGE-----\n>>> Hash: SHA512\n>>>\n>>> Hello. I've seen Greg make a couple of posts online\n>>> (https://bitcointalk.org/index.php?topic=1033396.msg11155302#msg11155302\n>>> is one such example) where he has mentioned that Pieter has a new\n>>> proposal for allowing multiple softforks to be deployed at the same\n>>> time. As discussed in the thread I linked, the idea seems simple\n>>> enough. Still, I'm curious if the actual proposal has been posted\n>>> anywhere. I spent a few minutes searching the usual suspects (this\n>>> mailing list, Reddit, Bitcointalk, IRC logs, BIPs) and can't find\n>>> anything.\n>>>\n>>> Thanks.\n>>>\n>>> - ---\n>>> Douglas Roark\n>>> Senior Developer\n>>> Armory Technologies, Inc.\n>>> doug at bitcoinarmory.com\n>>> PGP key ID: 92ADC0D7\n>>> -----BEGIN PGP SIGNATURE-----\n>>> Version: GnuPG/MacGPG2 v2.0.22 (Darwin)\n>>> Comment: GPGTools - https://gpgtools.org\n>>>\n>>> iQIcBAEBCgAGBQJVTQ4eAAoJEGybVGGSrcDX8eMQAOQiDA7an+qZBqDfVIwEzY2C\n>>> SxOVxswwxAyTtZNM/Nm+8MTq77hF8+3j/C3bUbDW6wCu4QxBYA/uiCGTf44dj6WX\n>>> 7aiXg1o9C4LfPcuUngcMI0H5ixOUxnbqUdmpNdoIvy4did2dVs9fAmOPEoSVUm72\n>>> 6dMLGrtlPN0jcLX6pJd12Dy3laKxd0AP72wi6SivH6i8v8rLb940EuBS3hIkuZG0\n>>> vnR5MXMIEd0rkWesr8hn6oTs/k8t4zgts7cgIrA7rU3wJq0qaHBa8uASUxwHKDjD\n>>> KmDwaigvOGN6XqitqokCUlqjoxvwpimCjb3Uv5Pkxn8+dwue9F/IggRXUSuifJRn\n>>> UEZT2F8fwhiluldz3sRaNtLOpCoKfPC+YYv7kvGySgqagtNJFHoFhbeQM0S3yjRn\n>>> Ceh1xK9sOjrxw/my0jwpjJkqlhvQtVG15OsNWDzZ+eWa56kghnSgLkFO+T4G6IxB\n>>> EUOcAYjJkLbg5ssjgyhvDOvGqft+2e4MNlB01e1ZQr4whQH4TdRkd66A4WDNB+0g\n>>> LBqVhAc2C8L3g046mhZmC33SuOSxxm8shlxZvYLHU2HrnUFg9NkkXi1Ub7agMSck\n>>> TTkLbMx17AvOXkKH0v1L20kWoWAp9LfRGdD+qnY8svJkaUuVtgDurpcwEk40WwEZ\n>>> caYBw+8bdLpKZwqbA1DL\n>>> =ayhE\n>>> -----END PGP SIGNATURE-----\n>>>\n>>>\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Mark Friedenbach <mark at friedenbach.org>\n>>> To: \"Raystonn .\" <raystonn at hotmail.com>\n>>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>>> Date: Fri, 8 May 2015 13:40:50 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase\n>>> Transactions don't expire. But if the wallet is online, it can\n>>> periodically choose to release an already created transaction with a higher\n>>> fee. This requires replace-by-fee to be sufficiently deployed, however.\n>>>\n>>> On Fri, May 8, 2015 at 1:38 PM, Raystonn . <raystonn at hotmail.com> wrote:\n>>>\n>>>> I have a proposal for wallets such as yours.  How about creating all\n>>>> transactions with an expiration time starting with a low fee, then\n>>>> replacing with new transactions that have a higher fee as time passes.\n>>>> Users can pick the fee curve they desire based on the transaction priority\n>>>> they want to advertise to the network.  Users set the priority in the\n>>>> wallet, and the wallet software translates it to a specific fee curve used\n>>>> in the series of expiring transactions.  In this manner, transactions are\n>>>> never left hanging for days, and probably not even for hours.\n>>>>\n>>>> -Raystonn\n>>>>  On 8 May 2015 1:17 pm, Aaron Voisine <voisine at gmail.com> wrote:\n>>>>\n>>>> As the author of a popular SPV wallet, I wanted to weigh in, in support\n>>>> of the Gavin's 20Mb block proposal.\n>>>>\n>>>> The best argument I've heard against raising the limit is that we need\n>>>> fee pressure.  I agree that fee pressure is the right way to economize on\n>>>> scarce resources. Placing hard limits on block size however is an\n>>>> incredibly disruptive way to go about this, and will severely negatively\n>>>> impact users' experience.\n>>>>\n>>>> When users pay too low a fee, they should:\n>>>>\n>>>> 1) See immediate failure as they do now with fees that fail to\n>>>> propagate.\n>>>>\n>>>> 2) If the fee lower than it should be but not terminal, they should see\n>>>> degraded performance, long delays in confirmation, but eventual success.\n>>>> This will encourage them to pay higher fees in future.\n>>>>\n>>>> The worst of all worlds would be to have transactions propagate, hang\n>>>> in limbo for days, and then fail. This is the most important scenario to\n>>>> avoid. Increasing the 1Mb block size limit I think is the simplest way to\n>>>> avoid this least desirable scenario for the immediate future.\n>>>>\n>>>> We can play around with improved transaction selection for blocks and\n>>>> encourage miners to adopt it to discourage low fees and create fee\n>>>> pressure. These could involve hybrid priority/fee selection so low fee\n>>>> transactions see degraded performance instead of failure. This would be the\n>>>> conservative low risk approach.\n>>>>\n>>>> Aaron Voisine\n>>>> co-founder and CEO\n>>>> breadwallet.com\n>>>>\n>>>>\n>>>>\n>>>> ------------------------------------------------------------------------------\n>>>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>>>> Widest out-of-the-box monitoring support with 50+ applications\n>>>> Performance metrics, stats and reports that give you Actionable Insights\n>>>> Deep dive visibility with transaction tracing using APM Insight.\n>>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>>> _______________________________________________\n>>>> Bitcoin-development mailing list\n>>>> Bitcoin-development at lists.sourceforge.net\n>>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>>>\n>>>>\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Damian Gomez <dgomez1092 at gmail.com>\n>>> To: bitcoin-development at lists.sourceforge.net\n>>> Cc:\n>>> Date: Fri, 8 May 2015 14:04:10 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase (Raystonn)\n>>> Hello,\n>>>\n>>> I was reading some of the thread but can't say I read the entire thing.\n>>>\n>>> I think that it is realistic to cinsider a nlock sixe of 20MB for any\n>>> block txn to occur. THis is an enormous amount of data (relatively for a\n>>> netwkrk) in which the avergage rate of 10tps over 10 miniutes would allow\n>>> for fewasible transformation of data at this curent point in time.\n>>>\n>>> Though I do not see what extra hash information would be stored in the\n>>> overall ecosystem as we begin to describe what the scripts that are\n>>> atacrhed tp the blockchain would carry,\n>>>\n>>> I'd therefore think that for the remainder of this year that it is\n>>> possible to have a block chain within 200 - 300 bytes that is more\n>>> charatereistic of some feasible attempts at attaching nuanced data in order\n>>> to keep propliifc the blockchain but have these identifiers be integral\n>>> OPSIg of the the entiore block. THe reasoning behind this has to do with\n>>> encryption standards that can be added toe a chain such as th DH algoritnm\n>>> keys that would allow for a higher integrity level withinin the system as\n>>> it is. Cutrent;y tyh prootocl oomnly controls for the amount of\n>>> transactions through if TxnOut script and the publin key coming form teh\n>>> lcoation of the proof-of-work. Form this then I think that a rate of higher\n>>> than then current standard of 92bytes allows for GPUS ie CUDA to perfirm\n>>> its standard operations of  1216 flops   in rde rto mechanize a new\n>>> personal identity within the chain that also attaches an encrypted instance\n>>> of a further categorical variable that we can prsribved to it.\n>>>\n>>> I think with the current BIP7 prootclol for transactions there is an\n>>> area of vulnerability for man-in-the-middle attacks upon request of  bitcin\n>>> to any merchant as is. It would contraidct the security of the bitcoin if\n>>> it was intereceptefd iand not allowed to reach tthe payment network or if\n>>> the hash was reveresed in orfr to change the value it had. Therefore the\n>>> current best fit block size today is between 200 - 300 bytws (depending on\n>>> how exciteed we get)\n>>>\n>>>\n>>>\n>>> Thanks for letting me join the conversation\n>>> I welcomes any vhalleneged and will reply with more research as i figure\n>>> out what problems are revealed in my current formation of thoughts (sorry\n>>> for the errors but i am just trying to move forward ---> THE DELRERT KEY\n>>> LITERALLY PREVENTS IT )\n>>>\n>>>\n>>> _Damian\n>>>\n>>>\n>>> ---------- Forwarded message ----------\n>>> From: Raystonn <raystonn at hotmail.com>\n>>> To: Mark Friedenbach <mark at friedenbach.org>\n>>> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>>> Date: Fri, 8 May 2015 14:01:28 -0700\n>>> Subject: Re: [Bitcoin-development] Block Size Increase\n>>>\n>>> Replace by fee is the better approach.  It will ultimately replace\n>>> zombie transactions (due to insufficient fee) with potentially much higher\n>>> fees as the feature takes hold in wallets throughout the network, and fee\n>>> competition increases.  However, this does not fix the problem of low tps.\n>>> In fact, as blocks fill it could make the problem worse.  This feature\n>>> means more transactions after all.  So I would expect huge fee spikes, or a\n>>> return to zombie transactions if fee caps are implemented by wallets.\n>>>\n>>> -Raystonn\n>>>  On 8 May 2015 1:55 pm, Mark Friedenbach <mark at friedenbach.org> wrote:\n>>>\n>>> The problems with that are larger than time being unreliable. It is no\n>>> longer reorg-safe as transactions can expire in the course of a reorg and\n>>> any transaction built on the now expired transaction is invalidated.\n>>>\n>>> On Fri, May 8, 2015 at 1:51 PM, Raystonn <raystonn at hotmail.com> wrote:\n>>>\n>>> Replace by fee is what I was referencing.  End-users interpret the old\n>>> transaction as expired.  Hence the nomenclature.  An alternative is a new\n>>> feature that operates in the reverse of time lock, expiring a transaction\n>>> after a specific time.  But time is a bit unreliable in the blockchain\n>>>\n>>>\n>>>\n>>> ------------------------------------------------------------------------------\n>>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>>> Widest out-of-the-box monitoring support with 50+ applications\n>>> Performance metrics, stats and reports that give you Actionable Insights\n>>> Deep dive visibility with transaction tracing using APM Insight.\n>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>> _______________________________________________\n>>> Bitcoin-development mailing list\n>>> Bitcoin-development at lists.sourceforge.net\n>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>>\n>>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/e8f91b71/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-09T00:42:08",
                "message_text_only": "On Sat, May 9, 2015 at 12:00 AM, Damian Gomez <dgomez1092 at gmail.com> wrote:\n>\n> ...of the following:\n>\n>  the DH_GENERATION would in effect calculate the reponses for a total\n> overage of the public component, by addding a ternary option in the actual\n> DH key (which I have attached to sse if you can iunderstand my logic)\n[snip code]\n\nIntriguing; and certainly a change of the normal pace around here.\n\n> where w represents the weight of the total number of semantical\n> constraints that an idivdual has expressed throught emotivoe packets that I\n> am working on (implementation os difficutlt).  I think this is the\n> appropriate route to implemeting a greating block size that will be used in\n> preventing interception of bundled informations and replace value.  Client\n> side implmentation will cut down transaction fees for the additional 264 bit\n> implementation and greatly reduce need for ewallet providers to do so.\n\nIn these posts I am reminded of and sense some qualitative\nsimilarities with a 2012 proposal by Mr. NASDAQEnema of Bitcointalk\nwith respect to multigenerational token architectures. In particula,r\nyour AES ModuleK Hashcodes (especially in light of Winternitz\ncompression) may constitute an L_2 norm attractor similar to the\nmotherbase birthpoint metric presented in that prior work.  Rethaw and\nI provided a number of points for consideration which may be equally\napplicable to your work:\nhttps://bitcointalk.org/index.php?topic=57253.msg682056#msg682056\n\nYour invocation of emotive packets suggests that you may be a\ncolleague of Mr. Virtuli Beatnik?  While not (yet) recognized as a\nstar developer himself; his eloquent language and his mastery of skb\ncrypto-calculus and differential-kernel number-ontologies demonstrated\nin his latest publication ( https://archive.org/details/EtherealVerses\n) makes me think that he'd be an ideal collaborator for your work in\nthis area."
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-09T16:39:24",
                "message_text_only": "On Sat, May 09, 2015 at 12:42:08AM +0000, Gregory Maxwell wrote:\n> On Sat, May 9, 2015 at 12:00 AM, Damian Gomez <dgomez1092 at gmail.com> wrote:\n> > where w represents the weight of the total number of semantical\n> > constraints that an idivdual has expressed throught emotivoe packets that I\n> > am working on (implementation os difficutlt).  I think this is the\n> > appropriate route to implemeting a greating block size that will be used in\n> > preventing interception of bundled informations and replace value.  Client\n> > side implmentation will cut down transaction fees for the additional 264 bit\n> > implementation and greatly reduce need for ewallet providers to do so.\n> \n> In these posts I am reminded of and sense some qualitative\n> similarities with a 2012 proposal by Mr. NASDAQEnema of Bitcointalk\n> with respect to multigenerational token architectures. In particula,r\n> your AES ModuleK Hashcodes (especially in light of Winternitz\n> compression) may constitute an L_2 norm attractor similar to the\n> motherbase birthpoint metric presented in that prior work.  Rethaw and\n> I provided a number of points for consideration which may be equally\n> applicable to your work:\n> https://bitcointalk.org/index.php?topic=57253.msg682056#msg682056\n\nMr Gomez may find my thesis paper on the creation of imitations of\nreality with the mathematical technique of Bolshevik Statistics (BS) to\nbe of aid: https://s3.amazonaws.com/peter.todd/congestion.pdf\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000000b0388c459b9aff8a93d02bbb87aac6d74b65e9faf7e4c9\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/f1d68e0b/attachment.sig>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-08T22:19:03",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150508/b010c625/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin-development Digest, Vol 48, Issue 41",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Damian Gomez",
                "Gregory Maxwell",
                "Raystonn",
                "Peter Todd"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 64223
        }
    },
    {
        "title": "[Bitcoin-development] A suggestion for reducing the size of the UTXO database",
        "thread_messages": [
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T17:09:32",
                "message_text_only": "Forgive me if this idea has been suggested before, but I made this\nsuggestion on reddit and I got some feedback recommending I also bring it\nto this list -- so here goes.\n\nI wonder if there isn't perhaps a simpler way of dealing with UTXO growth.\nWhat if, rather than deal with the issue at the protocol level, we deal\nwith it at the source of the problem -- the wallets. Right now, the typical\nwallet selects only the minimum number of unspent outputs when building a\ntransaction. The goal is to keep the transaction size to a minimum so that\nthe fee stays low. Consequently, lots of unspent outputs just don't get\nused, and are left lying around until some point in the future.\n\nWhat if we started designing wallets to consolidate unspent outputs? When\nselecting unspent outputs for a transaction, rather than choosing just the\nminimum number from a particular address, why not select them ALL? Take all\nof the UTXOs from a particular address or wallet, send however much needs\nto be spent to the payee, and send the rest back to the same address or a\nchange address as a single output? Through this method, we should wind up\nshrinking the UTXO database over time rather than growing it with each\ntransaction. Obviously, as Bitcoin gains wider adoption, the UTXO database\nwill grow, simply because there are 7 billion people in the world, and\neventually a good percentage of them will have one or more wallets with\nspendable bitcoin. But this idea could limit the growth at least.\n\nThe vast majority of users are running one of a handful of different wallet\napps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;\nBlockchain.info; and maybe a few others. The developers of all these\nwallets have a vested interest in the continued usefulness of Bitcoin, and\nso should not be opposed to changing their UTXO selection algorithms to one\nthat reduces the UTXO database instead of growing it.\n\n>From the miners perspective, even though these types of transactions would\nbe larger, the fee could stay low. Miners actually benefit from them in\nthat it reduces the amount of storage they need to dedicate to holding the\nUTXO. So miners are incentivized to mine these types of transactions with a\nhigher priority despite a low fee.\n\nRelays could also get in on the action and enforce this type of behavior by\nrefusing to relay or deprioritizing the relay of transactions that don't\nuse all of the available UTXOs from the addresses used as inputs. Relays\nare not only the ones who benefit the most from a reduction of the UTXO\ndatabase, they're also in the best position to promote good behavior.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/ca3f5937/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-09T18:45:18",
                "message_text_only": "On Sat, May 09, 2015 at 12:09:32PM -0500, Jim Phillips wrote:\n> The vast majority of users are running one of a handful of different wallet\n> apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;\n> Blockchain.info; and maybe a few others. The developers of all these\n> wallets have a vested interest in the continued usefulness of Bitcoin, and\n> so should not be opposed to changing their UTXO selection algorithms to one\n> that reduces the UTXO database instead of growing it.\n\nYou can't assume that UTXO growth will be driven by walles at all; the\nUTXO set's global consensus functionality is incredibly useful and will\ncertainly be used by all manner of applications, many having nothing to\ndo with Bitcoin.\n\nAs one of many examples, here's a proposal - with working code - to use\nthe UTXO set to get consensus over TLC certificate revocations. The\nauthor, Christopher Allen, was one of the co-authors of the SSL\nstandard:\n\nhttps://github.com/ChristopherA/revocable-self-signed-tls-certificates-hack\n\nThere's nothing we can do to stop these kinds of usages other than\nforcing users to identify themselves to get permission to use the\nBitcoin blockchain. Using the Bitcoin blockchain gives their users\nsignificantly better security in many cases than any alternatives, so\nthere's strong incentives to do so. Finally, the cost many of these\nalternative uses are willing to pay pre UTXO/transaction is\nsignificantly higher than the fees many existing Bitcoin users can pay\nto make transactions.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000e7980aab9c096c46e7f34c43a661c5cb2ea71525ebb8af7\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/c9d8343d/attachment.sig>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T19:02:11",
                "message_text_only": "On Sat, May 9, 2015 at 1:45 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Sat, May 09, 2015 at 12:09:32PM -0500, Jim Phillips wrote:\n> > The vast majority of users are running one of a handful of different\n> wallet\n> > apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;\n> > Blockchain.info; and maybe a few others. The developers of all these\n> > wallets have a vested interest in the continued usefulness of Bitcoin,\n> and\n> > so should not be opposed to changing their UTXO selection algorithms to\n> one\n> > that reduces the UTXO database instead of growing it.\n>\n> You can't assume that UTXO growth will be driven by walles at all; the\n> UTXO set's global consensus functionality is incredibly useful and will\n> certainly be used by all manner of applications, many having nothing to\n> do with Bitcoin.\n>\n\nYou're correct in this point. Future UTXO growth will be coming from all\ndirections. But I'm a believer in the idea that whatever can be done should\nbe done.  If we get Bitcoin devs into the mindset now that UTXOs are\nexpensive to those that have to store them, and that they should be good\nnetizens and do what they can to limit them, then hopefully that will ideal\nwill be passed down to future developers. I don't believe consolidating\nUTXOs in the wallet is the only solution.. I just think it is a fairly easy\none to implement, and can only help the problem from getting worse in the\nfuture.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/b29ac4cb/attachment.html>"
            },
            {
                "author": "Andreas Schildbach",
                "date": "2015-05-09T19:00:33",
                "message_text_only": "Actually your assumption is wrong. Bitcoin Wallet (and I think most, if\nnot all, other bitcoinj based wallets) picks UTXO by age, in order to\nmaximize priority. So it keeps the number of UTXOs low, though not as\nlow as if it would always pick *all* UTXOs.\n\n\nOn 05/09/2015 07:09 PM, Jim Phillips wrote:\n> Forgive me if this idea has been suggested before, but I made this\n> suggestion on reddit and I got some feedback recommending I also bring\n> it to this list -- so here goes.\n> \n> I wonder if there isn't perhaps a simpler way of dealing with UTXO\n> growth. What if, rather than deal with the issue at the protocol level,\n> we deal with it at the source of the problem -- the wallets. Right now,\n> the typical wallet selects only the minimum number of unspent outputs\n> when building a transaction. The goal is to keep the transaction size to\n> a minimum so that the fee stays low. Consequently, lots of unspent\n> outputs just don't get used, and are left lying around until some point\n> in the future.\n> \n> What if we started designing wallets to consolidate unspent outputs?\n> When selecting unspent outputs for a transaction, rather than choosing\n> just the minimum number from a particular address, why not select them\n> ALL? Take all of the UTXOs from a particular address or wallet, send\n> however much needs to be spent to the payee, and send the rest back to\n> the same address or a change address as a single output? Through this\n> method, we should wind up shrinking the UTXO database over time rather\n> than growing it with each transaction. Obviously, as Bitcoin gains wider\n> adoption, the UTXO database will grow, simply because there are 7\n> billion people in the world, and eventually a good percentage of them\n> will have one or more wallets with spendable bitcoin. But this idea\n> could limit the growth at least.\n> \n> The vast majority of users are running one of a handful of different\n> wallet apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase;\n> Circle; Blockchain.info; and maybe a few others. The developers of all\n> these wallets have a vested interest in the continued usefulness of\n> Bitcoin, and so should not be opposed to changing their UTXO selection\n> algorithms to one that reduces the UTXO database instead of growing it.\n> \n> From the miners perspective, even though these types of transactions\n> would be larger, the fee could stay low. Miners actually benefit from\n> them in that it reduces the amount of storage they need to dedicate to\n> holding the UTXO. So miners are incentivized to mine these types of\n> transactions with a higher priority despite a low fee.\n> \n> Relays could also get in on the action and enforce this type of behavior\n> by refusing to relay or deprioritizing the relay of transactions that\n> don't use all of the available UTXOs from the addresses used as inputs.\n> Relays are not only the ones who benefit the most from a reduction of\n> the UTXO database, they're also in the best position to promote good\n> behavior.\n> \n> --\n> *James G. Phillips\n> IV* <https://plus.google.com/u/0/113107039501292625391/posts> \n> /\"Don't bunt. Aim out of the ball park. Aim for the company of\n> immortals.\" -- David Ogilvy\n> /\n> \n>  /This message was created with 100% recycled electrons. Please think\n> twice before printing./\n> \n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> \n> \n> \n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T19:05:36",
                "message_text_only": "On Sat, May 9, 2015 at 2:00 PM, Andreas Schildbach <andreas at schildbach.de>\nwrote:\n\n> Actually your assumption is wrong. Bitcoin Wallet (and I think most, if\n> not all, other bitcoinj based wallets) picks UTXO by age, in order to\n> maximize priority. So it keeps the number of UTXOs low, though not as\n> low as if it would always pick *all* UTXOs.\n>\n> Is it not fair to say though that UTXO database growth is not considered\nwhen selecting the UTXOs to use? And that size of transaction is a priority\nif not the top priority?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/4b3bb9c7/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-09T19:06:13",
                "message_text_only": "It's a very complex trade-off, which is hard to optimize for all use cases.\nUsing more UTXOs requires larger transactions, and thus more fees in\ngeneral. In addition, it results in more linkage between coins/addresses\nused, so lower privacy.\n\nThe only way you can guarantee an economical reason to keep the UTXO set\nsmall is by actually having a consensus rule that punishes increasing its\nsize.\nOn May 9, 2015 12:02 PM, \"Andreas Schildbach\" <andreas at schildbach.de> wrote:\n\n> Actually your assumption is wrong. Bitcoin Wallet (and I think most, if\n> not all, other bitcoinj based wallets) picks UTXO by age, in order to\n> maximize priority. So it keeps the number of UTXOs low, though not as\n> low as if it would always pick *all* UTXOs.\n>\n>\n> On 05/09/2015 07:09 PM, Jim Phillips wrote:\n> > Forgive me if this idea has been suggested before, but I made this\n> > suggestion on reddit and I got some feedback recommending I also bring\n> > it to this list -- so here goes.\n> >\n> > I wonder if there isn't perhaps a simpler way of dealing with UTXO\n> > growth. What if, rather than deal with the issue at the protocol level,\n> > we deal with it at the source of the problem -- the wallets. Right now,\n> > the typical wallet selects only the minimum number of unspent outputs\n> > when building a transaction. The goal is to keep the transaction size to\n> > a minimum so that the fee stays low. Consequently, lots of unspent\n> > outputs just don't get used, and are left lying around until some point\n> > in the future.\n> >\n> > What if we started designing wallets to consolidate unspent outputs?\n> > When selecting unspent outputs for a transaction, rather than choosing\n> > just the minimum number from a particular address, why not select them\n> > ALL? Take all of the UTXOs from a particular address or wallet, send\n> > however much needs to be spent to the payee, and send the rest back to\n> > the same address or a change address as a single output? Through this\n> > method, we should wind up shrinking the UTXO database over time rather\n> > than growing it with each transaction. Obviously, as Bitcoin gains wider\n> > adoption, the UTXO database will grow, simply because there are 7\n> > billion people in the world, and eventually a good percentage of them\n> > will have one or more wallets with spendable bitcoin. But this idea\n> > could limit the growth at least.\n> >\n> > The vast majority of users are running one of a handful of different\n> > wallet apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase;\n> > Circle; Blockchain.info; and maybe a few others. The developers of all\n> > these wallets have a vested interest in the continued usefulness of\n> > Bitcoin, and so should not be opposed to changing their UTXO selection\n> > algorithms to one that reduces the UTXO database instead of growing it.\n> >\n> > From the miners perspective, even though these types of transactions\n> > would be larger, the fee could stay low. Miners actually benefit from\n> > them in that it reduces the amount of storage they need to dedicate to\n> > holding the UTXO. So miners are incentivized to mine these types of\n> > transactions with a higher priority despite a low fee.\n> >\n> > Relays could also get in on the action and enforce this type of behavior\n> > by refusing to relay or deprioritizing the relay of transactions that\n> > don't use all of the available UTXOs from the addresses used as inputs.\n> > Relays are not only the ones who benefit the most from a reduction of\n> > the UTXO database, they're also in the best position to promote good\n> > behavior.\n> >\n> > --\n> > *James G. Phillips\n> > IV* <https://plus.google.com/u/0/113107039501292625391/posts>\n> > /\"Don't bunt. Aim out of the ball park. Aim for the company of\n> > immortals.\" -- David Ogilvy\n> > /\n> >\n> >  /This message was created with 100% recycled electrons. Please think\n> > twice before printing./\n> >\n> >\n> >\n> ------------------------------------------------------------------------------\n> > One dashboard for servers and applications across Physical-Virtual-Cloud\n> > Widest out-of-the-box monitoring support with 50+ applications\n> > Performance metrics, stats and reports that give you Actionable Insights\n> > Deep dive visibility with transaction tracing using APM Insight.\n> > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> >\n> >\n> >\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/06cb545b/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T19:16:46",
                "message_text_only": "On Sat, May 9, 2015 at 2:06 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> It's a very complex trade-off, which is hard to optimize for all use\n> cases. Using more UTXOs requires larger transactions, and thus more fees in\n> general.\n>\nUnless the miner determines that the reduction in UTXO storage requirements\nis worth the lower fee. There's no protocol level enforcement of a fee as\nfar as I understand it. It's enforced by the miners and their willingness\nto include a transaction in a block.\n\n> In addition, it results in more linkage between coins/addresses used, so\n> lower privacy.\n>\nNot if you only select all the UTXOs from a single address. A wallet that\nis geared more towards privacy minded individuals may want to reduce the\namount of address linkage, but a wallet geared towards the general masses\nprobably won't have to worry so much about that.\n\n> The only way you can guarantee an economical reason to keep the UTXO set\n> small is by actually having a consensus rule that punishes increasing its\n> size.\n>\nThere's an economical reason right now to keeping the UTXO set small. The\nsmaller it is, the easier it is for the individual to run a full node. The\neasier it is to run a full node, the faster Bitcoin will spread to the\nmasses. The faster it spreads to the masses, the more valuable it becomes.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/34057496/attachment.html>"
            },
            {
                "author": "Ross Nicoll",
                "date": "2015-05-09T19:43:33",
                "message_text_only": "I think potential fee subsidies for cleaning up UTXO (and/or penalties \nfor creating more UTXO than you burn) are worth thinking about. As \nGavin's post ( gavinandresen.ninja/utxo-uhoh ) indicates, UTXO cost is \nfar higher than block storage, so charging differently for the in/out \nmismatches should make good economic sense.\n\nRoss\n\n\nOn 09/05/2015 20:16, Jim Phillips wrote:\n> On Sat, May 9, 2015 at 2:06 PM, Pieter Wuille <pieter.wuille at gmail.com \n> <mailto:pieter.wuille at gmail.com>> wrote:\n>\n>     It's a very complex trade-off, which is hard to optimize for all\n>     use cases. Using more UTXOs requires larger transactions, and thus\n>     more fees in general.\n>\n> Unless the miner determines that the reduction in UTXO storage \n> requirements is worth the lower fee. There's no protocol level \n> enforcement of a fee as far as I understand it. It's enforced by the \n> miners and their willingness to include a transaction in a block.\n>\n>     In addition, it results in more linkage between coins/addresses\n>     used, so lower privacy.\n>\n> Not if you only select all the UTXOs from a single address. A wallet \n> that is geared more towards privacy minded individuals may want to \n> reduce the amount of address linkage, but a wallet geared towards the \n> general masses probably won't have to worry so much about that.\n>\n>     The only way you can guarantee an economical reason to keep the\n>     UTXO set small is by actually having a consensus rule that\n>     punishes increasing its size.\n>\n> There's an economical reason right now to keeping the UTXO set small. \n> The smaller it is, the easier it is for the individual to run a full \n> node. The easier it is to run a full node, the faster Bitcoin will \n> spread to the masses. The faster it spreads to the masses, the more \n> valuable it becomes.\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/bbd40c7a/attachment.html>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-10T02:11:13",
                "message_text_only": "Minimizing the number of UTXOs in a wallet is sometimes not in the best interests of the user. In fact, quite often I've wished for a configuration option like \"Try to maintain _[number]_ UTXOs in the wallet.\" This is because I often want to make multiple spends from my wallet within one block, but spends of unconfirmed inputs are less reliable than spends of confirmed inputs, and some wallets (e.g., Andreas Schildbach's wallet) don't even allow it - you can only spend confirmed UTXOs. I can't tell you how aggravating it is to have to tell a friend, \"Oh, oops, I can't pay you yet. I have to wait for the last transaction I did to confirm first.\" All the more aggravating because I know, if I have multiple UTXOs in my wallet, I can make multiple spends within the same block.\n\n\nOn Saturday, 9 May 2015, at 12:09 pm, Jim Phillips wrote:\n> Forgive me if this idea has been suggested before, but I made this\n> suggestion on reddit and I got some feedback recommending I also bring it\n> to this list -- so here goes.\n> \n> I wonder if there isn't perhaps a simpler way of dealing with UTXO growth.\n> What if, rather than deal with the issue at the protocol level, we deal\n> with it at the source of the problem -- the wallets. Right now, the typical\n> wallet selects only the minimum number of unspent outputs when building a\n> transaction. The goal is to keep the transaction size to a minimum so that\n> the fee stays low. Consequently, lots of unspent outputs just don't get\n> used, and are left lying around until some point in the future.\n> \n> What if we started designing wallets to consolidate unspent outputs? When\n> selecting unspent outputs for a transaction, rather than choosing just the\n> minimum number from a particular address, why not select them ALL? Take all\n> of the UTXOs from a particular address or wallet, send however much needs\n> to be spent to the payee, and send the rest back to the same address or a\n> change address as a single output? Through this method, we should wind up\n> shrinking the UTXO database over time rather than growing it with each\n> transaction. Obviously, as Bitcoin gains wider adoption, the UTXO database\n> will grow, simply because there are 7 billion people in the world, and\n> eventually a good percentage of them will have one or more wallets with\n> spendable bitcoin. But this idea could limit the growth at least.\n> \n> The vast majority of users are running one of a handful of different wallet\n> apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;\n> Blockchain.info; and maybe a few others. The developers of all these\n> wallets have a vested interest in the continued usefulness of Bitcoin, and\n> so should not be opposed to changing their UTXO selection algorithms to one\n> that reduces the UTXO database instead of growing it.\n> \n> >From the miners perspective, even though these types of transactions would\n> be larger, the fee could stay low. Miners actually benefit from them in\n> that it reduces the amount of storage they need to dedicate to holding the\n> UTXO. So miners are incentivized to mine these types of transactions with a\n> higher priority despite a low fee.\n> \n> Relays could also get in on the action and enforce this type of behavior by\n> refusing to relay or deprioritizing the relay of transactions that don't\n> use all of the available UTXOs from the addresses used as inputs. Relays\n> are not only the ones who benefit the most from a reduction of the UTXO\n> database, they're also in the best position to promote good behavior.\n> \n> --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n> \n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy*\n> \n>  *This message was created with 100% recycled electrons. Please think twice\n> before printing.*"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-10T12:11:53",
                "message_text_only": "I feel your pain. I've had the same thing happen to me in the past. And I\nagree it's more likely to occur with my proposed scheme but I think with HD\nwallets there will still be UTXOs left unspent after most transactions\nsince, for privacy sake it's looking for the smallest set of addresses that\ncan be linked.\nOn May 9, 2015 9:11 PM, \"Matt Whitlock\" <bip at mattwhitlock.name> wrote:\n\n> Minimizing the number of UTXOs in a wallet is sometimes not in the best\n> interests of the user. In fact, quite often I've wished for a configuration\n> option like \"Try to maintain _[number]_ UTXOs in the wallet.\" This is\n> because I often want to make multiple spends from my wallet within one\n> block, but spends of unconfirmed inputs are less reliable than spends of\n> confirmed inputs, and some wallets (e.g., Andreas Schildbach's wallet)\n> don't even allow it - you can only spend confirmed UTXOs. I can't tell you\n> how aggravating it is to have to tell a friend, \"Oh, oops, I can't pay you\n> yet. I have to wait for the last transaction I did to confirm first.\" All\n> the more aggravating because I know, if I have multiple UTXOs in my wallet,\n> I can make multiple spends within the same block.\n>\n>\n> On Saturday, 9 May 2015, at 12:09 pm, Jim Phillips wrote:\n> > Forgive me if this idea has been suggested before, but I made this\n> > suggestion on reddit and I got some feedback recommending I also bring it\n> > to this list -- so here goes.\n> >\n> > I wonder if there isn't perhaps a simpler way of dealing with UTXO\n> growth.\n> > What if, rather than deal with the issue at the protocol level, we deal\n> > with it at the source of the problem -- the wallets. Right now, the\n> typical\n> > wallet selects only the minimum number of unspent outputs when building a\n> > transaction. The goal is to keep the transaction size to a minimum so\n> that\n> > the fee stays low. Consequently, lots of unspent outputs just don't get\n> > used, and are left lying around until some point in the future.\n> >\n> > What if we started designing wallets to consolidate unspent outputs? When\n> > selecting unspent outputs for a transaction, rather than choosing just\n> the\n> > minimum number from a particular address, why not select them ALL? Take\n> all\n> > of the UTXOs from a particular address or wallet, send however much needs\n> > to be spent to the payee, and send the rest back to the same address or a\n> > change address as a single output? Through this method, we should wind up\n> > shrinking the UTXO database over time rather than growing it with each\n> > transaction. Obviously, as Bitcoin gains wider adoption, the UTXO\n> database\n> > will grow, simply because there are 7 billion people in the world, and\n> > eventually a good percentage of them will have one or more wallets with\n> > spendable bitcoin. But this idea could limit the growth at least.\n> >\n> > The vast majority of users are running one of a handful of different\n> wallet\n> > apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;\n> > Blockchain.info; and maybe a few others. The developers of all these\n> > wallets have a vested interest in the continued usefulness of Bitcoin,\n> and\n> > so should not be opposed to changing their UTXO selection algorithms to\n> one\n> > that reduces the UTXO database instead of growing it.\n> >\n> > >From the miners perspective, even though these types of transactions\n> would\n> > be larger, the fee could stay low. Miners actually benefit from them in\n> > that it reduces the amount of storage they need to dedicate to holding\n> the\n> > UTXO. So miners are incentivized to mine these types of transactions\n> with a\n> > higher priority despite a low fee.\n> >\n> > Relays could also get in on the action and enforce this type of behavior\n> by\n> > refusing to relay or deprioritizing the relay of transactions that don't\n> > use all of the available UTXOs from the addresses used as inputs. Relays\n> > are not only the ones who benefit the most from a reduction of the UTXO\n> > database, they're also in the best position to promote good behavior.\n> >\n> > --\n> > *James G. Phillips IV*\n> > <https://plus.google.com/u/0/113107039501292625391/posts>\n> >\n> > *\"Don't bunt. Aim out of the ball park. Aim for the company of\n> immortals.\"\n> > -- David Ogilvy*\n> >\n> >  *This message was created with 100% recycled electrons. Please think\n> twice\n> > before printing.*\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/f595554a/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T18:41:06",
                "message_text_only": ">\n> some wallets (e.g., Andreas Schildbach's wallet) don't even allow it - you\n> can only spend confirmed UTXOs. I can't tell you how aggravating it is to\n> have to tell a friend, \"Oh, oops, I can't pay you yet. I have to wait for\n> the last transaction I did to confirm first.\" All the more aggravating\n> because I know, if I have multiple UTXOs in my wallet, I can make multiple\n> spends within the same block.\n>\n\nAndreas' wallet hasn't done that for years. Are you repeating this from\nsome very old memory or do you actually see this issue in reality?\n\nThe only time you're forced to wait for confirmations is when you have an\nunconfirmed inbound transaction, and thus the sender is unknown.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/31357329/attachment.html>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-25T20:03:28",
                "message_text_only": "On Monday, 25 May 2015, at 8:41 pm, Mike Hearn wrote:\n> > some wallets (e.g., Andreas Schildbach's wallet) don't even allow it - you\n> > can only spend confirmed UTXOs. I can't tell you how aggravating it is to\n> > have to tell a friend, \"Oh, oops, I can't pay you yet. I have to wait for\n> > the last transaction I did to confirm first.\" All the more aggravating\n> > because I know, if I have multiple UTXOs in my wallet, I can make multiple\n> > spends within the same block.\n> \n> Andreas' wallet hasn't done that for years. Are you repeating this from\n> some very old memory or do you actually see this issue in reality?\n> \n> The only time you're forced to wait for confirmations is when you have an\n> unconfirmed inbound transaction, and thus the sender is unknown.\n\nI see this behavior all the time. I am using the latest release, as far as I know. Version 4.30.\n\nThe same behavior occurs in the Testnet3 variant of the app. Go in there with an empty wallet and receive one payment and wait for it to confirm. Then send a payment and, before it confirms, try to send another one. The wallet won't let you send the second payment. It'll say something like, \"You need x.xxxxxx more bitcoins to make this payment.\" But if you wait for your first payment to confirm, then you'll be able to make the second payment.\n\nIf it matters, I configure the app to connect only to my own trusted Bitcoin node, so I only ever have one active connection at most. I notice that outgoing payments never show as \"Sent\" until they appear in a block, presumably because the app never sees the transaction come in over any connection."
            },
            {
                "author": "Andreas Schildbach",
                "date": "2015-05-25T20:29:26",
                "message_text_only": "On 05/25/2015 10:03 PM, Matt Whitlock wrote:\n> On Monday, 25 May 2015, at 8:41 pm, Mike Hearn wrote:\n>>> some wallets (e.g., Andreas Schildbach's wallet) don't even allow it - you\n>>> can only spend confirmed UTXOs. I can't tell you how aggravating it is to\n>>> have to tell a friend, \"Oh, oops, I can't pay you yet. I have to wait for\n>>> the last transaction I did to confirm first.\" All the more aggravating\n>>> because I know, if I have multiple UTXOs in my wallet, I can make multiple\n>>> spends within the same block.\n>>\n>> Andreas' wallet hasn't done that for years. Are you repeating this from\n>> some very old memory or do you actually see this issue in reality?\n>>\n>> The only time you're forced to wait for confirmations is when you have an\n>> unconfirmed inbound transaction, and thus the sender is unknown.\n> \n> I see this behavior all the time. I am using the latest release, as far as I know. Version 4.30.\n> \n> The same behavior occurs in the Testnet3 variant of the app. Go in there with an empty wallet and receive one payment and wait for it to confirm. Then send a payment and, before it confirms, try to send another one. The wallet won't let you send the second payment. It'll say something like, \"You need x.xxxxxx more bitcoins to make this payment.\" But if you wait for your first payment to confirm, then you'll be able to make the second payment.\n> \n> If it matters, I configure the app to connect only to my own trusted Bitcoin node, so I only ever have one active connection at most. I notice that outgoing payments never show as \"Sent\" until they appear in a block, presumably because the app never sees the transaction come in over any connection.\n\nYes, that's the issue. Because you're connecting only to one node, you\ndon't get any instant confirmations -- due to a Bitcoin protocol\nlimitation you can only get them from nodes you don't post the tx to."
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-25T21:05:41",
                "message_text_only": "On Mon, May 25, 2015 at 10:29:26PM +0200, Andreas Schildbach wrote:\n> > I see this behavior all the time. I am using the latest release, as far as I know. Version 4.30.\n> > \n> > The same behavior occurs in the Testnet3 variant of the app. Go in there with an empty wallet and receive one payment and wait for it to confirm. Then send a payment and, before it confirms, try to send another one. The wallet won't let you send the second payment. It'll say something like, \"You need x.xxxxxx more bitcoins to make this payment.\" But if you wait for your first payment to confirm, then you'll be able to make the second payment.\n> > \n> > If it matters, I configure the app to connect only to my own trusted Bitcoin node, so I only ever have one active connection at most. I notice that outgoing payments never show as \"Sent\" until they appear in a block, presumably because the app never sees the transaction come in over any connection.\n> \n> Yes, that's the issue. Because you're connecting only to one node, you\n> don't get any instant confirmations -- due to a Bitcoin protocol\n> limitation you can only get them from nodes you don't post the tx to.\n\nOdd, I just tried the above as well - with multiple peers connected -\nand had the exact same problem.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000e83c311f4244e4eefb54aa845abb181e46f16d126ab21e1\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/a2334287/attachment.sig>"
            },
            {
                "author": "Andreas Schildbach",
                "date": "2015-05-26T12:40:28",
                "message_text_only": "On 05/25/2015 11:05 PM, Peter Todd wrote:\n> On Mon, May 25, 2015 at 10:29:26PM +0200, Andreas Schildbach wrote:\n>>> I see this behavior all the time. I am using the latest release, as far as I know. Version 4.30.\n>>>\n>>> The same behavior occurs in the Testnet3 variant of the app. Go in there with an empty wallet and receive one payment and wait for it to confirm. Then send a payment and, before it confirms, try to send another one. The wallet won't let you send the second payment. It'll say something like, \"You need x.xxxxxx more bitcoins to make this payment.\" But if you wait for your first payment to confirm, then you'll be able to make the second payment.\n>>>\n>>> If it matters, I configure the app to connect only to my own trusted Bitcoin node, so I only ever have one active connection at most. I notice that outgoing payments never show as \"Sent\" until they appear in a block, presumably because the app never sees the transaction come in over any connection.\n>>\n>> Yes, that's the issue. Because you're connecting only to one node, you\n>> don't get any instant confirmations -- due to a Bitcoin protocol\n>> limitation you can only get them from nodes you don't post the tx to.\n> \n> Odd, I just tried the above as well - with multiple peers connected -\n> and had the exact same problem.\n\nIt should work, I'm testing this regularly. Can you report an issue\nthrough the app and attach your log when this happens again?"
            },
            {
                "author": "Warren Togami Jr.",
                "date": "2015-05-25T21:14:46",
                "message_text_only": "On Mon, May 25, 2015 at 1:29 PM, Andreas Schildbach\n<andreas at schildbach.de> wrote:\n> Yes, that's the issue. Because you're connecting only to one node, you\n> don't get any instant confirmations -- due to a Bitcoin protocol\n> limitation you can only get them from nodes you don't post the tx to.\n\nIs it really wise to call this a \"confirmation\"?  All this is really\ntelling you is one seemingly random peer has relayed the transaction\nback to you that you sent to a presumably different seemingly random\npeer.\n\nWarren"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T21:12:58",
                "message_text_only": ">\n> If it matters, I configure the app to connect only to my own trusted\n> Bitcoin node, so I only ever have one active connection at most.\n\n\nAh, I see, non default configuration. Because the Bitcoin network can and\ndoes change in backwards incompatible ways, the app wants to see that the\ntransaction it made actually propagated across the network. If you set a\ntrusted node it won't see that.\n\nProbably the logic should be tweaked so if you set a trusted node you're\njust assumed to know what you're doing and we assume the transactions we\nmake ourselves always work.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/2c1f1d06/attachment.html>"
            },
            {
                "author": "Bob McElrath",
                "date": "2015-05-10T13:35:25",
                "message_text_only": "This is my biggest headache with practical bitcoin usage. I'd love to hear it if\nanyone has any clever solutions to the wallet/utxo locked problem. Spending\nunconfirmed outputs really requires a different security model on the part of\nthe receiver than #confirmations, but isn't inherently bad if the receiver has a\nbetter security model and knows how to compute the probability that an\nunconfirmed-spend will get confirmed. Of course the bigger problem is wallet\nsoftware that refuses to spend unconfirmed outputs.\n\nI've thought a bit about a fork/merge design: if the change were computed by the\nnetwork instead of the submitter, two transactions having the same change\naddress and a common input could be straightforwardly merged or split (in a\nreorg), where with bitcoin currently it would be considered a double-spend.  Of\ncourse that has big privacy implications since it directly exposes the change\naddress, and is a hard fork, but is much closer to what people expect of a\ndebit-based \"account\" in traditional banking.\n\nThe fact of the matter is that having numerous sequential debits on an account\nis an extremely common use case, and bitcoin is obtuse in this respect.\n\nOn May 9, 2015 1:09:32 PM EDT, Jim Phillips <jim at ergophobia.org> wrote:\n>Forgive me if this idea has been suggested before, but I made this\n>suggestion on reddit and I got some feedback recommending I also bring\n>it\n>to this list -- so here goes.\n>\n>I wonder if there isn't perhaps a simpler way of dealing with UTXO\n>growth.\n>What if, rather than deal with the issue at the protocol level, we deal\n>with it at the source of the problem -- the wallets. Right now, the\n>typical\n>wallet selects only the minimum number of unspent outputs when building\n>a\n>transaction. The goal is to keep the transaction size to a minimum so\n>that\n>the fee stays low. Consequently, lots of unspent outputs just don't get\n>used, and are left lying around until some point in the future.\n>\n>What if we started designing wallets to consolidate unspent outputs?\n>When\n>selecting unspent outputs for a transaction, rather than choosing just\n>the\n>minimum number from a particular address, why not select them ALL? Take\n>all\n>of the UTXOs from a particular address or wallet, send however much\n>needs\n>to be spent to the payee, and send the rest back to the same address or\n>a\n>change address as a single output? Through this method, we should wind\n>up\n>shrinking the UTXO database over time rather than growing it with each\n>transaction. Obviously, as Bitcoin gains wider adoption, the UTXO\n>database\n>will grow, simply because there are 7 billion people in the world, and\n>eventually a good percentage of them will have one or more wallets with\n>spendable bitcoin. But this idea could limit the growth at least.\n>\n>The vast majority of users are running one of a handful of different\n>wallet\n>apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;\n>Blockchain.info; and maybe a few others. The developers of all these\n>wallets have a vested interest in the continued usefulness of Bitcoin,\n>and\n>so should not be opposed to changing their UTXO selection algorithms to\n>one\n>that reduces the UTXO database instead of growing it.\n>\n>>From the miners perspective, even though these types of transactions\n>would\n>be larger, the fee could stay low. Miners actually benefit from them in\n>that it reduces the amount of storage they need to dedicate to holding\n>the\n>UTXO. So miners are incentivized to mine these types of transactions\n>with a\n>higher priority despite a low fee.\n>\n>Relays could also get in on the action and enforce this type of\n>behavior by\n>refusing to relay or deprioritizing the relay of transactions that\n>don't\n>use all of the available UTXOs from the addresses used as inputs.\n>Relays\n>are not only the ones who benefit the most from a reduction of the UTXO\n>database, they're also in the best position to promote good behavior.\n>\n>--\n>*James G. Phillips IV*\n><https://plus.google.com/u/0/113107039501292625391/posts>\n>\n>*\"Don't bunt. Aim out of the ball park. Aim for the company of\n>immortals.\"\n>-- David Ogilvy*\n>\n>*This message was created with 100% recycled electrons. Please think\n>twice\n>before printing.*\n>\n>\n>!DSPAM:554e4e5450787476022393!\n>\n>\n>------------------------------------------------------------------------\n>\n>------------------------------------------------------------------------------\n>One dashboard for servers and applications across\n>Physical-Virtual-Cloud \n>Widest out-of-the-box monitoring support with 50+ applications\n>Performance metrics, stats and reports that give you Actionable\n>Insights\n>Deep dive visibility with transaction tracing using APM Insight.\n>http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>\n>!DSPAM:554e4e5450787476022393!\n>\n>\n>------------------------------------------------------------------------\n>\n>_______________________________________________\n>Bitcoin-development mailing list\n>Bitcoin-development at lists.sourceforge.net\n>https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n>!DSPAM:554e4e5450787476022393!\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/3d3c3382/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 198 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/3d3c3382/attachment.sig>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-10T14:33:56",
                "message_text_only": "This has been frequently explored on IRC.\n\nMy general conclusion is \"dollar bills\" - pick highly common denominations\nof bitcoins.  Aggregate to obtain these denominations, but do not aggregate\nfurther.\n\nThis permits merge avoidance (privacy++), easy coinjoin where many hide in\nthe noise (privacy++), wallet dust de-fragmentation, while avoiding the\nover-aggregation problem where you have consolidated down to one output.\n\nThus a wallet would have several consolidation targets.\n\nAnother strategy is simply doubling outputs.  Say you pay 0.1 BTC to\nStarbucks.  Add another 0.1 BTC output to yourself, and a final change\noutput.  Who can say which output goes to Starbucks?\n\nThere are many iterations and trade-offs between fragmentation and privacy.\n\n\n\n\n\n\n\n\n\nOn Sun, May 10, 2015 at 9:35 AM, Bob McElrath <bob_bitcoin at mcelrath.org>\nwrote:\n\n> This is my biggest headache with practical bitcoin usage. I'd love to hear\n> it if\n> anyone has any clever solutions to the wallet/utxo locked problem. Spending\n> unconfirmed outputs really requires a different security model on the part\n> of\n> the receiver than #confirmations, but isn't inherently bad if the receiver\n> has a\n> better security model and knows how to compute the probability that an\n> unconfirmed-spend will get confirmed. Of course the bigger problem is\n> wallet\n> software that refuses to spend unconfirmed outputs.\n>\n> I've thought a bit about a fork/merge design: if the change were computed\n> by the\n> network instead of the submitter, two transactions having the same change\n> address and a common input could be straightforwardly merged or split (in a\n> reorg), where with bitcoin currently it would be considered a\n> double-spend.  Of\n> course that has big privacy implications since it directly exposes the\n> change\n> address, and is a hard fork, but is much closer to what people expect of a\n> debit-based \"account\" in traditional banking.\n>\n> The fact of the matter is that having numerous sequential debits on an\n> account\n> is an extremely common use case, and bitcoin is obtuse in this respect.\n>\n> On May 9, 2015 1:09:32 PM EDT, Jim Phillips <jim at ergophobia.org> wrote:\n> >Forgive me if this idea has been suggested before, but I made this\n> >suggestion on reddit and I got some feedback recommending I also bring\n> >it\n> >to this list -- so here goes.\n> >\n> >I wonder if there isn't perhaps a simpler way of dealing with UTXO\n> >growth.\n> >What if, rather than deal with the issue at the protocol level, we deal\n> >with it at the source of the problem -- the wallets. Right now, the\n> >typical\n> >wallet selects only the minimum number of unspent outputs when building\n> >a\n> >transaction. The goal is to keep the transaction size to a minimum so\n> >that\n> >the fee stays low. Consequently, lots of unspent outputs just don't get\n> >used, and are left lying around until some point in the future.\n> >\n> >What if we started designing wallets to consolidate unspent outputs?\n> >When\n> >selecting unspent outputs for a transaction, rather than choosing just\n> >the\n> >minimum number from a particular address, why not select them ALL? Take\n> >all\n> >of the UTXOs from a particular address or wallet, send however much\n> >needs\n> >to be spent to the payee, and send the rest back to the same address or\n> >a\n> >change address as a single output? Through this method, we should wind\n> >up\n> >shrinking the UTXO database over time rather than growing it with each\n> >transaction. Obviously, as Bitcoin gains wider adoption, the UTXO\n> >database\n> >will grow, simply because there are 7 billion people in the world, and\n> >eventually a good percentage of them will have one or more wallets with\n> >spendable bitcoin. But this idea could limit the growth at least.\n> >\n> >The vast majority of users are running one of a handful of different\n> >wallet\n> >apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase; Circle;\n> >Blockchain.info; and maybe a few others. The developers of all these\n> >wallets have a vested interest in the continued usefulness of Bitcoin,\n> >and\n> >so should not be opposed to changing their UTXO selection algorithms to\n> >one\n> >that reduces the UTXO database instead of growing it.\n> >\n> >>From the miners perspective, even though these types of transactions\n> >would\n> >be larger, the fee could stay low. Miners actually benefit from them in\n> >that it reduces the amount of storage they need to dedicate to holding\n> >the\n> >UTXO. So miners are incentivized to mine these types of transactions\n> >with a\n> >higher priority despite a low fee.\n> >\n> >Relays could also get in on the action and enforce this type of\n> >behavior by\n> >refusing to relay or deprioritizing the relay of transactions that\n> >don't\n> >use all of the available UTXOs from the addresses used as inputs.\n> >Relays\n> >are not only the ones who benefit the most from a reduction of the UTXO\n> >database, they're also in the best position to promote good behavior.\n> >\n> >--\n> >*James G. Phillips IV*\n> ><https://plus.google.com/u/0/113107039501292625391/posts>\n> >\n> >*\"Don't bunt. Aim out of the ball park. Aim for the company of\n> >immortals.\"\n> >-- David Ogilvy*\n> >\n> >*This message was created with 100% recycled electrons. Please think\n> >twice\n> >before printing.*\n> >\n> >\n> >!DSPAM:554e4e5450787476022393!\n> >\n> >\n> >------------------------------------------------------------------------\n> >\n>\n> >------------------------------------------------------------------------------\n> >One dashboard for servers and applications across\n> >Physical-Virtual-Cloud\n> >Widest out-of-the-box monitoring support with 50+ applications\n> >Performance metrics, stats and reports that give you Actionable\n> >Insights\n> >Deep dive visibility with transaction tracing using APM Insight.\n> >http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> >\n> >!DSPAM:554e4e5450787476022393!\n> >\n> >\n> >------------------------------------------------------------------------\n> >\n> >_______________________________________________\n> >Bitcoin-development mailing list\n> >Bitcoin-development at lists.sourceforge.net\n> >https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >\n> >\n> >!DSPAM:554e4e5450787476022393!\n>\n> --\n> Sent from my Android device with K-9 Mail. Please excuse my brevity.\n>\n> This is my biggest headache with practical bitcoin usage. I'd love to hear\n> it if anyone has any clever solutions to the wallet/utxo locked problem.\n> Spending unconfirmed outputs really requires a different security model on\n> the part of the receiver than #confirmations, but isn't inherently bad if\n> the receiver has a better security model and knows how to compute the\n> probability that an unconfirmed-spend will get confirmed. Of course the\n> bigger problem is wallet software that refuses to spend unconfirmed outputs.\n>\n> I've thought a bit about a fork/merge design: if the change were computed\n> by the network instead of the submitter, two transactions having the same\n> change address could be straightforwardly merged or split (in a reorg). Of\n> course that has big privacy implications and is pretty far from bitcoin's\n> design, but is much closer to what people expect of a debit-based \"account\"\n> in traditional banking.\n>\n> The fact of the matter is that having numerous sequential debits on an\n> account is an extremely common use case, and bitcoin is obtuse in this\n> respect.\n>\n> On May 9, 2015 1:09:32 PM EDT, Jim Phillips <jim at ergophobia.org> wrote:\n>>\n>> Forgive me if this idea has been suggested before, but I made this\n>> suggestion on reddit and I got some feedback recommending I also bring it\n>> to this list -- so here goes.\n>>\n>> I wonder if there isn't perhaps a simpler way of dealing with UTXO\n>> growth. What if, rather than deal with the issue at the protocol level, we\n>> deal with it at the source of the problem -- the wallets. Right now, the\n>> typical wallet selects only the minimum number of unspent outputs when\n>> building a transaction. The goal is to keep the transaction size to a\n>> minimum so that the fee stays low. Consequently, lots of unspent outputs\n>> just don't get used, and are left lying around until some point in the\n>> future.\n>>\n>> What if we started designing wallets to consolidate unspent outputs? When\n>> selecting unspent outputs for a transaction, rather than choosing just the\n>> minimum number from a particular address, why not select them ALL? Take all\n>> of the UTXOs from a particular address or wallet, send however much needs\n>> to be spent to the payee, and send the rest back to the same address or a\n>> change address as a single output? Through this method, we should wind up\n>> shrinking the UTXO database over time rather than growing it with each\n>> transaction. Obviously, as Bitcoin gains wider adoption, the UTXO database\n>> will grow, simply because there are 7 billion people in the world, and\n>> eventually a good percentage of them will have one or more wallets with\n>> spendable bitcoin. But this idea could limit the growth at least.\n>>\n>> The vast majority of users are running one of a handful of different\n>> wallet apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase;\n>> Circle; Blockchain.info; and maybe a few others. The developers of all\n>> these wallets have a vested interest in the continued usefulness of\n>> Bitcoin, and so should not be opposed to changing their UTXO selection\n>> algorithms to one that reduces the UTXO database instead of growing it.\n>>\n>> From the miners perspective, even though these types of transactions\n>> would be larger, the fee could stay low. Miners actually benefit from them\n>> in that it reduces the amount of storage they need to dedicate to holding\n>> the UTXO. So miners are incentivized to mine these types of transactions\n>> with a higher priority despite a low fee.\n>>\n>> Relays could also get in on the action and enforce this type of behavior\n>> by refusing to relay or deprioritizing the relay of transactions that don't\n>> use all of the available UTXOs from the addresses used as inputs. Relays\n>> are not only the ones who benefit the most from a reduction of the UTXO\n>> database, they're also in the best position to promote good behavior.\n>>\n>> --\n>> *James G. Phillips IV*\n>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>>\n>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>> immortals.\" -- David Ogilvy*\n>>\n>>  *This message was created with 100% recycled electrons. Please think\n>> twice before printing.*\n>>  !DSPAM:554e4e5450787476022393!\n>>\n>> ------------------------------\n>>\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>\n>> !DSPAM:554e4e5450787476022393!\n>>\n>> ------------------------------\n>>\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>> !DSPAM:554e4e5450787476022393!\n>>\n>>\n> --\n> Sent from my Android device with K-9 Mail. Please excuse my brevity.\n>\n> -----BEGIN PGP SIGNATURE-----\n> Version: GnuPG v1.4.10 (GNU/Linux)\n>\n> iEYEARECAAYFAlVPXp0ACgkQjwioWRGe9K1+2ACfViY0D2ksVFe29SwhxbtmNSC3\n> TQAAnRoJLI9wW3DQRPqQ7PorKxelC2S2\n> =Er51\n> -----END PGP SIGNATURE-----\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/8dd225ad/attachment.html>"
            },
            {
                "author": "Bob McElrath",
                "date": "2015-05-10T14:42:54",
                "message_text_only": "That's a lot of work, a lot of extra utxo's, and a lot of blockchain spam, just\nso I can do a convoluted form of arithmetic on my balance.\n\nIf a tx contained an explicit miner fee and a change address, but did not\ncompute the change, letting the network compute it (and therefore merge\ntransactions spending the same utxo), could one add some form of ring signature\na la Dash to alleviate the worsened privacy implications?\n\nJeff Garzik [jgarzik at bitpay.com] wrote:\n> This has been frequently explored on IRC.\n> \n> My general conclusion is \"dollar bills\" - pick highly common denominations of\n> bitcoins.\u00a0 Aggregate to obtain these denominations, but do not aggregate\n> further.\n> \n> This permits merge avoidance (privacy++), easy coinjoin where many hide in the\n> noise (privacy++), wallet dust de-fragmentation, while avoiding the\n> over-aggregation problem where you have consolidated down to one output.\n> \n> Thus a wallet would have several consolidation targets.\n> \n> Another strategy is simply doubling outputs.\u00a0 Say you pay 0.1 BTC to\n> Starbucks.\u00a0 Add another 0.1 BTC output to yourself, and a final change output.\u00a0\n> Who can say which output goes to Starbucks?\n> \n> There are many iterations and trade-offs between fragmentation and privacy.\n\n\n\n--\nCheers, Bob McElrath\n\n\"The individual has always had to struggle to keep from being overwhelmed by\nthe tribe.  If you try it, you will be lonely often, and sometimes frightened.\nBut no price is too high to pay for the privilege of owning yourself.\" \n    -- Friedrich Nietzsche"
            },
            {
                "author": "Danny Thorpe",
                "date": "2015-05-12T19:50:16",
                "message_text_only": "Having thousands of utxos floating around for a single address is clearly a\nbad thing - it creates a lot of memory load on bitcoin nodes.\n\nHowever, having only one utxo for an address is also a bad thing, for\nconcurrent operations.\n\nHaving \"several\" utxos available to spend is good for parallelism, so that\n2 or more tasks which are spending from the same address don't have to line\nup single file waiting for one of the tasks to publish a tx first so that\nthe next task can spend the (unconfirmed) change output of the first.\nRequiring/Forcing/Having a single output carry the entire balance of an\naddress does not work at scale. (Yes, this presumes that the tasks are\ncoordinated so that they don't attempt to spend the same outputs. Internal\ncoordination is solvable.)\n\nIn multiple replies, you push for having \"all\" utxos of an address spent in\none transaction.  Why all?  If the objective is to reduce the size of the\nutxo pool, it would be sufficient simply to recommend that wallets and\nother spenders consume more utxos than they create, on average.\n\nI'm ok with \"consume more utxos than you generate\" as a good citizen / best\npractices recommendation, but a requirement that all prior outputs must be\nspent in one transaction seems excessive and impractical.\n\n-Danny\n\nOn Sat, May 9, 2015 at 10:09 AM, Jim Phillips <jim at ergophobia.org> wrote:\n\n> Forgive me if this idea has been suggested before, but I made this\n> suggestion on reddit and I got some feedback recommending I also bring it\n> to this list -- so here goes.\n>\n> I wonder if there isn't perhaps a simpler way of dealing with UTXO growth.\n> What if, rather than deal with the issue at the protocol level, we deal\n> with it at the source of the problem -- the wallets. Right now, the typical\n> wallet selects only the minimum number of unspent outputs when building a\n> transaction. The goal is to keep the transaction size to a minimum so that\n> the fee stays low. Consequently, lots of unspent outputs just don't get\n> used, and are left lying around until some point in the future.\n>\n> What if we started designing wallets to consolidate unspent outputs? When\n> selecting unspent outputs for a transaction, rather than choosing just the\n> minimum number from a particular address, why not select them ALL? Take all\n> of the UTXOs from a particular address or wallet, send however much needs\n> to be spent to the payee, and send the rest back to the same address or a\n> change address as a single output? Through this method, we should wind up\n> shrinking the UTXO database over time rather than growing it with each\n> transaction. Obviously, as Bitcoin gains wider adoption, the UTXO database\n> will grow, simply because there are 7 billion people in the world, and\n> eventually a good percentage of them will have one or more wallets with\n> spendable bitcoin. But this idea could limit the growth at least.\n>\n> The vast majority of users are running one of a handful of different\n> wallet apps: Core, Electrum; Armory; Mycelium; Breadwallet; Coinbase;\n> Circle; Blockchain.info; and maybe a few others. The developers of all\n> these wallets have a vested interest in the continued usefulness of\n> Bitcoin, and so should not be opposed to changing their UTXO selection\n> algorithms to one that reduces the UTXO database instead of growing it.\n>\n> From the miners perspective, even though these types of transactions would\n> be larger, the fee could stay low. Miners actually benefit from them in\n> that it reduces the amount of storage they need to dedicate to holding the\n> UTXO. So miners are incentivized to mine these types of transactions with a\n> higher priority despite a low fee.\n>\n> Relays could also get in on the action and enforce this type of behavior\n> by refusing to relay or deprioritizing the relay of transactions that don't\n> use all of the available UTXOs from the addresses used as inputs. Relays\n> are not only the ones who benefit the most from a reduction of the UTXO\n> database, they're also in the best position to promote good behavior.\n>\n> --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy*\n>\n>  *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/77139440/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T18:44:18",
                "message_text_only": "Wallets are incentivised to do a better job with defragmentation already,\nas if you have lots of tiny UTXOs then your fees end up being huge when\ntrying to make a payment.\n\nThe reason they largely don't is just one of manpower. Nobody is working on\nit.\n\nAs a wallet developer myself, one way I'd like to see this issue be fixed\nby making free transactions more reliable. Then wallets can submit free\ntransactions to the network to consolidate UTXOs together, e.g. at night\nwhen the user is sleeping. They would then fit into whatever space is\navailable in the block during periods of low demand, like on Sunday.\n\nIf we don't do this then wallets won't automatically defragment, as we'd be\nunable to explain to the user why their money is slowly leaking out of\ntheir wallet without them doing anything. Trying to explain the existing\ntransaction fees is hard enough already (\"I thought bitcoin doesn't have\nbanks\" etc).\n\nThere is another way:  as the fee is based on a rounded 1kb calculation, if\nyou go into the next fee band adding some more outputs and making a bigger\nchange output becomes \"free\" for another output or two. But wallets don't\nexploit this today.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/7676f9f7/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-25T21:26:38",
                "message_text_only": "On Mon, May 25, 2015 at 08:44:18PM +0200, Mike Hearn wrote:\n> Wallets are incentivised to do a better job with defragmentation already,\n> as if you have lots of tiny UTXOs then your fees end up being huge when\n> trying to make a payment.\n> \n> The reason they largely don't is just one of manpower. Nobody is working on\n> it.\n> \n> As a wallet developer myself, one way I'd like to see this issue be fixed\n> by making free transactions more reliable. Then wallets can submit free\n> transactions to the network to consolidate UTXOs together, e.g. at night\n> when the user is sleeping. They would then fit into whatever space is\n> available in the block during periods of low demand, like on Sunday.\n\nThis can cause problems as until those transactions confirm, even more\nof the user's outputs are unavailable for spending, causing confusion as\nto why they can't send their full balance. It's also inefficient, as in\nthe case where the user does try to send a small payment that could be\nsatisfied by one or more of these small UTXO's, the wallet has to use a\nlarger UTXO.\n\nWith replace-by-fee however this problem goes away, as you can simply\ndouble-spend the pending defragmentation transactions instead if they\nare still unconfirmed when you need to use them.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000aa9033c06c10d6131eafa3754c3157d74c2267c1dd2ca35\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/80f62ec5/attachment.sig>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T22:03:09",
                "message_text_only": "CPFP also solves it just fine.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/cc086865/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-09T19:25:16",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/976abe4e/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T19:33:21",
                "message_text_only": "On Sat, May 9, 2015 at 2:25 PM, Raystonn <raystonn at hotmail.com> wrote:\n\n> Lack of privacy is viral.  We shouldn't encourage policy in most wallets\n> that discourages privacy.  It adversely affects privacy across the entire\n> network.\n>\nHow about this as a happy medium default policy: Rather than select UTXOs\nbased solely on age and limiting the size of the transaction, we select as\nmany UTXOs as possible from as few addresses as possible, prioritizing\nwhich addresses to use based on the number of UTXOs it contains (more being\npreferable) and how old those UTXOs are (in order to reduce the fee)?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/ca094a42/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T19:28:12",
                "message_text_only": "On Sat, May 9, 2015 at 2:12 PM, Patrick Mccorry (PGR) <\npatrick.mccorry at newcastle.ac.uk> wrote:\n\n>   Not necessarily. If you want to ensure privacy, you could limit the\n> selection of UTXOs to a single address, and even go so far as to send\n> change back to that same address. This wouldn't be as effective as\n> combining the UTXOs from multiple addresses, but it would help. The key is\n> to do everything that can be done when building a transaction to ensure\n> that as many inputs as possible are consolidated into as few outputs as\n> possible.\n>\n>\n>  I would agree if you have multiple utxo for a single address then it\n> makes sense since there is no privacy loss. However sending the change back\n> to the same address would damage privacy (Hive does this) as it is then\n> obvious from looking at the transaction which output is change and which\n> output is sending funds.\n>\n\nI tend to agree with you here. But the change output could just as easily\nbe sent to a new change address.\n\n>  Also not everyone is concerned with their own privacy, and I'm not aware\n> of any HD-wallet implementations that won't already combine inputs from\n> multiple addresses within that wallet without user input.\n>\n>\n>  For people who do not care for privacy then it would work fine. But\n> adding it into the wallet as default behaviour would deter those who do\n> care for privacy - and making it a customisable option just adds complexity\n> for the users. Wallets do need to combine utxo at times to spend bitcoins\n> which is how people can be tracked today, using the minimum set of utxo\n> tries to reduce the risk.\n>\n> Different wallets are targeted at different demographics. Some are geared\ntowards more mainstream users (for whom the privacy issue is less a\nconcern) and some (such as DarkWallet) are geared more towards the privacy\nadvocates. These wallets may choose to set their defaults at oposite ends\nof the spectrum as to how they choose to select and link addresses and\nUTXOs, but they can all improve on their current algorithms and promote\nsome degree of consolidation.\n\n>   Additionally, large wallets that have lots of addresses owned by\n> multiple users like exchanges, blockchain.info, and Coinbase can\n> consolidate UTXOs very effectively when building transactions\n>\n>\n>  That's true - I'm not sure how they would feel about it though. I\n> imagine they probably are already to minimise key management.\n>\n> That's what these discussions are for. Hopefully this thread will be seen\nby developers of these wallets and give them something to consider.\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/f710a05f/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-09T19:43:55",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/9f2abd0c/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T19:52:26",
                "message_text_only": "On Sat, May 9, 2015 at 2:43 PM, Raystonn <raystonn at hotmail.com> wrote:\n\n> How about this as a happy medium default policy: Rather than select UTXOs\n>> based solely on age and limiting the size of the transaction, we select as\n>> many UTXOs as possible from as few addresses as possible, prioritizing\n>> which addresses to use based on the number of UTXOs it contains (more being\n>> preferable) and how old those UTXOs are (in order to reduce the fee)?\n>\n> If selecting older UTXOs gives higher priority for a lesser (or at least\n> not greater) fee, that is an incentive for a rational user to use the older\n> UTXOs.  Such policy needs to be defended or removed.  It doesn't support\n> privacy or a reduction in UTXOs.\n>\nBefore starting this thread, I had completely forgotten that age was even a\nfactor in determining which UTXOs to use. Frankly, I can't think of any\nreason why miners care how old a particular UTXO is when determining what\nfees to charge. I'm sure there is one, I just don't know what it is. I just\ntossed it in there as homage to Andreas who pointed out to me that it was\nstill part of the selection criteria.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/45a26e39/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-09T20:20:06",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/fe07c35b/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-09T20:38:56",
                "message_text_only": "Miners do not care about the age of a UTXO entry, apart for two exceptions.\nIt is also economically irrelevant.\n* There is a free transaction policy, which sets a small portion of block\nspace aside for transactions which do not pay sufficient fee. This is\nmostly an altruistic way of encouraging Bitcoin adoption. As a DoS\nprevention mechanism, there is a requirement that these free transactions\nare of sufficient priority (computed as BTC-days-destroyed per byte),\nessentially requiring these transactions to consume another scarce\nresource, even if not money.\n* Coinbase transaction outputs can, as a consensus rule, only be spent\nafter 100 confirmations. This is to prevent random reorganisations from\ninvalidating transactions that spend young coinbase transactions (which\ncan't move to the new chain). In addition, wallets also select more\nconfirmed outputs first to consume, for the same reason.\nOn May 9, 2015 1:20 PM, \"Raystonn\" <raystonn at hotmail.com> wrote:\n\n> That policy is included in Bitcoin Core.  Miners use it because it is the\n> default.  The policy was likely intended to help real transactions get\n> through in the face of spam.  But it favors those with more bitcoin, as the\n> priority is determined by amount spent multiplied by age of UTXOs.  At the\n> very least the amount spent should be removed as a factor, or fees are\n> unlikely to ever be paid by those who can afford them.  We can reassess the\n> role age plays later.  One change at a time is better.\n>  On 9 May 2015 12:52 pm, Jim Phillips <jim at ergophobia.org> wrote:\n>\n> On Sat, May 9, 2015 at 2:43 PM, Raystonn <raystonn at hotmail.com> wrote:\n>\n> How about this as a happy medium default policy: Rather than select UTXOs\n> based solely on age and limiting the size of the transaction, we select as\n> many UTXOs as possible from as few addresses as possible, prioritizing\n> which addresses to use based on the number of UTXOs it contains (more being\n> preferable) and how old those UTXOs are (in order to reduce the fee)?\n>\n> If selecting older UTXOs gives higher priority for a lesser (or at least\n> not greater) fee, that is an incentive for a rational user to use the older\n> UTXOs.  Such policy needs to be defended or removed.  It doesn't support\n> privacy or a reduction in UTXOs.\n>\n> Before starting this thread, I had completely forgotten that age was even\n> a factor in determining which UTXOs to use. Frankly, I can't think of any\n> reason why miners care how old a particular UTXO is when determining what\n> fees to charge. I'm sure there is one, I just don't know what it is. I just\n> tossed it in there as homage to Andreas who pointed out to me that it was\n> still part of the selection criteria.\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/33931428/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-09T21:11:57",
                "message_text_only": "Makes sense.. So with that said, I'd propose the following criteria for\nselecting UTXOs:\n\n1. Select the smallest possible set of addresses that can be linked in\norder to come up with enough BTC to send to the payee.\n2. Given multiple possible sets, select the one that has the largest number\nof UTXOs.\n3. Given multiple possible sets, choose the one that contains the largest\namount of total BTC.\n4. Given multiple possible sets, select the one that destroys the most\nbitcoin days.\n5. If there's still multiple possible sets, just choose one at random.\n\nOnce the final set of addresses has been identified, use ALL UTXOs from\nthat set, sending appropriate outputs to the recipient(s), a new change\naddress, and a mining fee.\n\nMiners should be cognisant of and reward the fact that the user is making\nan effort to consolidate UTXOs. They can easily spot these transactions by\nlooking at whether all possible UTXOs from each input addresses have been\nused. Since most miners use Bitcoin Core, and its defaults, this test can\nbe built into Bitcoin Core's logic for determining which transactions to\ninclude when mining a block.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n\nOn Sat, May 9, 2015 at 3:38 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> Miners do not care about the age of a UTXO entry, apart for two\n> exceptions. It is also economically irrelevant.\n> * There is a free transaction policy, which sets a small portion of block\n> space aside for transactions which do not pay sufficient fee. This is\n> mostly an altruistic way of encouraging Bitcoin adoption. As a DoS\n> prevention mechanism, there is a requirement that these free transactions\n> are of sufficient priority (computed as BTC-days-destroyed per byte),\n> essentially requiring these transactions to consume another scarce\n> resource, even if not money.\n> * Coinbase transaction outputs can, as a consensus rule, only be spent\n> after 100 confirmations. This is to prevent random reorganisations from\n> invalidating transactions that spend young coinbase transactions (which\n> can't move to the new chain). In addition, wallets also select more\n> confirmed outputs first to consume, for the same reason.\n> On May 9, 2015 1:20 PM, \"Raystonn\" <raystonn at hotmail.com> wrote:\n>\n>> That policy is included in Bitcoin Core.  Miners use it because it is the\n>> default.  The policy was likely intended to help real transactions get\n>> through in the face of spam.  But it favors those with more bitcoin, as the\n>> priority is determined by amount spent multiplied by age of UTXOs.  At the\n>> very least the amount spent should be removed as a factor, or fees are\n>> unlikely to ever be paid by those who can afford them.  We can reassess the\n>> role age plays later.  One change at a time is better.\n>>  On 9 May 2015 12:52 pm, Jim Phillips <jim at ergophobia.org> wrote:\n>>\n>> On Sat, May 9, 2015 at 2:43 PM, Raystonn <raystonn at hotmail.com> wrote:\n>>\n>> How about this as a happy medium default policy: Rather than select UTXOs\n>> based solely on age and limiting the size of the transaction, we select as\n>> many UTXOs as possible from as few addresses as possible, prioritizing\n>> which addresses to use based on the number of UTXOs it contains (more being\n>> preferable) and how old those UTXOs are (in order to reduce the fee)?\n>>\n>> If selecting older UTXOs gives higher priority for a lesser (or at least\n>> not greater) fee, that is an incentive for a rational user to use the older\n>> UTXOs.  Such policy needs to be defended or removed.  It doesn't support\n>> privacy or a reduction in UTXOs.\n>>\n>> Before starting this thread, I had completely forgotten that age was even\n>> a factor in determining which UTXOs to use. Frankly, I can't think of any\n>> reason why miners care how old a particular UTXO is when determining what\n>> fees to charge. I'm sure there is one, I just don't know what it is. I just\n>> tossed it in there as homage to Andreas who pointed out to me that it was\n>> still part of the selection criteria.\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150509/30a5f8d8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A suggestion for reducing the size of the UTXO database",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "Raystonn",
                "Andreas Schildbach",
                "Warren Togami Jr.",
                "Mike Hearn",
                "Peter Todd",
                "Jim Phillips",
                "Matt Whitlock",
                "Ross Nicoll",
                "Danny Thorpe",
                "Bob McElrath",
                "Pieter Wuille"
            ],
            "messages_count": 32,
            "total_messages_chars_count": 78574
        }
    },
    {
        "title": "[Bitcoin-development]  Cost savings by using replace-by-fee, 30-90%",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-05-26T00:10:34",
                "message_text_only": "On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n> CPFP also solves it just fine.\n\nCPFP is a significantly more expensive way of paying fees than RBF,\nparticularly for the use-case of defragmenting outputs, with cost\nsavings ranging from 30% to 90%\n\n\nCase 1: CPFP vs. RBF for increasing the fee on a single tx\n----------------------------------------------------------\n\nCreating an spending a P2PKH output uses 34 bytes of txout, and 148\nbytes of txin, 182 bytes total.\n\nLet's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\nAlice. This results in a 1in/2out transaction t1 that's 226 bytes in size.\nI forget to click on the \"priority fee\" option, so it goes out with the\nminimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\ncreating a new transaction t2 that's 192 bytes in size. I want to pay\n1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\ntransaction fees.\n\nOn the other hand, had I use RBF, my wallet would have simply\nrebroadcast t1 with the change address decreased. The rules require you\nto pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the new\nfee level, or 218uBTC of fees in total.\n\nCost savings: 48%\n\n\nCase 2: Paying multiple recipients in succession\n------------------------------------------------\n\nSuppose that after I pay Alice, I also decide to pay Bob for his hard\nwork demonstrating cryptographic protocols. I need to create a new\ntransaction t2 spending t1's change address. Normally t2 would be\nanother 226 bytes in size, resulting in 226uBTC additional fees.\n\nWith RBF on the other hand I can simply double-spend t1 with a\ntransaction paying both Alice and Bob. This new transaction is 260 bytes\nin size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\nconsumed broadcasting it, resulting in an additional 36uBTC of fees.\n\nCost savings: 84%\n\n\nCase 3: Paying multiple recipients from a 2-of-3 multisig wallet\n----------------------------------------------------------------\n\nThe above situation gets even worse with multisig. t1 in the multisig\ncase is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\nin fees. With RBF we rewrite t1 with an additional output, resulting in\na 399 byte transaction, with just 36uBTC in additional fees.\n\nCost savings: 90%\n\n\nCase 4: Dust defragmentation\n----------------------------\n\nMy wallet has a two transaction outputs that it wants to combine into\none for the purpose of UTXO defragmentation. It broadcasts transaction\nt1 with two inputs and one output, size 340 bytes, paying zero fees.\n\nPrior to the transaction confirming I find I need to spend those funds\nfor a priority transaction at the 1mBTC/KB fee level. This transaction,\nt2a, has one input and two outputs, 226 bytes in size. However it needs\nto pay fees for both transactions at once, resulting in a combined total\nfee of 556uBTC. If this situation happens frequently, defragmenting\nUTXOs is likely to cost more in additional fees than it saves.\n\nWith RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\nbytes in size, paying 374uBTC. Even better, if one of the two inputs is\nsufficiently large to cover my costs I can doublespend t1 with a\n1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n\nCost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n              costs you more than you save\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000134ce6577d4122094479f548b997baf84367eaf0c190bc9f\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/ae1be84d/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Cost savings by using replace-by-fee, 30-90%",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Peter Todd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3715
        }
    },
    {
        "title": "[Bitcoin-development] Cost savings by using replace-by-fee, 30-90%",
        "thread_messages": [
            {
                "author": "Danny Thorpe",
                "date": "2015-05-26T18:22:00",
                "message_text_only": "What prevents RBF from being used for fraudulent payment reversals?\n\nPay 1BTC to Alice for hard goods, then after you receive the goods\nbroadcast a double spend of that transaction to pay Alice nothing? Your\nonly cost is the higher network fee of the 2nd tx.\n\nThanks,\n-Danny\n\nOn Mon, May 25, 2015 at 5:10 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n> > CPFP also solves it just fine.\n>\n> CPFP is a significantly more expensive way of paying fees than RBF,\n> particularly for the use-case of defragmenting outputs, with cost\n> savings ranging from 30% to 90%\n>\n>\n> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n> ----------------------------------------------------------\n>\n> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n> bytes of txin, 182 bytes total.\n>\n> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in size.\n> I forget to click on the \"priority fee\" option, so it goes out with the\n> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n> creating a new transaction t2 that's 192 bytes in size. I want to pay\n> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n> transaction fees.\n>\n> On the other hand, had I use RBF, my wallet would have simply\n> rebroadcast t1 with the change address decreased. The rules require you\n> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the new\n> fee level, or 218uBTC of fees in total.\n>\n> Cost savings: 48%\n>\n>\n> Case 2: Paying multiple recipients in succession\n> ------------------------------------------------\n>\n> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n> work demonstrating cryptographic protocols. I need to create a new\n> transaction t2 spending t1's change address. Normally t2 would be\n> another 226 bytes in size, resulting in 226uBTC additional fees.\n>\n> With RBF on the other hand I can simply double-spend t1 with a\n> transaction paying both Alice and Bob. This new transaction is 260 bytes\n> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n>\n> Cost savings: 84%\n>\n>\n> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n> ----------------------------------------------------------------\n>\n> The above situation gets even worse with multisig. t1 in the multisig\n> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n> in fees. With RBF we rewrite t1 with an additional output, resulting in\n> a 399 byte transaction, with just 36uBTC in additional fees.\n>\n> Cost savings: 90%\n>\n>\n> Case 4: Dust defragmentation\n> ----------------------------\n>\n> My wallet has a two transaction outputs that it wants to combine into\n> one for the purpose of UTXO defragmentation. It broadcasts transaction\n> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n>\n> Prior to the transaction confirming I find I need to spend those funds\n> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n> t2a, has one input and two outputs, 226 bytes in size. However it needs\n> to pay fees for both transactions at once, resulting in a combined total\n> fee of 556uBTC. If this situation happens frequently, defragmenting\n> UTXOs is likely to cost more in additional fees than it saves.\n>\n> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n> sufficiently large to cover my costs I can doublespend t1 with a\n> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n>\n> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>               costs you more than you save\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 0000000000000000134ce6577d4122094479f548b997baf84367eaf0c190bc9f\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/4f1264c0/attachment.html>"
            },
            {
                "author": "Allen Piscitello",
                "date": "2015-05-26T18:38:08",
                "message_text_only": "What prevents you from writing a bad check using today's systems?\n\nOn Tue, May 26, 2015 at 1:22 PM, Danny Thorpe <danny.thorpe at gmail.com>\nwrote:\n\n> What prevents RBF from being used for fraudulent payment reversals?\n>\n> Pay 1BTC to Alice for hard goods, then after you receive the goods\n> broadcast a double spend of that transaction to pay Alice nothing? Your\n> only cost is the higher network fee of the 2nd tx.\n>\n> Thanks,\n> -Danny\n>\n> On Mon, May 25, 2015 at 5:10 PM, Peter Todd <pete at petertodd.org> wrote:\n>\n>> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n>> > CPFP also solves it just fine.\n>>\n>> CPFP is a significantly more expensive way of paying fees than RBF,\n>> particularly for the use-case of defragmenting outputs, with cost\n>> savings ranging from 30% to 90%\n>>\n>>\n>> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n>> ----------------------------------------------------------\n>>\n>> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n>> bytes of txin, 182 bytes total.\n>>\n>> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n>> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in size.\n>> I forget to click on the \"priority fee\" option, so it goes out with the\n>> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n>> creating a new transaction t2 that's 192 bytes in size. I want to pay\n>> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n>> transaction fees.\n>>\n>> On the other hand, had I use RBF, my wallet would have simply\n>> rebroadcast t1 with the change address decreased. The rules require you\n>> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the new\n>> fee level, or 218uBTC of fees in total.\n>>\n>> Cost savings: 48%\n>>\n>>\n>> Case 2: Paying multiple recipients in succession\n>> ------------------------------------------------\n>>\n>> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n>> work demonstrating cryptographic protocols. I need to create a new\n>> transaction t2 spending t1's change address. Normally t2 would be\n>> another 226 bytes in size, resulting in 226uBTC additional fees.\n>>\n>> With RBF on the other hand I can simply double-spend t1 with a\n>> transaction paying both Alice and Bob. This new transaction is 260 bytes\n>> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n>> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n>>\n>> Cost savings: 84%\n>>\n>>\n>> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n>> ----------------------------------------------------------------\n>>\n>> The above situation gets even worse with multisig. t1 in the multisig\n>> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n>> in fees. With RBF we rewrite t1 with an additional output, resulting in\n>> a 399 byte transaction, with just 36uBTC in additional fees.\n>>\n>> Cost savings: 90%\n>>\n>>\n>> Case 4: Dust defragmentation\n>> ----------------------------\n>>\n>> My wallet has a two transaction outputs that it wants to combine into\n>> one for the purpose of UTXO defragmentation. It broadcasts transaction\n>> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n>>\n>> Prior to the transaction confirming I find I need to spend those funds\n>> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n>> t2a, has one input and two outputs, 226 bytes in size. However it needs\n>> to pay fees for both transactions at once, resulting in a combined total\n>> fee of 556uBTC. If this situation happens frequently, defragmenting\n>> UTXOs is likely to cost more in additional fees than it saves.\n>>\n>> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n>> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n>> sufficiently large to cover my costs I can doublespend t1 with a\n>> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n>>\n>> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>>               costs you more than you save\n>>\n>> --\n>> 'peter'[:-1]@petertodd.org\n>> 0000000000000000134ce6577d4122094479f548b997baf84367eaf0c190bc9f\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/a7fd8020/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-26T18:42:37",
                "message_text_only": "See the \"first-seen-safe replace-by-fee\" thread\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Tue, May 26, 2015 at 11:22 AM, Danny Thorpe <danny.thorpe at gmail.com>\nwrote:\n\n> What prevents RBF from being used for fraudulent payment reversals?\n>\n> Pay 1BTC to Alice for hard goods, then after you receive the goods\n> broadcast a double spend of that transaction to pay Alice nothing? Your\n> only cost is the higher network fee of the 2nd tx.\n>\n> Thanks,\n> -Danny\n>\n> On Mon, May 25, 2015 at 5:10 PM, Peter Todd <pete at petertodd.org> wrote:\n>\n>> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n>> > CPFP also solves it just fine.\n>>\n>> CPFP is a significantly more expensive way of paying fees than RBF,\n>> particularly for the use-case of defragmenting outputs, with cost\n>> savings ranging from 30% to 90%\n>>\n>>\n>> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n>> ----------------------------------------------------------\n>>\n>> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n>> bytes of txin, 182 bytes total.\n>>\n>> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n>> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in size.\n>> I forget to click on the \"priority fee\" option, so it goes out with the\n>> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n>> creating a new transaction t2 that's 192 bytes in size. I want to pay\n>> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n>> transaction fees.\n>>\n>> On the other hand, had I use RBF, my wallet would have simply\n>> rebroadcast t1 with the change address decreased. The rules require you\n>> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the new\n>> fee level, or 218uBTC of fees in total.\n>>\n>> Cost savings: 48%\n>>\n>>\n>> Case 2: Paying multiple recipients in succession\n>> ------------------------------------------------\n>>\n>> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n>> work demonstrating cryptographic protocols. I need to create a new\n>> transaction t2 spending t1's change address. Normally t2 would be\n>> another 226 bytes in size, resulting in 226uBTC additional fees.\n>>\n>> With RBF on the other hand I can simply double-spend t1 with a\n>> transaction paying both Alice and Bob. This new transaction is 260 bytes\n>> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n>> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n>>\n>> Cost savings: 84%\n>>\n>>\n>> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n>> ----------------------------------------------------------------\n>>\n>> The above situation gets even worse with multisig. t1 in the multisig\n>> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n>> in fees. With RBF we rewrite t1 with an additional output, resulting in\n>> a 399 byte transaction, with just 36uBTC in additional fees.\n>>\n>> Cost savings: 90%\n>>\n>>\n>> Case 4: Dust defragmentation\n>> ----------------------------\n>>\n>> My wallet has a two transaction outputs that it wants to combine into\n>> one for the purpose of UTXO defragmentation. It broadcasts transaction\n>> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n>>\n>> Prior to the transaction confirming I find I need to spend those funds\n>> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n>> t2a, has one input and two outputs, 226 bytes in size. However it needs\n>> to pay fees for both transactions at once, resulting in a combined total\n>> fee of 556uBTC. If this situation happens frequently, defragmenting\n>> UTXOs is likely to cost more in additional fees than it saves.\n>>\n>> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n>> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n>> sufficiently large to cover my costs I can doublespend t1 with a\n>> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n>>\n>> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>>               costs you more than you save\n>>\n>> --\n>> 'peter'[:-1]@petertodd.org\n>> 0000000000000000134ce6577d4122094479f548b997baf84367eaf0c190bc9f\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/cf2d8823/attachment.html>"
            },
            {
                "author": "Adam Back",
                "date": "2015-05-26T18:47:39",
                "message_text_only": "The general idea for replace by fee is that it would be restricted so\nas to make it safe, eg all the original addresses should receive no\nless bitcoin (more addresses can be added).\n\nThe scorched earth game theory stuff (allowing removing recipients) is\nkind of orthogonal.\n\nAdam\n\nOn 26 May 2015 at 19:22, Danny Thorpe <danny.thorpe at gmail.com> wrote:\n> What prevents RBF from being used for fraudulent payment reversals?\n>\n> Pay 1BTC to Alice for hard goods, then after you receive the goods broadcast\n> a double spend of that transaction to pay Alice nothing? Your only cost is\n> the higher network fee of the 2nd tx.\n>\n> Thanks,\n> -Danny\n>\n> On Mon, May 25, 2015 at 5:10 PM, Peter Todd <pete at petertodd.org> wrote:\n>>\n>> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n>> > CPFP also solves it just fine.\n>>\n>> CPFP is a significantly more expensive way of paying fees than RBF,\n>> particularly for the use-case of defragmenting outputs, with cost\n>> savings ranging from 30% to 90%\n>>\n>>\n>> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n>> ----------------------------------------------------------\n>>\n>> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n>> bytes of txin, 182 bytes total.\n>>\n>> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n>> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in size.\n>> I forget to click on the \"priority fee\" option, so it goes out with the\n>> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n>> creating a new transaction t2 that's 192 bytes in size. I want to pay\n>> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n>> transaction fees.\n>>\n>> On the other hand, had I use RBF, my wallet would have simply\n>> rebroadcast t1 with the change address decreased. The rules require you\n>> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the new\n>> fee level, or 218uBTC of fees in total.\n>>\n>> Cost savings: 48%\n>>\n>>\n>> Case 2: Paying multiple recipients in succession\n>> ------------------------------------------------\n>>\n>> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n>> work demonstrating cryptographic protocols. I need to create a new\n>> transaction t2 spending t1's change address. Normally t2 would be\n>> another 226 bytes in size, resulting in 226uBTC additional fees.\n>>\n>> With RBF on the other hand I can simply double-spend t1 with a\n>> transaction paying both Alice and Bob. This new transaction is 260 bytes\n>> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n>> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n>>\n>> Cost savings: 84%\n>>\n>>\n>> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n>> ----------------------------------------------------------------\n>>\n>> The above situation gets even worse with multisig. t1 in the multisig\n>> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n>> in fees. With RBF we rewrite t1 with an additional output, resulting in\n>> a 399 byte transaction, with just 36uBTC in additional fees.\n>>\n>> Cost savings: 90%\n>>\n>>\n>> Case 4: Dust defragmentation\n>> ----------------------------\n>>\n>> My wallet has a two transaction outputs that it wants to combine into\n>> one for the purpose of UTXO defragmentation. It broadcasts transaction\n>> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n>>\n>> Prior to the transaction confirming I find I need to spend those funds\n>> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n>> t2a, has one input and two outputs, 226 bytes in size. However it needs\n>> to pay fees for both transactions at once, resulting in a combined total\n>> fee of 556uBTC. If this situation happens frequently, defragmenting\n>> UTXOs is likely to cost more in additional fees than it saves.\n>>\n>> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n>> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n>> sufficiently large to cover my costs I can doublespend t1 with a\n>> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n>>\n>> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>>               costs you more than you save\n>>\n>> --\n>> 'peter'[:-1]@petertodd.org\n>> 0000000000000000134ce6577d4122094479f548b997baf84367eaf0c190bc9f\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-26T20:18:06",
                "message_text_only": "On Tuesday, 26 May 2015, at 11:22 am, Danny Thorpe wrote:\n> What prevents RBF from being used for fraudulent payment reversals?\n> \n> Pay 1BTC to Alice for hard goods, then after you receive the goods\n> broadcast a double spend of that transaction to pay Alice nothing? Your\n> only cost is the higher network fee of the 2nd tx.\n\nThe \"First-Seen-Safe\" replace-by-fee presently being discussed on this list disallows fraudulent payment reversals, as it disallows a replacing transaction that pays less to any output script than the replaced transaction paid."
            },
            {
                "author": "joliver at airmail.cc",
                "date": "2015-05-26T20:30:48",
                "message_text_only": "You're the Chief Scientist of __ViaCoin__ a alt with 30 second blocks \nand you have big banks as clients. Shit like replace-by-fee and leading \nthe anti-scaling mob is for your clients, not Bitcoin. Get the fuck out.\n\nPeter Todd - 8930511 Canada Ltd.\n1214-1423 Mississauga Valley Blvd.\nMississauga ON L5A 4A5\nCanada\n\nhttps://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId=8930511\n\nOn 2015-05-26 00:10, Peter Todd wrote:\n> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n>> CPFP also solves it just fine.\n> \n> CPFP is a significantly more expensive way of paying fees than RBF,\n> particularly for the use-case of defragmenting outputs, with cost\n> savings ranging from 30% to 90%\n> \n> \n> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n> ----------------------------------------------------------\n> \n> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n> bytes of txin, 182 bytes total.\n> \n> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in \n> size.\n> I forget to click on the \"priority fee\" option, so it goes out with the\n> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n> creating a new transaction t2 that's 192 bytes in size. I want to pay\n> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n> transaction fees.\n> \n> On the other hand, had I use RBF, my wallet would have simply\n> rebroadcast t1 with the change address decreased. The rules require you\n> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the \n> new\n> fee level, or 218uBTC of fees in total.\n> \n> Cost savings: 48%\n> \n> \n> Case 2: Paying multiple recipients in succession\n> ------------------------------------------------\n> \n> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n> work demonstrating cryptographic protocols. I need to create a new\n> transaction t2 spending t1's change address. Normally t2 would be\n> another 226 bytes in size, resulting in 226uBTC additional fees.\n> \n> With RBF on the other hand I can simply double-spend t1 with a\n> transaction paying both Alice and Bob. This new transaction is 260 \n> bytes\n> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n> \n> Cost savings: 84%\n> \n> \n> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n> ----------------------------------------------------------------\n> \n> The above situation gets even worse with multisig. t1 in the multisig\n> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n> in fees. With RBF we rewrite t1 with an additional output, resulting in\n> a 399 byte transaction, with just 36uBTC in additional fees.\n> \n> Cost savings: 90%\n> \n> \n> Case 4: Dust defragmentation\n> ----------------------------\n> \n> My wallet has a two transaction outputs that it wants to combine into\n> one for the purpose of UTXO defragmentation. It broadcasts transaction\n> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n> \n> Prior to the transaction confirming I find I need to spend those funds\n> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n> t2a, has one input and two outputs, 226 bytes in size. However it needs\n> to pay fees for both transactions at once, resulting in a combined \n> total\n> fee of 556uBTC. If this situation happens frequently, defragmenting\n> UTXOs is likely to cost more in additional fees than it saves.\n> \n> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n> sufficiently large to cover my costs I can doublespend t1 with a\n> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n> \n> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>               costs you more than you save\n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across \n> Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable \n> Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> \n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-26T20:56:06",
                "message_text_only": "Please let's at least have some civility and decorum on this list.\n\nOn Tue, May 26, 2015 at 1:30 PM, <joliver at airmail.cc> wrote:\n\n> You're the Chief Scientist of __ViaCoin__ a alt with 30 second blocks\n> and you have big banks as clients. Shit like replace-by-fee and leading\n> the anti-scaling mob is for your clients, not Bitcoin. Get the fuck out.\n>\n> Peter Todd - 8930511 Canada Ltd.\n> 1214-1423 Mississauga Valley Blvd.\n> Mississauga ON L5A 4A5\n> Canada\n>\n>\n> https://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId=8930511\n>\n> On 2015-05-26 00:10, Peter Todd wrote:\n> > On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n> >> CPFP also solves it just fine.\n> >\n> > CPFP is a significantly more expensive way of paying fees than RBF,\n> > particularly for the use-case of defragmenting outputs, with cost\n> > savings ranging from 30% to 90%\n> >\n> >\n> > Case 1: CPFP vs. RBF for increasing the fee on a single tx\n> > ----------------------------------------------------------\n> >\n> > Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n> > bytes of txin, 182 bytes total.\n> >\n> > Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n> > Alice. This results in a 1in/2out transaction t1 that's 226 bytes in\n> > size.\n> > I forget to click on the \"priority fee\" option, so it goes out with the\n> > minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n> > creating a new transaction t2 that's 192 bytes in size. I want to pay\n> > 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n> > transaction fees.\n> >\n> > On the other hand, had I use RBF, my wallet would have simply\n> > rebroadcast t1 with the change address decreased. The rules require you\n> > to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the\n> > new\n> > fee level, or 218uBTC of fees in total.\n> >\n> > Cost savings: 48%\n> >\n> >\n> > Case 2: Paying multiple recipients in succession\n> > ------------------------------------------------\n> >\n> > Suppose that after I pay Alice, I also decide to pay Bob for his hard\n> > work demonstrating cryptographic protocols. I need to create a new\n> > transaction t2 spending t1's change address. Normally t2 would be\n> > another 226 bytes in size, resulting in 226uBTC additional fees.\n> >\n> > With RBF on the other hand I can simply double-spend t1 with a\n> > transaction paying both Alice and Bob. This new transaction is 260\n> > bytes\n> > in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n> > consumed broadcasting it, resulting in an additional 36uBTC of fees.\n> >\n> > Cost savings: 84%\n> >\n> >\n> > Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n> > ----------------------------------------------------------------\n> >\n> > The above situation gets even worse with multisig. t1 in the multisig\n> > case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n> > in fees. With RBF we rewrite t1 with an additional output, resulting in\n> > a 399 byte transaction, with just 36uBTC in additional fees.\n> >\n> > Cost savings: 90%\n> >\n> >\n> > Case 4: Dust defragmentation\n> > ----------------------------\n> >\n> > My wallet has a two transaction outputs that it wants to combine into\n> > one for the purpose of UTXO defragmentation. It broadcasts transaction\n> > t1 with two inputs and one output, size 340 bytes, paying zero fees.\n> >\n> > Prior to the transaction confirming I find I need to spend those funds\n> > for a priority transaction at the 1mBTC/KB fee level. This transaction,\n> > t2a, has one input and two outputs, 226 bytes in size. However it needs\n> > to pay fees for both transactions at once, resulting in a combined\n> > total\n> > fee of 556uBTC. If this situation happens frequently, defragmenting\n> > UTXOs is likely to cost more in additional fees than it saves.\n> >\n> > With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n> > bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n> > sufficiently large to cover my costs I can doublespend t1 with a\n> > 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n> >\n> > Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n> >               costs you more than you save\n> >\n> >\n> ------------------------------------------------------------------------------\n> > One dashboard for servers and applications across\n> > Physical-Virtual-Cloud\n> > Widest out-of-the-box monitoring support with 50+ applications\n> > Performance metrics, stats and reports that give you Actionable\n> > Insights\n> > Deep dive visibility with transaction tracing using APM Insight.\n> > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> >\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/38c108d3/attachment.html>"
            },
            {
                "author": "s7r",
                "date": "2015-05-26T21:29:28",
                "message_text_only": "What is wrong with the man testing some ideas on his custom branch? This\nis how improvements come to life. I saw in the BIPs some really\ninteresting ideas and nice brainstorming which came from Peter Todd.\n\nNow, my question, if replace by fee doesn't allow me to change the\ninputs or the outputs, I can only add outputs... what can I do with this\nfeature? If I sent a tx and want to replace it with a higher fee one,\nthe higher fee one can only have maybe additional change addresses or\nanother payment, if the inputs suffice? Do we have any real use cases?\n\nP.S. is it planned to include this by default in bitcoin core 10.0.3 or\nit will remain just on Peter's branch?\n\nOn 5/26/2015 11:30 PM, joliver at airmail.cc wrote:\n> You're the Chief Scientist of __ViaCoin__ a alt with 30 second blocks \n> and you have big banks as clients. Shit like replace-by-fee and leading \n> the anti-scaling mob is for your clients, not Bitcoin. Get the fuck out.\n> \n> Peter Todd - 8930511 Canada Ltd.\n> 1214-1423 Mississauga Valley Blvd.\n> Mississauga ON L5A 4A5\n> Canada\n> \n> https://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId=8930511\n> \n> On 2015-05-26 00:10, Peter Todd wrote:\n>> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n>>> CPFP also solves it just fine.\n>>\n>> CPFP is a significantly more expensive way of paying fees than RBF,\n>> particularly for the use-case of defragmenting outputs, with cost\n>> savings ranging from 30% to 90%\n>>\n>>\n>> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n>> ----------------------------------------------------------\n>>\n>> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n>> bytes of txin, 182 bytes total.\n>>\n>> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n>> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in \n>> size.\n>> I forget to click on the \"priority fee\" option, so it goes out with the\n>> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n>> creating a new transaction t2 that's 192 bytes in size. I want to pay\n>> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n>> transaction fees.\n>>\n>> On the other hand, had I use RBF, my wallet would have simply\n>> rebroadcast t1 with the change address decreased. The rules require you\n>> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the \n>> new\n>> fee level, or 218uBTC of fees in total.\n>>\n>> Cost savings: 48%\n>>\n>>\n>> Case 2: Paying multiple recipients in succession\n>> ------------------------------------------------\n>>\n>> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n>> work demonstrating cryptographic protocols. I need to create a new\n>> transaction t2 spending t1's change address. Normally t2 would be\n>> another 226 bytes in size, resulting in 226uBTC additional fees.\n>>\n>> With RBF on the other hand I can simply double-spend t1 with a\n>> transaction paying both Alice and Bob. This new transaction is 260 \n>> bytes\n>> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n>> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n>>\n>> Cost savings: 84%\n>>\n>>\n>> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n>> ----------------------------------------------------------------\n>>\n>> The above situation gets even worse with multisig. t1 in the multisig\n>> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n>> in fees. With RBF we rewrite t1 with an additional output, resulting in\n>> a 399 byte transaction, with just 36uBTC in additional fees.\n>>\n>> Cost savings: 90%\n>>\n>>\n>> Case 4: Dust defragmentation\n>> ----------------------------\n>>\n>> My wallet has a two transaction outputs that it wants to combine into\n>> one for the purpose of UTXO defragmentation. It broadcasts transaction\n>> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n>>\n>> Prior to the transaction confirming I find I need to spend those funds\n>> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n>> t2a, has one input and two outputs, 226 bytes in size. However it needs\n>> to pay fees for both transactions at once, resulting in a combined \n>> total\n>> fee of 556uBTC. If this situation happens frequently, defragmenting\n>> UTXOs is likely to cost more in additional fees than it saves.\n>>\n>> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n>> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n>> sufficiently large to cover my costs I can doublespend t1 with a\n>> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n>>\n>> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>>               costs you more than you save\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across \n>> Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable \n>> Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> \n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Adam Back",
                "date": "2015-05-26T22:06:42",
                "message_text_only": "Well so for example it could have an additional input (to increase the\nBTC paid into the transaction) and pay more to an existing change\naddress and higher fee, or add an additional change address, and leave\na larger fee, or if you had a right-sized coin add an additional input\nthat all goes to fees.\n\n(As well as optionally tacking on additional pending payments to other\naddresses funded from the higher input).\n\nAdam\n\nOn 26 May 2015 at 22:29, s7r <s7r at sky-ip.org> wrote:\n> What is wrong with the man testing some ideas on his custom branch? This\n> is how improvements come to life. I saw in the BIPs some really\n> interesting ideas and nice brainstorming which came from Peter Todd.\n>\n> Now, my question, if replace by fee doesn't allow me to change the\n> inputs or the outputs, I can only add outputs... what can I do with this\n> feature? If I sent a tx and want to replace it with a higher fee one,\n> the higher fee one can only have maybe additional change addresses or\n> another payment, if the inputs suffice? Do we have any real use cases?\n>\n> P.S. is it planned to include this by default in bitcoin core 10.0.3 or\n> it will remain just on Peter's branch?\n>\n> On 5/26/2015 11:30 PM, joliver at airmail.cc wrote:\n>> You're the Chief Scientist of __ViaCoin__ a alt with 30 second blocks\n>> and you have big banks as clients. Shit like replace-by-fee and leading\n>> the anti-scaling mob is for your clients, not Bitcoin. Get the fuck out.\n>>\n>> Peter Todd - 8930511 Canada Ltd.\n>> 1214-1423 Mississauga Valley Blvd.\n>> Mississauga ON L5A 4A5\n>> Canada\n>>\n>> https://www.ic.gc.ca/app/scr/cc/CorporationsCanada/fdrlCrpDtls.html?corpId=8930511\n>>\n>> On 2015-05-26 00:10, Peter Todd wrote:\n>>> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n>>>> CPFP also solves it just fine.\n>>>\n>>> CPFP is a significantly more expensive way of paying fees than RBF,\n>>> particularly for the use-case of defragmenting outputs, with cost\n>>> savings ranging from 30% to 90%\n>>>\n>>>\n>>> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n>>> ----------------------------------------------------------\n>>>\n>>> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n>>> bytes of txin, 182 bytes total.\n>>>\n>>> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n>>> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in\n>>> size.\n>>> I forget to click on the \"priority fee\" option, so it goes out with the\n>>> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n>>> creating a new transaction t2 that's 192 bytes in size. I want to pay\n>>> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n>>> transaction fees.\n>>>\n>>> On the other hand, had I use RBF, my wallet would have simply\n>>> rebroadcast t1 with the change address decreased. The rules require you\n>>> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the\n>>> new\n>>> fee level, or 218uBTC of fees in total.\n>>>\n>>> Cost savings: 48%\n>>>\n>>>\n>>> Case 2: Paying multiple recipients in succession\n>>> ------------------------------------------------\n>>>\n>>> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n>>> work demonstrating cryptographic protocols. I need to create a new\n>>> transaction t2 spending t1's change address. Normally t2 would be\n>>> another 226 bytes in size, resulting in 226uBTC additional fees.\n>>>\n>>> With RBF on the other hand I can simply double-spend t1 with a\n>>> transaction paying both Alice and Bob. This new transaction is 260\n>>> bytes\n>>> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n>>> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n>>>\n>>> Cost savings: 84%\n>>>\n>>>\n>>> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n>>> ----------------------------------------------------------------\n>>>\n>>> The above situation gets even worse with multisig. t1 in the multisig\n>>> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n>>> in fees. With RBF we rewrite t1 with an additional output, resulting in\n>>> a 399 byte transaction, with just 36uBTC in additional fees.\n>>>\n>>> Cost savings: 90%\n>>>\n>>>\n>>> Case 4: Dust defragmentation\n>>> ----------------------------\n>>>\n>>> My wallet has a two transaction outputs that it wants to combine into\n>>> one for the purpose of UTXO defragmentation. It broadcasts transaction\n>>> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n>>>\n>>> Prior to the transaction confirming I find I need to spend those funds\n>>> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n>>> t2a, has one input and two outputs, 226 bytes in size. However it needs\n>>> to pay fees for both transactions at once, resulting in a combined\n>>> total\n>>> fee of 556uBTC. If this situation happens frequently, defragmenting\n>>> UTXOs is likely to cost more in additional fees than it saves.\n>>>\n>>> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n>>> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n>>> sufficiently large to cover my costs I can doublespend t1 with a\n>>> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n>>>\n>>> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>>>               costs you more than you save\n>>>\n>>> ------------------------------------------------------------------------------\n>>> One dashboard for servers and applications across\n>>> Physical-Virtual-Cloud\n>>> Widest out-of-the-box monitoring support with 50+ applications\n>>> Performance metrics, stats and reports that give you Actionable\n>>> Insights\n>>> Deep dive visibility with transaction tracing using APM Insight.\n>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>>\n>>> _______________________________________________\n>>> Bitcoin-development mailing list\n>>> Bitcoin-development at lists.sourceforge.net\n>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>> ------------------------------------------------------------------------------\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-27T01:25:21",
                "message_text_only": "On Wed, May 27, 2015 at 12:29:28AM +0300, s7r wrote:\n> What is wrong with the man testing some ideas on his custom branch? This\n> is how improvements come to life. I saw in the BIPs some really\n> interesting ideas and nice brainstorming which came from Peter Todd.\n> \n> Now, my question, if replace by fee doesn't allow me to change the\n> inputs or the outputs, I can only add outputs... what can I do with this\n> feature? If I sent a tx and want to replace it with a higher fee one,\n> the higher fee one can only have maybe additional change addresses or\n> another payment, if the inputs suffice? Do we have any real use cases?\n\nYou're a bit mistaken there: standard RBF lets you change anything, and\nFSS RBF lets you modify inputs and add outputs and/or make the value of\noutputs higher.\n\n> P.S. is it planned to include this by default in bitcoin core 10.0.3 or\n> it will remain just on Peter's branch?\n\nAny significant change to mempool policy like RBF is very unlikely to be\nincorporated in the Bitcoin Core v0.10.x branch, simply because it'd be\ntoo large a change for a minor, mostly bugfix, release.\n\nHaving said that, I already maintain a standard RBF branch for v0.10.x,\nand have been asked by a major minor to backport FSS RBF for v0.10.x as\nwell.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000b9e6c1ce35e6e06c01b1f381840bcd9297f307cb1e6aae8\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/c047c16b/attachment.sig>"
            },
            {
                "author": "s7r",
                "date": "2015-05-27T19:28:55",
                "message_text_only": "Hi Peter,\n\nThanks for your reply.\n\nI know and bookmarked your branch - nice work.\n\nSo, to clarify:\n- bitcoin core (official / default) 0.10.x currently has First-seen\nmempool behavior\n- your custom branch uses replace by fee mempool behavior which allows\nan user to change anything in a tx (I guess it needs just to have at\nleast one same input, so it can link it to another previously signed tx\nwith lower fee and substitute it in the mempool, correct?).\n\n- First Seen Safe Replace by Fee (FSF-RBF) mempool behavior which allows\nan user only to add inputs and/or increase the value of outputs will be\nin yet another branch, maintained by you, but not in default / official\nbitcoin core?\n\nAnother thing, if FSF-RBF lets you change TXes in the manner described\nabove, how does the client know which tx needs to be replaced in the\nmempool? Since the txid naturally changes. How does it map tx1 with tx2\n(to know tx2 has a higher fee and needs to substitute tx1) if quite a\nlot of params from the transaction structure can change?\n\nThanks!\n\nOn 5/27/2015 4:25 AM, Peter Todd wrote:\n> On Wed, May 27, 2015 at 12:29:28AM +0300, s7r wrote:\n>> What is wrong with the man testing some ideas on his custom branch? This\n>> is how improvements come to life. I saw in the BIPs some really\n>> interesting ideas and nice brainstorming which came from Peter Todd.\n>>\n>> Now, my question, if replace by fee doesn't allow me to change the\n>> inputs or the outputs, I can only add outputs... what can I do with this\n>> feature? If I sent a tx and want to replace it with a higher fee one,\n>> the higher fee one can only have maybe additional change addresses or\n>> another payment, if the inputs suffice? Do we have any real use cases?\n> \n> You're a bit mistaken there: standard RBF lets you change anything, and\n> FSS RBF lets you modify inputs and add outputs and/or make the value of\n> outputs higher.\n> \n>> P.S. is it planned to include this by default in bitcoin core 10.0.3 or\n>> it will remain just on Peter's branch?\n> \n> Any significant change to mempool policy like RBF is very unlikely to be\n> incorporated in the Bitcoin Core v0.10.x branch, simply because it'd be\n> too large a change for a minor, mostly bugfix, release.\n> \n> Having said that, I already maintain a standard RBF branch for v0.10.x,\n> and have been asked by a major minor to backport FSS RBF for v0.10.x as\n> well.\n>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-26T22:29:40",
                "message_text_only": "That attitude and doxxing is not appropriate for this list.\n\n\nOn Tue, May 26, 2015 at 4:30 PM, <joliver at airmail.cc> wrote:\n\n> You're the Chief Scientist of __ViaCoin__ a alt with 30 second blocks\n> and you have big banks as clients. Shit like replace-by-fee and leading\n> the anti-scaling mob is for your clients, not Bitcoin. Get the fuck out.\n> <https://lists.sourceforge.net/lists/listinfo/bitcoin-development>\n>\n\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/74d8851a/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-26T18:43:14",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/06a5078b/attachment.html>"
            },
            {
                "author": "Allen Piscitello",
                "date": "2015-05-26T20:12:41",
                "message_text_only": "I am not the one presenting this as some kind of novel attack on\ntransactions in general.\n\nOn Tue, May 26, 2015 at 1:43 PM, Raystonn <raystonn at hotmail.com> wrote:\n\n> Trust, regulation, law, and the threat of force.  Are you serious?\n>  On 26 May 2015 11:38 am, Allen Piscitello <allen.piscitello at gmail.com>\n> wrote:\n>\n> What prevents you from writing a bad check using today's systems?\n>\n> On Tue, May 26, 2015 at 1:22 PM, Danny Thorpe <danny.thorpe at gmail.com>\n> wrote:\n>\n> What prevents RBF from being used for fraudulent payment reversals?\n>\n> Pay 1BTC to Alice for hard goods, then after you receive the goods\n> broadcast a double spend of that transaction to pay Alice nothing? Your\n> only cost is the higher network fee of the 2nd tx.\n>\n> Thanks,\n> -Danny\n>\n> On Mon, May 25, 2015 at 5:10 PM, Peter Todd <pete at petertodd.org> wrote:\n>\n> On Tue, May 26, 2015 at 12:03:09AM +0200, Mike Hearn wrote:\n> > CPFP also solves it just fine.\n>\n> CPFP is a significantly more expensive way of paying fees than RBF,\n> particularly for the use-case of defragmenting outputs, with cost\n> savings ranging from 30% to 90%\n>\n>\n> Case 1: CPFP vs. RBF for increasing the fee on a single tx\n> ----------------------------------------------------------\n>\n> Creating an spending a P2PKH output uses 34 bytes of txout, and 148\n> bytes of txin, 182 bytes total.\n>\n> Let's suppose I have a 1 BTC P2PKH output and I want to pay 0.1 BTC to\n> Alice. This results in a 1in/2out transaction t1 that's 226 bytes in size.\n> I forget to click on the \"priority fee\" option, so it goes out with the\n> minimum fee of 2.26uBTC. Whoops! I use CPFP to spend that output,\n> creating a new transaction t2 that's 192 bytes in size. I want to pay\n> 1mBTC/KB for a fast confirmation, so I'm now paying 418uBTC of\n> transaction fees.\n>\n> On the other hand, had I use RBF, my wallet would have simply\n> rebroadcast t1 with the change address decreased. The rules require you\n> to pay 2.26uBTC for the bandwidth consumed broadcasting it, plus the new\n> fee level, or 218uBTC of fees in total.\n>\n> Cost savings: 48%\n>\n>\n> Case 2: Paying multiple recipients in succession\n> ------------------------------------------------\n>\n> Suppose that after I pay Alice, I also decide to pay Bob for his hard\n> work demonstrating cryptographic protocols. I need to create a new\n> transaction t2 spending t1's change address. Normally t2 would be\n> another 226 bytes in size, resulting in 226uBTC additional fees.\n>\n> With RBF on the other hand I can simply double-spend t1 with a\n> transaction paying both Alice and Bob. This new transaction is 260 bytes\n> in size. I have to pay 2.6uBTC additional fees to pay for the bandwidth\n> consumed broadcasting it, resulting in an additional 36uBTC of fees.\n>\n> Cost savings: 84%\n>\n>\n> Case 3: Paying multiple recipients from a 2-of-3 multisig wallet\n> ----------------------------------------------------------------\n>\n> The above situation gets even worse with multisig. t1 in the multisig\n> case is 367 bytes; t2 another 367 bytes, costing an additional 367uBTC\n> in fees. With RBF we rewrite t1 with an additional output, resulting in\n> a 399 byte transaction, with just 36uBTC in additional fees.\n>\n> Cost savings: 90%\n>\n>\n> Case 4: Dust defragmentation\n> ----------------------------\n>\n> My wallet has a two transaction outputs that it wants to combine into\n> one for the purpose of UTXO defragmentation. It broadcasts transaction\n> t1 with two inputs and one output, size 340 bytes, paying zero fees.\n>\n> Prior to the transaction confirming I find I need to spend those funds\n> for a priority transaction at the 1mBTC/KB fee level. This transaction,\n> t2a, has one input and two outputs, 226 bytes in size. However it needs\n> to pay fees for both transactions at once, resulting in a combined total\n> fee of 556uBTC. If this situation happens frequently, defragmenting\n> UTXOs is likely to cost more in additional fees than it saves.\n>\n> With RBF I'd simply doublespend t1 with a 2-in-2-out transaction 374\n> bytes in size, paying 374uBTC. Even better, if one of the two inputs is\n> sufficiently large to cover my costs I can doublespend t1 with a\n> 1-in-2-out tx just 226 bytes in size, paying 226uBTC.\n>\n> Cost savings: 32% to 59%, or even infinite if defragmentation w/o RBF\n>               costs you more than you save\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 0000000000000000134ce6577d4122094479f548b997baf84367eaf0c190bc9f\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/0d10c771/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Cost savings by using replace-by-fee, 30-90%",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "Raystonn",
                "Adam Back",
                "s7r",
                "Peter Todd",
                "joliver at airmail.cc",
                "Matt Whitlock",
                "Mark Friedenbach",
                "Danny Thorpe",
                "Allen Piscitello",
                "Aaron Voisine"
            ],
            "messages_count": 14,
            "total_messages_chars_count": 55175
        }
    },
    {
        "title": "[Bitcoin-development] A way to create a fee market even without a block size limit (2013)",
        "thread_messages": [
            {
                "author": "Sergio Lerner",
                "date": "2015-05-10T20:45:32",
                "message_text_only": "Two years ago I presented a new way to create a fee market that does not\ndepend on the block chain limit.\n\nThis proposal has not been formally analyzed in any paper since then,\nbut I think it holds a good promise to untangle the current problem\nregarding increasing the tps and creating the fee market. BTW, think the\nmaximum tps should be increased, but not by increasing the block size,\nbut by increasing the block rate (I'll expose why in my next e-mail).\n\nThe original post is here (I was overly optimistic back then):\nhttps://bitcointalk.org/index.php?topic=147124.msg1561612#msg1561612\n\nI'll summarize it here again, with a little editing and a few more\nquestions at the end:\n\nThe idea is simple, but requires a hardfork, but is has minimum impact\nin the code and in the economics.\n\nSolution: Require that the set of fees collected in a block has a\ndispersion below a threshold. Use, for example, the Coefficient of\nVariation (http://en.wikipedia.org/wiki/Coefficient_of_variation). If\nthe CoVar is higher than a fixed threshold, the block is considered invalid.\n\nThe Coefficient of variation is computed as the standard deviation over\nthe mean value, so it's very easy to compute. (if the mean is zero, we\nassume CoVar=0). Note that the CoVar function *does not depend on the\nscale*, so is just what a coin with a floating price requires.\n\nThis means that if there are many transactions containing high fees in a\nblock, then free transactions cannot be included.\nThe core devs should tweak the transaction selection algorithm to take\ninto account this maximum bound.\n\n*Example*\n\nIf the transaction fee set is: 0,0,0,0,5,5,6,7,8,7\nThe CoVar is 0.85\nSuppose we limit the CoVar to a maximum of 1.\n\nSuppose the transaction fee set is: 0,0,0,0,0,0,0,0,0,10\nThen the CoVar is 3.0\n\nIn this case the miner should have to either drop the \"10\" from the fee\nset or drop the zeros. Obviously the miner will drop some zeros, and\nchoose the set: 0,10, that has a CoVar of 1.\n\n*Why it reduces the Tx spamming Problem?*\n\nUsing this little modification, spamming users would require to use\nhigher fees, only if the remaining users in the community rises their\nfees. And miners won't be able to include an enormous amounts of\nspamming txs.\n\n*Why it helps solving **the tragedy-of-the-commons fee \"problem\"?*\n\nAs miners are forced to keep the CoVar below the threshold, if people\nrises the fees to confirm faster than spamming txs, automatically\nsmamming txs become less likely to appear in blocks, and fee-estimators\nwill automatically increase future fees, creating a the desired feedback\nloop.\n\n*Why it helps solving the block size problem?*\n\nBecause if we increase the block size, miners that do not care about the\nfee market won't be able to fill the block with spamming txs and destroy\nthe market that is being created. This is not a solution against an\nattacker-miner, which can always fill the block with transactions.\n\n*Can the system by gamed? Can it be attacked?*\n\nI don't think so. An attacker would need to spend a high amount in fees\nto prevent transactions with low fees to be included in a block.\nHowever, a formal analysis would be required. Miller, Gun Sirer, Eyal..\nWant to give it a try?\n*\nCan create a positive feedback to a rise the fees to the top or push\nfess to the bottom?\n\n*Again, I don't think so. This depends on the dynamics between the each\nnode's fee estimator and the transaction backlog. MIT guys?\n\n*Doesn't it force miners to run more complex algorithms (such as linear\nprogramming) to find the optimum tx subset ?\n\n*Yes, but I don't see it as a drawback, but as a positive stimulus for\nresearchers to develop better tx selection algorithms. Anyway, the\ngreedy algorithm of picking the transactions with highest fees fees\nwould be good enough.\n\n*\nPLEASE don't confuse the acronym CoVar I used here with co-variance.*\n\nBest regard,\n  Sergio.\n\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/2fa8f7e2/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-10T20:51:41",
                "message_text_only": "On Sun, May 10, 2015 at 05:45:32PM -0300, Sergio Lerner wrote:\n> Two years ago I presented a new way to create a fee market that does not\n> depend on the block chain limit.\n\n<snip>\n\n> Solution: Require that the set of fees collected in a block has a\n> dispersion below a threshold. Use, for example, the Coefficient of\n> Variation (http://en.wikipedia.org/wiki/Coefficient_of_variation). If\n> the CoVar is higher than a fixed threshold, the block is considered invalid.\n\nIt's not possible to create consensus rules enforcing anything about\nfees because it's trivial to pay miners out of band.\n\nFor instance, you can pay transaction fees by including anyone-can-spend\noutputs in your transactions. The miner creating the block then simply\nadds a transaction at the end of their block collecting all the\nanyone-can-spend outputs. Equally, if you try to prohibit that - e.g. by\npreventing respending of funds in the same block - they can simply\npublish fee addresses and have people put individual outputs for those\naddresses in their transactions. (IIRC Eligius gave people the option to\npay fees that way for awhile)\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000fa57b40dc86a61d35aaf9241c86f047ef6f4bab8f13dfb7\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150510/db3d6fe4/attachment.sig>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-10T21:07:43",
                "message_text_only": "On Sun, May 10, 2015 at 8:45 PM, Sergio Lerner\n<sergiolerner at certimix.com> wrote:\n> Can the system by gamed?\n\nUsers can pay fees or a portion of fees out of band to miner(s); this\nis undetectable to the network.\n\nIt's also behavior that miners have engaged in since at least 2011 (in\ntwo forms;  treating transactions that paid them directly via outputs\nas having that much more in fees;  and taking contracts for fast\nprocessing for identified transactions (e.g. address matching or via\nan API) e.g. \"I'll pay you x at the end of the month for each of my\ntransactions you process, you can poll this API\". I'm aware of at\nleast two companies having had this arrangement with miners).\n\nI think what you suggested then just further rewards this behavior as\nit allows bypassing your controls.-- I suspect generally any scheme\nthe looks at the fee values has this property."
            },
            {
                "author": "Sergio Lerner",
                "date": "2015-05-11T06:09:58",
                "message_text_only": "El 10/05/2015 06:07 p.m., Gregory Maxwell escribi\u00f3:\n> On Sun, May 10, 2015 at 8:45 PM, Sergio Lerner\n> <sergiolerner at certimix.com> wrote:\n>> Can the system by gamed?\n> Users can pay fees or a portion of fees out of band to miner(s); this\n> is undetectable to the network.\nThen this is exactly what is needed. Let me explain.\n\nI know of 5 methods for a user to pay fees to a miner. I will explain\neach method and why these methods do not prevent the fee market from\nbeing created:\n\n1) By transaction fees\n\nThis is the standard, which would be limited by the CoVar algorithm, and\nwould create the fee market, if it were the only way to pay fees.\n\n2) By creating multiple transactions, each adding an output that pays to\neach miner (to a known miner address) the fees. User does not\npre-negotiate anything with miners.\n\nThis requires a transaction to have an additional output and requires\nsending through the p2p network one different transaction to each miner,\neach having an output with a \"known\" address of that miner. But the\nnetwork does not propagates double-spends, so those transaction would\nneed to be sent directly to the top miners, and to all at the same time.\nThe IP addresses of the top miners are not generally publicly available,\nand then may not accept new incoming connections. Also having an\nadditional output means the transactions would be larger, so they will\nscore lower by any metric the miner uses to choose transactions. Last,\nminers must be programmed to automatically interpret payments to their\naddresses as fees. The resulting protocol is very difficult to do\nreliably, expensive, as any delay would make one miner receive the\ntransaction from other miner and reject the double-spend that is being\nsend directly to it, increasing the average confirmation time.\n\n3) By adding an anyone-can-spend output for fees, so the miner can spend\nthat output in the same block.  User does not pre-negotiate anything\nwith miners.\n\nWe can hard-fork not to allow spending outputs created in the same\nblock. This is a drawback, unless we reduce the block rate, which is my\nproposal. However, spending in the same block also requires an storing\nin the block an additional input, which consumes at least 40 bytes more,\nand the transaction containing the input cannot be relayed to the\nnetwork in advance. Then the block that uses this method to collect fees\nfrom many transactions will propagate slower, and the miner may end\nloosing money. The any-one-can-spend output would take approximately 10\nbytes. So if transmitting 10+40=50 bytes, cost more than the fees\nearned, then miners do not have an incentive to game the system. It's\nhas been studied that \"each kilobyte costs an additional 80ms delay\nuntil a majority knows about the block.\" (Information propagation in the\nBitcoin network). So 50 bytes costs 3.9 ms in propagation time, which\nhaving a a 25 BTC subsidy is roughly equivalent to 0.2 mBTC. Currently\nthis is more than what transactions do pay in fees (about 0.1 mBTC), so\nthis should not be a problem for at least 5 years. And again, we could\njust prevent spending outputs in the same block they are created.\n\n4) Using a transaction having a single input having exactly the desired\noutput amount plus fees and signing the input with SIGHASH_SINGLE |\nSIGHASH_ANYONECANPAY and adding to the transaction a single output with\nthe desired amount. The miner will be able to join many of these\ntransactions and finally add an output to collect all fees together,\nwithout using standard transaction fees.\n\nThis is unreliable and cannot be systematically repeated without\ncreating a pre-transaction just to prepare the single input having the\namount plus fees exactly. The pre-transaction would need to pay fees, so\nthe problem is not avoided, just moved around.\n\n5) By negotiating out of band with the miner previously. Anything could\nbe agreed by the user and the miner.\n\nThis actually creates a parallel out-of-band market for fees, which is\nexactly what we want. If a user-to-miner pre-negotiation will take\nplace, then the miner can establish whatever price policy he wants to\ncompete and stay in business, as block data propagation costs money. So\nthere will be two fee markets, the \"out-of-band\" market, and the\n\"in-band\" market, and both should converge.\n\nMy conclusion is that fee markets will be created, and any alternate\nfee-paying methods (without a pre-negotiation) are not reliable nor\ncost-saving options. The full proposal would be to use the CoVar method,\nreduce the block rate to 1 minute, and do not allow spending outputs in\nthe same block they are created.\n\nBest regards,\n Sergio."
            },
            {
                "author": "Stephen",
                "date": "2015-05-11T05:31:09",
                "message_text_only": "Why do so many tie the block size debate to creating \"a fee market\", as if one didn't already exist? Yes, today we frequently see many low priority transactions included into the next block, but that does not mean there is not a marketplace for block space. It just means miners are not being sufficiently tough to create a *competitive* marketplace. \n\nBut who are we to say that the marketplace should be more competitive, and to go further and try to force it by altering consensus rules like the block size limit? If miners want to see more competitive fees, then they need only to alter their block creation protocol. \n\nThere are many arguments for and against changing the consensus limit on block size. I'm simply saying that \"to force a marketplace for fees/block space\" should not be one of them. Let the market develop on it's own. \n\n- Stephen\n\n\n\n> On May 10, 2015, at 4:45 PM, Sergio Lerner <sergiolerner at certimix.com> wrote:\n> \n> Two years ago I presented a new way to create a fee market that does not depend on the block chain limit.\n> \n> This proposal has not been formally analyzed in any paper since then, but I think it holds a good promise to untangle the current problem regarding increasing the tps and creating the fee market. BTW, think the maximum tps should be increased, but not by increasing the block size, but by increasing the block rate (I'll     expose why in my next e-mail).\n> \n> The original post is here (I was overly optimistic back then): https://bitcointalk.org/index.php?topic=147124.msg1561612#msg1561612\n> \n> I'll summarize it here again, with a little editing and a few more questions at the end:\n> \n> The idea is simple, but requires a hardfork, but is has minimum impact in the code and in the economics.\n> \n> Solution: Require that the set of fees collected in a block has a dispersion below a threshold. Use, for example, the Coefficient of Variation (http://en.wikipedia.org/wiki/Coefficient_of_variation). If the CoVar is higher than a fixed threshold, the block is considered invalid.\n> \n> The Coefficient of variation is computed as the standard deviation over the mean value, so it's very easy to compute. (if the mean is zero, we assume CoVar=0). Note that the CoVar function does not depend on the scale, so is just what a coin with a floating price requires.\n> \n> This means that if there are many transactions containing high fees in a block, then free transactions cannot be included.\n> The core devs should tweak the transaction selection algorithm to take into account this maximum bound.\n> \n> Example\n> \n> If the transaction fee set is: 0,0,0,0,5,5,6,7,8,7\n> The CoVar is 0.85\n> Suppose we limit the CoVar to a maximum of 1.\n> \n> Suppose the transaction fee set is: 0,0,0,0,0,0,0,0,0,10\n> Then the CoVar is 3.0\n> \n> In this case the miner should have to either drop the \"10\" from the fee set or drop the zeros. Obviously the miner will drop some zeros, and choose the set: 0,10, that has a CoVar of 1.\n> \n> Why it reduces the Tx spamming Problem?\n> \n> Using this little modification, spamming users would require to use higher fees, only if the remaining users in the community rises their fees. And miners won't be able to include an enormous amounts of spamming txs.\n> \n> Why it helps solving the tragedy-of-the-commons fee \"problem\"?\n> \n> As miners are forced to keep the CoVar below the threshold, if people rises the fees to confirm faster than spamming txs, automatically smamming txs become less likely to appear in blocks, and fee-estimators will automatically increase future fees, creating a the desired feedback loop.\n> \n> Why it helps solving the block size problem?\n> \n> Because if we increase the block size, miners that do not care about the fee market won't be able to fill the block with spamming txs and destroy the market that is being created. This is not a solution against an attacker-miner, which can always fill the block with transactions.\n> \n> Can the system by gamed? Can it be attacked?\n> \n> I don't think so. An attacker would need to spend a high amount in fees to prevent transactions with low fees to be included in a block. \n> However, a formal analysis would be required. Miller, Gun Sirer, Eyal.. Want to give it a try?\n> \n> Can create a positive feedback to a rise the fees to the top or push fess to the bottom?\n> \n> Again, I don't think so. This depends on the dynamics between the each node's fee estimator and the transaction backlog. MIT guys? \n> \n> Doesn't it force miners to run more complex algorithms (such as linear programming) to find the optimum tx subset ?\n> \n> Yes, but I don't see it as a drawback, but as a positive stimulus for researchers to develop better tx selection algorithms. Anyway, the greedy algorithm of picking the transactions with highest fees fees would be good enough. \n> \n> \n> PLEASE don't confuse the acronym CoVar I used here with co-variance.\n> \n> Best regard,\n>   Sergio.\n> \n> \n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/6eaf0f71/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A way to create a fee market even without a block size limit (2013)",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Gregory Maxwell",
                "Stephen",
                "Sergio Lerner",
                "Peter Todd"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 16753
        }
    },
    {
        "title": "[Bitcoin-development] Reducing the block rate instead of increasing the maximum block size",
        "thread_messages": [
            {
                "author": "Sergio Lerner",
                "date": "2015-05-11T07:03:29",
                "message_text_only": "In this e-mail I'll do my best to argue than if you accept that\nincreasing the transactions/second is a good direction to go, then\nincreasing the maximum block size is not the best way to do it. I argue\nthat the right direction to go is to decrease the block rate to 1\nminute, while keeping the block size limit to 1 Megabyte (or increasing\nit from a lower value such as 100 Kbyte and then have a step function).\nI'm backing up my claims with many hours of research simulating the\nBitcoin network under different conditions [1].  I'll try to convince\nyou by responding to each of the arguments I've heard against it.\n\nArguments against reducing the block interval\n\n1. It will encourage centralization, because participants of mining\npools will loose more money because of excessive initial block template\nlatency, which leads to higher stale shares\n\nWhen a new block is solved, that information needs to propagate\nthroughout the Bitcoin network up to the mining pool operator nodes,\nthen a new block header candidate is created, and this header must be\npropagated to all the mining pool users, ether by a push or a pull\nmodel. Generally the mining server pushes new work units to the\nindividual miners. If done other way around, the server would need to\nhandle a high load of continuous work requests that would be difficult\nto distinguish from a DDoS attack. So if the server pushes new block\nheader candidates to clients, then the problem boils down to increasing\nbandwidth of the servers to achieve a tenfold increase in work\ndistribution. Or distributing the servers geographically to achieve a\nlower latency. Propagating blocks does not require additional CPU\nresources, so mining pools administrators would need to increase\nmoderately their investment in the server infrastructure to achieve\nlower latency and higher bandwidth, but I guess the investment would be low.\n\n2. It will increase the probability of a block-chain split\n\nThe convergence of the network relies on the diminishing probability of\ntwo honest miners creating simultaneous competing blocks chains. To\nincrease the competition chain, competing blocks must be generated in\nalmost simultaneously (in the same time window approximately bounded by\nthe network average block propagation delay). The probability of a block\ncompetition decreases exponentially with the number of blocks. In fact,\nthe probability of a sustained competition on ten 1-minute blocks is one\nmillion times lower than the probability of a competition of one\n10-minute block. So even if the competition probability of six 1-minute\nblocks is higher than of six ten-minute blocks, this does not imply\nreducing the block rate increases this chance, but on the contrary, \nreduces it.\n\n3, It will reduce the security of the network\n\nThe security of the network is based on two facts:\nA- The miners are incentivized to extend the best chain\nB- The probability of a reversal based on a long block competition\ndecreases as more confirmation blocks are appended.\nC- Renting or buying hardware to perform a 51% attack is costly.\n\nA still holds. B holds for the same amount of confirmation blocks, so 6\nconfirmation blocks in a 10-minute block-chain is approximately\nequivalent to 6 confirmation blocks in a 1-minute block-chain.\nOnly C changes, as renting the hashing power for 6 minutes is ten times\nless expensive as renting it for 1 hour. However, there is no shop where\none can find 51% of the hashing power to rent right now, nor probably\nwill ever be if Bitcoin succeeds. Last, you can still have a 1 hour\nconfirmation (60 1-minute blocks) if you wish for high-valued payments,\nso the security decreases only if participant wish to decrease it.\n\n4. Reducing the block propagation time on the average case is good, but\nwhat happen in the worse case?\n\nMost methods proposed to reduce the block propagation delay do it only\non the average case. Any kind of block compression relies on both\nparties sharing some previous information. In the worse case it's true\nthat a miner can create and try to broadcast a block that takes too much\ntime to verify or bandwidth to transmit. This is currently true on the\nBitcoin network. Nevertheless there is no such incentive for miners,\nsince they will be shooting on their own foots. Peter Todd has argued\nthat the best strategy for miners is actually to reach 51% of the\nnetwork, but not more. In other words, to exclude the slowest 49%\npercent. But this strategy of creating bloated blocks is too risky in\npractice, and surely doomed to fail, as network conditions dynamically \nchange. Also it would be perceived as an attack to the network, and the\nminer (if it is a public mining pool) would be probably blacklisted.\n\n5. Thousands of SPV wallets running in mobile devices would need to be\nupgraded (thanks Mike).\n\nThat depends on the current upgrade rate for SPV wallets like Bitcoin\nWallet  and BreadWallet. Suppose that the upgrade rate is 80%/year: we\ndevelop the source code for the change now and apply the change in Q2\n2016, then  most of the nodes will already be upgraded by when the\nhardfork takes place. Also a public notice telling people to upgrade in\nweb pages, bitcointalk, SPV wallets warnings, coindesk, one year in\nadvance will give plenty of time to SPV wallet users to upgrade.\n\n6. If there are 10x more blocks, then there are 10x more block headers,\nand that increases the amount of bandwidth SPV wallets need to catch up\nwith the chain\n \nA standard smartphone with average cellular downstream speed downloads\n2.6 headers per second (1600 kbits/sec) [3], so if synchronization were\nto be done only at night when the phone is connected to the power line,\nthen it would take 9 minutes to synchronize with 1440 headers/day. If a\nperson should accept a payment, and the smart-phone is 1 day\nout-of-synch, then it takes less time to download all the missing\nheaders than to wait for a 10-minute one block confirmation. Obviously\nall smartphones with 3G have a downstream bandwidth much higher,\naveraging 1 Mbps. So the whole synchronization will be done less than a\n1-minute block confirmation.\n \nAccording to CISCO mobile bandwidth connection speed increases 20% every\nyear. In four years, it will have doubled, so mobile phones with lower\nthan average data connection will soon be able to catchup.\nAlso there is low-hanging-fruit optimizations to the protocol that have\nnot been implemented: each header is 80 bytes in length. When a set of\nchained headers is transferred, the headers could be compressed,\nstripping 32 bytes of each header that is derived from the previous\nheader hash digest. So a 40% compression is already possible by slightly\nmodifying the wire protocol.\n \n7. There has been insufficient testing and/or insufficient research into\ntechnical/economic implications or reducing the block rate\n \nThis is partially true. in the GHOST paper, this has been analyzed, and\nthe problem was shown to be solvable for block intervals of just a few\nseconds. There are several proof-of-work cryptocurrencies in existence\nthat have lower than 1 minute block intervals and they work just fine.\nFirst there was Bitcoin with a 10 minute interval, then was LiteCoin\nusing a 2.5 interval, then was DogeCoin with 1 minute, and then\nQuarkCoin with just 30 seconds. Every new cryptocurrency lowers it a\nlittle bit. Some time ago I decided to research on the block rate to\nunderstand how the block interval impacts the stability and capability\nof the cryptocurrency network, and I came up with the idea of the DECOR+\nprotocol [4] (which requires changes in the consensus code). In my\nresearch I also showed how the stale rate can be easily reduced only\nwith changes in the networking code, and not in the consensus code.\nThese networking optimizations ( O(1) propagation using headers-first or\nIBLTs), can be added later.\n \nMortifying Bitcoin to accommodate the change to lower the block rate\nrequires at least:\n \n- Changing the 21 BTC reward per block to 2.1 BTC\n- Changing the nPowTargetTimespan constant\n- Writing code to hard-fork automatically when the majority of miners\nhave upgraded.\n- Allow transaction version 3, and interpret nLockTimes of transaction\nversion 2 as being multiplied by 10.\n\nAll changes comprises no more than 15 lines of code. This is much less\nthan the number of lines modified by Gavin's 20Mb patch.\n \nAs a conclusion, I haven't yet heard a good argument against lowering\nthe block rate.\n\nBest regards,\n Sergio.\n \n[0] https://medium.com/@octskyward/the-capacity-cliff-586d1bf7715e\n[1] https://bitslog.wordpress.com/2014/02/17/5-sec-block-interval/\n[2] http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n[3]\nhttp://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/white_paper_c11-520862.html\n[4] https://bitslog.wordpress.com/2014/05/02/decor/"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-11T10:34:02",
                "message_text_only": "On Mon, May 11, 2015 at 04:03:29AM -0300, Sergio Lerner wrote:\n> Arguments against reducing the block interval\n> \n> 1. It will encourage centralization, because participants of mining\n> pools will loose more money because of excessive initial block template\n> latency, which leads to higher stale shares\n> \n> When a new block is solved, that information needs to propagate\n> throughout the Bitcoin network up to the mining pool operator nodes,\n> then a new block header candidate is created, and this header must be\n> propagated to all the mining pool users, ether by a push or a pull\n> model. Generally the mining server pushes new work units to the\n> individual miners. If done other way around, the server would need to\n> handle a high load of continuous work requests that would be difficult\n> to distinguish from a DDoS attack. So if the server pushes new block\n> header candidates to clients, then the problem boils down to increasing\n> bandwidth of the servers to achieve a tenfold increase in work\n> distribution. Or distributing the servers geographically to achieve a\n> lower latency. Propagating blocks does not require additional CPU\n> resources, so mining pools administrators would need to increase\n> moderately their investment in the server infrastructure to achieve\n> lower latency and higher bandwidth, but I guess the investment would be low.\n\nIt's *way* easier to buy more bandwidth that it is to get lower latency.\n\nAfter all, getting to the other side of the planet via fiber takes at\n*minimum* 100ms simply due to the speed of light; routing overheads\napproximately double or triple that for all but highly specialized and\nvery, very expensive, networking services. Bandwidth simply can't fix\nthe speed of light.\n\nIt's also not at all realistic or desirable to assume connectivity in a\nsingle hop, so you can again multiply that base latency by 2-5 times.\n\nAnd on top of *that* you have to take into account latency from hasher\nto mining pool - time that the hashing power isn't working on the new\nblock because they're work unit hasn't been updated matters just as much\nas the time to get that block to the pool in the first place. Being\nforced to reduce that latency is very damaging to the ecosystem as\nyou're making it more profitable to keep hashing power centralized.\n\nIn any case, even with 10 minute blocks pools already pay a lot of\nattention to latency... Why make that problem 10x worse?\n\n> 2. It will increase the probability of a block-chain split\n> \n> The convergence of the network relies on the diminishing probability of\n> two honest miners creating simultaneous competing blocks chains. To\n> increase the competition chain, competing blocks must be generated in\n> almost simultaneously (in the same time window approximately bounded by\n> the network average block propagation delay). The probability of a block\n> competition decreases exponentially with the number of blocks. In fact,\n> the probability of a sustained competition on ten 1-minute blocks is one\n> million times lower than the probability of a competition of one\n> 10-minute block. So even if the competition probability of six 1-minute\n> blocks is higher than of six ten-minute blocks, this does not imply\n> reducing the block rate increases this chance, but on the contrary, \n> reduces it.\n\nCan you explain your reasoning here in detail?\n\n> 4. Reducing the block propagation time on the average case is good, but\n> what happen in the worse case?\n> \n> Most methods proposed to reduce the block propagation delay do it only\n> on the average case. Any kind of block compression relies on both\n> parties sharing some previous information. In the worse case it's true\n> that a miner can create and try to broadcast a block that takes too much\n> time to verify or bandwidth to transmit. This is currently true on the\n> Bitcoin network. Nevertheless there is no such incentive for miners,\n> since they will be shooting on their own foots. Peter Todd has argued\n> that the best strategy for miners is actually to reach 51% of the\n> network, but not more. In other words, to exclude the slowest 49%\n\nActually the correct figure is less than ~30%:\n\nhttp://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg03200.html\n\n> percent. But this strategy of creating bloated blocks is too risky in\n> practice, and surely doomed to fail, as network conditions dynamically \n> change.\n\nThey dynamically change? Source?\n\nRemember that the strategy still gives you a benefit if you simply\ntarget, say, 75% rather than the minimum threshold.\n\n> Also it would be perceived as an attack to the network, and the\n> miner (if it is a public mining pool) would be probably blacklisted.\n\nHow do you see that blacklisting actually being done?\n\nEqually, it's easy to portray such mining as being \"for the good of\nBitcoin\" - \"we're just making transaction cheap! tough luck if your\nshitty pool can't keep up\" This is quite unlike selfish mining.\n\n> 7. There has been insufficient testing and/or insufficient research into\n> technical/economic implications or reducing the block rate\n>  \n> This is partially true. in the GHOST paper, this has been analyzed, and\n> the problem was shown to be solvable for block intervals of just a few\n> seconds.\n\nGHOST works radically differently than a linear blockchain, and it's not\nclear that it actually has the correct economic incentives.\n\n> These networking optimizations ( O(1) propagation using headers-first or\n> IBLTs), can be added later.\n\nKeep in mind that miners already use optimized propagation techniques,\nlike p2pool's implementation or Matt Corallo's block relaying network.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000c2b75113c6d2539f436ee9ac90abf620d9d3a3a4a19d3e8\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/7490b435/attachment.sig>"
            },
            {
                "author": "insecurity at national.shitposting.agency",
                "date": "2015-05-11T11:10:09",
                "message_text_only": "On 2015-05-11 10:34, Peter Todd wrote:\n> How do you see that blacklisting actually being done?\n\nSame way ghash.io was banned from the network when used Finney attacks\nagainst BetCoin Dice.\n\nAs Andreas Antonopoulos says, if any of the miners do anything bad, we\njust ban them from mining. Any sort of attack like this only lasts 10\nminutes as a result. Stop worrying so much.\n\nhttps://youtu.be/ncPyMUfNyVM?t=20s"
            },
            {
                "author": "Dave Hudson",
                "date": "2015-05-11T11:49:03",
                "message_text_only": "> On 11 May 2015, at 12:10, insecurity at national.shitposting.agency wrote:\n> \n> On 2015-05-11 10:34, Peter Todd wrote:\n>> How do you see that blacklisting actually being done?\n> \n> Same way ghash.io was banned from the network when used Finney attacks\n> against BetCoin Dice.\n> \n> As Andreas Antonopoulos says, if any of the miners do anything bad, we\n> just ban them from mining. Any sort of attack like this only lasts 10\n> minutes as a result. Stop worrying so much.\n\nThis doesn't work because a large-scale miner can trivially make themselves look like a very large number of much smaller scale miners. Their ability to minimize variance comes from the cumulative totals they control so 10 pools of 1% of the network cumulatively have the same variance as 1 pool with 10% of the network. It's also very easy for miners to relay blocks via different addresses and the cost is minimal. The biggest cost would be in DDoS prevention and a miner that actually split their pool into lots of small fragments would actually give themselves the ability to do quite a lot of DDoS mitigation anyway. If no-one is doing this right now it's simply because they've not had the right incentives to make it worthwhile; if the incentives make it worthwhile then this is pretty trivial to do.\n\nThis is one area where anonymity on behalf of transaction validators and block makers essentially makes it pretty-much impossible to maintain any sort of sanctions against antisocial behaviour."
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-11T12:34:43",
                "message_text_only": "The propagation speed gain from having smaller blocks is linear in the size\nreduction, down to a small size, after which the delay of the first byte\nprevails [1], however the blockchain fork rate increases superlinearly,\ngiving an overall worse tradeoff. A high blockchain fork rate is a symptom\nof inefficient use of the network's mining resources and may give an\nadvantage to an attacker that is more efficient in communicating internally.\n\nI'd strongly against increasing the block generation rate in Bitcoin, it'd\nbe a very controversial proposal and would not solve anything.\n\n[1]\nhttp://www.tik.ee.ethz.ch/file/49318d3f56c1d525aabf7fda78b23fc0/P2P2013_041.pdf\n\nOn Mon, May 11, 2015 at 1:51 PM Dave Hudson <dave at hashingit.com> wrote:\n\n>\n> > On 11 May 2015, at 12:10, insecurity at national.shitposting.agency wrote:\n> >\n> > On 2015-05-11 10:34, Peter Todd wrote:\n> >> How do you see that blacklisting actually being done?\n> >\n> > Same way ghash.io was banned from the network when used Finney attacks\n> > against BetCoin Dice.\n> >\n> > As Andreas Antonopoulos says, if any of the miners do anything bad, we\n> > just ban them from mining. Any sort of attack like this only lasts 10\n> > minutes as a result. Stop worrying so much.\n>\n> This doesn't work because a large-scale miner can trivially make\n> themselves look like a very large number of much smaller scale miners.\n> Their ability to minimize variance comes from the cumulative totals they\n> control so 10 pools of 1% of the network cumulatively have the same\n> variance as 1 pool with 10% of the network. It's also very easy for miners\n> to relay blocks via different addresses and the cost is minimal. The\n> biggest cost would be in DDoS prevention and a miner that actually split\n> their pool into lots of small fragments would actually give themselves the\n> ability to do quite a lot of DDoS mitigation anyway. If no-one is doing\n> this right now it's simply because they've not had the right incentives to\n> make it worthwhile; if the incentives make it worthwhile then this is\n> pretty trivial to do.\n>\n> This is one area where anonymity on behalf of transaction validators and\n> block makers essentially makes it pretty-much impossible to maintain any\n> sort of sanctions against antisocial behaviour.\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/9b14788a/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-05-11T16:47:47",
                "message_text_only": "On Monday, May 11, 2015 7:03:29 AM Sergio Lerner wrote:\n> 1. It will encourage centralization, because participants of mining\n> pools will loose more money because of excessive initial block template\n> latency, which leads to higher stale shares\n> \n> When a new block is solved, that information needs to propagate\n> throughout the Bitcoin network up to the mining pool operator nodes,\n> then a new block header candidate is created, and this header must be\n> propagated to all the mining pool users, ether by a push or a pull\n> model. Generally the mining server pushes new work units to the\n> individual miners. If done other way around, the server would need to\n> handle a high load of continuous work requests that would be difficult\n> to distinguish from a DDoS attack. So if the server pushes new block\n> header candidates to clients, then the problem boils down to increasing\n> bandwidth of the servers to achieve a tenfold increase in work\n> distribution. Or distributing the servers geographically to achieve a\n> lower latency. Propagating blocks does not require additional CPU\n> resources, so mining pools administrators would need to increase\n> moderately their investment in the server infrastructure to achieve\n> lower latency and higher bandwidth, but I guess the investment would be\n> low.\n\n1. Latency is what matters here, not bandwidth so much. And latency reduction \nis either expensive or impossible.\n2. Mining pools are mostly run at a loss (with exception to only the most \ncentralised pools), and have nothing to invest in increasing infrastructure.\n\n> 3, It will reduce the security of the network\n> \n> The security of the network is based on two facts:\n> A- The miners are incentivized to extend the best chain\n> B- The probability of a reversal based on a long block competition\n> decreases as more confirmation blocks are appended.\n> C- Renting or buying hardware to perform a 51% attack is costly.\n> \n> A still holds. B holds for the same amount of confirmation blocks, so 6\n> confirmation blocks in a 10-minute block-chain is approximately\n> equivalent to 6 confirmation blocks in a 1-minute block-chain.\n> Only C changes, as renting the hashing power for 6 minutes is ten times\n> less expensive as renting it for 1 hour. However, there is no shop where\n> one can find 51% of the hashing power to rent right now, nor probably\n> will ever be if Bitcoin succeeds. Last, you can still have a 1 hour\n> confirmation (60 1-minute blocks) if you wish for high-valued payments,\n> so the security decreases only if participant wish to decrease it.\n\nYou're overlooking at least:\n1. The real network has to suffer wasted work as a result of the stale blocks, \nwhile an attacker does not. If 20% of blocks are stale, the attacker only \nneeds 40% of the legitimate hashrate to achieve 50%-in-practice.\n2. Since blocks are individually weaker, it becomes cheaper to DoS nodes with \ninvalid blocks. (not sure if this is a real concern, but it ought to be \nconsidered and addressed)\n\n> 4. Reducing the block propagation time on the average case is good, but\n> what happen in the worse case?\n> \n> Most methods proposed to reduce the block propagation delay do it only\n> on the average case. Any kind of block compression relies on both\n> parties sharing some previous information. In the worse case it's true\n> that a miner can create and try to broadcast a block that takes too much\n> time to verify or bandwidth to transmit. This is currently true on the\n> Bitcoin network. Nevertheless there is no such incentive for miners,\n> since they will be shooting on their own foots. Peter Todd has argued\n> that the best strategy for miners is actually to reach 51% of the\n> network, but not more. In other words, to exclude the slowest 49%\n> percent. But this strategy of creating bloated blocks is too risky in\n> practice, and surely doomed to fail, as network conditions dynamically\n> change. Also it would be perceived as an attack to the network, and the\n> miner (if it is a public mining pool) would be probably blacklisted.\n\nOne can probably overcome changing network conditions merely by trying to \nreach 75% and exclude the slowest 25%. Also, there is no way to identify or \nblacklist miners.\n\n> 5. Thousands of SPV wallets running in mobile devices would need to be\n> upgraded (thanks Mike).\n> \n> That depends on the current upgrade rate for SPV wallets like Bitcoin\n> Wallet  and BreadWallet. Suppose that the upgrade rate is 80%/year: we\n> develop the source code for the change now and apply the change in Q2\n> 2016, then  most of the nodes will already be upgraded by when the\n> hardfork takes place. Also a public notice telling people to upgrade in\n> web pages, bitcointalk, SPV wallets warnings, coindesk, one year in\n> advance will give plenty of time to SPV wallet users to upgrade.\n\nI agree this shouldn't be a real concern. SPV wallets are also more likely and \nless risky (globally) to be auto-updated.\n\n> 6. If there are 10x more blocks, then there are 10x more block headers,\n> and that increases the amount of bandwidth SPV wallets need to catch up\n> with the chain\n> \n> A standard smartphone with average cellular downstream speed downloads\n> 2.6 headers per second (1600 kbits/sec) [3], so if synchronization were\n> to be done only at night when the phone is connected to the power line,\n> then it would take 9 minutes to synchronize with 1440 headers/day. If a\n> person should accept a payment, and the smart-phone is 1 day\n> out-of-synch, then it takes less time to download all the missing\n> headers than to wait for a 10-minute one block confirmation. Obviously\n> all smartphones with 3G have a downstream bandwidth much higher,\n> averaging 1 Mbps. So the whole synchronization will be done less than a\n> 1-minute block confirmation.\n\nUh, I think you need to be using at least median speeds. As an example, I can \nonly sustain (over 3G) about 40 kbps, with a peak of around 400 kbps. 3G has \nworse range/coverage than 2G. No doubt the *average* is skewed so high because \nof densely populated areas like San Francisco having 400+ Mbps cellular data. \nIt's not reasonable to assume sync only at night: most payments will be during \nthe day, on battery - so increased power use must also be considered.\n\n> According to CISCO mobile bandwidth connection speed increases 20% every\n> year.\n\nOnly in small densely populated areas of first-world countries.\n\nLuke"
            },
            {
                "author": "Thy Shizzle",
                "date": "2015-05-11T07:30:49",
                "message_text_only": "Yes This!\n\nSo many people seem hung up on growing the block size! If gaining a higher tps throughput is the main aim, I think that this proposition to speed up block creation has merit!\n\nYes it will lead to an increase in the block chain still due to 1mb ~1 minute instead of ~10 minute, but the change to the protocol is minor, you are only adding in a different difficulty rate starting from hight blah, no new features or anything are being added so there seems to me much less of a security risk! Also that impact if a hard fork should be minimal because there is nothing but absolute incentive for miners to mine at the new easier difficulty!\n\nI feel this deserves a great deal of consideration as opposed to blowing out the block through miners voting etc!!!!\n________________________________\nFrom: Sergio Lerner<mailto:sergiolerner at certimix.com>\nSent: \u200e11/\u200e05/\u200e2015 5:05 PM\nTo: bitcoin-development at lists.sourceforge.net<mailto:bitcoin-development at lists.sourceforge.net>\nSubject: [Bitcoin-development] Reducing the block rate instead of increasing the maximum block size\n\nIn this e-mail I'll do my best to argue than if you accept that\nincreasing the transactions/second is a good direction to go, then\nincreasing the maximum block size is not the best way to do it. I argue\nthat the right direction to go is to decrease the block rate to 1\nminute, while keeping the block size limit to 1 Megabyte (or increasing\nit from a lower value such as 100 Kbyte and then have a step function).\nI'm backing up my claims with many hours of research simulating the\nBitcoin network under different conditions [1].  I'll try to convince\nyou by responding to each of the arguments I've heard against it.\n\nArguments against reducing the block interval\n\n1. It will encourage centralization, because participants of mining\npools will loose more money because of excessive initial block template\nlatency, which leads to higher stale shares\n\nWhen a new block is solved, that information needs to propagate\nthroughout the Bitcoin network up to the mining pool operator nodes,\nthen a new block header candidate is created, and this header must be\npropagated to all the mining pool users, ether by a push or a pull\nmodel. Generally the mining server pushes new work units to the\nindividual miners. If done other way around, the server would need to\nhandle a high load of continuous work requests that would be difficult\nto distinguish from a DDoS attack. So if the server pushes new block\nheader candidates to clients, then the problem boils down to increasing\nbandwidth of the servers to achieve a tenfold increase in work\ndistribution. Or distributing the servers geographically to achieve a\nlower latency. Propagating blocks does not require additional CPU\nresources, so mining pools administrators would need to increase\nmoderately their investment in the server infrastructure to achieve\nlower latency and higher bandwidth, but I guess the investment would be low.\n\n2. It will increase the probability of a block-chain split\n\nThe convergence of the network relies on the diminishing probability of\ntwo honest miners creating simultaneous competing blocks chains. To\nincrease the competition chain, competing blocks must be generated in\nalmost simultaneously (in the same time window approximately bounded by\nthe network average block propagation delay). The probability of a block\ncompetition decreases exponentially with the number of blocks. In fact,\nthe probability of a sustained competition on ten 1-minute blocks is one\nmillion times lower than the probability of a competition of one\n10-minute block. So even if the competition probability of six 1-minute\nblocks is higher than of six ten-minute blocks, this does not imply\nreducing the block rate increases this chance, but on the contrary,\nreduces it.\n\n3, It will reduce the security of the network\n\nThe security of the network is based on two facts:\nA- The miners are incentivized to extend the best chain\nB- The probability of a reversal based on a long block competition\ndecreases as more confirmation blocks are appended.\nC- Renting or buying hardware to perform a 51% attack is costly.\n\nA still holds. B holds for the same amount of confirmation blocks, so 6\nconfirmation blocks in a 10-minute block-chain is approximately\nequivalent to 6 confirmation blocks in a 1-minute block-chain.\nOnly C changes, as renting the hashing power for 6 minutes is ten times\nless expensive as renting it for 1 hour. However, there is no shop where\none can find 51% of the hashing power to rent right now, nor probably\nwill ever be if Bitcoin succeeds. Last, you can still have a 1 hour\nconfirmation (60 1-minute blocks) if you wish for high-valued payments,\nso the security decreases only if participant wish to decrease it.\n\n4. Reducing the block propagation time on the average case is good, but\nwhat happen in the worse case?\n\nMost methods proposed to reduce the block propagation delay do it only\non the average case. Any kind of block compression relies on both\nparties sharing some previous information. In the worse case it's true\nthat a miner can create and try to broadcast a block that takes too much\ntime to verify or bandwidth to transmit. This is currently true on the\nBitcoin network. Nevertheless there is no such incentive for miners,\nsince they will be shooting on their own foots. Peter Todd has argued\nthat the best strategy for miners is actually to reach 51% of the\nnetwork, but not more. In other words, to exclude the slowest 49%\npercent. But this strategy of creating bloated blocks is too risky in\npractice, and surely doomed to fail, as network conditions dynamically\nchange. Also it would be perceived as an attack to the network, and the\nminer (if it is a public mining pool) would be probably blacklisted.\n\n5. Thousands of SPV wallets running in mobile devices would need to be\nupgraded (thanks Mike).\n\nThat depends on the current upgrade rate for SPV wallets like Bitcoin\nWallet  and BreadWallet. Suppose that the upgrade rate is 80%/year: we\ndevelop the source code for the change now and apply the change in Q2\n2016, then  most of the nodes will already be upgraded by when the\nhardfork takes place. Also a public notice telling people to upgrade in\nweb pages, bitcointalk, SPV wallets warnings, coindesk, one year in\nadvance will give plenty of time to SPV wallet users to upgrade.\n\n6. If there are 10x more blocks, then there are 10x more block headers,\nand that increases the amount of bandwidth SPV wallets need to catch up\nwith the chain\n\nA standard smartphone with average cellular downstream speed downloads\n2.6 headers per second (1600 kbits/sec) [3], so if synchronization were\nto be done only at night when the phone is connected to the power line,\nthen it would take 9 minutes to synchronize with 1440 headers/day. If a\nperson should accept a payment, and the smart-phone is 1 day\nout-of-synch, then it takes less time to download all the missing\nheaders than to wait for a 10-minute one block confirmation. Obviously\nall smartphones with 3G have a downstream bandwidth much higher,\naveraging 1 Mbps. So the whole synchronization will be done less than a\n1-minute block confirmation.\n\nAccording to CISCO mobile bandwidth connection speed increases 20% every\nyear. In four years, it will have doubled, so mobile phones with lower\nthan average data connection will soon be able to catchup.\nAlso there is low-hanging-fruit optimizations to the protocol that have\nnot been implemented: each header is 80 bytes in length. When a set of\nchained headers is transferred, the headers could be compressed,\nstripping 32 bytes of each header that is derived from the previous\nheader hash digest. So a 40% compression is already possible by slightly\nmodifying the wire protocol.\n\n7. There has been insufficient testing and/or insufficient research into\ntechnical/economic implications or reducing the block rate\n\nThis is partially true. in the GHOST paper, this has been analyzed, and\nthe problem was shown to be solvable for block intervals of just a few\nseconds. There are several proof-of-work cryptocurrencies in existence\nthat have lower than 1 minute block intervals and they work just fine.\nFirst there was Bitcoin with a 10 minute interval, then was LiteCoin\nusing a 2.5 interval, then was DogeCoin with 1 minute, and then\nQuarkCoin with just 30 seconds. Every new cryptocurrency lowers it a\nlittle bit. Some time ago I decided to research on the block rate to\nunderstand how the block interval impacts the stability and capability\nof the cryptocurrency network, and I came up with the idea of the DECOR+\nprotocol [4] (which requires changes in the consensus code). In my\nresearch I also showed how the stale rate can be easily reduced only\nwith changes in the networking code, and not in the consensus code.\nThese networking optimizations ( O(1) propagation using headers-first or\nIBLTs), can be added later.\n\nMortifying Bitcoin to accommodate the change to lower the block rate\nrequires at least:\n\n- Changing the 21 BTC reward per block to 2.1 BTC\n- Changing the nPowTargetTimespan constant\n- Writing code to hard-fork automatically when the majority of miners\nhave upgraded.\n- Allow transaction version 3, and interpret nLockTimes of transaction\nversion 2 as being multiplied by 10.\n\nAll changes comprises no more than 15 lines of code. This is much less\nthan the number of lines modified by Gavin's 20Mb patch.\n\nAs a conclusion, I haven't yet heard a good argument against lowering\nthe block rate.\n\nBest regards,\n Sergio.\n\n[0] https://medium.com/@octskyward/the-capacity-cliff-586d1bf7715e\n[1] https://bitslog.wordpress.com/2014/02/17/5-sec-block-interval/\n[2] http://gavinandresen.ninja/time-to-roll-out-bigger-blocks\n[3]\nhttp://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/white_paper_c11-520862.html\n[4] https://bitslog.wordpress.com/2014/05/02/decor/\n\n------------------------------------------------------------------------------\nOne dashboard for servers and applications across Physical-Virtual-Cloud\nWidest out-of-the-box monitoring support with 50+ applications\nPerformance metrics, stats and reports that give you Actionable Insights\nDeep dive visibility with transaction tracing using APM Insight.\nhttp://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/daee8db4/attachment.html>"
            },
            {
                "author": "Dave Hudson",
                "date": "2015-05-11T08:16:17",
                "message_text_only": "I proposed the same thing last year (there's a video of the presentation I was giving somewhere around). My intuition was that this would require slowly reducing the inter-block time, probably by step reductions at particular block heights.\n\nHaving had almost a year to think about it some more there are a few subtleties:\n\n1) I think it could discourage decentralisation if the nominal 2 week period per difficulty retarget is retained. If we reached 4032 blocks and a 5 minute block time then there would be 2x as many blocks at any given difficulty which increases the odds of a smaller pool finding a block and thus getting a reward. Block rewards would have to drop in proportion to the reduced interval to keep the total schedule of 21M coins on track though, but the reduction in variance is a win for smaller miners.\n\n2) There are limits to the block time. The speed of light is an ultimately limiting factor here, but we would want to avoid excessive orphan rates.\n\n3) There would be some amount of confusion about numbers of confirmations. I actually think that confirmation numbers are a really misleading idea anyway and it would be safer to think in terms of \"minutes of security\". A zero conf transaction has \"zero minutes\", while right now 1, 2, 3 and 6 would be \"ten minutes\", \"twenty minutes\", \"thirty minutes\" and \"sixty minutes\" respectively. If our block time were 5 minutes then 8 confirmations would be \"forty minutes\" of security; if the block time was 2.5 minutes then 8 confirmations would be \"twenty minutes\" of security. The \"minutes of security\" measure indicates the mean number of minutes of the entire network's hash rate would be required to undo a transaction.\n\n4) Reducing the inter-block time reduces the variance in reaching that \"sixty minutes\" of security level. The variance around finding 6 blocks with a ten minute interval is much wider than the variance for finding 12 blocks with a 5 minute interval.\n\n\n\n> On 11 May 2015, at 08:30, Thy Shizzle <thyshizzle at outlook.com> wrote:\n> \n> Yes This!\n> \n> So many people seem hung up on growing the block size! If gaining a higher tps throughput is the main aim, I think that this proposition to speed up block creation has merit!\n> \n> Yes it will lead to an increase in the block chain still due to 1mb ~1 minute instead of ~10 minute, but the change to the protocol is minor, you are only adding in a different difficulty rate starting from hight blah, no new features or anything are being added so there seems to me much less of a security risk! Also that impact if a hard fork should be minimal because there is nothing but absolute incentive for miners to mine at the new easier difficulty!\n> \n> I feel this deserves a great deal of consideration as opposed to blowing out the block through miners voting etc!!!!\n> From: Sergio Lerner <mailto:sergiolerner at certimix.com>\n> Sent: \u200e11/\u200e05/\u200e2015 5:05 PM\n> To: bitcoin-development at lists.sourceforge.net <mailto:bitcoin-development at lists.sourceforge.net>\n> Subject: [Bitcoin-development] Reducing the block rate instead of increasing the maximum block size\n> \n> In this e-mail I'll do my best to argue than if you accept that\n> increasing the transactions/second is a good direction to go, then\n> increasing the maximum block size is not the best way to do it. I argue\n> that the right direction to go is to decrease the block rate to 1\n> minute, while keeping the block size limit to 1 Megabyte (or increasing\n> it from a lower value such as 100 Kbyte and then have a step function).\n> I'm backing up my claims with many hours of research simulating the\n> Bitcoin network under different conditions [1].  I'll try to convince\n> you by responding to each of the arguments I've heard against it.\n> \n> Arguments against reducing the block interval\n> \n> 1. It will encourage centralization, because participants of mining\n> pools will loose more money because of excessive initial block template\n> latency, which leads to higher stale shares\n> \n> When a new block is solved, that information needs to propagate\n> throughout the Bitcoin network up to the mining pool operator nodes,\n> then a new block header candidate is created, and this header must be\n> propagated to all the mining pool users, ether by a push or a pull\n> model. Generally the mining server pushes new work units to the\n> individual miners. If done other way around, the server would need to\n> handle a high load of continuous work requests that would be difficult\n> to distinguish from a DDoS attack. So if the server pushes new block\n> header candidates to clients, then the problem boils down to increasing\n> bandwidth of the servers to achieve a tenfold increase in work\n> distribution. Or distributing the servers geographically to achieve a\n> lower latency. Propagating blocks does not require additional CPU\n> resources, so mining pools administrators would need to increase\n> moderately their investment in the server infrastructure to achieve\n> lower latency and higher bandwidth, but I guess the investment would be low.\n> \n> 2. It will increase the probability of a block-chain split\n> \n> The convergence of the network relies on the diminishing probability of\n> two honest miners creating simultaneous competing blocks chains. To\n> increase the competition chain, competing blocks must be generated in\n> almost simultaneously (in the same time window approximately bounded by\n> the network average block propagation delay). The probability of a block\n> competition decreases exponentially with the number of blocks. In fact,\n> the probability of a sustained competition on ten 1-minute blocks is one\n> million times lower than the probability of a competition of one\n> 10-minute block. So even if the competition probability of six 1-minute\n> blocks is higher than of six ten-minute blocks, this does not imply\n> reducing the block rate increases this chance, but on the contrary, \n> reduces it.\n> \n> 3, It will reduce the security of the network\n> \n> The security of the network is based on two facts:\n> A- The miners are incentivized to extend the best chain\n> B- The probability of a reversal based on a long block competition\n> decreases as more confirmation blocks are appended.\n> C- Renting or buying hardware to perform a 51% attack is costly.\n> \n> A still holds. B holds for the same amount of confirmation blocks, so 6\n> confirmation blocks in a 10-minute block-chain is approximately\n> equivalent to 6 confirmation blocks in a 1-minute block-chain.\n> Only C changes, as renting the hashing power for 6 minutes is ten times\n> less expensive as renting it for 1 hour. However, there is no shop where\n> one can find 51% of the hashing power to rent right now, nor probably\n> will ever be if Bitcoin succeeds. Last, you can still have a 1 hour\n> confirmation (60 1-minute blocks) if you wish for high-valued payments,\n> so the security decreases only if participant wish to decrease it.\n> \n> 4. Reducing the block propagation time on the average case is good, but\n> what happen in the worse case?\n> \n> Most methods proposed to reduce the block propagation delay do it only\n> on the average case. Any kind of block compression relies on both\n> parties sharing some previous information. In the worse case it's true\n> that a miner can create and try to broadcast a block that takes too much\n> time to verify or bandwidth to transmit. This is currently true on the\n> Bitcoin network. Nevertheless there is no such incentive for miners,\n> since they will be shooting on their own foots. Peter Todd has argued\n> that the best strategy for miners is actually to reach 51% of the\n> network, but not more. In other words, to exclude the slowest 49%\n> percent. But this strategy of creating bloated blocks is too risky in\n> practice, and surely doomed to fail, as network conditions dynamically \n> change. Also it would be perceived as an attack to the network, and the\n> miner (if it is a public mining pool) would be probably blacklisted.\n> \n> 5. Thousands of SPV wallets running in mobile devices would need to be\n> upgraded (thanks Mike).\n> \n> That depends on the current upgrade rate for SPV wallets like Bitcoin\n> Wallet  and BreadWallet. Suppose that the upgrade rate is 80%/year: we\n> develop the source code for the change now and apply the change in Q2\n> 2016, then  most of the nodes will already be upgraded by when the\n> hardfork takes place. Also a public notice telling people to upgrade in\n> web pages, bitcointalk, SPV wallets warnings, coindesk, one year in\n> advance will give plenty of time to SPV wallet users to upgrade.\n> \n> 6. If there are 10x more blocks, then there are 10x more block headers,\n> and that increases the amount of bandwidth SPV wallets need to catch up\n> with the chain\n>  \n> A standard smartphone with average cellular downstream speed downloads\n> 2.6 headers per second (1600 kbits/sec) [3], so if synchronization were\n> to be done only at night when the phone is connected to the power line,\n> then it would take 9 minutes to synchronize with 1440 headers/day. If a\n> person should accept a payment, and the smart-phone is 1 day\n> out-of-synch, then it takes less time to download all the missing\n> headers than to wait for a 10-minute one block confirmation. Obviously\n> all smartphones with 3G have a downstream bandwidth much higher,\n> averaging 1 Mbps. So the whole synchronization will be done less than a\n> 1-minute block confirmation.\n>  \n> According to CISCO mobile bandwidth connection speed increases 20% every\n> year. In four years, it will have doubled, so mobile phones with lower\n> than average data connection will soon be able to catchup.\n> Also there is low-hanging-fruit optimizations to the protocol that have\n> not been implemented: each header is 80 bytes in length. When a set of\n> chained headers is transferred, the headers could be compressed,\n> stripping 32 bytes of each header that is derived from the previous\n> header hash digest. So a 40% compression is already possible by slightly\n> modifying the wire protocol.\n>  \n> 7. There has been insufficient testing and/or insufficient research into\n> technical/economic implications or reducing the block rate\n>  \n> This is partially true. in the GHOST paper, this has been analyzed, and\n> the problem was shown to be solvable for block intervals of just a few\n> seconds. There are several proof-of-work cryptocurrencies in existence\n> that have lower than 1 minute block intervals and they work just fine.\n> First there was Bitcoin with a 10 minute interval, then was LiteCoin\n> using a 2.5 interval, then was DogeCoin with 1 minute, and then\n> QuarkCoin with just 30 seconds. Every new cryptocurrency lowers it a\n> little bit. Some time ago I decided to research on the block rate to\n> understand how the block interval impacts the stability and capability\n> of the cryptocurrency network, and I came up with the idea of the DECOR+\n> protocol [4] (which requires changes in the consensus code). In my\n> research I also showed how the stale rate can be easily reduced only\n> with changes in the networking code, and not in the consensus code.\n> These networking optimizations ( O(1) propagation using headers-first or\n> IBLTs), can be added later.\n>  \n> Mortifying Bitcoin to accommodate the change to lower the block rate\n> requires at least:\n>  \n> - Changing the 21 BTC reward per block to 2.1 BTC\n> - Changing the nPowTargetTimespan constant\n> - Writing code to hard-fork automatically when the majority of miners\n> have upgraded.\n> - Allow transaction version 3, and interpret nLockTimes of transaction\n> version 2 as being multiplied by 10.\n> \n> All changes comprises no more than 15 lines of code. This is much less\n> than the number of lines modified by Gavin's 20Mb patch.\n>  \n> As a conclusion, I haven't yet heard a good argument against lowering\n> the block rate.\n> \n> Best regards,\n>  Sergio.\n>  \n> [0] https://medium.com/@octskyward/the-capacity-cliff-586d1bf7715e <https://medium.com/@octskyward/the-capacity-cliff-586d1bf7715e>\n> [1] https://bitslog.wordpress.com/2014/02/17/5-sec-block-interval/ <https://bitslog.wordpress.com/2014/02/17/5-sec-block-interval/>\n> [2] http://gavinandresen.ninja/time-to-roll-out-bigger-blocks <http://gavinandresen.ninja/time-to-roll-out-bigger-blocks>\n> [3]\n> http://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/white_paper_c11-520862.html <http://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/white_paper_c11-520862.html>\n> [4] https://bitslog.wordpress.com/2014/05/02/decor/ <https://bitslog.wordpress.com/2014/05/02/decor/>\n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y <http://ad.doubleclick.net/ddm/clk/290420510;117567292;y>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development <https://lists.sourceforge.net/lists/listinfo/bitcoin-development>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y_______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/e26992a3/attachment.html>"
            },
            {
                "author": "insecurity at national.shitposting.agency",
                "date": "2015-05-11T08:50:20",
                "message_text_only": "> So if the server pushes new block\n> header candidates to clients, then the problem boils down to increasing\n> bandwidth of the servers to achieve a tenfold increase in work\n> distribution.\n\nMost Stratum pools already do multiple updates of the header every block \nperiod,\nbandwidth is really inconsequential, it's the latency that kills. At the \npresent\ntime you are looking up to 15 seconds between the first and last pools \nto push\nheaders to their clients for the latest block. It's sort of \ninconsequential with\na 10 minute block time, but it cuts into a 1 minute one very heavily.\n\nSome pools already don't do their own validation of blocks, but simply \nmirror\nother pools, pushing them to be even more latency focused will just make \nthis an\nepidemic of invalidity rather than a solution.\n\n\n> There are several proof-of-work cryptocurrencies in existence\n> that have lower than 1 minute block intervals and they work just fine.\n> First there was Bitcoin with a 10 minute interval, then was LiteCoin\n> using a 2.5 interval, then was DogeCoin with 1 minute, and then\n> QuarkCoin with just 30 seconds.\n\nYou can't really use these as examples of things going just fine. None \nof these\nnetworks see anything approaching the Bitcoin transaction volume and \nnone have\neven remotely the same network size. Some Bitcoin forks use floats in \nconsensus\ncritical code and work \"just fine\", for the moment. We can't justify \npoor\ndecisions with \"but the altcoins are doing it\".\n\nIs there even a single study of the stale rates within these networks?"
            },
            {
                "author": "Sergio Lerner",
                "date": "2015-05-12T18:55:05",
                "message_text_only": "On 11/05/2015 04:25 p.m., Leo Wandersleb wrote:\n> I assume that 1 minute block target will not get any substantial support but\n> just in case only few people speaking up might be taken as careful\nsupport of\n> the idea, here's my two cents:\n>\n> In mining, stale shares depend on delay between pool/network and the\nminer. This\n> varies substantially globally and as Peter Todd/Luke-Jr mentioned,\nspeed of\n> light will always keep those at a disadvantage that are 100 light\nmilli seconds\n> away from the creation of the last block. If anything, this warrants\nto increase\n> block target, not reduce. (The increase might wait until we have\nminers on Mars\n> though ;) )\n\nAn additional delay of 200 milliseconds means loosing approximately 0.3%\nof the revenue.\nDo you really think this is going to be the key factor to prevent a\nmining pool from being used?\nThere are lot of other factors, such as DoS protections, security,\nprivacy, variance, trust, algorithm to distribute shares, that are much\nmore important than that.\n\nAnd having a 1 minute block actually reduces the payout variance 10x, so\nminers will be happy for that. And many pool miners may opt to do solo\nmining, and create new full-nodes.\n\n>\n>\n> If SPV also becomes 10 times more traffic intensive, I can only urge\nyou to\n> travel to anything but central Europe or the USA.\nThe SPV traffic is minuscule. Bloom-filers are an ugly solution that\nincreases bandwidth and does not provide a real privacy solution.\nSmall improvements in the wire protocol can reduce the traffic two-fold.\n\n>\n>\n> I want bitcoin to be the currency for the other x billion and thus I\noppose any\n> change that moves the balance towards the economically upper billion.\nBecause having a 10 minute rate Bitcoin is a good Internet money. If you\nhave a 1 minute rate, then it can also be a retail payment method, an\nvirtual game trading payment method, a gambling, XXX-video renting \n(hey, it takes less than 10 minutes to see one of those :), and much more.\n\nYou can reach more billions by having near instant payments.\nDon't tell me about the morning caffe, I would like that everyone is\nbuying their coffe with Bitcoin and there are millions of users before\nwe figure out how to do that off-chain.\n\nBest regards,\n Sergio.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/7c99eb91/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Reducing the block rate instead of increasing the maximum block size",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Sergio Lerner",
                "Peter Todd",
                "Dave Hudson",
                "Luke Dashjr",
                "insecurity at national.shitposting.agency",
                "Thy Shizzle",
                "Christian Decker"
            ],
            "messages_count": 10,
            "total_messages_chars_count": 54949
        }
    },
    {
        "title": "[Bitcoin-development] Fwd:  Bitcoin core 0.11 planning",
        "thread_messages": [
            {
                "author": "Wladimir",
                "date": "2015-05-11T14:49:53",
                "message_text_only": "On Tue, Apr 28, 2015 at 11:01 AM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n> As softforks almost certainly require backports to older releases and other\n> software anyway, I don't think they should necessarily be bound to Bitcoin\n> Core major releases. If they don't require large code changes, we can easily\n> do them in minor releases too.\n\nAgree here - there is no need to time consensus changes with a major\nrelease, as they need to be ported back to older releases anyhow.\n(I don't really classify them as software features, but properties of\nthe underlying system that we need to adopt to)\n\nWladimir"
            }
        ],
        "thread_summary": {
            "title": "Fwd:  Bitcoin core 0.11 planning",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 614
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin core 0.11 planning",
        "thread_messages": [
            {
                "author": "Wladimir",
                "date": "2015-05-11T15:00:03",
                "message_text_only": "A reminder - feature freeze and string freeze is coming up this Friday the 15th.\n\nLet me know if your pull request is ready to be merged before then,\n\nWladimir\n\nOn Tue, Apr 28, 2015 at 7:44 AM, Wladimir J. van der Laan\n<laanwj at gmail.com> wrote:\n> Hello all,\n>\n> The release window for 0.11 is nearing, I'd propose the following schedule:\n>\n> 2015-05-01  Soft translation string freeze\n>             Open Transifex translations for 0.11\n>             Finalize and close translation for 0.9\n>\n> 2015-05-15  Feature freeze, string freeze\n>\n> 2015-06-01  Split off 0.11 branch\n>             Tag and release 0.11.0rc1\n>             Start merging for 0.12 on master branch\n>\n> 2015-07-01  Release 0.11.0 final (aim)\n>\n> In contrast to former releases, which were protracted for months, let's try to be more strict about the dates. Of course it is always possible for last-minute critical issues to interfere with the planning. The release will not be held up for features, though, and anything that will not make it to 0.11 will be postponed to next release scheduled for end of the year.\n>\n> Wladimir"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin core 0.11 planning",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1098
        }
    },
    {
        "title": "[Bitcoin-development] Long-term mining incentives",
        "thread_messages": [
            {
                "author": "Thomas Voegtlin",
                "date": "2015-05-11T16:28:46",
                "message_text_only": "The discussion on block size increase has brought some attention to the\nother elephant in the room: Long-term mining incentives.\n\nBitcoin derives its current market value from the assumption that a\nstable, steady-state regime will be reached in the future, where miners\nhave an incentive to keep mining to protect the network. Such a steady\nstate regime does not exist today, because miners get most of their\nreward from the block subsidy, which will progressively be removed.\n\nThus, today's 3 billion USD question is the following: Will a steady\nstate regime be reached in the future? Can such a regime exist? What are\nthe necessary conditions for its existence?\n\nSatoshi's paper suggests that this may be achieved through miner fees.\nQuite a few people seem to take this for granted, and are working to\nmake it happen (developing cpfp and replace-by-fee). This explains part\nof the opposition to raising the block size limit; some people would\nlike to see some fee pressure building up first, in order to get closer\nto a regime where miners are incentivised by transaction fees instead of\nblock subsidy. Indeed, the emergence of a working fee market would be\nextremely reassuring for the long-term viability of bitcoin. So, the\nthinking goes, by raising the block size limit, we would be postponing a\ncrucial reality check. We would be buying time, at the expenses of\nBitcoin's decentralization.\n\nOTOH, proponents of a block size increase have a very good point: if the\nblock size is not raised soon, Bitcoin is going to enter a new, unknown\nand potentially harmful regime. In the current regime, almost all\ntransaction get confirmed quickly, and fee pressure does not exist. Mike\nHearn suggested that, when blocks reach full capacity and users start to\nexperience confirmation delays and confirmation uncertainty, users will\nsimply go away and stop using Bitcoin. To me, that outcome sounds very\nplausible indeed. Thus, proponents of the block size increase are\nconservative; they are trying to preserve the current regime, which is\nknown to work, instead of letting the network enter uncharted territory.\n\nMy problem is that this seems to lacks a vision. If the maximal block\nsize is increased only to buy time, or because some people think that 7\ntps is not enough to compete with VISA, then I guess it would be\nhealthier to try and develop off-chain infrastructure first, such as the\nLightning network.\n\nOTOH, I also fail to see evidence that a limited block capacity will\nlead to a functional fee market, able to sustain a steady state. A\nfunctional market requires well-informed participants who make rational\nchoices and accept the outcomes of their choices. That is not the case\ntoday, and to believe that it will magically happen because blocks start\nto reach full capacity sounds a lot like like wishful thinking.\n\nSo here is my question, to both proponents and opponents of a block size\nincrease: What steady-state regime do you envision for Bitcoin, and what\nis is your plan to get there? More specifically, how will the\nsteady-state regime look like? Will users experience fee pressure and\ndelays, or will it look more like a scaled up version of what we enjoy\ntoday? Should fee pressure be increased jointly with subsidy decrease,\nor as soon as possible, or never? What incentives will exist for miners\nonce the subsidy is gone? Will miners have an incentive to permanently\nfork off the last block and capture its fees? Do you expect Bitcoin to\nwork because miners are altruistic/selfish/honest/caring?\n\nA clear vision would be welcome."
            },
            {
                "author": "insecurity at national.shitposting.agency",
                "date": "2015-05-11T16:52:10",
                "message_text_only": "On 2015-05-11 16:28, Thomas Voegtlin wrote:\n> My problem is that this seems to lacks a vision. If the maximal block\n> size is increased only to buy time, or because some people think that 7\n> tps is not enough to compete with VISA, then I guess it would be\n> healthier to try and develop off-chain infrastructure first, such as \n> the\n> Lightning network.\n\nIf your end goal is \"compete with VISA\" you might as well just give up\nand go home right now. There's lots of terrible proposals where people\ntry to demonstrate that so many hundred thousand transactions a second\nare possible if we just make the block size 500GB. In the real world\nwith physical limits, you literally can not verify more than a few\nthousand ECDSA signatures a second on a CPU core. The tradeoff taken\nin Bitcoin is that the signatures are pretty small, but they are also\nslow to verify on any sort of scale. There's no way competing with a\ncentralised entity using on-chain transactions is even a sane goal."
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-11T17:29:02",
                "message_text_only": "I think long-term the chain will not be secured purely by proof-of-work. I\nthink when the Bitcoin network was tiny running solely on people's home\ncomputers proof-of-work was the right way to secure the chain, and the only\nfair way to both secure the chain and distribute the coins.\n\nSee https://gist.github.com/gavinandresen/630d4a6c24ac6144482a  for some\nhalf-baked thoughts along those lines. I don't think proof-of-work is the\nlast word in distributed consensus (I also don't think any alternatives are\nanywhere near ready to deploy, but they might be in ten years).\n\nI also think it is premature to worry about what will happen in twenty or\nthirty years when the block subsidy is insignificant. A lot will happen in\nthe next twenty years. I could spin a vision of what will secure the chain\nin twenty years, but I'd put a low probability on that vision actually\nturning out to be correct.\n\nThat is why I keep saying Bitcoin is an experiment. But I also believe that\nthe incentives are correct, and there are a lot of very motivated, smart,\nhard-working people who will make it work. When you're talking about trying\nto predict what will happen decades from now, I think that is the best you\ncan (honestly) do.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/98eda946/attachment.html>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2015-05-12T12:35:02",
                "message_text_only": "Thank you for your answer.\n\nI agree that a lot of things will change, and I am not asking for a\nprediction of technological developments; prediction is certainly\nimpossible. What I would like to have is some sort of reference scenario\nfor the future of Bitcoin. Something a bit like the Standard Model in\nPhysics. The reference scenario should not be a prediction of the\nfuture, that's not the point. In fact, it will have to be updated\neverytime technological evolutions or code changes render it obsolete.\n\nHowever, the reference scenario should be a workable path through the\nfuture, using today's technologies and today's knowlegde, and including\nall planned code changes. It should be, as much as possible, amenable to\nquantitative analysis. It could be used to justify controversial\ndecisions such as a hard fork.\n\nYour proposal of a block size increase would be much stronger if it came\nwith such a scenario. It would show that you know where you are going.\n\n\n\nLe 11/05/2015 19:29, Gavin Andresen a \u00e9crit :\n> I think long-term the chain will not be secured purely by proof-of-work. I\n> think when the Bitcoin network was tiny running solely on people's home\n> computers proof-of-work was the right way to secure the chain, and the only\n> fair way to both secure the chain and distribute the coins.\n> \n> See https://gist.github.com/gavinandresen/630d4a6c24ac6144482a  for some\n> half-baked thoughts along those lines. I don't think proof-of-work is the\n> last word in distributed consensus (I also don't think any alternatives are\n> anywhere near ready to deploy, but they might be in ten years).\n> \n> I also think it is premature to worry about what will happen in twenty or\n> thirty years when the block subsidy is insignificant. A lot will happen in\n> the next twenty years. I could spin a vision of what will secure the chain\n> in twenty years, but I'd put a low probability on that vision actually\n> turning out to be correct.\n> \n> That is why I keep saying Bitcoin is an experiment. But I also believe that\n> the incentives are correct, and there are a lot of very motivated, smart,\n> hard-working people who will make it work. When you're talking about trying\n> to predict what will happen decades from now, I think that is the best you\n> can (honestly) do.\n>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-14T00:11:47",
                "message_text_only": "On Mon, May 11, 2015 at 7:29 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> I think long-term the chain will not be secured purely by proof-of-work. I\n> think when the Bitcoin network was tiny running solely on people's home\n> computers proof-of-work was the right way to secure the chain, and the only\n> fair way to both secure the chain and distribute the coins.\n>\n> See https://gist.github.com/gavinandresen/630d4a6c24ac6144482a  for some\n> half-baked thoughts along those lines. I don't think proof-of-work is the\n> last word in distributed consensus (I also don't think any alternatives are\n> anywhere near ready to deploy, but they might be in ten years).\n\nOr never, nobody knows at this point.\n\n> I also think it is premature to worry about what will happen in twenty or\n> thirty years when the block subsidy is insignificant. A lot will happen in\n> the next twenty years. I could spin a vision of what will secure the chain\n> in twenty years, but I'd put a low probability on that vision actually\n> turning out to be correct.\n\nI think is very healthy to worry about that since we know it's\nsomething that will happen.\nThe system should work without subsidies.\n\n> That is why I keep saying Bitcoin is an experiment. But I also believe that\n> the incentives are correct, and there are a lot of very motivated, smart,\n> hard-working people who will make it work. When you're talking about trying\n> to predict what will happen decades from now, I think that is the best you\n> can (honestly) do.\n\nLightning payment channels may be a new idea, but payment channels are\nnot, and nobody is using them.\nThey are the best solution to scalability we have right now,\nincreasing the block size is simply not a solution, it's just kicking\nthe can down the road (while reducing the incentives to deploy real\nsolutions like payment channels).\n\nNot worrying about 10 years in the future but asking people to trust\nestimates and speculations about how everything will burn in 2 years\nif we don't act right now seems pretty arbitrary to me.\nOne could just as well argue that there's smart hard-working people\nthat will solve those problems before they hit us.\n\nIt is true that the more distant the future you're trying to predict\nis, the more difficult it is to predict it, but any threshold that\nseparates \"relevant worries\" from \"too far in the future to worry\nabout it\" will always be arbitrary.\nFortunately we don't need to all share the same time horizon for what\nis worrying and what is not.\nWhat we need is a clear criterion for what is acceptable for a\nhardfork and a general plan to deploy them:\n\n-Do all the hardfork changes need to be uncontroversial? How do we\ndefine uncontroversial?\n-Should we maintain and test implementation of hardfork whises that\nseem too small to justify a hardfork on their own (ie time travel fix,\nallowing to sign inputs values...) to also deploy them at the same\ntime that other more necessary hardforks?\n\nI agree that hardforks shouldn't be impossible and in that sense I'm\nglad that you started the hardfork debate, but I believe we should be\nfocusing on that debate rather than the block size one.\nOnce we have a clear criteria, hopefully the block size debate should\nbecome less noisy and more productive."
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-14T00:48:41",
                "message_text_only": "> increasing the block size is simply not a solution, it's just kicking\n> the can down the road (while reducing the incentives to deploy real\n> solutions like payment channels).\n\nPlacing hard limits on blocksize is not the right solution. There are still\nplenty of options to be explored to increase fees, resulting in users\nvoluntarily economizing on block space. It's premature to resort to\ndestroying the reliability of propagated transaction getting into blocks.\n\nChild-pays-for-parent is useful, but requires the recipient to spend inputs\nupon receipt, consuming even more block space. Replace-by-fee may also\nhelp, but users won't know the fee they are getting charged until after the\nfact, and it will make worse all the problems that tx malleability causes\ntoday.\n\nWe have $3billion plus of value in this system to defend. The safe,\nconservative course is to increase the block size. Miners already have an\nincentive to find ways to encourage higher fees  and we can help them with\nstandard recommended propagation rules and hybrid priority/fee transaction\nselection for blocks that increases confirmation delays for low fee\ntransactions.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Wed, May 13, 2015 at 5:11 PM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> On Mon, May 11, 2015 at 7:29 PM, Gavin Andresen <gavinandresen at gmail.com>\n> wrote:\n> > I think long-term the chain will not be secured purely by proof-of-work.\n> I\n> > think when the Bitcoin network was tiny running solely on people's home\n> > computers proof-of-work was the right way to secure the chain, and the\n> only\n> > fair way to both secure the chain and distribute the coins.\n> >\n> > See https://gist.github.com/gavinandresen/630d4a6c24ac6144482a  for some\n> > half-baked thoughts along those lines. I don't think proof-of-work is the\n> > last word in distributed consensus (I also don't think any alternatives\n> are\n> > anywhere near ready to deploy, but they might be in ten years).\n>\n> Or never, nobody knows at this point.\n>\n> > I also think it is premature to worry about what will happen in twenty or\n> > thirty years when the block subsidy is insignificant. A lot will happen\n> in\n> > the next twenty years. I could spin a vision of what will secure the\n> chain\n> > in twenty years, but I'd put a low probability on that vision actually\n> > turning out to be correct.\n>\n> I think is very healthy to worry about that since we know it's\n> something that will happen.\n> The system should work without subsidies.\n>\n> > That is why I keep saying Bitcoin is an experiment. But I also believe\n> that\n> > the incentives are correct, and there are a lot of very motivated, smart,\n> > hard-working people who will make it work. When you're talking about\n> trying\n> > to predict what will happen decades from now, I think that is the best\n> you\n> > can (honestly) do.\n>\n> Lightning payment channels may be a new idea, but payment channels are\n> not, and nobody is using them.\n> They are the best solution to scalability we have right now,\n> increasing the block size is simply not a solution, it's just kicking\n> the can down the road (while reducing the incentives to deploy real\n> solutions like payment channels).\n>\n> Not worrying about 10 years in the future but asking people to trust\n> estimates and speculations about how everything will burn in 2 years\n> if we don't act right now seems pretty arbitrary to me.\n> One could just as well argue that there's smart hard-working people\n> that will solve those problems before they hit us.\n>\n> It is true that the more distant the future you're trying to predict\n> is, the more difficult it is to predict it, but any threshold that\n> separates \"relevant worries\" from \"too far in the future to worry\n> about it\" will always be arbitrary.\n> Fortunately we don't need to all share the same time horizon for what\n> is worrying and what is not.\n> What we need is a clear criterion for what is acceptable for a\n> hardfork and a general plan to deploy them:\n>\n> -Do all the hardfork changes need to be uncontroversial? How do we\n> define uncontroversial?\n> -Should we maintain and test implementation of hardfork whises that\n> seem too small to justify a hardfork on their own (ie time travel fix,\n> allowing to sign inputs values...) to also deploy them at the same\n> time that other more necessary hardforks?\n>\n> I agree that hardforks shouldn't be impossible and in that sense I'm\n> glad that you started the hardfork debate, but I believe we should be\n> focusing on that debate rather than the block size one.\n> Once we have a clear criteria, hopefully the block size debate should\n> become less noisy and more productive.\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/ded3ba9b/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-14T00:58:28",
                "message_text_only": "On Wed, May 13, 2015 at 5:48 PM, Aaron Voisine <voisine at gmail.com> wrote:\n\n> We have $3billion plus of value in this system to defend. The safe,\n> conservative course is to increase the block size. Miners already have an\n> incentive to find ways to encourage higher fees  and we can help them with\n> standard recommended propagation rules and hybrid priority/fee transaction\n> selection for blocks that increases confirmation delays for low fee\n> transactions.\n>\n\nYou may find that the most economical solution, but I can't understand how\nyou can call it conservative.\n\nSuggesting a hard fork is betting the survival of the entire ecosystem on\nthe bet that everyone will agree with and upgrade to new suggested software\nbefore a flag date.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/deca7b80/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-14T01:13:14",
                "message_text_only": "Conservative is a relative term. Dropping transactions in a way that is\nunpredictable to the sender sounds incredibly drastic to me. I'm suggesting\nincreasing the blocksize, drastic as it is, is the more conservative\nchoice. I would recommend that the fork take effect when some specific\nlarge supermajority of the pervious 1000 blocks indicate they have\nupgraded, as a safer alternative to a simple flag date, but I'm sure I\nwouldn't have to point out that option to people here.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Wed, May 13, 2015 at 5:58 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Wed, May 13, 2015 at 5:48 PM, Aaron Voisine <voisine at gmail.com> wrote:\n>\n>> We have $3billion plus of value in this system to defend. The safe,\n>> conservative course is to increase the block size. Miners already have an\n>> incentive to find ways to encourage higher fees  and we can help them with\n>> standard recommended propagation rules and hybrid priority/fee transaction\n>> selection for blocks that increases confirmation delays for low fee\n>> transactions.\n>>\n>\n> You may find that the most economical solution, but I can't understand how\n> you can call it conservative.\n>\n> Suggesting a hard fork is betting the survival of the entire ecosystem on\n> the bet that everyone will agree with and upgrade to new suggested software\n> before a flag date.\n>\n> --\n> Pieter\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/f4f8a27c/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-14T01:19:45",
                "message_text_only": "On Wed, May 13, 2015 at 6:13 PM, Aaron Voisine <voisine at gmail.com> wrote:\n\n> Conservative is a relative term. Dropping transactions in a way that is\n> unpredictable to the sender sounds incredibly drastic to me. I'm suggesting\n> increasing the blocksize, drastic as it is, is the more conservative choice.\n>\n\nTransactions are already being dropped, in a more indirect way: by people\nand businesses deciding to not use on-chain settlement. That is very sad,\nbut it's completely inevitable that there is space for some use cases and\nnot for others (at whatever block size). It's only a \"things don't fit\nanymore\" when you see on-chain transactions as the only means for doing\npayments, and that is already not the case. Increasing the block size\nallows for more utility on-chain, but it does not fundamentally add more\nuse cases - only more growth space for people already invested in being\nable to do things on-chain while externalizing the costs to others.\n\n\n> I would recommend that the fork take effect when some specific large\n> supermajority of the pervious 1000 blocks indicate they have upgraded, as a\n> safer alternative to a simple flag date, but I'm sure I wouldn't have to\n> point out that option to people here.\n>\n\nThat only measures miner adoption, which is the least relevant. The\nquestion is whether people using full nodes will upgrade. If they do, then\nminers are forced to upgrade too, or become irrelevant. If they don't, the\nupgrade is risky with or without miner adoption.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/e909c936/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-14T01:31:56",
                "message_text_only": "> by people and businesses deciding to not use on-chain settlement.\n\nI completely agree. Increasing fees will cause people voluntary economize\non blockspace by finding alternatives, i.e. not bitcoin. A fee however is a\nknown, upfront cost... unpredictable transaction failure in most cases will\nbe a far higher, unacceptable cost to the user than the actual fee. The\nhigher the costs of using the system, the lower the adoption as a\nstore-of-value. The lower the adoption as store-of-value, the lower the\nprice, and the lower the value of bitcoin to the world.\n\n> That only measures miner adoption, which is the least relevant.\n\nI concede the point. Perhaps a flag date based on previous observation of\nnetwork upgrade rates with a conservative additional margin in addition to\nsupermajority of mining power.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Wed, May 13, 2015 at 6:19 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Wed, May 13, 2015 at 6:13 PM, Aaron Voisine <voisine at gmail.com> wrote:\n>\n>> Conservative is a relative term. Dropping transactions in a way that is\n>> unpredictable to the sender sounds incredibly drastic to me. I'm suggesting\n>> increasing the blocksize, drastic as it is, is the more conservative choice.\n>>\n>\n> Transactions are already being dropped, in a more indirect way: by people\n> and businesses deciding to not use on-chain settlement. That is very sad,\n> but it's completely inevitable that there is space for some use cases and\n> not for others (at whatever block size). It's only a \"things don't fit\n> anymore\" when you see on-chain transactions as the only means for doing\n> payments, and that is already not the case. Increasing the block size\n> allows for more utility on-chain, but it does not fundamentally add more\n> use cases - only more growth space for people already invested in being\n> able to do things on-chain while externalizing the costs to others.\n>\n>\n>> I would recommend that the fork take effect when some specific large\n>> supermajority of the pervious 1000 blocks indicate they have upgraded, as a\n>> safer alternative to a simple flag date, but I'm sure I wouldn't have to\n>> point out that option to people here.\n>>\n>\n> That only measures miner adoption, which is the least relevant. The\n> question is whether people using full nodes will upgrade. If they do, then\n> miners are forced to upgrade too, or become irrelevant. If they don't, the\n> upgrade is risky with or without miner adoption.\n>\n> --\n> Pieter\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/0268ac4b/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-14T02:34:29",
                "message_text_only": "> I concede the point. Perhaps a flag date based on previous observation of\nnetwork upgrade rates with a conservative additional margin in addition to\nsupermajority of mining power.\n\nIt occurs to me that this would allow for a relatively small percentage of\nminers to stop the upgrade if the flag date turns out to be poorly chosen\nand a large number of non-mining nodes haven't upgraded yet. Would be a\nnice safety fallback.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet.com\n\nOn Wed, May 13, 2015 at 6:31 PM, Aaron Voisine <voisine at gmail.com> wrote:\n\n> > by people and businesses deciding to not use on-chain settlement.\n>\n> I completely agree. Increasing fees will cause people voluntary economize\n> on blockspace by finding alternatives, i.e. not bitcoin. A fee however is a\n> known, upfront cost... unpredictable transaction failure in most cases will\n> be a far higher, unacceptable cost to the user than the actual fee. The\n> higher the costs of using the system, the lower the adoption as a\n> store-of-value. The lower the adoption as store-of-value, the lower the\n> price, and the lower the value of bitcoin to the world.\n>\n> > That only measures miner adoption, which is the least relevant.\n>\n> I concede the point. Perhaps a flag date based on previous observation of\n> network upgrade rates with a conservative additional margin in addition to\n> supermajority of mining power.\n>\n>\n> Aaron Voisine\n> co-founder and CEO\n> breadwallet.com\n>\n> On Wed, May 13, 2015 at 6:19 PM, Pieter Wuille <pieter.wuille at gmail.com>\n> wrote:\n>\n>> On Wed, May 13, 2015 at 6:13 PM, Aaron Voisine <voisine at gmail.com> wrote:\n>>\n>>> Conservative is a relative term. Dropping transactions in a way that is\n>>> unpredictable to the sender sounds incredibly drastic to me. I'm suggesting\n>>> increasing the blocksize, drastic as it is, is the more conservative choice.\n>>>\n>>\n>> Transactions are already being dropped, in a more indirect way: by people\n>> and businesses deciding to not use on-chain settlement. That is very sad,\n>> but it's completely inevitable that there is space for some use cases and\n>> not for others (at whatever block size). It's only a \"things don't fit\n>> anymore\" when you see on-chain transactions as the only means for doing\n>> payments, and that is already not the case. Increasing the block size\n>> allows for more utility on-chain, but it does not fundamentally add more\n>> use cases - only more growth space for people already invested in being\n>> able to do things on-chain while externalizing the costs to others.\n>>\n>>\n>>> I would recommend that the fork take effect when some specific large\n>>> supermajority of the pervious 1000 blocks indicate they have upgraded, as a\n>>> safer alternative to a simple flag date, but I'm sure I wouldn't have to\n>>> point out that option to people here.\n>>>\n>>\n>> That only measures miner adoption, which is the least relevant. The\n>> question is whether people using full nodes will upgrade. If they do, then\n>> miners are forced to upgrade too, or become irrelevant. If they don't, the\n>> upgrade is risky with or without miner adoption.\n>>\n>> --\n>> Pieter\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/92f0f8d2/attachment.html>"
            },
            {
                "author": "Owen Gunden",
                "date": "2015-05-16T20:35:05",
                "message_text_only": "On 05/13/2015 09:31 PM, Aaron Voisine wrote:\n>  > by people and businesses deciding to not use on-chain settlement.\n>\n> I completely agree. Increasing fees will cause people voluntary\n> economize on blockspace by finding alternatives, i.e. not bitcoin.\n\nThis strikes me as a leap. There are alternatives that still use bitcoin \nas the unit of value, such as sidechains, offchain, etc. To say that \nthese are \"not bitcoin\" is misleading.\n\n> A fee however is a known, upfront cost... unpredictable transaction failure in\n> most cases will be a far higher, unacceptable cost to the user than the\n> actual fee.\n\nAre we sure that raising the block size is the only way to avoid \n\"unpredictable transaction failure\"? If so, and it's as bad as you say \nit is, aren't we screwed anyway when we inevitably start hitting the cap \n(even if it's raised 10x or 20x)? And if that's the case, then don't we \ndo a disservice to users by continuing to pretend that we can make this \nproblem go away?\n\n> The higher the costs of using the system, the lower the\n> adoption as a store-of-value.\n\nOn what do you base this? Gold has a very high cost of using (storage, \ntransport) and yet is perhaps the most widely accepted store of value."
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-16T22:18:59",
                "message_text_only": "On 5/16/2015 1:35 PM, Owen Gunden wrote:\n> There are alternatives that still use bitcoin as the unit of value,\n> such as sidechains, offchain, etc. To say that these are \"not bitcoin\"\n> is misleading.\n\n\nIs it?  Nobody thinks \"euro accepted\" implies Visa is ok, even though\nVisa is just a bunch of extra protocol surrounding an eventual bank deposit."
            },
            {
                "author": "Aaron Voisine",
                "date": "2015-05-17T01:08:16",
                "message_text_only": "On Sat, May 16, 2015 at 1:35 PM, Owen Gunden <ogunden at phauna.org> wrote:\n\n>\n> This strikes me as a leap. There are alternatives that still use bitcoin\n> as the unit of value, such as sidechains, offchain, etc. To say that\n> these are \"not bitcoin\" is misleading.\n>\n\nThe only options available today and in the near future that I'm aware of\nare of the centralized custody variety, which is pretty bad in my opinion,\nbut your point is taken.\n\n\n>\n> Are we sure that raising the block size is the only way to avoid\n> \"unpredictable transaction failure\"? If so, and it's as bad as you say\n> it is, aren't we screwed anyway when we inevitably start hitting the cap\n> (even if it's raised 10x or 20x)? And if that's the case, then don't we\n> do a disservice to users by continuing to pretend that we can make this\n> problem go away?\n>\n\nWhen we start bumping up against the block size limit, the transactions at\nthe margins will experience failure in a way that will be unpredictable to\ncurrent wallet software. We can slow blockchain growth by increasing fees\nalone, without introducing the additional cost of unpredictability around\nconfirmation failure, which when it comes down to it is just another\n(extreme) way to keep usage low. Instead of fees and unpredictable\nconfirmation, why not just have fees alone. A single, upfront, known cost.\n\n\n>\n>\n> On what do you base this? Gold has a very high cost of using (storage,\n> transport) and yet is perhaps the most widely accepted store of value.\n>\n\nI would argue that the reason gold is not the one world global currency\nthat it once was is because of those costs. That's why people shifted over\ntime to gold backed bank notes and eventually fiat.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150516/b3aa6233/attachment.html>"
            },
            {
                "author": "Melvin Carvalho",
                "date": "2015-05-14T00:44:01",
                "message_text_only": "On 11 May 2015 at 18:28, Thomas Voegtlin <thomasv at electrum.org> wrote:\n\n> The discussion on block size increase has brought some attention to the\n> other elephant in the room: Long-term mining incentives.\n>\n> Bitcoin derives its current market value from the assumption that a\n> stable, steady-state regime will be reached in the future, where miners\n> have an incentive to keep mining to protect the network. Such a steady\n> state regime does not exist today, because miners get most of their\n> reward from the block subsidy, which will progressively be removed.\n>\n> Thus, today's 3 billion USD question is the following: Will a steady\n> state regime be reached in the future? Can such a regime exist? What are\n> the necessary conditions for its existence?\n>\n> Satoshi's paper suggests that this may be achieved through miner fees.\n> Quite a few people seem to take this for granted, and are working to\n> make it happen (developing cpfp and replace-by-fee). This explains part\n> of the opposition to raising the block size limit; some people would\n> like to see some fee pressure building up first, in order to get closer\n> to a regime where miners are incentivised by transaction fees instead of\n> block subsidy. Indeed, the emergence of a working fee market would be\n> extremely reassuring for the long-term viability of bitcoin. So, the\n> thinking goes, by raising the block size limit, we would be postponing a\n> crucial reality check. We would be buying time, at the expenses of\n> Bitcoin's decentralization.\n>\n> OTOH, proponents of a block size increase have a very good point: if the\n> block size is not raised soon, Bitcoin is going to enter a new, unknown\n> and potentially harmful regime. In the current regime, almost all\n> transaction get confirmed quickly, and fee pressure does not exist. Mike\n> Hearn suggested that, when blocks reach full capacity and users start to\n> experience confirmation delays and confirmation uncertainty, users will\n> simply go away and stop using Bitcoin. To me, that outcome sounds very\n> plausible indeed. Thus, proponents of the block size increase are\n> conservative; they are trying to preserve the current regime, which is\n> known to work, instead of letting the network enter uncharted territory.\n>\n> My problem is that this seems to lacks a vision. If the maximal block\n> size is increased only to buy time, or because some people think that 7\n> tps is not enough to compete with VISA, then I guess it would be\n> healthier to try and develop off-chain infrastructure first, such as the\n> Lightning network.\n>\n> OTOH, I also fail to see evidence that a limited block capacity will\n> lead to a functional fee market, able to sustain a steady state. A\n> functional market requires well-informed participants who make rational\n> choices and accept the outcomes of their choices. That is not the case\n> today, and to believe that it will magically happen because blocks start\n> to reach full capacity sounds a lot like like wishful thinking.\n>\n> So here is my question, to both proponents and opponents of a block size\n> increase: What steady-state regime do you envision for Bitcoin, and what\n> is is your plan to get there? More specifically, how will the\n> steady-state regime look like? Will users experience fee pressure and\n> delays, or will it look more like a scaled up version of what we enjoy\n> today? Should fee pressure be increased jointly with subsidy decrease,\n> or as soon as possible, or never? What incentives will exist for miners\n> once the subsidy is gone? Will miners have an incentive to permanently\n> fork off the last block and capture its fees? Do you expect Bitcoin to\n> work because miners are altruistic/selfish/honest/caring?\n>\n> A clear vision would be welcome.\n>\n\nI am guided here by Satoshi's paper:\n\n\"Commerce on the Internet has come to rely almost exclusively on financial\ninstitutions serving as trusted third parties to process electronic\npayments. While the system works well enough for *most transactions*\"\n\nThis suggests to me that most tx will occur off-block with the block chain\nused for settlement.  Indeed Satoshi was working on a trust based market\nbefore he left.\n\nIf commerce works well enough off-block with zero trust settlement\nsupporting it, people might even forget that the block chain exists, like\nwith gold settlement.  But it can be used for transactions.  To this end I\nwelcome higher fees, so that the block chain becomes the reserve currency\nof the internet and is used sparingly.\n\nBut as Gavin pointed out, bitcoin is still an experiment and we are all\nstill learning.  We are also learning from alt coin mechanisms.  I am\nunsure there is huge urgency here, and would lean towards caution as\nbitcoin infrastructure rapidly grows.\n\n\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150514/14a689aa/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T18:31:12",
                "message_text_only": "Hi Thomas,\n\nMy problem is that this seems to lacks a vision.\n>\n\nAre you aware of my proposal for network assurance contracts?\n\nThere is a discussion here:\n\n\nhttps://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg07552.html\n\nBut I agree with Gavin that attempting to plan for 20 years from now is\nambitious at best. Bitcoin might not even exist 20 years from now, or might\nbe an abandoned backwater a la USENET.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/3c45d04b/attachment.html>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2015-05-26T18:47:15",
                "message_text_only": "Hello Mike,\n\n> \n> Are you aware of my proposal for network assurance contracts?\n> \n\nYes I am aware of that; sorry for not mentioning it. I think it is an\ninteresting proposal, but I would not rely on it today, because it\nincludes a large share of unproven social experiment.\n\n(Bitcoin too is a social experiment, but so far it has been working)\n\n\n> But I agree with Gavin that attempting to plan for 20 years from now is\n> ambitious at best. Bitcoin might not even exist 20 years from now, or might\n> be an abandoned backwater a la USENET.\n\nI agree with that, but I don't think it can be used as a way to justify\nhow decisions are made today.\n\nThe opposition to block size increase comes from two things:\n(1) The perceived risk of increased centralization.\n(2) Long-term considerations on the need for fee pressure.\n\nI believe you and Gavin have properly addressed (1). Concerning (2), I\nthink the belief that miners can eventually be funded by a fee market is\nwishful thinking. Thus, I am not against the proposed block size increase.\n\nHowever, the issue of long-term mining incentives remains. So far, the\nonly proven method to incentivize mining has been direct block reward.\n\nThe easiest solution to ensure long-term viability of Bitcoin would be\nto put an end to the original block halving schedule, and to keep the\nblock reward constant (this is what Monero does, btw). That solution is\ninflationary, but in practice, users happen to lose private keys all the\ntime. The rate of coins loss would eventually converge to whatever rate\nof emission is chosen, because the care people take of their coins\ndepends on their value.\n\nAnother solution, that does not break Bitcoin's social contract, would\nbe to keep the original block halving schedule, but to allow miners to\nalso redeem lost coins (defined as: coins that have not moved for a\nfixed number of years. Some time averaging of the lost coins may be\nneeded in order to prevent non-productive miner strategies). That\nsolution would create less uncertainty on the actual money supply, and\nbetter acceptability.\n\nI do not expect such a solution to be adopted before miner incentives\nbecome a problem. Neither am I attempting to predict the future; a\ncompletely different solution might be found before the problem arises,\nor Bitcoin might stop to exist for some other reason.\n\nHowever, if I had to decide today, I would choose such a solution,\ninstead of relying on completely unproven mechanisms.\n\nMore important, since we need to decide about block size today, I want\nto make it clear that my support is motivated by that long-term\npossibility. I believe that the \"we will need fee pressure\" argument can\nreasonably be dismissed, not because we don't know how Bitcoin will work\nin 20 years, but because we know how it works today, and it is not\nthanks to fee pressure.\n\nThomas"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-27T21:59:02",
                "message_text_only": "I wrote an article that explains the hashing assurance contract concept:\n\nhttps://medium.com/@octskyward/hashing-7d04a887acc8\n\n(it doesn't contain an in depth protocol description)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/2c8684cb/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-27T22:22:48",
                "message_text_only": "On Wed, May 27, 2015 at 9:59 PM, Mike Hearn <mike at plan99.net> wrote:\n> I wrote an article that explains the hashing assurance contract concept:\n>\n> https://medium.com/@octskyward/hashing-7d04a887acc8\n>\n> (it doesn't contain an in depth protocol description)\n\nThe prior (and seemingly this) assurance contract proposals pay the\nminers who mines a chain supportive of your interests and miners whom\nmine against your interests identically.\n\nThere is already a mechanism built into Bitcoin for paying for\nsecurity which doesn't have this problem, and which mitigates the\ncommon action problem of people just sitting around for other people\nto pay for security: transaction fees. Fixing the problem with\nassurance contracts effectively makes them end up working like\ntransaction fees in any case.  Considering the near-failure in just\nkeeping development funded, I'm not sure where the believe this this\nmodel will be workable comes from; in particular unlike a lighthouse\n(but like development) security is ongoing and not primarily a fixed\none time cost. I note that many existing crowdfunding platforms\n(including your own) do not do ongoing costs with this kind of binary\ncontract.\n\nAlso work reminding people that mining per-contract is a long\nidentified existential risk to Bitcoin which has been seeing more\nanalysis lately:\nhttp://www.jbonneau.com/doc/BFGKN14-bitcoin_bribery.pdf"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-28T10:30:55",
                "message_text_only": ">\n> The prior (and seemingly this) assurance contract proposals pay the\n> miners who mines a chain supportive of your interests and miners whom\n> mine against your interests identically.\n>\n\nThe same is true today - via inflation I pay for blocks regardless of\nwhether they contain or double spend my transactions or not. So I don't see\nwhy it'd be different in future.\n\n\n> There is already a mechanism built into Bitcoin for paying for\n> security which doesn't have this problem, and which mitigates the\n> common action problem of people just sitting around for other people\n> to pay for security: transaction fees.\n\n\nThe article states quite clearly that assurance contracts are proposed only\nif people setting transaction fees themselves doesn't work. There's some\nreasonably good arguments that it probably won't work, but I don't assign\nvery high weight to game theoretic arguments these days so it wouldn't\nsurprise me if Satoshi's original plan worked out OK too.\n\nOf course, by the time this matters I plan to be sipping a pina colada on\nmy private retirement beach :) It's a problem the next generation can\ntackle, as far as I am concerned.\n\n\n> Considering the near-failure in just keeping development funded, I'm not\n> sure where the believe this this model will be workable comes from\n\n\nPatience :)\n\nRight now it's a lot easier to get development money from VC funds and rich\nbenefactors than raising it directly from the community, so unsurprisingly\nthat's what most people do.\n\nDespite that, the Hourglass design document project now has sufficient\npre-pledges that it should be possible to crowdfund it successfully once I\nget around to actually doing the work. And BitSquare was able to raise\nnearly half of their target despite an incredibly aggressive deadline and\nthe fact that they hadn't shipped a usable prototype. I think as people get\nbetter at crafting their contracts and people get more experience with\nfunding work this way, we'll see it get more common.\n\nBut yes. Paying for things via assurance contracts is a long term and very\nexperimental plan, for sure.\n\n\n> one time cost. I note that many existing crowdfunding platforms\n> (including your own) do not do ongoing costs with this kind of binary\n> contract.\n>\n\nLighthouse wasn't written to do hashing assurance contracts, so no, it\ndoesn't have such a feature. Perhaps in version 2.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/c7d06ab0/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-12T16:10:53",
                "message_text_only": "Added back the list, I didn't mean to reply privately:\n\nFair enough, I'll try to find time in the next month or three to write up\nfour plausible future scenarios for how mining incentives might work:\n\n1) Fee-supported with very large blocks containing lots of tiny-fee\ntransactions\n2) Proof-of-idle supported (I wish Tadge Dryja would publish his\nproof-of-idle idea....)\n3) Fees purely as transaction-spam-prevention measure, chain security via\nalternative consensus algorithm (in this scenario there is very little\nmining).\n4) Fee supported with small blocks containing high-fee transactions moving\ncoins to/from sidechains.\n\nWould that be helpful, or do you have some reason for thinking that we\nshould pick just one and focus all of our efforts on making that one\nscenario happen?\n\nI always think it is better, when possible, not to \"bet on one horse.\"\n\n\nOn Tue, May 12, 2015 at 10:39 AM, Thomas Voegtlin <thomasv at electrum.org>\nwrote:\n\n> Le 12/05/2015 15:44, Gavin Andresen a \u00e9crit :\n> > Ok, here's my scenario:\n> >\n> > https://blog.bitcoinfoundation.org/a-scalability-roadmap/\n> >\n> > It might be wrong. I welcome other people to present their road maps.\n> >\n>\n> [answering to you only because you answered to me and not to the list;\n> feel free to repost this to the list though]\n>\n> Yes, that's exactly the kind of roadmap I am asking for. But your blog\n> post does not say anything about long term mining incentives, it only\n> talks about scalability. My point is that we need the same kind of thing\n> for miners incentives.\n>\n\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/3ab5aa3d/attachment.html>"
            },
            {
                "author": "Dave Hudson",
                "date": "2015-05-12T16:21:40",
                "message_text_only": "I think proof-of-idle had a potentially serious problem when I last looked at it. The risk is that a largish miner can use everyone else's idle time to construct a very long chain; it's also easy enough for them to make it appear to be the work of a large number of distinct miners. Given that this would allow them to arbitrarily re-mine any block rewards and potentially censor any transactions then that just seems like a huge security hole?\n\n\nCheers,\nDave\n\n\n> On 12 May 2015, at 17:10, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> \n> Added back the list, I didn't mean to reply privately:\n> \n> Fair enough, I'll try to find time in the next month or three to write up four plausible future scenarios for how mining incentives might work:\n> \n> 1) Fee-supported with very large blocks containing lots of tiny-fee transactions\n> 2) Proof-of-idle supported (I wish Tadge Dryja would publish his proof-of-idle idea....)\n> 3) Fees purely as transaction-spam-prevention measure, chain security via alternative consensus algorithm (in this scenario there is very little mining).\n> 4) Fee supported with small blocks containing high-fee transactions moving coins to/from sidechains.\n> \n> Would that be helpful, or do you have some reason for thinking that we should pick just one and focus all of our efforts on making that one scenario happen?\n> \n> I always think it is better, when possible, not to \"bet on one horse.\"\n> \n> \n> On Tue, May 12, 2015 at 10:39 AM, Thomas Voegtlin <thomasv at electrum.org <mailto:thomasv at electrum.org>> wrote:\n> Le 12/05/2015 15:44, Gavin Andresen a \u00e9crit :\n> > Ok, here's my scenario:\n> >\n> > https://blog.bitcoinfoundation.org/a-scalability-roadmap/ <https://blog.bitcoinfoundation.org/a-scalability-roadmap/>\n> >\n> > It might be wrong. I welcome other people to present their road maps.\n> >\n> \n> [answering to you only because you answered to me and not to the list;\n> feel free to repost this to the list though]\n> \n> Yes, that's exactly the kind of roadmap I am asking for. But your blog\n> post does not say anything about long term mining incentives, it only\n> talks about scalability. My point is that we need the same kind of thing\n> for miners incentives.\n> \n> \n> \n> -- \n> --\n> Gavin Andresen\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y_______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/73ba86e4/attachment.html>"
            },
            {
                "author": "Pedro Worcel",
                "date": "2015-05-12T21:24:41",
                "message_text_only": "Disclaimer: I don't know anything about Bitcoin.\n\n> \u200b2) Proof-of-idle supported (I wish Tadge Dryja would publish his\nproof-of-idle idea....)\n> 3) Fees purely as transaction-spam-prevention measure, chain security via\nalternative consensus algorithm (in this scenario there is very little\nmining).\n\nI don't understand why you would casually mention moving away from Proof of\nWork, I thought that was the big breakthrough that made Bitcoin possible at\nall?\n\nThanks,\nPedro\n\n2015-05-13 4:10 GMT+12:00 Gavin Andresen <gavinandresen at gmail.com>:\n\n> Added back the list, I didn't mean to reply privately:\n>\n> Fair enough, I'll try to find time in the next month or three to write up\n> four plausible future scenarios for how mining incentives might work:\n>\n> 1) Fee-supported with very large blocks containing lots of tiny-fee\n> transactions\n> \u200b\u200b\n> 2) Proof-of-idle supported (I wish Tadge Dryja would publish his\n> proof-of-idle idea....)\n> 3) Fees purely as transaction-spam-prevention measure, chain security via\n> alternative consensus algorithm (in this scenario there is very little\n> mining).\n> 4) Fee supported with small blocks containing high-fee transactions moving\n> coins to/from sidechains.\n>\n> Would that be helpful, or do you have some reason for thinking that we\n> should pick just one and focus all of our efforts on making that one\n> scenario happen?\n>\n> I always think it is better, when possible, not to \"bet on one horse.\"\n>\n>\n> On Tue, May 12, 2015 at 10:39 AM, Thomas Voegtlin <thomasv at electrum.org>\n> wrote:\n>\n>> Le 12/05/2015 15:44, Gavin Andresen a \u00e9crit :\n>> > Ok, here's my scenario:\n>> >\n>> > https://blog.bitcoinfoundation.org/a-scalability-roadmap/\n>> >\n>> > It might be wrong. I welcome other people to present their road maps.\n>> >\n>>\n>> [answering to you only because you answered to me and not to the list;\n>> feel free to repost this to the list though]\n>>\n>> Yes, that's exactly the kind of roadmap I am asking for. But your blog\n>> post does not say anything about long term mining incentives, it only\n>> talks about scalability. My point is that we need the same kind of thing\n>> for miners incentives.\n>>\n>\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/018ff86d/attachment.html>"
            },
            {
                "author": "Adam Back",
                "date": "2015-05-12T23:48:06",
                "message_text_only": "I think its fair to say no one knows how to make a consensus that\nworks in a decentralised fashion that doesnt weaken the bitcoin\nsecurity model without proof-of-work for now.\n\nI am presuming Gavin is just saying in the context of not pre-judging\nthe future that maybe in the far future another innovation might be\nfound (or alternatively maybe its not mathematically possible).\n\nTowards that it would be useful to try further to prove this one way\nor another (prove that proof of stake cant work if that is generically\nmathematically provable).\n\nAdam\n\nOn 12 May 2015 at 14:24, Pedro Worcel <pedro at worcel.com> wrote:\n> Disclaimer: I don't know anything about Bitcoin.\n>\n>> 2) Proof-of-idle supported (I wish Tadge Dryja would publish his\n>> proof-of-idle idea....)\n>> 3) Fees purely as transaction-spam-prevention measure, chain security via\n>> alternative consensus algorithm (in this scenario there is very little\n>> mining).\n>\n> I don't understand why you would casually mention moving away from Proof of\n> Work, I thought that was the big breakthrough that made Bitcoin possible at\n> all?\n>\n> Thanks,\n> Pedro\n>\n> 2015-05-13 4:10 GMT+12:00 Gavin Andresen <gavinandresen at gmail.com>:\n>>\n>> Added back the list, I didn't mean to reply privately:\n>>\n>> Fair enough, I'll try to find time in the next month or three to write up\n>> four plausible future scenarios for how mining incentives might work:\n>>\n>> 1) Fee-supported with very large blocks containing lots of tiny-fee\n>> transactions\n>> 2) Proof-of-idle supported (I wish Tadge Dryja would publish his\n>> proof-of-idle idea....)\n>> 3) Fees purely as transaction-spam-prevention measure, chain security via\n>> alternative consensus algorithm (in this scenario there is very little\n>> mining).\n>> 4) Fee supported with small blocks containing high-fee transactions moving\n>> coins to/from sidechains.\n>>\n>> Would that be helpful, or do you have some reason for thinking that we\n>> should pick just one and focus all of our efforts on making that one\n>> scenario happen?\n>>\n>> I always think it is better, when possible, not to \"bet on one horse.\"\n>>\n>>\n>> On Tue, May 12, 2015 at 10:39 AM, Thomas Voegtlin <thomasv at electrum.org>\n>> wrote:\n>>>\n>>> Le 12/05/2015 15:44, Gavin Andresen a \u00e9crit :\n>>> > Ok, here's my scenario:\n>>> >\n>>> > https://blog.bitcoinfoundation.org/a-scalability-roadmap/\n>>> >\n>>> > It might be wrong. I welcome other people to present their road maps.\n>>> >\n>>>\n>>> [answering to you only because you answered to me and not to the list;\n>>> feel free to repost this to the list though]\n>>>\n>>> Yes, that's exactly the kind of roadmap I am asking for. But your blog\n>>> post does not say anything about long term mining incentives, it only\n>>> talks about scalability. My point is that we need the same kind of thing\n>>> for miners incentives.\n>>\n>>\n>>\n>>\n>> --\n>> --\n>> Gavin Andresen\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-13T15:41:16",
                "message_text_only": "On Tue, May 12, 2015 at 7:48 PM, Adam Back <adam at cypherspace.org> wrote:\n\n> I think its fair to say no one knows how to make a consensus that\n> works in a decentralised fashion that doesnt weaken the bitcoin\n> security model without proof-of-work for now.\n>\n\nYes.\n\n\n> I am presuming Gavin is just saying in the context of not pre-judging\n> the future that maybe in the far future another innovation might be\n> found (or alternatively maybe its not mathematically possible).\n>\n\nYes... or an alternative might be found that weakens the Bitcoin security\nmodel by a small enough amount that it either doesn't matter or the\nweakening is vastly overwhelmed by some other benefit.\n\nI'm influenced by the way the Internet works; packets addressed to\n74.125.226.67 reliably get to Google through a very decentralized system\nthat I'll freely admit I don't understand. Yes, a determined attacker can\nre-route packets, but layers of security on top means re-routing packets\nisn't enough to pull off profitable attacks.\n\nI think Bitcoin's proof-of-work might evolve in a similar way. Yes, you\nmight be able to 51% attack the POW, but layers of security on top of POW\nwill mean that won't be enough to pull off profitable attacks.\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/1cad34e9/attachment.html>"
            },
            {
                "author": "Pedro Worcel",
                "date": "2015-05-13T20:05:55",
                "message_text_only": "Thank you for your response, that does make sense. It's going to be\ninteresting to follow what is going to happen!\n\n2015-05-14 3:41 GMT+12:00 Gavin Andresen <gavinandresen at gmail.com>:\n\n> On Tue, May 12, 2015 at 7:48 PM, Adam Back <adam at cypherspace.org> wrote:\n>\n>> I think its fair to say no one knows how to make a consensus that\n>> works in a decentralised fashion that doesnt weaken the bitcoin\n>> security model without proof-of-work for now.\n>>\n>\n> Yes.\n>\n>\n>> I am presuming Gavin is just saying in the context of not pre-judging\n>> the future that maybe in the far future another innovation might be\n>> found (or alternatively maybe its not mathematically possible).\n>>\n>\n> Yes... or an alternative might be found that weakens the Bitcoin security\n> model by a small enough amount that it either doesn't matter or the\n> weakening is vastly overwhelmed by some other benefit.\n>\n> I'm influenced by the way the Internet works; packets addressed to\n> 74.125.226.67 reliably get to Google through a very decentralized system\n> that I'll freely admit I don't understand. Yes, a determined attacker can\n> re-route packets, but layers of security on top means re-routing packets\n> isn't enough to pull off profitable attacks.\n>\n> I think Bitcoin's proof-of-work might evolve in a similar way. Yes, you\n> might be able to 51% attack the POW, but layers of security on top of POW\n> will mean that won't be enough to pull off profitable attacks.\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150514/0556dc6c/attachment.html>"
            },
            {
                "author": "Thomas Voegtlin",
                "date": "2015-05-13T09:49:13",
                "message_text_only": "Le 12/05/2015 18:10, Gavin Andresen a \u00e9crit :\n> Added back the list, I didn't mean to reply privately:\n> \n> Fair enough, I'll try to find time in the next month or three to write up\n> four plausible future scenarios for how mining incentives might work:\n> \n> 1) Fee-supported with very large blocks containing lots of tiny-fee\n> transactions\n> 2) Proof-of-idle supported (I wish Tadge Dryja would publish his\n> proof-of-idle idea....)\n> 3) Fees purely as transaction-spam-prevention measure, chain security via\n> alternative consensus algorithm (in this scenario there is very little\n> mining).\n> 4) Fee supported with small blocks containing high-fee transactions moving\n> coins to/from sidechains.\n> \n> Would that be helpful, or do you have some reason for thinking that we\n> should pick just one and focus all of our efforts on making that one\n> scenario happen?\n> \n> I always think it is better, when possible, not to \"bet on one horse.\"\n> \n\nSorry if I did not make myself clear. It is not about betting on one\nsingle horse, or about making one particular scenario happen. It is not\nabout predicting whether something else will replace PoW in the future,\nand I am in no way asking you to focus your efforts in one particular\ndirection at the expenses of others. Various directions will be explored\nby various people, and that's great.\n\nI am talking about what we know today. I would like an answer to the\nfollowing question: Do we have a reason to believe that Bitcoin can work\nin the long run, without involving technologies that have not been\ninvented yet? Is there a single scenario that we know could work?\n\nExotic and unproven technologies are not an answer to that question. The\nreference scenario should be as boring as possible, and as verifiable as\npossible. I am not asking what you think is the most likely to happen,\nbut what is the most likely to work, given the knowledge we have today.\n\nIf I was asking: \"Can we send humans to the moon by 2100?\", I guess your\nanswer would be: \"Yes we can, because it has been done in the past with\nchemical rockets, and we know how to build them\". You would probably not\nuse a space elevator in your answer.\n\nThe reason I am asking that is, there seems to be no consensus among\ncore developers on how Bitcoin can work without miner subsidy. How it\n*will* work is another question."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T10:14:06",
                "message_text_only": "On Wed, May 13, 2015 at 10:49 AM, Thomas Voegtlin <thomasv at electrum.org>\nwrote:\n\n>\n> The reason I am asking that is, there seems to be no consensus among\n> core developers on how Bitcoin can work without miner subsidy. How it\n> *will* work is another question.\n>\n\nThe position seems to be that it will continue to work for the time being,\nso there is still time for more research.\n\nProof of stake has problems with handling long term reversals.  The main\nproposal is to slightly weaken the security requirements.\n\nWith POW, a new node only needs to know the genesis block (and network\nrules) to fully determine which of two chains is the strongest.\n\nPenalties for abusing POS inherently create a time horizon.  A suggested\nPOS security model would assume that a full node is a node that resyncs\nwith the network regularly (every N blocks).    N would be depend on the\nnetwork rules of the coin.\n\nThe alternative is that 51% of the holders of coins at the genesis block\ncan rewrite the entire chain.  The genesis block might not be the first\nblock, a POS coin might still use POW for minting.\n\nhttps://blog.ethereum.org/2014/11/25/proof-stake-learned-love-weak-subjectivity/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/d8d2e855/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-13T10:31:47",
                "message_text_only": "> With POW, a new node only needs to know the genesis block (and network\n> rules) to fully determine which of two chains is the strongest.\n>\n\nBut this matters if a new node has access to the globally strongest chain.\nIf attacker is able to block connections to legitimate nodes, a new node\nwill happily accept attacker's chain.\n\nSo PoW, by itself, doesn't give strong security guarantees. This problem is\nso fundamental people avoid talking about it.\n\nIn practice, Bitcoin already embraces \"weak subjectivity\" e.g. in form of\ncheckpoints embedded into the source code. So it's hard to take PoW purists\nseriously.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/39177196/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T11:29:23",
                "message_text_only": "On Wed, May 13, 2015 at 11:31 AM, Alex Mizrahi <alex.mizrahi at gmail.com>\nwrote:\n\n>\n> But this matters if a new node has access to the globally strongest chain.\n>\n\nA node only needs a path of honest nodes to the network.\n\nIf a node is connected to 99 dishonest nodes and 1 honest node, it can\nstill sync with the main network.\n\n>\n> In practice, Bitcoin already embraces \"weak subjectivity\" e.g. in form of\n> checkpoints embedded into the source code. So it's hard to take PoW purists\n> seriously.\n>\n>\nThat isn't why checkpoints exist.  They are to prevent a disk consumption\nDOS attack.\n\nThey also allow verification to go faster.  Signature operations are\nassumed to be correct without checking if they are in blocks before the\nlast checkpoint.\n\nThey do protect against multi-month forks though, even if not the reason\nthat they exist.\n\nIf releases happen every 6 months, and the checkpoint is 3 months deep at\nrelease, then for the average node, the checkpoint is 3 to 9 months old.\n\nA 3 month reversal would be devastating, so the checkpoint isn't adding\nmuch extra security.\n\nWith headers first downloading, the checkpoints could be removed.  They\ncould still be used for speeding up verification of historical blocks.\nBlocks behind the last checkpoint wouldn't need their signatures checked.\n\nRemoving them could cause a hard-fork though, so maybe they could be\ndefined as legacy artifacts of the blockchain.  Future checkpoints could be\nadvisory.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/5607021d/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-13T12:26:17",
                "message_text_only": "Let's consider a concrete example:\n\n1. User wants to accept Bitcoin payments, as his customers want this.\n2. He downloads a recent version of Bitcoin Core, checks hashes and so on.\n(Maybe even builds from source.)\n3. Let's it to sync for several hours or days.\n4. After wallet is synced, he gives his address to customer.\n5. Customer pays.\n6. User waits 10 confirmations and ships the goods. (Suppose it's something\nvery expensive.)\n7. Some time later, user wants to convert some of his bitcoins to dollars.\nHe sends his bitcoins to an exchange but they never arrive.\n\nHe tries to investigate, and after some time discovers that his router (or\nhis ISP's router) was hijacked. His Bitcoin node couldn't connect to any of\nthe legitimate nodes, and thus got a complete fake chain from the attacker.\nBitcoins he received were totally fake.\n\nBitcoin Core did a shitty job and confirmed some fake transactions.\nUser doesn't care that *if *his network was not impaired, Bitcoin Core *would\nhave *worked properly.\nThe main duty of Bitcoin Core is to check whether transactions are\nconfirmed, and if it can be fooled by a simple router hack, then it does\nits job poorly.\n\nIf you don't see it being a problem, you should't be allowed to develop\nanything security-related.\n\nIf a node is connected to 99 dishonest nodes and 1 honest node, it can\n> still sync with the main network.\n>\n\nYes, it is good against Sybil attack, but not good against a network-level\nattack.\nAttack on user's routers is a very realistic, plausible attack.\nImagine if SSL could be hacked by hacking a router, would people still use\nit?\n\nFucking no.\n\n\n> A 3 month reversal would be devastating, so the checkpoint isn't adding\n> much extra security.\n>\n\nWIthout checkpoints an attacker could prepare a fork for $10.\nWith checkpoints, it would cost him at least $1000, but more likely upwards\nof $100000.\nThat's quite a difference, no?\n\nI do not care what do you think about the reasons why checkpoints were\nadded, but it is a fact that they make the attack scenario I describe above\nhard to impossible.\n\nWithout checkpoints, you could perform this attack using a laptop.\nWith checkpoints, you need access to significant amounts of mining ASICs.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/1683d04a/attachment.html>"
            },
            {
                "author": "Gavin",
                "date": "2015-05-13T13:24:04",
                "message_text_only": "Checkpoints will be replaced by compiled-in 'at THIS timestamp the main chain had THIS much proof of work.'\n\nThat is enough information to prevent attacks and still allow optimizations like skipping signature checking for ancient transactions.\n\nI don't think anybody is proposing replacing checkpoints with nothing.\n\n--\nGavin Andresen\n\n\n> On May 13, 2015, at 8:26 AM, Alex Mizrahi <alex.mizrahi at gmail.com> wrote:\n> \n> Let's consider a concrete example:\n> \n> 1. User wants to accept Bitcoin payments, as his customers want this.\n> 2. He downloads a recent version of Bitcoin Core, checks hashes and so on. (Maybe even builds from source.)\n> 3. Let's it to sync for several hours or days.\n> 4. After wallet is synced, he gives his address to customer.\n> 5. Customer pays. \n> 6. User waits 10 confirmations and ships the goods. (Suppose it's something very expensive.)\n> 7. Some time later, user wants to convert some of his bitcoins to dollars. He sends his bitcoins to an exchange but they never arrive.\n> \n> He tries to investigate, and after some time discovers that his router (or his ISP's router) was hijacked. His Bitcoin node couldn't connect to any of the legitimate nodes, and thus got a complete fake chain from the attacker.\n> Bitcoins he received were totally fake.\n> \n> Bitcoin Core did a shitty job and confirmed some fake transactions.\n> User doesn't care that if his network was not impaired, Bitcoin Core would have worked properly.\n> The main duty of Bitcoin Core is to check whether transactions are confirmed, and if it can be fooled by a simple router hack, then it does its job poorly.\n> \n> If you don't see it being a problem, you should't be allowed to develop anything security-related.\n> \n>> If a node is connected to 99 dishonest nodes and 1 honest node, it can still sync with the main network.\n> \n> Yes, it is good against Sybil attack, but not good against a network-level attack.\n> Attack on user's routers is a very realistic, plausible attack.\n> Imagine if SSL could be hacked by hacking a router, would people still use it?\n> \n> Fucking no.\n>   \n>> A 3 month reversal would be devastating, so the checkpoint isn't adding much extra security.\n> \n> WIthout checkpoints an attacker could prepare a fork for $10.\n> With checkpoints, it would cost him at least $1000, but more likely upwards of $100000.\n> That's quite a difference, no?\n> \n> I do not care what do you think about the reasons why checkpoints were added, but it is a fact that they make the attack scenario I describe above hard to impossible.\n> \n> Without checkpoints, you could perform this attack using a laptop.\n> With checkpoints, you need access to significant amounts of mining ASICs.\n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/43b037e0/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T13:28:44",
                "message_text_only": "On Wed, May 13, 2015 at 1:26 PM, Alex Mizrahi <alex.mizrahi at gmail.com>\nwrote:\n\n> He tries to investigate, and after some time discovers that his router (or\n> his ISP's router) was hijacked. His Bitcoin node couldn't connect to any of\n> the legitimate nodes, and thus got a complete fake chain from the attacker.\n> Bitcoins he received were totally fake.\n>\n> Bitcoin Core did a shitty job and confirmed some fake transactions.\n>\n\nI don't really see how you can protect against total isolation of a node\n(POS or POW).  You would need to find an alternative route for the\ninformation.\n\nEven encrypted connections are pointless without authentication of who you\nare communicating with.\n\nAgain, it is part of the security model that you can connect to at least\none honest node.\n\nSomeone tweated all the bitcoin headers at one point.  The problem is that\nif everyone uses the same check, then that source can be compromised.\n\n> WIthout checkpoints an attacker could prepare a fork for $10.\n> With checkpoints, it would cost him at least $1000, but more likely\nupwards of $100000.\n> That's quite a difference, no?\n\nHeaders first mean that you can't knock a synced node off the main chain\nwithout winning the POW race.\n\nCheckpoints can be replaced with a minimum amount of POW for initial sync.\nThis prevents spam of low POW blocks.  Once a node is on a chain with at\nleast that much POW, it considers it the main chain.,\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/c0aa03e4/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-13T14:26:52",
                "message_text_only": "> I don't really see how you can protect against total isolation of a node\n> (POS or POW). You would need to find an alternative route for the\n> information.\n>\n\n\"Alternative route for the information\" is the whole point of weak\nsubjectivity, no?\n\nPoS depends on weak subjectivity to prevent \"long term reversals\", but\nusing it also prevents \"total isolation\" attacks.\n\nThe argument that PoW is better than PoS because PoS has to depend on weak\nsubjectivity, but PoW doesn't is wrong.\nAny practical implementation of PoW will also have to rely on weak\nsubjectivity to be secure against isolation attack.\nAnd if we have to rely on weak subjectivity anyway, then why not PoS?\n\n\n> Again, it is part of the security model that you can connect to at least\n> one honest node.\n>\n\nThis is the security model of PoW-based consensus. If you study\nPoW-consensus, then yes, this is the model you have to use.\n\nBut people use Bitcoin Core as a piece of software. They do not care what\nsecurity model you use, they expect it to work.\nIf there are realistic scenarios in which it fails, then this must be\ndocumented. Users should be made aware of the problem, should be able to\ntake preventative measures (e.g. manually check the latest block against\nsources they trust), etc.\n\n\n> The problem is that if everyone uses the same check, then that source can\n> be compromised.\n>\n\nYes, this problem cannot be solved in a 100% decentralized and automatic\nway.\nWhich doesn't mean it's not worth solving, does it?\n\n1. There are non-decentralized, trust-based solutions: refuse to work if\nnone of well-known nodes are accessible.\nWell-known nodes are already used for bootstrapping, and this is another\npoint which can be attacked.\nSo if it's impossible to make it 100% decentralized and secure, why not\nmake it 99% decentralized and secure?\n\n2. It is a common practice to check sha256sum after downloading the\npackage, and this is usually done manually.\nWhy can't checking block hashes against some source become a common\npractice as well?\n\n\nAlso it's worth noting that these security measures are additive.\nIsolating a node AND hijacking one of well-known nodes AND hijacking a\nblock explorer site user checks hashes against is exponentially harder than\ndefeating a single measure.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/4be37675/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-13T23:46:04",
                "message_text_only": "On Wed, May 13, 2015 at 12:31 PM, Alex Mizrahi <alex.mizrahi at gmail.com> wrote:\n> But this matters if a new node has access to the globally strongest chain.\n> If attacker is able to block connections to legitimate nodes, a new node\n> will happily accept attacker's chain.\n\nIf you get isolated from the network you may not get the longest valid\nchain. I don't think any other consensus mechanism deals with this\nbetter than Bitcoin.\n\n> So PoW, by itself, doesn't give strong security guarantees. This problem is\n> so fundamental people avoid talking about it.\n>\n> In practice, Bitcoin already embraces \"weak subjectivity\" e.g. in form of\n> checkpoints embedded into the source code. So it's hard to take PoW purists\n> seriously.\n\nCheckpoints are NOT part of the consensus rules, they're just an\noptimization that can be removed.\nTry keeping the genesis block as your only checkpoint and rebuild: it\nwill work. You can also define your own checkpoints, there's no need\nfor everyone to use the same ones.\nIn a future with committed utxo the optimization could be bigger, but\nstill, we shouldn't rely on checkpoints for consensus, they're just an\noptimization and you should only trust checkpoints that are buried in\nthe chain. Trusting a committed utxo checkpoint from 2 years ago\ndoesn't seem very risky. If the code is not already done (not really\nsure if it was done as part of auto-prune), we should be prepared for\nreorgs that invalidate checkpoints.\nSo, no, Bitcoin does NOT rely on that \"weak subjectivity\" thing."
            },
            {
                "author": "Damian Gomez",
                "date": "2015-05-13T17:49:04",
                "message_text_only": "I hope to keep continuing this conversations. Pardon my absence, but I\ndon't alway feel like I have much to contribute especially if it's not\ntechincal.\n\nOn my part I have been a proponent, of an alterrnativ consensus, that\nbegins shifting away from teh current cooinbase reward system in order to\nreduce mining on the whole and thus limit those who do mine to do so on a\nlevel of integrity.\n\n\nI took a look at the ethereum blog on weak subjectivity, it does seem to be\na good transtition to use a gravity schema to be implemented in a Log\nStructured Merge tree  in order to find doscrepancy in forks.\n\n\nUsing this sama data structure could still be used in a consensus model. In\nterms of how nodes communicate on teh network their speed and latency\ncommunication are at least halfway solved based off their intereactions\n(kernel software changes) with how nodes write and read memory { smp_wrb()\n || smp_rmb() } This would allow for a connection on the\n\n\nLet me provide a use case:  Say that we wanted to begin a new model for\nintegrity, then the current value for integrity would utilize a OTS from\nthe previous hash in order to establish the previous owner address of the\nblock it was previously part of.  THE MAIN ISSUE here is being able to\nverify, which value of integrity is useful for being able to establish a\ngenesis block. A paper by Lee & Ewe (2001) called *The Byzantine General's\nProblem* gives insight as to how a  O(n^c) model is suitable to send a\nmessage w/ value through out the system, each node is then sent a\nread-invalidate request in order to change their cache logs for old system\nmemory in a new fixed address. Upon consensus of this value the rest of the\n\"brainer\" {1st recipeients} nodes would be able to send a forward\npropagation of  the learnt value and, after acceptance the value would then\nbe backpropagated to the genesis block upoon every round in orderr to set a\ndeterministic standard for the dynamic increase of integrity of the system.\n\n\nIn POW systems the nonce generated would be the accumulation of the\nintegrity within a system and what their computatiuonal exertion in terms\nof the overall rate of integrity increase in the system as the new coinbase\n-> this value then is assigned and signed to the hash and teh Merkel Root\n as two layers encoded to its base and then reencrypted using EDCSA from\nthe 256 to 512 bit transformation so that the new address given has a\nvalidity that cannot be easily fingerprinted and the malleability of teh\ntransaction becomes much more difficult due to the overall  2 ^ 28\nverification stamp provided to the new hash.   The parameters  T T r P\n\n(Trust value)  -> foud in the new coinbase or the scriptSig\n( Hidden) -> found in the Hash, and the merkel root hash\n(TRust overall)  R = within the target range for  new nonces and address\nlocations\nParadigm (integrity) = held within the genesis block as a backpropogated\nsolution\n\n\n\nUsing this signature then the  nodes would then be able to communicate and\ntransition the memory resevres for previous transaction on the block based\non the byzantine consensus. What noone has yet mentioned which I have\nforgotten too, is how these datacenters of pool woul be supported w/out\nfees. I will thrw that one out to all of you.  The current consensus system\nleaves room for orp[haned transactions if there were miltiple signature\nrequests the queue would be lined up based off integrity values in order to\nhave the most effective changes occcur first.\n\nI have some more thoughts and will continue working on the techinical\nvernacular and how a noob developer and decent computer science student\ncould make such an mplementation a reality.  Thanks in advance for\nlistengin to this.\n\n\n\n<Thank you to Greg Maxwell for allowing us to liosten to his talk online,\nwas hearing while writing this.>  And to Krzysztof Okupsi and Paul\nMcKenny(Memory Barriers Hardware View for Software hackers) for their help\nin nudging my brain and the relentles people behind the scenes who make all\nour minds possible.\n\n\n\n\n\n\n\nOn Wed, May 13, 2015 at 4:26 AM, <\nbitcoin-development-request at lists.sourceforge.net> wrote:\n\n> Send Bitcoin-development mailing list submissions to\n>         bitcoin-development at lists.sourceforge.net\n>\n> To subscribe or unsubscribe via the World Wide Web, visit\n>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> or, via email, send a message with subject or body 'help' to\n>         bitcoin-development-request at lists.sourceforge.net\n>\n> You can reach the person managing the list at\n>         bitcoin-development-owner at lists.sourceforge.net\n>\n> When replying, please edit your Subject line so it is more specific\n> than \"Re: Contents of Bitcoin-development digest...\"\n>\n> Today's Topics:\n>\n>    1. Re: Long-term mining incentives (Thomas Voegtlin)\n>    2. Re: Long-term mining incentives (Tier Nolan)\n>    3. Re: Long-term mining incentives (Alex Mizrahi)\n>    4. Re: Proposed alternatives to the 20MB step        function (Tier\n> Nolan)\n>    5. Re: Block Size Increase (Oliver Egginger)\n>    6. Re: Block Size Increase (Angel Leon)\n>\n>\n> ---------- Forwarded message ----------\n> From: Thomas Voegtlin <thomasv at electrum.org>\n> To: Gavin Andresen <gavinandresen at gmail.com>, Bitcoin Dev <\n> bitcoin-development at lists.sourceforge.net>\n> Cc:\n> Date: Wed, 13 May 2015 11:49:13 +0200\n> Subject: Re: [Bitcoin-development] Long-term mining incentives\n>\n> Le 12/05/2015 18:10, Gavin Andresen a \u00e9crit :\n> > Added back the list, I didn't mean to reply privately:\n> >\n> > Fair enough, I'll try to find time in the next month or three to write up\n> > four plausible future scenarios for how mining incentives might work:\n> >\n> > 1) Fee-supported with very large blocks containing lots of tiny-fee\n> > transactions\n> > 2) Proof-of-idle supported (I wish Tadge Dryja would publish his\n> > proof-of-idle idea....)\n> > 3) Fees purely as transaction-spam-prevention measure, chain security via\n> > alternative consensus algorithm (in this scenario there is very little\n> > mining).\n> > 4) Fee supported with small blocks containing high-fee transactions\n> moving\n> > coins to/from sidechains.\n> >\n> > Would that be helpful, or do you have some reason for thinking that we\n> > should pick just one and focus all of our efforts on making that one\n> > scenario happen?\n> >\n> > I always think it is better, when possible, not to \"bet on one horse.\"\n> >\n>\n> Sorry if I did not make myself clear. It is not about betting on one\n> single horse, or about making one particular scenario happen. It is not\n> about predicting whether something else will replace PoW in the future,\n> and I am in no way asking you to focus your efforts in one particular\n> direction at the expenses of others. Various directions will be explored\n> by various people, and that's great.\n>\n> I am talking about what we know today. I would like an answer to the\n> following question: Do we have a reason to believe that Bitcoin can work\n> in the long run, without involving technologies that have not been\n> invented yet? Is there a single scenario that we know could work?\n>\n> Exotic and unproven technologies are not an answer to that question. The\n> reference scenario should be as boring as possible, and as verifiable as\n> possible. I am not asking what you think is the most likely to happen,\n> but what is the most likely to work, given the knowledge we have today.\n>\n> If I was asking: \"Can we send humans to the moon by 2100?\", I guess your\n> answer would be: \"Yes we can, because it has been done in the past with\n> chemical rockets, and we know how to build them\". You would probably not\n> use a space elevator in your answer.\n>\n> The reason I am asking that is, there seems to be no consensus among\n> core developers on how Bitcoin can work without miner subsidy. How it\n> *will* work is another question.\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: Tier Nolan <tier.nolan at gmail.com>\n> To:\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Date: Wed, 13 May 2015 11:14:06 +0100\n> Subject: Re: [Bitcoin-development] Long-term mining incentives\n> On Wed, May 13, 2015 at 10:49 AM, Thomas Voegtlin <thomasv at electrum.org>\n> wrote:\n>\n>>\n>> The reason I am asking that is, there seems to be no consensus among\n>> core developers on how Bitcoin can work without miner subsidy. How it\n>> *will* work is another question.\n>>\n>\n> The position seems to be that it will continue to work for the time being,\n> so there is still time for more research.\n>\n> Proof of stake has problems with handling long term reversals.  The main\n> proposal is to slightly weaken the security requirements.\n>\n> With POW, a new node only needs to know the genesis block (and network\n> rules) to fully determine which of two chains is the strongest.\n>\n> Penalties for abusing POS inherently create a time horizon.  A suggested\n> POS security model would assume that a full node is a node that resyncs\n> with the network regularly (every N blocks).    N would be depend on the\n> network rules of the coin.\n>\n> The alternative is that 51% of the holders of coins at the genesis block\n> can rewrite the entire chain.  The genesis block might not be the first\n> block, a POS coin might still use POW for minting.\n>\n>\n> https://blog.ethereum.org/2014/11/25/proof-stake-learned-love-weak-subjectivity/\n>\n>\n> ---------- Forwarded message ----------\n> From: Alex Mizrahi <alex.mizrahi at gmail.com>\n> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Cc:\n> Date: Wed, 13 May 2015 13:31:47 +0300\n> Subject: Re: [Bitcoin-development] Long-term mining incentives\n>\n>\n>> With POW, a new node only needs to know the genesis block (and network\n>> rules) to fully determine which of two chains is the strongest.\n>>\n>\n> But this matters if a new node has access to the globally strongest chain.\n> If attacker is able to block connections to legitimate nodes, a new node\n> will happily accept attacker's chain.\n>\n> So PoW, by itself, doesn't give strong security guarantees. This problem\n> is so fundamental people avoid talking about it.\n>\n> In practice, Bitcoin already embraces \"weak subjectivity\" e.g. in form of\n> checkpoints embedded into the source code. So it's hard to take PoW purists\n> seriously.\n>\n>\n> ---------- Forwarded message ----------\n> From: Tier Nolan <tier.nolan at gmail.com>\n> To:\n> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n> Date: Wed, 13 May 2015 11:43:08 +0100\n> Subject: Re: [Bitcoin-development] Proposed alternatives to the 20MB step\n> function\n> On Sat, May 9, 2015 at 4:36 AM, Gregory Maxwell <gmaxwell at gmail.com>\n> wrote:\n>\n>> An example would\n>> be tx_size = MAX( real_size >> 1,  real_size + 4*utxo_created_size -\n>> 3*utxo_consumed_size).\n>\n>\n> This could be implemented as a soft fork too.\n>\n> * 1MB hard size limit\n> * 900kB soft limit\n>\n> S = block size\n> U = UTXO_adjusted_size = S + 4 * outputs - 3 * inputs\n>\n> A block is valid if S < 1MB and U < 1MB\n>\n> A 250 byte transaction with 2 inputs and 2 outputs would have an adjusted\n> size of 252 bytes.\n>\n> The memory pool could be sorted by fee per adjusted_size.\n>\n>  Coin selection could be adjusted so it tries to have at least 2 inputs\n> when creating transactions, unless the input is worth more than a threshold\n> (say 0.001 BTC).\n>\n> This is a pretty weak incentive, especially if the block size is\n> increased.  Maybe it will cause a \"nudge\"\n>\n>\n> ---------- Forwarded message ----------\n> From: Oliver Egginger <bitcoin at olivere.de>\n> To: bitcoin-development at lists.sourceforge.net\n> Cc:\n> Date: Wed, 13 May 2015 12:37:17 +0200\n> Subject: Re: [Bitcoin-development] Block Size Increase\n> 08.05.2015 at 5:49 Jeff Garzik wrote:\n> > To repeat, the very first point in my email reply was: \"Agree that 7 tps\n> > is too low\"\n>\n> For interbank trading that would maybe enough but I don't know.\n>\n> I'm not a developer but as a (former) user and computer scientist I'm\n> also asking myself what is the core of the problem? Personally, for\n> privacy reasons I do not want to leave a footprint in the blockchain for\n> each pizza. And why should this expense be good for trivial things of\n> everyday life?\n>\n> If one encounters the block boundary, he or she will do more effort or\n> give up. I'm thinking most people will give up because their\n> transactions are not really economical. It is much better for them to\n> use third-partys (or another payment system).\n>\n> And that's where we are at the heart of the problem. The Bitcoin\n> third-party economy. With few exceptions this is pure horror. More worse\n> than any used car dealer. And the community just waits that things get\n> better. But that will never happen of its own accord. We are living in a\n> Wild West Town. So we need a Sheriff and many other things.\n>\n> We need a small but good functioning economy around the blockchain. To\n> create one, we have to accept a few unpleasant truths. I do not know if\n> the community is ready for it.\n>\n> Nevertheless, I know that some companies do a good job. But they have to\n> prevail against their dishonest competitors.\n>\n> People take advantage of the blockchain, because they no longer trust\n> anyone. But this will not scale in the long run.\n>\n> - oliver\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: Angel Leon <gubatron at gmail.com>\n> To: Oliver Egginger <bitcoin at olivere.de>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Date: Wed, 13 May 2015 07:25:47 -0400\n> Subject: Re: [Bitcoin-development] Block Size Increase\n> > Personally, for privacy reasons I do not want to leave a footprint in\n> the blockchain for each pizza. And  why should this expense be good for\n> trivial things of everyday life?\n>\n> Then what's the point?\n> Isn't this supposed to be an Open transactional network, it doesn't matter\n> if you don't want that, what matters is what people want to do with it, and\n> there's nothing you can do to stop someone from opening a wallet and buying\n> a pizza with it, except the core of the problem you ask yourself about,\n> which is, the minute this goes mainstream and people get their wallets out\n> the whole thing will collapse, regardless of what you want the blockchain\n> for.\n>\n> Why talk about the billions of unbanked and all the romantic vision if you\n> can't let them use their money however they want in a decentralized\n> fashion. Otherwise let's just go back to centralized banking because the\n> minute you want to put things off chain, you need an organization that will\n> need to respond to government regulation and that's the end for the\n> billions of unbanked to be part of the network.\n>\n>\n> http://twitter.com/gubatron\n>\n> On Wed, May 13, 2015 at 6:37 AM, Oliver Egginger <bitcoin at olivere.de>\n> wrote:\n>\n>> 08.05.2015 at 5:49 Jeff Garzik wrote:\n>> > To repeat, the very first point in my email reply was: \"Agree that 7 tps\n>> > is too low\"\n>>\n>> For interbank trading that would maybe enough but I don't know.\n>>\n>> I'm not a developer but as a (former) user and computer scientist I'm\n>> also asking myself what is the core of the problem? Personally, for\n>> privacy reasons I do not want to leave a footprint in the blockchain for\n>> each pizza. And why should this expense be good for trivial things of\n>> everyday life?\n>>\n>> If one encounters the block boundary, he or she will do more effort or\n>> give up. I'm thinking most people will give up because their\n>> transactions are not really economical. It is much better for them to\n>> use third-partys (or another payment system).\n>>\n>> And that's where we are at the heart of the problem. The Bitcoin\n>> third-party economy. With few exceptions this is pure horror. More worse\n>> than any used car dealer. And the community just waits that things get\n>> better. But that will never happen of its own accord. We are living in a\n>> Wild West Town. So we need a Sheriff and many other things.\n>>\n>> We need a small but good functioning economy around the blockchain. To\n>> create one, we have to accept a few unpleasant truths. I do not know if\n>> the community is ready for it.\n>>\n>> Nevertheless, I know that some companies do a good job. But they have to\n>> prevail against their dishonest competitors.\n>>\n>> People take advantage of the blockchain, because they no longer trust\n>> anyone. But this will not scale in the long run.\n>>\n>> - oliver\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/198e7dd0/attachment.html>"
            },
            {
                "author": "Michael Jensen",
                "date": "2015-05-18T02:29:57",
                "message_text_only": "I think the basic reality is that a) an arbitrarily elevated level of\nhashing is fundamental to a truly decentralised, autonomous network,\nand is an essential cost to maintaining Bitcoin, b) there are no signs\nthat this fact will change, c) there must be some replacement to the\ncurrent system of incentivisation through debasement (inflation).\n\nArguments about limiting block size versus setting minimum fees are\nconfused because ultimately both mechanisms should ideally achieve the\nsame outcome: a market price for transactions which means that a) not\neveryone who would make a TX if TXs were unpriced does so (reduced\nnumber of TXs) and b) the market price is used to fund hashing. This\nis just the nature of prices, it always reduces effective demand, but\nwithout prices supply must collapse and the market must fail.\n\nRegardless, if every time the network gets close to reaching the block\nsize limit the development community gets scared and raises the limit,\nthen such a limit will never be an effective tool for setting a market\nprice. Personally I think trying to artificially limit supply to\ncreate a price for transactions is a needlessly complicated way of\ntrying to achieve this goal. I think minimum fees for transactions is\na better, simpler option.\n\nI would go a step further and say that the development community will\nstruggle forever if it tries to play the role of the centralised\neconomic planner in setting prices for network services. The community\nshould look at more dynamic ways to let network users express their\npreferences for security and their willingness to pay for it. I've\nwritten on the issue -\nhttps://medium.com/@mike0/securing-bitcoin-5-determing-an-optimal-funding-level-9873fa1322a7\n\nAs far as the argument that fees will drive people away from Bitcoin,\nI can't believe that. Everything we desire has to be paid for somehow.\nPeople will accept a fee for making Bitcoin transactions if Bitcoin as\na result is a stable, useful service. Bitcoin as both a currency and\nas a transaction network has strong network effects, so, ignoring\nsidechains, it's highly unrealistic that a mandatory fee will drive\npeople away from Bitcoin when the alternatives are dubious knock-offs\nwith no network effect, and fiat, which is even worse in regards to\nthe hidden and malignant costs it exacts."
            }
        ],
        "thread_summary": {
            "title": "Long-term mining incentives",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Dave Hudson",
                "Tier Nolan",
                "insecurity at national.shitposting.agency",
                "Tom Harding",
                "Adam Back",
                "Mike Hearn",
                "Owen Gunden",
                "Jorge Tim\u00f3n",
                "Gregory Maxwell",
                "Melvin Carvalho",
                "Aaron Voisine",
                "Damian Gomez",
                "Gavin",
                "Michael Jensen",
                "Alex Mizrahi",
                "Gavin Andresen",
                "Thomas Voegtlin",
                "Pedro Worcel",
                "Pieter Wuille"
            ],
            "messages_count": 37,
            "total_messages_chars_count": 96593
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin-development Digest, Vol 48, Issue 62",
        "thread_messages": [
            {
                "author": "Damian Gomez",
                "date": "2015-05-11T20:20:46",
                "message_text_only": "Hllo\n\nI want to build from a conversation that I had w/ Peter (T?) regarding the\nincrease in block size in the bitcoin from its's current structure would be\nthe proposasl of an prepend to the hash chain itself that would be the\nfirst DER decoded script in order to verify integrity(trust) within a set\nof transactions and the originiator themselves.\n\nIt is my belief that the process to begin a new encryption tool using a\nvariant of the WinterNitz OTS for its existential unforgeability to be the\nadded signatures with every  Wallet transaction in order to provide a\nconsesnus systemt that takes into accont a personal level of intergrity for\nthe intention fo a transaction to occur. This signature would then be\nhashes for there to be an intermediate proxy state that then verifies and\nevaluates the trust fucntion for the receiving trnsactions.  This\nevaluation loop would itself be a state in which the mining power and the\nrewards derived from them would be an increased level of integrity as\nprovided for the \"brainers\" of a systems who are then the \"signatuers\" of\nthe transaction authenticity, and additiaonally program extranonces of x\nbits {72} in order  to have a double valid signature that the rest of the\nnodes would accept in order to have a valid address from which to be able\nto continuously receive transactions.\n\nThere is a level of diffculty in obtaining brainers, fees would only apply\nuin so much as they are able to create authentic transactions based off the\nvoting power of the rest of the received nodes. The greater number of\nfaults within the system from a brainer then the more, so would his\ncomputational power be restricted in order to provide a reward feedback\nsystem. This singularity in a Byzantine consensus is only achieved if the\nroute of an appropriate transformation occurs, one that is invariant to the\nparticipants of the system, thus being able to provide initial vector\ntransformations from a person's online identity is the responsibilty that\nwe have to ensure and calulate a lagrangian method that utilisizes a set of\nconvolutional neural network funcitons [backpropagation, fuzzy logic] and\nand tranformation function taking the vectors of tranformations in a\nkahunen-loeve algorithm and using the convergence of a baryon wave function\nin order to proceed with a baseline reading of the current level of\nintegrity in the state today that is an instance of actionable acceleration\nwithin a system.\n\nThis is something that I am trying to continue to parse out. Therefore\nthere are still heavy questions to be answered(the most important being the\nconsent of the people to measure their own levels of integrity through\nmined information)> There must always be the option to disconnect from a\ntransactional system where payments occur in order to allow a level of\nsolace and peace within individuals -- withour repercussions and a seperate\nsystem that supports the offline realm as well. (THis is a design problem)\n\nUltimately, quite literally such a transaction system could exist to\nprovide detailed analysis that promotes integrity being the basis for\nsharing information.  The fee structure would be eliminated, due to the\nlevel of integrity and procesing power to have messages and transactions\nand reviews of unfiduciary responsible orgnizations be merited as highly\ntrue (.9 in fizzy logic) in order to promote a well-being in the state.\nThat is its own reward, the strenght of having more processing speed.\n\n\nFYI(thank you to peter whom nudged my thinking and interest (again) in this\narea. )\n\nThis is something I am attempting to design in order to program it. Though\nI am not an expert and my technology stack is limited to java and c (and my\nissues from it).  I provided a class the other day the was pseudo code for\nthe beginning of the consensus. Now I might to now if I am missing any of\nteh technical paradigms that might make this illogical? I now with the\nadvent of 7petabyte computers one could easily store 2.5 petabytes of human\ninformation for just an instance of integrity not to mention otehr\nemotions.\n\n\n\n*Also, might someone be able to provide a bit of information on Bitcoin\ncore project?*\n\nthank you again. Damain.\n\nOn Mon, May 11, 2015 at 10:29 AM, <\nbitcoin-development-request at lists.sourceforge.net> wrote:\n\n> Send Bitcoin-development mailing list submissions to\n>         bitcoin-development at lists.sourceforge.net\n>\n> To subscribe or unsubscribe via the World Wide Web, visit\n>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> or, via email, send a message with subject or body 'help' to\n>         bitcoin-development-request at lists.sourceforge.net\n>\n> You can reach the person managing the list at\n>         bitcoin-development-owner at lists.sourceforge.net\n>\n> When replying, please edit your Subject line so it is more specific\n> than \"Re: Contents of Bitcoin-development digest...\"\n>\n> Today's Topics:\n>\n>    1. Fwd:  Bitcoin core 0.11 planning (Wladimir)\n>    2. Re: Bitcoin core 0.11 planning (Wladimir)\n>    3. Long-term mining incentives (Thomas Voegtlin)\n>    4. Re: Long-term mining incentives\n>       (insecurity at national.shitposting.agency)\n>    5. Re: Reducing the block rate instead of    increasing the maximum\n>       block size (Luke Dashjr)\n>    6. Re: Long-term mining incentives (Gavin Andresen)\n>\n>\n> ---------- Forwarded message ----------\n> From: Wladimir <laanwj at gmail.com>\n> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Cc:\n> Date: Mon, 11 May 2015 14:49:53 +0000\n> Subject: [Bitcoin-development] Fwd: Bitcoin core 0.11 planning\n> On Tue, Apr 28, 2015 at 11:01 AM, Pieter Wuille <pieter.wuille at gmail.com>\n> wrote:\n> > As softforks almost certainly require backports to older releases and\n> other\n> > software anyway, I don't think they should necessarily be bound to\n> Bitcoin\n> > Core major releases. If they don't require large code changes, we can\n> easily\n> > do them in minor releases too.\n>\n> Agree here - there is no need to time consensus changes with a major\n> release, as they need to be ported back to older releases anyhow.\n> (I don't really classify them as software features, but properties of\n> the underlying system that we need to adopt to)\n>\n> Wladimir\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: Wladimir <laanwj at gmail.com>\n> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Cc:\n> Date: Mon, 11 May 2015 15:00:03 +0000\n> Subject: Re: [Bitcoin-development] Bitcoin core 0.11 planning\n> A reminder - feature freeze and string freeze is coming up this Friday the\n> 15th.\n>\n> Let me know if your pull request is ready to be merged before then,\n>\n> Wladimir\n>\n> On Tue, Apr 28, 2015 at 7:44 AM, Wladimir J. van der Laan\n> <laanwj at gmail.com> wrote:\n> > Hello all,\n> >\n> > The release window for 0.11 is nearing, I'd propose the following\n> schedule:\n> >\n> > 2015-05-01  Soft translation string freeze\n> >             Open Transifex translations for 0.11\n> >             Finalize and close translation for 0.9\n> >\n> > 2015-05-15  Feature freeze, string freeze\n> >\n> > 2015-06-01  Split off 0.11 branch\n> >             Tag and release 0.11.0rc1\n> >             Start merging for 0.12 on master branch\n> >\n> > 2015-07-01  Release 0.11.0 final (aim)\n> >\n> > In contrast to former releases, which were protracted for months, let's\n> try to be more strict about the dates. Of course it is always possible for\n> last-minute critical issues to interfere with the planning. The release\n> will not be held up for features, though, and anything that will not make\n> it to 0.11 will be postponed to next release scheduled for end of the year.\n> >\n> > Wladimir\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: Thomas Voegtlin <thomasv at electrum.org>\n> To: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n> Cc:\n> Date: Mon, 11 May 2015 18:28:46 +0200\n> Subject: [Bitcoin-development] Long-term mining incentives\n> The discussion on block size increase has brought some attention to the\n> other elephant in the room: Long-term mining incentives.\n>\n> Bitcoin derives its current market value from the assumption that a\n> stable, steady-state regime will be reached in the future, where miners\n> have an incentive to keep mining to protect the network. Such a steady\n> state regime does not exist today, because miners get most of their\n> reward from the block subsidy, which will progressively be removed.\n>\n> Thus, today's 3 billion USD question is the following: Will a steady\n> state regime be reached in the future? Can such a regime exist? What are\n> the necessary conditions for its existence?\n>\n> Satoshi's paper suggests that this may be achieved through miner fees.\n> Quite a few people seem to take this for granted, and are working to\n> make it happen (developing cpfp and replace-by-fee). This explains part\n> of the opposition to raising the block size limit; some people would\n> like to see some fee pressure building up first, in order to get closer\n> to a regime where miners are incentivised by transaction fees instead of\n> block subsidy. Indeed, the emergence of a working fee market would be\n> extremely reassuring for the long-term viability of bitcoin. So, the\n> thinking goes, by raising the block size limit, we would be postponing a\n> crucial reality check. We would be buying time, at the expenses of\n> Bitcoin's decentralization.\n>\n> OTOH, proponents of a block size increase have a very good point: if the\n> block size is not raised soon, Bitcoin is going to enter a new, unknown\n> and potentially harmful regime. In the current regime, almost all\n> transaction get confirmed quickly, and fee pressure does not exist. Mike\n> Hearn suggested that, when blocks reach full capacity and users start to\n> experience confirmation delays and confirmation uncertainty, users will\n> simply go away and stop using Bitcoin. To me, that outcome sounds very\n> plausible indeed. Thus, proponents of the block size increase are\n> conservative; they are trying to preserve the current regime, which is\n> known to work, instead of letting the network enter uncharted territory.\n>\n> My problem is that this seems to lacks a vision. If the maximal block\n> size is increased only to buy time, or because some people think that 7\n> tps is not enough to compete with VISA, then I guess it would be\n> healthier to try and develop off-chain infrastructure first, such as the\n> Lightning network.\n>\n> OTOH, I also fail to see evidence that a limited block capacity will\n> lead to a functional fee market, able to sustain a steady state. A\n> functional market requires well-informed participants who make rational\n> choices and accept the outcomes of their choices. That is not the case\n> today, and to believe that it will magically happen because blocks start\n> to reach full capacity sounds a lot like like wishful thinking.\n>\n> So here is my question, to both proponents and opponents of a block size\n> increase: What steady-state regime do you envision for Bitcoin, and what\n> is is your plan to get there? More specifically, how will the\n> steady-state regime look like? Will users experience fee pressure and\n> delays, or will it look more like a scaled up version of what we enjoy\n> today? Should fee pressure be increased jointly with subsidy decrease,\n> or as soon as possible, or never? What incentives will exist for miners\n> once the subsidy is gone? Will miners have an incentive to permanently\n> fork off the last block and capture its fees? Do you expect Bitcoin to\n> work because miners are altruistic/selfish/honest/caring?\n>\n> A clear vision would be welcome.\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: insecurity at national.shitposting.agency\n> To: thomasv at electrum.org\n> Cc: bitcoin-development at lists.sourceforge.net\n> Date: Mon, 11 May 2015 16:52:10 +0000\n> Subject: Re: [Bitcoin-development] Long-term mining incentives\n> On 2015-05-11 16:28, Thomas Voegtlin wrote:\n>\n>> My problem is that this seems to lacks a vision. If the maximal block\n>> size is increased only to buy time, or because some people think that 7\n>> tps is not enough to compete with VISA, then I guess it would be\n>> healthier to try and develop off-chain infrastructure first, such as the\n>> Lightning network.\n>>\n>\n> If your end goal is \"compete with VISA\" you might as well just give up\n> and go home right now. There's lots of terrible proposals where people\n> try to demonstrate that so many hundred thousand transactions a second\n> are possible if we just make the block size 500GB. In the real world\n> with physical limits, you literally can not verify more than a few\n> thousand ECDSA signatures a second on a CPU core. The tradeoff taken\n> in Bitcoin is that the signatures are pretty small, but they are also\n> slow to verify on any sort of scale. There's no way competing with a\n> centralised entity using on-chain transactions is even a sane goal.\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: Luke Dashjr <luke at dashjr.org>\n> To: bitcoin-development at lists.sourceforge.net\n> Cc:\n> Date: Mon, 11 May 2015 16:47:47 +0000\n> Subject: Re: [Bitcoin-development] Reducing the block rate instead of\n> increasing the maximum block size\n> On Monday, May 11, 2015 7:03:29 AM Sergio Lerner wrote:\n> > 1. It will encourage centralization, because participants of mining\n> > pools will loose more money because of excessive initial block template\n> > latency, which leads to higher stale shares\n> >\n> > When a new block is solved, that information needs to propagate\n> > throughout the Bitcoin network up to the mining pool operator nodes,\n> > then a new block header candidate is created, and this header must be\n> > propagated to all the mining pool users, ether by a push or a pull\n> > model. Generally the mining server pushes new work units to the\n> > individual miners. If done other way around, the server would need to\n> > handle a high load of continuous work requests that would be difficult\n> > to distinguish from a DDoS attack. So if the server pushes new block\n> > header candidates to clients, then the problem boils down to increasing\n> > bandwidth of the servers to achieve a tenfold increase in work\n> > distribution. Or distributing the servers geographically to achieve a\n> > lower latency. Propagating blocks does not require additional CPU\n> > resources, so mining pools administrators would need to increase\n> > moderately their investment in the server infrastructure to achieve\n> > lower latency and higher bandwidth, but I guess the investment would be\n> > low.\n>\n> 1. Latency is what matters here, not bandwidth so much. And latency\n> reduction\n> is either expensive or impossible.\n> 2. Mining pools are mostly run at a loss (with exception to only the most\n> centralised pools), and have nothing to invest in increasing\n> infrastructure.\n>\n> > 3, It will reduce the security of the network\n> >\n> > The security of the network is based on two facts:\n> > A- The miners are incentivized to extend the best chain\n> > B- The probability of a reversal based on a long block competition\n> > decreases as more confirmation blocks are appended.\n> > C- Renting or buying hardware to perform a 51% attack is costly.\n> >\n> > A still holds. B holds for the same amount of confirmation blocks, so 6\n> > confirmation blocks in a 10-minute block-chain is approximately\n> > equivalent to 6 confirmation blocks in a 1-minute block-chain.\n> > Only C changes, as renting the hashing power for 6 minutes is ten times\n> > less expensive as renting it for 1 hour. However, there is no shop where\n> > one can find 51% of the hashing power to rent right now, nor probably\n> > will ever be if Bitcoin succeeds. Last, you can still have a 1 hour\n> > confirmation (60 1-minute blocks) if you wish for high-valued payments,\n> > so the security decreases only if participant wish to decrease it.\n>\n> You're overlooking at least:\n> 1. The real network has to suffer wasted work as a result of the stale\n> blocks,\n> while an attacker does not. If 20% of blocks are stale, the attacker only\n> needs 40% of the legitimate hashrate to achieve 50%-in-practice.\n> 2. Since blocks are individually weaker, it becomes cheaper to DoS nodes\n> with\n> invalid blocks. (not sure if this is a real concern, but it ought to be\n> considered and addressed)\n>\n> > 4. Reducing the block propagation time on the average case is good, but\n> > what happen in the worse case?\n> >\n> > Most methods proposed to reduce the block propagation delay do it only\n> > on the average case. Any kind of block compression relies on both\n> > parties sharing some previous information. In the worse case it's true\n> > that a miner can create and try to broadcast a block that takes too much\n> > time to verify or bandwidth to transmit. This is currently true on the\n> > Bitcoin network. Nevertheless there is no such incentive for miners,\n> > since they will be shooting on their own foots. Peter Todd has argued\n> > that the best strategy for miners is actually to reach 51% of the\n> > network, but not more. In other words, to exclude the slowest 49%\n> > percent. But this strategy of creating bloated blocks is too risky in\n> > practice, and surely doomed to fail, as network conditions dynamically\n> > change. Also it would be perceived as an attack to the network, and the\n> > miner (if it is a public mining pool) would be probably blacklisted.\n>\n> One can probably overcome changing network conditions merely by trying to\n> reach 75% and exclude the slowest 25%. Also, there is no way to identify or\n> blacklist miners.\n>\n> > 5. Thousands of SPV wallets running in mobile devices would need to be\n> > upgraded (thanks Mike).\n> >\n> > That depends on the current upgrade rate for SPV wallets like Bitcoin\n> > Wallet  and BreadWallet. Suppose that the upgrade rate is 80%/year: we\n> > develop the source code for the change now and apply the change in Q2\n> > 2016, then  most of the nodes will already be upgraded by when the\n> > hardfork takes place. Also a public notice telling people to upgrade in\n> > web pages, bitcointalk, SPV wallets warnings, coindesk, one year in\n> > advance will give plenty of time to SPV wallet users to upgrade.\n>\n> I agree this shouldn't be a real concern. SPV wallets are also more likely\n> and\n> less risky (globally) to be auto-updated.\n>\n> > 6. If there are 10x more blocks, then there are 10x more block headers,\n> > and that increases the amount of bandwidth SPV wallets need to catch up\n> > with the chain\n> >\n> > A standard smartphone with average cellular downstream speed downloads\n> > 2.6 headers per second (1600 kbits/sec) [3], so if synchronization were\n> > to be done only at night when the phone is connected to the power line,\n> > then it would take 9 minutes to synchronize with 1440 headers/day. If a\n> > person should accept a payment, and the smart-phone is 1 day\n> > out-of-synch, then it takes less time to download all the missing\n> > headers than to wait for a 10-minute one block confirmation. Obviously\n> > all smartphones with 3G have a downstream bandwidth much higher,\n> > averaging 1 Mbps. So the whole synchronization will be done less than a\n> > 1-minute block confirmation.\n>\n> Uh, I think you need to be using at least median speeds. As an example, I\n> can\n> only sustain (over 3G) about 40 kbps, with a peak of around 400 kbps. 3G\n> has\n> worse range/coverage than 2G. No doubt the *average* is skewed so high\n> because\n> of densely populated areas like San Francisco having 400+ Mbps cellular\n> data.\n> It's not reasonable to assume sync only at night: most payments will be\n> during\n> the day, on battery - so increased power use must also be considered.\n>\n> > According to CISCO mobile bandwidth connection speed increases 20% every\n> > year.\n>\n> Only in small densely populated areas of first-world countries.\n>\n> Luke\n>\n>\n>\n>\n> ---------- Forwarded message ----------\n> From: Gavin Andresen <gavinandresen at gmail.com>\n> To: insecurity at national.shitposting.agency\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Date: Mon, 11 May 2015 13:29:02 -0400\n> Subject: Re: [Bitcoin-development] Long-term mining incentives\n> I think long-term the chain will not be secured purely by proof-of-work. I\n> think when the Bitcoin network was tiny running solely on people's home\n> computers proof-of-work was the right way to secure the chain, and the only\n> fair way to both secure the chain and distribute the coins.\n>\n> See https://gist.github.com/gavinandresen/630d4a6c24ac6144482a  for some\n> half-baked thoughts along those lines. I don't think proof-of-work is the\n> last word in distributed consensus (I also don't think any alternatives are\n> anywhere near ready to deploy, but they might be in ten years).\n>\n> I also think it is premature to worry about what will happen in twenty or\n> thirty years when the block subsidy is insignificant. A lot will happen in\n> the next twenty years. I could spin a vision of what will secure the chain\n> in twenty years, but I'd put a low probability on that vision actually\n> turning out to be correct.\n>\n> That is why I keep saying Bitcoin is an experiment. But I also believe\n> that the incentives are correct, and there are a lot of very motivated,\n> smart, hard-working people who will make it work. When you're talking about\n> trying to predict what will happen decades from now, I think that is the\n> best you can (honestly) do.\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/46bc687a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin-development Digest, Vol 48, Issue 62",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Damian Gomez"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 22225
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin-development Digest, Vol 48, Issue 63",
        "thread_messages": [
            {
                "author": "Damian Gomez",
                "date": "2015-05-11T20:28:39",
                "message_text_only": "Btw How awful that I didn't cite my sources, please exucse me, this is\ndefinitely not my intention sometimes I get too caught up in my own\nexcitemtnt\n\n1) Martin, J., Alvisi, L., Fast Byzantine Consensus. *IEEE Transactions on\nDependable and Secure Computing. 2006. *3(3) doi: <?>  Please see\nJohn-Phillipe Martin and Lorenzo ALvisi\n\n2) https://eprint.iacr.org/2011/191.pdf  One_Time Winternitz Signatures.\n\n\nOn Mon, May 11, 2015 at 1:20 PM, <\nbitcoin-development-request at lists.sourceforge.net> wrote:\n\n> Send Bitcoin-development mailing list submissions to\n>         bitcoin-development at lists.sourceforge.net\n>\n> To subscribe or unsubscribe via the World Wide Web, visit\n>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> or, via email, send a message with subject or body 'help' to\n>         bitcoin-development-request at lists.sourceforge.net\n>\n> You can reach the person managing the list at\n>         bitcoin-development-owner at lists.sourceforge.net\n>\n> When replying, please edit your Subject line so it is more specific\n> than \"Re: Contents of Bitcoin-development digest...\"\n>\n> Today's Topics:\n>\n>    1. Re: Bitcoin-development Digest, Vol 48,   Issue 62 (Damian Gomez)\n>\n>\n> ---------- Forwarded message ----------\n> From: Damian Gomez <dgomez1092 at gmail.com>\n> To: bitcoin-development at lists.sourceforge.net\n> Cc:\n> Date: Mon, 11 May 2015 13:20:46 -0700\n> Subject: Re: [Bitcoin-development] Bitcoin-development Digest, Vol 48,\n> Issue 62\n> Hllo\n>\n> I want to build from a conversation that I had w/ Peter (T?) regarding the\n> increase in block size in the bitcoin from its's current structure would be\n> the proposasl of an prepend to the hash chain itself that would be the\n> first DER decoded script in order to verify integrity(trust) within a set\n> of transactions and the originiator themselves.\n>\n> It is my belief that the process to begin a new encryption tool using a\n> variant of the WinterNitz OTS for its existential unforgeability to be the\n> added signatures with every  Wallet transaction in order to provide a\n> consesnus systemt that takes into accont a personal level of intergrity for\n> the intention fo a transaction to occur. This signature would then be\n> hashes for there to be an intermediate proxy state that then verifies and\n> evaluates the trust fucntion for the receiving trnsactions.  This\n> evaluation loop would itself be a state in which the mining power and the\n> rewards derived from them would be an increased level of integrity as\n> provided for the \"brainers\" of a systems who are then the \"signatuers\" of\n> the transaction authenticity, and additiaonally program extranonces of x\n> bits {72} in order  to have a double valid signature that the rest of the\n> nodes would accept in order to have a valid address from which to be able\n> to continuously receive transactions.\n>\n> There is a level of diffculty in obtaining brainers, fees would only apply\n> uin so much as they are able to create authentic transactions based off the\n> voting power of the rest of the received nodes. The greater number of\n> faults within the system from a brainer then the more, so would his\n> computational power be restricted in order to provide a reward feedback\n> system. This singularity in a Byzantine consensus is only achieved if the\n> route of an appropriate transformation occurs, one that is invariant to the\n> participants of the system, thus being able to provide initial vector\n> transformations from a person's online identity is the responsibilty that\n> we have to ensure and calulate a lagrangian method that utilisizes a set of\n> convolutional neural network funcitons [backpropagation, fuzzy logic] and\n> and tranformation function taking the vectors of tranformations in a\n> kahunen-loeve algorithm and using the convergence of a baryon wave function\n> in order to proceed with a baseline reading of the current level of\n> integrity in the state today that is an instance of actionable acceleration\n> within a system.\n>\n> This is something that I am trying to continue to parse out. Therefore\n> there are still heavy questions to be answered(the most important being the\n> consent of the people to measure their own levels of integrity through\n> mined information)> There must always be the option to disconnect from a\n> transactional system where payments occur in order to allow a level of\n> solace and peace within individuals -- withour repercussions and a seperate\n> system that supports the offline realm as well. (THis is a design problem)\n>\n> Ultimately, quite literally such a transaction system could exist to\n> provide detailed analysis that promotes integrity being the basis for\n> sharing information.  The fee structure would be eliminated, due to the\n> level of integrity and procesing power to have messages and transactions\n> and reviews of unfiduciary responsible orgnizations be merited as highly\n> true (.9 in fizzy logic) in order to promote a well-being in the state.\n> That is its own reward, the strenght of having more processing speed.\n>\n>\n> FYI(thank you to peter whom nudged my thinking and interest (again) in\n> this area. )\n>\n> This is something I am attempting to design in order to program it. Though\n> I am not an expert and my technology stack is limited to java and c (and my\n> issues from it).  I provided a class the other day the was pseudo code for\n> the beginning of the consensus. Now I might to now if I am missing any of\n> teh technical paradigms that might make this illogical? I now with the\n> advent of 7petabyte computers one could easily store 2.5 petabytes of human\n> information for just an instance of integrity not to mention otehr\n> emotions.\n>\n>\n>\n> *Also, might someone be able to provide a bit of information on Bitcoin\n> core project?*\n>\n> thank you again. Damain.\n>\n> On Mon, May 11, 2015 at 10:29 AM, <\n> bitcoin-development-request at lists.sourceforge.net> wrote:\n>\n>> Send Bitcoin-development mailing list submissions to\n>>         bitcoin-development at lists.sourceforge.net\n>>\n>> To subscribe or unsubscribe via the World Wide Web, visit\n>>         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>> or, via email, send a message with subject or body 'help' to\n>>         bitcoin-development-request at lists.sourceforge.net\n>>\n>> You can reach the person managing the list at\n>>         bitcoin-development-owner at lists.sourceforge.net\n>>\n>> When replying, please edit your Subject line so it is more specific\n>> than \"Re: Contents of Bitcoin-development digest...\"\n>>\n>> Today's Topics:\n>>\n>>    1. Fwd:  Bitcoin core 0.11 planning (Wladimir)\n>>    2. Re: Bitcoin core 0.11 planning (Wladimir)\n>>    3. Long-term mining incentives (Thomas Voegtlin)\n>>    4. Re: Long-term mining incentives\n>>       (insecurity at national.shitposting.agency)\n>>    5. Re: Reducing the block rate instead of    increasing the maximum\n>>       block size (Luke Dashjr)\n>>    6. Re: Long-term mining incentives (Gavin Andresen)\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Wladimir <laanwj at gmail.com>\n>> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>> Cc:\n>> Date: Mon, 11 May 2015 14:49:53 +0000\n>> Subject: [Bitcoin-development] Fwd: Bitcoin core 0.11 planning\n>> On Tue, Apr 28, 2015 at 11:01 AM, Pieter Wuille <pieter.wuille at gmail.com>\n>> wrote:\n>> > As softforks almost certainly require backports to older releases and\n>> other\n>> > software anyway, I don't think they should necessarily be bound to\n>> Bitcoin\n>> > Core major releases. If they don't require large code changes, we can\n>> easily\n>> > do them in minor releases too.\n>>\n>> Agree here - there is no need to time consensus changes with a major\n>> release, as they need to be ported back to older releases anyhow.\n>> (I don't really classify them as software features, but properties of\n>> the underlying system that we need to adopt to)\n>>\n>> Wladimir\n>>\n>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Wladimir <laanwj at gmail.com>\n>> To: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>> Cc:\n>> Date: Mon, 11 May 2015 15:00:03 +0000\n>> Subject: Re: [Bitcoin-development] Bitcoin core 0.11 planning\n>> A reminder - feature freeze and string freeze is coming up this Friday\n>> the 15th.\n>>\n>> Let me know if your pull request is ready to be merged before then,\n>>\n>> Wladimir\n>>\n>> On Tue, Apr 28, 2015 at 7:44 AM, Wladimir J. van der Laan\n>> <laanwj at gmail.com> wrote:\n>> > Hello all,\n>> >\n>> > The release window for 0.11 is nearing, I'd propose the following\n>> schedule:\n>> >\n>> > 2015-05-01  Soft translation string freeze\n>> >             Open Transifex translations for 0.11\n>> >             Finalize and close translation for 0.9\n>> >\n>> > 2015-05-15  Feature freeze, string freeze\n>> >\n>> > 2015-06-01  Split off 0.11 branch\n>> >             Tag and release 0.11.0rc1\n>> >             Start merging for 0.12 on master branch\n>> >\n>> > 2015-07-01  Release 0.11.0 final (aim)\n>> >\n>> > In contrast to former releases, which were protracted for months, let's\n>> try to be more strict about the dates. Of course it is always possible for\n>> last-minute critical issues to interfere with the planning. The release\n>> will not be held up for features, though, and anything that will not make\n>> it to 0.11 will be postponed to next release scheduled for end of the year.\n>> >\n>> > Wladimir\n>>\n>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Thomas Voegtlin <thomasv at electrum.org>\n>> To: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n>> Cc:\n>> Date: Mon, 11 May 2015 18:28:46 +0200\n>> Subject: [Bitcoin-development] Long-term mining incentives\n>> The discussion on block size increase has brought some attention to the\n>> other elephant in the room: Long-term mining incentives.\n>>\n>> Bitcoin derives its current market value from the assumption that a\n>> stable, steady-state regime will be reached in the future, where miners\n>> have an incentive to keep mining to protect the network. Such a steady\n>> state regime does not exist today, because miners get most of their\n>> reward from the block subsidy, which will progressively be removed.\n>>\n>> Thus, today's 3 billion USD question is the following: Will a steady\n>> state regime be reached in the future? Can such a regime exist? What are\n>> the necessary conditions for its existence?\n>>\n>> Satoshi's paper suggests that this may be achieved through miner fees.\n>> Quite a few people seem to take this for granted, and are working to\n>> make it happen (developing cpfp and replace-by-fee). This explains part\n>> of the opposition to raising the block size limit; some people would\n>> like to see some fee pressure building up first, in order to get closer\n>> to a regime where miners are incentivised by transaction fees instead of\n>> block subsidy. Indeed, the emergence of a working fee market would be\n>> extremely reassuring for the long-term viability of bitcoin. So, the\n>> thinking goes, by raising the block size limit, we would be postponing a\n>> crucial reality check. We would be buying time, at the expenses of\n>> Bitcoin's decentralization.\n>>\n>> OTOH, proponents of a block size increase have a very good point: if the\n>> block size is not raised soon, Bitcoin is going to enter a new, unknown\n>> and potentially harmful regime. In the current regime, almost all\n>> transaction get confirmed quickly, and fee pressure does not exist. Mike\n>> Hearn suggested that, when blocks reach full capacity and users start to\n>> experience confirmation delays and confirmation uncertainty, users will\n>> simply go away and stop using Bitcoin. To me, that outcome sounds very\n>> plausible indeed. Thus, proponents of the block size increase are\n>> conservative; they are trying to preserve the current regime, which is\n>> known to work, instead of letting the network enter uncharted territory.\n>>\n>> My problem is that this seems to lacks a vision. If the maximal block\n>> size is increased only to buy time, or because some people think that 7\n>> tps is not enough to compete with VISA, then I guess it would be\n>> healthier to try and develop off-chain infrastructure first, such as the\n>> Lightning network.\n>>\n>> OTOH, I also fail to see evidence that a limited block capacity will\n>> lead to a functional fee market, able to sustain a steady state. A\n>> functional market requires well-informed participants who make rational\n>> choices and accept the outcomes of their choices. That is not the case\n>> today, and to believe that it will magically happen because blocks start\n>> to reach full capacity sounds a lot like like wishful thinking.\n>>\n>> So here is my question, to both proponents and opponents of a block size\n>> increase: What steady-state regime do you envision for Bitcoin, and what\n>> is is your plan to get there? More specifically, how will the\n>> steady-state regime look like? Will users experience fee pressure and\n>> delays, or will it look more like a scaled up version of what we enjoy\n>> today? Should fee pressure be increased jointly with subsidy decrease,\n>> or as soon as possible, or never? What incentives will exist for miners\n>> once the subsidy is gone? Will miners have an incentive to permanently\n>> fork off the last block and capture its fees? Do you expect Bitcoin to\n>> work because miners are altruistic/selfish/honest/caring?\n>>\n>> A clear vision would be welcome.\n>>\n>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: insecurity at national.shitposting.agency\n>> To: thomasv at electrum.org\n>> Cc: bitcoin-development at lists.sourceforge.net\n>> Date: Mon, 11 May 2015 16:52:10 +0000\n>> Subject: Re: [Bitcoin-development] Long-term mining incentives\n>> On 2015-05-11 16:28, Thomas Voegtlin wrote:\n>>\n>>> My problem is that this seems to lacks a vision. If the maximal block\n>>> size is increased only to buy time, or because some people think that 7\n>>> tps is not enough to compete with VISA, then I guess it would be\n>>> healthier to try and develop off-chain infrastructure first, such as the\n>>> Lightning network.\n>>>\n>>\n>> If your end goal is \"compete with VISA\" you might as well just give up\n>> and go home right now. There's lots of terrible proposals where people\n>> try to demonstrate that so many hundred thousand transactions a second\n>> are possible if we just make the block size 500GB. In the real world\n>> with physical limits, you literally can not verify more than a few\n>> thousand ECDSA signatures a second on a CPU core. The tradeoff taken\n>> in Bitcoin is that the signatures are pretty small, but they are also\n>> slow to verify on any sort of scale. There's no way competing with a\n>> centralised entity using on-chain transactions is even a sane goal.\n>>\n>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Luke Dashjr <luke at dashjr.org>\n>> To: bitcoin-development at lists.sourceforge.net\n>> Cc:\n>> Date: Mon, 11 May 2015 16:47:47 +0000\n>> Subject: Re: [Bitcoin-development] Reducing the block rate instead of\n>> increasing the maximum block size\n>> On Monday, May 11, 2015 7:03:29 AM Sergio Lerner wrote:\n>> > 1. It will encourage centralization, because participants of mining\n>> > pools will loose more money because of excessive initial block template\n>> > latency, which leads to higher stale shares\n>> >\n>> > When a new block is solved, that information needs to propagate\n>> > throughout the Bitcoin network up to the mining pool operator nodes,\n>> > then a new block header candidate is created, and this header must be\n>> > propagated to all the mining pool users, ether by a push or a pull\n>> > model. Generally the mining server pushes new work units to the\n>> > individual miners. If done other way around, the server would need to\n>> > handle a high load of continuous work requests that would be difficult\n>> > to distinguish from a DDoS attack. So if the server pushes new block\n>> > header candidates to clients, then the problem boils down to increasing\n>> > bandwidth of the servers to achieve a tenfold increase in work\n>> > distribution. Or distributing the servers geographically to achieve a\n>> > lower latency. Propagating blocks does not require additional CPU\n>> > resources, so mining pools administrators would need to increase\n>> > moderately their investment in the server infrastructure to achieve\n>> > lower latency and higher bandwidth, but I guess the investment would be\n>> > low.\n>>\n>> 1. Latency is what matters here, not bandwidth so much. And latency\n>> reduction\n>> is either expensive or impossible.\n>> 2. Mining pools are mostly run at a loss (with exception to only the most\n>> centralised pools), and have nothing to invest in increasing\n>> infrastructure.\n>>\n>> > 3, It will reduce the security of the network\n>> >\n>> > The security of the network is based on two facts:\n>> > A- The miners are incentivized to extend the best chain\n>> > B- The probability of a reversal based on a long block competition\n>> > decreases as more confirmation blocks are appended.\n>> > C- Renting or buying hardware to perform a 51% attack is costly.\n>> >\n>> > A still holds. B holds for the same amount of confirmation blocks, so 6\n>> > confirmation blocks in a 10-minute block-chain is approximately\n>> > equivalent to 6 confirmation blocks in a 1-minute block-chain.\n>> > Only C changes, as renting the hashing power for 6 minutes is ten times\n>> > less expensive as renting it for 1 hour. However, there is no shop where\n>> > one can find 51% of the hashing power to rent right now, nor probably\n>> > will ever be if Bitcoin succeeds. Last, you can still have a 1 hour\n>> > confirmation (60 1-minute blocks) if you wish for high-valued payments,\n>> > so the security decreases only if participant wish to decrease it.\n>>\n>> You're overlooking at least:\n>> 1. The real network has to suffer wasted work as a result of the stale\n>> blocks,\n>> while an attacker does not. If 20% of blocks are stale, the attacker only\n>> needs 40% of the legitimate hashrate to achieve 50%-in-practice.\n>> 2. Since blocks are individually weaker, it becomes cheaper to DoS nodes\n>> with\n>> invalid blocks. (not sure if this is a real concern, but it ought to be\n>> considered and addressed)\n>>\n>> > 4. Reducing the block propagation time on the average case is good, but\n>> > what happen in the worse case?\n>> >\n>> > Most methods proposed to reduce the block propagation delay do it only\n>> > on the average case. Any kind of block compression relies on both\n>> > parties sharing some previous information. In the worse case it's true\n>> > that a miner can create and try to broadcast a block that takes too much\n>> > time to verify or bandwidth to transmit. This is currently true on the\n>> > Bitcoin network. Nevertheless there is no such incentive for miners,\n>> > since they will be shooting on their own foots. Peter Todd has argued\n>> > that the best strategy for miners is actually to reach 51% of the\n>> > network, but not more. In other words, to exclude the slowest 49%\n>> > percent. But this strategy of creating bloated blocks is too risky in\n>> > practice, and surely doomed to fail, as network conditions dynamically\n>> > change. Also it would be perceived as an attack to the network, and the\n>> > miner (if it is a public mining pool) would be probably blacklisted.\n>>\n>> One can probably overcome changing network conditions merely by trying to\n>> reach 75% and exclude the slowest 25%. Also, there is no way to identify\n>> or\n>> blacklist miners.\n>>\n>> > 5. Thousands of SPV wallets running in mobile devices would need to be\n>> > upgraded (thanks Mike).\n>> >\n>> > That depends on the current upgrade rate for SPV wallets like Bitcoin\n>> > Wallet  and BreadWallet. Suppose that the upgrade rate is 80%/year: we\n>> > develop the source code for the change now and apply the change in Q2\n>> > 2016, then  most of the nodes will already be upgraded by when the\n>> > hardfork takes place. Also a public notice telling people to upgrade in\n>> > web pages, bitcointalk, SPV wallets warnings, coindesk, one year in\n>> > advance will give plenty of time to SPV wallet users to upgrade.\n>>\n>> I agree this shouldn't be a real concern. SPV wallets are also more\n>> likely and\n>> less risky (globally) to be auto-updated.\n>>\n>> > 6. If there are 10x more blocks, then there are 10x more block headers,\n>> > and that increases the amount of bandwidth SPV wallets need to catch up\n>> > with the chain\n>> >\n>> > A standard smartphone with average cellular downstream speed downloads\n>> > 2.6 headers per second (1600 kbits/sec) [3], so if synchronization were\n>> > to be done only at night when the phone is connected to the power line,\n>> > then it would take 9 minutes to synchronize with 1440 headers/day. If a\n>> > person should accept a payment, and the smart-phone is 1 day\n>> > out-of-synch, then it takes less time to download all the missing\n>> > headers than to wait for a 10-minute one block confirmation. Obviously\n>> > all smartphones with 3G have a downstream bandwidth much higher,\n>> > averaging 1 Mbps. So the whole synchronization will be done less than a\n>> > 1-minute block confirmation.\n>>\n>> Uh, I think you need to be using at least median speeds. As an example, I\n>> can\n>> only sustain (over 3G) about 40 kbps, with a peak of around 400 kbps. 3G\n>> has\n>> worse range/coverage than 2G. No doubt the *average* is skewed so high\n>> because\n>> of densely populated areas like San Francisco having 400+ Mbps cellular\n>> data.\n>> It's not reasonable to assume sync only at night: most payments will be\n>> during\n>> the day, on battery - so increased power use must also be considered.\n>>\n>> > According to CISCO mobile bandwidth connection speed increases 20% every\n>> > year.\n>>\n>> Only in small densely populated areas of first-world countries.\n>>\n>> Luke\n>>\n>>\n>>\n>>\n>> ---------- Forwarded message ----------\n>> From: Gavin Andresen <gavinandresen at gmail.com>\n>> To: insecurity at national.shitposting.agency\n>> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>> Date: Mon, 11 May 2015 13:29:02 -0400\n>> Subject: Re: [Bitcoin-development] Long-term mining incentives\n>> I think long-term the chain will not be secured purely by proof-of-work.\n>> I think when the Bitcoin network was tiny running solely on people's home\n>> computers proof-of-work was the right way to secure the chain, and the only\n>> fair way to both secure the chain and distribute the coins.\n>>\n>> See https://gist.github.com/gavinandresen/630d4a6c24ac6144482a  for some\n>> half-baked thoughts along those lines. I don't think proof-of-work is the\n>> last word in distributed consensus (I also don't think any alternatives are\n>> anywhere near ready to deploy, but they might be in ten years).\n>>\n>> I also think it is premature to worry about what will happen in twenty or\n>> thirty years when the block subsidy is insignificant. A lot will happen in\n>> the next twenty years. I could spin a vision of what will secure the chain\n>> in twenty years, but I'd put a low probability on that vision actually\n>> turning out to be correct.\n>>\n>> That is why I keep saying Bitcoin is an experiment. But I also believe\n>> that the incentives are correct, and there are a lot of very motivated,\n>> smart, hard-working people who will make it work. When you're talking about\n>> trying to predict what will happen decades from now, I think that is the\n>> best you can (honestly) do.\n>>\n>> --\n>> --\n>> Gavin Andresen\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150511/8f777233/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin-development Digest, Vol 48, Issue 63",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Damian Gomez"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 24862
        }
    },
    {
        "title": "[Bitcoin-development] simplified costing analysis",
        "thread_messages": [
            {
                "author": "gb",
                "date": "2015-05-11T21:00:51",
                "message_text_only": "Hi,\n\nthe attached document is a simplified costing analysis that may serve a\nuseful approach for network scaling discussions.\n\nRegards.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: costing.pdf\nType: application/pdf\nSize: 112525 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/c0c16c3f/attachment.pdf>"
            }
        ],
        "thread_summary": {
            "title": "simplified costing analysis",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "gb"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 399
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin transaction",
        "thread_messages": [
            {
                "author": "Telephone Lemien",
                "date": "2015-05-12T09:45:11",
                "message_text_only": "Hello evry body,\nI want to know what is the difference between a bitcoin transaction and\ncolored coins transaction technically.\n\nThanks\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/638501ae/attachment.html>"
            },
            {
                "author": "Telephone Lemien",
                "date": "2015-05-12T09:54:04",
                "message_text_only": "Thank You,\nI know this, but I want to have mores details in the inputs/outputs, or in\nthe script of input/output and how i will proceed in the code.\nThanks for all replaying\n\n2015-05-12 11:47 GMT+02:00 Patrick Mccorry (PGR) <\npatrick.mccorry at newcastle.ac.uk>:\n\n>  There is no difference to the transaction as far as im aware \u2013 just the\n> inputs / outputs have a special meaning (and should have a special order).\n> So you can track 1 BTC throughout the blockchain and this 1 BTC represents\n> my asset. Someone may give a more useful answer.\n>\n>\n>\n> *From:* Telephone Lemien [mailto:lemientelephone at gmail.com]\n> *Sent:* 12 May 2015 10:45\n> *To:* Bitcoin Dev\n> *Subject:* [Bitcoin-development] Bitcoin transaction\n>\n>\n>\n> Hello evry body,\n>\n> I want to know what is the difference between a bitcoin transaction and\n> colored coins transaction technically.\n>\n> Thanks\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/f5200cd8/attachment.html>"
            },
            {
                "author": "Danny Thorpe",
                "date": "2015-05-12T18:40:31",
                "message_text_only": "See the Open Assets protocol specification for technical details on how a\ncolored coin (of the Open Asset flavor) is represented in a bitcoin\ntransaction.\n\nhttps://github.com/OpenAssets/open-assets-protocol\n\nhttp://www.CoinPrism.com also has a discussion forum where some colored\ncoin devs hang out.\n\nhttp://www.coinprism.info is a blockchain explorer that is colored-coin\naware.\n\nOn Tue, May 12, 2015 at 2:54 AM, Telephone Lemien <lemientelephone at gmail.com\n> wrote:\n\n> Thank You,\n> I know this, but I want to have mores details in the inputs/outputs, or in\n> the script of input/output and how i will proceed in the code.\n> Thanks for all replaying\n>\n> 2015-05-12 11:47 GMT+02:00 Patrick Mccorry (PGR) <\n> patrick.mccorry at newcastle.ac.uk>:\n>\n>>  There is no difference to the transaction as far as im aware \u2013 just the\n>> inputs / outputs have a special meaning (and should have a special order).\n>> So you can track 1 BTC throughout the blockchain and this 1 BTC represents\n>> my asset. Someone may give a more useful answer.\n>>\n>>\n>>\n>> *From:* Telephone Lemien [mailto:lemientelephone at gmail.com]\n>> *Sent:* 12 May 2015 10:45\n>> *To:* Bitcoin Dev\n>> *Subject:* [Bitcoin-development] Bitcoin transaction\n>>\n>>\n>>\n>> Hello evry body,\n>>\n>> I want to know what is the difference between a bitcoin transaction and\n>> colored coins transaction technically.\n>>\n>> Thanks\n>>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/09f3bae2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin transaction",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Danny Thorpe",
                "Telephone Lemien"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 3564
        }
    },
    {
        "title": "[Bitcoin-development] Proposed additional options for pruned nodes",
        "thread_messages": [
            {
                "author": "gabe appleton",
                "date": "2015-05-12T15:26:55",
                "message_text_only": "Hi,\n\nThere's been a lot of talk in the rest of the community about how the 20MB\nstep would increase storage needs, and that switching to pruned nodes\n(partially) would reduce network security. I think I may have a solution.\n\nThere could be a hybrid option in nodes. Selecting this would do the\nfollowing:\nFlip the --no-wallet toggle\nSelect a section of the blockchain to store fully (percentage based,\npossibly on hash % sections?)\nBegin pruning all sections not included in 2\nThe idea is that you can implement it similar to how a Koorde is done, in\nthat the network will decide which sections it retrieves. So if the user\nprompts it to store 50% of the blockchain, it would look at its peers, and\nat their peers (if secure), and choose the least-occurring options from\nthem.\n\nThis would allow them to continue validating all transactions, and still\nstore a full copy, just distributed among many nodes. It should overall\nhave little impact on security (unless I'm mistaken), and it would\nsignificantly reduce storage needs on a node.\n\nIt would also allow for a retroactive --max-size flag, where it will prune\nuntil it is at the specified size, and continue to prune over time, while\nkeeping to the sections defined by the network.\n\nWhat sort of side effects or network vulnerabilities would this introduce?\nI know some said it wouldn't be Sybil resistant, but how would this be less\nso than a fully pruned node?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/d448267a/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-12T16:05:44",
                "message_text_only": "A general assumption is that you will have a few archive nodes with the\nfull blockchain, and a majority of nodes are pruned, able to serve only the\ntail of the chains.\n\n\nOn Tue, May 12, 2015 at 8:26 AM, gabe appleton <gappleto97 at gmail.com> wrote:\n\n> Hi,\n>\n> There's been a lot of talk in the rest of the community about how the 20MB\n> step would increase storage needs, and that switching to pruned nodes\n> (partially) would reduce network security. I think I may have a solution.\n>\n> There could be a hybrid option in nodes. Selecting this would do the\n> following:\n> Flip the --no-wallet toggle\n> Select a section of the blockchain to store fully (percentage based,\n> possibly on hash % sections?)\n> Begin pruning all sections not included in 2\n> The idea is that you can implement it similar to how a Koorde is done, in\n> that the network will decide which sections it retrieves. So if the user\n> prompts it to store 50% of the blockchain, it would look at its peers, and\n> at their peers (if secure), and choose the least-occurring options from\n> them.\n>\n> This would allow them to continue validating all transactions, and still\n> store a full copy, just distributed among many nodes. It should overall\n> have little impact on security (unless I'm mistaken), and it would\n> significantly reduce storage needs on a node.\n>\n> It would also allow for a retroactive --max-size flag, where it will prune\n> until it is at the specified size, and continue to prune over time, while\n> keeping to the sections defined by the network.\n>\n> What sort of side effects or network vulnerabilities would this introduce?\n> I know some said it wouldn't be Sybil resistant, but how would this be less\n> so than a fully pruned node?\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/0ee91628/attachment.html>"
            },
            {
                "author": "gabe appleton",
                "date": "2015-05-12T16:56:37",
                "message_text_only": "Yes, but that just increases the incentive for partially-full nodes. It\nwould add to the assumed-small number of full nodes.\n\nOr am I misunderstanding?\n\nOn Tue, May 12, 2015 at 12:05 PM, Jeff Garzik <jgarzik at bitpay.com> wrote:\n\n> A general assumption is that you will have a few archive nodes with the\n> full blockchain, and a majority of nodes are pruned, able to serve only the\n> tail of the chains.\n>\n>\n> On Tue, May 12, 2015 at 8:26 AM, gabe appleton <gappleto97 at gmail.com>\n> wrote:\n>\n>> Hi,\n>>\n>> There's been a lot of talk in the rest of the community about how the\n>> 20MB step would increase storage needs, and that switching to pruned nodes\n>> (partially) would reduce network security. I think I may have a solution.\n>>\n>> There could be a hybrid option in nodes. Selecting this would do the\n>> following:\n>> Flip the --no-wallet toggle\n>> Select a section of the blockchain to store fully (percentage based,\n>> possibly on hash % sections?)\n>> Begin pruning all sections not included in 2\n>> The idea is that you can implement it similar to how a Koorde is done, in\n>> that the network will decide which sections it retrieves. So if the user\n>> prompts it to store 50% of the blockchain, it would look at its peers, and\n>> at their peers (if secure), and choose the least-occurring options from\n>> them.\n>>\n>> This would allow them to continue validating all transactions, and still\n>> store a full copy, just distributed among many nodes. It should overall\n>> have little impact on security (unless I'm mistaken), and it would\n>> significantly reduce storage needs on a node.\n>>\n>> It would also allow for a retroactive --max-size flag, where it will\n>> prune until it is at the specified size, and continue to prune over time,\n>> while keeping to the sections defined by the network.\n>>\n>> What sort of side effects or network vulnerabilities would this\n>> introduce? I know some said it wouldn't be Sybil resistant, but how would\n>> this be less so than a fully pruned node?\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> --\n> Jeff Garzik\n> Bitcoin core developer and open source evangelist\n> BitPay, Inc.      https://bitpay.com/\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/af7820e4/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-12T17:16:40",
                "message_text_only": "On Tue, May 12, 2015 at 09:05:44AM -0700, Jeff Garzik wrote:\n> A general assumption is that you will have a few archive nodes with the\n> full blockchain, and a majority of nodes are pruned, able to serve only the\n> tail of the chains.\n\nHmm?\n\nLots of people are tossing around ideas for partial archival nodes that\nwould store a subset of blocks, such that collectively the whole\nblockchain would be available even if no one node had the entire chain.\n\n-- \n'peter'[:-1]@petertodd.org\n0000000000000000156d2069eeebb3309455f526cfe50efbf8a85ec630df7f7c\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/ae0f4b3b/attachment.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-12T18:23:48",
                "message_text_only": "On Tue, May 12, 2015 at 6:16 PM, Peter Todd <pete at petertodd.org> wrote:\n\n>\n> Lots of people are tossing around ideas for partial archival nodes that\n> would store a subset of blocks, such that collectively the whole\n> blockchain would be available even if no one node had the entire chain.\n>\n\nA compact way to describe which blocks are stored helps to mitigate against\nfingerprint attacks.\n\nIt also means that a node could compactly indicate which blocks it stores\nwith service bits.\n\nThe node could pick two numbers\n\nW = window = a power of 2\nP = position = random value less than W\n\nThe node would store all blocks with a height of P mod W.  The block hash\ncould be used too.\n\nThis has the nice feature that the node can throw away half of its data and\nstill represent what is stored.\n\nW_new = W * 2\nP_new = (random_bool()) ? P + W/2 : P;\n\nHalf of the stored blocks would match P_new mod W_new and the other half\ncould be deleted.  This means that the store would use up between 50% and\n100% of the allocated size.\n\nAnother benefit is that it increases the probability that at least someone\nhas every block.\n\nIf N nodes each store 1% of the blocks, then the odds of a block being\nstored is pow(0.99, N).  For 1000 nodes, that gives odds of 1 in 23,164\nthat a block will be missing.  That means that around 13 out of 300,000\nblocks would be missing.  There would likely be more nodes than that, and\nalso storage nodes, so it is not a major risk.\n\nIf everyone is storing 1% of blocks, then they would set W to 128.  As long\nas all of the 128 buckets is covered by some nodes, then all blocks are\nstored.  With 1000 nodes, that gives odds of 0.6% that at least one bucket\nwill be missed.  That is better than around 13 blocks being missing.\n\nNodes could inform peers of their W and P parameters on connection.  The\nversion message could be amended or a \"getparams\" message of some kind\ncould be added.\n\nW could be encoded with 4 bits and P could be encoded with 16 bits, for 20\nin total.  W = 1 << bits[19:16] and P = bits[14:0].  That gives a maximum W\nof 32768, which is likely to many bits for P.\n\nInitial download would be harder, since new nodes would have to connect to\nat least 100 different nodes.  They could download from random nodes, and\njust download the ones they are missing from storage nodes.  Even storage\nnodes could have a range of W values.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/52cee4d6/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-12T19:03:55",
                "message_text_only": "It's a little frustrating to see this just repeated without even\npaying attention to the desirable characteristics from the prior\ndiscussions.\n\nSummarizing from memory:\n\n(0) Block coverage should have locality; historical blocks are\n(almost) always needed in contiguous ranges.   Having random peers\nwith totally random blocks would be horrific for performance; as you'd\nhave to hunt down a working peer and make a connection for each block\nwith high probability.\n\n(1) Block storage on nodes with a fraction of the history should not\ndepend on believing random peers; because listening to peers can\neasily create attacks (e.g. someone could break the network; by\nconvincing nodes to become unbalanced) and not useful-- it's not like\nthe blockchain is substantially different for anyone; if you're to the\npoint of needing to know coverage to fill then something is wrong.\nGaps would be handled by archive nodes, so there is no reason to\nincrease vulnerability by doing anything but behaving uniformly.\n\n(2) The decision to contact a node should need O(1) communications,\nnot just because of the delay of chasing around just to find who has\nsomeone; but because that chasing process usually makes the process\n_highly_ sybil vulnerable.\n\n(3) The expression of what blocks a node has should be compact (e.g.\nnot a dense list of blocks) so it can be rumored efficiently.\n\n(4) Figuring out what block (ranges) a peer has given should be\ncomputationally efficient.\n\n(5) The communication about what blocks a node has should be compact.\n\n(6) The coverage created by the network should be uniform, and should\nremain uniform as the blockchain grows; ideally it you shouldn't need\nto update your state to know what blocks a peer will store in the\nfuture, assuming that it doesn't change the amount of data its\nplanning to use. (What Tier Nolan proposes sounds like it fails this\npoint)\n\n(7) Growth of the blockchain shouldn't cause much (or any) need to\nrefetch old blocks.\n\nI've previously proposed schemes which come close but fail one of the above.\n\n(e.g. a scheme based on reservoir sampling that gives uniform\nselection of contiguous ranges, communicating only 64 bits of data to\nknow what blocks a node claims to have, remaining totally uniform as\nthe chain grows, without any need to refetch -- but needs O(height)\nwork to figure out what blocks a peer has from the data it\ncommunicated.;   or another scheme based on consistent hashes that has\nlog(height) computation; but sometimes may result in a node needing to\ngo refetch an old block range it previously didn't store-- creating\nre-balancing traffic.)\n\nSo far something that meets all those criteria (and/or whatever ones\nI'm not remembering) has not been discovered; but I don't really think\nmuch time has been spent on it. I think its very likely possible."
            },
            {
                "author": "gabe appleton",
                "date": "2015-05-12T19:24:20",
                "message_text_only": "0, 1, 3, 4, 5, 6 can be solved by looking at chunks chronologically. Ie,\ngive the signed (by sender) hash of the first and last block in your range.\nThis is less data dense than the idea above, but it might work better.\n\nThat said, this is likely a less secure way to do it. To improve upon that,\na node could request a block of random height within that range and verify\nit, but that violates point 2. And the scheme in itself definitely violates\npoint 7.\nOn May 12, 2015 3:07 PM, \"Gregory Maxwell\" <gmaxwell at gmail.com> wrote:\n\n> It's a little frustrating to see this just repeated without even\n> paying attention to the desirable characteristics from the prior\n> discussions.\n>\n> Summarizing from memory:\n>\n> (0) Block coverage should have locality; historical blocks are\n> (almost) always needed in contiguous ranges.   Having random peers\n> with totally random blocks would be horrific for performance; as you'd\n> have to hunt down a working peer and make a connection for each block\n> with high probability.\n>\n> (1) Block storage on nodes with a fraction of the history should not\n> depend on believing random peers; because listening to peers can\n> easily create attacks (e.g. someone could break the network; by\n> convincing nodes to become unbalanced) and not useful-- it's not like\n> the blockchain is substantially different for anyone; if you're to the\n> point of needing to know coverage to fill then something is wrong.\n> Gaps would be handled by archive nodes, so there is no reason to\n> increase vulnerability by doing anything but behaving uniformly.\n>\n> (2) The decision to contact a node should need O(1) communications,\n> not just because of the delay of chasing around just to find who has\n> someone; but because that chasing process usually makes the process\n> _highly_ sybil vulnerable.\n>\n> (3) The expression of what blocks a node has should be compact (e.g.\n> not a dense list of blocks) so it can be rumored efficiently.\n>\n> (4) Figuring out what block (ranges) a peer has given should be\n> computationally efficient.\n>\n> (5) The communication about what blocks a node has should be compact.\n>\n> (6) The coverage created by the network should be uniform, and should\n> remain uniform as the blockchain grows; ideally it you shouldn't need\n> to update your state to know what blocks a peer will store in the\n> future, assuming that it doesn't change the amount of data its\n> planning to use. (What Tier Nolan proposes sounds like it fails this\n> point)\n>\n> (7) Growth of the blockchain shouldn't cause much (or any) need to\n> refetch old blocks.\n>\n> I've previously proposed schemes which come close but fail one of the\n> above.\n>\n> (e.g. a scheme based on reservoir sampling that gives uniform\n> selection of contiguous ranges, communicating only 64 bits of data to\n> know what blocks a node claims to have, remaining totally uniform as\n> the chain grows, without any need to refetch -- but needs O(height)\n> work to figure out what blocks a peer has from the data it\n> communicated.;   or another scheme based on consistent hashes that has\n> log(height) computation; but sometimes may result in a node needing to\n> go refetch an old block range it previously didn't store-- creating\n> re-balancing traffic.)\n>\n> So far something that meets all those criteria (and/or whatever ones\n> I'm not remembering) has not been discovered; but I don't really think\n> much time has been spent on it. I think its very likely possible.\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/e8557e1a/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-12T19:38:20",
                "message_text_only": "One general problem is that security is weakened when an attacker can DoS a\nsmall part of the chain by DoS'ing a small number of nodes - yet the impact\nis a network-wide DoS because nobody can complete a sync.\n\n\nOn Tue, May 12, 2015 at 12:24 PM, gabe appleton <gappleto97 at gmail.com>\nwrote:\n\n> 0, 1, 3, 4, 5, 6 can be solved by looking at chunks chronologically. Ie,\n> give the signed (by sender) hash of the first and last block in your range.\n> This is less data dense than the idea above, but it might work better.\n>\n> That said, this is likely a less secure way to do it. To improve upon\n> that, a node could request a block of random height within that range and\n> verify it, but that violates point 2. And the scheme in itself definitely\n> violates point 7.\n> On May 12, 2015 3:07 PM, \"Gregory Maxwell\" <gmaxwell at gmail.com> wrote:\n>\n>> It's a little frustrating to see this just repeated without even\n>> paying attention to the desirable characteristics from the prior\n>> discussions.\n>>\n>> Summarizing from memory:\n>>\n>> (0) Block coverage should have locality; historical blocks are\n>> (almost) always needed in contiguous ranges.   Having random peers\n>> with totally random blocks would be horrific for performance; as you'd\n>> have to hunt down a working peer and make a connection for each block\n>> with high probability.\n>>\n>> (1) Block storage on nodes with a fraction of the history should not\n>> depend on believing random peers; because listening to peers can\n>> easily create attacks (e.g. someone could break the network; by\n>> convincing nodes to become unbalanced) and not useful-- it's not like\n>> the blockchain is substantially different for anyone; if you're to the\n>> point of needing to know coverage to fill then something is wrong.\n>> Gaps would be handled by archive nodes, so there is no reason to\n>> increase vulnerability by doing anything but behaving uniformly.\n>>\n>> (2) The decision to contact a node should need O(1) communications,\n>> not just because of the delay of chasing around just to find who has\n>> someone; but because that chasing process usually makes the process\n>> _highly_ sybil vulnerable.\n>>\n>> (3) The expression of what blocks a node has should be compact (e.g.\n>> not a dense list of blocks) so it can be rumored efficiently.\n>>\n>> (4) Figuring out what block (ranges) a peer has given should be\n>> computationally efficient.\n>>\n>> (5) The communication about what blocks a node has should be compact.\n>>\n>> (6) The coverage created by the network should be uniform, and should\n>> remain uniform as the blockchain grows; ideally it you shouldn't need\n>> to update your state to know what blocks a peer will store in the\n>> future, assuming that it doesn't change the amount of data its\n>> planning to use. (What Tier Nolan proposes sounds like it fails this\n>> point)\n>>\n>> (7) Growth of the blockchain shouldn't cause much (or any) need to\n>> refetch old blocks.\n>>\n>> I've previously proposed schemes which come close but fail one of the\n>> above.\n>>\n>> (e.g. a scheme based on reservoir sampling that gives uniform\n>> selection of contiguous ranges, communicating only 64 bits of data to\n>> know what blocks a node claims to have, remaining totally uniform as\n>> the chain grows, without any need to refetch -- but needs O(height)\n>> work to figure out what blocks a peer has from the data it\n>> communicated.;   or another scheme based on consistent hashes that has\n>> log(height) computation; but sometimes may result in a node needing to\n>> go refetch an old block range it previously didn't store-- creating\n>> re-balancing traffic.)\n>>\n>> So far something that meets all those criteria (and/or whatever ones\n>> I'm not remembering) has not been discovered; but I don't really think\n>> much time has been spent on it. I think its very likely possible.\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/e498ae86/attachment.html>"
            },
            {
                "author": "gabe appleton",
                "date": "2015-05-12T19:43:45",
                "message_text_only": "Yet this holds true in our current assumptions of the network as well: that\nit will become a collection of pruned nodes with a few storage nodes.\n\nA hybrid option makes this better, because it spreads the risk, rather than\nconcentrating it in full nodes.\nOn May 12, 2015 3:38 PM, \"Jeff Garzik\" <jgarzik at bitpay.com> wrote:\n\n> One general problem is that security is weakened when an attacker can DoS\n> a small part of the chain by DoS'ing a small number of nodes - yet the\n> impact is a network-wide DoS because nobody can complete a sync.\n>\n>\n> On Tue, May 12, 2015 at 12:24 PM, gabe appleton <gappleto97 at gmail.com>\n> wrote:\n>\n>> 0, 1, 3, 4, 5, 6 can be solved by looking at chunks chronologically. Ie,\n>> give the signed (by sender) hash of the first and last block in your range.\n>> This is less data dense than the idea above, but it might work better.\n>>\n>> That said, this is likely a less secure way to do it. To improve upon\n>> that, a node could request a block of random height within that range and\n>> verify it, but that violates point 2. And the scheme in itself definitely\n>> violates point 7.\n>> On May 12, 2015 3:07 PM, \"Gregory Maxwell\" <gmaxwell at gmail.com> wrote:\n>>\n>>> It's a little frustrating to see this just repeated without even\n>>> paying attention to the desirable characteristics from the prior\n>>> discussions.\n>>>\n>>> Summarizing from memory:\n>>>\n>>> (0) Block coverage should have locality; historical blocks are\n>>> (almost) always needed in contiguous ranges.   Having random peers\n>>> with totally random blocks would be horrific for performance; as you'd\n>>> have to hunt down a working peer and make a connection for each block\n>>> with high probability.\n>>>\n>>> (1) Block storage on nodes with a fraction of the history should not\n>>> depend on believing random peers; because listening to peers can\n>>> easily create attacks (e.g. someone could break the network; by\n>>> convincing nodes to become unbalanced) and not useful-- it's not like\n>>> the blockchain is substantially different for anyone; if you're to the\n>>> point of needing to know coverage to fill then something is wrong.\n>>> Gaps would be handled by archive nodes, so there is no reason to\n>>> increase vulnerability by doing anything but behaving uniformly.\n>>>\n>>> (2) The decision to contact a node should need O(1) communications,\n>>> not just because of the delay of chasing around just to find who has\n>>> someone; but because that chasing process usually makes the process\n>>> _highly_ sybil vulnerable.\n>>>\n>>> (3) The expression of what blocks a node has should be compact (e.g.\n>>> not a dense list of blocks) so it can be rumored efficiently.\n>>>\n>>> (4) Figuring out what block (ranges) a peer has given should be\n>>> computationally efficient.\n>>>\n>>> (5) The communication about what blocks a node has should be compact.\n>>>\n>>> (6) The coverage created by the network should be uniform, and should\n>>> remain uniform as the blockchain grows; ideally it you shouldn't need\n>>> to update your state to know what blocks a peer will store in the\n>>> future, assuming that it doesn't change the amount of data its\n>>> planning to use. (What Tier Nolan proposes sounds like it fails this\n>>> point)\n>>>\n>>> (7) Growth of the blockchain shouldn't cause much (or any) need to\n>>> refetch old blocks.\n>>>\n>>> I've previously proposed schemes which come close but fail one of the\n>>> above.\n>>>\n>>> (e.g. a scheme based on reservoir sampling that gives uniform\n>>> selection of contiguous ranges, communicating only 64 bits of data to\n>>> know what blocks a node claims to have, remaining totally uniform as\n>>> the chain grows, without any need to refetch -- but needs O(height)\n>>> work to figure out what blocks a peer has from the data it\n>>> communicated.;   or another scheme based on consistent hashes that has\n>>> log(height) computation; but sometimes may result in a node needing to\n>>> go refetch an old block range it previously didn't store-- creating\n>>> re-balancing traffic.)\n>>>\n>>> So far something that meets all those criteria (and/or whatever ones\n>>> I'm not remembering) has not been discovered; but I don't really think\n>>> much time has been spent on it. I think its very likely possible.\n>>>\n>>>\n>>> ------------------------------------------------------------------------------\n>>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>>> Widest out-of-the-box monitoring support with 50+ applications\n>>> Performance metrics, stats and reports that give you Actionable Insights\n>>> Deep dive visibility with transaction tracing using APM Insight.\n>>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>>> _______________________________________________\n>>> Bitcoin-development mailing list\n>>> Bitcoin-development at lists.sourceforge.net\n>>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> --\n> Jeff Garzik\n> Bitcoin core developer and open source evangelist\n> BitPay, Inc.      https://bitpay.com/\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/1ba83e55/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-12T20:02:36",
                "message_text_only": "On Tue, May 12, 2015 at 7:38 PM, Jeff Garzik <jgarzik at bitpay.com> wrote:\n> One general problem is that security is weakened when an attacker can DoS a\n> small part of the chain by DoS'ing a small number of nodes - yet the impact\n> is a network-wide DoS because nobody can complete a sync.\n\nIt might be more interesting to think of that attack as a bandwidth\nexhaustion DOS attack on the archive nodes... if you can't get a copy\nwithout them, thats where you'll go.\n\nSo the question arises: does the option make some nodes that would\nhave been archive not be? Probably some-- but would it do so much that\nit would offset the gain of additional copies of the data when those\nattacks are not going no. I suspect not.\n\nIt's also useful to give people incremental ways to participate even\nwhen they can't swollow the whole pill; or choose to provide the\nresource thats cheap for them to provide.  In particular, if there is\nonly two kinds of full nodes-- archive and pruned; then the archive\nnodes take both a huge disk and bandwidth cost; where as if there are\nfractional then archives take low(er) bandwidth unless the fractionals\nget DOS attacked."
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-12T20:10:56",
                "message_text_only": "True.  Part of the issue rests on the block sync horizon/cliff.  There is a\nvalue X which is the average number of blocks the 90th percentile of nodes\nneed in order to sync.  It is sufficient for the [semi-]pruned nodes to\nkeep X blocks, after which nodes must fall back to archive nodes for older\ndata.\n\nThere is simply far, far more demand for recent blocks, and the demand for\nold blocks very rapidly falls off.\n\nThere was even a more radical suggestion years ago - refuse to sync if too\nold (>2 weeks?), and force the user to download ancient data via torrent.\n\n\n\nOn Tue, May 12, 2015 at 1:02 PM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n\n> On Tue, May 12, 2015 at 7:38 PM, Jeff Garzik <jgarzik at bitpay.com> wrote:\n> > One general problem is that security is weakened when an attacker can\n> DoS a\n> > small part of the chain by DoS'ing a small number of nodes - yet the\n> impact\n> > is a network-wide DoS because nobody can complete a sync.\n>\n> It might be more interesting to think of that attack as a bandwidth\n> exhaustion DOS attack on the archive nodes... if you can't get a copy\n> without them, thats where you'll go.\n>\n> So the question arises: does the option make some nodes that would\n> have been archive not be? Probably some-- but would it do so much that\n> it would offset the gain of additional copies of the data when those\n> attacks are not going no. I suspect not.\n>\n> It's also useful to give people incremental ways to participate even\n> when they can't swollow the whole pill; or choose to provide the\n> resource thats cheap for them to provide.  In particular, if there is\n> only two kinds of full nodes-- archive and pruned; then the archive\n> nodes take both a huge disk and bandwidth cost; where as if there are\n> fractional then archives take low(er) bandwidth unless the fractionals\n> get DOS attacked.\n>\n\n\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/091723db/attachment.html>"
            },
            {
                "author": "gabe appleton",
                "date": "2015-05-12T20:41:10",
                "message_text_only": "I suppose this begs two questions:\n\n1) why not have a partial archive store the most recent X% of the\nblockchain by default?\n\n2) why not include some sort of torrent in QT, to mitigate this risk? I\ndon't think this is necessarily a good idea, but I'd like to hear the\nreasoning.\nOn May 12, 2015 4:11 PM, \"Jeff Garzik\" <jgarzik at bitpay.com> wrote:\n\n> True.  Part of the issue rests on the block sync horizon/cliff.  There is\n> a value X which is the average number of blocks the 90th percentile of\n> nodes need in order to sync.  It is sufficient for the [semi-]pruned nodes\n> to keep X blocks, after which nodes must fall back to archive nodes for\n> older data.\n>\n> There is simply far, far more demand for recent blocks, and the demand for\n> old blocks very rapidly falls off.\n>\n> There was even a more radical suggestion years ago - refuse to sync if too\n> old (>2 weeks?), and force the user to download ancient data via torrent.\n>\n>\n>\n> On Tue, May 12, 2015 at 1:02 PM, Gregory Maxwell <gmaxwell at gmail.com>\n> wrote:\n>\n>> On Tue, May 12, 2015 at 7:38 PM, Jeff Garzik <jgarzik at bitpay.com> wrote:\n>> > One general problem is that security is weakened when an attacker can\n>> DoS a\n>> > small part of the chain by DoS'ing a small number of nodes - yet the\n>> impact\n>> > is a network-wide DoS because nobody can complete a sync.\n>>\n>> It might be more interesting to think of that attack as a bandwidth\n>> exhaustion DOS attack on the archive nodes... if you can't get a copy\n>> without them, thats where you'll go.\n>>\n>> So the question arises: does the option make some nodes that would\n>> have been archive not be? Probably some-- but would it do so much that\n>> it would offset the gain of additional copies of the data when those\n>> attacks are not going no. I suspect not.\n>>\n>> It's also useful to give people incremental ways to participate even\n>> when they can't swollow the whole pill; or choose to provide the\n>> resource thats cheap for them to provide.  In particular, if there is\n>> only two kinds of full nodes-- archive and pruned; then the archive\n>> nodes take both a huge disk and bandwidth cost; where as if there are\n>> fractional then archives take low(er) bandwidth unless the fractionals\n>> get DOS attacked.\n>>\n>\n>\n>\n> --\n> Jeff Garzik\n> Bitcoin core developer and open source evangelist\n> BitPay, Inc.      https://bitpay.com/\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/01931028/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-12T20:47:41",
                "message_text_only": "On Tue, May 12, 2015 at 8:10 PM, Jeff Garzik <jgarzik at bitpay.com> wrote:\n> True.  Part of the issue rests on the block sync horizon/cliff.  There is a\n> value X which is the average number of blocks the 90th percentile of nodes\n> need in order to sync.  It is sufficient for the [semi-]pruned nodes to keep\n> X blocks, after which nodes must fall back to archive nodes for older data.\n\n\nPrior discussion had things like \"the definition of pruned means you\nhave and will serve at least the last 288 from your tip\" (which is\nwhat I put in the pruned service bip text); and another flag for \"I\nhave at least the last 2016\".  (2016 should be reevaluated-- it was\njust a round number near where sipa's old data showed the fetch\nprobability flatlined.\n\nBut that data was old,  but what it showed that the probability of a\nblock being fetched vs depth looked like a exponential drop-off (I\nthink with a 50% at 3-ish days); plus a constant low probability.\nWhich is probably what we should have expected.\n\n> There was even a more radical suggestion years ago - refuse to sync if too\n> old (>2 weeks?), and force the user to download ancient data via torrent.\n\nI'm not fond of this; it makes the system dependent on centralized\nservices (e.g. trackers and sources of torrents). A torrent also\ncannot very efficiently handle fractional copies; cannot efficiently\ngrow over time. Bitcoin should be complete-- plus, many nodes already\nhave the data."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-12T22:00:33",
                "message_text_only": "On Tue, May 12, 2015 at 8:03 PM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n\n>\n> (0) Block coverage should have locality; historical blocks are\n> (almost) always needed in contiguous ranges.   Having random peers\n> with totally random blocks would be horrific for performance; as you'd\n> have to hunt down a working peer and make a connection for each block\n> with high probability.\n>\n> (1) Block storage on nodes with a fraction of the history should not\n> depend on believing random peers; because listening to peers can\n> easily create attacks (e.g. someone could break the network; by\n> convincing nodes to become unbalanced) and not useful-- it's not like\n> the blockchain is substantially different for anyone; if you're to the\n> point of needing to know coverage to fill then something is wrong.\n> Gaps would be handled by archive nodes, so there is no reason to\n> increase vulnerability by doing anything but behaving uniformly.\n>\n> (2) The decision to contact a node should need O(1) communications,\n> not just because of the delay of chasing around just to find who has\n> someone; but because that chasing process usually makes the process\n> _highly_ sybil vulnerable.\n>\n> (3) The expression of what blocks a node has should be compact (e.g.\n> not a dense list of blocks) so it can be rumored efficiently.\n>\n> (4) Figuring out what block (ranges) a peer has given should be\n> computationally efficient.\n>\n> (5) The communication about what blocks a node has should be compact.\n>\n> (6) The coverage created by the network should be uniform, and should\n> remain uniform as the blockchain grows; ideally it you shouldn't need\n> to update your state to know what blocks a peer will store in the\n> future, assuming that it doesn't change the amount of data its\n> planning to use. (What Tier Nolan proposes sounds like it fails this\n> point)\n>\n> (7) Growth of the blockchain shouldn't cause much (or any) need to\n> refetch old blocks.\n>\n\nM = 1,000,000\nN = number of \"starts\"\n\nS(0) = hash(seed) mod M\n...\nS(n) = hash(S(n-1)) mod M\n\nThis generates a sequence of start points.  If the start point is less than\nthe block height, then it counts as a hit.\n\nThe node stores the 50MB of data starting at the block at height S(n).\n\nAs the blockchain increases in size, new starts will be less than the block\nheight.  This means some other runs would be deleted.\n\nA weakness is that it is random with regards to block heights.  Tiny blocks\nhave the same priority as larger blocks.\n\n0) Blocks are local, in 50MB runs\n1) Agreed, nodes should download headers-first (or some other compact way\nof finding the highest POW chain)\n2) M could be fixed, N and the seed are all that is required.  The seed\ndoesn't have to be that large.  If 1% of the blockchain is stored, then 16\nbits should be sufficient so that every block is covered by seeds.\n3) N is likely to be less than 2 bytes and the seed can be 2 bytes\n4) A 1% cover of 50GB of blockchain would have 10 starts @ 50MB per run.\nThat is 10 hashes.  They don't even necessarily need to be crypt hashes\n5) Isn't this the same as 3?\n6) Every block has the same odds of being included.  There inherently needs\nto be an update when a node deletes some info due to exceeding its cap.  N\ncan be dropped one run at a time.\n7) When new starts drop below the tip height, N can be decremented and that\none run is deleted.\n\nThere would need to be a special rule to ensure the low height blocks are\ncovered.  Nodes should keep the first 50MB of blocks with some probability\n(10%?)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/27f2d685/attachment.html>"
            },
            {
                "author": "gabe appleton",
                "date": "2015-05-12T22:09:44",
                "message_text_only": "This is exactly the sort of solution I was hoping for. It seems this is the\nminimal modification to make it work, and, if someone was willing to work\nwith me, I would love to help implement this.\n\nMy only concern would be if the - - max-size flag is not included than this\ndelivers significantly less benefit to the end user. Still a good chunk,\nbut possibly not enough.\nOn May 12, 2015 6:03 PM, \"Tier Nolan\" <tier.nolan at gmail.com> wrote:\n\n>\n>\n> On Tue, May 12, 2015 at 8:03 PM, Gregory Maxwell <gmaxwell at gmail.com>\n> wrote:\n>\n>>\n>> (0) Block coverage should have locality; historical blocks are\n>> (almost) always needed in contiguous ranges.   Having random peers\n>> with totally random blocks would be horrific for performance; as you'd\n>> have to hunt down a working peer and make a connection for each block\n>> with high probability.\n>>\n>> (1) Block storage on nodes with a fraction of the history should not\n>> depend on believing random peers; because listening to peers can\n>> easily create attacks (e.g. someone could break the network; by\n>> convincing nodes to become unbalanced) and not useful-- it's not like\n>> the blockchain is substantially different for anyone; if you're to the\n>> point of needing to know coverage to fill then something is wrong.\n>> Gaps would be handled by archive nodes, so there is no reason to\n>> increase vulnerability by doing anything but behaving uniformly.\n>>\n>> (2) The decision to contact a node should need O(1) communications,\n>> not just because of the delay of chasing around just to find who has\n>> someone; but because that chasing process usually makes the process\n>> _highly_ sybil vulnerable.\n>>\n>> (3) The expression of what blocks a node has should be compact (e.g.\n>> not a dense list of blocks) so it can be rumored efficiently.\n>>\n>> (4) Figuring out what block (ranges) a peer has given should be\n>> computationally efficient.\n>>\n>> (5) The communication about what blocks a node has should be compact.\n>>\n>> (6) The coverage created by the network should be uniform, and should\n>> remain uniform as the blockchain grows; ideally it you shouldn't need\n>> to update your state to know what blocks a peer will store in the\n>> future, assuming that it doesn't change the amount of data its\n>> planning to use. (What Tier Nolan proposes sounds like it fails this\n>> point)\n>>\n>> (7) Growth of the blockchain shouldn't cause much (or any) need to\n>> refetch old blocks.\n>>\n>\n> M = 1,000,000\n> N = number of \"starts\"\n>\n> S(0) = hash(seed) mod M\n> ...\n> S(n) = hash(S(n-1)) mod M\n>\n> This generates a sequence of start points.  If the start point is less\n> than the block height, then it counts as a hit.\n>\n> The node stores the 50MB of data starting at the block at height S(n).\n>\n> As the blockchain increases in size, new starts will be less than the\n> block height.  This means some other runs would be deleted.\n>\n> A weakness is that it is random with regards to block heights.  Tiny\n> blocks have the same priority as larger blocks.\n>\n> 0) Blocks are local, in 50MB runs\n> 1) Agreed, nodes should download headers-first (or some other compact way\n> of finding the highest POW chain)\n> 2) M could be fixed, N and the seed are all that is required.  The seed\n> doesn't have to be that large.  If 1% of the blockchain is stored, then 16\n> bits should be sufficient so that every block is covered by seeds.\n> 3) N is likely to be less than 2 bytes and the seed can be 2 bytes\n> 4) A 1% cover of 50GB of blockchain would have 10 starts @ 50MB per run.\n> That is 10 hashes.  They don't even necessarily need to be crypt hashes\n> 5) Isn't this the same as 3?\n> 6) Every block has the same odds of being included.  There inherently\n> needs to be an update when a node deletes some info due to exceeding its\n> cap.  N can be dropped one run at a time.\n> 7) When new starts drop below the tip height, N can be decremented and\n> that one run is deleted.\n>\n> There would need to be a special rule to ensure the low height blocks are\n> covered.  Nodes should keep the first 50MB of blocks with some probability\n> (10%?)\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/97fc216a/attachment.html>"
            },
            {
                "author": "Daniel Kraft",
                "date": "2015-05-13T05:19:54",
                "message_text_only": "Hi all!\n\nOn 2015-05-12 21:03, Gregory Maxwell wrote:\n> Summarizing from memory:\n\nIn the context of this discussion, let me also restate an idea I've\nproposed in Bitcointalk for this.  It is probably not perfect and could\nsurely be adapted (I'm interested in that), but I think it meets\nmost/all of the criteria stated below.  It is similar to the idea with\n\"start points\", but gives O(log height) instead of O(height) for\ndetermining which blocks a node has.\n\nLet me for simplicity assume that the node wants to store 50% of all\nblocks.  It is straight-forward to extend the scheme so that this is\nconfigurable:\n\n1) Create some kind of \"seed\" that can be compact and will be sent to\nother peers to define which blocks the node has.  Use it to initialise a\nPRNG of some sort.\n\n2) Divide the range of all blocks into intervals with exponentially\ngrowing size.  I. e., something like this:\n\n1, 1, 2, 2, 4, 4, 8, 8, 16, 16, ...\n\nWith this, only O(log height) intervals are necessary to cover height\nblocks.\n\n3) Using the PRNG, *one* of the two intervals of each length is\nselected.  The node stores these blocks and discards the others.\n(Possibly keeping the last 200 or 2,016 or whatever blocks additionally.)\n\n> (0) Block coverage should have locality; historical blocks are\n> (almost) always needed in contiguous ranges.   Having random peers\n> with totally random blocks would be horrific for performance; as you'd\n> have to hunt down a working peer and make a connection for each block\n> with high probability.\n\nYou get contiguous block ranges (with at most O(log height) \"breaks\").\nAlso ranges of newer blocks are longer, which may be an advantage if\nthose blocks are needed more often.\n\n> (1) Block storage on nodes with a fraction of the history should not\n> depend on believing random peers; because listening to peers can\n> easily create attacks (e.g. someone could break the network; by\n> convincing nodes to become unbalanced) and not useful-- it's not like\n> the blockchain is substantially different for anyone; if you're to the\n> point of needing to know coverage to fill then something is wrong.\n> Gaps would be handled by archive nodes, so there is no reason to\n> increase vulnerability by doing anything but behaving uniformly.\n\nWith my proposal, each node determines randomly and on its own which\nblocks to store.  No believing anyone.\n\n> (2) The decision to contact a node should need O(1) communications,\n> not just because of the delay of chasing around just to find who has\n> someone; but because that chasing process usually makes the process\n> _highly_ sybil vulnerable.\n\nNot exactly sure what you mean by that, but I think that's fulfilled.\nYou can (locally) compute in O(log height) from a node's seed whether or\nnot it has the blocks you need.  This needs only communication about the\nnode's seed.\n\n> (3) The expression of what blocks a node has should be compact (e.g.\n> not a dense list of blocks) so it can be rumored efficiently.\n\nSee above.\n\n> (4) Figuring out what block (ranges) a peer has given should be\n> computationally efficient.\n\nO(log height).  Not O(1), but that's probably not a big issue.\n\n> (5) The communication about what blocks a node has should be compact.\n\nSee above.\n\n> (6) The coverage created by the network should be uniform, and should\n> remain uniform as the blockchain grows; ideally it you shouldn't need\n> to update your state to know what blocks a peer will store in the\n> future, assuming that it doesn't change the amount of data its\n> planning to use. (What Tier Nolan proposes sounds like it fails this\n> point)\n\nCoverage will be uniform if the seed is created randomly and the PRNG\nhas good properties.  No need to update the seed if the other node's\nfraction is unchanged.  (Not sure if you suggest for nodes to define a\n\"fraction\" or rather an \"absolute size\".)\n\n> (7) Growth of the blockchain shouldn't cause much (or any) need to\n> refetch old blocks.\n\nNo need to do that with the scheme.\n\nWhat do you think about this idea?  Some random thoughts from myself:\n\n*) I need to formulate it in a more general way so that the fraction can\nbe arbitrary and not just 50%.  This should be easy to do, and I can do\nit if there's interest.\n\n*) It is O(log height) and not O(1), but that should not be too\ndifferent for the heights that are relevant.\n\n*) Maybe it would be better / easier to not use the PRNG at all; just\ndecide to *always* use the first or the second interval with a given\nsize.  Not sure about that.\n\n*) With the proposed scheme, the node's actual fraction of stored blocks\nwill vary between 1/2 and 2/3 (if I got the mathematics right, it is\nstill early) as the blocks come in.  Not sure if that's a problem.  I\ncan do a precise analysis of this property for an extended scheme if you\nare interested in it.\n\nYours,\nDaniel\n\n-- \nhttp://www.domob.eu/\nOpenPGP: 1142 850E 6DFF 65BA 63D6  88A8 B249 2AC4 A733 0737\nNamecoin: id/domob -> https://nameid.org/?name=domob\n--\nDone:  Arc-Bar-Cav-Hea-Kni-Ran-Rog-Sam-Tou-Val-Wiz\nTo go: Mon-Pri\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 836 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/0da57426/attachment.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T09:34:03",
                "message_text_only": "On Wed, May 13, 2015 at 6:19 AM, Daniel Kraft <d at domob.eu> wrote:\n\n> 2) Divide the range of all blocks into intervals with exponentially\n> growing size.  I. e., something like this:\n>\n> 1, 1, 2, 2, 4, 4, 8, 8, 16, 16, ...\n>\n\nInteresting.  This can be combined with the system I suggested.\n\nA node broadcasts 3 pieces of information\n\nSeed (16 bits): This is the seed\nM_bits_lsb (1 bit):  Used to indicate M during a transition\nN (7 bits):  This is the count of the last range held (or partially held)\n\nM = 1 << M_bits\n\nM should be set to the lowest power of 2 greater than double the block\nchain height\n\nThat gives M = 1 million at the moment.  During changing M, some nodes will\nbe using the higher M and others will use the lower M.\n\nThe M_bits_lsb field allows those to be distinguished.\n\nAs the block height approaches 512k, nodes can begin to upgrade.  For a\nperiod around block 512k, some nodes could use M = 1 million and others\ncould use M = 2 million.\n\nAssuming M is around 3 times higher than the block height, then the odds of\na start being less than the block height is around 35%.  If they runs by\n25% each step, then that is approx a double for each hit.\n\nSize(n) = ((4 + (n & 0x3)) << (n >> 2)) * 2.5MB\n\nThis gives an exponential increase, but groups of 4 are linearly\ninterpolated.\n\n\n*Size(0) = 10 MB*\nSize(1) = 12.5MB\nSize(2) = 15 MB\nSize(3) = 17.5MB\nSize(4) = 20MB\n\n*Size(5) = 25MB*\nSize(6) = 30MB\nSize(7) = 35MB\n\n*Size(8) = 40MB*\n\nStart(n) = Hash(seed + n) mod M\n\nA node should store as much of its last start as possible.  Assuming start\n0, 5, and 8 were \"hits\" but the node had a max size of 60MB.  It can store\n0 and 5 and have 25MB left.  That isn't enough to store all of run 8, but\nit should store 25MB of the blocks in run 8 anyway.\n\nSize(255) = pow(2, 31) * 17.5MB = 35,840 TB\n\nDecreasing N only causes previously accepted runs to be invalidated.\n\nWhen a node approaches a transition point for N, it would select a block\nheight within 25,000 of the transition point.  Once it reaches that block,\nit will begin downloading the new runs that it needs.  When updating, it\ncan set N to zero.  This spreads out the upgrade (over around a year), with\nonly a small number of nodes upgrading at any time.\n\nNew nodes should use the higher M, if near a transition point (say within\n100,000).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/94dd908e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposed additional options for pruned nodes",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "gabe appleton",
                "Peter Todd",
                "Daniel Kraft",
                "Tier Nolan",
                "Gregory Maxwell"
            ],
            "messages_count": 17,
            "total_messages_chars_count": 52438
        }
    },
    {
        "title": "[Bitcoin-development] [Bulk] Re: Proposed additional options for pruned nodes",
        "thread_messages": [
            {
                "author": "gb",
                "date": "2015-05-12T21:30:03",
                "message_text_only": "This seems like a good place to add in an idea I had about\npartially-connected nodes that are able to throttle bandwidth demands.\nWhile we will be having partial-blockchain nodes with a spectrum of\nstorage options the requirement to be connected is somewhat binary, I\nthink many users manually throttle by turning nodes on/off already with\na minimum to just keep the chain up to date. A throttling option would\nleverage on bitcoin's asychronous design to reduce bandwidth demands for\nweaker nodes.\n\nSo throttling to allow for a spectrum of bandwidth connectivity:\n\n1) an option for the user -throttle=XXX that would allow the user to\nspecify a desirable total bandwidth XXX in Gbytes/day the bitcoin client\ncan use.\n\n2) the client reduces the number of continuous connections, transaction\nor block relaying to achieve the desired throttling rate\n\n3) it could do this by being partially connected throughout the duty\ncycle or cycling the node on/off for a percentage of a 24(?) hr period\n\n4) have an auto setting where some smart traffic management 'just takes\ncare of it' and manual settings that can be user configured\n\n5) reduces minimum requirement for any 24(?) hr period it has received a\nfull copy of all blocks to remain fully-validating\n\nNot sure if anyone has bought such an idea forward or if there are\nobvious holes, so pre-emptive apologies for time-wasting if so.\n\nOn Tue, 2015-05-12 at 15:43 -0400, gabe appleton wrote:\n> Yet this holds true in our current assumptions of the network as well:\n> that it will become a collection of pruned nodes with a few storage\n> nodes. \n> \n> A hybrid option makes this better, because it spreads the risk, rather\n> than concentrating it in full nodes. \n> \n> On May 12, 2015 3:38 PM, \"Jeff Garzik\" <jgarzik at bitpay.com> wrote:\n>         One general problem is that security is weakened when an\n>         attacker can DoS a small part of the chain by DoS'ing a small\n>         number of nodes - yet the impact is a network-wide DoS because\n>         nobody can complete a sync.\n>         \n>         \n>         \n>         On Tue, May 12, 2015 at 12:24 PM, gabe appleton\n>         <gappleto97 at gmail.com> wrote:\n>                 0, 1, 3, 4, 5, 6 can be solved by looking at chunks\n>                 chronologically. Ie, give the signed (by sender) hash\n>                 of the first and last block in your range. This is\n>                 less data dense than the idea above, but it might work\n>                 better. \n>                 \n>                 That said, this is likely a less secure way to do it.\n>                 To improve upon that, a node could request a block of\n>                 random height within that range and verify it, but\n>                 that violates point 2. And the scheme in itself\n>                 definitely violates point 7.\n>                 \n>                 On May 12, 2015 3:07 PM, \"Gregory Maxwell\"\n>                 <gmaxwell at gmail.com> wrote:\n>                         It's a little frustrating to see this just\n>                         repeated without even\n>                         paying attention to the desirable\n>                         characteristics from the prior\n>                         discussions.\n>                         \n>                         Summarizing from memory:\n>                         \n>                         (0) Block coverage should have locality;\n>                         historical blocks are\n>                         (almost) always needed in contiguous ranges.\n>                          Having random peers\n>                         with totally random blocks would be horrific\n>                         for performance; as you'd\n>                         have to hunt down a working peer and make a\n>                         connection for each block\n>                         with high probability.\n>                         \n>                         (1) Block storage on nodes with a fraction of\n>                         the history should not\n>                         depend on believing random peers; because\n>                         listening to peers can\n>                         easily create attacks (e.g. someone could\n>                         break the network; by\n>                         convincing nodes to become unbalanced) and not\n>                         useful-- it's not like\n>                         the blockchain is substantially different for\n>                         anyone; if you're to the\n>                         point of needing to know coverage to fill then\n>                         something is wrong.\n>                         Gaps would be handled by archive nodes, so\n>                         there is no reason to\n>                         increase vulnerability by doing anything but\n>                         behaving uniformly.\n>                         \n>                         (2) The decision to contact a node should need\n>                         O(1) communications,\n>                         not just because of the delay of chasing\n>                         around just to find who has\n>                         someone; but because that chasing process\n>                         usually makes the process\n>                         _highly_ sybil vulnerable.\n>                         \n>                         (3) The expression of what blocks a node has\n>                         should be compact (e.g.\n>                         not a dense list of blocks) so it can be\n>                         rumored efficiently.\n>                         \n>                         (4) Figuring out what block (ranges) a peer\n>                         has given should be\n>                         computationally efficient.\n>                         \n>                         (5) The communication about what blocks a node\n>                         has should be compact.\n>                         \n>                         (6) The coverage created by the network should\n>                         be uniform, and should\n>                         remain uniform as the blockchain grows;\n>                         ideally it you shouldn't need\n>                         to update your state to know what blocks a\n>                         peer will store in the\n>                         future, assuming that it doesn't change the\n>                         amount of data its\n>                         planning to use. (What Tier Nolan proposes\n>                         sounds like it fails this\n>                         point)\n>                         \n>                         (7) Growth of the blockchain shouldn't cause\n>                         much (or any) need to\n>                         refetch old blocks.\n>                         \n>                         I've previously proposed schemes which come\n>                         close but fail one of the above.\n>                         \n>                         (e.g. a scheme based on reservoir sampling\n>                         that gives uniform\n>                         selection of contiguous ranges, communicating\n>                         only 64 bits of data to\n>                         know what blocks a node claims to have,\n>                         remaining totally uniform as\n>                         the chain grows, without any need to refetch\n>                         -- but needs O(height)\n>                         work to figure out what blocks a peer has from\n>                         the data it\n>                         communicated.;   or another scheme based on\n>                         consistent hashes that has\n>                         log(height) computation; but sometimes may\n>                         result in a node needing to\n>                         go refetch an old block range it previously\n>                         didn't store-- creating\n>                         re-balancing traffic.)\n>                         \n>                         So far something that meets all those criteria\n>                         (and/or whatever ones\n>                         I'm not remembering) has not been discovered;\n>                         but I don't really think\n>                         much time has been spent on it. I think its\n>                         very likely possible.\n>                         \n>                         ------------------------------------------------------------------------------\n>                         One dashboard for servers and applications\n>                         across Physical-Virtual-Cloud\n>                         Widest out-of-the-box monitoring support with\n>                         50+ applications\n>                         Performance metrics, stats and reports that\n>                         give you Actionable Insights\n>                         Deep dive visibility with transaction tracing\n>                         using APM Insight.\n>                         http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>                         _______________________________________________\n>                         Bitcoin-development mailing list\n>                         Bitcoin-development at lists.sourceforge.net\n>                         https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>                 \n>                 ------------------------------------------------------------------------------\n>                 One dashboard for servers and applications across\n>                 Physical-Virtual-Cloud\n>                 Widest out-of-the-box monitoring support with 50+\n>                 applications\n>                 Performance metrics, stats and reports that give you\n>                 Actionable Insights\n>                 Deep dive visibility with transaction tracing using\n>                 APM Insight.\n>                 http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>                 _______________________________________________\n>                 Bitcoin-development mailing list\n>                 Bitcoin-development at lists.sourceforge.net\n>                 https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>                 \n>         \n>         \n>         \n>         \n>         -- \n>         Jeff Garzik\n>         Bitcoin core developer and open source evangelist\n>         BitPay, Inc.      https://bitpay.com/\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________ Bitcoin-development mailing list Bitcoin-development at lists.sourceforge.net https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            }
        ],
        "thread_summary": {
            "title": "Re: Proposed additional options for pruned nodes",
            "categories": [
                "Bitcoin-development",
                "Bulk"
            ],
            "authors": [
                "gb"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 10987
        }
    },
    {
        "title": "[Bitcoin-development] Fwd: Proposed additional options for pruned nodes",
        "thread_messages": [
            {
                "author": "Adam Weiss",
                "date": "2015-05-12T21:17:14",
                "message_text_only": "FYI on behalf of jgarzik...\n\n---------- Forwarded message ----------\nFrom: Jeff Garzik <jgarzik at bitpay.com>\nDate: Tue, May 12, 2015 at 4:48 PM\nSubject: Re: [Bitcoin-development] Proposed additional options for pruned\nnodes\nTo: Adam Weiss <adam at signal11.com>\n\n\nMaybe you could forward my response to the list as an FYI?\n\n\nOn Tue, May 12, 2015 at 12:43 PM, Jeff Garzik <jgarzik at bitpay.com> wrote:\n\n> You are the 12th person to report this.  It is SF, not bitpay, rewriting\n> email headers and breaking authentication.\n>\n>\n> On Tue, May 12, 2015 at 12:40 PM, Adam Weiss <adam at signal11.com> wrote:\n>\n>> fyi, your email to bitcoin-dev is still generating google spam warnings...\n>>\n>> --adam\n>>\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150512/f8a85601/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd: Proposed additional options for pruned nodes",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Adam Weiss"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 890
        }
    },
    {
        "title": "[Bitcoin-development] [BIP] Normalized Transaction IDs",
        "thread_messages": [
            {
                "author": "Christian Decker",
                "date": "2015-05-13T12:48:04",
                "message_text_only": "Hi All,\n\nI'd like to propose a BIP to normalize transaction IDs in order to address\ntransaction malleability and facilitate higher level protocols.\n\nThe normalized transaction ID is an alias used in parallel to the current\n(legacy) transaction IDs to address outputs in transactions. It is\ncalculated by removing (zeroing) the scriptSig before computing the hash,\nwhich ensures that only data whose integrity is also guaranteed by the\nsignatures influences the hash. Thus if anything causes the normalized ID\nto change it automatically invalidates the signature. When validating a\nclient supporting this BIP would use both the normalized tx ID as well as\nthe legacy tx ID when validating transactions.\n\nThe detailed writeup can be found here:\nhttps://github.com/cdecker/bips/blob/normalized-txid/bip-00nn.mediawiki.\n\n@gmaxwell: I'd like to request a BIP number, unless there is something\nreally wrong with the proposal.\n\nIn addition to being a simple alternative that solves transaction\nmalleability it also hugely simplifies higher level protocols. We can now\nuse template transactions upon which sequences of transactions can be built\nbefore signing them.\n\nI hesitated quite a while to propose it since it does require a hardfork\n(old clients would not find the prevTx identified by the normalized\ntransaction ID and deem the spending transaction invalid), but it seems\nthat hardforks are no longer the dreaded boogeyman nobody talks about.\nI left out the details of how the hardfork is to be done, as it does not\nreally matter and we may have a good mechanism to apply a bunch of\nhardforks concurrently in the future.\n\nI'm sure it'll take time to implement and upgrade, but I think it would be\na nice addition to the functionality and would solve a long standing\nproblem :-)\n\nPlease let me know what you think, the proposal is definitely not set in\nstone at this point and I'm sure we can improve it further.\n\nRegards,\nChristian\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/2131ae26/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T13:12:43",
                "message_text_only": "I think this is a good way to handle things, but as you say, it is a hard\nfork.\n\nCHECKLOCKTIMEVERIFY covers many of the use cases, but it would be nice to\nfix malleability once and for all.\n\nThis has the effect of doubling the size of the UTXO database.  At minimum,\nthere needs to be a legacy txid to normalized txid map in the database.\n\nAn addition to the BIP would eliminate the need for the 2nd index.  You\ncould require a SPV proof of the spending transaction to be included with\nlegacy transactions.  This would allow clients to verify that the\nnormalized txid matched the legacy id.\n\nThe OutPoint would be {LegacyId | SPV Proof to spending tx  | spending tx |\nindex}.  This allows a legacy transaction to be upgraded.  OutPoints which\nuse a normalized txid don't need the SPV proof.\n\nThe hard fork would be followed by a transitional period, in which both\ntxids could be used.  Afterwards, legacy transactions have to have the SPV\nproof added.  This means that old transactions with locktimes years in the\nfuture can be upgraded for spending, without nodes needing to maintain two\nindexes.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/84f2cb0a/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-13T13:41:44",
                "message_text_only": "I think this needs more details before it gets a BIP number; for example,\nwhich opcodes does this affect, and how, exactly, does it affect them? Is\nthe merkle root in the block header computed using normalized transaction\nids or normalized ids?\n\nI think there might actually be two or three or four BIPs here:\n\n + Overall \"what is trying to be accomplished\"\n + Changes to the OP_*SIG* opcodes\n + Changes to the bloom-filtering SPV support\n + ...eventually, hard fork rollout plan\n\nI also think that it is a good idea to have actually implemented a proposal\nbefore getting a BIP number. At least, I find that actually writing the\ncode often turns up issues I hadn't considered when thinking about the\nproblem at a high level. And I STRONGLY believe BIPs should be descriptive\n(\"here is how this thing works\") not proscriptive (\"here's how I think we\nshould all do it\").\n\nFinally: I like the idea of moving to a normalized txid. But it might make\nsense to bundle that change with a bigger change to OP_CHECKSIG; see Greg\nMaxwell's excellent talk about his current thoughts on that topic:\n  https://www.youtube.com/watch?v=Gs9lJTRZCDc\n\n\nOn Wed, May 13, 2015 at 9:12 AM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n> I think this is a good way to handle things, but as you say, it is a hard\n> fork.\n>\n> CHECKLOCKTIMEVERIFY covers many of the use cases, but it would be nice to\n> fix malleability once and for all.\n>\n> This has the effect of doubling the size of the UTXO database.  At\n> minimum, there needs to be a legacy txid to normalized txid map in the\n> database.\n>\n> An addition to the BIP would eliminate the need for the 2nd index.  You\n> could require a SPV proof of the spending transaction to be included with\n> legacy transactions.  This would allow clients to verify that the\n> normalized txid matched the legacy id.\n>\n> The OutPoint would be {LegacyId | SPV Proof to spending tx  | spending tx\n> | index}.  This allows a legacy transaction to be upgraded.  OutPoints\n> which use a normalized txid don't need the SPV proof.\n>\n> The hard fork would be followed by a transitional period, in which both\n> txids could be used.  Afterwards, legacy transactions have to have the SPV\n> proof added.  This means that old transactions with locktimes years in the\n> future can be upgraded for spending, without nodes needing to maintain two\n> indexes.\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/01cccf57/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-13T15:24:34",
                "message_text_only": "Glad you like it, I was afraid that I missed something obvious :-)\n\nThe points the two of you raised are valid and I will address them as soon\nas possible. I certainly will implement this proposal so that it becomes\nmore concrete, but my C++ is a bit rusty and it'll take some time, so I\nwanted to gauge interest first.\n\n> This has the effect of doubling the size of the UTXO database.  At\nminimum, there needs to be a legacy txid to normalized txid map in the\ndatabase.\n>\n> An addition to the BIP would eliminate the need for the 2nd index.  You\ncould require a SPV proof of the spending transaction to be included with\nlegacy transactions.  This would allow clients to verify that the\nnormalized txid matched the legacy id.\n>\n>The OutPoint would be {LegacyId | SPV Proof to spending tx  | spending tx\n| index}.  This allows a legacy transaction to be upgraded.  OutPoints\nwhich use a normalized txid don't need the SPV proof.\n\nIt does and I should have mentioned it in the draft, according to my\ncalculations a mapping legacy ID -> normalized ID is about 256 MB in size,\nor at least it was at height 330'000, things might have changed a bit and\nI'll recompute that. I omitted the deprecation of legacy IDs on purpose\nsince we don't know whether we will migrate completely or leave keep both\noptions viable.\n\n> I think this needs more details before it gets a BIP number; for example,\nwhich opcodes does this affect, and how, exactly, does it affect them? Is\nthe merkle root in the block header computed using normalized transaction\nids or normalized ids?\n\nI think both IDs can be used in the merkle tree, since we lookup an ID in\nboth indices we can use both to address them and we will find them either\nway.\n\nAs for the opcodes I'll have to check, but I currently don't see how they\ncould be affected. The OP_*SIG* codes calculate their own (more\ncomplicated) stripped transaction before hashing and checking the\nsignature. The input of the stripped transaction simply contains whatever\nhash was used to reference the output, so we do not replace IDs during the\noperation. The stripped format used by OP_*SIG* operations does not have to\nadhere to the hashes used to reference a transaction in the input.\n\n> I think there might actually be two or three or four BIPs here:\n>\n>  + Overall \"what is trying to be accomplished\"\n>  + Changes to the OP_*SIG* opcodes\n>  + Changes to the bloom-filtering SPV support\n>  + ...eventually, hard fork rollout plan\n>\n> I also think that it is a good idea to have actually implemented a\nproposal before getting a BIP number. At least, I find that actually\nwriting the code often turns up issues I hadn't considered when thinking\nabout the problem at a high level. And I STRONGLY believe BIPs should be\ndescriptive (\"here is how this thing works\") not proscriptive (\"here's how\nI think we should all do it\").\n\nWe can certainly split the proposal should it get too large, for now it\nseems manageable, since opcodes are not affected. Bloom-filtering is\nresolved by adding the normalized transaction IDs and checking for both IDs\nin the filter. Since you mention bundling the change with other changes\nthat require a hard-fork it might be a good idea to build a separate\nproposal for a generic hard-fork rollout mechanism.\n\nIf there are no obvious roadblocks and the change seems generally a good\nthing I will implement it in Bitcoin Core :-)\n\nRegards,\nChris\n\nOn Wed, May 13, 2015 at 3:44 PM Gavin Andresen <gavinandresen at gmail.com>\nwrote:\n\n> I think this needs more details before it gets a BIP number; for example,\n> which opcodes does this affect, and how, exactly, does it affect them? Is\n> the merkle root in the block header computed using normalized transaction\n> ids or normalized ids?\n>\n> I think there might actually be two or three or four BIPs here:\n>\n>  + Overall \"what is trying to be accomplished\"\n>  + Changes to the OP_*SIG* opcodes\n>  + Changes to the bloom-filtering SPV support\n>  + ...eventually, hard fork rollout plan\n>\n> I also think that it is a good idea to have actually implemented a\n> proposal before getting a BIP number. At least, I find that actually\n> writing the code often turns up issues I hadn't considered when thinking\n> about the problem at a high level. And I STRONGLY believe BIPs should be\n> descriptive (\"here is how this thing works\") not proscriptive (\"here's how\n> I think we should all do it\").\n>\n> Finally: I like the idea of moving to a normalized txid. But it might make\n> sense to bundle that change with a bigger change to OP_CHECKSIG; see Greg\n> Maxwell's excellent talk about his current thoughts on that topic:\n>   https://www.youtube.com/watch?v=Gs9lJTRZCDc\n>\n>\n> On Wed, May 13, 2015 at 9:12 AM, Tier Nolan <tier.nolan at gmail.com> wrote:\n>\n>> I think this is a good way to handle things, but as you say, it is a hard\n>> fork.\n>>\n>> CHECKLOCKTIMEVERIFY covers many of the use cases, but it would be nice to\n>> fix malleability once and for all.\n>>\n>> This has the effect of doubling the size of the UTXO database.  At\n>> minimum, there needs to be a legacy txid to normalized txid map in the\n>> database.\n>>\n>> An addition to the BIP would eliminate the need for the 2nd index.  You\n>> could require a SPV proof of the spending transaction to be included with\n>> legacy transactions.  This would allow clients to verify that the\n>> normalized txid matched the legacy id.\n>>\n>> The OutPoint would be {LegacyId | SPV Proof to spending tx  | spending tx\n>> | index}.  This allows a legacy transaction to be upgraded.  OutPoints\n>> which use a normalized txid don't need the SPV proof.\n>>\n>> The hard fork would be followed by a transitional period, in which both\n>> txids could be used.  Afterwards, legacy transactions have to have the SPV\n>> proof added.  This means that old transactions with locktimes years in the\n>> future can be upgraded for spending, without nodes needing to maintain two\n>> indexes.\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/4ba21be6/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T16:18:24",
                "message_text_only": "On Wed, May 13, 2015 at 4:24 PM, Christian Decker <\ndecker.christian at gmail.com> wrote\n\n> It does and I should have mentioned it in the draft, according to my\n> calculations a mapping legacy ID -> normalized ID is about 256 MB in size,\n> or at least it was at height 330'000, things might have changed a bit and\n> I'll recompute that. I omitted the deprecation of legacy IDs on purpose\n> since we don't know whether we will migrate completely or leave keep both\n> options viable.\n>\n\nThere are around 20 million UTXOs.  At 2*32 bytes per entry, that is more\nthan 1GB.  There are more UTXOs than transactions, but 256MB seems a little\nlow.\n\nI think both IDs can be used in the merkle tree, since we lookup an ID in\n> both indices we can use both to address them and we will find them either\n> way.\n>\n\nThe id that is used to sign should be used in the merkle tree.  The hard\nfork should simply be to allow transactions that use the normalized\ntransaction hash.\n\n\n> As for the opcodes I'll have to check, but I currently don't see how they\n> could be affected.\n>\n\nAgreed, the transaction is simply changed and all the standard rules apply.\n\n\n> We can certainly split the proposal should it get too large, for now it\n> seems manageable, since opcodes are not affected.\n>\n\nRight it is just a database update.  The undo info also needs to be changed\nso that both txids are included.\n\n\n> Bloom-filtering is resolved by adding the normalized transaction IDs and\n> checking for both IDs in the filter.\n>\n\nYeah, if a transaction spends with a legacy txid, it should still match if\nthe normalized txid is included in the filter.\n\n> Since you mention bundling the change with other changes that require a\nhard-fork it might be a good idea to build a separate proposal for a\ngeneric hard-fork rollout mechanism.\n\nThat would be useful.  On the other hand, we don't want to make them to\neasy.\n\nI think this is a good choice for a hard fork test, since it is\nuncontroversial.  With a time machine, it would have been done this way at\nthe start.\n\nWhat about the following:\n\nThe reference client is updated so that it uses version 2 transactions by\ndefault (but it can be changed by user).  A pop-up could appear for the GUI.\n\nThere is no other change.\n\nAll transactions in blocks 375000 to 385000 are considered votes and\nweighted by bitcoin days destroyed (max 60 days).\n\nIf > 75% of the transactions by weight are version 2, then the community\nare considered to support the hard fork.\n\nThere would need to be a way to protect against miners censoring\ntransactions/votes.\n\nUsers could submit their transactions directly to a p2p tallying system.\nThe coin would be aged based on the age in block 375000 unless included in\nthe blockchain.  These votes don't need to be ordered and multiple votes\nfor the same coin would only count once.\n\nIn fact, votes could just be based on holding in block X.\n\nThis is an opinion poll rather than a referendum though.\n\nAssuming support of the community, the hard fork can then proceed in a\nsimilar way to the way a soft fork does.\n\nDevs update the reference client to produce version 4 blocks and version 3\ntransactions.  Miners could watch version 3 transactions to gauge user\ninterest and use that to help decide if they should update.\n\nIf 750 of the last 1000 blocks are version 4 or higher, reject blocks with\ntransactions of less than version 3 in version 4 blocks\n\n    This means that legacy clients will be slow to confirm their\ntransactions, since their transactions cannot go into version 4 blocks.\nThis is encouragement to upgrade.\n\nIf 950 of the last 1000 blocks are version 4 or higher, reject blocks with\ntransactions of less than version 3 in all blocks\n\n    This means that legacy nodes can no longer send transactions but can\nstill receive.  Transactions received from other legacy nodes would remain\nunconfirmed.\n\nIf 990 of the last 1000 blocks are version 4 or higher, reject version 3 or\nlower blocks\n\n    This is the point of no return.  Rejecting version 3 blocks means that\nthe next rule is guaranteed to activate within the next 2016 blocks.\nLegacy nodes remain on the main chain, but cannot send.  Miners mining with\nlegacy clients are (soft) forked off the chain.\n\nIf 1000 of the last 1000 blocks are version 4 or higher and the difficulty\nretarget has just happened, activate hard fork rule\n\n    This hard forks legacy nodes off the chain.  99% of miners support this\nchange and users have been encouraged to update.  The block rate for the\nnon-forked chain is ast most 1% of normal.  Blocks happen every 16 hours.\nBy timing activation after a difficulty retarget, it makes it harder for\nthe other fork to adapt to the reduced hash rate.\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/11fbf72e/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-05-13T16:34:52",
                "message_text_only": "I think this hardfork is dead-on-arrival given the ideas for OP_CHECKSIG \nsoftforking. Instead of referring to previous transactions by a normalised \nhash, it makes better sense to simply change the outpoints in the signed data \nand allow nodes to hotfix dependent transactions when/if they are malleated. \nFurthermore, the approach of using a hash of scriptPubKey in the input rather \nthan an outpoint also solves dependencies in the face of intentional \nmalleability (respending with a higher fee, or CoinJoin, for a few examples).\n\nThese aren't barriers to making the proposal or being assigned a BIP number if \nyou want to go forward with that, but you may wish to reconsider spending time \non it.\n\nLuke\n\n\nOn Wednesday, May 13, 2015 12:48:04 PM Christian Decker wrote:\n> Hi All,\n> \n> I'd like to propose a BIP to normalize transaction IDs in order to address\n> transaction malleability and facilitate higher level protocols.\n> \n> The normalized transaction ID is an alias used in parallel to the current\n> (legacy) transaction IDs to address outputs in transactions. It is\n> calculated by removing (zeroing) the scriptSig before computing the hash,\n> which ensures that only data whose integrity is also guaranteed by the\n> signatures influences the hash. Thus if anything causes the normalized ID\n> to change it automatically invalidates the signature. When validating a\n> client supporting this BIP would use both the normalized tx ID as well as\n> the legacy tx ID when validating transactions.\n> \n> The detailed writeup can be found here:\n> https://github.com/cdecker/bips/blob/normalized-txid/bip-00nn.mediawiki.\n> \n> @gmaxwell: I'd like to request a BIP number, unless there is something\n> really wrong with the proposal.\n> \n> In addition to being a simple alternative that solves transaction\n> malleability it also hugely simplifies higher level protocols. We can now\n> use template transactions upon which sequences of transactions can be built\n> before signing them.\n> \n> I hesitated quite a while to propose it since it does require a hardfork\n> (old clients would not find the prevTx identified by the normalized\n> transaction ID and deem the spending transaction invalid), but it seems\n> that hardforks are no longer the dreaded boogeyman nobody talks about.\n> I left out the details of how the hardfork is to be done, as it does not\n> really matter and we may have a good mechanism to apply a bunch of\n> hardforks concurrently in the future.\n> \n> I'm sure it'll take time to implement and upgrade, but I think it would be\n> a nice addition to the functionality and would solve a long standing\n> problem :-)\n> \n> Please let me know what you think, the proposal is definitely not set in\n> stone at this point and I'm sure we can improve it further.\n> \n> Regards,\n> Christian"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-13T17:14:07",
                "message_text_only": "Normalized transaction ids are only effectively non-malleable when all\ninputs they refer to are also non-malleable (or you can have malleability\nin 2nd level dependencies), so I do not believe it makes sense to allow\nmixed usage of the txids at all. They do not provide the actual benefit of\nguaranteed non-malleability before it becomes disallowed to use the old\nmechanism. That, together with the +- resource doubling needed for the UTXO\nset (as earlier mentioned) and the fact that an alternative which is only a\nsoftfork are available, makes this a bad idea IMHO.\n\nUnsure to what extent this has been presented on the mailinglist, but the\nsoftfork idea is this:\n* Transactions get 2 txids, one used to reference them (computed as\nbefore), and one used in an (extended) sighash.\n* The txins keep using the normal txid, so not structural changes to\nBitcoin.\n* The ntxid is computed by replacing the scriptSigs in inputs by the empty\nstring, and by replacing the txids in txins by their corresponding ntxids.\n* A new checksig operator is softforked in, which uses the ntxids in its\nsighashes rather than the full txid.\n* To support efficiently computing ntxids, every tx in the utxo set\n(currently around 6M) stores the ntxid, but only supports lookup bu txid\nstill.\n\nThis does result in a system where a changed dependency indeed invalidates\nthe spending transaction, but the fix is trivial and can be done without\naccess to the private key.\nOn May 13, 2015 5:50 AM, \"Christian Decker\" <decker.christian at gmail.com>\nwrote:\n\n> Hi All,\n>\n> I'd like to propose a BIP to normalize transaction IDs in order to address\n> transaction malleability and facilitate higher level protocols.\n>\n> The normalized transaction ID is an alias used in parallel to the current\n> (legacy) transaction IDs to address outputs in transactions. It is\n> calculated by removing (zeroing) the scriptSig before computing the hash,\n> which ensures that only data whose integrity is also guaranteed by the\n> signatures influences the hash. Thus if anything causes the normalized ID\n> to change it automatically invalidates the signature. When validating a\n> client supporting this BIP would use both the normalized tx ID as well as\n> the legacy tx ID when validating transactions.\n>\n> The detailed writeup can be found here:\n> https://github.com/cdecker/bips/blob/normalized-txid/bip-00nn.mediawiki.\n>\n> @gmaxwell: I'd like to request a BIP number, unless there is something\n> really wrong with the proposal.\n>\n> In addition to being a simple alternative that solves transaction\n> malleability it also hugely simplifies higher level protocols. We can now\n> use template transactions upon which sequences of transactions can be built\n> before signing them.\n>\n> I hesitated quite a while to propose it since it does require a hardfork\n> (old clients would not find the prevTx identified by the normalized\n> transaction ID and deem the spending transaction invalid), but it seems\n> that hardforks are no longer the dreaded boogeyman nobody talks about.\n> I left out the details of how the hardfork is to be done, as it does not\n> really matter and we may have a good mechanism to apply a bunch of\n> hardforks concurrently in the future.\n>\n> I'm sure it'll take time to implement and upgrade, but I think it would be\n> a nice addition to the functionality and would solve a long standing\n> problem :-)\n>\n> Please let me know what you think, the proposal is definitely not set in\n> stone at this point and I'm sure we can improve it further.\n>\n> Regards,\n> Christian\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/bb0fcadf/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-13T18:04:54",
                "message_text_only": "If the inputs to my transaction have been long confirmed I can be\nreasonably safe in assuming that the transaction hash does not change\nanymore. It's true that I have to be careful not to build on top of\ntransactions that use legacy references to transactions that are\nunconfirmed or have few confirmations, however that does not invalidate the\nutility of the normalized transaction IDs.\n\nThe resource doubling is not optimal, I agree, but compare that to dragging\naround malleability and subsequent hacks to sort-of fix it forever.\nAdditionally if we were to decide to abandon legacy transaction IDs we\ncould eventually drop the legacy index after a sufficient transition period.\n\nI remember reading about the SIGHASH proposal somewhere. It feels really\nhackish to me: It is a substantial change to the way signatures are\nverified, I cannot really see how this is a softfork if clients that did\nnot update are unable to verify transactions using that SIGHASH Flag and it\nis adding more data (the normalized hash) to the script, which has to be\nstored as part of the transaction. It may be true that a node observing\nchanges in the input transactions of a transaction using this flag could\nfix the problem, however it requires the node's intervention.\n\nCompare that to the simple and clean solution in the proposal, which does\nnot add extra data to be stored, keeps the OP_*SIG* semantics as they are\nand where once you sign a transaction it does not have to be monitored or\nchanged in order to be valid.\n\nThere certainly are merits using the SIGHASH approach in the short term (it\ndoes not require a hard fork), however I think the normalized transaction\nID is a cleaner and simpler long-term solution, even though it requires a\nhard-fork.\n\nRegards,\nChristian\n\nOn Wed, May 13, 2015 at 7:14 PM Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> Normalized transaction ids are only effectively non-malleable when all\n> inputs they refer to are also non-malleable (or you can have malleability\n> in 2nd level dependencies), so I do not believe it makes sense to allow\n> mixed usage of the txids at all. They do not provide the actual benefit of\n> guaranteed non-malleability before it becomes disallowed to use the old\n> mechanism. That, together with the +- resource doubling needed for the UTXO\n> set (as earlier mentioned) and the fact that an alternative which is only a\n> softfork are available, makes this a bad idea IMHO.\n>\n> Unsure to what extent this has been presented on the mailinglist, but the\n> softfork idea is this:\n> * Transactions get 2 txids, one used to reference them (computed as\n> before), and one used in an (extended) sighash.\n> * The txins keep using the normal txid, so not structural changes to\n> Bitcoin.\n> * The ntxid is computed by replacing the scriptSigs in inputs by the empty\n> string, and by replacing the txids in txins by their corresponding ntxids.\n> * A new checksig operator is softforked in, which uses the ntxids in its\n> sighashes rather than the full txid.\n> * To support efficiently computing ntxids, every tx in the utxo set\n> (currently around 6M) stores the ntxid, but only supports lookup bu txid\n> still.\n>\n> This does result in a system where a changed dependency indeed invalidates\n> the spending transaction, but the fix is trivial and can be done without\n> access to the private key.\n> On May 13, 2015 5:50 AM, \"Christian Decker\" <decker.christian at gmail.com>\n> wrote:\n>\n>> Hi All,\n>>\n>> I'd like to propose a BIP to normalize transaction IDs in order to\n>> address transaction malleability and facilitate higher level protocols.\n>>\n>> The normalized transaction ID is an alias used in parallel to the current\n>> (legacy) transaction IDs to address outputs in transactions. It is\n>> calculated by removing (zeroing) the scriptSig before computing the hash,\n>> which ensures that only data whose integrity is also guaranteed by the\n>> signatures influences the hash. Thus if anything causes the normalized ID\n>> to change it automatically invalidates the signature. When validating a\n>> client supporting this BIP would use both the normalized tx ID as well as\n>> the legacy tx ID when validating transactions.\n>>\n>> The detailed writeup can be found here:\n>> https://github.com/cdecker/bips/blob/normalized-txid/bip-00nn.mediawiki.\n>>\n>> @gmaxwell: I'd like to request a BIP number, unless there is something\n>> really wrong with the proposal.\n>>\n>> In addition to being a simple alternative that solves transaction\n>> malleability it also hugely simplifies higher level protocols. We can now\n>> use template transactions upon which sequences of transactions can be built\n>> before signing them.\n>>\n>> I hesitated quite a while to propose it since it does require a hardfork\n>> (old clients would not find the prevTx identified by the normalized\n>> transaction ID and deem the spending transaction invalid), but it seems\n>> that hardforks are no longer the dreaded boogeyman nobody talks about.\n>> I left out the details of how the hardfork is to be done, as it does not\n>> really matter and we may have a good mechanism to apply a bunch of\n>> hardforks concurrently in the future.\n>>\n>> I'm sure it'll take time to implement and upgrade, but I think it would\n>> be a nice addition to the functionality and would solve a long standing\n>> problem :-)\n>>\n>> Please let me know what you think, the proposal is definitely not set in\n>> stone at this point and I'm sure we can improve it further.\n>>\n>> Regards,\n>> Christian\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/4a02d331/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-13T18:40:34",
                "message_text_only": "On Wed, May 13, 2015 at 11:04 AM, Christian Decker <\ndecker.christian at gmail.com> wrote:\n\n> If the inputs to my transaction have been long confirmed I can be\n> reasonably safe in assuming that the transaction hash does not change\n> anymore. It's true that I have to be careful not to build on top of\n> transactions that use legacy references to transactions that are\n> unconfirmed or have few confirmations, however that does not invalidate the\n> utility of the normalized transaction IDs.\n>\n\nSufficient confirmations help of course, but make systems like this less\nuseful for more complex interactions where you have multiple unconfirmed\ntransactions waiting on each other. I think being able to rely on this\nproblem being solved unconditionally is what makes the proposal attractive.\nFor the simple cases, see BIP62.\n\nI remember reading about the SIGHASH proposal somewhere. It feels really\n> hackish to me: It is a substantial change to the way signatures are\n> verified, I cannot really see how this is a softfork if clients that did\n> not update are unable to verify transactions using that SIGHASH Flag and it\n> is adding more data (the normalized hash) to the script, which has to be\n> stored as part of the transaction. It may be true that a node observing\n> changes in the input transactions of a transaction using this flag could\n> fix the problem, however it requires the node's intervention.\n>\n\nI think you misunderstand the idea. This is related, but orthogonal to the\nideas about extended the sighash flags that have been discussed here before.\n\nAll it's doing is adding a new CHECKSIG operator to script, which, in its\ninternally used signature hash, 1) removes the scriptSigs from transactions\nbefore hashing 2) replaces the txids in txins by their ntxid. It does not\nadd any data to transactions, and it is a softfork, because it only impacts\nscripts which actually use the new CHECKSIG operator. Wallets that don't\nsupport signing with this new operator would not give out addresses that\nuse it.\n\n>\n> Compare that to the simple and clean solution in the proposal, which does\n> not add extra data to be stored, keeps the OP_*SIG* semantics as they are\n> and where once you sign a transaction it does not have to be monitored or\n> changed in order to be valid.\n>\n\nOP_*SIG* semantics don't change here either, we're just adding a superior\nopcode (which in most ways behaves the same as the existing operators). I\nagree with the advantage of not needing to monitor transactions afterwards\nfor malleated inputs, but I think you underestimate the deployment costs.\nIf you want to upgrade the world (eventually, after the old index is\ndropped, which is IMHO the only point where this proposal becomes superior\nto the alternatives) to this, you're changing *every single piece of\nBitcoin software on the planet*. This is not just changing some validation\nrules that are opt-in to use, you're fundamentally changing how\ntransactions refer to each other.\n\nAlso, what do blocks commit to? Do you keep using the old transaction ids\nfor this? Because if you don't, any relayer on the network can invalidate a\nblock (and have the receiver mark it as invalid) by changing the txids. You\nneed to somehow commit to the scriptSig data in blocks still so the POW of\na block is invalidated by changing a scriptSig.\n\nThere certainly are merits using the SIGHASH approach in the short term (it\n> does not require a hard fork), however I think the normalized transaction\n> ID is a cleaner and simpler long-term solution, even though it requires a\n> hard-fork.\n>\n\nIt requires a hard fork, but more importantly, it requires the whole world\nto change their software (not just validation code) to effectively use it.\nThat, plus large up-front deployment costs (doubling the cache size for\nevery full node for the same propagation speed is not a small thing) which\nmay not end up being effective.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/dfcb3945/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-13T19:14:57",
                "message_text_only": "On Wed, May 13, 2015 at 8:40 PM Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Wed, May 13, 2015 at 11:04 AM, Christian Decker <\n> decker.christian at gmail.com> wrote:\n>\n>> If the inputs to my transaction have been long confirmed I can be\n>> reasonably safe in assuming that the transaction hash does not change\n>> anymore. It's true that I have to be careful not to build on top of\n>> transactions that use legacy references to transactions that are\n>> unconfirmed or have few confirmations, however that does not invalidate the\n>> utility of the normalized transaction IDs.\n>>\n>\n> Sufficient confirmations help of course, but make systems like this less\n> useful for more complex interactions where you have multiple unconfirmed\n> transactions waiting on each other. I think being able to rely on this\n> problem being solved unconditionally is what makes the proposal attractive.\n> For the simple cases, see BIP62.\n>\n\nIf we are building a long running contract using a complex chain of\ntransactions, or multiple transactions that depend on each other, there is\nno point in ever using any malleable legacy transaction IDs and I would\nsimply stop cooperating if you tried. I don't think your argument applies.\nIf we build our contract using only normalized transaction IDs there is no\nway of suffering any losses due to malleability.\n\nThe reason I mentioned the confirmation is that all protocols I can think\nof start by collaboratively creating a transaction that locks in funds into\na multisig output, that is committed to the blockchain. Starting from this\ninitial setup transaction would be using normalized transaction IDs,\ntherefore not be susceptible to malleability.\n\n\n>\n> I remember reading about the SIGHASH proposal somewhere. It feels really\n>> hackish to me: It is a substantial change to the way signatures are\n>> verified, I cannot really see how this is a softfork if clients that did\n>> not update are unable to verify transactions using that SIGHASH Flag and it\n>> is adding more data (the normalized hash) to the script, which has to be\n>> stored as part of the transaction. It may be true that a node observing\n>> changes in the input transactions of a transaction using this flag could\n>> fix the problem, however it requires the node's intervention.\n>>\n>\n> I think you misunderstand the idea. This is related, but orthogonal to the\n> ideas about extended the sighash flags that have been discussed here before.\n>\n> All it's doing is adding a new CHECKSIG operator to script, which, in its\n> internally used signature hash, 1) removes the scriptSigs from transactions\n> before hashing 2) replaces the txids in txins by their ntxid. It does not\n> add any data to transactions, and it is a softfork, because it only impacts\n> scripts which actually use the new CHECKSIG operator. Wallets that don't\n> support signing with this new operator would not give out addresses that\n> use it.\n>\n\nIn that case I don't think I heard this proposal before, and I might be\nmissing out :-)\nSo if transaction B spends an output from A, then the input from B contains\nthe CHECKSIG operator telling the validating client to do what exactly? It\nappears that it wants us to go and fetch A, normalize it, put the\nnormalized hash in the txIn of B and then continue the validation? Wouldn't\nthat also need a mapping from the normalized transaction ID to the legacy\ntransaction ID that was confirmed?\n\nA client that did not update still would have no clue on how to handle\nthese transactions, since it simply does not understand the CHECKSIG\noperator. If such a transaction ends up in a block I cannot even catch up\nwith the network since the transaction does not validate for me.\n\nCould you provide an example of how this works?\n\n\n>\n>> Compare that to the simple and clean solution in the proposal, which does\n>> not add extra data to be stored, keeps the OP_*SIG* semantics as they are\n>> and where once you sign a transaction it does not have to be monitored or\n>> changed in order to be valid.\n>>\n>\n> OP_*SIG* semantics don't change here either, we're just adding a superior\n> opcode (which in most ways behaves the same as the existing operators). I\n> agree with the advantage of not needing to monitor transactions afterwards\n> for malleated inputs, but I think you underestimate the deployment costs.\n> If you want to upgrade the world (eventually, after the old index is\n> dropped, which is IMHO the only point where this proposal becomes superior\n> to the alternatives) to this, you're changing *every single piece of\n> Bitcoin software on the planet*. This is not just changing some validation\n> rules that are opt-in to use, you're fundamentally changing how\n> transactions refer to each other.\n>\n\nAs I mentioned before, this is a really long term strategy, hoping to get\nthe cleanest and easiest solution, so that we do not further complicate the\ninner workings of Bitcoin. I don't think that it is completely out of\nquestion to eventually upgrade to use normalized transactions, after all\nthe average lifespan of hardware is a few years tops.\n\n\n>\n> Also, what do blocks commit to? Do you keep using the old transaction ids\n> for this? Because if you don't, any relayer on the network can invalidate a\n> block (and have the receiver mark it as invalid) by changing the txids. You\n> need to somehow commit to the scriptSig data in blocks still so the POW of\n> a block is invalidated by changing a scriptSig.\n>\n\nHow could I change the transaction IDs if I am a relayer? The miner decides\nwhich flavor of IDs it is adding into its merkle tree, the block hash locks\nin the choice. If we saw a transaction having a valid sigScript, it does\nnot matter how we reference it in the block.\n\n\n>\n> There certainly are merits using the SIGHASH approach in the short term\n>> (it does not require a hard fork), however I think the normalized\n>> transaction ID is a cleaner and simpler long-term solution, even though it\n>> requires a hard-fork.\n>>\n>\n> It requires a hard fork, but more importantly, it requires the whole world\n> to change their software (not just validation code) to effectively use it.\n> That, plus large up-front deployment costs (doubling the cache size for\n> every full node for the same propagation speed is not a small thing) which\n> may not end up being effective.\n>\n\nYes, hard forks are hard, I'm under no illusion that pushing such a change\nthrough takes time, but in the end the advantages will prevail.\n\nI didn't want to put it in the initial proposal, but we could also increase\nthe transaction version which signals to the client that the transaction\nmay only be referenced by the normalized transaction ID. So every\ntransaction would be either in one index or the other, reducing the\ndeployment cost to almost nothing.\n\n\n>\n> --\n> Pieter\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/4e66a7fe/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-13T19:40:54",
                "message_text_only": "On Wed, May 13, 2015 at 12:14 PM, Christian Decker <\ndecker.christian at gmail.com> wrote:\n\n>\n> On Wed, May 13, 2015 at 8:40 PM Pieter Wuille <pieter.wuille at gmail.com>\n> wrote:\n>\n>> On Wed, May 13, 2015 at 11:04 AM, Christian Decker <\n>> decker.christian at gmail.com> wrote:\n>>\n>>> If the inputs to my transaction have been long confirmed I can be\n>>> reasonably safe in assuming that the transaction hash does not change\n>>> anymore. It's true that I have to be careful not to build on top of\n>>> transactions that use legacy references to transactions that are\n>>> unconfirmed or have few confirmations, however that does not invalidate the\n>>> utility of the normalized transaction IDs.\n>>>\n>>\n>> Sufficient confirmations help of course, but make systems like this less\n>> useful for more complex interactions where you have multiple unconfirmed\n>> transactions waiting on each other. I think being able to rely on this\n>> problem being solved unconditionally is what makes the proposal attractive.\n>> For the simple cases, see BIP62.\n>>\n>\n> If we are building a long running contract using a complex chain of\n> transactions, or multiple transactions that depend on each other, there is\n> no point in ever using any malleable legacy transaction IDs and I would\n> simply stop cooperating if you tried. I don't think your argument applies.\n> If we build our contract using only normalized transaction IDs there is no\n> way of suffering any losses due to malleability.\n>\n\nThat's correct as long as you stay within your contract, but you likely\nwant compatibility with other software, without waiting an age before and\nafter your contract settles on the chain. It's a weaker argument, though, I\nagree.\n\nI remember reading about the SIGHASH proposal somewhere. It feels really\n>>> hackish to me: It is a substantial change to the way signatures are\n>>> verified, I cannot really see how this is a softfork if clients that did\n>>> not update are unable to verify transactions using that SIGHASH Flag and it\n>>> is adding more data (the normalized hash) to the script, which has to be\n>>> stored as part of the transaction. It may be true that a node observing\n>>> changes in the input transactions of a transaction using this flag could\n>>> fix the problem, however it requires the node's intervention.\n>>>\n>>\n>> I think you misunderstand the idea. This is related, but orthogonal to\n>> the ideas about extended the sighash flags that have been discussed here\n>> before.\n>>\n>> All it's doing is adding a new CHECKSIG operator to script, which, in its\n>> internally used signature hash, 1) removes the scriptSigs from transactions\n>> before hashing 2) replaces the txids in txins by their ntxid. It does not\n>> add any data to transactions, and it is a softfork, because it only impacts\n>> scripts which actually use the new CHECKSIG operator. Wallets that don't\n>> support signing with this new operator would not give out addresses that\n>> use it.\n>>\n>\n> In that case I don't think I heard this proposal before, and I might be\n> missing out :-)\n> So if transaction B spends an output from A, then the input from B\n> contains the CHECKSIG operator telling the validating client to do what\n> exactly? It appears that it wants us to go and fetch A, normalize it, put\n> the normalized hash in the txIn of B and then continue the validation?\n> Wouldn't that also need a mapping from the normalized transaction ID to the\n> legacy transaction ID that was confirmed?\n>\n\nThere would just be an OP_CHECKAWESOMESIG, which can do anything. It can\nidentical to how OP_CHECKSIG works now, but has a changed algorithm for its\nsignature hash algorithm. Optionally (and likely in practice, I think), it\ncan do various other proposed improvements, like using Schnorr signatures,\nhaving a smaller signature encoding, supporting batch validation, have\nextended sighash flags, ...\n\nIt wouldn't fetch A and normalize it; that's impossible as you would need\nto go fetch all of A's dependencies too and recurse until you hit the\ncoinbases that produced them. Instead, your UTXO set contains the\nnormalized txid for every normal txid (which adds around 26% to the UTXO\nset size now), but lookups in it remain only by txid.\n\nYou don't need a ntxid->txid mapping, as transactions and blocks keep\nreferring to transactions by txid. Only the OP_CHECKAWESOMESIG operator\nwould do the conversion, and at most once.\n\nA client that did not update still would have no clue on how to handle\n> these transactions, since it simply does not understand the CHECKSIG\n> operator. If such a transaction ends up in a block I cannot even catch up\n> with the network since the transaction does not validate for me.\n>\n\nAs for every softfork, it works by redefining an OP_NOP operator, so old\nnodes simply consider these checksigs unconditionally valid. That does mean\nyou don't want to use them before the consensus rule is forked in\n(=enforced by a majority of the hashrate), and that you suffer from the\ntemporary security reduction that an old full node is unknowingly reduced\nto SPV security for these opcodes. However, as full node wallet, this\nproblem does not affect you, as your wallet would simply not give out\naddresses using the new opcode (and thus, wouldn't receive coins using it),\nunless it was upgraded to support it.\n\nCould you provide an example of how this works?\n>\n>\n>>\n>>> Compare that to the simple and clean solution in the proposal, which\n>>> does not add extra data to be stored, keeps the OP_*SIG* semantics as they\n>>> are and where once you sign a transaction it does not have to be monitored\n>>> or changed in order to be valid.\n>>>\n>>\n>> OP_*SIG* semantics don't change here either, we're just adding a superior\n>> opcode (which in most ways behaves the same as the existing operators). I\n>> agree with the advantage of not needing to monitor transactions afterwards\n>> for malleated inputs, but I think you underestimate the deployment costs.\n>> If you want to upgrade the world (eventually, after the old index is\n>> dropped, which is IMHO the only point where this proposal becomes superior\n>> to the alternatives) to this, you're changing *every single piece of\n>> Bitcoin software on the planet*. This is not just changing some validation\n>> rules that are opt-in to use, you're fundamentally changing how\n>> transactions refer to each other.\n>>\n>\n> As I mentioned before, this is a really long term strategy, hoping to get\n> the cleanest and easiest solution, so that we do not further complicate the\n> inner workings of Bitcoin. I don't think that it is completely out of\n> question to eventually upgrade to use normalized transactions, after all\n> the average lifespan of hardware is a few years tops.\n>\n\nFair enough, I definitely agree the end result is superior in this case.\n\nAlso, what do blocks commit to? Do you keep using the old transaction ids\n>> for this? Because if you don't, any relayer on the network can invalidate a\n>> block (and have the receiver mark it as invalid) by changing the txids. You\n>> need to somehow commit to the scriptSig data in blocks still so the POW of\n>> a block is invalidated by changing a scriptSig.\n>>\n>\n> How could I change the transaction IDs if I am a relayer? The miner\n> decides which flavor of IDs it is adding into its merkle tree, the block\n> hash locks in the choice. If we saw a transaction having a valid sigScript,\n> it does not matter how we reference it in the block.\n>\n\nIf the merkle tree of a block only commits to a transaction's normalized\nhash, that means that the block hash does not change when the scriptSig is\naltered. So, anyone on the network can take a random valid block, and\nmodify its scriptSig, and the block will become invalid _without_\ninvalidating the block header. This means that nodes on the network will\nnow classify that block header as having invalid transactions, and reject\nit. Not having the ability anymore to mark blocks as invalid opens\nsignificant DoS risks.\n\nSo yes, seeing a block with valid scriptSigs is indeed a proof the\ntransaction was legitimately authored. But the oppose is no longer true,\nand we need that. The correct solution is to either keep using the old full\ntransaction ids in blocks, but ntxids everywhere else, or having some\nalternative means to commit to the scriptSigs inside the block (for example\nin the coinbase or using one of the more efficient block commitment\nproposals), and have that enforced as consensus rule.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/1f2ceb1f/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T18:11:30",
                "message_text_only": "On Wed, May 13, 2015 at 6:14 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> Normalized transaction ids are only effectively non-malleable when all\n> inputs they refer to are also non-malleable (or you can have malleability\n> in 2nd level dependencies), so I do not believe it makes sense to allow\n> mixed usage of the txids at all.\n>\n\nThe txid or txid-norm is signed, so can't be changed after signing.\n\nThe hard fork is to allow transactions to refer to their inputs by txid or\ntxid-norm.  You pick one before signing.\n\n> They do not provide the actual benefit of guaranteed non-malleability\n> before it becomes disallowed to use the old mechanism.\n>\nA signed transaction cannot have its txid changed.  It is true that users\nof the system would have to use txid-norm.\n\nThe basic refund transaction is as follows.\n\n A creates TX1: \"Pay w BTC to <B's public key> if signed by A & B\"\n\n A creates TX2: \"Pay w BTC from TX1-norm to <A's public key>, locked 48\nhours in the future, signed by A\"\n\n A sends TX2 to B\n\n B signs TX2 and returns to A\n\nA broadcasts TX1.  It is mutated before entering the chain to become\nTX1-mutated.\n\nA can still submit TX2 to the blockchain, since TX1 and TX1-mutated have\nthe same txid-norm.\n\n>\n> That, together with the +- resource doubling needed for the UTXO set (as\n> earlier mentioned) and the fact that an alternative which is only a\n> softfork are available, makes this a bad idea IMHO.\n>\n> Unsure to what extent this has been presented on the mailinglist, but the\n> softfork idea is this:\n> * Transactions get 2 txids, one used to reference them (computed as\n> before), and one used in an (extended) sighash.\n> * The txins keep using the normal txid, so not structural changes to\n> Bitcoin.\n> * The ntxid is computed by replacing the scriptSigs in inputs by the empty\n> string, and by replacing the txids in txins by their corresponding ntxids.\n> * A new checksig operator is softforked in, which uses the ntxids in its\n> sighashes rather than the full txid.\n> * To support efficiently computing ntxids, every tx in the utxo set\n> (currently around 6M) stores the ntxid, but only supports lookup bu txid\n> still.\n>\n> This does result in a system where a changed dependency indeed invalidates\n> the spending transaction, but the fix is trivial and can be done without\n> access to the private key.\n>\nThe problem with this is that 2 level malleability is not protected against.\n\nC spends B which spends A.\n\nA is mutated before it hits the chain.  The only change in A is in the\nscriptSig.\n\nB can be converted to B-new without breaking the signature.  This is\nbecause the only change to A was in the sciptSig, which is dropped when\ncomputing the txid-norm.\n\nB-new spends A-mutated.  B-new is different from B in a different place.\nThe txid it uses to refer to the previous output is changed.\n\nThe signed transaction C cannot be converted to a valid C-new.  The txid of\nthe input points to B.  It is updated to point at B-new.  B-new and B don't\nhave the same txid-norm, since the change is outside the scriptSig.  This\nmeans that the signature for C is invalid.\n\nThe txid replacements should be done recursively.  All input txids should\nbe replaced by txid-norms when computing the txid-norm for the\ntransaction.  I think this repairs the problem with only allowing one level?\n\nComputing txid-norm:\n\n- replace all txids in inputs with txid-norms of those transactions\n- replace all input scriptSigs with empty scripts\n- transaction hash is txid-norm for that transaction\n\nThe same situation as above is not fatal now.\n\nC spends B which spends A.\n\nA is mutated before it hits the chain.  The only change in A is in the\nscriptSig.\n\nB can be converted to B-new without breaking the signature.  This is\nbecause the only change to A was in the sciptSig, which is dropped when\ncomputing the txid-norm (as before).\n\nB-new spends A mutated.  B-new is different from B in for the previous\ninputs.\n\nThe input for B-new points to A-mutated.  When computing the txid-norm,\nthat would be replaced with the txid-norm for A.\n\nSimilarly, the input for B points to A and that would have been replaced\nwith the txid-norm for A.\n\nThis means that B and B-new have the same txid-norm.\n\nThe signed transaction C can be converted to a valid C-new.  The txid of\nthe input points to B.  It is updated to point at B-new.  B-new and B now\nhave have the same txid-norm and so C is valid.\n\nI think this reasoning is valid, but probably needs writing out actual\nserializations.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/b02d5c75/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T20:27:14",
                "message_text_only": "After more thought, I think I came up with a clearer description of the\nrecursive version.\n\nThe simple definition is that the hash for the new signature opcode should\nsimply assume that the normalized txid system was used since the\nbeginning.  All txids in the entire blockchain should be replaced with the\n\"correct\" values.\n\nThis requires a full re-index of the blockchain.  You can't work out what\nthe TXID-N of a transaction is without knowning the TXID-N of its parents,\nin order to do the replacement.\n\nThe non-recursive version can only handle refunds one level deep.\n\nA:\nfrom: IN\nsigA: based on hash(...)\n\nB:\nfrom A\nsig: based on hash(from: TXID-N(A) | \"\")  // sig removed\n\nC:\nfrom B\nsig: based on hash(from: TXID-N(B) | \"\")  // sig removed\n\nIf A is mutated before being added into the chain, then B can be modified\nto a valid transaction (B-new).\n\nA-mutated:\nfrom: IN\nsig_mutated: based on hash(...) with some mutation\n\nB has to be modified to B-new to make it valid.\n\nB-new:\nfrom A-mutated\nsig: based on hash(from: TXID-N(A-mutated), \"\")\n\nSince TXID-N(A-mutated) is equal to TXID-N(A), the signature from B is\nstill valid.\n\nHowver, C-new cannot be created.\n\nC-new:\nfrom B-new\nsig: based on hash(from: TXID-N(B-new), \"\")\n\nTXID-N(B-new) is not the same as TXID-N(B).  Since the from field is not\nremoved by the TXID-N operation, differences in that field mean that the\nTXIDs are difference.\n\nThis means that the signature for C is not valid for C-new.\n\nThe recursive version repairs this problem.\n\nRather than simply delete the scriptSig from the transaction.  All txids\nmust also be replaced with their TXID-N versions.\n\nAgain, A is mutated before being added into the chain and B-new is produced.\n\nA-mutated:\nfrom: IN\nsig_mutated: based on hash(...) with some mutation\nTXID-N: TXID-N(A)\n\nB has to be modified to B-new to make it valid.\n\nB-new:\nfrom A-mutated\nsig: based on hash(from: TXID-N(A-mutated), \"\")\nTXID-N: TXID-N(B)\n\nSince TXID-N(A-mutated) is equal to TXID-N(A), the signature from B is\nstill valid.\n\nLikewise the TXID-N(B-new) is equal to TXID-N(B).\n\nThe from field is replaced by the TXID-N from A-mutated which is equal to\nTXID-N(A) and the sig is the same.\n\nC-new:\nfrom B-new\nsig: based on hash(from: TXID-N(B-new), \"\")\n\nThe signature is still valid, since TXID-N(B-new) is the same as TXID-N(B).\n\nThis means that multi-level refunds are possible.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/39c05fa9/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-13T20:31:06",
                "message_text_only": "On Wed, May 13, 2015 at 1:27 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n> After more thought, I think I came up with a clearer description of the\n> recursive version.\n>\n> The simple definition is that the hash for the new signature opcode should\n> simply assume that the normalized txid system was used since the\n> beginning.  All txids in the entire blockchain should be replaced with the\n> \"correct\" values.\n>\n> This requires a full re-index of the blockchain.  You can't work out what\n> the TXID-N of a transaction is without knowning the TXID-N of its parents,\n> in order to do the replacement.\n>\n> The non-recursive version can only handle refunds one level deep.\n>\n\nThis was what I was suggesting all along, sorry if I wasn't clear.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/049abb31/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-13T20:32:43",
                "message_text_only": "On Wed, May 13, 2015 at 9:31 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n>\n> This was what I was suggesting all along, sorry if I wasn't clear.\n>\n>\nThat's great.  So, basically the multi-level refund problem is solved by\nthis?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/9199160d/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-14T00:37:30",
                "message_text_only": "On Wed, May 13, 2015 at 1:32 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n>\n> On Wed, May 13, 2015 at 9:31 PM, Pieter Wuille <pieter.wuille at gmail.com>\n> wrote:\n>\n>>\n>> This was what I was suggesting all along, sorry if I wasn't clear.\n>>\n>> That's great.  So, basically the multi-level refund problem is solved by\n> this?\n>\n\nYes. So to be clear, I think there are 2 desirable end-goal proposals\n(ignoring difficulty of changing things for a minute):\n\n* Transactions and blocks keep referring to other transactions by full\ntxid, but signature hashes are computed off normalized txids (which are\nrecursively defined to use normalized txids all the way back to coinbases).\nIs this what you are suggesting now as well?\n\n* Blocks commit to full transaction data, but transactions and signature\nhashes use normalized txids.\n\nThe benefit of the latter solution is that it doesn't need \"fixing up\"\ntransactions whose inputs have been malleated, but comes at the cost of\ndoing a very invasive hard fork.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150513/36bd2804/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-14T11:01:56",
                "message_text_only": "Ok, I think I got the OP_CHECKAWESOMESIG proposal, transactions keep\nreferencing using hashes of complete transactions (including signatures),\nwhile the OP_CHECKAWESOMESIG looks up the previous transaction (which we\nalready need to do anyway in order to insert the prevOut pubkeyScript),\nnormalizes the prevout and calculates its normalized transaction ID. It\nthen inserts the normalized transaction IDs in the OutPoint before\ncalculating its own hash which is then signed. Is that correct so far?\n\nLet me try to summarize the discussion so far:\n\nI think we have consensus that transaction malleability needs to be\naddressed, and normalized transaction IDs seem to be the way to go forward.\n\nThe discussion now is how to use normalized transaction IDs and we have two\napproaches to implement them:\n\n   - OP_CHECKAWESOMESIG which continues to use the current hashes to\n   reference a specific signed instance of a class of semantically identical\n   transactions. Internally only the semantic class is enforced. Transactions\n   can be fixed to reference the correct signed instance if the transaction\n   has been changed along the way.is a softfork using the \"if I don't know\n   this opcode the TX is automatically valid\" trick\n\n\nOn Thu, May 14, 2015 at 2:40 AM Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Wed, May 13, 2015 at 1:32 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n>\n>>\n>> On Wed, May 13, 2015 at 9:31 PM, Pieter Wuille <pieter.wuille at gmail.com>\n>> wrote:\n>>\n>>>\n>>> This was what I was suggesting all along, sorry if I wasn't clear.\n>>>\n>>> That's great.  So, basically the multi-level refund problem is solved by\n>> this?\n>>\n>\n> Yes. So to be clear, I think there are 2 desirable end-goal proposals\n> (ignoring difficulty of changing things for a minute):\n>\n> * Transactions and blocks keep referring to other transactions by full\n> txid, but signature hashes are computed off normalized txids (which are\n> recursively defined to use normalized txids all the way back to coinbases).\n> Is this what you are suggesting now as well?\n>\n> * Blocks commit to full transaction data, but transactions and signature\n> hashes use normalized txids.\n>\n> The benefit of the latter solution is that it doesn't need \"fixing up\"\n> transactions whose inputs have been malleated, but comes at the cost of\n> doing a very invasive hard fork.\n>\n> --\n> Pieter\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150514/7291d57f/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-14T11:26:44",
                "message_text_only": "Sorry about that, sometimes I hate keyboard shortcuts :-)\n\n\nOk, I think I got the OP_CHECKAWESOMESIG proposal, transactions keep\nreferencing using hashes of complete transactions (including signatures),\nwhile the OP_CHECKAWESOMESIG looks up the previous transaction (which we\nalready need to do anyway in order to insert the prevOut pubkeyScript),\nnormalizes the prevout and calculates its normalized transaction ID. It\nthen inserts the normalized transaction IDs in the OutPoint before\ncalculating its own hash which is then signed. Is that correct so far?\n\nLet me try to summarize the discussion so far:\n\nI think we have consensus that transaction malleability needs to be\naddressed, and normalized transaction IDs seem to be the way to go forward.\n\nThe discussion now is how to use normalized transaction IDs and we have two\napproaches to implement them:\n\n\n   - OP_CHECKAWESOMESIG which continues to use the current hashes to\n   reference a specific signed instance of a class of semantically identical\n   transactions. Internally only the semantic class is enforced. Transactions\n   can be fixed to reference the correct signed instance if the transaction\n   has been changed along the way.\n   - The second proposal advocates using the normalized transaction IDs\n   directly in the transactions, requiring no further intervention to fix an\n   eventually malleated transaction.\n\nBoth approaches have their own advantages and problems:\n\nOP_CHECKAWESOMESIG is a soft-fork which makes it somewhat less problematic\nto roll-out and does not break existing software. The normalized\ntransaction ID can be computed on the fly (possibly increasing lookup\ntimes) or stored alongside the UTXO (increasing storage needs). If the\nnormalized transaction IDs really need to be recomputed down to the\ncoinbase then the increased storage is the only option, and would add 32\nbyte to every transaction metadata in the UTXO.\n\nMy proposal is harder to migrate to, as it requires a hardfork, and will\nrequire more storage (64 byte raw data for a normalized to legacy\ntransaction ID) for every transaction in the UTXO set. At 6 million\ndistinct transactions which unspent outputs this boils down to 384 MB\n(though this may change in future by introducing an aggregation strategy or\nfragment further). Some of that space may be reclaimed. There is absolutely\nno interaction required to fix up transactions if a dependency has been\nmalleated, since we address a semantic class, not the specific instance. We\nlimit the use of normalized transaction IDs to the OutPoint in\ntransactions, since there we want to reference the semantic class not the\nactual signed instance. At protocol message level (inv, getdata) and blocks\nwe continue to use the legacy ID. This is not as nice as having one ID for\nevery transaction that is used everywhere.\n\nBoth solutions solve malleability, just with different tradeoffs.\n\nI don't see them as mutually exclusive, if we adopt the OP_CHECKAWESOMESIG\nas short term fix, that can be rolled out and applied, then my proposal can\nbe seen as long-term goal that is semantically cleaner and easier to\nimplement.\n\nPersonally I think hard-forks shouldn't be the dreaded boogeyman everybody\nmakes them out to be, we have never really tested rolling out a hardfork\nand they might just turn out to be possible. I don't thing we loose\nanything by attempting this, except maybe reduce the urgency to apply some\nperfect future thing.\n\nRegards,\nChristian\n\nOn Thu, May 14, 2015 at 1:01 PM, Christian Decker <\ndecker.christian at gmail.com> wrote:\n\n> Ok, I think I got the OP_CHECKAWESOMESIG proposal, transactions keep\n> referencing using hashes of complete transactions (including signatures),\n> while the OP_CHECKAWESOMESIG looks up the previous transaction (which we\n> already need to do anyway in order to insert the prevOut pubkeyScript),\n> normalizes the prevout and calculates its normalized transaction ID. It\n> then inserts the normalized transaction IDs in the OutPoint before\n> calculating its own hash which is then signed. Is that correct so far?\n>\n> Let me try to summarize the discussion so far:\n>\n> I think we have consensus that transaction malleability needs to be\n> addressed, and normalized transaction IDs seem to be the way to go forward.\n>\n> The discussion now is how to use normalized transaction IDs and we have\n> two approaches to implement them:\n>\n>    - OP_CHECKAWESOMESIG which continues to use the current hashes to\n>    reference a specific signed instance of a class of semantically identical\n>    transactions. Internally only the semantic class is enforced. Transactions\n>    can be fixed to reference the correct signed instance if the transaction\n>    has been changed along the way.is a softfork using the \"if I don't\n>    know this opcode the TX is automatically valid\" trick\n>\n>\n> On Thu, May 14, 2015 at 2:40 AM Pieter Wuille <pieter.wuille at gmail.com>\n> wrote:\n>\n>> On Wed, May 13, 2015 at 1:32 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n>>\n>>>\n>>> On Wed, May 13, 2015 at 9:31 PM, Pieter Wuille <pieter.wuille at gmail.com>\n>>> wrote:\n>>>\n>>>>\n>>>> This was what I was suggesting all along, sorry if I wasn't clear.\n>>>>\n>>>> That's great.  So, basically the multi-level refund problem is solved\n>>> by this?\n>>>\n>>\n>> Yes. So to be clear, I think there are 2 desirable end-goal proposals\n>> (ignoring difficulty of changing things for a minute):\n>>\n>> * Transactions and blocks keep referring to other transactions by full\n>> txid, but signature hashes are computed off normalized txids (which are\n>> recursively defined to use normalized txids all the way back to coinbases).\n>> Is this what you are suggesting now as well?\n>>\n>> * Blocks commit to full transaction data, but transactions and signature\n>> hashes use normalized txids.\n>>\n>> The benefit of the latter solution is that it doesn't need \"fixing up\"\n>> transactions whose inputs have been malleated, but comes at the cost of\n>> doing a very invasive hard fork.\n>>\n>> --\n>> Pieter\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150514/acae1f4f/attachment.html>"
            },
            {
                "author": "s7r",
                "date": "2015-05-15T09:54:55",
                "message_text_only": "Hello,\n\nHow will this exactly be safe against:\na) the malleability of the parent tx (2nd level malleability)\nb) replays\n\nIf you strip just the scriptSig of the input(s), the txid(s) can still\nbe mutated (with higher probability before it gets confirmed).\n\nIf you strip both the scriptSig of the parent and the txid, nothing can\nany longer be mutated but this is not safe against replays. This could\nwork if we were using only one scriptPubKey per tx. But this is not\nenforced, and I don't think it's the proper way to do it.\n\nSomething similar can be achieved if you would use a combination of\nflags from here:\n\nhttps://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md\n\nBut this has some issues too.\n\nI've read your draft but didn't understand how exactly will this prevent\nnormal malleability as we know it, second level malleability and replays\nas well as how will we do the transition into mapping the txes in the\nblockchain to normalized txids. Looking forward to read more on this\ntopic. Thanks for the brainstorming ;)\n\n\nOn 5/13/2015 3:48 PM, Christian Decker wrote:\n> Hi All,\n> \n> I'd like to propose a BIP to normalize transaction IDs in order to\n> address transaction malleability and facilitate higher level protocols.\n> \n> The normalized transaction ID is an alias used in parallel to the\n> current (legacy) transaction IDs to address outputs in transactions. It\n> is calculated by removing (zeroing) the scriptSig before computing the\n> hash, which ensures that only data whose integrity is also guaranteed by\n> the signatures influences the hash. Thus if anything causes the\n> normalized ID to change it automatically invalidates the signature. When\n> validating a client supporting this BIP would use both the normalized tx\n> ID as well as the legacy tx ID when validating transactions.\n> \n> The detailed writeup can be found\n> here: https://github.com/cdecker/bips/blob/normalized-txid/bip-00nn.mediawiki.\n> \n> @gmaxwell: I'd like to request a BIP number, unless there is something\n> really wrong with the proposal.\n> \n> In addition to being a simple alternative that solves transaction\n> malleability it also hugely simplifies higher level protocols. We can\n> now use template transactions upon which sequences of transactions can\n> be built before signing them.\n> \n> I hesitated quite a while to propose it since it does require a hardfork\n> (old clients would not find the prevTx identified by the normalized\n> transaction ID and deem the spending transaction invalid), but it seems\n> that hardforks are no longer the dreaded boogeyman nobody talks about.\n> I left out the details of how the hardfork is to be done, as it does not\n> really matter and we may have a good mechanism to apply a bunch of\n> hardforks concurrently in the future.\n> \n> I'm sure it'll take time to implement and upgrade, but I think it would\n> be a nice addition to the functionality and would solve a long standing\n> problem :-)\n> \n> Please let me know what you think, the proposal is definitely not set in\n> stone at this point and I'm sure we can improve it further.\n> \n> Regards,\n> Christian\n> \n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> \n> \n> \n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-15T10:45:05",
                "message_text_only": "On Fri, May 15, 2015 at 10:54 AM, s7r <s7r at sky-ip.org> wrote:\n\n> Hello,\n>\n> How will this exactly be safe against:\n> a) the malleability of the parent tx (2nd level malleability)\n>\n\nThe signature signs everything except the signature itself.  The normalized\ntxid doesn't include that signature, so mutations of the signature don't\ncause the normalized txid to change.\n\nIf the refund transaction refers to the parent using the normalised txid,\nthen it doesn't matter if the parent has a mutated signature.  The\nnormalized transaction ignores the mutation.\n\nIf the parent is mutated, then the refund doesn't even have to be modified,\nit still refers to it.\n\nIf you want a multi-level refund transaction, then all refund transactions\nmust use the normalized txids to refer to their parents.  The \"root\"\ntransaction is submitted to the blockchain and locked down.\n\n\n> b) replays\n>\n\nIf there are 2 transactions which are mutations of each other, then only\none can be added to the block chain, since the other is a double spend.\n\nThe normalized txid refers to all of them, rather than a specific\ntransaction.\n\n\n> If you strip just the scriptSig of the input(s), the txid(s) can still\n> be mutated (with higher probability before it gets confirmed).\n>\n\nMutation is only a problem if it occurs after signing.  The signature signs\neverything except the signature itself.\n\n\n> If you strip both the scriptSig of the parent and the txid, nothing can\n> any longer be mutated but this is not safe against replays.\n\n\nCorrect, but normalized txids are safe against replays, so are better.\n\nI think the new signature opcode fixes things too.  The question is hard\nfork but clean solution vs a soft fork but a little more hassle.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150515/5b2b0e9e/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-05-15T16:31:47",
                "message_text_only": "On Friday, May 15, 2015 9:54:55 AM s7r wrote:\n> If you strip both the scriptSig of the parent and the txid, nothing can\n> any longer be mutated but this is not safe against replays. This could\n> work if we were using only one scriptPubKey per tx. But this is not\n> enforced, ...\n\nAssuming you mean one output per scriptPubKey (and not limiting tx to one \noutput), the alternative is essentially undefined, and creates real problems \nfor Bitcoin today. It's not something we should go out of the way to support \nor encourage. Therefore, regardless of whatever other options are available, I \nwould like to see a scriptPubKey-only sighash type for strong safety within \nall malleability situations (including CoinJoin and other sender-respends) \nthat more advanced wallet software could take advantage of in the future \n(while strictly enforcing no-reuse on its own wallet to avoid known replays).\n\nLuke"
            },
            {
                "author": "Stephen",
                "date": "2015-05-16T03:58:56",
                "message_text_only": "We should make sure to consider how BIP34 affects normalized transaction ids, since the height of the block is included in the scriptSig ensuring that the txid will be different. We wouldn't want to enable replay attacks in the form of spending coinbase outputs in the same way they were spent from a previous block. \n\nSo maybe normalized txids should strip the scriptSigs of all transactions except for coinbase transactions? This seems to make sense, since coinbase transactions are inherently not malleable anyway. \n\nAlso, s7r linked to my 'Build your own nHashType' proposal (although V2 is here: https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal_v2.md). I just wanted to add that I think even with normalized ids, it could still be useful to be able to apply these flags to choose which parts of the transaction become signed. I've also seen vague references to some kind of a merklized abstract syntax tree, but am not fully sure how that would work. Maybe someone on here could explain it? \n\nBest,\nStephen\n\n\n\n> On May 15, 2015, at 5:54 AM, s7r <s7r at sky-ip.org> wrote:\n> \n> Hello,\n> \n> How will this exactly be safe against:\n> a) the malleability of the parent tx (2nd level malleability)\n> b) replays\n> \n> If you strip just the scriptSig of the input(s), the txid(s) can still\n> be mutated (with higher probability before it gets confirmed).\n> \n> If you strip both the scriptSig of the parent and the txid, nothing can\n> any longer be mutated but this is not safe against replays. This could\n> work if we were using only one scriptPubKey per tx. But this is not\n> enforced, and I don't think it's the proper way to do it.\n> \n> Something similar can be achieved if you would use a combination of\n> flags from here:\n> \n> https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md\n> \n> But this has some issues too.\n> \n> I've read your draft but didn't understand how exactly will this prevent\n> normal malleability as we know it, second level malleability and replays\n> as well as how will we do the transition into mapping the txes in the\n> blockchain to normalized txids. Looking forward to read more on this\n> topic. Thanks for the brainstorming ;)\n> \n> \n>> On 5/13/2015 3:48 PM, Christian Decker wrote:\n>> Hi All,\n>> \n>> I'd like to propose a BIP to normalize transaction IDs in order to\n>> address transaction malleability and facilitate higher level protocols.\n>> \n>> The normalized transaction ID is an alias used in parallel to the\n>> current (legacy) transaction IDs to address outputs in transactions. It\n>> is calculated by removing (zeroing) the scriptSig before computing the\n>> hash, which ensures that only data whose integrity is also guaranteed by\n>> the signatures influences the hash. Thus if anything causes the\n>> normalized ID to change it automatically invalidates the signature. When\n>> validating a client supporting this BIP would use both the normalized tx\n>> ID as well as the legacy tx ID when validating transactions.\n>> \n>> The detailed writeup can be found\n>> here: https://github.com/cdecker/bips/blob/normalized-txid/bip-00nn.mediawiki.\n>> \n>> @gmaxwell: I'd like to request a BIP number, unless there is something\n>> really wrong with the proposal.\n>> \n>> In addition to being a simple alternative that solves transaction\n>> malleability it also hugely simplifies higher level protocols. We can\n>> now use template transactions upon which sequences of transactions can\n>> be built before signing them.\n>> \n>> I hesitated quite a while to propose it since it does require a hardfork\n>> (old clients would not find the prevTx identified by the normalized\n>> transaction ID and deem the spending transaction invalid), but it seems\n>> that hardforks are no longer the dreaded boogeyman nobody talks about.\n>> I left out the details of how the hardfork is to be done, as it does not\n>> really matter and we may have a good mechanism to apply a bunch of\n>> hardforks concurrently in the future.\n>> \n>> I'm sure it'll take time to implement and upgrade, but I think it would\n>> be a nice addition to the functionality and would solve a long standing\n>> problem :-)\n>> \n>> Please let me know what you think, the proposal is definitely not set in\n>> stone at this point and I'm sure we can improve it further.\n>> \n>> Regards,\n>> Christian\n>> \n>> \n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud \n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> \n>> \n>> \n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> \n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud \n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-16T10:52:34",
                "message_text_only": "On Sat, May 16, 2015 at 4:58 AM, Stephen <stephencalebmorse at gmail.com>\nwrote:\n\n> We should make sure to consider how BIP34 affects normalized transaction\n> ids, since the height of the block is included in the scriptSig ensuring\n> that the txid will be different. We wouldn't want to enable replay attacks\n> in the form of spending coinbase outputs in the same way they were spent\n> from a previous block.\n>\n> So maybe normalized txids should strip the scriptSigs of all transactions\n> except for coinbase transactions? This seems to make sense, since coinbase\n> transactions are inherently not malleable anyway.\n>\n\nThat is a good point.  Since the point is the change is to use good\npractice right back until the genesis block, maybe the scriptSig for\ncoinbases could be replaced by the height expressed as a varint.  That\nmeans that all coinbases get a unique normalized txid.  The coinbases with\nduplicate txids still wouldn't be spendable though.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150516/3f138563/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-19T08:28:58",
                "message_text_only": "Thanks Stephen, I hadn't thought about BIP 34 and we need to address this\nin both proposals. If we can avoid it I'd like not to have one transaction\nhashed one way and other transactions in another way.\n\nSince BIP 34 explicitly uses the scriptSig to make the coinbase transaction\nunique, simply removing the scriptSig is not an option as it would\npotentially cause collisions. I don't remember why the scriptSig was\nchosen, but we also have the option of putting the blockchain height in the\nsequence number of the coinbase input or the locktime of the transaction,\nrestoring the uniqueness constraint in normalized transaction IDs (for both\nproposals). Is there a specific reason why that was not chosen at the time?\n\nOn Sat, May 16, 2015 at 5:58 AM Stephen <stephencalebmorse at gmail.com> wrote:\n\n> We should make sure to consider how BIP34 affects normalized transaction\n> ids, since the height of the block is included in the scriptSig ensuring\n> that the txid will be different. We wouldn't want to enable replay attacks\n> in the form of spending coinbase outputs in the same way they were spent\n> from a previous block.\n>\n> So maybe normalized txids should strip the scriptSigs of all transactions\n> except for coinbase transactions? This seems to make sense, since coinbase\n> transactions are inherently not malleable anyway.\n>\n> Also, s7r linked to my 'Build your own nHashType' proposal (although V2 is\n> here:\n> https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal_v2.md).\n> I just wanted to add that I think even with normalized ids, it could still\n> be useful to be able to apply these flags to choose which parts of the\n> transaction become signed. I've also seen vague references to some kind of\n> a merklized abstract syntax tree, but am not fully sure how that would\n> work. Maybe someone on here could explain it?\n>\n> Best,\n> Stephen\n>\n>\n>\n> > On May 15, 2015, at 5:54 AM, s7r <s7r at sky-ip.org> wrote:\n> >\n> > Hello,\n> >\n> > How will this exactly be safe against:\n> > a) the malleability of the parent tx (2nd level malleability)\n> > b) replays\n> >\n> > If you strip just the scriptSig of the input(s), the txid(s) can still\n> > be mutated (with higher probability before it gets confirmed).\n> >\n> > If you strip both the scriptSig of the parent and the txid, nothing can\n> > any longer be mutated but this is not safe against replays. This could\n> > work if we were using only one scriptPubKey per tx. But this is not\n> > enforced, and I don't think it's the proper way to do it.\n> >\n> > Something similar can be achieved if you would use a combination of\n> > flags from here:\n> >\n> > https://github.com/scmorse/bitcoin-misc/blob/master/sighash_proposal.md\n> >\n> > But this has some issues too.\n> >\n> > I've read your draft but didn't understand how exactly will this prevent\n> > normal malleability as we know it, second level malleability and replays\n> > as well as how will we do the transition into mapping the txes in the\n> > blockchain to normalized txids. Looking forward to read more on this\n> > topic. Thanks for the brainstorming ;)\n> >\n> >\n> >> On 5/13/2015 3:48 PM, Christian Decker wrote:\n> >> Hi All,\n> >>\n> >> I'd like to propose a BIP to normalize transaction IDs in order to\n> >> address transaction malleability and facilitate higher level protocols.\n> >>\n> >> The normalized transaction ID is an alias used in parallel to the\n> >> current (legacy) transaction IDs to address outputs in transactions. It\n> >> is calculated by removing (zeroing) the scriptSig before computing the\n> >> hash, which ensures that only data whose integrity is also guaranteed by\n> >> the signatures influences the hash. Thus if anything causes the\n> >> normalized ID to change it automatically invalidates the signature. When\n> >> validating a client supporting this BIP would use both the normalized tx\n> >> ID as well as the legacy tx ID when validating transactions.\n> >>\n> >> The detailed writeup can be found\n> >> here:\n> https://github.com/cdecker/bips/blob/normalized-txid/bip-00nn.mediawiki.\n> >>\n> >> @gmaxwell: I'd like to request a BIP number, unless there is something\n> >> really wrong with the proposal.\n> >>\n> >> In addition to being a simple alternative that solves transaction\n> >> malleability it also hugely simplifies higher level protocols. We can\n> >> now use template transactions upon which sequences of transactions can\n> >> be built before signing them.\n> >>\n> >> I hesitated quite a while to propose it since it does require a hardfork\n> >> (old clients would not find the prevTx identified by the normalized\n> >> transaction ID and deem the spending transaction invalid), but it seems\n> >> that hardforks are no longer the dreaded boogeyman nobody talks about.\n> >> I left out the details of how the hardfork is to be done, as it does not\n> >> really matter and we may have a good mechanism to apply a bunch of\n> >> hardforks concurrently in the future.\n> >>\n> >> I'm sure it'll take time to implement and upgrade, but I think it would\n> >> be a nice addition to the functionality and would solve a long standing\n> >> problem :-)\n> >>\n> >> Please let me know what you think, the proposal is definitely not set in\n> >> stone at this point and I'm sure we can improve it further.\n> >>\n> >> Regards,\n> >> Christian\n> >>\n> >>\n> >>\n> ------------------------------------------------------------------------------\n> >> One dashboard for servers and applications across Physical-Virtual-Cloud\n> >> Widest out-of-the-box monitoring support with 50+ applications\n> >> Performance metrics, stats and reports that give you Actionable Insights\n> >> Deep dive visibility with transaction tracing using APM Insight.\n> >> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> >>\n> >>\n> >>\n> >> _______________________________________________\n> >> Bitcoin-development mailing list\n> >> Bitcoin-development at lists.sourceforge.net\n> >> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >\n> >\n> ------------------------------------------------------------------------------\n> > One dashboard for servers and applications across Physical-Virtual-Cloud\n> > Widest out-of-the-box monitoring support with 50+ applications\n> > Performance metrics, stats and reports that give you Actionable Insights\n> > Deep dive visibility with transaction tracing using APM Insight.\n> > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/f943227e/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-19T09:13:17",
                "message_text_only": "On Tue, May 19, 2015 at 9:28 AM, Christian Decker <\ndecker.christian at gmail.com> wrote:\n\n> Thanks Stephen, I hadn't thought about BIP 34 and we need to address this\n> in both proposals. If we can avoid it I'd like not to have one\n> transaction hashed one way and other transactions in another way.\n>\n\nThe normalized TXID cannot depend on height for other transactions.\nOtherwise, it gets mutated when been added to the chain, depending on\nheight.\n\nAn option would be that the height is included in the scriptSig for all\ntransactions, but for non-coinbase transctions, the height used is zero.\n\nI think if height has to be an input into the normalized txid function, the\nspecifics of inclusion don't matter.\n\nThe previous txid for coinbases are required to be all zeros, so the\nnormalized txid could be to add the height to the txids of all inputs.\nAgain, non-coinbase transactions would have heights of zero.\n\n\n> Is there a specific reason why that was not chosen at the time?\n>\n\nI assumed that since the scriptSig in the coinbase is specifically intended\nto be \"random\" bytes/extra nonce, so putting a restriction on it was\nguaranteed to be backward compatible.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/acacf82f/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-19T10:43:39",
                "message_text_only": "On Tue, May 19, 2015 at 11:16 AM Tier Nolan <tier.nolan at gmail.com> wrote:\n\n> On Tue, May 19, 2015 at 9:28 AM, Christian Decker <\n> decker.christian at gmail.com> wrote:\n>\n>> Thanks Stephen, I hadn't thought about BIP 34 and we need to address this\n>> in both proposals. If we can avoid it I'd like not to have one\n>> transaction hashed one way and other transactions in another way.\n>>\n>\n> The normalized TXID cannot depend on height for other transactions.\n> Otherwise, it gets mutated when been added to the chain, depending on\n> height.\n>\nWell in the case of coinbase transactions we want them to be dependent on\nthe height they are included in, which is not a problem since they are only\nvalid in conjunction with the block that mined them.\n\n>\n> An option would be that the height is included in the scriptSig for all\n> transactions, but for non-coinbase transctions, the height used is zero.\n>\nNo need to add an extra field to the transaction just to include the\nheight. We can just add a rule that the height specified in the scriptSig\nin coinbase transactions (and only coinbase transactions) is copied into\nthe locktime of the transaction before computing the normalized transaction\nID and leave the locktime untouched for all normal transactions\n\n>\n> I think if height has to be an input into the normalized txid function,\n> the specifics of inclusion don't matter.\n>\n> The previous txid for coinbases are required to be all zeros, so the\n> normalized txid could be to add the height to the txids of all inputs.\n> Again, non-coinbase transactions would have heights of zero.\n>\n>\n>> Is there a specific reason why that was not chosen at the time?\n>>\n>\n> I assumed that since the scriptSig in the coinbase is specifically\n> intended to be \"random\" bytes/extra nonce, so putting a restriction on it\n> was guaranteed to be backward compatible.\n>\nSounds reasonable :-)\n\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/ffe7ffe5/attachment.html>"
            },
            {
                "author": "Stephen Morse",
                "date": "2015-05-19T12:48:20",
                "message_text_only": ">\n> An option would be that the height is included in the scriptSig for all\n>> transactions, but for non-coinbase transctions, the height used is zero.\n>>\n> No need to add an extra field to the transaction just to include the\n> height. We can just add a rule that the height specified in the scriptSig\n> in coinbase transactions (and only coinbase transactions) is copied into\n> the locktime of the transaction before computing the normalized transaction\n> ID and leave the locktime untouched for all normal transactions\n>\n\nNo need to replace lock times (or any other part of the transaction) at\nall. If you have to, just serialize the height right before serializing the\ntransaction (into the same buffer). And you could pre-serialize 0 instead\nof the height for all non-coinbase transactions. I don't really see what\nthat gets you, though, because the 0 is not really doing anything.\n\nBut, I don't see any reason you have to mess with the serialization this\nmuch at all. Just do:\n\nuint256 normalized_txid(CTransaction tx)\n{\n  // Coinbase transactions are already normalized\n  if (!tx.IsCoinbase())\n  {\n    foreach(CTxIn in : tx.vin)\n    {\n      if (!ReplacePrevoutHashWithNormalizedHash(in.prevout))\n        throw NormalizationError(\"Could not lookup prevout\");\n      in.scriptSig.clear();\n    }\n  }\n\n  // Serialize\n  CHashWriter ss(SER_GETHASH, 0);\n  ss << tx;\n  return ss.GetHash();\n}\n\nAn alternative could be (although I like the above option better):\n\nuint256 normalized_txid(CTransaction tx, int nHeight)\n{\n  foreach(CTxIn in : tx.vin)\n  {\n    if (!in.prevout.IsNull() &&\n!ReplacePrevoutHashWithNormalizedHash(in.prevout))\n      throw NormalizationError(\"Could not lookup prevout\");\n    in.scriptSig.clear();\n  }\n\n  // Serialize\n  CHashWriter ss(SER_GETHASH, 0);\n\nif (tx.IsCoinbase())\nss << nHeight;\n// or:\n// ss << (tx.IsCoinbase() ? nHeight : 0);\n\n  ss << tx;\n  return ss.GetHash();\n}\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/308eb60e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Normalized Transaction IDs",
            "categories": [
                "Bitcoin-development",
                "BIP"
            ],
            "authors": [
                "Stephen Morse",
                "s7r",
                "Stephen",
                "Tier Nolan",
                "Luke Dashjr",
                "Gavin Andresen",
                "Pieter Wuille",
                "Christian Decker"
            ],
            "messages_count": 27,
            "total_messages_chars_count": 97826
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin Core 0.10.1 release candidate 2 available",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2015-05-14T09:18:26",
                "message_text_only": "Binaries for bitcoin Core version 0.10.2rc1 are now available from:\n\n  https://bitcoin.org/bin/0.10.2/test\n\nSource code can be found on github under the signed tag\n\n  https://github.com/bitcoin/bitcoin/tree/v0.10.2rc1\n\nThis is a release candidate for a minor version release, with mainly a fix for\na bug that affected Windows users with non-ASCII characters in the data directory.\nThe release also contains translation updates. \n\nIf you experienced no issues with 0.10.1, there is no need to upgrade.\n\nPreliminary release notes for the 0.10.2 release can be found here:\n\n  https://github.com/bitcoin/bitcoin/blob/v0.10.2rc1/doc/release-notes.md\n\nRelease candidates are test runs for releases, when no critical\nproblems are found this release candidate will be tagged as 0.10.2.\n\nPlease report bugs using the issue tracker at github:\n\n  https://github.com/bitcoin/bitcoin/issues"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.10.1 release candidate 2 available",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 877
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin Core 0.10.2 release candidate 1 available",
        "thread_messages": [
            {
                "author": "Wladimir",
                "date": "2015-05-14T09:21:50",
                "message_text_only": "The subject should obviously be \"Bitcoin Core 0.10.2 release candidate\n1 available\", not the other way around,"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.10.2 release candidate 1 available",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 110
        }
    },
    {
        "title": "[Bitcoin-development] No Bitcoin For You",
        "thread_messages": [
            {
                "author": "Tom Harding",
                "date": "2015-05-14T15:22:41",
                "message_text_only": "A recent post, which I cannot find after much effort, made an excellent\npoint.\n\nIf capacity grows, fewer individuals would be able to run full nodes. \nThose individuals, like many already, would have to give up running a\nfull-node wallet :(\n\nThat sounds bad, until you consider that the alternative is running a\nfull node on the bitcoin 'settlement network', while massive numbers of\npeople *give up any hope of directly owning bitcoin at all*.\n\nIf today's global payments are 100Ktps, and move to the Lightning\nNetwork, they will have to be consolidated by a factor of 25000:1 to fit\ninto bitcoin's current 4tps capacity as a settlement network.  You\nexecuting a personal transaction on that network will be about as likely\nas you personally conducting a $100 SWIFT transfer to yourself today. \nFor current holders, just selling or spending will get very expensive!\n\nForcing block capacity to stay small, so that individuals can run full\nnodes, is precisely what will force bitcoin to become a backbone that is\ntoo expensive for individuals to use.  I can't avoid the conclusion that\nBitcoin has to scale, and we might as well be thinking about how.\n\nThere may be a an escape window.  As current trends continue toward a\nlandscape of billions of SPV wallets, it may still be possible for\nindividuals collectively to make up the majority of the network, if more\nparts of the network itself rely on SPV-level security.\n\nWith SPV-level security, it might be possible to implement a scalable\nDHT-type network of nodes that collectively store and index the\nexhaustive and fast-growing corpus of transaction history, up to and\nincluding currently unconfirmed transactions.  Each individual node\ncould host a slice of the transaction set with a configurable size,\nlet's say down to a few GB today.\n\nSuch a network would have the desirable property of being run by the\ncommunity.  Most transactions would be submitted to it, and like today's\nnetwork, it would disseminate blocks (which would be rapidly torn apart\nand digested).  Therefore miners and other full nodes would depend on\nit, which is rather critical as those nodes grow closer to data-center\nproportions."
            },
            {
                "author": "Ryan X. Charles",
                "date": "2015-05-17T02:31:10",
                "message_text_only": "I agree with this analysis. I'm not sure if we will increase 1 MB\nblock size or not, but with a block size that small, it is all but\nimpossible for most people on the planet to ever own even a single utxo.\n\nAt 7tps, how long would it take to give 1 utxo to all of the 7 billion\npeople currently alive? It would take 1 billion seconds, or about 32\nyears.[1]  So for all practical purposes, at 1 MB block size, far less\nthan 1% of people will ever be able to own even a single satoshi.\nUnless those people are willing to wait around 30 years for their\nlightning network to settle, they will either not use bitcoin, or they\nwill use a substitute (such as a parallel decentralized network, or a\ncentralized service) that lacks the full trust-minimized security\nguarantees of the main bitcoin blockchain.\n\nI can't speak for most people, but for me personally, the thing I care\nmost about as an individual (besides being able to send bitcoin to and\nfrom anyone on the planet) is being able to validate the blockchain.\nWith a pruning node, this means I need to download the blockchain one\ntime (not store it), and maintain the utxo set. The utxo set is,\nroughly speaking, 30 bytes per utxo, and therefore, at one utxo per\nperson, about 7*30 billion bytes, or 210 GB. That's very achievable on\nthe hardware of today. Of course, some individuals or companies will\nhave far more than one utxo. Estimating an average of ten utxos per\nperson, that will be 2.1 TB. Also very achievable on the hardware of\ntoday.\n\nI don't think every transaction in the world should be on the\nblockchain, but I think it should be able to handle (long-term) enough\ntransactions that everyone can have their transactions settled on a\ntimescale suitable for human life. 30 years is unsuitable, but 1 day\nwould be pretty good. It would be great if I could send trillions of\ntransactions per day on networks built on top of bitcoin, and have my\ntransactions settle on the actual blockchain once per day. This means\nwe would need to support about 1 utxo per person per day, or 7 billion\ntransactions per day. That translates to about 81 thousand\ntransactions per second [2], or approximately 10,000 times the current\nrate. That would be 10 GB per ten minutes, which is achievable on\ncurrent hardware (albeit not yet inexpensively).\n\nUsing SPV security rather than pruning security makes the cost even\nlower. A person relying on SPV would not have to download every 10 GB\nblock, but only their transactions (or a small superset of them),\nwhich is already being done - scaling to 7 billion people would not\nrequire that SPV nodes perform any more computation than they already\ndo. Nonetheless, I think pruning should be considered the default\nminimum, since that it what is required to get the full\ntrust-minimized security guarantees of the blockchain. And that\nrequires 10 GB blocks (or thereabouts).\n\nThe number of people on the planet will also grow, perhaps to 14\nbillion people in the next few decades. However, the estimates here\nwould still be roughly correct. 10 GB blocks, or approximately so,\nallows everyone in the world to have their transactions settled on the\nblockchain in a timely manner, whereas 1 MB blocks do not. And this is\nalready achievable on current hardware. The most significant cost is\nbandwidth, but that will probably become substantially less expensive\nin the coming years, making it possible for everyone to inexpensively\nand securely send and receive bitcoin to anyone else, without having\nto use a parallel network with reduced security or rely on trusted\nthird parties.\n\n[1] 10^9 / 60 / 60 / 24 / 365 ~= 32.\n\n[2] 7*10^9 / 24 / 60 / 60 ~= 81018\n\nOn 05/14/2015 08:22 AM, Tom Harding wrote:\n> A recent post, which I cannot find after much effort, made an \n> excellent point.\n> \n> If capacity grows, fewer individuals would be able to run full\n> nodes. Those individuals, like many already, would have to give up\n> running a full-node wallet :(\n> \n> That sounds bad, until you consider that the alternative is running\n> a full node on the bitcoin 'settlement network', while massive\n> numbers of people *give up any hope of directly owning bitcoin at\n> all*.\n> \n> If today's global payments are 100Ktps, and move to the Lightning \n> Network, they will have to be consolidated by a factor of 25000:1\n> to fit into bitcoin's current 4tps capacity as a settlement\n> network. You executing a personal transaction on that network will\n> be about as likely as you personally conducting a $100 SWIFT\n> transfer to yourself today. For current holders, just selling or\n> spending will get very expensive!\n> \n> Forcing block capacity to stay small, so that individuals can run \n> full nodes, is precisely what will force bitcoin to become a\n> backbone that is too expensive for individuals to use.  I can't\n> avoid the conclusion that Bitcoin has to scale, and we might as\n> well be thinking about how.\n> \n> There may be a an escape window.  As current trends continue toward\n> a landscape of billions of SPV wallets, it may still be possible\n> for individuals collectively to make up the majority of the\n> network, if more parts of the network itself rely on SPV-level\n> security.\n> \n> With SPV-level security, it might be possible to implement a\n> scalable DHT-type network of nodes that collectively store and\n> index the exhaustive and fast-growing corpus of transaction\n> history, up to and including currently unconfirmed transactions.\n> Each individual node could host a slice of the transaction set with\n> a configurable size, let's say down to a few GB today.\n> \n> Such a network would have the desirable property of being run by\n> the community.  Most transactions would be submitted to it, and\n> like today's network, it would disseminate blocks (which would be\n> rapidly torn apart and digested).  Therefore miners and other full\n> nodes would depend on it, which is rather critical as those nodes\n> grow closer to data-center proportions.\n> \n> \n> \n> ------------------------------------------------------------------------------\n>\n>\n>\n> \nOne dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications \n> Performance metrics, stats and reports that give you Actionable \n> Insights Deep dive visibility with transaction tracing using APM \n> Insight. http://ad.doubleclick.net/ddm/clk/290420510;117567292;y \n> _______________________________________________ Bitcoin-development\n>  mailing list Bitcoin-development at lists.sourceforge.net \n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> \n\n-- \nRyan X. Charles\nSoftware Engineer @BitGo\n\ntwitter.com/ryanxcharles\ngithub.com/ryanxcharles\nkeybase.io/ryanxcharles\nonename.com/ryanxcharles"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T18:36:04",
                "message_text_only": ">\n> If capacity grows, fewer individuals would be able to run full nodes.\n>\n\nHardly. Nobody is currently exhausting the CPU capacity of even a normal\ncomputer currently and even if we did a 20x increase in load overnight,\nthat still wouldn't even warm up most machines good enough to be always on.\n\nThe reasons full nodes are unpopular to run seem to be:\n\n1. Uncontrollable bandwidth usage from sending people the chain\n2. People don't run them all the time, then don't want to wait for them to\ncatch up\n\nThe first can be fixed with better code (you can already easily opt out of\nuploading the chain, it's just not as fine-grained as desirable), and the\nsecond is fundamental to what full nodes do and how people work. For\nmerchants, who are the most important demographic we want to be using full\nnodes, they can just keep it running all the time. No biggie.\n\n\n> Therefore miners and other full nodes would depend on\n> it, which is rather critical as those nodes grow closer to data-center\n> proportions.\n>\n\nThis meme about datacenter-sized nodes has to die. The Bitcoin wiki is down\nright now, but I showed years ago that you could keep up with VISA on a\nsingle well specced server with today's technology. Only people living in a\ndreamworld think that Bitcoin might actually have to match that level of\ntransaction demand with today's hardware. As noted previously, \"too many\nusers\" is simply not a problem Bitcoin has .... and may never have!\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/2b8563b3/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-26T02:26:42",
                "message_text_only": "On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n\nThis meme about datacenter-sized nodes has to die. The Bitcoin wiki is down\n> right now, but I showed years ago that you could keep up with VISA on a\n> single well specced server with today's technology. Only people living in a\n> dreamworld think that Bitcoin might actually have to match that level of\n> transaction demand with today's hardware. As noted previously, \"too many\n> users\" is simply not a problem Bitcoin has .... and may never have!\n>\n>\n... And will certainly NEVER have if we can't solve the capacity problem\nSOON.\n\nIn a former life, I was a capacity planner for Bank of America's mid-range\nserver group. We had one hard and fast rule. When you are typically\nexceeding 75% of capacity on a given metric, it's time to expand capacity.\nPeriod. You don't do silly things like adjusting the business model to\ndisincentivize use. Unless there's some flaw in the system and it's leaking\nresources, if usage has increased to the point where you are at or near the\nlimits of capacity, you expand capacity. It's as simple as that, and I've\nfound that same rule fits quite well in a number of systems.\n\nIn Bitcoin, we're not leaking resources. There's no flaw. The system is\nperforming as intended. Usage is increasing because it works so well, and\nthere is huge potential for future growth as we identify more uses and\nattract more users. There might be a few technical things we can do to\nreduce consumption, but the metric we're concerned with right now is how\nmany transactions we can fit in a block. We've broken through the 75%\nmarker and are regularly bumping up against the 100% limit.\n\nIt is time to stop debating this and take action to expand capacity. The\nonly questions that should remain are how much capacity do we add, and how\nsoon can we do it. Given that most existing computer systems and networks\ncan easily handle 20MB blocks every 10 minutes, and given that that will\nincrease capacity 20-fold, I can't think of a single reason why we can't go\nto 20MB as soon as humanly possible. And in a few years, when the average\nblock size is over 15MB, we bump it up again to as high as we can go then\nwithout pushing typical computers or networks beyond their capacity. We can\nworry about ways to slow down growth without affecting the usefulness of\nBitcoin as we get closer to the hard technical limits on our capacity.\n\nAnd you know what else? If miners need higher fees to accommodate the costs\nof bigger blocks, they can configure their nodes to only mine transactions\nwith higher fees.. Let the miners decide how to charge enough to pay for\ntheir costs. We don't need to cripple the network just for them.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/4c46b15f/attachment.html>"
            },
            {
                "author": "Thy Shizzle",
                "date": "2015-05-26T02:30:52",
                "message_text_only": "Nah don't make blocks 20mb, then you are slowing down block propagation and blowing out conf tikes as a result. Just decrease the time it takes to make a 1mb block, then you still see the same propagation times today and just increase the transaction throughput.\n________________________________\nFrom: Jim Phillips<mailto:jim at ergophobia.org>\nSent: \u200e26/\u200e05/\u200e2015 12:27 PM\nTo: Mike Hearn<mailto:mike at plan99.net>\nCc: Bitcoin Dev<mailto:bitcoin-development at lists.sourceforge.net>\nSubject: Re: [Bitcoin-development] No Bitcoin For You\n\nOn Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n\nThis meme about datacenter-sized nodes has to die. The Bitcoin wiki is down\n> right now, but I showed years ago that you could keep up with VISA on a\n> single well specced server with today's technology. Only people living in a\n> dreamworld think that Bitcoin might actually have to match that level of\n> transaction demand with today's hardware. As noted previously, \"too many\n> users\" is simply not a problem Bitcoin has .... and may never have!\n>\n>\n... And will certainly NEVER have if we can't solve the capacity problem\nSOON.\n\nIn a former life, I was a capacity planner for Bank of America's mid-range\nserver group. We had one hard and fast rule. When you are typically\nexceeding 75% of capacity on a given metric, it's time to expand capacity.\nPeriod. You don't do silly things like adjusting the business model to\ndisincentivize use. Unless there's some flaw in the system and it's leaking\nresources, if usage has increased to the point where you are at or near the\nlimits of capacity, you expand capacity. It's as simple as that, and I've\nfound that same rule fits quite well in a number of systems.\n\nIn Bitcoin, we're not leaking resources. There's no flaw. The system is\nperforming as intended. Usage is increasing because it works so well, and\nthere is huge potential for future growth as we identify more uses and\nattract more users. There might be a few technical things we can do to\nreduce consumption, but the metric we're concerned with right now is how\nmany transactions we can fit in a block. We've broken through the 75%\nmarker and are regularly bumping up against the 100% limit.\n\nIt is time to stop debating this and take action to expand capacity. The\nonly questions that should remain are how much capacity do we add, and how\nsoon can we do it. Given that most existing computer systems and networks\ncan easily handle 20MB blocks every 10 minutes, and given that that will\nincrease capacity 20-fold, I can't think of a single reason why we can't go\nto 20MB as soon as humanly possible. And in a few years, when the average\nblock size is over 15MB, we bump it up again to as high as we can go then\nwithout pushing typical computers or networks beyond their capacity. We can\nworry about ways to slow down growth without affecting the usefulness of\nBitcoin as we get closer to the hard technical limits on our capacity.\n\nAnd you know what else? If miners need higher fees to accommodate the costs\nof bigger blocks, they can configure their nodes to only mine transactions\nwith higher fees.. Let the miners decide how to charge enough to pay for\ntheir costs. We don't need to cripple the network just for them.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/744b3e51/attachment.html>\n-------------- next part --------------\n------------------------------------------------------------------------------\nOne dashboard for servers and applications across Physical-Virtual-Cloud \nWidest out-of-the-box monitoring support with 50+ applications\nPerformance metrics, stats and reports that give you Actionable Insights\nDeep dive visibility with transaction tracing using APM Insight.\nhttp://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n-------------- next part --------------\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "gabe appleton",
                "date": "2015-05-26T02:41:28",
                "message_text_only": "But don't you see the same trade-off in the end there? You're still\npropagating the same amount of data over the same amount of time, so unless\nI misunderstand, the costs of such a move should be approximately the same,\njust in different areas. The risks as I understand are as follows:\n\n20MB:\n\n\n   1. Longer per-block propagation (eventually)\n   2. Longer processing time (eventually)\n   3. Longer sync time\n\n1 Minute:\n\n   1. Weaker individual confirmations (approx. equal per confirmation*time)\n   2. Higher orphan rate (immediately)\n   3. Longer sync time\n\nThat risk-set makes me want a middle-ground approach. Something where the\nimmediate consequences aren't all that strong, and where we have some idea\nof what to do in the future. Is there any chance we can get decent network\nsimulations at various configurations (5MB/4min, etc)? Perhaps\nre-appropriate the testnet?\n\nOn Mon, May 25, 2015 at 10:30 PM, Thy Shizzle <thyshizzle at outlook.com>\nwrote:\n\n>  Nah don't make blocks 20mb, then you are slowing down block propagation\n> and blowing out conf tikes as a result. Just decrease the time it takes to\n> make a 1mb block, then you still see the same propagation times today and\n> just increase the transaction throughput.\n>  ------------------------------\n> From: Jim Phillips <jim at ergophobia.org>\n> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n> To: Mike Hearn <mike at plan99.net>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>\n>\n> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>\n>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki is\n> down right now, but I showed years ago that you could keep up with VISA on\n> a single well specced server with today's technology. Only people living in\n> a dreamworld think that Bitcoin might actually have to match that level of\n> transaction demand with today's hardware. As noted previously, \"too many\n> users\" is simply not a problem Bitcoin has .... and may never have!\n>\n>\n>  ... And will certainly NEVER have if we can't solve the capacity problem\n> SOON.\n>\n>  In a former life, I was a capacity planner for Bank of America's\n> mid-range server group. We had one hard and fast rule. When you are\n> typically exceeding 75% of capacity on a given metric, it's time to expand\n> capacity. Period. You don't do silly things like adjusting the business\n> model to disincentivize use. Unless there's some flaw in the system and\n> it's leaking resources, if usage has increased to the point where you are\n> at or near the limits of capacity, you expand capacity. It's as simple as\n> that, and I've found that same rule fits quite well in a number of systems.\n>\n>  In Bitcoin, we're not leaking resources. There's no flaw. The system is\n> performing as intended. Usage is increasing because it works so well, and\n> there is huge potential for future growth as we identify more uses and\n> attract more users. There might be a few technical things we can do to\n> reduce consumption, but the metric we're concerned with right now is how\n> many transactions we can fit in a block. We've broken through the 75%\n> marker and are regularly bumping up against the 100% limit.\n>\n>  It is time to stop debating this and take action to expand capacity. The\n> only questions that should remain are how much capacity do we add, and how\n> soon can we do it. Given that most existing computer systems and networks\n> can easily handle 20MB blocks every 10 minutes, and given that that will\n> increase capacity 20-fold, I can't think of a single reason why we can't go\n> to 20MB as soon as humanly possible. And in a few years, when the average\n> block size is over 15MB, we bump it up again to as high as we can go then\n> without pushing typical computers or networks beyond their capacity. We can\n> worry about ways to slow down growth without affecting the usefulness of\n> Bitcoin as we get closer to the hard technical limits on our capacity.\n>\n>  And you know what else? If miners need higher fees to accommodate the\n> costs of bigger blocks, they can configure their nodes to only mine\n> transactions with higher fees.. Let the miners decide how to charge enough\n> to pay for their costs. We don't need to cripple the network just for them.\n>\n>  --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy *\n>\n>   *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/7cd4d54f/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-26T02:53:10",
                "message_text_only": "Frankly I'm good with either way. I'm definitely in favor of faster\nconfirmation times.\n\nThe important thing is that we need to increase the amount of transactions\nthat get into blocks over a given time frame to a point that is in line\nwith what current technology can handle. We can handle WAY more than we are\ndoing right now. The Bitcoin network is not currently Disk, CPU, or RAM\nbound.. Not even close. The metric we're closest to being restricted by\nwould be Network bandwidth. I live in a developing country. 2Mbps is a\ntypical broadband speed here (although 5Mbps and 10Mbps connections are\naffordable). That equates to about 17MB per minute, or 170x more capacity\nthan what I need to receive a full copy of the blockchain if I only talk to\none peer. If I relay to say 10 peers, I can still handle 17x larger block\nsizes on a slow 2Mbps connection.\n\nAlso, even if we reduce the difficulty so that we're doing 1MB blocks every\nminute, that's still only 10MB every 10 minutes. Eventually we're going to\nhave to increase that, and we can only reduce the confirmation period so\nmuch. I think someone once said 30 seconds or so is about the shortest\nperiod you can practically achieve.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n\nOn Mon, May 25, 2015 at 9:30 PM, Thy Shizzle <thyshizzle at outlook.com> wrote:\n\n>  Nah don't make blocks 20mb, then you are slowing down block propagation\n> and blowing out conf tikes as a result. Just decrease the time it takes to\n> make a 1mb block, then you still see the same propagation times today and\n> just increase the transaction throughput.\n>  ------------------------------\n> From: Jim Phillips <jim at ergophobia.org>\n> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n> To: Mike Hearn <mike at plan99.net>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>\n>\n> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>\n>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki is\n> down right now, but I showed years ago that you could keep up with VISA on\n> a single well specced server with today's technology. Only people living in\n> a dreamworld think that Bitcoin might actually have to match that level of\n> transaction demand with today's hardware. As noted previously, \"too many\n> users\" is simply not a problem Bitcoin has .... and may never have!\n>\n>\n>  ... And will certainly NEVER have if we can't solve the capacity problem\n> SOON.\n>\n>  In a former life, I was a capacity planner for Bank of America's\n> mid-range server group. We had one hard and fast rule. When you are\n> typically exceeding 75% of capacity on a given metric, it's time to expand\n> capacity. Period. You don't do silly things like adjusting the business\n> model to disincentivize use. Unless there's some flaw in the system and\n> it's leaking resources, if usage has increased to the point where you are\n> at or near the limits of capacity, you expand capacity. It's as simple as\n> that, and I've found that same rule fits quite well in a number of systems.\n>\n>  In Bitcoin, we're not leaking resources. There's no flaw. The system is\n> performing as intended. Usage is increasing because it works so well, and\n> there is huge potential for future growth as we identify more uses and\n> attract more users. There might be a few technical things we can do to\n> reduce consumption, but the metric we're concerned with right now is how\n> many transactions we can fit in a block. We've broken through the 75%\n> marker and are regularly bumping up against the 100% limit.\n>\n>  It is time to stop debating this and take action to expand capacity. The\n> only questions that should remain are how much capacity do we add, and how\n> soon can we do it. Given that most existing computer systems and networks\n> can easily handle 20MB blocks every 10 minutes, and given that that will\n> increase capacity 20-fold, I can't think of a single reason why we can't go\n> to 20MB as soon as humanly possible. And in a few years, when the average\n> block size is over 15MB, we bump it up again to as high as we can go then\n> without pushing typical computers or networks beyond their capacity. We can\n> worry about ways to slow down growth without affecting the usefulness of\n> Bitcoin as we get closer to the hard technical limits on our capacity.\n>\n>  And you know what else? If miners need higher fees to accommodate the\n> costs of bigger blocks, they can configure their nodes to only mine\n> transactions with higher fees.. Let the miners decide how to charge enough\n> to pay for their costs. We don't need to cripple the network just for them.\n>\n>  --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy *\n>\n>   *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/3c8eea08/attachment.html>"
            },
            {
                "author": "Thy Shizzle",
                "date": "2015-05-26T02:51:00",
                "message_text_only": "I wouldn't say same trade-off because you need the whole 20mb block before you can start to use it where as a 1mb block can be used quicker thus transactions found in tge block quicker etc. As for tge higher rate of orphans, I think this would be complimented by a faster correction rate, so if you're pumping out blocks at a rate of 1 per minute, if we get a fork and the next block comes in 10 minutes and is the decider, it took 10 minutes to determine which block is the orphan. But at a rate of 1 block per 1 minute then it only takes 1 minute to resolve the orphan (obviously this is very simplified) so I'm not so sure that orphan rate is a big issue here. Indeed you would need to draw upon more confirmations for easier block creation but surely that is not an issue?\n\nWhy would sync time be longer as opposed to 20mb blocks?\n________________________________\nFrom: gabe appleton<mailto:gappleto97 at gmail.com>\nSent: \u200e26/\u200e05/\u200e2015 12:41 PM\nTo: Thy Shizzle<mailto:thyshizzle at outlook.com>\nCc: Jim Phillips<mailto:jim at ergophobia.org>; Mike Hearn<mailto:mike at plan99.net>; Bitcoin Dev<mailto:bitcoin-development at lists.sourceforge.net>\nSubject: Re: [Bitcoin-development] No Bitcoin For You\n\nBut don't you see the same trade-off in the end there? You're still\npropagating the same amount of data over the same amount of time, so unless\nI misunderstand, the costs of such a move should be approximately the same,\njust in different areas. The risks as I understand are as follows:\n\n20MB:\n\n\n   1. Longer per-block propagation (eventually)\n   2. Longer processing time (eventually)\n   3. Longer sync time\n\n1 Minute:\n\n   1. Weaker individual confirmations (approx. equal per confirmation*time)\n   2. Higher orphan rate (immediately)\n   3. Longer sync time\n\nThat risk-set makes me want a middle-ground approach. Something where the\nimmediate consequences aren't all that strong, and where we have some idea\nof what to do in the future. Is there any chance we can get decent network\nsimulations at various configurations (5MB/4min, etc)? Perhaps\nre-appropriate the testnet?\n\nOn Mon, May 25, 2015 at 10:30 PM, Thy Shizzle <thyshizzle at outlook.com>\nwrote:\n\n>  Nah don't make blocks 20mb, then you are slowing down block propagation\n> and blowing out conf tikes as a result. Just decrease the time it takes to\n> make a 1mb block, then you still see the same propagation times today and\n> just increase the transaction throughput.\n>  ------------------------------\n> From: Jim Phillips <jim at ergophobia.org>\n> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n> To: Mike Hearn <mike at plan99.net>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>\n>\n> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>\n>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki is\n> down right now, but I showed years ago that you could keep up with VISA on\n> a single well specced server with today's technology. Only people living in\n> a dreamworld think that Bitcoin might actually have to match that level of\n> transaction demand with today's hardware. As noted previously, \"too many\n> users\" is simply not a problem Bitcoin has .... and may never have!\n>\n>\n>  ... And will certainly NEVER have if we can't solve the capacity problem\n> SOON.\n>\n>  In a former life, I was a capacity planner for Bank of America's\n> mid-range server group. We had one hard and fast rule. When you are\n> typically exceeding 75% of capacity on a given metric, it's time to expand\n> capacity. Period. You don't do silly things like adjusting the business\n> model to disincentivize use. Unless there's some flaw in the system and\n> it's leaking resources, if usage has increased to the point where you are\n> at or near the limits of capacity, you expand capacity. It's as simple as\n> that, and I've found that same rule fits quite well in a number of systems.\n>\n>  In Bitcoin, we're not leaking resources. There's no flaw. The system is\n> performing as intended. Usage is increasing because it works so well, and\n> there is huge potential for future growth as we identify more uses and\n> attract more users. There might be a few technical things we can do to\n> reduce consumption, but the metric we're concerned with right now is how\n> many transactions we can fit in a block. We've broken through the 75%\n> marker and are regularly bumping up against the 100% limit.\n>\n>  It is time to stop debating this and take action to expand capacity. The\n> only questions that should remain are how much capacity do we add, and how\n> soon can we do it. Given that most existing computer systems and networks\n> can easily handle 20MB blocks every 10 minutes, and given that that will\n> increase capacity 20-fold, I can't think of a single reason why we can't go\n> to 20MB as soon as humanly possible. And in a few years, when the average\n> block size is over 15MB, we bump it up again to as high as we can go then\n> without pushing typical computers or networks beyond their capacity. We can\n> worry about ways to slow down growth without affecting the usefulness of\n> Bitcoin as we get closer to the hard technical limits on our capacity.\n>\n>  And you know what else? If miners need higher fees to accommodate the\n> costs of bigger blocks, they can configure their nodes to only mine\n> transactions with higher fees.. Let the miners decide how to charge enough\n> to pay for their costs. We don't need to cripple the network just for them.\n>\n>  --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy *\n>\n>   *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/f03594e9/attachment.html>"
            },
            {
                "author": "Thy Shizzle",
                "date": "2015-05-26T03:02:00",
                "message_text_only": "Indeed Jim, your internet connection makes a good reason why I don't like 20mb blocks (right now). It would take you well over a minute to download the block before you could even relay it on, so much slow down in propagation! Yes I do see how decreasing the time to create blocks is a bit of a band-aid fix, and to use tge term I've seen mentioned here \"kicking the can down the road\" I agree that this is doing this, however as you say bandwidth is our biggest enemy right now and so hopefully by the time we exceed the capacity gained by the decrease in block time, we can then look to bump up block size because hopefully 20mbps connections will be baseline by then etc.\n________________________________\nFrom: Jim Phillips<mailto:jim at ergophobia.org>\nSent: \u200e26/\u200e05/\u200e2015 12:53 PM\nTo: Thy Shizzle<mailto:thyshizzle at outlook.com>\nCc: Mike Hearn<mailto:mike at plan99.net>; Bitcoin Dev<mailto:bitcoin-development at lists.sourceforge.net>\nSubject: Re: [Bitcoin-development] No Bitcoin For You\n\nFrankly I'm good with either way. I'm definitely in favor of faster\nconfirmation times.\n\nThe important thing is that we need to increase the amount of transactions\nthat get into blocks over a given time frame to a point that is in line\nwith what current technology can handle. We can handle WAY more than we are\ndoing right now. The Bitcoin network is not currently Disk, CPU, or RAM\nbound.. Not even close. The metric we're closest to being restricted by\nwould be Network bandwidth. I live in a developing country. 2Mbps is a\ntypical broadband speed here (although 5Mbps and 10Mbps connections are\naffordable). That equates to about 17MB per minute, or 170x more capacity\nthan what I need to receive a full copy of the blockchain if I only talk to\none peer. If I relay to say 10 peers, I can still handle 17x larger block\nsizes on a slow 2Mbps connection.\n\nAlso, even if we reduce the difficulty so that we're doing 1MB blocks every\nminute, that's still only 10MB every 10 minutes. Eventually we're going to\nhave to increase that, and we can only reduce the confirmation period so\nmuch. I think someone once said 30 seconds or so is about the shortest\nperiod you can practically achieve.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n\nOn Mon, May 25, 2015 at 9:30 PM, Thy Shizzle <thyshizzle at outlook.com> wrote:\n\n>  Nah don't make blocks 20mb, then you are slowing down block propagation\n> and blowing out conf tikes as a result. Just decrease the time it takes to\n> make a 1mb block, then you still see the same propagation times today and\n> just increase the transaction throughput.\n>  ------------------------------\n> From: Jim Phillips <jim at ergophobia.org>\n> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n> To: Mike Hearn <mike at plan99.net>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>\n>\n> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>\n>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki is\n> down right now, but I showed years ago that you could keep up with VISA on\n> a single well specced server with today's technology. Only people living in\n> a dreamworld think that Bitcoin might actually have to match that level of\n> transaction demand with today's hardware. As noted previously, \"too many\n> users\" is simply not a problem Bitcoin has .... and may never have!\n>\n>\n>  ... And will certainly NEVER have if we can't solve the capacity problem\n> SOON.\n>\n>  In a former life, I was a capacity planner for Bank of America's\n> mid-range server group. We had one hard and fast rule. When you are\n> typically exceeding 75% of capacity on a given metric, it's time to expand\n> capacity. Period. You don't do silly things like adjusting the business\n> model to disincentivize use. Unless there's some flaw in the system and\n> it's leaking resources, if usage has increased to the point where you are\n> at or near the limits of capacity, you expand capacity. It's as simple as\n> that, and I've found that same rule fits quite well in a number of systems.\n>\n>  In Bitcoin, we're not leaking resources. There's no flaw. The system is\n> performing as intended. Usage is increasing because it works so well, and\n> there is huge potential for future growth as we identify more uses and\n> attract more users. There might be a few technical things we can do to\n> reduce consumption, but the metric we're concerned with right now is how\n> many transactions we can fit in a block. We've broken through the 75%\n> marker and are regularly bumping up against the 100% limit.\n>\n>  It is time to stop debating this and take action to expand capacity. The\n> only questions that should remain are how much capacity do we add, and how\n> soon can we do it. Given that most existing computer systems and networks\n> can easily handle 20MB blocks every 10 minutes, and given that that will\n> increase capacity 20-fold, I can't think of a single reason why we can't go\n> to 20MB as soon as humanly possible. And in a few years, when the average\n> block size is over 15MB, we bump it up again to as high as we can go then\n> without pushing typical computers or networks beyond their capacity. We can\n> worry about ways to slow down growth without affecting the usefulness of\n> Bitcoin as we get closer to the hard technical limits on our capacity.\n>\n>  And you know what else? If miners need higher fees to accommodate the\n> costs of bigger blocks, they can configure their nodes to only mine\n> transactions with higher fees.. Let the miners decide how to charge enough\n> to pay for their costs. We don't need to cripple the network just for them.\n>\n>  --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy *\n>\n>   *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/40d01923/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-26T03:23:35",
                "message_text_only": "I don't see how the fact that my 2Mbps connection causes me to not be a\nvery good relay has any bearing on whether or not the network as a whole\nwould be negatively impacted by a 20MB block. My inability to rapidly\npropagate blocks doesn't really harm the network. It's only if MOST relays\nare as slow as mine that it creates an issue. I'm one node in thousands\n(potentially tens or hundreds of thousands if/when Bitcoin goes\nmainstream). And I'm an individual. There's no reason at all for me to run\na full node from my home, except to have my own trusted and validated copy\nof the blockchain on a computer I control directly. I don't need to act as\na relay for that and as long as I can download blocks faster than they are\ncreated I'm fine. Also, I can easily afford a VPS server or several to run\nfull nodes as relays if I am feeling altruistic. It's actually cheaper for\nme to lease a VPS than to keep my own home PC on 24/7, which is why I have\n2 of them.\n\nAnd as a business, the cost of a server and bandwidth to run a full node is\na drop in the bucket. I'm involved in several projects where we have full\nnodes running on leased servers with multiple 1Gbps connections. It's an\nalmost zero cost. Those nodes could handle 20MB blocks today without\nthinking about it, and I'm sure our nodes are just a few amongst thousands\njust like them. I'm not at all concerned about the network being too\ncentralized.\n\nWhat concerns me is the fact that we are using edge cases like my home PC\nas a lame excuse to debate expanding the capacity of the network.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n\nOn Mon, May 25, 2015 at 10:02 PM, Thy Shizzle <thyshizzle at outlook.com>\nwrote:\n\n>  Indeed Jim, your internet connection makes a good reason why I don't\n> like 20mb blocks (right now). It would take you well over a minute to\n> download the block before you could even relay it on, so much slow down in\n> propagation! Yes I do see how decreasing the time to create blocks is a bit\n> of a band-aid fix, and to use tge term I've seen mentioned here \"kicking\n> the can down the road\" I agree that this is doing this, however as you say\n> bandwidth is our biggest enemy right now and so hopefully by the time we\n> exceed the capacity gained by the decrease in block time, we can then look\n> to bump up block size because hopefully 20mbps connections will be baseline\n> by then etc.\n>  ------------------------------\n> From: Jim Phillips <jim at ergophobia.org>\n> Sent: \u200e26/\u200e05/\u200e2015 12:53 PM\n> To: Thy Shizzle <thyshizzle at outlook.com>\n> Cc: Mike Hearn <mike at plan99.net>; Bitcoin Dev\n> <bitcoin-development at lists.sourceforge.net>\n>\n> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>\n>  Frankly I'm good with either way. I'm definitely in favor of faster\n> confirmation times.\n>\n>  The important thing is that we need to increase the amount of\n> transactions that get into blocks over a given time frame to a point that\n> is in line with what current technology can handle. We can handle WAY more\n> than we are doing right now. The Bitcoin network is not currently Disk,\n> CPU, or RAM bound.. Not even close. The metric we're closest to being\n> restricted by would be Network bandwidth. I live in a developing country.\n> 2Mbps is a typical broadband speed here (although 5Mbps and 10Mbps\n> connections are affordable). That equates to about 17MB per minute, or 170x\n> more capacity than what I need to receive a full copy of the blockchain if\n> I only talk to one peer. If I relay to say 10 peers, I can still handle 17x\n> larger block sizes on a slow 2Mbps connection.\n>\n>  Also, even if we reduce the difficulty so that we're doing 1MB blocks\n> every minute, that's still only 10MB every 10 minutes. Eventually we're\n> going to have to increase that, and we can only reduce the confirmation\n> period so much. I think someone once said 30 seconds or so is about the\n> shortest period you can practically achieve.\n>\n>  --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n> <http://www.linkedin.com/in/ergophobe>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy *\n>\n>   *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n> On Mon, May 25, 2015 at 9:30 PM, Thy Shizzle <thyshizzle at outlook.com>\n> wrote:\n>\n>  Nah don't make blocks 20mb, then you are slowing down block propagation\n> and blowing out conf tikes as a result. Just decrease the time it takes to\n> make a 1mb block, then you still see the same propagation times today and\n> just increase the transaction throughput.\n>  ------------------------------\n> From: Jim Phillips <jim at ergophobia.org>\n> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n> To: Mike Hearn <mike at plan99.net>\n> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>\n>\n> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>\n>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki is\n> down right now, but I showed years ago that you could keep up with VISA on\n> a single well specced server with today's technology. Only people living in\n> a dreamworld think that Bitcoin might actually have to match that level of\n> transaction demand with today's hardware. As noted previously, \"too many\n> users\" is simply not a problem Bitcoin has .... and may never have!\n>\n>\n>  ... And will certainly NEVER have if we can't solve the capacity problem\n> SOON.\n>\n>  In a former life, I was a capacity planner for Bank of America's\n> mid-range server group. We had one hard and fast rule. When you are\n> typically exceeding 75% of capacity on a given metric, it's time to expand\n> capacity. Period. You don't do silly things like adjusting the business\n> model to disincentivize use. Unless there's some flaw in the system and\n> it's leaking resources, if usage has increased to the point where you are\n> at or near the limits of capacity, you expand capacity. It's as simple as\n> that, and I've found that same rule fits quite well in a number of systems.\n>\n>  In Bitcoin, we're not leaking resources. There's no flaw. The system is\n> performing as intended. Usage is increasing because it works so well, and\n> there is huge potential for future growth as we identify more uses and\n> attract more users. There might be a few technical things we can do to\n> reduce consumption, but the metric we're concerned with right now is how\n> many transactions we can fit in a block. We've broken through the 75%\n> marker and are regularly bumping up against the 100% limit.\n>\n>  It is time to stop debating this and take action to expand capacity. The\n> only questions that should remain are how much capacity do we add, and how\n> soon can we do it. Given that most existing computer systems and networks\n> can easily handle 20MB blocks every 10 minutes, and given that that will\n> increase capacity 20-fold, I can't think of a single reason why we can't go\n> to 20MB as soon as humanly possible. And in a few years, when the average\n> block size is over 15MB, we bump it up again to as high as we can go then\n> without pushing typical computers or networks beyond their capacity. We can\n> worry about ways to slow down growth without affecting the usefulness of\n> Bitcoin as we get closer to the hard technical limits on our capacity.\n>\n>  And you know what else? If miners need higher fees to accommodate the\n> costs of bigger blocks, they can configure their nodes to only mine\n> transactions with higher fees.. Let the miners decide how to charge enough\n> to pay for their costs. We don't need to cripple the network just for them.\n>\n>  --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy *\n>\n>   *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/5c6d4699/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-26T03:49:04",
                "message_text_only": "Incidentally, even once we have the \"Internet of Things\" brought on by 21,\nInc. or whoever beats them to it, I would expect the average home to have\nonly a single full node \"hub\" receiving the blockchain and broadcasting\ntransactions created by all the minor SPV connected devices running within\nthe house. The in-home full node would be peered with high bandwidth\nfull-node relays running at the ISP or in the cloud. There are more than\nenough ISPs and cloud compute providers in the world such that there should\nbe no concern at all about centralization of relays. Full nodes could some\nday become as ubiquitous on the Internet as authoritative DNS servers. And\njust like DNS servers, if you don't trust the nodes your ISP creates or\nit's too slow or censors transactions, there's nothing preventing you from\npeering with nodes hosted by the Googles or OpenDNSs out there, or running\nyour own if you're really paranoid and have a few extra bucks for a VPS.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n\nOn Mon, May 25, 2015 at 10:23 PM, Jim Phillips <jim at ergophobia.org> wrote:\n\n> I don't see how the fact that my 2Mbps connection causes me to not be a\n> very good relay has any bearing on whether or not the network as a whole\n> would be negatively impacted by a 20MB block. My inability to rapidly\n> propagate blocks doesn't really harm the network. It's only if MOST relays\n> are as slow as mine that it creates an issue. I'm one node in thousands\n> (potentially tens or hundreds of thousands if/when Bitcoin goes\n> mainstream). And I'm an individual. There's no reason at all for me to run\n> a full node from my home, except to have my own trusted and validated copy\n> of the blockchain on a computer I control directly. I don't need to act as\n> a relay for that and as long as I can download blocks faster than they are\n> created I'm fine. Also, I can easily afford a VPS server or several to run\n> full nodes as relays if I am feeling altruistic. It's actually cheaper for\n> me to lease a VPS than to keep my own home PC on 24/7, which is why I have\n> 2 of them.\n>\n> And as a business, the cost of a server and bandwidth to run a full node\n> is a drop in the bucket. I'm involved in several projects where we have\n> full nodes running on leased servers with multiple 1Gbps connections. It's\n> an almost zero cost. Those nodes could handle 20MB blocks today without\n> thinking about it, and I'm sure our nodes are just a few amongst thousands\n> just like them. I'm not at all concerned about the network being too\n> centralized.\n>\n> What concerns me is the fact that we are using edge cases like my home PC\n> as a lame excuse to debate expanding the capacity of the network.\n>\n> --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n> <http://www.linkedin.com/in/ergophobe>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy*\n>\n>  *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n> On Mon, May 25, 2015 at 10:02 PM, Thy Shizzle <thyshizzle at outlook.com>\n> wrote:\n>\n>>  Indeed Jim, your internet connection makes a good reason why I don't\n>> like 20mb blocks (right now). It would take you well over a minute to\n>> download the block before you could even relay it on, so much slow down in\n>> propagation! Yes I do see how decreasing the time to create blocks is a bit\n>> of a band-aid fix, and to use tge term I've seen mentioned here \"kicking\n>> the can down the road\" I agree that this is doing this, however as you say\n>> bandwidth is our biggest enemy right now and so hopefully by the time we\n>> exceed the capacity gained by the decrease in block time, we can then look\n>> to bump up block size because hopefully 20mbps connections will be baseline\n>> by then etc.\n>>  ------------------------------\n>> From: Jim Phillips <jim at ergophobia.org>\n>> Sent: \u200e26/\u200e05/\u200e2015 12:53 PM\n>> To: Thy Shizzle <thyshizzle at outlook.com>\n>> Cc: Mike Hearn <mike at plan99.net>; Bitcoin Dev\n>> <bitcoin-development at lists.sourceforge.net>\n>>\n>> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>>\n>>  Frankly I'm good with either way. I'm definitely in favor of faster\n>> confirmation times.\n>>\n>>  The important thing is that we need to increase the amount of\n>> transactions that get into blocks over a given time frame to a point that\n>> is in line with what current technology can handle. We can handle WAY more\n>> than we are doing right now. The Bitcoin network is not currently Disk,\n>> CPU, or RAM bound.. Not even close. The metric we're closest to being\n>> restricted by would be Network bandwidth. I live in a developing country.\n>> 2Mbps is a typical broadband speed here (although 5Mbps and 10Mbps\n>> connections are affordable). That equates to about 17MB per minute, or 170x\n>> more capacity than what I need to receive a full copy of the blockchain if\n>> I only talk to one peer. If I relay to say 10 peers, I can still handle 17x\n>> larger block sizes on a slow 2Mbps connection.\n>>\n>>  Also, even if we reduce the difficulty so that we're doing 1MB blocks\n>> every minute, that's still only 10MB every 10 minutes. Eventually we're\n>> going to have to increase that, and we can only reduce the confirmation\n>> period so much. I think someone once said 30 seconds or so is about the\n>> shortest period you can practically achieve.\n>>\n>>  --\n>> *James G. Phillips IV*\n>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>> <http://www.linkedin.com/in/ergophobe>\n>>\n>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>> immortals.\" -- David Ogilvy *\n>>\n>>   *This message was created with 100% recycled electrons. Please think\n>> twice before printing.*\n>>\n>> On Mon, May 25, 2015 at 9:30 PM, Thy Shizzle <thyshizzle at outlook.com>\n>> wrote:\n>>\n>>  Nah don't make blocks 20mb, then you are slowing down block propagation\n>> and blowing out conf tikes as a result. Just decrease the time it takes to\n>> make a 1mb block, then you still see the same propagation times today and\n>> just increase the transaction throughput.\n>>  ------------------------------\n>> From: Jim Phillips <jim at ergophobia.org>\n>> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n>> To: Mike Hearn <mike at plan99.net>\n>> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>>\n>>\n>> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>>\n>>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki is\n>> down right now, but I showed years ago that you could keep up with VISA on\n>> a single well specced server with today's technology. Only people living in\n>> a dreamworld think that Bitcoin might actually have to match that level of\n>> transaction demand with today's hardware. As noted previously, \"too many\n>> users\" is simply not a problem Bitcoin has .... and may never have!\n>>\n>>\n>>  ... And will certainly NEVER have if we can't solve the capacity\n>> problem SOON.\n>>\n>>  In a former life, I was a capacity planner for Bank of America's\n>> mid-range server group. We had one hard and fast rule. When you are\n>> typically exceeding 75% of capacity on a given metric, it's time to expand\n>> capacity. Period. You don't do silly things like adjusting the business\n>> model to disincentivize use. Unless there's some flaw in the system and\n>> it's leaking resources, if usage has increased to the point where you are\n>> at or near the limits of capacity, you expand capacity. It's as simple as\n>> that, and I've found that same rule fits quite well in a number of systems.\n>>\n>>  In Bitcoin, we're not leaking resources. There's no flaw. The system is\n>> performing as intended. Usage is increasing because it works so well, and\n>> there is huge potential for future growth as we identify more uses and\n>> attract more users. There might be a few technical things we can do to\n>> reduce consumption, but the metric we're concerned with right now is how\n>> many transactions we can fit in a block. We've broken through the 75%\n>> marker and are regularly bumping up against the 100% limit.\n>>\n>>  It is time to stop debating this and take action to expand capacity.\n>> The only questions that should remain are how much capacity do we add, and\n>> how soon can we do it. Given that most existing computer systems and\n>> networks can easily handle 20MB blocks every 10 minutes, and given that\n>> that will increase capacity 20-fold, I can't think of a single reason why\n>> we can't go to 20MB as soon as humanly possible. And in a few years, when\n>> the average block size is over 15MB, we bump it up again to as high as we\n>> can go then without pushing typical computers or networks beyond their\n>> capacity. We can worry about ways to slow down growth without affecting the\n>> usefulness of Bitcoin as we get closer to the hard technical limits on our\n>> capacity.\n>>\n>>  And you know what else? If miners need higher fees to accommodate the\n>> costs of bigger blocks, they can configure their nodes to only mine\n>> transactions with higher fees.. Let the miners decide how to charge enough\n>> to pay for their costs. We don't need to cripple the network just for them.\n>>\n>>  --\n>> *James G. Phillips IV*\n>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>>\n>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>> immortals.\" -- David Ogilvy *\n>>\n>>   *This message was created with 100% recycled electrons. Please think\n>> twice before printing.*\n>>\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/96ec3c6b/attachment.html>"
            },
            {
                "author": "gabe appleton",
                "date": "2015-05-26T05:43:17",
                "message_text_only": "Sync time wouldn't be longer compared to 20MB, it would (eventually) be\nlonger under either setup.\n\nAlso, and this is probably a silly concern, but wouldn't changing block\ntime change the supply curve? If we cut the rate in half or a power of two,\nthat affects nothing, but if we want to keep it in round numbers, we need\nto do it by 10, 5, or 2. I feel like most people would bank for 10 or 5,\nboth of which change the supply curve due to truncation.\n\nAgain, it's a trivial concern, but probably one that should be addressed.\nOn May 25, 2015 11:52 PM, \"Jim Phillips\" <jim at ergophobia.org> wrote:\n\n> Incidentally, even once we have the \"Internet of Things\" brought on by 21,\n> Inc. or whoever beats them to it, I would expect the average home to have\n> only a single full node \"hub\" receiving the blockchain and broadcasting\n> transactions created by all the minor SPV connected devices running within\n> the house. The in-home full node would be peered with high bandwidth\n> full-node relays running at the ISP or in the cloud. There are more than\n> enough ISPs and cloud compute providers in the world such that there should\n> be no concern at all about centralization of relays. Full nodes could some\n> day become as ubiquitous on the Internet as authoritative DNS servers. And\n> just like DNS servers, if you don't trust the nodes your ISP creates or\n> it's too slow or censors transactions, there's nothing preventing you from\n> peering with nodes hosted by the Googles or OpenDNSs out there, or running\n> your own if you're really paranoid and have a few extra bucks for a VPS.\n>\n> --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n> <http://www.linkedin.com/in/ergophobe>\n>\n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy*\n>\n>  *This message was created with 100% recycled electrons. Please think\n> twice before printing.*\n>\n> On Mon, May 25, 2015 at 10:23 PM, Jim Phillips <jim at ergophobia.org> wrote:\n>\n>> I don't see how the fact that my 2Mbps connection causes me to not be a\n>> very good relay has any bearing on whether or not the network as a whole\n>> would be negatively impacted by a 20MB block. My inability to rapidly\n>> propagate blocks doesn't really harm the network. It's only if MOST relays\n>> are as slow as mine that it creates an issue. I'm one node in thousands\n>> (potentially tens or hundreds of thousands if/when Bitcoin goes\n>> mainstream). And I'm an individual. There's no reason at all for me to run\n>> a full node from my home, except to have my own trusted and validated copy\n>> of the blockchain on a computer I control directly. I don't need to act as\n>> a relay for that and as long as I can download blocks faster than they are\n>> created I'm fine. Also, I can easily afford a VPS server or several to run\n>> full nodes as relays if I am feeling altruistic. It's actually cheaper for\n>> me to lease a VPS than to keep my own home PC on 24/7, which is why I have\n>> 2 of them.\n>>\n>> And as a business, the cost of a server and bandwidth to run a full node\n>> is a drop in the bucket. I'm involved in several projects where we have\n>> full nodes running on leased servers with multiple 1Gbps connections. It's\n>> an almost zero cost. Those nodes could handle 20MB blocks today without\n>> thinking about it, and I'm sure our nodes are just a few amongst thousands\n>> just like them. I'm not at all concerned about the network being too\n>> centralized.\n>>\n>> What concerns me is the fact that we are using edge cases like my home PC\n>> as a lame excuse to debate expanding the capacity of the network.\n>>\n>> --\n>> *James G. Phillips IV*\n>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>> <http://www.linkedin.com/in/ergophobe>\n>>\n>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>> immortals.\" -- David Ogilvy*\n>>\n>>  *This message was created with 100% recycled electrons. Please think\n>> twice before printing.*\n>>\n>> On Mon, May 25, 2015 at 10:02 PM, Thy Shizzle <thyshizzle at outlook.com>\n>> wrote:\n>>\n>>>  Indeed Jim, your internet connection makes a good reason why I don't\n>>> like 20mb blocks (right now). It would take you well over a minute to\n>>> download the block before you could even relay it on, so much slow down in\n>>> propagation! Yes I do see how decreasing the time to create blocks is a bit\n>>> of a band-aid fix, and to use tge term I've seen mentioned here \"kicking\n>>> the can down the road\" I agree that this is doing this, however as you say\n>>> bandwidth is our biggest enemy right now and so hopefully by the time we\n>>> exceed the capacity gained by the decrease in block time, we can then look\n>>> to bump up block size because hopefully 20mbps connections will be baseline\n>>> by then etc.\n>>>  ------------------------------\n>>> From: Jim Phillips <jim at ergophobia.org>\n>>> Sent: \u200e26/\u200e05/\u200e2015 12:53 PM\n>>> To: Thy Shizzle <thyshizzle at outlook.com>\n>>> Cc: Mike Hearn <mike at plan99.net>; Bitcoin Dev\n>>> <bitcoin-development at lists.sourceforge.net>\n>>>\n>>> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>>>\n>>>  Frankly I'm good with either way. I'm definitely in favor of faster\n>>> confirmation times.\n>>>\n>>>  The important thing is that we need to increase the amount of\n>>> transactions that get into blocks over a given time frame to a point that\n>>> is in line with what current technology can handle. We can handle WAY more\n>>> than we are doing right now. The Bitcoin network is not currently Disk,\n>>> CPU, or RAM bound.. Not even close. The metric we're closest to being\n>>> restricted by would be Network bandwidth. I live in a developing country.\n>>> 2Mbps is a typical broadband speed here (although 5Mbps and 10Mbps\n>>> connections are affordable). That equates to about 17MB per minute, or 170x\n>>> more capacity than what I need to receive a full copy of the blockchain if\n>>> I only talk to one peer. If I relay to say 10 peers, I can still handle 17x\n>>> larger block sizes on a slow 2Mbps connection.\n>>>\n>>>  Also, even if we reduce the difficulty so that we're doing 1MB blocks\n>>> every minute, that's still only 10MB every 10 minutes. Eventually we're\n>>> going to have to increase that, and we can only reduce the confirmation\n>>> period so much. I think someone once said 30 seconds or so is about the\n>>> shortest period you can practically achieve.\n>>>\n>>>  --\n>>> *James G. Phillips IV*\n>>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>>> <http://www.linkedin.com/in/ergophobe>\n>>>\n>>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>>> immortals.\" -- David Ogilvy *\n>>>\n>>>   *This message was created with 100% recycled electrons. Please think\n>>> twice before printing.*\n>>>\n>>> On Mon, May 25, 2015 at 9:30 PM, Thy Shizzle <thyshizzle at outlook.com>\n>>> wrote:\n>>>\n>>>  Nah don't make blocks 20mb, then you are slowing down block\n>>> propagation and blowing out conf tikes as a result. Just decrease the time\n>>> it takes to make a 1mb block, then you still see the same propagation times\n>>> today and just increase the transaction throughput.\n>>>  ------------------------------\n>>> From: Jim Phillips <jim at ergophobia.org>\n>>> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n>>> To: Mike Hearn <mike at plan99.net>\n>>> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>>> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>>>\n>>>\n>>> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>>>\n>>>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki\n>>> is down right now, but I showed years ago that you could keep up with VISA\n>>> on a single well specced server with today's technology. Only people living\n>>> in a dreamworld think that Bitcoin might actually have to match that level\n>>> of transaction demand with today's hardware. As noted previously, \"too many\n>>> users\" is simply not a problem Bitcoin has .... and may never have!\n>>>\n>>>\n>>>  ... And will certainly NEVER have if we can't solve the capacity\n>>> problem SOON.\n>>>\n>>>  In a former life, I was a capacity planner for Bank of America's\n>>> mid-range server group. We had one hard and fast rule. When you are\n>>> typically exceeding 75% of capacity on a given metric, it's time to expand\n>>> capacity. Period. You don't do silly things like adjusting the business\n>>> model to disincentivize use. Unless there's some flaw in the system and\n>>> it's leaking resources, if usage has increased to the point where you are\n>>> at or near the limits of capacity, you expand capacity. It's as simple as\n>>> that, and I've found that same rule fits quite well in a number of systems.\n>>>\n>>>  In Bitcoin, we're not leaking resources. There's no flaw. The system\n>>> is performing as intended. Usage is increasing because it works so well,\n>>> and there is huge potential for future growth as we identify more uses and\n>>> attract more users. There might be a few technical things we can do to\n>>> reduce consumption, but the metric we're concerned with right now is how\n>>> many transactions we can fit in a block. We've broken through the 75%\n>>> marker and are regularly bumping up against the 100% limit.\n>>>\n>>>  It is time to stop debating this and take action to expand capacity.\n>>> The only questions that should remain are how much capacity do we add, and\n>>> how soon can we do it. Given that most existing computer systems and\n>>> networks can easily handle 20MB blocks every 10 minutes, and given that\n>>> that will increase capacity 20-fold, I can't think of a single reason why\n>>> we can't go to 20MB as soon as humanly possible. And in a few years, when\n>>> the average block size is over 15MB, we bump it up again to as high as we\n>>> can go then without pushing typical computers or networks beyond their\n>>> capacity. We can worry about ways to slow down growth without affecting the\n>>> usefulness of Bitcoin as we get closer to the hard technical limits on our\n>>> capacity.\n>>>\n>>>  And you know what else? If miners need higher fees to accommodate the\n>>> costs of bigger blocks, they can configure their nodes to only mine\n>>> transactions with higher fees.. Let the miners decide how to charge enough\n>>> to pay for their costs. We don't need to cripple the network just for them.\n>>>\n>>>  --\n>>> *James G. Phillips IV*\n>>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>>>\n>>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>>> immortals.\" -- David Ogilvy *\n>>>\n>>>   *This message was created with 100% recycled electrons. Please think\n>>> twice before printing.*\n>>>\n>>>\n>>>\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/4cbf3ecd/attachment.html>"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-26T08:29:31",
                "message_text_only": "I think all the suggestions recommending cutting the block time down also\nsuggest reducing the rewards to compensate.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n\nOn Tue, May 26, 2015 at 12:43 AM, gabe appleton <gappleto97 at gmail.com>\nwrote:\n\n> Sync time wouldn't be longer compared to 20MB, it would (eventually) be\n> longer under either setup.\n>\n> Also, and this is probably a silly concern, but wouldn't changing block\n> time change the supply curve? If we cut the rate in half or a power of two,\n> that affects nothing, but if we want to keep it in round numbers, we need\n> to do it by 10, 5, or 2. I feel like most people would bank for 10 or 5,\n> both of which change the supply curve due to truncation.\n>\n> Again, it's a trivial concern, but probably one that should be addressed.\n> On May 25, 2015 11:52 PM, \"Jim Phillips\" <jim at ergophobia.org> wrote:\n>\n>> Incidentally, even once we have the \"Internet of Things\" brought on by\n>> 21, Inc. or whoever beats them to it, I would expect the average home to\n>> have only a single full node \"hub\" receiving the blockchain and\n>> broadcasting transactions created by all the minor SPV connected devices\n>> running within the house. The in-home full node would be peered with high\n>> bandwidth full-node relays running at the ISP or in the cloud. There are\n>> more than enough ISPs and cloud compute providers in the world such that\n>> there should be no concern at all about centralization of relays. Full\n>> nodes could some day become as ubiquitous on the Internet as authoritative\n>> DNS servers. And just like DNS servers, if you don't trust the nodes your\n>> ISP creates or it's too slow or censors transactions, there's nothing\n>> preventing you from peering with nodes hosted by the Googles or OpenDNSs\n>> out there, or running your own if you're really paranoid and have a few\n>> extra bucks for a VPS.\n>>\n>> --\n>> *James G. Phillips IV*\n>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>> <http://www.linkedin.com/in/ergophobe>\n>>\n>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>> immortals.\" -- David Ogilvy*\n>>\n>>  *This message was created with 100% recycled electrons. Please think\n>> twice before printing.*\n>>\n>> On Mon, May 25, 2015 at 10:23 PM, Jim Phillips <jim at ergophobia.org>\n>> wrote:\n>>\n>>> I don't see how the fact that my 2Mbps connection causes me to not be a\n>>> very good relay has any bearing on whether or not the network as a whole\n>>> would be negatively impacted by a 20MB block. My inability to rapidly\n>>> propagate blocks doesn't really harm the network. It's only if MOST relays\n>>> are as slow as mine that it creates an issue. I'm one node in thousands\n>>> (potentially tens or hundreds of thousands if/when Bitcoin goes\n>>> mainstream). And I'm an individual. There's no reason at all for me to run\n>>> a full node from my home, except to have my own trusted and validated copy\n>>> of the blockchain on a computer I control directly. I don't need to act as\n>>> a relay for that and as long as I can download blocks faster than they are\n>>> created I'm fine. Also, I can easily afford a VPS server or several to run\n>>> full nodes as relays if I am feeling altruistic. It's actually cheaper for\n>>> me to lease a VPS than to keep my own home PC on 24/7, which is why I have\n>>> 2 of them.\n>>>\n>>> And as a business, the cost of a server and bandwidth to run a full node\n>>> is a drop in the bucket. I'm involved in several projects where we have\n>>> full nodes running on leased servers with multiple 1Gbps connections. It's\n>>> an almost zero cost. Those nodes could handle 20MB blocks today without\n>>> thinking about it, and I'm sure our nodes are just a few amongst thousands\n>>> just like them. I'm not at all concerned about the network being too\n>>> centralized.\n>>>\n>>> What concerns me is the fact that we are using edge cases like my home\n>>> PC as a lame excuse to debate expanding the capacity of the network.\n>>>\n>>> --\n>>> *James G. Phillips IV*\n>>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>>> <http://www.linkedin.com/in/ergophobe>\n>>>\n>>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>>> immortals.\" -- David Ogilvy*\n>>>\n>>>  *This message was created with 100% recycled electrons. Please think\n>>> twice before printing.*\n>>>\n>>> On Mon, May 25, 2015 at 10:02 PM, Thy Shizzle <thyshizzle at outlook.com>\n>>> wrote:\n>>>\n>>>>  Indeed Jim, your internet connection makes a good reason why I don't\n>>>> like 20mb blocks (right now). It would take you well over a minute to\n>>>> download the block before you could even relay it on, so much slow down in\n>>>> propagation! Yes I do see how decreasing the time to create blocks is a bit\n>>>> of a band-aid fix, and to use tge term I've seen mentioned here \"kicking\n>>>> the can down the road\" I agree that this is doing this, however as you say\n>>>> bandwidth is our biggest enemy right now and so hopefully by the time we\n>>>> exceed the capacity gained by the decrease in block time, we can then look\n>>>> to bump up block size because hopefully 20mbps connections will be baseline\n>>>> by then etc.\n>>>>  ------------------------------\n>>>> From: Jim Phillips <jim at ergophobia.org>\n>>>> Sent: \u200e26/\u200e05/\u200e2015 12:53 PM\n>>>> To: Thy Shizzle <thyshizzle at outlook.com>\n>>>> Cc: Mike Hearn <mike at plan99.net>; Bitcoin Dev\n>>>> <bitcoin-development at lists.sourceforge.net>\n>>>>\n>>>> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>>>>\n>>>>  Frankly I'm good with either way. I'm definitely in favor of faster\n>>>> confirmation times.\n>>>>\n>>>>  The important thing is that we need to increase the amount of\n>>>> transactions that get into blocks over a given time frame to a point that\n>>>> is in line with what current technology can handle. We can handle WAY more\n>>>> than we are doing right now. The Bitcoin network is not currently Disk,\n>>>> CPU, or RAM bound.. Not even close. The metric we're closest to being\n>>>> restricted by would be Network bandwidth. I live in a developing country.\n>>>> 2Mbps is a typical broadband speed here (although 5Mbps and 10Mbps\n>>>> connections are affordable). That equates to about 17MB per minute, or 170x\n>>>> more capacity than what I need to receive a full copy of the blockchain if\n>>>> I only talk to one peer. If I relay to say 10 peers, I can still handle 17x\n>>>> larger block sizes on a slow 2Mbps connection.\n>>>>\n>>>>  Also, even if we reduce the difficulty so that we're doing 1MB blocks\n>>>> every minute, that's still only 10MB every 10 minutes. Eventually we're\n>>>> going to have to increase that, and we can only reduce the confirmation\n>>>> period so much. I think someone once said 30 seconds or so is about the\n>>>> shortest period you can practically achieve.\n>>>>\n>>>>  --\n>>>> *James G. Phillips IV*\n>>>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>>>> <http://www.linkedin.com/in/ergophobe>\n>>>>\n>>>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>>>> immortals.\" -- David Ogilvy *\n>>>>\n>>>>   *This message was created with 100% recycled electrons. Please think\n>>>> twice before printing.*\n>>>>\n>>>> On Mon, May 25, 2015 at 9:30 PM, Thy Shizzle <thyshizzle at outlook.com>\n>>>> wrote:\n>>>>\n>>>>  Nah don't make blocks 20mb, then you are slowing down block\n>>>> propagation and blowing out conf tikes as a result. Just decrease the time\n>>>> it takes to make a 1mb block, then you still see the same propagation times\n>>>> today and just increase the transaction throughput.\n>>>>  ------------------------------\n>>>> From: Jim Phillips <jim at ergophobia.org>\n>>>> Sent: \u200e26/\u200e05/\u200e2015 12:27 PM\n>>>> To: Mike Hearn <mike at plan99.net>\n>>>> Cc: Bitcoin Dev <bitcoin-development at lists.sourceforge.net>\n>>>> Subject: Re: [Bitcoin-development] No Bitcoin For You\n>>>>\n>>>>\n>>>> On Mon, May 25, 2015 at 1:36 PM, Mike Hearn <mike at plan99.net> wrote:\n>>>>\n>>>>   This meme about datacenter-sized nodes has to die. The Bitcoin wiki\n>>>> is down right now, but I showed years ago that you could keep up with VISA\n>>>> on a single well specced server with today's technology. Only people living\n>>>> in a dreamworld think that Bitcoin might actually have to match that level\n>>>> of transaction demand with today's hardware. As noted previously, \"too many\n>>>> users\" is simply not a problem Bitcoin has .... and may never have!\n>>>>\n>>>>\n>>>>  ... And will certainly NEVER have if we can't solve the capacity\n>>>> problem SOON.\n>>>>\n>>>>  In a former life, I was a capacity planner for Bank of America's\n>>>> mid-range server group. We had one hard and fast rule. When you are\n>>>> typically exceeding 75% of capacity on a given metric, it's time to expand\n>>>> capacity. Period. You don't do silly things like adjusting the business\n>>>> model to disincentivize use. Unless there's some flaw in the system and\n>>>> it's leaking resources, if usage has increased to the point where you are\n>>>> at or near the limits of capacity, you expand capacity. It's as simple as\n>>>> that, and I've found that same rule fits quite well in a number of systems.\n>>>>\n>>>>  In Bitcoin, we're not leaking resources. There's no flaw. The system\n>>>> is performing as intended. Usage is increasing because it works so well,\n>>>> and there is huge potential for future growth as we identify more uses and\n>>>> attract more users. There might be a few technical things we can do to\n>>>> reduce consumption, but the metric we're concerned with right now is how\n>>>> many transactions we can fit in a block. We've broken through the 75%\n>>>> marker and are regularly bumping up against the 100% limit.\n>>>>\n>>>>  It is time to stop debating this and take action to expand capacity.\n>>>> The only questions that should remain are how much capacity do we add, and\n>>>> how soon can we do it. Given that most existing computer systems and\n>>>> networks can easily handle 20MB blocks every 10 minutes, and given that\n>>>> that will increase capacity 20-fold, I can't think of a single reason why\n>>>> we can't go to 20MB as soon as humanly possible. And in a few years, when\n>>>> the average block size is over 15MB, we bump it up again to as high as we\n>>>> can go then without pushing typical computers or networks beyond their\n>>>> capacity. We can worry about ways to slow down growth without affecting the\n>>>> usefulness of Bitcoin as we get closer to the hard technical limits on our\n>>>> capacity.\n>>>>\n>>>>  And you know what else? If miners need higher fees to accommodate the\n>>>> costs of bigger blocks, they can configure their nodes to only mine\n>>>> transactions with higher fees.. Let the miners decide how to charge enough\n>>>> to pay for their costs. We don't need to cripple the network just for them.\n>>>>\n>>>>  --\n>>>> *James G. Phillips IV*\n>>>> <https://plus.google.com/u/0/113107039501292625391/posts>\n>>>>\n>>>> *\"Don't bunt. Aim out of the ball park. Aim for the company of\n>>>> immortals.\" -- David Ogilvy *\n>>>>\n>>>>   *This message was created with 100% recycled electrons. Please think\n>>>> twice before printing.*\n>>>>\n>>>>\n>>>>\n>>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/909baed8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "No Bitcoin For You",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Mike Hearn",
                "gabe appleton",
                "Ryan X. Charles",
                "Jim Phillips",
                "Thy Shizzle",
                "Tom Harding"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 83775
        }
    },
    {
        "title": "[Bitcoin-development] Open Bitcoin Privacy Project Spring 2015 Report",
        "thread_messages": [
            {
                "author": "Justus Ranvier",
                "date": "2015-05-18T16:16:48",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nWe're produced the first in what we hope to be a long series of\nreviews of Bitcoin wallet privacy features, available here:\n\nhttp://www.openbitcoinprivacyproject.org/2015/05/spring-2015-wallet-privacy-rating-report/\n\nhttps://github.com/OpenBitcoinPrivacyProject/wallet-ratings/raw/master/2015-1/OBPP%20Bitcoin%20Wallet%20Privacy%20Rating%20Report%20-%20Spring%202015.pdf\n\nSpecifically from the readers of this list, we are very interested in\nfeedback regarding our privacy threat model and the rating criteria we\nderive from it.\n\nThreat model:\nhttps://github.com/OpenBitcoinPrivacyProject/wallet-ratings/blob/master/2015-1/threat%20model.wiki\n\nPlease send any suggestions or corrections via a GitHub issue to the\nwallet-ratings repository so that we can incorporate it into future\nreports.\n\n- -- \nJustus Ranvier\nOpen Bitcoin Privacy Project\nhttp://www.openbitcoinprivacyproject.org/\njustus at openbitcoinprivacyproject.org\nE7AD 8215 8497 3673 6D9E 61C4 2A5F DA70 EAD9 E623\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVWhBvAAoJECpf2nDq2eYj3JEP/jw/Xkgq4yZRE4FK8a8kPBqn\n3ULyS74KtNyPDdpTK0bdH7//0d1ep+LvNpIkFhWCJ/WQ7T/Ft3iQl1HJvXGC0ZzM\nXKd7ptXLpBrKElARAORgUlFPOeKzOrOyP4uvvGAMZ+CXEnKeyxDzK+WzfYnWyDEU\nQ4XQ/ndwPqbZm9Nb+GeF186TXcA/KqcQEuOIw7/jFGHfFT6QBKVz1SraVrgdcMky\n8KKYOsYJt8lTUjVN1INmLFoRZ0cGUd+IFKweSCibyAu9TdvVQfQSPfSnfZtDLk6X\ng1oPYaxX6r9+/1zm2v5ISk97nKrvNslPjeQbuW3vWlROSxGZ9lV+MVNvkqm1jSpF\nip0Il+VJb8hhh9LRJV6euhLQyR+xnLIVJvslJt883rhKIBi3M/OipMuhipIeAbnV\n0WQmnpQ9ZgagPoFRxGp86Cz7PWTfj7zllB3yk/M3tA8e64VQFhEnoyJO8hPqfEQh\nwPZP3UQ+K3PM/2oe4W5ZkfkqD6tIzGTQeMkGFeCvTsMmsW9+Ml8j4YTwKA0z5vr5\nIebJ51Vpaq1noVEl46q8utpLp1wATI8SG5sBwSaR4/REUGkSWjSCeZeLcK8WzdEa\nSaZ8t5Rqr1AcwtD6n9rVvYoF26285120jM/YX8XgMldWi0RXrlWD4B4lA1i7eVfZ\nhINSIR+QsJsw7yq7Ox3/\n=NMNo\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150518/fd9f9a2c/attachment.bin>"
            },
            {
                "author": "Justus Ranvier",
                "date": "2015-05-18T21:57:09",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\nReplying to the list because this is a common question.\n\nWe rated as many wallets as we could based on the amount of manpower\nwe had available to perform the ratings.\n\nWe will be holding a recruiting drive shortly to solicit additional\nvolunteers so that we can cover more wallet for the next round of ratings.\n\n\nOn 05/18/2015 11:40 PM, Eric Lombrozo wrote:\n> mSIGNA is notably absent from the report despite having some of the\n> most advanced security and privacy features and having been on the\n> bitcoin.org site longer than some of the other wallets reviewed.\n> \n> Is there some process to get reviewed I missed? Please add us to\n> the report.\n\n- -- \nJustus Ranvier\nOpen Bitcoin Privacy Project\nhttp://www.openbitcoinprivacyproject.org/\njustus at openbitcoinprivacyproject.org\nE7AD 8215 8497 3673 6D9E 61C4 2A5F DA70 EAD9 E623\n-----BEGIN PGP SIGNATURE-----\n\niQIcBAEBAgAGBQJVWmA1AAoJECpf2nDq2eYjXGgP/2CpIYpG49WKAfG/PXmu1zFV\n+zzU/7PCz+0ez0mGCz5l3QMc9TCDs6KDx0sPHZngwwLio3C4JMpu+zfnw6gngw/G\n6NNtmDZ9xWgWx3kQyBLWBCM/K63rLE1QYJqiIc7QaDuDTk5w0upwkFxLejWeem1j\nZ8Zy6ycHNNHE19o5pIYViWaRXojMD70fBFSoU9sAyvnOup7b5Cj4PLx7mbMbaegV\ngCHd7zd88AH0f2ilFiVMQJBKf03KzYjarRVrAyvAb/8VLiYMRC8SS3y2tzM+qRKl\nZADAp8FiQ6GOFRssNg+x7tc3DX2ngBljLLKM90kPLR2cvwp8VHi0I0biqM8n2M17\nCMjZVRNvXitNnKFc42nk51HsZR5LkDK3Rf9I1E3fBVKJ5feEbDi3tlEg9PCWD2xM\nPmSRzXoUBAjpr9xL2kBzlE4aTYZCmmMxAMF1mcSYujdNzQ2IN0gD1urJtCwZ8zZ1\nLL2cPimS3GTWEpoQQ/Fu+0NDwiPditDGt+DN16Mi5uioPvaumHMIV6u9ZNdD9ZEv\nUTGW8C2Hj+ENzqhpdhlV5YfYUnKo6/ukw+xxTXRxbbLGxA6iRrVnPzsYTZoF0Vrt\nT4IsdzPIg5yeF5IQKEV1lLyx+gOIvmDF1RZE36NY8bGn2zusFzUGxiqWa6yw0hfw\nl4ytd8pfhIvTkuz1o9aQ\n=kWhU\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: 0xEAD9E623.asc\nType: application/pgp-keys\nSize: 18381 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150518/58a72a8b/attachment.bin>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2015-05-19T11:19:09",
                "message_text_only": "Freundliche Gr\u00fcsse\n---\nJonas Schnelli\n\n\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\ninclude7 AG\nJonas Schnelli\nMattengasse 27\nCH-8005 Z\u00fcrich\nSwitzerland\nOffice: +41 44 500 16 70\n\nMail: jonas.schnelli at include7.ch\nWeb: www.include7.ch\nV-Card: www.include7.ch/js.vcf\n\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\n\nACHTUNG\nBitte senden sie uns keine sensitiven Daten in unverschl\u00fcsselten E-Mails.\nVerwenden Sie hierzu folgenden Link:\nhttps://include7.ch/contact/secureform\n\n\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\u00b0\n\n> Am 18.05.2015 um 23:57 schrieb Justus Ranvier <justus at openbitcoinprivacyproject.org>:\n> \n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA1\n> \n> Replying to the list because this is a common question.\n> \n> We rated as many wallets as we could based on the amount of manpower\n> we had available to perform the ratings.\n> \n> We will be holding a recruiting drive shortly to solicit additional\n> volunteers so that we can cover more wallet for the next round of ratings.\n> \n\nHi Justus\n\nIs there a reason why Bitcoin-Core and Breadwallet (iOS) is missing?\n\nThanks\n\u2014\n</Jonas>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/9d785e63/attachment.html>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2015-05-19T11:50:03",
                "message_text_only": "> \n>> Am 18.05.2015 um 23:57 schrieb Justus Ranvier <justus at openbitcoinprivacyproject.org <mailto:justus at openbitcoinprivacyproject.org>>:\n>> \n>> -----BEGIN PGP SIGNED MESSAGE-----\n>> Hash: SHA1\n>> \n>> Replying to the list because this is a common question.\n>> \n>> We rated as many wallets as we could based on the amount of manpower\n>> we had available to perform the ratings.\n>> \n>> We will be holding a recruiting drive shortly to solicit additional\n>> volunteers so that we can cover more wallet for the next round of ratings.\n>> \n> \n> Hi Justus\n> \n> Is there a reason why Bitcoin-Core and Breadwallet (iOS) is missing?\n> \n> Thanks\n> \u2014\n> </Jonas>\n\nSorry for the horrible top post, i accidentally added a text signature on top.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/7a245dd9/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Open Bitcoin Privacy Project Spring 2015 Report",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Justus Ranvier",
                "Jonas Schnelli"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 6252
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin Core 0.9.5 release candidate 1 tagged",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2015-05-19T08:25:57",
                "message_text_only": "Hello,\n\nI've just tagged release candidate 1 for 0.9.5 (tag `v0.9.5rc1`).\n\nThe reason for this backport release is to make BIP66 available on the 0.9 branch. This has been requested by a few users, mostly miners.\n\nFull (preliminary) release notes can be found at: \nhttps://github.com/bitcoin/bitcoin/blob/0.9/doc/release-notes.md\n\nIn contrast to 0.10.x releases it is unclear whether there will be enough gitian signatures for a binary release, any help with building is welcome.\nSee https://github.com/bitcoin/bitcoin/blob/0.9/doc/release-process.md\n(many steps have changed since then, so the release process for 0.10 or master will not work)\n\nWladimir"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.9.5 release candidate 1 tagged",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 654
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin Core 0.10.2 released",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2015-05-19T13:44:09",
                "message_text_only": ""
            },
            {
                "author": "Wladimir",
                "date": "2015-05-19T14:29:16",
                "message_text_only": "Bitcoin Core version 0.10.2 is now available from:\n\n  <https://bitcoin.org/bin/bitcoin-core-0.10.2/>\n\nThe distribution is also available as torrent:\n\n   https://bitcoin.org/bin/bitcoin-core-0.10.2/bitcoin-0.10.2.torrent\n\n   magnet:?xt=urn:btih:746a616aa8de97856c207e7a899c7ee315e8c44d&dn=bitcoin-core-0.10.2&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Fopen.demonii.com%3A1337&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2\n\nThe source code can be found in git under the tag `v0.10.2`, or in\n`bitcoin-0.10.2.tar.gz` in the distribution.\n\nThis is a new minor version release, bringing minor bug fixes and translation\nupdates. It is only necessary to upgrade to this version when unable\nto start the\napplication on Windows with 0.10.1.\n\nPlease report bugs using the issue tracker at github:\n\n  <https://github.com/bitcoin/bitcoin/issues>\n\nUpgrading and downgrading\n=========================\n\nHow to Upgrade\n--------------\n\nIf you are running an older version, shut it down. Wait until it has completely\nshut down (which might take a few minutes for older versions), then run the\ninstaller (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or\nbitcoind/bitcoin-qt (on Linux).\n\nDowngrade warning\n------------------\n\nBecause release 0.10.0 and later makes use of headers-first synchronization and\nparallel block download (see further), the block files and databases are not\nbackwards-compatible with pre-0.10 versions of Bitcoin Core or other software:\n\n* Blocks will be stored on disk out of order (in the order they are\nreceived, really), which makes it incompatible with some tools or\nother programs. Reindexing using earlier versions will also not work\nanymore as a result of this.\n\n* The block index database will now hold headers for which no block is\nstored on disk, which earlier versions won't support.\n\nIf you want to be able to downgrade smoothly, make a backup of your entire data\ndirectory. Without this your node will need start syncing (or importing from\nbootstrap.dat) anew afterwards. It is possible that the data from a completely\nsynchronised 0.10 node may be usable in older versions as-is, but this is not\nsupported and may break as soon as the older version attempts to reindex.\n\nThis does not affect wallet forward or backward compatibility.\n\nNotable changes\n===============\n\nThis fixes a serious problem on Windows with data directories that\nhave non-ASCII\ncharacters (https://github.com/bitcoin/bitcoin/issues/6078).\n\nFor other platforms there are no notable changes.\n\nFor the notable changes in 0.10, refer to the release notes\nat https://github.com/bitcoin/bitcoin/blob/v0.10.0/doc/release-notes.md\n\n0.10.2 Change log\n=================\n\nDetailed release notes follow. This overview includes changes that\naffect external\nbehavior, not code moves, refactors or string updates.\n\nWallet:\n- `824c011` fix boost::get usage with boost 1.58\n\nMiscellaneous:\n- `da65606` Avoid crash on start in TestBlockValidity with gen=1.\n- `424ae66` don't imbue boost::filesystem::path with locale \"C\" on\nwindows (fixes #6078)\n\nCredits\n=======\n\nThanks to everyone who directly contributed to this release:\n\n- Cory Fields\n- Gregory Maxwell\n- Jonas Schnelli\n- Wladimir J. van der Laan\n\nAnd all those who contributed additional code review and/or security research:\n\n- dexX7\n- Pieter Wuille\n- vayvanne\n\nAs well as everyone that helped translating on\n[Transifex](https://www.transifex.com/projects/p/bitcoin/).\n\nOn Tue, May 19, 2015 at 1:44 PM, Wladimir J. van der Laan\n<laanwj at gmail.com> wrote:\n>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.10.2 released",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir",
                "Wladimir J. van der Laan"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3671
        }
    },
    {
        "title": "[Bitcoin-development] Regtest Consensus Forking Behavior Introduced in Bitcoin Core in May 2014",
        "thread_messages": [
            {
                "author": "Dave Collins",
                "date": "2015-05-19T16:05:18",
                "message_text_only": "Hello All,\n\nJosh Rickmar and I discovered a subtle consensus change in Bitcoin Core\nwhich causes forking behavior on regtest.  Luckily it does not affect\nmainnet or testnet, however it does mean that regtest difficulty\nretargetting is broken.\n\nI've made a post to the bitcointalk forums at\nhttps://bitcointalk.org/index.php?topic=1065504 for the nicer formatting\nwhich explains in detail.\n\nRegards,\n\nDave Collins\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 834 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150519/05125acd/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Regtest Consensus Forking Behavior Introduced in Bitcoin Core in May 2014",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Dave Collins"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 698
        }
    },
    {
        "title": "[Bitcoin-development] ChainDB Whitepaper",
        "thread_messages": [
            {
                "author": "Eric Martindale",
                "date": "2015-05-19T16:13:49",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nHello all,\n\nBitPay has just released our first whitepaper on ChainDB, a new\npeer-to-peer database system backed by the Bitcoin blockchain.  This\npaper outlines our intended consensus mechanism, proof of fee.\n\nPlease take a look at the paper here: https://bitpay.com/chaindb.pdf\n\nWe are seeking comments and feedback on this mechanism.  I am happy to\nanswer any questions about the paper itself.\n\nSincerely,\n\nEric Martindale\n-----BEGIN PGP SIGNATURE-----\nComment: GPGTools - https://gpgtools.org\n\niQIcBAEBCgAGBQJVW2E9AAoJEHLoNvKeOhrJkLwP/1J14yGlZzddp4ApGRFsnnIz\nt8U9uZVvjsqxseYv6Pw3ZStQRkuBgcPDcQwMexeBi/0Z5K34LOM1565XRLtNG2sb\nAeLHG11ZLNK9SQSga2B0yc95uXs2Zje7Z+A+Q+h7HjhnkcQKbuLA+kB2+ZJv1CA3\ndV/5A0oCMBbZukzuFkbgmnhCaNwYjWY15UbwksKb2c3ktuLxZ5zUq/ZI+W+0PZsN\nPx2m/qkKb0UiUfbZU5Zva8HSI8lxQrEm/dkv4voglwlG3M7fvmgXcUi+8q0VslDi\n2Bx99rhpBaC79eHDUouhTNvLykP7Hal4KdyuzShlNBN+Z6AQyoeOdAQhk9YNw/iq\nc/tyiw6fFQVjEOJuJfetl2thByI+/hNH2m70BRXnaOtM+rQ4iIeaR7KevMi+WyYr\n+X9M6eqaYvkVXD1y0lEDCfsatYIhLUcXUVkM8gAdXF2yatqfCHENVYdZu9EDhYNa\nzC/N2akO+XNmj0a4mder3Oy0/j7vHTXq8HLHGFbCy3S3nld+A0Qe0/JTo/Vj1IZX\nREyBnWsaguIE8l/I/+423rzQYKlSEwP5j+V/ObTYouVCwmy+uJC8evNCI8T/APy9\nY04ocYLb2DnKLDOD8mlf+huf4x9WwK8+CdF/wm2g1SxLBchy5lkrmhbbD846HiRF\nm7EvzfRGI5zweCNIyx9Y\n=nDJ2\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-20T06:26:11",
                "message_text_only": "On Tue, May 19, 2015 at 09:13:49AM -0700, Eric Martindale wrote:\n> \n> Hello all,\n> \n> BitPay has just released our first whitepaper on ChainDB, a new\n> peer-to-peer database system backed by the Bitcoin blockchain.  This\n> paper outlines our intended consensus mechanism, proof of fee.\n> \n> Please take a look at the paper here: https://bitpay.com/chaindb.pdf\n> \n> We are seeking comments and feedback on this mechanism.  I am happy to\n> answer any questions about the paper itself.\n\nI'm quite disappointed to see that you still haven't fixed the problem\nthat transaction fees prove nothing at all. Among other things, you risk\ncreating a system where miners can much more cheaply sell the service of\nincluding the requisite \"high-fee\" transactions in blocks they mine for\nthe much lower cost of the risk of the blocks being orphaned and other\nminers getting those fees. In particular, the more hash power you have,\nthe lower that cost is - exactly the opposite kind of incentive than we\nwant. As described this is an extremely dangerous project, both to its\nusers, and Bitcoin as a whole; in general the idea of anything that\ntries to use transaction fees as \"proof\" is highly dangerous.\n\nYou should implement this with direct provably unspendable OP_RETURN\nsacrifices for now, and perhaps in the future, sacrifice to\nany-one-can-spend-in-the-future scriptPubKeys once CLTV is deployed.  If\nyou do this the interval needs to be long enough to robustly get past\nbusiness cycles - e.g. 1 year - to avoid the well-known problem that\nlarge miners can sell these proofs cheaply.\n\n\nOther comments:\n\n* Bitcoin does not securely store data; Bitcoin proves the publication of\ndata.(1) This can be seen by the recently added(2) pruning functionality\nwhich allows full nodes to discard all blockchain data other than the\nUTXO set and some number of recent blocks. (to handle reorganizations\nefficiently) Additionally even the UTXO set can be discarded in\nprinciple if my TXO commitments proposal is implemented.  Between both\nproposals there's no guarantee that data published to the Bitcoin\nblockchain will be stored by anyone at all, let alone be readily made\navailable.\n\n* The paper lacks a clear statement about what exactly the ChainDB\nproposal is attempting to accomplish, and what ChainDB attempts to\nprevent from happening. Are we trying to prove that data existed before\na certain time? (timestamping) Are we trying to prove that data reached\na certain audience? (proof-of-publication) Are we trying to come to\nconsensus on some type of mapping? (key:value consensus) What are we\nassuming from miners? Might miners attempt to censor ChainDB\ntransactions? For instance you say \"In the second rule, applying an\nunpredictable order for selecting the best chain mitigates certain\nattacks by Bitcoin miners\" but you don't say what those attacks are.  A\nkey question related to that is whether or not the ChainDB chains are or\nare not private, both recent and historical history.\n\n* \"A comprehensive ordering of all transactions also makes it possible\nto select a block even when some blocks are being withheld.\" Keep in\nmind that what has been \"withheld\" depends on what blocks you have\naccess too; from the point of view of one ChainDB user the withheld\nblocks may be the blocks another ChainDB user has access too and\nvice-versa. Again, the Bitcoin consensus is a way to prove publication\nof data with strong sybil attack detection - the cost to sybil attack\nChainDB will be quite low in many situations as miners have the\npriviledged position of having very low costs to include a transaction.\n\n* \"To minimize the risk that a builder loses bitcoin in the bidding\nprocess, builders coordinate to select a common UTXO that all bid\ntransactions use as an input. In so doing, bid transactions are created\nsuch that they deliberately conflict.\" This is a clever idea; I believe\nJeff Garzik deserves credit for this. (his auction proposal) Note too\nthat with SIGHASH_ANYONECANPAY the consensus scheme could be arranged\nsuch that anyone can also add additional funds to a proposed consensus\nthat they agree with. Better yet, with SIGHASH_SINGLE by \"stacking\"\nadditional inputs to the transaction you would ensure all bids end up in\nthe same transaction, simplifying the consensus logic. (otherwise the\ntotal bid is the sum of potentially multiple transactions sacrificing\nfunds in support of the same consensus)\n\n* Speaking of, proof-of-sacrifice or proof-of-burn is the common term\nused for a cryptographically provable expenditure. (e.g. for a fidelity\nbond(3)) Although in this case, it's not a true sacrifice as fee-paying\ntransactions by themselves can be trivially collected by miners.\n\n* \"And Factom [8] has advanced the concept of using the Bitcoin block\nchain directly for timestamping data\" Factom goes well beyond simply\ntimestamping data. (something my much earlier OpenTimestamps project did\namong many others) Rather Factom acts as a proof-of-publication layer\nthat allows the proving of the negative.(4)\n\n\nZookeyv\n-------\n\nChainDB has a lot of similarities with my Zookeyv(5) proposal, as well\nas some key differences. To recap the idea was to come to consensus on a\nkey:value mapping, such that there was a well-defined cost to change any\nparticular k:v pair, and such that 'uncontroversial' key:value pairs\nwould become more expensive to change over time as latter k:v pairs\nwould add to the cost to change of previous ones.\n\nMy original proposal was create a DAG of sacrifices, each committing a\nkey:value pair, and one or more previous nodes. (the case where n=1\nbeing a linear chain) Nodes that set a key:value already assigned would\nbe considered invalid. For any tip you'd be able to determine a sum\nsacrifice, and equally, a sum sacrificed on top of any key:value pair.\nIn hindsight, the rule set could be extended to all kinds of situations\nakin to a blockchain. (as you propose)\n\nA key question I came up with was whether or not the minimal data\nrequired to prove the shape of the graph be published directly in the\nblockchain. e.g. if a node consists of {H(key), H(value),\nprev_node_hash[]} do you require those values to be themselves published\nin the blockchain, or are they hidden behind a hash? The latter is more\nefficient and censorship resistant, while the former makes it possible\nto detect possible 51% attacks and outspend them. (Note how this notion\nof \"reactive security\" can be efficiently used to fend off attackers by\noutspending them after the fact, while keeping sacrifices low in the\ngeneral case; the sacrifice could even be crowdfunded with\nSIGHASH_ANYONECANPAY transactions)\n\n\n1) \"[Bitcoin-development] Disentangling Crypto-Coin Mining: Timestamping, Proof-of-Publication, and Validation\"\n   Peter Todd, Nov 19th, 2013,\n   http://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg03307.html\n\n2) https://github.com/bitcoin/bitcoin/pull/5863\n\n3) https://en.bitcoin.it/wiki/Fidelity_bonds\n\n4) \"Factom - Business Processes Secured by Immutable Audit Trails on the Blockchain\"\n   Paul Snow et. al, Nov 17th 2014,\n   https://github.com/FactomProject/FactomDocs/blob/master/Factom_Whitepaper.pdf\n\n5) #bitcoin-wizards discussion, May 31st 2013\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000e7980aab9c096c46e7f34c43a661c5cb2ea71525ebb8af7\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150520/e3843511/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "ChainDB Whitepaper",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Peter Todd",
                "Eric Martindale"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 8876
        }
    },
    {
        "title": "[Bitcoin-development] Scaling Bitcoin with Subchains",
        "thread_messages": [
            {
                "author": "Andrew",
                "date": "2015-05-20T02:55:49",
                "message_text_only": "Hi\n\nI briefly mentioned something about this on the bitcoin-dev IRC room. In\ngeneral, it seems experts (like sipa i.e. Pieter) are against using\nsidechains as a way of scaling. As I only have a high level understanding\nof the Bitcoin protocol, I cannot be sure if what I want to do is actually\ndefined as a side chain, but let me just propose it, and please let me know\nwhether it can work, and if not why not (I'm not scared of digging into\nmore technical resources in order to fully understand). I do have a good\nacademic/practical background for Bitcoin, and I'm ready to contribute code\nif needed (one of my contributions includes a paper wallet creator written\nin C).\n\nThe main problem I see with increasing the block size limit is that it\nincreases the amount of storage required to fully validate all transactions\nfor say 100 years (a person's life). With 1 MB blocks, you can store all\nlifetime transactions on a 5 TB drive, which basically any regular user can\ndo. With 10 MB blocks, you need a 50 TB drive, not accessible for regular\nusers. Yes, it's possible that in the future hard drive technology will get\ncheaper and smaller, but this still didn't happen, so we can't just say \"it\nshould be doable at the rate of Moore's law etc...\", we need to know that\nit is accessible for everyone, now. Also, don't forget that human life\nexpectancy can increase with time as well. I know, it sounds silly to use a\nhuman lifetime as a measurement of how far back each user should be able to\nstore transactions for, but what is a better measurement? This is a\ntechnology made for people i.e. humans, right, and the important part is\nthat it is for regular people and not just well privileged people. You can\nsearch my last four emails for some more calculations.\n\nWhat sipa told me on the IRC channel is that Bitcoin Core does not care\nabout old transactions. It only looks at the current blocks. Yes, that\nmakes sense, but how do you know that your machine wasn't compromised when\nvalidating the previous blocks? And what if you want to check some old\ntransactions (assuming you didn't index everything)? What if some of your\nold transaction data was lost or corrupted? I think it is clear that it is\nuseful to be able to validate all blocks (since 100 years) rather than just\na pruned part. It empowers people to have as much information about Bitcoin\ntransactions as do large data centers; transactions that may include\ngovernment or corporate corruption. This is the key to how Bitcoin enables\ntransparency for those who should be transparent (individual users with\nprivate addresses can still remain anonymous). Also, 5 TB takes about 20\ndays to sync starting fresh, on a regular computer, so it allows easy entry\ninto the system.\n\nSo assuming we agree that people should be able to store ~ a lifetime of\ntransactions, then we need 1 MB blocks. But of course, this leads to huge\ntransaction costs, and small purchases will be out of limits. So to fix\nthis, I propose adding a 10 1 MB chains below the main chain (sorry on the\nIRC room I said 10 10 MB chains by mistake), so effectively, you have a new\n10 MB chain that is partitioned into 10 parts. You can also add a third\nlevel: 100 1 MB chains, and keep going like that. The idea is that when you\nmake a large transaction, you put it through the top chain; when you make a\nmedium sized transaction, you put it through one of the middle chains,\nwhich will be verified (mined) by the middle chain, and the top chain will\nverify the aggregate transactions of the middle chain. If you have a small\nsized transaction, you put it through one of the bottom chains, the bottom\nchain verifies it, the middle chain verifies the aggregate transactions of\nthe bottom chain, and the top chain verifies the aggregate transactions of\nthe middle chain. By aggregate transaction, I mean the net result of\nmultiple transactions, and I suppose it can be 20 transactions belonging\nonly to one \"sibling\" chain for level 2, or 200 transactions for level 3,\netc...\n\nNow, how does the system decide to which of the 10 chains the middle sized\ntransaction goes to? I propose just taking some simple function of the\ninput addresses mod 10, so that you can just keep randomly generating a\nwallet until you get one with only addresses that map to only one of the 10\nchains (even distribution), so that someone can choose one of the 10\nchains, and store only the transactions that belong to that chain. They\nshould also choose a chain from level 3, etc... So in effect, they will be\nstoring a chain with block size O(n) where n is the number of levels. They\nmay store multiple sibling chains at one level, if they want to track of\nother people's transactions, such as those of their government MP, or\nperhaps, they want to have a separate identity that would be more anonymous\nwith a separate series of sibling chains. This will increase the storage\nsize, but the increase will be proportional to the number of things you\nwant to keep track of (at least this kind of system gives you the ability\nto fine tune your storage needs to the level of \"things\" you want to keep\ntrack of). Also, note that there may likely be duplication of transactions,\nsince transactions can include addresses that are associated with different\nsilbling chains, but this effect shouldn't make a big difference, and again\nwill depend on the complexity of the transactions you want to keep track of.\n\nSo how can this work? I propose that we keep the current chain as the top\nchain, and then create 10 level 2 chains that also store Bitcoin and the\nBitcoin can be transferred between chains (I think this is the idea of\nsidechains). How can we incentivize people to keep mining on the level 1\nchain? Perhaps force it into the (soft fork) protocol that anyone mining on\nlevel 2, has to also mine on level 1, and in general, anyone mining on\nlevel n+1 has to also mine on levels n,n-1,...,1. Also, level 1 will have\nthe best decentralization, so there should be enough people paying fees to\nget transactions there for their large transactions that require a high\nlevel of security and trust. Even if people stop using level 1, any bitcoin\nyou own in the level 1 chain, can be transferred to level 2, and still you\nhave 1 MB blocks due to the partitioning scheme. How to prevent\ntransactions from clustering on one or a  few sibling chains in a\nparticular level? Well the more empty chains should have lower fees, so\nthat should incentivize people... Note: This system also allows for the\nfine tuning of the transaction size to security ratio. Yes the lower chains\nwill be less secure, but a lower sized transaction does not need as much\nsecurity.\n\nFor instant transactions, there can also be Lightning channels linked to\nwhatever level of chain you want.\n\nOK so it seems to me that this can work and would only require a soft fork.\nBut, as I said, I only have a high level understanding of the Bitcoin\nprotocol, so it feels kind of too good to be true, and I am ready for heavy\ncriticism. I am only writing this because I care about Bitcoin and I want\nit to remain decentralized and become the best that it can be. I think it\nis important to do the right thing rather than (as most people naturally\ndo) the most convenient thing.\n\nThanks\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150520/ca03dd02/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T18:15:39",
                "message_text_only": "Hi Andrew,\n\nYour belief that Bitcoin has to be constrained by the belief that hardware\nwill never improve is extremist, but regardless, your concerns are easy to\nassuage: there is no requirement that the block chain be stored on hard\ndisks. As you note yourself the block chain is used for building/auditing\nthe ledger. Random access to it is not required, if all you care about is\nrunning a full node.\n\nLuckily this makes it a great fit for tape backup. Technology that can\nstore 185 terabytes *per cartridge* has already been developed:\n\nhttp://www.itworld.com/article/2693369/sony-develops-tape-tech-that-could-lead-to-185-tb-cartridges.html\n\nAs you could certainly share costs of a block chain archive with other\npeople, the cost would not be a major concern even today. And it's\nvirtually guaranteed that humanity will not hit a storage technology wall\nin 2015.\n\nIf your computer is compromised then all bets are off. Validating the chain\non a compromised host is meaningless.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/2d786fb8/attachment.html>"
            },
            {
                "author": "Andrew",
                "date": "2015-05-28T02:16:36",
                "message_text_only": "Hi All\n\nI discussed this idea with some other core developers (on IRC) and they\ngenerally seem to agree that it can be done.\n\nIt may be equivalent to an idea called \"blockchain extensions\" but when I\nlooked it up on bitcointalk.org I didn't see exactly the same proposal I am\nmaking.\n\nOne person suggested I should replace the address to chain function with a\nprotocol addition that allows one to specify the target chain. Yes, this\ncan also be done without changing the key properties.\n\nOne person said that the main problem is that I am not saying anything\nspecific, and I should address the sidechain problems written about in the\nsidechains paper. Well, actually, there is one quite specific thing I am\nsaying, in case you didn't notice: With this system, the network can\nachieve effectively 5^{n-1} MB blocks with each participant only storing n\nMB blocks. So for example, you can have effectively a block size of 625 MB\n(= 5^4) with each participant only storing 3 MB blocks; or 3.125 GB blocks\nwith each participant only storing 4 MB blocks. For these calculations, I\nam assuming that only two separate sibling chains are involved in a\ntransaction, so there is a duplication effect that divides in two the\neffective size of a given level of blocks (that's why it's 5 instead of 10\nas would be without duplication). If you want to involve multiple sibling\nchains in one transaction, you can effectively achieve this by performing\nmultiple transactions involving 2 of the multiple chains. Yes, the fees\nwould be higher since you have more transactions to make, but it is\nreasonable to expect more fees for more complicated transactions, and I\ndon't think it will result in people clustering on one chain (people who do\nthese kinds of transactions would probably track multiple chain paths). As\nfor the problems with sidechains, I think they would be eliminated due to\nthe child-parent dependence I specified. I also propose the following\nadditional rule: In case of conflict between parent and child chains (due\nto reorganizations), the child chain must choose the consensus of the\nparent chain. Also, for transferring from child to parent, the miners on\nthe parent have the final say, but to make it more clear, they can use the\nrelative difference of difficulty between their chain and the child chain\nto decide how many blocks deep a transaction in the child chain has to be\nto be accepted in the parent chain.\n\nGavin was the only one who disapproved of this, but I am not sure if he\nactually read the whole thing that I wrote. He said something along the\nline of \"the outputs will span the subchains\" and when I asked for an\nexplanation he just said that I need to learn more about things. I stated\nto him my willingness to learn, but have yet to get a response from him.\n\nMike: You should also keep in mind the big picture when it comes to\ndecentralization. If the hard drives (or tapes) can only be produced by a\nsmall number of large companies like Western Digital or Seagate, then you\ncan't really count those for a decentralized system. A truly decentralized\nsystem would have the devices needed to participate in (and verify) the\nsystem be easily created by a regular user of the system without relying on\na central power. So for example, the hard drives needed to store the\nbitcoin transaction records should be able to be produced at a regular\nperson's home on a 3D printer starting from just the raw materials. I don't\nknow how close we are to this ideal, but just pointing out that it needs to\nbe considered. This is also a reason why I like that Bitcoin uses the\nsimple SHA sum for mining instead of a more complicated function such as\nscrypt. It makes it easier for small scale entities to understand and to\nproduce the ASIC miners. Also, in addition to the centralization of storage\ndevice manufacturing, one should also consider what would happen if\neveryone wanted to have a 5 TB drive at home. What would happen to the\nprice of hard drives? Keep in mind also that the human population is likely\nincreasing, so there are less real resources per person... Yes maybe in the\nfuture we can solve these problems, but we still haven't, so let's not\nassume they are solved. Also, you mentioned sharing the costs of a hard\ndrive with other people. Do you mean trusting that others did not\ncompromise the hard drives? If you want you can do so, but not every\nparticipant should be forced to trust others, a point I think I made\nalready. And finally, this is all a discussion on the costs of running a\nBitcoin node. Bitcoin is not all that people will use hard drives and\ncomputers for; we need to leave room for other things.\n\nSo Mike, I have a question for you. Are you supporting a block size\nincrease partly due to philosophical reasons (i.e. you believe that regular\npeople shouldn't have such strong freedom as I want) or do you just not\ncare so much about the long term future and you just want to get your\nBitcoin related projects up and running with minimal complications? Or is\nit a combination of both things? You should disclose this to the people\nfollowing your words because they trust you as an experienced professional\nwith a good reputation, and it would be dishonest to not disclose this to\nthem. (same goes for Gavin)\n\nOverall, I think this system is the only system that I heard of that can\nscale decentralization without a block size increase. Lightning by itself,\nfor example, requires a block size increase that depends on how many such\nLightning contracts are being made, so relies on people changing the\nprotocol, which is obviously less secure and robust than a fixed protocol.\nBut I am not ruling out any other possibilities, so other things should\nalso be considered. But eventually, we may have to decide how to scale\nwithout knowing for sure whether the chosen scaling method is the ultimate\nscaling method. And I think this is a good candidate for that, and also,\ncan be reversed later on without changing the original protocol before the\nsoftfork. Actually, we can just make nodes advertise whether they support\nthe soft fork or not, and if a better scaling protocol comes along, those\nnodes can switch to advertise the better one. So it is quite a harmless\nsoft fork to make, in my opinion.\n\n\nOn Mon, May 25, 2015 at 6:15 PM, Mike Hearn <mike at plan99.net> wrote:\n\n> Hi Andrew,\n>\n> Your belief that Bitcoin has to be constrained by the belief that hardware\n> will never improve is extremist, but regardless, your concerns are easy to\n> assuage: there is no requirement that the block chain be stored on hard\n> disks. As you note yourself the block chain is used for building/auditing\n> the ledger. Random access to it is not required, if all you care about is\n> running a full node.\n>\n> Luckily this makes it a great fit for tape backup. Technology that can\n> store 185 terabytes *per cartridge* has already been developed:\n>\n>\n> http://www.itworld.com/article/2693369/sony-develops-tape-tech-that-could-lead-to-185-tb-cartridges.html\n>\n> As you could certainly share costs of a block chain archive with other\n> people, the cost would not be a major concern even today. And it's\n> virtually guaranteed that humanity will not hit a storage technology wall\n> in 2015.\n>\n> If your computer is compromised then all bets are off. Validating the\n> chain on a compromised host is meaningless.\n>\n\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/5f7a0d37/attachment.html>"
            },
            {
                "author": "Bryan Bishop",
                "date": "2015-05-28T02:34:28",
                "message_text_only": "On Wed, May 27, 2015 at 9:16 PM, Andrew <onelineproof at gmail.com> wrote:\n\n> You should also keep in mind the big picture when it comes to\n> decentralization. If the hard drives (or tapes) can only be produced by a\n> small number of large companies like Western Digital or Seagate, then you\n> can't really count those for a decentralized system. A truly decentralized\n> system would have the devices needed to participate in (and verify) the\n> system be easily created by a regular user of the system without relying on\n> a central power. So for example, the hard drives needed to store the\n> bitcoin transaction records should be able to be produced at a regular\n> person's home on a 3D printer starting from just the raw materials. I don't\n> know how close we are to this ideal, but just pointing out that it needs to\n> be considered. This is also a reason why I like that Bitcoin uses the\n> simple SHA sum for mining instead of a more complicated function such as\n> scrypt. It makes it easier for small scale entities to understand and to\n> produce the ASIC miners.\n\n\nI am a huge fan of do-it-yourself at-home ASIC manufacturing. The original\n4004 and earlier devices are within the scope of what could be accomplished\nin a home environment. The homecmos project is an interesting glimpse at\nthese possibilities. Relevant-scale mining will most likely never be an\noption for home manufacturing, but bitcoin wallets and other devices can\ndefinitely be etched by hand or using maskless projector lithography.\n\nHere's what the homecmos group was up to:\nhttps://code.google.com/p/homecmos/\nhttp://homecmos.drawersteak.com/wiki/\nhttp://diyhpl.us/~bryan/papers2/optics/photolithography/DIY%20fabrication%20of%20microstructures%20by%20projection%20photolithography.pdf\n\nLCD projection lithography:\nhttp://diyhpl.us/~bryan/papers2/optics/photolithography/Cell%20micropatterning%20using%20photopolymerization%20with%20a%20liquid%20crystal%20device%20(LCD)%20commercial%20projector%20-%20Itoga%20-%202003.pdf\nhttp://diyhpl.us/~bryan/papers2/optics/photolithography/Development%20of%20microfabrication%20technology%20with%20maskless%20photolithography%20device%20using%20LCD%20projector%20-%20Itoga%20-%202010.pdf\nhttp://diyhpl.us/~bryan/papers2/optics/photolithography/Second-generation%20maskless%20photolithography%20device%20for%20surface%20micropatterning%20and%20microfluidic%20channel%20fabrication%20(using%20an%20LCD%20projector).pdf\n\nDMD lithography:\nhttp://diyhpl.us/~bryan/papers2/optics/photolithography/Maskless%20microscopic%20lithography%20through%20shaping%20ultraviolet%20(UV)%20laser%20with%20digital%20micromirror%20device%20(DMD)%20-%202013.pdf\nhttp://diyhpl.us/~bryan/papers2/optics/photolithography/A%20maskless%20photolithographic%20prototyping%20system%20using%20a%20low-cost%20consumer%20projector%20and%20a%20microscope.pdf\n\nThere's actually a method of doing this with conventional camera roll film:\nhttps://groups.google.com/d/msg/diybio/5hpQXZ6hFKY/baGNfY_-Wx8J\n\n- Bryan\nhttp://heybryan.org/\n1 512 203 0507\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/7e468480/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Scaling Bitcoin with Subchains",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Bryan Bishop",
                "Mike Hearn",
                "Andrew"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 19439
        }
    },
    {
        "title": "[Bitcoin-development] Virtual Notary.",
        "thread_messages": [
            {
                "author": "Emin G\u00fcn Sirer",
                "date": "2015-05-20T10:25:01",
                "message_text_only": "Hi everyone,\n\nGiven the recent discussions on projects that use the Bitcoin blockchain to\nrecord factoids, people on this list might be interested in the Virtual\nNotary project. Virtual Notary is essentially an online witness (aka\nattestor) to online factoids. It can provide:\n\n  * proof of Bitcoin funds (without revealing public addresses or fund\nlocation on the blockchain)\n\n  * proof of Bitcoin address ownership\n\n  * proof of Tweet\n\n  * proof of real estate value\n\n  * proof of DNS ownership\n\n  * proof of existence\n\n  * proof of web page contents\n\n  * proof of weather conditions\n\nThe factoids can be recorded on the blockchain (if you pay for the\ntransaction with Bitcoin or PayPal), or they can be part of a free\nattestation chain that we maintain. The website provides a permanent URL to\nthe factoids it generates; it also provides an X.509 certificate that you\ncan download and keep safe in perpetuity, independent of the website.\n\nThe link to the website is here:\n  http://virtual-notary.org\n\nThe link to the writeup describing the various factoids and their use cases\nis here:\n  http://hackingdistributed.com/2013/06/20/virtual-notary-intro/\n\nWe are actively looking for people who are interested in developing the\nservice further. Specifically, if you have suggestions for how to extend\nthe service, for new proof/factoid types, or for how to build a business\ncase around the core idea, please let us know.\n\nBest,\n- egs\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150520/995e8043/attachment.html>"
            },
            {
                "author": "Jeff Garzik",
                "date": "2015-05-20T15:54:33",
                "message_text_only": "On Wed, May 20, 2015 at 3:25 AM, Emin G\u00fcn Sirer <el33th4x0r at gmail.com>\nwrote:\n\n> Hi everyone,\n>\n> Given the recent discussions on projects that use the Bitcoin blockchain\n> to record factoids, people on this list might be interested in the Virtual\n> Notary project. Virtual Notary is essentially an online witness (aka\n> attestor) to online factoids. It can provide:\n>\n>   * proof of Bitcoin funds (without revealing public addresses or fund\n> location on the blockchain)\n>\n>   * proof of Bitcoin address ownership\n>\n>   * proof of Tweet\n>\n\n\nFor what it's worth, a subsidiary of Dunvegan Space Systems is pursuing\nexactly this as a business.\n\nEMail JGarzik at DSS.co if you want to know more.\n\n-- \nJeff Garzik\nBitcoin core developer and open source evangelist\nBitPay, Inc.      https://bitpay.com/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150520/99197051/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-25T18:07:09",
                "message_text_only": "Very nice Emin! This could be very useful as a building block for oracle\nbased services. If only there were opcodes for working with X.509 ;)\n\nI'd suggest at least documenting in the FAQ how to extract the data from\nthe certificate:\n\nopenssl pkcs12 -in virtual-notary-cert-stocks-16070.p12 -nodes -passin\npass:\"\" | openssl x509 -text|less\n\nThat's good enough to get started, but I note two issues:\n\n\n   1. X.509 is kind of annoying to work with: example code in popular\n   languages/frameworks to extract the statement would be useful.\n\n   2. The stock price plugin, at least, embeds the data as text inside the\n   X.509 certificate. That's also not terribly developer friendly and risks\n   parsing errors undermining security schemes built on it.\n\n   The way I'd solve this is to embed either a protocol buffer or DER\n   encoded structure inside the extension, so developers can extract the\n   notarised data directly, without needing to do any additional parsing.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/eae53ab1/attachment.html>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2015-05-22T09:00:24",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA1\n\n> * proof of Bitcoin funds (without revealing public addresses or \n> fund location on the blockchain)\n> \n> * proof of Bitcoin address ownership\n> \n> * proof of Tweet\n> \n> * proof of real estate value\n> \n> * proof of DNS ownership\n> \n> * proof of existence\n> \n> * proof of web page contents\n> \n> * proof of weather conditions\n> \n> The factoids can be recorded on the blockchain (if you pay for the \n> transaction with Bitcoin or PayPal), or they can be part of a free \n> attestation chain that we maintain. The website provides a \n> permanent URL to the factoids it generates; it also provides an \n> X.509 certificate that you can download and keep safe in \n> perpetuity, independent of the website.\n\n\nHi Emin\n\nThis is going into the right direction. Well done!\nThe certificates (X.509/p12) are far more enduser-friendly than just a\nnormal PoE hash.\n\nYour site needs some UX love and i just tried to OR_RETURN a\nEmail-Address-Verification. But after creating a 0.0001 tx and waiting\nfor two confirmations it still said that the payment has not yet been\nreceived. There is probably something broken regarding the bitcoin\npayment verification.\n\nThe weather and real estate notarization definitively needs a \u201eUS\nonly\u201c badge somewhere.\n\nTwo ideas:\n- - Maybe adding a way of decentralize your notary service log via a\nopensource p2p daemon (obviously sensitive data should be somehow\nencrypted)?\n- - Adding a opensource UI app to examine certs (maybe offline capable\nwith p2p daemon chain as mentioned above). This could prove\nindependence from your website/service.\n\n</jonas>\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niQIcBAEBAgAGBQJVXvAoAAoJECnUvLZBb1PsJpsQAKRYDUTUYA59765w0jbBlK+S\nArxpaxwPmG7ZLhDYoTHJ/welvXsMSzREZrJNKYl7LBHQBPldeTRQHfHwH05qiwBL\nH5rC+BTyaglud3x7Bxo0fNrXJB4tkfX2ykPJs+2bqPi9OE0uVlXi2Vh/6cV1U/Uq\nRWRfpa19GnSE7IRft5G19FVsG8hrrpuLhzVraAQeZTLyGBKd+hlpjI/qr6TOl8ra\n5K3bFb2J1+UoaFXLlKCSsSx+9PsydlcJFwnr2H/Z7r1M39j5XYag3Ba6W58ats4z\n6DCTL1xRVOCTNDbVgkYzZUDCtv5oDspQ2S0nauJLDVz5ADUaZ+bsmBFwseo+XuZV\nTLUxsYfPsqEzUKF2a7ZfMjQG0EUx8oh5DU+o1F5wcBSXDOM+ucdOGYbMrBMV1i9W\nGoPUN1QPkqfUTTRiYYnzP2ySyPUoJqZrcwEB7E7nV8O2xE4Q00zCDlOSqU8aVl7j\n9lIs/sTpcK5S0kFfc68n6NVlWYU+CBdGlnmvMdbEkydV2P3ft1+THqQ8LNiYRhWX\n7kHzgG58yUQjvgsOEEb4xTXgA5u4euxBVY+SwKN6cTXA7dWg1s39UEJroaKuXYN5\niZTj2XkBtD+pwtUPIMs79Kb2/PZRGy7SJui2RbExFX1/8oBbHRV5Ii6+MNw11RNz\nRTV0kHof6+6u1BFYHTFX\n=UmgJ\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Ben Vulpes",
                "date": "2015-05-23T01:46:12",
                "message_text_only": "I cannot let http://www.deedbot.org go unmentioned on this thread.\n\nOn Friday, May 22, 2015, Jonas Schnelli <dev at jonasschnelli.ch> wrote:\n\n> -----BEGIN PGP SIGNED MESSAGE-----\n> Hash: SHA1\n>\n> > * proof of Bitcoin funds (without revealing public addresses or\n> > fund location on the blockchain)\n> >\n> > * proof of Bitcoin address ownership\n> >\n> > * proof of Tweet\n> >\n> > * proof of real estate value\n> >\n> > * proof of DNS ownership\n> >\n> > * proof of existence\n> >\n> > * proof of web page contents\n> >\n> > * proof of weather conditions\n> >\n> > The factoids can be recorded on the blockchain (if you pay for the\n> > transaction with Bitcoin or PayPal), or they can be part of a free\n> > attestation chain that we maintain. The website provides a\n> > permanent URL to the factoids it generates; it also provides an\n> > X.509 certificate that you can download and keep safe in\n> > perpetuity, independent of the website.\n>\n>\n> Hi Emin\n>\n> This is going into the right direction. Well done!\n> The certificates (X.509/p12) are far more enduser-friendly than just a\n> normal PoE hash.\n>\n> Your site needs some UX love and i just tried to OR_RETURN a\n> Email-Address-Verification. But after creating a 0.0001 tx and waiting\n> for two confirmations it still said that the payment has not yet been\n> received. There is probably something broken regarding the bitcoin\n> payment verification.\n>\n> The weather and real estate notarization definitively needs a \u201eUS\n> only\u201c badge somewhere.\n>\n> Two ideas:\n> - - Maybe adding a way of decentralize your notary service log via a\n> opensource p2p daemon (obviously sensitive data should be somehow\n> encrypted)?\n> - - Adding a opensource UI app to examine certs (maybe offline capable\n> with p2p daemon chain as mentioned above). This could prove\n> independence from your website/service.\n>\n> </jonas>\n> -----BEGIN PGP SIGNATURE-----\n> Version: GnuPG v1\n>\n> iQIcBAEBAgAGBQJVXvAoAAoJECnUvLZBb1PsJpsQAKRYDUTUYA59765w0jbBlK+S\n> ArxpaxwPmG7ZLhDYoTHJ/welvXsMSzREZrJNKYl7LBHQBPldeTRQHfHwH05qiwBL\n> H5rC+BTyaglud3x7Bxo0fNrXJB4tkfX2ykPJs+2bqPi9OE0uVlXi2Vh/6cV1U/Uq\n> RWRfpa19GnSE7IRft5G19FVsG8hrrpuLhzVraAQeZTLyGBKd+hlpjI/qr6TOl8ra\n> 5K3bFb2J1+UoaFXLlKCSsSx+9PsydlcJFwnr2H/Z7r1M39j5XYag3Ba6W58ats4z\n> 6DCTL1xRVOCTNDbVgkYzZUDCtv5oDspQ2S0nauJLDVz5ADUaZ+bsmBFwseo+XuZV\n> TLUxsYfPsqEzUKF2a7ZfMjQG0EUx8oh5DU+o1F5wcBSXDOM+ucdOGYbMrBMV1i9W\n> GoPUN1QPkqfUTTRiYYnzP2ySyPUoJqZrcwEB7E7nV8O2xE4Q00zCDlOSqU8aVl7j\n> 9lIs/sTpcK5S0kFfc68n6NVlWYU+CBdGlnmvMdbEkydV2P3ft1+THqQ8LNiYRhWX\n> 7kHzgG58yUQjvgsOEEb4xTXgA5u4euxBVY+SwKN6cTXA7dWg1s39UEJroaKuXYN5\n> iZTj2XkBtD+pwtUPIMs79Kb2/PZRGy7SJui2RbExFX1/8oBbHRV5Ii6+MNw11RNz\n> RTV0kHof6+6u1BFYHTFX\n> =UmgJ\n> -----END PGP SIGNATURE-----\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net <javascript:;>\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150522/bd3e1553/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Virtual Notary.",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "Emin G\u00fcn Sirer",
                "Mike Hearn",
                "Ben Vulpes",
                "Jonas Schnelli"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 9699
        }
    },
    {
        "title": "[Bitcoin-development] BIP for deterministic pay-to-script-hash multi-signature addresses",
        "thread_messages": [
            {
                "author": "Thomas Kerin",
                "date": "2015-05-22T17:28:02",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nI wonder are there any other blockers or modifications that need to be\nmade for this to be merged?\n\nLatest version of the document:\nhttps://github.com/afk11/bips/blob/213e8a27a3a2eaaf44f79221a9f9f888af002801/bip-0067.mediawiki\n\n\n\nOn 13/02/15 23:43, Thomas Kerin wrote:\n>\n> On 12/02/15 22:13, Luke Dashjr wrote:\n>> Where is the Specification section?? Does this support arbitrary\nscripts, or\n>> only the simplest CHECKMULTISIG case?\n>\n> The BIP is a process for deriving only the type of scripts you would\nencounter doing addmultisigaddress. More complicated scripts would\nrequire more metadata to be shared, but the only case we describe is\nwhen given public keys and the number of signatures required.\n>\n> You're right, we're missing a Specification. I have tweaked the\ndocument to cover this now.\n>\n>\n>\n> On 13/02/15 07:53, Peter Todd wrote:\n>> It might be enough to rewrite this BIP to basically say \"all pubkeys\nexecuted by all CHECKMULTISIG opcodes will be in the following canonical\norder\", followed by some explanatory examples of how to apply this\nsimple rule. OTOH we don't yet have a standard way of even talking about\narbitrary scripts, so it may very well turn out to be the case that the\nabove rule is too restrictive in many cases - I certainly would not want\nto do a soft-fork to enforce this, or even make it an IsStandard() rule.\n>\n> It would be interesting, but I agree it should not be brought into\nthese validation rules - just a convention for people to follow for now.\nI think it's fair that implementers are free to order them however they\nplease. But I think there is good reason for wallets to opt in to the\nconvention and declare this, for ease of recovery, and for\ninteroperability reasons.\n>\n>\n> --\n> Thomas Kerin\n> -------------------------\n> My PGP key can be found here\n<http://pgp.mit.edu/pks/lookup?op=get&search=0x3F0D2F83A2966155>\n>\n>\n>\n------------------------------------------------------------------------------\n> Dive into the World of Parallel Programming. The Go Parallel Website,\n> sponsored by Intel and developed in partnership with Slashdot Media,\nis your\n> hub for all things parallel software development, from weekly thought\n> leadership blogs to news, videos, case studies, tutorials and more. Take a\n> look and join the conversation now. http://goparallel.sourceforge.net/\n>\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n- -- \nThomas Kerin\n- -------------------------\n\nMy PGP key can be found here\n<http://pgp.mit.edu/pks/lookup?op=get&search=0x3F0D2F83A2966155>\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v2.0.22 (GNU/Linux)\n\niQJ8BAEBCgBmBQJVX2ciXxSAAAAAAC4AKGlzc3Vlci1mcHJAbm90YXRpb25zLm9w\nZW5wZ3AuZmlmdGhob3JzZW1hbi5uZXQ2MzI1MzM4QjJGOTU5OEUzREMzQzc0MzAz\nRjBEMkY4M0EyOTY2MTU1AAoJED8NL4OilmFVlOwP/1w/Omr/6jGyi7spqW22HQ7P\n4+lNNzcsWp5/pv8e6YelUOSYiHuh/KxRoFfWL+wF+PNS2EtjRxSsXxg/R2nMft7B\nJQLNmIG6zTzVg/lhVObeSslXaia7repZxZ1S4nyEcs8rDVt7kkNnNguFOeONF95O\n3usCnrch+QbQacIt9StySAz155u1SuHeSmGmA/fRGLfArndXDdN0fYwE1KGyv5wm\nLqZ1PQfmYaCc0TUKRvpDRuc/+KF7q1fDMzuP9mZ3WiPdvTDKCXSRxYfbQYJdxplg\nAC0CFiOne+DXgiBdTOIcs9pcp1/6SyZs75Bkpv71AxBCmTlRTuYpsfH7O3VuZBGP\nFrN/4BMYnzMbGnNmvZwerUKH59MmzZTAzLSwZlpvj7ZxRks6KOp1CHInFWQlHAXJ\nO5c5McvqSdQ0rPHLcQ4DwB00Q1els18NRULjxdsTfLrT32birIXn3M1Hn/Q9d8Sa\nN+Y/cfXkojf4rJt75+XwjLyHECwS378ZFC8lfs1m/B3VSQxTtTZWA8905a1IRv/F\nnPQ2eaxBrFjm4OatE5lx+I/xmVAQuybG54UdcZGaKVXJbMg3sOslcYg7eA77pmR5\n7jRoRU+q7GhiRsUmxSkD+57FfhaMzX7iUl4xe3YK14KUS/pONuv0USC9to8a62kA\ngz9kb4pJMEhTtZNv7z9C\n=iq37\n-----END PGP SIGNATURE-----\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150522/09d9bd89/attachment.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-05-24T00:44:20",
                "message_text_only": "A few months back, William Swanson and I had worked on a more general script template format. Unfortunately, other work has prevented us from being able to fully complete it - but here\u2019s the start:\n\nhttps://docs.google.com/document/d/1nGF6LjGwhzuiJ9AQwKAhN1a1SXvGGHWxoKmDSkiIsPI <https://docs.google.com/document/d/1nGF6LjGwhzuiJ9AQwKAhN1a1SXvGGHWxoKmDSkiIsPI>/\n\n- Eric Lombrozo\n\n> On Feb 12, 2015, at 11:53 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> On Thu, Feb 12, 2015 at 10:13:33PM +0000, Luke Dashjr wrote:\n>> Where is the Specification section?? Does this support arbitrary scripts, or\n>> only the simplest CHECKMULTISIG case?\n> \n> It might be enough to rewrite this BIP to basically say \"all pubkeys\n> executed by all CHECKMULTISIG opcodes will be in the following canonical\n> order\", followed by some explanatory examples of how to apply this\n> simple rule.\n> \n> OTOH we don't yet have a standard way of even talking about arbitrary\n> scripts, so it may very well turn out to be the case that the above rule\n> is too restrictive in many cases - I certainly would not want to do a\n> soft-fork to enforce this, or even make it an IsStandard() rule.\n> \n> --\n> 'peter'[:-1]@petertodd.org\n> 000000000000000013cf8270118ba2efce8b304f8de359599fef95c3ab43dcb1\n> ------------------------------------------------------------------------------\n> Dive into the World of Parallel Programming. The Go Parallel Website,\n> sponsored by Intel and developed in partnership with Slashdot Media, is your\n> hub for all things parallel software development, from weekly thought\n> leadership blogs to news, videos, case studies, tutorials and more. Take a\n> look and join the conversation now. http://goparallel.sourceforge.net/_______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150523/076f18ee/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 842 bytes\nDesc: Message signed with OpenPGP using GPGMail\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150523/076f18ee/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "BIP for deterministic pay-to-script-hash multi-signature addresses",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Thomas Kerin",
                "Eric Lombrozo"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6269
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin Core 0.9.5 released",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2015-05-24T07:47:57",
                "message_text_only": "Bitcoin Core version 0.9.5 is now available from:\n\n  https://bitcoin.org/bin/0.9.5/\n\nThis is a new minor version release, with the goal of backporting BIP66. There\nare also a few bug fixes and updated translations. Upgrading to this release is\nrecommended.\n\nPlease report bugs using the issue tracker at github:\n\n  https://github.com/bitcoin/bitcoin/issues\n\nHow to Upgrade\n===============\n\nIf you are running an older version, shut it down. Wait until it has completely\nshut down (which might take a few minutes for older versions), then run the\ninstaller (on Windows) or just copy over /Applications/Bitcoin-Qt (on Mac) or\nbitcoind/bitcoin-qt (on Linux).\n\nNotable changes\n================\n\nMining and relay policy enhancements\n------------------------------------\n\nBitcoin Core's block templates are now for version 3 blocks only, and any mining\nsoftware relying on its `getblocktemplate` must be updated in parallel to use\nlibblkmaker either version 0.4.2 or any version from 0.5.1 onward.\nIf you are solo mining, this will affect you the moment you upgrade Bitcoin\nCore, which must be done prior to BIP66 achieving its 951/1001 status.\nIf you are mining with the stratum mining protocol: this does not affect you.\nIf you are mining with the getblocktemplate protocol to a pool: this will affect\nyou at the pool operator's discretion, which must be no later than BIP66\nachieving its 951/1001 status.\n\n0.9.5 changelog\n================\n\n- `74f29c2` Check pindexBestForkBase for null\n- `9cd1dd9` Fix priority calculation in CreateTransaction\n- `6b4163b` Sanitize command strings before logging them.\n- `3230b32` Raise version of created blocks, and enforce DERSIG in mempool\n- `989d499` Backport of some of BIP66's tests\n- `ab03660` Implement BIP 66 validation rules and switchover logic\n- `8438074` build: fix dynamic boost check when --with-boost= is used\n\nCredits\n--------\n\nThanks to who contributed to this release, at least:\n\n- 21E14\n- Alex Morcos\n- Cory Fields\n- Gregory Maxwell\n- Pieter Wuille\n- Wladimir J. van der Laan\n\nAs well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/)."
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.9.5 released",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2134
        }
    },
    {
        "title": "[Bitcoin-development] Proof of Payment BIP-able?",
        "thread_messages": [
            {
                "author": "Kalle Rosenbaum",
                "date": "2015-05-24T20:45:16",
                "message_text_only": "Hi all!\n\nAs indicated in my first email regarding Proof of Payment (Mars 13, subject\n\"Proof of Payment\"), I would like to BIP it. I have two proposals:\n\n* PoP datastructure and process:\nhttps://github.com/kallerosenbaum/poppoc/wiki/Proof-of-Payment\n* btcpop: URI scheme:\nhttps://github.com/kallerosenbaum/poppoc/wiki/btcpop-scheme\n\nBasically, my question to the community is: Do you agree that these are\nBIP-able?\n\nThe proposals are not yet BIP formatted, but pretty complete. An\nimplementation is avaliable at https://github.com/kallerosenbaum/poppoc.\nSpecifically, the PoP validating code is in PopValidator.java\n<https://github.com/kallerosenbaum/poppoc/blob/master/src/main/java/se/rosenbaum/poppoc/core/validate/PopValidator.java>\n.\n\nAs far as I can tell from the previous thread, no major objection against\nthe idea was raised. PoP, if standardized, would bring a lot of utility to\nbitcoin: Paysite login, concert tickets, left luggage lockers, lotteries,\nvideo rental, etc.\n\nFurther on, I'd like to extend BIP70 to support PoP, but that will have to\nwait until we have consensus around the two basic proposals above.\n\nI have received some great feedback from the community and included most of\nit in the updated version of the specification. The essential changes are:\n\n* If a PoP for some reason appears in the bitcoin p2p network, we must make\nsure that IF it is included in a block it should have minimal impact. The\nsolution I chose was to include all outputs of the original paymet in the\nPoP. That way, if the PoP is included it will not alter the payees. (Thanks\nto Tom Harding for pointing out the problem and Magnus Andersson for the\ninitial solution).\n\n* The check if the transaction is actually a tx that you want proof for is\nmoved to later in the validation process. Otherwise, one could get\ninformation on what transactions pays for which services by simply sending\nerroneously signed PoPs with a transaction id we're interested in.\n\n* A version field of 2 bytes is included. Currently the only valid version\nis 0x00 0x01. (Thanks Martin Lie)\n\n* The \"PoP\" literal is removed. It provides little value as the receiver of\na PoP expects a PoP. (Again, thanks Martin Lie for making me think about\nthis.)\n\nRegards,\nKalle Rosenbaum\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150524/4aa5257e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proof of Payment BIP-able?",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Kalle Rosenbaum"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2429
        }
    },
    {
        "title": "[Bitcoin-development] alternatives to the 20MB block limit, measure first!",
        "thread_messages": [
            {
                "author": "Ron",
                "date": "2015-05-25T20:14:17",
                "message_text_only": "Hello all,\n\nWith all the discussion about the Block size limit, I thought it would be interesting to measure, in some sense, the average Tx size.\u00a0 Then given a fixed average block period (Bp) of 10 minutes (i.e 600 seconds), all one needs to do to estimate an average block size is ask the question: what average transaction rate (tps) do you want?\n\nSo for tps ~ 10 (Tx/sec) and an average transaction size (avgTxSz) of 612 Bytes (last ten blocks up to block 357998 (2:05pm EDT 5/25/2015) we have a block size of 612 * 10 * 600 = 3,672,000 Bytes\n\nAlternatively, given an avgTxSz ~612 and maxBl = 1,000,000 we have (maxBl / avgBlSz) / Bp is the actual current max tps, which is ~2.72 tps.\n\nThe avgBlSz for the 10 blocks up to block # 357999 is ~ 576 Bytes, so the current possible tps is ~2.89 and the maxBL for a tps = 10 is 3,456,000 bytes.\n\nSo I think one should state one's assumed tps and a measured or presumed avgTxSz before saying what a maxBl should be. So for a maxBl ~20,000,000 Bytes and a current avgTxSz ~600 Bytes, the tps ~55.5 FWIW\n\nRon (aka old c coder)\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/943ed1d9/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "alternatives to the 20MB block limit, measure first!",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Ron"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1254
        }
    },
    {
        "title": "[Bitcoin-development] Zero-Conf for Full Node Discovery",
        "thread_messages": [
            {
                "author": "Jim Phillips",
                "date": "2015-05-26T04:06:38",
                "message_text_only": "Is there any work being done on using some kind of zero-conf service\ndiscovery protocol so that lightweight clients can find a full node on the\nsame LAN to peer with rather than having to tie up WAN bandwidth?\n\nI envision a future where lightweight devices within a home use SPV over\nWiFi to connect with a home server which in turn relays the transactions\nthey create out to the larger and faster relays on the Internet.\n\nIn a situation where there are hundreds or thousands of small SPV devices\nin a single home (if 21, Inc. is successful) monitoring the blockchain,\nthis could result in lower traffic across the slow WAN connection.  And\nyes, I realize it could potentially take a LOT of these devices before the\ntotal bandwidth is greater than downloading a full copy of the blockchain,\nbut there's other reasons to host your own full node -- trust being one.\n\n--\n*James G. Phillips IV*\n<https://plus.google.com/u/0/113107039501292625391/posts>\n<http://www.linkedin.com/in/ergophobe>\n\n*\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n-- David Ogilvy*\n\n *This message was created with 100% recycled electrons. Please think twice\nbefore printing.*\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/1f012536/attachment.html>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-26T04:37:18",
                "message_text_only": "This is very simple to do. Just ping the \"all nodes\" address (ff02::1) and try connecting to TCP port 8333 of each node that responds. Shouldn't take but more than a few milliseconds on any but the most densely populated LANs.\n\n\nOn Monday, 25 May 2015, at 11:06 pm, Jim Phillips wrote:\n> Is there any work being done on using some kind of zero-conf service\n> discovery protocol so that lightweight clients can find a full node on the\n> same LAN to peer with rather than having to tie up WAN bandwidth?\n> \n> I envision a future where lightweight devices within a home use SPV over\n> WiFi to connect with a home server which in turn relays the transactions\n> they create out to the larger and faster relays on the Internet.\n> \n> In a situation where there are hundreds or thousands of small SPV devices\n> in a single home (if 21, Inc. is successful) monitoring the blockchain,\n> this could result in lower traffic across the slow WAN connection.  And\n> yes, I realize it could potentially take a LOT of these devices before the\n> total bandwidth is greater than downloading a full copy of the blockchain,\n> but there's other reasons to host your own full node -- trust being one.\n> \n> --\n> *James G. Phillips IV*\n> <https://plus.google.com/u/0/113107039501292625391/posts>\n> <http://www.linkedin.com/in/ergophobe>\n> \n> *\"Don't bunt. Aim out of the ball park. Aim for the company of immortals.\"\n> -- David Ogilvy*\n> \n>  *This message was created with 100% recycled electrons. Please think twice\n> before printing.*"
            },
            {
                "author": "Kevin Greene",
                "date": "2015-05-26T04:46:22",
                "message_text_only": "This is something you actually don't want. In order to make it as difficult\nas possible for an attacker to perform a sybil attack, you want to choose a\nset of peers that is as diverse, and unpredictable as possible.\n\n\nOn Mon, May 25, 2015 at 9:37 PM, Matt Whitlock <bip at mattwhitlock.name>\nwrote:\n\n> This is very simple to do. Just ping the \"all nodes\" address (ff02::1) and\n> try connecting to TCP port 8333 of each node that responds. Shouldn't take\n> but more than a few milliseconds on any but the most densely populated LANs.\n>\n>\n> On Monday, 25 May 2015, at 11:06 pm, Jim Phillips wrote:\n> > Is there any work being done on using some kind of zero-conf service\n> > discovery protocol so that lightweight clients can find a full node on\n> the\n> > same LAN to peer with rather than having to tie up WAN bandwidth?\n> >\n> > I envision a future where lightweight devices within a home use SPV over\n> > WiFi to connect with a home server which in turn relays the transactions\n> > they create out to the larger and faster relays on the Internet.\n> >\n> > In a situation where there are hundreds or thousands of small SPV devices\n> > in a single home (if 21, Inc. is successful) monitoring the blockchain,\n> > this could result in lower traffic across the slow WAN connection.  And\n> > yes, I realize it could potentially take a LOT of these devices before\n> the\n> > total bandwidth is greater than downloading a full copy of the\n> blockchain,\n> > but there's other reasons to host your own full node -- trust being one.\n> >\n> > --\n> > *James G. Phillips IV*\n> > <https://plus.google.com/u/0/113107039501292625391/posts>\n> > <http://www.linkedin.com/in/ergophobe>\n> >\n> > *\"Don't bunt. Aim out of the ball park. Aim for the company of\n> immortals.\"\n> > -- David Ogilvy*\n> >\n> >  *This message was created with 100% recycled electrons. Please think\n> twice\n> > before printing.*\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/1324d033/attachment.html>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-26T04:56:28",
                "message_text_only": "Who would be performing a Sybil attack against themselves? We're talking about a LAN here. All the nodes would be under the control of the same entity. In that case, you actually want them all connecting solely to a central hub node on the LAN, and the hub node should connect to \"diverse and unpredictable\" other nodes on the Bitcoin network.\n\n\nOn Monday, 25 May 2015, at 9:46 pm, Kevin Greene wrote:\n> This is something you actually don't want. In order to make it as difficult\n> as possible for an attacker to perform a sybil attack, you want to choose a\n> set of peers that is as diverse, and unpredictable as possible.\n> \n> \n> On Mon, May 25, 2015 at 9:37 PM, Matt Whitlock <bip at mattwhitlock.name>\n> wrote:\n> \n> > This is very simple to do. Just ping the \"all nodes\" address (ff02::1) and\n> > try connecting to TCP port 8333 of each node that responds. Shouldn't take\n> > but more than a few milliseconds on any but the most densely populated LANs.\n> >\n> >\n> > On Monday, 25 May 2015, at 11:06 pm, Jim Phillips wrote:\n> > > Is there any work being done on using some kind of zero-conf service\n> > > discovery protocol so that lightweight clients can find a full node on\n> > the\n> > > same LAN to peer with rather than having to tie up WAN bandwidth?\n> > >\n> > > I envision a future where lightweight devices within a home use SPV over\n> > > WiFi to connect with a home server which in turn relays the transactions\n> > > they create out to the larger and faster relays on the Internet.\n> > >\n> > > In a situation where there are hundreds or thousands of small SPV devices\n> > > in a single home (if 21, Inc. is successful) monitoring the blockchain,\n> > > this could result in lower traffic across the slow WAN connection.  And\n> > > yes, I realize it could potentially take a LOT of these devices before\n> > the\n> > > total bandwidth is greater than downloading a full copy of the\n> > blockchain,\n> > > but there's other reasons to host your own full node -- trust being one.\n> > >\n> > > --\n> > > *James G. Phillips IV*\n> > > <https://plus.google.com/u/0/113107039501292625391/posts>\n> > > <http://www.linkedin.com/in/ergophobe>\n> > >\n> > > *\"Don't bunt. Aim out of the ball park. Aim for the company of\n> > immortals.\"\n> > > -- David Ogilvy*\n> > >\n> > >  *This message was created with 100% recycled electrons. Please think\n> > twice\n> > > before printing.*\n> >\n> >\n> > ------------------------------------------------------------------------------\n> > One dashboard for servers and applications across Physical-Virtual-Cloud\n> > Widest out-of-the-box monitoring support with 50+ applications\n> > Performance metrics, stats and reports that give you Actionable Insights\n> > Deep dive visibility with transaction tracing using APM Insight.\n> > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >"
            },
            {
                "author": "Kevin Greene",
                "date": "2015-05-26T05:12:04",
                "message_text_only": "This is true, but the device doesn't know if the LAN it's on is a safe\nnetwork or a hotel wifi, for example. So there would be a tricky UX there.\nYou'd have to ask the user during set up if this is a trusted LAN or not;\nor something like that. That may not be an issue though depending on the\nnature of the product. For example, Chromecast doesn't need any security\nprotections against trolls on the same LAN. I guess it just depends on what\nyou're planning to build.\n\nOn Mon, May 25, 2015 at 9:56 PM, Matt Whitlock <bip at mattwhitlock.name>\nwrote:\n\n> Who would be performing a Sybil attack against themselves? We're talking\n> about a LAN here. All the nodes would be under the control of the same\n> entity. In that case, you actually want them all connecting solely to a\n> central hub node on the LAN, and the hub node should connect to \"diverse\n> and unpredictable\" other nodes on the Bitcoin network.\n>\n>\n> On Monday, 25 May 2015, at 9:46 pm, Kevin Greene wrote:\n> > This is something you actually don't want. In order to make it as\n> difficult\n> > as possible for an attacker to perform a sybil attack, you want to\n> choose a\n> > set of peers that is as diverse, and unpredictable as possible.\n> >\n> >\n> > On Mon, May 25, 2015 at 9:37 PM, Matt Whitlock <bip at mattwhitlock.name>\n> > wrote:\n> >\n> > > This is very simple to do. Just ping the \"all nodes\" address (ff02::1)\n> and\n> > > try connecting to TCP port 8333 of each node that responds. Shouldn't\n> take\n> > > but more than a few milliseconds on any but the most densely populated\n> LANs.\n> > >\n> > >\n> > > On Monday, 25 May 2015, at 11:06 pm, Jim Phillips wrote:\n> > > > Is there any work being done on using some kind of zero-conf service\n> > > > discovery protocol so that lightweight clients can find a full node\n> on\n> > > the\n> > > > same LAN to peer with rather than having to tie up WAN bandwidth?\n> > > >\n> > > > I envision a future where lightweight devices within a home use SPV\n> over\n> > > > WiFi to connect with a home server which in turn relays the\n> transactions\n> > > > they create out to the larger and faster relays on the Internet.\n> > > >\n> > > > In a situation where there are hundreds or thousands of small SPV\n> devices\n> > > > in a single home (if 21, Inc. is successful) monitoring the\n> blockchain,\n> > > > this could result in lower traffic across the slow WAN connection.\n> And\n> > > > yes, I realize it could potentially take a LOT of these devices\n> before\n> > > the\n> > > > total bandwidth is greater than downloading a full copy of the\n> > > blockchain,\n> > > > but there's other reasons to host your own full node -- trust being\n> one.\n> > > >\n> > > > --\n> > > > *James G. Phillips IV*\n> > > > <https://plus.google.com/u/0/113107039501292625391/posts>\n> > > > <http://www.linkedin.com/in/ergophobe>\n> > > >\n> > > > *\"Don't bunt. Aim out of the ball park. Aim for the company of\n> > > immortals.\"\n> > > > -- David Ogilvy*\n> > > >\n> > > >  *This message was created with 100% recycled electrons. Please think\n> > > twice\n> > > > before printing.*\n> > >\n> > >\n> > >\n> ------------------------------------------------------------------------------\n> > > One dashboard for servers and applications across\n> Physical-Virtual-Cloud\n> > > Widest out-of-the-box monitoring support with 50+ applications\n> > > Performance metrics, stats and reports that give you Actionable\n> Insights\n> > > Deep dive visibility with transaction tracing using APM Insight.\n> > > http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> > > _______________________________________________\n> > > Bitcoin-development mailing list\n> > > Bitcoin-development at lists.sourceforge.net\n> > > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> > >\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/05bd344b/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-05-26T05:23:04",
                "message_text_only": "On Tuesday, May 26, 2015 4:46:22 AM Kevin Greene wrote:\n> This is something you actually don't want. In order to make it as difficult\n> as possible for an attacker to perform a sybil attack, you want to choose a\n> set of peers that is as diverse, and unpredictable as possible.\n\nIt doesn't hurt to have a local node or two, though. Might as well to improve \npropagation, while maintaining the other peers to avoid sybil attacks.\n\nLuke"
            },
            {
                "author": "Jim Phillips",
                "date": "2015-05-26T04:48:16",
                "message_text_only": "Do any wallets actually do this yet?\nOn May 25, 2015 11:37 PM, \"Matt Whitlock\" <bip at mattwhitlock.name> wrote:\n\n> This is very simple to do. Just ping the \"all nodes\" address (ff02::1) and\n> try connecting to TCP port 8333 of each node that responds. Shouldn't take\n> but more than a few milliseconds on any but the most densely populated LANs.\n>\n>\n> On Monday, 25 May 2015, at 11:06 pm, Jim Phillips wrote:\n> > Is there any work being done on using some kind of zero-conf service\n> > discovery protocol so that lightweight clients can find a full node on\n> the\n> > same LAN to peer with rather than having to tie up WAN bandwidth?\n> >\n> > I envision a future where lightweight devices within a home use SPV over\n> > WiFi to connect with a home server which in turn relays the transactions\n> > they create out to the larger and faster relays on the Internet.\n> >\n> > In a situation where there are hundreds or thousands of small SPV devices\n> > in a single home (if 21, Inc. is successful) monitoring the blockchain,\n> > this could result in lower traffic across the slow WAN connection.  And\n> > yes, I realize it could potentially take a LOT of these devices before\n> the\n> > total bandwidth is greater than downloading a full copy of the\n> blockchain,\n> > but there's other reasons to host your own full node -- trust being one.\n> >\n> > --\n> > *James G. Phillips IV*\n> > <https://plus.google.com/u/0/113107039501292625391/posts>\n> > <http://www.linkedin.com/in/ergophobe>\n> >\n> > *\"Don't bunt. Aim out of the ball park. Aim for the company of\n> immortals.\"\n> > -- David Ogilvy*\n> >\n> >  *This message was created with 100% recycled electrons. Please think\n> twice\n> > before printing.*\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150525/a9d0482b/attachment.html>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-26T04:52:07",
                "message_text_only": "On Monday, 25 May 2015, at 11:48 pm, Jim Phillips wrote:\n> Do any wallets actually do this yet?\n\nNot that I know of, but they do seed their address database via DNS, which you can poison if you control the LAN's DNS resolver. I did this for a Bitcoin-only Wi-Fi network I operated at a remote festival. We had well over a hundred lightweight wallets, all trying to connect to the Bitcoin P2P network over a very bandwidth-constrained Internet link, so I poisoned the DNS and rejected all outbound connection attempts on port 8333, to force all the wallets to connect to a single local full node, which had connectivity to a single remote node over the Internet. Thus, all the lightweight wallets at the festival had Bitcoin network connectivity, but we only needed to backhaul the Bitcoin network's transaction traffic once.\n\n\n\n> On May 25, 2015 11:37 PM, \"Matt Whitlock\" <bip at mattwhitlock.name> wrote:\n> \n> > This is very simple to do. Just ping the \"all nodes\" address (ff02::1) and\n> > try connecting to TCP port 8333 of each node that responds. Shouldn't take\n> > but more than a few milliseconds on any but the most densely populated LANs.\n> >\n> >\n> > On Monday, 25 May 2015, at 11:06 pm, Jim Phillips wrote:\n> > > Is there any work being done on using some kind of zero-conf service\n> > > discovery protocol so that lightweight clients can find a full node on\n> > the\n> > > same LAN to peer with rather than having to tie up WAN bandwidth?\n> > >\n> > > I envision a future where lightweight devices within a home use SPV over\n> > > WiFi to connect with a home server which in turn relays the transactions\n> > > they create out to the larger and faster relays on the Internet.\n> > >\n> > > In a situation where there are hundreds or thousands of small SPV devices\n> > > in a single home (if 21, Inc. is successful) monitoring the blockchain,\n> > > this could result in lower traffic across the slow WAN connection.  And\n> > > yes, I realize it could potentially take a LOT of these devices before\n> > the\n> > > total bandwidth is greater than downloading a full copy of the\n> > blockchain,\n> > > but there's other reasons to host your own full node -- trust being one.\n> > >\n> > > --\n> > > *James G. Phillips IV*\n> > > <https://plus.google.com/u/0/113107039501292625391/posts>\n> > > <http://www.linkedin.com/in/ergophobe>\n> > >\n> > > *\"Don't bunt. Aim out of the ball park. Aim for the company of\n> > immortals.\"\n> > > -- David Ogilvy*\n> > >\n> > >  *This message was created with 100% recycled electrons. Please think\n> > twice\n> > > before printing.*\n> >"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-26T05:15:46",
                "message_text_only": "On Tue, May 26, 2015 at 12:52:07AM -0400, Matt Whitlock wrote:\n> On Monday, 25 May 2015, at 11:48 pm, Jim Phillips wrote:\n> > Do any wallets actually do this yet?\n> \n> Not that I know of, but they do seed their address database via DNS, which you can poison if you control the LAN's DNS resolver. I did this for a Bitcoin-only Wi-Fi network I operated at a remote festival. We had well over a hundred lightweight wallets, all trying to connect to the Bitcoin P2P network over a very bandwidth-constrained Internet link, so I poisoned the DNS and rejected all outbound connection attempts on port 8333, to force all the wallets to connect to a single local full node, which had connectivity to a single remote node over the Internet. Thus, all the lightweight wallets at the festival had Bitcoin network connectivity, but we only needed to backhaul the Bitcoin network's transaction traffic once.\n\nInteresting!\n\nWhat festival was this?\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000003ce9f2f90736ab7bd24d29f40346057f9e217b3753896bb\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/bf034861/attachment.sig>"
            },
            {
                "author": "Matt Whitlock",
                "date": "2015-05-26T05:47:37",
                "message_text_only": "On Tuesday, 26 May 2015, at 1:15 am, Peter Todd wrote:\n> On Tue, May 26, 2015 at 12:52:07AM -0400, Matt Whitlock wrote:\n> > On Monday, 25 May 2015, at 11:48 pm, Jim Phillips wrote:\n> > > Do any wallets actually do this yet?\n> > \n> > Not that I know of, but they do seed their address database via DNS, which you can poison if you control the LAN's DNS resolver. I did this for a Bitcoin-only Wi-Fi network I operated at a remote festival. We had well over a hundred lightweight wallets, all trying to connect to the Bitcoin P2P network over a very bandwidth-constrained Internet link, so I poisoned the DNS and rejected all outbound connection attempts on port 8333, to force all the wallets to connect to a single local full node, which had connectivity to a single remote node over the Internet. Thus, all the lightweight wallets at the festival had Bitcoin network connectivity, but we only needed to backhaul the Bitcoin network's transaction traffic once.\n> \n> Interesting!\n> \n> What festival was this?\n\nThe Porcupine Freedom Festival (\"PorcFest\") in New Hampshire last summer. I strongly suspect that it's the largest gathering of Bitcoin users at any event that is not specifically Bitcoin-themed. There's a lot of overlap between the Bitcoin and liberty communities. PorcFest draws somewhere around 1000-2000 attendees, a solid quarter of whom have Bitcoin wallets on their mobile devices.\n\nThe backhaul was a 3G cellular Internet connection, and the local Bitcoin node and network router were hosted on a Raspberry Pi with some Netfilter tricks to restrict connectivity. The net result was that all Bitcoin nodes (lightweight and heavyweight) on the local Wi-Fi network were unable to connect to any Bitcoin nodes except for the local node, which they discovered via DNS. I also had provisions in place to allow outbound connectivity to the API servers for Mycelium, Blockchain, and Coinbase wallets, by feeding the DNS resolver's results in real-time into a whitelisting Netfilter rule utilizing IP Sets.\n\nFor your amusement, here's the graphic for the banner that I had made to advertise the network at the festival (*chuckle*): http://www.mattwhitlock.com/bitcoin_wifi.png"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-26T10:48:46",
                "message_text_only": "Very interesting Matt.\n\nFor what it's worth, in future bitcoinj is very likely to bootstrap from\nCartographer nodes (signed HTTP) rather than DNS, and we're also steadily\nworking towards Tor by default. So this approach will probably stop working\nat some point. As breaking PorcFest would kind of suck, we might want a\nZeroConf/Rendezvous solution in place so local LANs can capture Bitcoin\ntraffic away from Tor (with some notification to the user, presumably).\n\n\n\nOn Tue, May 26, 2015 at 7:47 AM, Matt Whitlock <bip at mattwhitlock.name>\nwrote:\n\n> On Tuesday, 26 May 2015, at 1:15 am, Peter Todd wrote:\n> > On Tue, May 26, 2015 at 12:52:07AM -0400, Matt Whitlock wrote:\n> > > On Monday, 25 May 2015, at 11:48 pm, Jim Phillips wrote:\n> > > > Do any wallets actually do this yet?\n> > >\n> > > Not that I know of, but they do seed their address database via DNS,\n> which you can poison if you control the LAN's DNS resolver. I did this for\n> a Bitcoin-only Wi-Fi network I operated at a remote festival. We had well\n> over a hundred lightweight wallets, all trying to connect to the Bitcoin\n> P2P network over a very bandwidth-constrained Internet link, so I poisoned\n> the DNS and rejected all outbound connection attempts on port 8333, to\n> force all the wallets to connect to a single local full node, which had\n> connectivity to a single remote node over the Internet. Thus, all the\n> lightweight wallets at the festival had Bitcoin network connectivity, but\n> we only needed to backhaul the Bitcoin network's transaction traffic once.\n> >\n> > Interesting!\n> >\n> > What festival was this?\n>\n> The Porcupine Freedom Festival (\"PorcFest\") in New Hampshire last summer.\n> I strongly suspect that it's the largest gathering of Bitcoin users at any\n> event that is not specifically Bitcoin-themed. There's a lot of overlap\n> between the Bitcoin and liberty communities. PorcFest draws somewhere\n> around 1000-2000 attendees, a solid quarter of whom have Bitcoin wallets on\n> their mobile devices.\n>\n> The backhaul was a 3G cellular Internet connection, and the local Bitcoin\n> node and network router were hosted on a Raspberry Pi with some Netfilter\n> tricks to restrict connectivity. The net result was that all Bitcoin nodes\n> (lightweight and heavyweight) on the local Wi-Fi network were unable to\n> connect to any Bitcoin nodes except for the local node, which they\n> discovered via DNS. I also had provisions in place to allow outbound\n> connectivity to the API servers for Mycelium, Blockchain, and Coinbase\n> wallets, by feeding the DNS resolver's results in real-time into a\n> whitelisting Netfilter rule utilizing IP Sets.\n>\n> For your amusement, here's the graphic for the banner that I had made to\n> advertise the network at the festival (*chuckle*):\n> http://www.mattwhitlock.com/bitcoin_wifi.png\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/31cce561/attachment.html>"
            },
            {
                "author": "Louis Rossouw",
                "date": "2015-05-27T10:16:49",
                "message_text_only": "Also think it would be useful.\n\nCreated an issue for it some time back:\nhttps://github.com/bitcoin/bitcoin/issues/3802\nI think nodes don't \"only\" have to connect to LAN nodes. Especially with\nheaders first.\nThey can still connect to other nodes as well.  Having said that security\nis problematic in any case on a hotel wifi or similar.  All traffic can be\nspoofed.\nWith HF they'd be loading most of the data from the LAN node though.\nThis will help people having multiple nodes at home reduce bandwidth and\nimprove sync without difficult setup.\n\n\nOn Tue, 26 May 2015 at 12:50 Mike Hearn <mike at plan99.net> wrote:\n\n> Very interesting Matt.\n>\n> For what it's worth, in future bitcoinj is very likely to bootstrap from\n> Cartographer nodes (signed HTTP) rather than DNS, and we're also steadily\n> working towards Tor by default. So this approach will probably stop working\n> at some point. As breaking PorcFest would kind of suck, we might want a\n> ZeroConf/Rendezvous solution in place so local LANs can capture Bitcoin\n> traffic away from Tor (with some notification to the user, presumably).\n>\n>\n>\n> On Tue, May 26, 2015 at 7:47 AM, Matt Whitlock <bip at mattwhitlock.name>\n> wrote:\n>\n>> On Tuesday, 26 May 2015, at 1:15 am, Peter Todd wrote:\n>> > On Tue, May 26, 2015 at 12:52:07AM -0400, Matt Whitlock wrote:\n>> > > On Monday, 25 May 2015, at 11:48 pm, Jim Phillips wrote:\n>> > > > Do any wallets actually do this yet?\n>> > >\n>> > > Not that I know of, but they do seed their address database via DNS,\n>> which you can poison if you control the LAN's DNS resolver. I did this for\n>> a Bitcoin-only Wi-Fi network I operated at a remote festival. We had well\n>> over a hundred lightweight wallets, all trying to connect to the Bitcoin\n>> P2P network over a very bandwidth-constrained Internet link, so I poisoned\n>> the DNS and rejected all outbound connection attempts on port 8333, to\n>> force all the wallets to connect to a single local full node, which had\n>> connectivity to a single remote node over the Internet. Thus, all the\n>> lightweight wallets at the festival had Bitcoin network connectivity, but\n>> we only needed to backhaul the Bitcoin network's transaction traffic once.\n>> >\n>> > Interesting!\n>> >\n>> > What festival was this?\n>>\n>> The Porcupine Freedom Festival (\"PorcFest\") in New Hampshire last summer.\n>> I strongly suspect that it's the largest gathering of Bitcoin users at any\n>> event that is not specifically Bitcoin-themed. There's a lot of overlap\n>> between the Bitcoin and liberty communities. PorcFest draws somewhere\n>> around 1000-2000 attendees, a solid quarter of whom have Bitcoin wallets on\n>> their mobile devices.\n>>\n>> The backhaul was a 3G cellular Internet connection, and the local Bitcoin\n>> node and network router were hosted on a Raspberry Pi with some Netfilter\n>> tricks to restrict connectivity. The net result was that all Bitcoin nodes\n>> (lightweight and heavyweight) on the local Wi-Fi network were unable to\n>> connect to any Bitcoin nodes except for the local node, which they\n>> discovered via DNS. I also had provisions in place to allow outbound\n>> connectivity to the API servers for Mycelium, Blockchain, and Coinbase\n>> wallets, by feeding the DNS resolver's results in real-time into a\n>> whitelisting Netfilter rule utilizing IP Sets.\n>>\n>> For your amusement, here's the graphic for the banner that I had made to\n>> advertise the network at the festival (*chuckle*):\n>> http://www.mattwhitlock.com/bitcoin_wifi.png\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> One dashboard for servers and applications across Physical-Virtual-Cloud\n>> Widest out-of-the-box monitoring support with 50+ applications\n>> Performance metrics, stats and reports that give you Actionable Insights\n>> Deep dive visibility with transaction tracing using APM Insight.\n>> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/d9684800/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Zero-Conf for Full Node Discovery",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Kevin Greene",
                "Louis Rossouw",
                "Mike Hearn",
                "Peter Todd",
                "Jim Phillips",
                "Matt Whitlock",
                "Luke Dashjr"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 29392
        }
    },
    {
        "title": "[Bitcoin-development] First-Seen-Safe Replace-by-Fee",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2015-05-26T05:13:05",
                "message_text_only": "Summary\n-------\n\nFirst-seen-safe replace-by-fee (FSS RBF) does the following:\n\n1) Give users effective ways of getting \"stuck\" transactions unstuck.\n2) Use blockchain space efficiently.\n\nwithout:\n\n3) Changing the status quo with regard to zeroconf.\n\nThe current Bitcoin Core implementation has \"first-seen\" mempool\nbehavior. Once transaction t1 has been accepted, the transaction is\nnever removed from the mempool until mined, or double-spent by a\ntransaction in a block. The author's previously proposed replace-by-fee\nreplaced this behavior with simply accepting the transaction paying the\nhighest fee.\n\nFSS RBF is a compromise between these two behaviors. Transactions may be\nreplaced by higher-fee paying transactions, provided that all outputs in\nthe previous transaction are still paid by the replacement. While not as\ngeneral as standard RBF, and with higher costs than standard RBF, this\nstill allows fees on transaction to be increased after the fact with\nless cost and higher efficiency than child-pays-for-parent in many\ncommon situations; in some situations CPFP is unusable, leaving RBF as\nthe only option.\n\n\nSemantics\n---------\n\nFor reference, standard replace-by-fee has the following criteria for\ndetermining whether to replace a transaction.\n\n1) t2 pays > fees than t1\n\n2) The delta fees pay by t2, t2.fee - t1.fee, are >= the minimum fee\n   required to relay t2. (t2.size * min_fee_per_kb)\n\n3) t2 pays more fees/kb than t1\n\nFSS RBF adds the following additional criteria to replace-by-fee before\nallowing a transaction t1 to be replaced with t2:\n\n1) All outputs of t1 exist in t2 and pay >= the value in t1.\n\n2) All outputs of t1 are unspent.\n\n3) The order of outputs in t2 is the same as in t1 with additional new\n   outputs at the end of the output list.\n\n4) t2 only conflicts with a single transaction, t1\n\n5) t2 does not spend any outputs of t1 (which would make it an invalid\n   transaction, impossible to mine)\n\nThese additional criteria respect the existing \"first-seen\" behavior of\nthe Bitcoin Core mempool implementation, such that once an address is\npayed some amount of BTC, all subsequent replacement transactions will\npay an equal or greater amount. In short, FSS-RBF is \"zeroconf safe\" and\nhas no affect on the ability of attackers to doublespend. (beyond of\ncourse the fact that any changes what-so-ever to mempool behavior are\npotential zeroconf doublespend vulnerabilities)\n\n\nImplementation\n--------------\n\nPull-req for git HEAD: https://github.com/bitcoin/bitcoin/pull/6176\n\nA backport to v0.10.2 is pending.\n\nAn implementation of fee bumping respecting FSS rules is available at:\n\nhttps://github.com/petertodd/replace-by-fee-tools/blob/master/bump-fee.py\n\n\nUsage Scenarios\n---------------\n\nCase 1: Increasing the fee on a single tx\n-----------------------------------------\n\nWe start with a 1-in-2-out P2PKH using transaction t1, 226 bytes in size\nwith the minimal relay fee, 2.26uBTC. Increasing the fee while\nrespecting FSS-RBF rules requires the addition of one more txin, with\nthe change output value increased appropriately, resulting in\ntransaction t2, size 374 bytes. If the change txout is sufficient for\nthe fee increase, increasing the fee via CPFP requires a second\n1-in-1-out transaction, 192 bytes, for a total of 418 bytes; if another\ninput is required, CPFP requires a 2-in-1-out tx, 340 bytes, for a total\nof 566 bytes.\n\nBenefits: 11% to 34%+ cost savings, and RBF can increase fees even in\n          cases where the original transaction didn't have a change\n          output.\n\n\nCase 2: Paying multiple recipients in succession\n------------------------------------------------\n\nWe have a 1-in-2-out P2PKH transaction t1, 226 bytes, that pays Alice.\nWe now need to pay Bob. With plain RBF we'd just add a new outptu and\nreduce the value of the change address, a 90% savings. However with FSS\nRBF, decreasing the value is not allowed, so we have to add an input.\n\nIf the change of t1 is sufficient to pay Bob, a second 1-in-2-out tx can\nbe created, 2*226=452 bytes in total. With FSS RBF we can replace t1\nwith a 2-in-3-out tx paying both, increasing the value of the change\noutput appropriately, resulting in 408 bytes transaction saving 10%\n\nSimilar to the above example in the case where the change address of t1\nis insufficient to pay Bob the end result is one less transaction output\nin the wallet, defragmenting it. Spending these outputs later on would\nrequire two 148 byte inputs compared to one with RBF, resulting in an\noverall savings of 25%\n\n\nCase 3: Paying the same recipient multiple times\n------------------------------------------------\n\nFor example, consider the situation of an exchange, Acme Bitcoin Sales,\nthat keeps the majority of coins in cold storage. Acme wants to move\nfunds to cold storage at the lowest possible cost, taking advantage of\nperiods of higher capacity. (inevitable due to the poisson nature of\nblock creation) At the same time they would like to defragment their\nincoming outputs to keep redemption costs low, particularly since\nspending their high-security 3-of-7 P2SH multisigs is expensive. Acme\ncreates a low fee transaction with a single output to cold storage,\nperiodically adding new inputs as funds need to be moved to storage.\nEstimating the cost savings here is complex, and depends greatly on\ndetails of Acme's business, but regardless the approach works from a\ntechnical point of view. For instance if Acme's business is such that\nthe total hotwallet size needed heavily depends on external factors like\nvolatility, as hotwallet demand decreases throughout a day they can add\ninputs to the pending transaction. (simply asking customers to deposit\nfunds directly to the cold storage is also a useful strategy)\n\nHowever this is another case where standard RBF is significantly more\nuseful. For instance, as withdrawal requests come in the exchange can\nquickly replace their pending transactions sending funds to cold storage\nwith transactions sending those funds to customers instead, each time\navoiding multiple costly transactions. In particular, by reducing the\nneed to access cold storage at all, the security of the cold-stored\nfunds is increased.\n\n\nWallet Compatibility\n--------------------\n\nAll wallets should treat conflicting incoming transactions as equivalent\nso long as the transaction outputs owned by them do not change. In\naddition to compatibility with RBF-related practices, this prevents\nunnecessary user concern if transactions are mutated. Wallets must not\nassume TXIDs are fixed until confirmed in the blockchain; a fixed TXID\nis not guaranteed by the Bitcoin protocol.\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000c7ea0fcac58a9d7267fef8551b9d6a5206bf42b849618cb\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/e5647cbe/attachment.sig>"
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-26T17:54:05",
                "message_text_only": "I think this is a significant step forward.\n\nI suggest you also need to ensure that no inputs can be removed or \nchanged (other than scriptsigs) -- only added.  Otherwise, the semantics \nchange too much for the original signers.  Imagine a tx with two inputs \nfrom different parties.  Should it be easy for party 1 to be able to \neliminate party 2 as a contributor of funds?  It's not difficult to \nimagine real-world consequences to not having contributed to the \ntransaction.  And unless you can think of a reason, tx-level attributes \nlike nLocktime should not change either.\n\nThe result would be something very like CPFP, but with the new inputs \nand outputs merged into the original tx, keeping most of the overhead \nsavings you describe.\n\nIt should be submitted to bitcoin/bitcoin because like most inconsistent \nrelay policies, inconsistently deployed FSS RBF invites attacks (see \nhttps://gist.github.com/aalness/a78e3e35b90f52140f0d).\n\nGenerally, to be kind to zeroconf:\n\n  - Align relay and validation rules\n  - Keep first-seen\n  - Relay double-spends as alerts\n  - Allow nLocktime transactions into the mempool a bit before they \nbecome final\n  - ...\n\nIt's not unlike making a best-effort to reduce sources of malleability.  \nFSS RBF should be compatible with this if deployed consistently.\n\n\n\nOn 5/25/2015 10:13 PM, Peter Todd wrote:\n> Summary\n> -------\n>\n> First-seen-safe replace-by-fee (FSS RBF) does the following:\n>\n> 1) Give users effective ways of getting \"stuck\" transactions unstuck.\n> 2) Use blockchain space efficiently.\n>\n> without:\n>\n> 3) Changing the status quo with regard to zeroconf.\n>\n> The current Bitcoin Core implementation has \"first-seen\" mempool\n> behavior. Once transaction t1 has been accepted, the transaction is\n> never removed from the mempool until mined, or double-spent by a\n> transaction in a block. The author's previously proposed replace-by-fee\n> replaced this behavior with simply accepting the transaction paying the\n> highest fee.\n>\n> FSS RBF is a compromise between these two behaviors. Transactions may be\n> replaced by higher-fee paying transactions, provided that all outputs in\n> the previous transaction are still paid by the replacement. While not as\n> general as standard RBF, and with higher costs than standard RBF, this\n> still allows fees on transaction to be increased after the fact with\n> less cost and higher efficiency than child-pays-for-parent in many\n> common situations; in some situations CPFP is unusable, leaving RBF as\n> the only option.\n>\n>\n> Semantics\n> ---------\n>\n> For reference, standard replace-by-fee has the following criteria for\n> determining whether to replace a transaction.\n>\n> 1) t2 pays > fees than t1\n>\n> 2) The delta fees pay by t2, t2.fee - t1.fee, are >= the minimum fee\n>     required to relay t2. (t2.size * min_fee_per_kb)\n>\n> 3) t2 pays more fees/kb than t1\n>\n> FSS RBF adds the following additional criteria to replace-by-fee before\n> allowing a transaction t1 to be replaced with t2:\n>\n> 1) All outputs of t1 exist in t2 and pay >= the value in t1.\n>\n> 2) All outputs of t1 are unspent.\n>\n> 3) The order of outputs in t2 is the same as in t1 with additional new\n>     outputs at the end of the output list.\n>\n> 4) t2 only conflicts with a single transaction, t1\n>\n> 5) t2 does not spend any outputs of t1 (which would make it an invalid\n>     transaction, impossible to mine)\n>\n> These additional criteria respect the existing \"first-seen\" behavior of\n> the Bitcoin Core mempool implementation, such that once an address is\n> payed some amount of BTC, all subsequent replacement transactions will\n> pay an equal or greater amount. In short, FSS-RBF is \"zeroconf safe\" and\n> has no affect on the ability of attackers to doublespend. (beyond of\n> course the fact that any changes what-so-ever to mempool behavior are\n> potential zeroconf doublespend vulnerabilities)\n>\n>\n> Implementation\n> --------------\n>\n> Pull-req for git HEAD: https://github.com/bitcoin/bitcoin/pull/6176\n>\n> A backport to v0.10.2 is pending.\n>\n> An implementation of fee bumping respecting FSS rules is available at:\n>\n> https://github.com/petertodd/replace-by-fee-tools/blob/master/bump-fee.py\n>\n>\n> Usage Scenarios\n> ---------------\n>\n> Case 1: Increasing the fee on a single tx\n> -----------------------------------------\n>\n> We start with a 1-in-2-out P2PKH using transaction t1, 226 bytes in size\n> with the minimal relay fee, 2.26uBTC. Increasing the fee while\n> respecting FSS-RBF rules requires the addition of one more txin, with\n> the change output value increased appropriately, resulting in\n> transaction t2, size 374 bytes. If the change txout is sufficient for\n> the fee increase, increasing the fee via CPFP requires a second\n> 1-in-1-out transaction, 192 bytes, for a total of 418 bytes; if another\n> input is required, CPFP requires a 2-in-1-out tx, 340 bytes, for a total\n> of 566 bytes.\n>\n> Benefits: 11% to 34%+ cost savings, and RBF can increase fees even in\n>            cases where the original transaction didn't have a change\n>            output.\n>\n>\n> Case 2: Paying multiple recipients in succession\n> ------------------------------------------------\n>\n> We have a 1-in-2-out P2PKH transaction t1, 226 bytes, that pays Alice.\n> We now need to pay Bob. With plain RBF we'd just add a new outptu and\n> reduce the value of the change address, a 90% savings. However with FSS\n> RBF, decreasing the value is not allowed, so we have to add an input.\n>\n> If the change of t1 is sufficient to pay Bob, a second 1-in-2-out tx can\n> be created, 2*226=452 bytes in total. With FSS RBF we can replace t1\n> with a 2-in-3-out tx paying both, increasing the value of the change\n> output appropriately, resulting in 408 bytes transaction saving 10%\n>\n> Similar to the above example in the case where the change address of t1\n> is insufficient to pay Bob the end result is one less transaction output\n> in the wallet, defragmenting it. Spending these outputs later on would\n> require two 148 byte inputs compared to one with RBF, resulting in an\n> overall savings of 25%\n>\n>\n> Case 3: Paying the same recipient multiple times\n> ------------------------------------------------\n>\n> For example, consider the situation of an exchange, Acme Bitcoin Sales,\n> that keeps the majority of coins in cold storage. Acme wants to move\n> funds to cold storage at the lowest possible cost, taking advantage of\n> periods of higher capacity. (inevitable due to the poisson nature of\n> block creation) At the same time they would like to defragment their\n> incoming outputs to keep redemption costs low, particularly since\n> spending their high-security 3-of-7 P2SH multisigs is expensive. Acme\n> creates a low fee transaction with a single output to cold storage,\n> periodically adding new inputs as funds need to be moved to storage.\n> Estimating the cost savings here is complex, and depends greatly on\n> details of Acme's business, but regardless the approach works from a\n> technical point of view. For instance if Acme's business is such that\n> the total hotwallet size needed heavily depends on external factors like\n> volatility, as hotwallet demand decreases throughout a day they can add\n> inputs to the pending transaction. (simply asking customers to deposit\n> funds directly to the cold storage is also a useful strategy)\n>\n> However this is another case where standard RBF is significantly more\n> useful. For instance, as withdrawal requests come in the exchange can\n> quickly replace their pending transactions sending funds to cold storage\n> with transactions sending those funds to customers instead, each time\n> avoiding multiple costly transactions. In particular, by reducing the\n> need to access cold storage at all, the security of the cold-stored\n> funds is increased.\n>\n>\n> Wallet Compatibility\n> --------------------\n>\n> All wallets should treat conflicting incoming transactions as equivalent\n> so long as the transaction outputs owned by them do not change. In\n> addition to compatibility with RBF-related practices, this prevents\n> unnecessary user concern if transactions are mutated. Wallets must not\n> assume TXIDs are fixed until confirmed in the blockchain; a fixed TXID\n> is not guaranteed by the Bitcoin protocol.\n>\n>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-26T19:10:25",
                "message_text_only": "On Tue, May 26, 2015 at 5:54 PM, Tom Harding <tomh at thinlink.com> wrote:\n>  It's not difficult to\n> imagine real-world consequences to not having contributed to the\n> transaction.\n\nI'm having a hard time. Can you help me understand a specific case\nwhere this makes a difference.\n\nIt appears to be a gratuitous requirement;  if I have another unused\ninput that happens to be larger by the required fee-- why not just use\nit?\n\nThe inherent malleability of signatures makes it unreliable to depend\non the signature content of a transaction until its good and buried,\nregardless. And an inability to replace an input means you could not\nRBF for additional fees without taking change in more cases; there\nought to be a benefit to that."
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-26T23:00:01",
                "message_text_only": "On 5/26/2015 12:10 PM, Gregory Maxwell wrote:\n> On Tue, May 26, 2015 at 5:54 PM, Tom Harding <tomh at thinlink.com> wrote:\n>>   It's not difficult to\n>> imagine real-world consequences to not having contributed to the\n>> transaction.\n> I'm having a hard time. Can you help me understand a specific case\n> where this makes a difference.\n>\n\nThe bitcoin transaction is part of a real-world \"deal\" with unknown \nconnections to the other parts.  New inputs combined with new or \nincreased outputs can be thought of as a second deal sharing the same \nenvelope. That's not the case if paying parties are kicked out of the \ndeal, and possibly don't learn about it right away.\n\nA subset of parties to an Armory simulfunding transaction (an actual \nmulti-input use case) could replace one signer's input after they \nbroadcast it.  Whether that's a problem depends on real-world \nconnections.  Maybe the receiver cares where he is paid from or is \nbasing a subsequent decision on it.  Maybe a new output is being added, \nwhose presence makes the transaction less likely to be confirmed \nquickly, with that speed affecting the business.\n\nWith Kalle's Proof of Payment proposed standard, one payer in a \ntwo-input transaction could decide to boot the other, and claim the \nconcert tickets all for himself.  The fact that he pays is not the only \nconsideration in the real world -- what if these are the last 2 tickets?\n\nMempool policy shouldn't help one payer make a unilateral decision to \nbecome the sole party to a deal after various parties have seen it \nbroadcast.\n\n\n> The inherent malleability of signatures makes it unreliable to depend\n> on the signature content of a transaction until its good and buried,\n> regardless.\n\nI'd argue that changing how an input is signed doesn't change the deal.  \nFor example if a different 2 of 3 multisig participants sign, those 3 \npeople gave up that level of control when they created the multisig.\n\nReplacement is new - we have a choice what kind of warnings we need to \ngive to signers of multi-input transactions.  IMHO we should avoid \nneeding a stronger warning than is already needed for 0-conf."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-26T23:11:17",
                "message_text_only": "On Tue, May 26, 2015 at 11:00 PM, Tom Harding <tomh at thinlink.com> wrote:\n> The bitcoin transaction is part of a real-world \"deal\" with unknown\n> connections to the other parts\n\nI'm having a hard time parsing this.  You might as well say that its\npart of a weeblix for how informative it is, since you've not defined\nit.\n\n> not the case if paying parties are kicked out of the deal, and possibly\n> don't learn about it right away.\n\nThe signatures of a transaction can always be changed any any time,\nincluding by the miner, as they're not signed.\n\n> A subset of parties to an Armory simulfunding transaction (an actual\n> multi-input use case) could replace one signer's input after they broadcast\n> it.\n\nThey can already do this.\n\n> Maybe the\n> receiver cares where he is paid from or is basing a subsequent decision on\n> it.  Maybe a new output is being added, whose presence makes the transaction\n> less likely to be confirmed quickly, with that speed affecting the business.\n\nThe RBF behavior always moves in the direction of more prefered or\notherwise the node would not switch to the replacement. Petertodd\nshould perhaps make that more clear.\n\nBut your \"maybe\"s are what I was asking you to clarify. You said it\nwasn't hard to imagine; so I was asking for specific clarification.\n\n> With Kalle's Proof of Payment proposed standard, one payer in a two-input\n> transaction could decide to boot the other, and claim the concert tickets\n> all for himself.  The fact that he pays is not the only consideration in the\n> real world -- what if these are the last 2 tickets?\n\nThey can already do that.\n\n> I'd argue that changing how an input is signed doesn't change the deal.  For\n> example if a different 2 of 3 multisig participants sign, those 3 people\n> gave up that level of control when they created the multisig.\n\nThen why do you not argue that changing the input set does not change\nthe weeblix?\n\nWhy is one case of writing out a participant different that the other\ncase of writing out a participant?\n\n> Replacement is new - we have a choice what kind of warnings we need to give\n> to signers of multi-input transactions.  IMHO we should avoid needing a\n> stronger warning than is already needed for 0-conf.\n\nHow could a _stronger_ warning be required?"
            },
            {
                "author": "Tom Harding",
                "date": "2015-05-26T23:42:23",
                "message_text_only": "On 5/26/2015 4:11 PM, Gregory Maxwell wrote:\n> On Tue, May 26, 2015 at 11:00 PM, Tom Harding <tomh at thinlink.com> wrote:\n>> The bitcoin transaction is part of a real-world \"deal\" with unknown\n>> connections to the other parts\n> I'm having a hard time parsing this.  You might as well say that its\n> part of a weeblix for how informative it is, since you've not defined\n> it.\n\nFor example, you are paying for concert tickets.  The deal is concert \ntickets for bitcoin.  Or you're buying a company with 3 other investors.\n\n\n>> not the case if paying parties are kicked out of the deal, and possibly\n>> don't learn about it right away.\n> The signatures of a transaction can always be changed any any time,\n> including by the miner, as they're not signed.\n\nMiners can't update the signature on input #0 after removing input #1.\n\n\n>\n>> A subset of parties to an Armory simulfunding transaction (an actual\n>> multi-input use case) could replace one signer's input after they broadcast\n>> it.\n> They can already do this.\n\nReplacement is about how difficult it is to change the tx after it is \nbroadcast and seen by observers.\n\n\n>> Maybe the\n>> receiver cares where he is paid from or is basing a subsequent decision on\n>> it.  Maybe a new output is being added, whose presence makes the transaction\n>> less likely to be confirmed quickly, with that speed affecting the business.\n> The RBF behavior always moves in the direction of more prefered or\n> otherwise the node would not switch to the replacement. Petertodd\n> should perhaps make that more clear.\n>\n> But your \"maybe\"s are what I was asking you to clarify. You said it\n> wasn't hard to imagine; so I was asking for specific clarification.\n\nPick any one \"maybe\".  They're only maybes because it's not realistic \nfor them all to happen at once.\n\n\n>\n>> With Kalle's Proof of Payment proposed standard, one payer in a two-input\n>> transaction could decide to boot the other, and claim the concert tickets\n>> all for himself.  The fact that he pays is not the only consideration in the\n>> real world -- what if these are the last 2 tickets?\n> They can already do that.\n\nNot without replacement, after broadcast, unless they successfully pay \ntwice.\n\n\n>\n>> I'd argue that changing how an input is signed doesn't change the deal.  For\n>> example if a different 2 of 3 multisig participants sign, those 3 people\n>> gave up that level of control when they created the multisig.\n> Then why do you not argue that changing the input set does not change\n> the weeblix?\n>\n> Why is one case of writing out a participant different that the other\n> case of writing out a participant?\n\nIn the multisig input case, each signer already accepted the possibility \nof being written out.  Peter Todd's proposal is in the spirit of not \nwillfully making unconfirmed txes less reliable.  I'm suggesting that \nmulti-input signers should be included in the set of people for whom \nthey don't get less reliable.\n\n\n>\n>> Replacement is new - we have a choice what kind of warnings we need to give\n>> to signers of multi-input transactions.  IMHO we should avoid needing a\n>> stronger warning than is already needed for 0-conf.\n> How could a _stronger_ warning be required?\n\nWe'd have to warn signers to multi-input txes instead of just warning \nreceivers."
            },
            {
                "author": "Danny Thorpe",
                "date": "2015-05-26T21:20:35",
                "message_text_only": "Apologies if this has already been stated and I missed it, but:\n\nCan transactions in a buried block be overridden/replaced by RBF?\n\nOr is RBF strictly limited to transactions that have not yet been\nincorporated into a block?\n\nThanks,\n-Danny\n\nOn Mon, May 25, 2015 at 10:13 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> Summary\n> -------\n>\n> First-seen-safe replace-by-fee (FSS RBF) does the following:\n>\n> 1) Give users effective ways of getting \"stuck\" transactions unstuck.\n> 2) Use blockchain space efficiently.\n>\n> without:\n>\n> 3) Changing the status quo with regard to zeroconf.\n>\n> The current Bitcoin Core implementation has \"first-seen\" mempool\n> behavior. Once transaction t1 has been accepted, the transaction is\n> never removed from the mempool until mined, or double-spent by a\n> transaction in a block. The author's previously proposed replace-by-fee\n> replaced this behavior with simply accepting the transaction paying the\n> highest fee.\n>\n> FSS RBF is a compromise between these two behaviors. Transactions may be\n> replaced by higher-fee paying transactions, provided that all outputs in\n> the previous transaction are still paid by the replacement. While not as\n> general as standard RBF, and with higher costs than standard RBF, this\n> still allows fees on transaction to be increased after the fact with\n> less cost and higher efficiency than child-pays-for-parent in many\n> common situations; in some situations CPFP is unusable, leaving RBF as\n> the only option.\n>\n>\n> Semantics\n> ---------\n>\n> For reference, standard replace-by-fee has the following criteria for\n> determining whether to replace a transaction.\n>\n> 1) t2 pays > fees than t1\n>\n> 2) The delta fees pay by t2, t2.fee - t1.fee, are >= the minimum fee\n>    required to relay t2. (t2.size * min_fee_per_kb)\n>\n> 3) t2 pays more fees/kb than t1\n>\n> FSS RBF adds the following additional criteria to replace-by-fee before\n> allowing a transaction t1 to be replaced with t2:\n>\n> 1) All outputs of t1 exist in t2 and pay >= the value in t1.\n>\n> 2) All outputs of t1 are unspent.\n>\n> 3) The order of outputs in t2 is the same as in t1 with additional new\n>    outputs at the end of the output list.\n>\n> 4) t2 only conflicts with a single transaction, t1\n>\n> 5) t2 does not spend any outputs of t1 (which would make it an invalid\n>    transaction, impossible to mine)\n>\n> These additional criteria respect the existing \"first-seen\" behavior of\n> the Bitcoin Core mempool implementation, such that once an address is\n> payed some amount of BTC, all subsequent replacement transactions will\n> pay an equal or greater amount. In short, FSS-RBF is \"zeroconf safe\" and\n> has no affect on the ability of attackers to doublespend. (beyond of\n> course the fact that any changes what-so-ever to mempool behavior are\n> potential zeroconf doublespend vulnerabilities)\n>\n>\n> Implementation\n> --------------\n>\n> Pull-req for git HEAD: https://github.com/bitcoin/bitcoin/pull/6176\n>\n> A backport to v0.10.2 is pending.\n>\n> An implementation of fee bumping respecting FSS rules is available at:\n>\n> https://github.com/petertodd/replace-by-fee-tools/blob/master/bump-fee.py\n>\n>\n> Usage Scenarios\n> ---------------\n>\n> Case 1: Increasing the fee on a single tx\n> -----------------------------------------\n>\n> We start with a 1-in-2-out P2PKH using transaction t1, 226 bytes in size\n> with the minimal relay fee, 2.26uBTC. Increasing the fee while\n> respecting FSS-RBF rules requires the addition of one more txin, with\n> the change output value increased appropriately, resulting in\n> transaction t2, size 374 bytes. If the change txout is sufficient for\n> the fee increase, increasing the fee via CPFP requires a second\n> 1-in-1-out transaction, 192 bytes, for a total of 418 bytes; if another\n> input is required, CPFP requires a 2-in-1-out tx, 340 bytes, for a total\n> of 566 bytes.\n>\n> Benefits: 11% to 34%+ cost savings, and RBF can increase fees even in\n>           cases where the original transaction didn't have a change\n>           output.\n>\n>\n> Case 2: Paying multiple recipients in succession\n> ------------------------------------------------\n>\n> We have a 1-in-2-out P2PKH transaction t1, 226 bytes, that pays Alice.\n> We now need to pay Bob. With plain RBF we'd just add a new outptu and\n> reduce the value of the change address, a 90% savings. However with FSS\n> RBF, decreasing the value is not allowed, so we have to add an input.\n>\n> If the change of t1 is sufficient to pay Bob, a second 1-in-2-out tx can\n> be created, 2*226=452 bytes in total. With FSS RBF we can replace t1\n> with a 2-in-3-out tx paying both, increasing the value of the change\n> output appropriately, resulting in 408 bytes transaction saving 10%\n>\n> Similar to the above example in the case where the change address of t1\n> is insufficient to pay Bob the end result is one less transaction output\n> in the wallet, defragmenting it. Spending these outputs later on would\n> require two 148 byte inputs compared to one with RBF, resulting in an\n> overall savings of 25%\n>\n>\n> Case 3: Paying the same recipient multiple times\n> ------------------------------------------------\n>\n> For example, consider the situation of an exchange, Acme Bitcoin Sales,\n> that keeps the majority of coins in cold storage. Acme wants to move\n> funds to cold storage at the lowest possible cost, taking advantage of\n> periods of higher capacity. (inevitable due to the poisson nature of\n> block creation) At the same time they would like to defragment their\n> incoming outputs to keep redemption costs low, particularly since\n> spending their high-security 3-of-7 P2SH multisigs is expensive. Acme\n> creates a low fee transaction with a single output to cold storage,\n> periodically adding new inputs as funds need to be moved to storage.\n> Estimating the cost savings here is complex, and depends greatly on\n> details of Acme's business, but regardless the approach works from a\n> technical point of view. For instance if Acme's business is such that\n> the total hotwallet size needed heavily depends on external factors like\n> volatility, as hotwallet demand decreases throughout a day they can add\n> inputs to the pending transaction. (simply asking customers to deposit\n> funds directly to the cold storage is also a useful strategy)\n>\n> However this is another case where standard RBF is significantly more\n> useful. For instance, as withdrawal requests come in the exchange can\n> quickly replace their pending transactions sending funds to cold storage\n> with transactions sending those funds to customers instead, each time\n> avoiding multiple costly transactions. In particular, by reducing the\n> need to access cold storage at all, the security of the cold-stored\n> funds is increased.\n>\n>\n> Wallet Compatibility\n> --------------------\n>\n> All wallets should treat conflicting incoming transactions as equivalent\n> so long as the transaction outputs owned by them do not change. In\n> addition to compatibility with RBF-related practices, this prevents\n> unnecessary user concern if transactions are mutated. Wallets must not\n> assume TXIDs are fixed until confirmed in the blockchain; a fixed TXID\n> is not guaranteed by the Bitcoin protocol.\n>\n> --\n> 'peter'[:-1]@petertodd.org\n> 00000000000000000c7ea0fcac58a9d7267fef8551b9d6a5206bf42b849618cb\n>\n>\n> ------------------------------------------------------------------------------\n> One dashboard for servers and applications across Physical-Virtual-Cloud\n> Widest out-of-the-box monitoring support with 50+ applications\n> Performance metrics, stats and reports that give you Actionable Insights\n> Deep dive visibility with transaction tracing using APM Insight.\n> http://ad.doubleclick.net/ddm/clk/290420510;117567292;y\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/833cbbca/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2015-05-26T21:27:04",
                "message_text_only": "It's just a mempool policy rule.\n\nAllowing the contents of blocks to change (other than by mining a competing\nchain) would be pretty much the largest possible change to Bitcoin's\ndesign....\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/f6aef9dd/attachment.html>"
            },
            {
                "author": "Danny Thorpe",
                "date": "2015-05-26T22:09:35",
                "message_text_only": "Thanks for the clarification.\n\nSo, since RBF applies only to pending transactions in the mempool awaiting\nincorporation into a block, there is a window of opportunity in which the\npending tx is incorporated into a block at the same time that the spender\nis constructing and publishing a replacement for that pending tx.\n\nThe replacement transaction would be rejected by the peer network as a\ndouble spend because it conflicts with the now confirmed original tx, and\nthe spender will have to go back and create a new stand-alone transaction\nto accomplish what they hoped to do with an RBF replacement.\n\nSo an implementation that wishes to take advantage of RBF will still need\nto have a \"plan B\" implementation path to handle the corner case of a\nreplacement tx being rejected as a double spend.\n\nIs this correct?\n\nI'm just trying to get my head around the implementation cost vs benefit of\nRBF in the context of my applications.\n\nThanks,\n-Danny\n\nOn Tue, May 26, 2015 at 2:27 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> It's just a mempool policy rule.\n>\n> Allowing the contents of blocks to change (other than by mining a\n> competing chain) would be pretty much the largest possible change to\n> Bitcoin's design....\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/ac340045/attachment.html>"
            },
            {
                "author": "Adam Back",
                "date": "2015-05-26T22:18:42",
                "message_text_only": "I think the point is the replacement tx spends the same inputs (plus\nadditional inputs), so if the original tx is accepted, and your\nreplacement rejected, thats good news - you dont have to pay the\nhigher fee, the extra input remains unspent (and can be used later for\nother purpose) and the extra change address is unused.\n\n(If you had bundled extra transactions into the replacement, spending\nfrom the additional inputs, then you'll need to resubmit those as a\nseparate transaction).\n\n(You have to keep in mind re-orgs so for example the original tx could\nbe put into a block, and then that block could get reorged by another\nblock that grows into a longer chain with the replacement tx in it (or\nvice versa)).\n\nAdam\n\nOn 26 May 2015 at 23:09, Danny Thorpe <danny.thorpe at gmail.com> wrote:\n> Thanks for the clarification.\n>\n> So, since RBF applies only to pending transactions in the mempool awaiting\n> incorporation into a block, there is a window of opportunity in which the\n> pending tx is incorporated into a block at the same time that the spender is\n> constructing and publishing a replacement for that pending tx.\n>\n> The replacement transaction would be rejected by the peer network as a\n> double spend because it conflicts with the now confirmed original tx, and\n> the spender will have to go back and create a new stand-alone transaction to\n> accomplish what they hoped to do with an RBF replacement.\n>\n> So an implementation that wishes to take advantage of RBF will still need to\n> have a \"plan B\" implementation path to handle the corner case of a\n> replacement tx being rejected as a double spend.\n>\n> Is this correct?\n>\n> I'm just trying to get my head around the implementation cost vs benefit of\n> RBF in the context of my applications.\n>\n> Thanks,\n> -Danny\n>\n> On Tue, May 26, 2015 at 2:27 PM, Pieter Wuille <pieter.wuille at gmail.com>\n> wrote:\n>>\n>> It's just a mempool policy rule.\n>>\n>> Allowing the contents of blocks to change (other than by mining a\n>> competing chain) would be pretty much the largest possible change to\n>> Bitcoin's design....\n>\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-27T07:30:32",
                "message_text_only": "On Tue, May 26, 2015 at 01:13:05AM -0400, Peter Todd wrote:\n> Case 1: Increasing the fee on a single tx\n> -----------------------------------------\n> \n> We start with a 1-in-2-out P2PKH using transaction t1, 226 bytes in size\n> with the minimal relay fee, 2.26uBTC. Increasing the fee while\n> respecting FSS-RBF rules requires the addition of one more txin, with\n> the change output value increased appropriately, resulting in\n> transaction t2, size 374 bytes. If the change txout is sufficient for\n> the fee increase, increasing the fee via CPFP requires a second\n> 1-in-1-out transaction, 192 bytes, for a total of 418 bytes; if another\n> input is required, CPFP requires a 2-in-1-out tx, 340 bytes, for a total\n> of 566 bytes.\n> \n> Benefits: 11% to 34%+ cost savings, and RBF can increase fees even in\n>           cases where the original transaction didn't have a change\n>           output.\n\nTo clarify a point raised(1) on the pull-req itself:\n\nThe replacement transaction is allowed to not only add new txin's, but\nalso replace txins. Suppose t1 is a 2-in-2-out P2PKH using transaction,\n374 bytes in size. With CPFP accomplished by a 1-in-1-out tx, 192 bytes,\nyou have 566 bytes total. With FSS RBF if you have an unspent output\ngreater in value than one of the outputs spent by t1, you can replace\nthat output in t1's vin txin set and rebroadcast the transaction, still\n374 bytes in size. This gives you a 34% cost savings vs. CPFP.\n\n> Case 2: Paying multiple recipients in succession\n> ------------------------------------------------\n> \n> We have a 1-in-2-out P2PKH transaction t1, 226 bytes, that pays Alice.\n> We now need to pay Bob. With plain RBF we'd just add a new outptu and\n> reduce the value of the change address, a 90% savings. However with FSS\n> RBF, decreasing the value is not allowed, so we have to add an input.\n> \n> If the change of t1 is sufficient to pay Bob, a second 1-in-2-out tx can\n> be created, 2*226=452 bytes in total. With FSS RBF we can replace t1\n> with a 2-in-3-out tx paying both, increasing the value of the change\n> output appropriately, resulting in 408 bytes transaction saving 10%\n> \n> Similar to the above example in the case where the change address of t1\n> is insufficient to pay Bob the end result is one less transaction output\n> in the wallet, defragmenting it. Spending these outputs later on would\n> require two 148 byte inputs compared to one with RBF, resulting in an\n> overall savings of 25%\n\nSimilarly in the multiple recipients case, if sufficiently large\noutputs are available the additional funds can be obtained by swapping\none input for another.\n\nFor instance if Alice has three outputs, 1.0, 0.5, and 0.2 BTC, and\nneeds to pay Bob 1.1 BTC, she can create t1:\n\n    1.0 -> Bob   1.1\n    0.2 -> Alice 0.1\n\nIf she then needs to pay Charlie 0.2 BTC she can doublespend that with:\n\n    1.0 -> Bob     1.1\n    0.5 -> Charlie 0.2\n        -> Alice   0.2\n\nNote that care does need to be taken to ensure that multiple rounds of\nthis always leave at least one input unchanged.\n\n\n1) https://github.com/bitcoin/bitcoin/pull/6176#issuecomment-105630255\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000ec0c3a90baa52289171046469fe4a21dc5a0dac4cb758a9\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/c079858a/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "First-Seen-Safe Replace-by-Fee",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Adam Back",
                "Peter Todd",
                "Gregory Maxwell",
                "Danny Thorpe",
                "Pieter Wuille",
                "Tom Harding"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 39308
        }
    },
    {
        "title": "[Bitcoin-development] Bitcoin Survey Paper",
        "thread_messages": [
            {
                "author": "Florian Tschorsch",
                "date": "2015-05-26T13:54:03",
                "message_text_only": "Hi all,\n\nsome time ago, we became interested in Bitcoin, but gathering relevant\nwork and getting an overview was kind of painful. We took it as a sign\nthat a survey paper on Bitcoin is desperately needed.\n\nSince then we observed the activities of the Bitcoin community. Recently\nwe finished a technical report that is our attempt to contribute the\nlacking Bitcoin survey. Maybe it will be of interest to some of you:\nhttps://eprint.iacr.org/2015/464\n\nFeedback is highly appreciated and would help to improve the quality of\nfuture revisions.\n\nCheers, Florian."
            },
            {
                "author": "Daniel Kraft",
                "date": "2015-05-26T14:11:14",
                "message_text_only": "Hi Florian!\n\nOn 2015-05-26 15:54, Florian Tschorsch wrote:\n> some time ago, we became interested in Bitcoin, but gathering relevant\n> work and getting an overview was kind of painful. We took it as a sign\n> that a survey paper on Bitcoin is desperately needed.\n> \n> Since then we observed the activities of the Bitcoin community. Recently\n> we finished a technical report that is our attempt to contribute the\n> lacking Bitcoin survey. Maybe it will be of interest to some of you:\n> https://eprint.iacr.org/2015/464\n\nThanks for the work you (and your collegue) put into this paper!  It is\ndefinitely a good step in the right direction!\n\n> Feedback is highly appreciated and would help to improve the quality of\n> future revisions.\n\nDo you know [1]?  I've only glanced at both papers, but it seems that's\nalso a research paper about Bitcoin & co.\n\n  [1] http://www.jbonneau.com/doc/BMCNKF15-IEEESP-bitcoin.pdf\n\nApart from that, let me advertise myself a little bit. ;)  In case you\nare interested in the difficulty mechanism (which you only mention\nbriefly), I've recently written about it [2] (official publication) [3]\n(preprint without paywall).\n\n  [2] http://link.springer.com/article/10.1007/s12083-015-0347-x\n  [3] http://www.domob.eu/research/DifficultyControl.pdf\n\nGood luck with your further research!\n\nYours,\nDaniel\n\n-- \nhttp://www.domob.eu/\nOpenPGP: 1142 850E 6DFF 65BA 63D6  88A8 B249 2AC4 A733 0737\nNamecoin: id/domob -> https://nameid.org/?name=domob\n--\nDone:  Arc-Bar-Cav-Hea-Kni-Ran-Rog-Sam-Tou-Val-Wiz\nTo go: Mon-Pri\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 836 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/16225ec7/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Survey Paper",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Florian Tschorsch",
                "Daniel Kraft"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2376
        }
    },
    {
        "title": "[Bitcoin-development] please remove me from the list",
        "thread_messages": [
            {
                "author": "Da Xu",
                "date": "2015-05-26T18:39:36",
                "message_text_only": "Thanks.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/87d21b75/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "please remove me from the list",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Da Xu"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 190
        }
    },
    {
        "title": "[Bitcoin-development] Version bits proposal",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2015-05-27T01:48:05",
                "message_text_only": "Hello everyone,\n\nhere is a proposal for how to coordinate future soft-forking consensus\nchanges: https://gist.github.com/sipa/bf69659f43e763540550\n\nIt supports multiple parallel changes, as well as changes that get\npermanently rejected without obstructing the rollout of others.\n\nFeel free to comment. As the gist does not support notifying participants\nof new comments, I would suggest using the mailing list instead.\n\nThis is joint work with Peter Todd and Greg Maxwell.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/87f14925/attachment.html>"
            },
            {
                "author": "Douglas Roark",
                "date": "2015-05-27T02:31:21",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nOn 2015/5/26 21:48, Pieter Wuille wrote:\n> here is a proposal for how to coordinate future soft-forking\n> consensus changes:\n> https://gist.github.com/sipa/bf69659f43e763540550\n> \n> It supports multiple parallel changes, as well as changes that get \n> permanently rejected without obstructing the rollout of others.\n> \n> Feel free to comment. As the gist does not support notifying \n> participants of new comments, I would suggest using the mailing\n> list instead.\n\nHi Pieter. Thanks for posting the proposal. I think the concept itself\nis pretty solid. I know some people have been proposing alternate\nmethods too. I hope they'll share here, assuming they haven't already.\nAs is, my comments concern typos and general copy editing.\n\n- - Just speaking in general, I found the BIP to be a bit hard to read.\nAFAIK, the basic facts are accurate. I just found myself having to\nre-read certain passages two or three times. A little polish wouldn't\nhurt. For example, using the \"it\" pronoun can be confusing, such as\nmultiple uses in the abstract. Specifying what \"it\" is (e.g., \"The\nproposed change relies on...\") would really help. In addition, the way\nthe \"W\" value is handled seems like it could be improved a bit. I know\nthe wording is accurate. Seeing 1000 change to 1001 is still a little\nweird.\n- - In \"Multi-stage soft forks,\" I presume the second sentence should\nend as follows: \"[...] with additional validation rules that get\nenabled one by _one_.\" Depending on semantics, I'd consider changing\n\"one by one\" to \"incremental steps,\" but that's your call.\n- - I found the \"High bits\" section to be confusing at first. It looks\nlike you chose to show everything as little endian data, matching\nwhat's actually in the block. My personal preference would be to\nrepresent the data, for readability purposes, as big endian. I doubt\nI'm the only one who finds big endian to be much easier to process\nmentally.\n- - Some sort of legend showing A, I, W, etc. would really help, as\nopposed to just running into them as one goes along. Otherwise, the\nalphabet soup can be a bit confusing.\n\nThanks again.\n\n- -- \n- ---\nDouglas Roark\nSenior Developer\nArmory Technologies, Inc.\ndoug at bitcoinarmory.com\nPGP key ID: 92ADC0D7\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG/MacGPG2 v2.0.22 (Darwin)\nComment: GPGTools - https://gpgtools.org\n\niQIcBAEBCgAGBQJVZSx5AAoJEGybVGGSrcDXrYEQAOIrsggoZv0LdJHZjPGpEkeb\n7ULhO4krZtQmKXjWDP0KnHAsFiyo5EOh1fYFRZz11OCqO4QmteTLPbodZFz47tKp\ntIYv5uc0qYhjfo5uLkzxuUky08VE4dUoELfqdbNciC45xHras7Wh/+KXc1a20Fib\nTaisWx9aL6VfPf7urM8b6mQ9XMba4YB3e2syAY8AA+qAEEP4DK2V6tuOQJD3kxP2\ntbHtJnDvkDoXEY6tnL7fePo9X/IrlXLi8vNWGqPIf/hoiHmdvU+ORwHta7z9YeIO\nzi4LRs8n8sYmifY4nt6Wkkc1aoPsmpoXmI3tKgFM2h5bfdg0n3fN3K0nTMWtnR6z\nHUq8JhrQkZUP8uunN/23bt94FZolvnHTdL9YuWoyrlJ0gQri5YxV1BAN4hM9oCZy\n1SqlSmFRplIFWu45q8/I5duDSphmA4NP2qc59QRjftcGYpNxmzaeSViiCDWzAjI9\nqTLZgLTa/nf3TFN8oU8RwquGpwD82/fFo9V+uKdNGj79kdV8WOv4sa9q63OTVimJ\nw+r4l1gDZYyToe0heKtV2kL9Tt4HTn23bj7EvU+98uaKEpfWSP8a3BN9mPR7ork/\nlNRGEGQ0tvkeDUzKy9IHuAjXo2XkKctbBRJwZJCGc5WW2sN0HdSu/GFPXrOOLf0J\nJXqeKpfaS0UriFXkxVHO\n=8uNL\n-----END PGP SIGNATURE-----"
            },
            {
                "author": "Luke Dashjr",
                "date": "2015-05-27T03:46:15",
                "message_text_only": "On Wednesday, May 27, 2015 1:48:05 AM Pieter Wuille wrote:\n> Feel free to comment. As the gist does not support notifying participants\n> of new comments, I would suggest using the mailing list instead.\n\nI suggest adding a section describing how this interacts with and changes GBT.\n\nCurrently, the client tells the server what the highest block version it \nsupports is, and the server indicates a block version to use in its template, \nas well as optional instructions for the client to forcefully use this version \ndespite its own maximum version number. Making the version a bitfield \ncontradicts the increment-only assumption of this design, and since GBT \nclients are not aware of overall network consensus state, reused bits can \neasily become confused. I suggest, therefore, that GBT clients should indicate \n(instead of a maximum supported version number) a list of softforks by \nidentifier keyword, and the GBT server respond with a template indicating:\n- An object of softfork keywords to bit values, that the server will accept.\n- The version number, as presently conveyed, indicating the preferred softfork \nflags.\n\nDoes this sound reasonable, and/or am I missing anything else?\n\nLuke"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-27T03:51:00",
                "message_text_only": "It would also help to see the actual code changes required, which I'm sure\nwill be much shorter than the explanation itself.\nOn May 27, 2015 5:47 AM, \"Luke Dashjr\" <luke at dashjr.org> wrote:\n\n> On Wednesday, May 27, 2015 1:48:05 AM Pieter Wuille wrote:\n> > Feel free to comment. As the gist does not support notifying participants\n> > of new comments, I would suggest using the mailing list instead.\n>\n> I suggest adding a section describing how this interacts with and changes\n> GBT.\n>\n> Currently, the client tells the server what the highest block version it\n> supports is, and the server indicates a block version to use in its\n> template,\n> as well as optional instructions for the client to forcefully use this\n> version\n> despite its own maximum version number. Making the version a bitfield\n> contradicts the increment-only assumption of this design, and since GBT\n> clients are not aware of overall network consensus state, reused bits can\n> easily become confused. I suggest, therefore, that GBT clients should\n> indicate\n> (instead of a maximum supported version number) a list of softforks by\n> identifier keyword, and the GBT server respond with a template indicating:\n> - An object of softfork keywords to bit values, that the server will\n> accept.\n> - The version number, as presently conveyed, indicating the preferred\n> softfork\n> flags.\n>\n> Does this sound reasonable, and/or am I missing anything else?\n>\n> Luke\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/52292ec6/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-27T09:35:03",
                "message_text_only": "I think it would be better to have the deadlines set as block counts.  That\neliminates the need to use the median time mechanism.\n\nThe deadline could be matched to a \"start-line\".  The definition would then\nbe something like\n\nBIP 105\nStart block: 325000\nEnd block: 350000\nActivation: 750 of 1000\nImplication: 950 of 1000\nBit: 9\n\nThis would allow creation of a simple table of known BIPs.  It also keeps\nmultiple users of the bit as strictly separate.\n\nThe alternative to the start time is that it is set equal to the deadline\nor implication time of the previous user of the bit.\n\nWas the intention to change the 95% rule.  You need 750 of the last 1000 to\nactivate and then must wait at least 1000 for implication?\n\n\nOn Wed, May 27, 2015 at 4:51 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> It would also help to see the actual code changes required, which I'm sure\n> will be much shorter than the explanation itself.\n> On May 27, 2015 5:47 AM, \"Luke Dashjr\" <luke at dashjr.org> wrote:\n>\n>> On Wednesday, May 27, 2015 1:48:05 AM Pieter Wuille wrote:\n>> > Feel free to comment. As the gist does not support notifying\n>> participants\n>> > of new comments, I would suggest using the mailing list instead.\n>>\n>> I suggest adding a section describing how this interacts with and changes\n>> GBT.\n>>\n>> Currently, the client tells the server what the highest block version it\n>> supports is, and the server indicates a block version to use in its\n>> template,\n>> as well as optional instructions for the client to forcefully use this\n>> version\n>> despite its own maximum version number. Making the version a bitfield\n>> contradicts the increment-only assumption of this design, and since GBT\n>> clients are not aware of overall network consensus state, reused bits can\n>> easily become confused. I suggest, therefore, that GBT clients should\n>> indicate\n>> (instead of a maximum supported version number) a list of softforks by\n>> identifier keyword, and the GBT server respond with a template indicating:\n>> - An object of softfork keywords to bit values, that the server will\n>> accept.\n>> - The version number, as presently conveyed, indicating the preferred\n>> softfork\n>> flags.\n>>\n>> Does this sound reasonable, and/or am I missing anything else?\n>>\n>> Luke\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/cd5723c3/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-27T10:15:16",
                "message_text_only": "On Wed, May 27, 2015 at 10:35:03AM +0100, Tier Nolan wrote:\n> I think it would be better to have the deadlines set as block counts.  That\n> eliminates the need to use the median time mechanism.\n\nThe median time mechanism is basically a way for hashing power to show\nwhat time they think it is. Equally, the nVersion soft-fork mechanism is\na way for hashing power to show what features they want to support.\n\nBlock counts are inconvenient for planning, as there's no guarantee\nthey'll actually happen in any particular time frame, forward and back.\nThere's no particular incentive problems here - the median time clearly\nshows support by a majority of hashing power - so I don't see any reason\nto make planning more difficult.\n\n> The deadline could be matched to a \"start-line\".  The definition would then\n> be something like\n> \n> BIP 105\n> Start block: 325000\n> End block: 350000\n> Activation: 750 of 1000\n> Implication: 950 of 1000\n> Bit: 9\n> \n> This would allow creation of a simple table of known BIPs.  It also keeps\n> multiple users of the bit as strictly separate.\n\nIf you assume no large reorganizations, your table of known BIPs can\njust as easily be a list of block heights even if the median time\nmechanism is used.\n\nIf you do assume there may be large reorganizations you can't have a\n\"simple table\"\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000001643f7706f3dcbc3a386e4c1bfba852ff628d8024f875b6\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/2107dc0e/attachment.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-27T11:26:33",
                "message_text_only": "On Wed, May 27, 2015 at 11:15 AM, Peter Todd <pete at petertodd.org> wrote:\n\n> The median time mechanism is basically a way for hashing power to show\n> what time they think it is. Equally, the nVersion soft-fork mechanism is\n> a way for hashing power to show what features they want to support.\n>\n>\nFair enough.  It means slightly more processing, but the median time could\nbe cached in the header index, so no big deal.\n\nBlock counts are inconvenient for planning, as there's no guarantee\n> they'll actually happen in any particular time frame, forward and back.\n>\n\nI don't think the deadline needs to be set that accurately.  A roughly 6\nmonth deadline should be fine, but as you say a majority of miners is\nneeded to abuse the median time and it is already a miner poll.\n\nPerhaps the number of blocks used in the median could be increased to\nreduce \"noise\".\n\nThe median time could be median of the last 144 blocks plus 12 hours.\n\n\n> If you assume no large reorganizations, your table of known BIPs can\n> just as easily be a list of block heights even if the median time\n> mechanism is used.\n>\n\nI think it makes it easier to write the code.  It reduced the state that\nneeds to be stored per BIP.  You don't need to check if the previous bips\nwere all accepted.\n\nEach bit is assigned to a particular BIP for a particular range of times\n(or blocks).\n\nIf block numbers were used for the deadline, you just need to check the\nblock index for the deadline block.\n\nenum {\n    BIP_INACTIVE = 0,\n    BIP_ACTIVE,\n    BIP_LOCKED\n    BIP_INVALID_BLOCK,\n}\n\nint GetBIPState(block, bip)\n{\n    if (block.height == bip.deadline)  // Bit must be set to match\nlocked/unlocked at deadline\n    {\n        int bipState = check_supermajority(...);\n        if (bipState == BIP_LOCKED && (block.nVersion & bip.bit)\n            return BIP_LOCKED;\n\n        if (bipState != BIP_LOCKED && (block.nVersion & (~bip.bit)))\n            return BIP_INACTIVE;\n\n        return BIP_INVALID_BLOCK;\n    }\n\n    if (block.height > deadline) // Look at the deadline block to determine\nif the BIP is locked\n        return (block_index[deadline].nVersion & bip_bit) != 0 ? BIP_LOCKED\n: BIP_INACTIVE;\n\n    if (block.height < startline + I) // BIP cannot activate/lock until\nstartline + implicit window size\n        return INACTIVE;\n\n    return check_supermajority(....) // Check supermajority of bit\n}\n\nThe block at height deadline would indicate if the BIP was locked in.\n\nBlock time could still be used as long as the block height was set after\nthat.  The deadline_time could be in six months.  The startline height\ncould be the current block height and the deadline_height could be\nstartline + 35000.\n\nThe gives roughly\n\nstart time = now\ndeadline time = now + six months\ndeadline height = now + eight months\n\nThe deadline height is the block height when the bit is returned to the\npool but the deadline time is when the BIP has to be accepted.\n\nIt also helps with the warning system.  For each block height, there is a\nset of known BIP bits that are allowed.  Once the final deadline is passed,\nthe expected mask is zeros.\n\nOn Wed, May 27, 2015 at 11:15 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> On May 27, 2015 11:35 AM, \"Tier Nolan\" <tier.nolan at gmail.com> wrote:\n>\n> > Was the intention to change the 95% rule.  You need 750 of the last 1000\n> to activate and then must wait at least 1000 for implication?\n>\n> You need 75% to start applying it, 95% to start rejecting blocks that\n> don't apply it.\n>\n\nI think the phrasing is ambiguous.  I was just asking for clarification.\n\n\"Whenever I out of any W *subsequent* blocks (regardless of the block\nitself) have bit B set,\"\n\nThat suggests that the I of W blocks for the 95% rule must happen after\nactivation.  This makes the rule checking harder.  Easier to use the\ncurrent system, where blocks that were part of the 750 rule also count\ntowards the 95% rule.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/90a022e8/attachment.html>"
            },
            {
                "author": "Sergio Lerner",
                "date": "2015-05-27T22:52:18",
                "message_text_only": "I like the idea but I think we should leave at least 16 bits of the\nversion fixed as an extra-nonce.\nIf we don't then miners may use them as a nonce anyway, and mess with\nthe soft-fork voting system.\nMy original proposal was this: https://github.com/bitcoin/bitcoin/pull/5102\n\nBest regards"
            },
            {
                "author": "Patrick Strateman",
                "date": "2015-05-28T01:05:08",
                "message_text_only": "There is absolutely no reason to do this.\n\nAny reasonable micro-controller can build merkle tree roots\nsignificantly faster than is necessary.\n\n1 Th/s walks the nonce range once every 4.3ms.\n\nThe largest valid merkle trees are 14 nodes high.\n\nThat translates to 28 SHA256 ops per 4.3ms or 6511 SHA256 ops/second.\n\nFor reference an RPi 1 model B does 2451050 SHA256 ops/second.\n\nOn 05/27/2015 03:52 PM, Sergio Lerner wrote:\n> I like the idea but I think we should leave at least 16 bits of the\n> version fixed as an extra-nonce.\n> If we don't then miners may use them as a nonce anyway, and mess with\n> the soft-fork voting system.\n> My original proposal was this: https://github.com/bitcoin/bitcoin/pull/5102\n>\n> Best regards\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Christian Decker",
                "date": "2015-05-28T07:51:39",
                "message_text_only": "Agreed, there is no need to misuse the version field as well. There is more\nthan enough variability you could roll in the merkle tree including and\nexcluding transactions, and the scriptSig of the coinbase transaction,\nwhich also influences the merkle root.\n\nI have a fundamental dislike of retroactively changing semantics, and the\nversion field should be used just for that: a version. I don't even\nparticularly like flagging support for a fork in the version field, but\nsince I have no better solution, count me as supporting Sipa's proposal. We\ndefinitely need a more comfortable way of rolling out new features.\n\nRegards,\nChris\n\nOn Thu, May 28, 2015 at 3:08 AM Patrick Strateman <\npatrick.strateman at gmail.com> wrote:\n\n> There is absolutely no reason to do this.\n>\n> Any reasonable micro-controller can build merkle tree roots\n> significantly faster than is necessary.\n>\n> 1 Th/s walks the nonce range once every 4.3ms.\n>\n> The largest valid merkle trees are 14 nodes high.\n>\n> That translates to 28 SHA256 ops per 4.3ms or 6511 SHA256 ops/second.\n>\n> For reference an RPi 1 model B does 2451050 SHA256 ops/second.\n>\n> On 05/27/2015 03:52 PM, Sergio Lerner wrote:\n> > I like the idea but I think we should leave at least 16 bits of the\n> > version fixed as an extra-nonce.\n> > If we don't then miners may use them as a nonce anyway, and mess with\n> > the soft-fork voting system.\n> > My original proposal was this:\n> https://github.com/bitcoin/bitcoin/pull/5102\n> >\n> > Best regards\n> >\n> >\n> >\n> ------------------------------------------------------------------------------\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/ee647d97/attachment.html>"
            },
            {
                "author": "Adam Back",
                "date": "2015-05-28T08:11:21",
                "message_text_only": "Or as far as that goes, permuting (the non-dependent) transactions in\nthe block by permuting the internal merkle tree nodes at increasing\ndepths.  (Dependent because transactions that depend on each other\nhave to come in-order; but one could eg put the n-1 of each n sequence\nof in-order transactions in the left-half and unordered in the right\nhalf.)\n\nThat makes the tree manipulations maximum depth independent, and even\ntransaction independent possibly - just need to know enough depth in\nthe tree of hashes that are permutation safe.\n\nAdam\n\nOn 28 May 2015 at 08:51, Christian Decker <decker.christian at gmail.com> wrote:\n> Agreed, there is no need to misuse the version field as well. There is more\n> than enough variability you could roll in the merkle tree including and\n> excluding transactions, and the scriptSig of the coinbase transaction, which\n> also influences the merkle root.\n>\n> I have a fundamental dislike of retroactively changing semantics, and the\n> version field should be used just for that: a version. I don't even\n> particularly like flagging support for a fork in the version field, but\n> since I have no better solution, count me as supporting Sipa's proposal. We\n> definitely need a more comfortable way of rolling out new features.\n>\n> Regards,\n> Chris\n>\n> On Thu, May 28, 2015 at 3:08 AM Patrick Strateman\n> <patrick.strateman at gmail.com> wrote:\n>>\n>> There is absolutely no reason to do this.\n>>\n>> Any reasonable micro-controller can build merkle tree roots\n>> significantly faster than is necessary.\n>>\n>> 1 Th/s walks the nonce range once every 4.3ms.\n>>\n>> The largest valid merkle trees are 14 nodes high.\n>>\n>> That translates to 28 SHA256 ops per 4.3ms or 6511 SHA256 ops/second.\n>>\n>> For reference an RPi 1 model B does 2451050 SHA256 ops/second.\n>>\n>> On 05/27/2015 03:52 PM, Sergio Lerner wrote:\n>> > I like the idea but I think we should leave at least 16 bits of the\n>> > version fixed as an extra-nonce.\n>> > If we don't then miners may use them as a nonce anyway, and mess with\n>> > the soft-fork voting system.\n>> > My original proposal was this:\n>> > https://github.com/bitcoin/bitcoin/pull/5102\n>> >\n>> > Best regards\n>> >\n>> >\n>> >\n>> > ------------------------------------------------------------------------------\n>> > _______________________________________________\n>> > Bitcoin-development mailing list\n>> > Bitcoin-development at lists.sourceforge.net\n>> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-27T10:15:46",
                "message_text_only": "On May 27, 2015 11:35 AM, \"Tier Nolan\" <tier.nolan at gmail.com> wrote:\n\n> Was the intention to change the 95% rule.  You need 750 of the last 1000\nto activate and then must wait at least 1000 for implication?\n\nYou need 75% to start applying it, 95% to start rejecting blocks that don't\napply it.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/2cc99991/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Version bits proposal",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Douglas Roark",
                "Patrick Strateman",
                "Adam Back",
                "Sergio Lerner",
                "Peter Todd",
                "Tier Nolan",
                "Jorge Tim\u00f3n",
                "Luke Dashjr",
                "Pieter Wuille",
                "Christian Decker"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 22777
        }
    },
    {
        "title": "[Bitcoin-development] Consensus-enforced transaction replacement via sequence numbers",
        "thread_messages": [
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-27T01:50:29",
                "message_text_only": "Sequence numbers appear to have been originally intended as a mechanism for\ntransaction replacement within the context of multi-party transaction\nconstruction, e.g. a micropayment channel. The idea is that a participant\ncan sign successive versions of a transaction, each time incrementing the\nsequence field by some amount. Relay nodes perform transaction replacement\naccording to some policy rule making use of the sequence numbers, e.g.\nrequiring sequence numbers in a replacement to be monotonically increasing.\n\nAs it happens, this cannot be made safe in the bitcoin protocol as deployed\ntoday, as there is no enforcement of the rule that miners include the most\nrecent transaction in their blocks. As such, any protocol relying on a\ntransaction replacement policy can be defeated by miners choosing not to\nfollow that policy, which they may even be incentivised to do so (if older\ntransactions provide higher fee per byte, for example). Transaction\nreplacement is presently disabled in Bitcoin Core.\n\nThese shortcomings can be fixed in an elegant way by giving sequence\nnumbers new consensus-enforced semantics as a relative lock-time: if a\nsequence number is non-final (MAX_INT), its bitwise inverse is interpreted\nas either a relative height or time delta which is added to the height or\nmedian time of the block containing the output being spent to form a\nper-input lock-time. The lock-time of each input constructed in this manor,\nplus the nLockTime of the transaction itself if any input is non-final must\nbe satisfied for a transaction to be valid.\n\nFor example, a transaction with an txin.nSequence set to 0xffffff9b [==\n~(uint32_t)100] is prevented by consensus rule from being selected for\ninclusion in a block until the 100th block following the one including the\nparent transaction referenced by that input.\n\nIn this way one may construct, for example, a bidirectional micropayment\nchannel where each change of direction increments sequence numbers to make\nthe transaction become valid prior to any of the previously exchanged\ntransactions.\n\nThis also enables the discussed relative-form of CHECKLOCKTIMEVERIFY to be\nimplemented in the same way: by checking transaction data only and not\nrequiring contextual information like the block height or timestamp.\n\nAn example implementation of this concept, as a policy change to the\nmempool processing of Bitcoin Core is available on github:\n\nhttps://github.com/maaku/bitcoin/tree/sequencenumbers\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150526/74091cca/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-27T07:47:13",
                "message_text_only": "On Tue, May 26, 2015 at 06:50:29PM -0700, Mark Friedenbach wrote:\n> Sequence numbers appear to have been originally intended as a mechanism for\n> transaction replacement within the context of multi-party transaction\n> construction, e.g. a micropayment channel. The idea is that a participant\n> can sign successive versions of a transaction, each time incrementing the\n> sequence field by some amount. Relay nodes perform transaction replacement\n> according to some policy rule making use of the sequence numbers, e.g.\n> requiring sequence numbers in a replacement to be monotonically increasing.\n\nCan you provide a worked example of this in use? I think I see a major\nflaw, but I'd like to see a worked example first.\n\nKeep in mind that there's absolutely no reason to have pending\ntransactions in mempools until we actually expect them to be mined.\nEqually this proposal is no more \"consensus enforcement\" than simply\nincreasing the fee (and possibly decreasing the absolute nLockTime) for\neach replacement would be; increasing the fee for each mempool\nreplacement is a hard requirement as an anti-DoS anyway. (this was all\ndiscussed on the mailing list two years ago when RBF was first proposed)\n\n-- \n'peter'[:-1]@petertodd.org\n00000000000000000ec0c3a90baa52289171046469fe4a21dc5a0dac4cb758a9\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/fa8ed6a4/attachment.sig>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2015-05-27T08:18:52",
                "message_text_only": "On Wed, May 27, 2015 at 7:47 AM, Peter Todd <pete at petertodd.org> wrote:\n> Equally this proposal is no more \"consensus enforcement\" than simply\n> increasing the fee (and possibly decreasing the absolute nLockTime) for\n\nYou've misunderstood it, I think-- Functionally nlocktime but relative\nto each txin's height.\n\nBut the construction gives the sequence numbers a rational meaning,\nthey count down the earliest position a transaction can be included.\n(e.g. the highest possible sequence number can be included any time\nthe inputs are included) the next lower sequence number can only be\nincluded one block later than the input its assigned to is included,\nthe next lower one block beyond that. All consensus enforced.   A\nminer could opt to not include the higher sequence number (which is\nthe only one of the set which it _can_ include) it the hopes of\ncollecting more fees later on the next block, similar to how someone\ncould ignore an eligible locked transaction in the hopes that a future\ndouble spend will be more profitable (and that it'll enjoy that\nprofit) but in both cases it must take nothing at all this block, and\nrisk being cut off by someone else (and, of course, nothing requires\nusers use sequence numbers only one apart...).\n\nIt makes sequence numbers work exactly like you'd expect-- within the\nbounds of whats possible in a decentralized system.  At the same time,\nall it is ... is relative nlocktime."
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-27T10:00:16",
                "message_text_only": "This could cause legacy transactions to become unspendable.\n\n\nA new transaction version number should be used to indicate the change of\nthe field from sequence number to relative lock time.\n\nLegacy transactions should not have the rule applied to them.\n\nOn Wed, May 27, 2015 at 9:18 AM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n\n> On Wed, May 27, 2015 at 7:47 AM, Peter Todd <pete at petertodd.org> wrote:\n> > Equally this proposal is no more \"consensus enforcement\" than simply\n> > increasing the fee (and possibly decreasing the absolute nLockTime) for\n>\n> You've misunderstood it, I think-- Functionally nlocktime but relative\n> to each txin's height.\n>\n> But the construction gives the sequence numbers a rational meaning,\n> they count down the earliest position a transaction can be included.\n> (e.g. the highest possible sequence number can be included any time\n> the inputs are included) the next lower sequence number can only be\n> included one block later than the input its assigned to is included,\n> the next lower one block beyond that. All consensus enforced.   A\n> miner could opt to not include the higher sequence number (which is\n> the only one of the set which it _can_ include) it the hopes of\n> collecting more fees later on the next block, similar to how someone\n> could ignore an eligible locked transaction in the hopes that a future\n> double spend will be more profitable (and that it'll enjoy that\n> profit) but in both cases it must take nothing at all this block, and\n> risk being cut off by someone else (and, of course, nothing requires\n> users use sequence numbers only one apart...).\n>\n> It makes sequence numbers work exactly like you'd expect-- within the\n> bounds of whats possible in a decentralized system.  At the same time,\n> all it is ... is relative nlocktime.\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/e68c2a37/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-27T10:58:05",
                "message_text_only": "On Wed, May 27, 2015 at 08:18:52AM +0000, Gregory Maxwell wrote:\n> On Wed, May 27, 2015 at 7:47 AM, Peter Todd <pete at petertodd.org> wrote:\n> > Equally this proposal is no more \"consensus enforcement\" than simply\n> > increasing the fee (and possibly decreasing the absolute nLockTime) for\n> \n> You've misunderstood it, I think-- Functionally nlocktime but relative\n> to each txin's height.\n> \n> But the construction gives the sequence numbers a rational meaning,\n> they count down the earliest position a transaction can be included.\n> (e.g. the highest possible sequence number can be included any time\n> the inputs are included) the next lower sequence number can only be\n> included one block later than the input its assigned to is included,\n> the next lower one block beyond that. All consensus enforced.   A\n> miner could opt to not include the higher sequence number (which is\n> the only one of the set which it _can_ include) it the hopes of\n> collecting more fees later on the next block, similar to how someone\n> could ignore an eligible locked transaction in the hopes that a future\n> double spend will be more profitable (and that it'll enjoy that\n> profit) but in both cases it must take nothing at all this block, and\n> risk being cut off by someone else (and, of course, nothing requires\n> users use sequence numbers only one apart...).\n\nI understand that part.\n\nI'm just saying it's not clear to me what's the functional difference in\npractice between it and having both parties sign a decreasing absolute\nnLockTime. For instance, you and I could setup a payment channel using\nthe following transaction t0:\n\n    1.0 BTC: PT -> 1.0 BTC: PT && (GM || <expiry> CLTV)\n    1.0 BTC: GM -> 1.0 BTC: GM && (PT || <expiry> CLTV)\n\nAfter <expiry> both of us are guaranteed to get our funds back\nregardless. I can then give you funds by signing my part of t1a:\n\n    t0.vout[0] <PT sig> <blank> -> 0.5 BTC: PT\n    t0.vout[1] <blank> <PT sig> -> 1.5 BTC: GM\n    nLockTime = <expiry - 1>\n\nYou can then give me funds with t1b:\n\n    t0.vout[0] <blank> <GM sig> -> 1.5 BTC: PT\n    t0.vout[1] <GM sig> <blank> -> 0.5 BTC: GM\n    nLockTime = <expiry - 2>\n\netc. etc. We can close the channel by signing a non-nLockTime'd tx at\nany time. If you don't co-operate, I have to wait, and hope I get my tx\nmined before you get yours.\n\nWhat I'm not seeing is how the relative nLockTime that nSequence\nprovides fundamentally changes any of this.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000001643f7706f3dcbc3a386e4c1bfba852ff628d8024f875b6\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/a3370384/attachment.sig>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2015-05-27T17:07:24",
                "message_text_only": "On May 27, 2015 12:58 PM, \"Peter Todd\" <pete at petertodd.org> wrote:\n\n> What I'm not seeing is how the relative nLockTime that nSequence\n> provides fundamentally changes any of this.\n\nThis allows the implementation of a rcltv that doesn't make script depend\non the current height, in a similar way that cltv uses the nLockTime (which\nhas been compared with the current height already when checking the script).\nIn fact, the implementation could be simpler if the goal of maintaining the\noriginal nSequence semantics was ignored ( although not that simpler, but\nyou wouldn't need to use ~ (bitwise not).\nI'm still not sure whether there should be 2 BIPs for this or just one.\n\n> --\n> 'peter'[:-1]@petertodd.org\n> 000000000000000001643f7706f3dcbc3a386e4c1bfba852ff628d8024f875b6\n>\n>\n------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/244fa6f5/attachment.html>"
            },
            {
                "author": "Telephone Lemien",
                "date": "2015-05-27T08:04:09",
                "message_text_only": "Please remove me from the mailing list\n\n2015-05-27 3:50 GMT+02:00 Mark Friedenbach <mark at friedenbach.org>:\n\n> Sequence numbers appear to have been originally intended as a mechanism\n> for transaction replacement within the context of multi-party transaction\n> construction, e.g. a micropayment channel. The idea is that a participant\n> can sign successive versions of a transaction, each time incrementing the\n> sequence field by some amount. Relay nodes perform transaction replacement\n> according to some policy rule making use of the sequence numbers, e.g.\n> requiring sequence numbers in a replacement to be monotonically increasing.\n>\n> As it happens, this cannot be made safe in the bitcoin protocol as\n> deployed today, as there is no enforcement of the rule that miners include\n> the most recent transaction in their blocks. As such, any protocol relying\n> on a transaction replacement policy can be defeated by miners choosing not\n> to follow that policy, which they may even be incentivised to do so (if\n> older transactions provide higher fee per byte, for example). Transaction\n> replacement is presently disabled in Bitcoin Core.\n>\n> These shortcomings can be fixed in an elegant way by giving sequence\n> numbers new consensus-enforced semantics as a relative lock-time: if a\n> sequence number is non-final (MAX_INT), its bitwise inverse is interpreted\n> as either a relative height or time delta which is added to the height or\n> median time of the block containing the output being spent to form a\n> per-input lock-time. The lock-time of each input constructed in this manor,\n> plus the nLockTime of the transaction itself if any input is non-final must\n> be satisfied for a transaction to be valid.\n>\n> For example, a transaction with an txin.nSequence set to 0xffffff9b [==\n> ~(uint32_t)100] is prevented by consensus rule from being selected for\n> inclusion in a block until the 100th block following the one including the\n> parent transaction referenced by that input.\n>\n> In this way one may construct, for example, a bidirectional micropayment\n> channel where each change of direction increments sequence numbers to make\n> the transaction become valid prior to any of the previously exchanged\n> transactions.\n>\n> This also enables the discussed relative-form of CHECKLOCKTIMEVERIFY to be\n> implemented in the same way: by checking transaction data only and not\n> requiring contextual information like the block height or timestamp.\n>\n> An example implementation of this concept, as a policy change to the\n> mempool processing of Bitcoin Core is available on github:\n>\n> https://github.com/maaku/bitcoin/tree/sequencenumbers\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/ed83c67b/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-27T10:11:26",
                "message_text_only": ">\n> Sequence numbers appear to have been originally intended as a mechanism\n> for transaction replacement within the context of multi-party transaction\n> construction, e.g. a micropayment channel.\n>\n\nYes indeed they were. Satoshis mechanism was more general than micropayment\nchannels and could do HFT between any set of parties.\n\n\n> As it happens, this cannot be made safe in the bitcoin protocol as\n> deployed today, as there is no enforcement of the rule that miners include\n> the most recent transaction in their blocks.\n>\n\nSafe is relative - this is the same logic the original replace-by-fee\nargument uses. There's no enforcement that miners use any particular\nordering of transactions.\n\nAs I believe out of all proposed protocols Satoshi's is still the most\npowerful, I would suggest that any change to the semantics on nSequence be\ngated by a high bit or something, so the original meaning remains available\nif/when resource scheduling and update flood damping are implemented. That\nway people can try it out and if miners are breaking things too frequently\nby ignoring the chronological ordering people can abandon protocols that\nrely on it, and if they aren't they can proceed and benefit from the\ngreater flexibility.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/4ed103cd/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-27T15:26:52",
                "message_text_only": "On Wed, May 27, 2015 at 3:11 AM, Mike Hearn <mike at plan99.net> wrote:\n\n>\n> As I believe out of all proposed protocols Satoshi's is still the most\n> powerful, I would suggest that any change to the semantics on nSequence be\n> gated by a high bit or something, so the original meaning remains available\n> if/when resource scheduling and update flood damping are implemented. That\n> way people can try it out and if miners are breaking things too frequently\n> by ignoring the chronological ordering people can abandon protocols that\n> rely on it, and if they aren't they can proceed and benefit from the\n> greater flexibility.\n>\n>\nMike, this proposal was purposefully constructed to maintain as well as\npossible the semantics of Satoshi's original construction. Higher sequence\nnumbers -- chronologically later transactions -- are able to hit the chain\nearlier, and therefore it can be reasonably argued will be selected by\nminers before the later transactions mature. Did I fail in some way to\ncapture that original intent?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/7d354be0/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-27T17:39:29",
                "message_text_only": ">\n> Mike, this proposal was purposefully constructed to maintain as well as\n> possible the semantics of Satoshi's original construction. Higher sequence\n> numbers -- chronologically later transactions -- are able to hit the chain\n> earlier, and therefore it can be reasonably argued will be selected by\n> miners before the later transactions mature. Did I fail in some way to\n> capture that original intent?\n>\n\nRight, but the original protocol allowed for e.g. millions of revisions of\nthe transaction, hence for high frequency trading (that's actually how\nSatoshi originally explained it to me - as a way to do HFT - back then the\nchannel concept didn't exist).\n\nAs you point out, with a careful construction of channels you should only\nneed to bump the sequence number when the channel reverses direction. If\nyour app only needs to do that rarely, it's a fine approach.And your\nproposal does sounds better than sequence numbers being useless like at the\nmoment. I'm just wondering if we can get back to the original somehow or at\nleast leave a path open to it, as it seems to be a superset of all other\nproposals, features-wise.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150527/8c096952/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-28T09:56:36",
                "message_text_only": "I have no problem with modifying the proposal to have the most significant\nbit signal use of the nSequence field as a relative lock-time. That leaves\na full 31 bits for experimentation when relative lock-time is not in use. I\nhave adjusted the code appropriately:\n\nhttps://github.com/maaku/bitcoin/tree/sequencenumbers\n\nOn Wed, May 27, 2015 at 10:39 AM, Mike Hearn <mike at plan99.net> wrote:\n\n> Mike, this proposal was purposefully constructed to maintain as well as\n>> possible the semantics of Satoshi's original construction. Higher sequence\n>> numbers -- chronologically later transactions -- are able to hit the chain\n>> earlier, and therefore it can be reasonably argued will be selected by\n>> miners before the later transactions mature. Did I fail in some way to\n>> capture that original intent?\n>>\n>\n> Right, but the original protocol allowed for e.g. millions of revisions of\n> the transaction, hence for high frequency trading (that's actually how\n> Satoshi originally explained it to me - as a way to do HFT - back then the\n> channel concept didn't exist).\n>\n> As you point out, with a careful construction of channels you should only\n> need to bump the sequence number when the channel reverses direction. If\n> your app only needs to do that rarely, it's a fine approach.And your\n> proposal does sounds better than sequence numbers being useless like at the\n> moment. I'm just wondering if we can get back to the original somehow or at\n> least leave a path open to it, as it seems to be a superset of all other\n> proposals, features-wise.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/ae531ae4/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2015-05-28T10:23:48",
                "message_text_only": "Cool, thanks.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/855f7291/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-28T10:30:18",
                "message_text_only": "Can you update it so that it only applies to transactions with version\nnumber 3 and higher.  Changing the meaning of a field is exactly what the\nversion numbers are for.\n\nYou could even decode version 3 transactions like that.\n\nVersion 3 transactions have a sequence number of 0xFFFFFFFF and the\nsequence number field is re-purposed for relative lock time.\n\nThis means that legacy transactions that have already been signed but have\na locktime in the future will still be able to enter the blockchain\n(without having to wait significantly longer than expected).\n\nOn Thu, May 28, 2015 at 10:56 AM, Mark Friedenbach <mark at friedenbach.org>\nwrote:\n\n> I have no problem with modifying the proposal to have the most significant\n> bit signal use of the nSequence field as a relative lock-time. That leaves\n> a full 31 bits for experimentation when relative lock-time is not in use. I\n> have adjusted the code appropriately:\n>\n> https://github.com/maaku/bitcoin/tree/sequencenumbers\n>\n> On Wed, May 27, 2015 at 10:39 AM, Mike Hearn <mike at plan99.net> wrote:\n>\n>> Mike, this proposal was purposefully constructed to maintain as well as\n>>> possible the semantics of Satoshi's original construction. Higher sequence\n>>> numbers -- chronologically later transactions -- are able to hit the chain\n>>> earlier, and therefore it can be reasonably argued will be selected by\n>>> miners before the later transactions mature. Did I fail in some way to\n>>> capture that original intent?\n>>>\n>>\n>> Right, but the original protocol allowed for e.g. millions of revisions\n>> of the transaction, hence for high frequency trading (that's actually how\n>> Satoshi originally explained it to me - as a way to do HFT - back then the\n>> channel concept didn't exist).\n>>\n>> As you point out, with a careful construction of channels you should only\n>> need to bump the sequence number when the channel reverses direction. If\n>> your app only needs to do that rarely, it's a fine approach.And your\n>> proposal does sounds better than sequence numbers being useless like at the\n>> moment. I'm just wondering if we can get back to the original somehow or at\n>> least leave a path open to it, as it seems to be a superset of all other\n>> proposals, features-wise.\n>>\n>\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/e03cd831/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2015-05-28T12:04:34",
                "message_text_only": "On Thu, May 28, 2015 at 11:30:18AM +0100, Tier Nolan wrote:\n> Can you update it so that it only applies to transactions with version\n> number 3 and higher.  Changing the meaning of a field is exactly what the\n> version numbers are for.\n> \n> You could even decode version 3 transactions like that.\n> \n> Version 3 transactions have a sequence number of 0xFFFFFFFF and the\n> sequence number field is re-purposed for relative lock time.\n> \n> This means that legacy transactions that have already been signed but have\n> a locktime in the future will still be able to enter the blockchain\n> (without having to wait significantly longer than expected).\n\nFor that matter, we probably don't want to treat this as a *version*\nchange, but rather a *feature* flag. For instance, nSequence is\npotentially useful for co-ordinating multiple signatures to ensure they\ncan only be used in certain combinations, a use-case not neccesarily\ncompatible with this idea of a relative lock. Similarly it's potentially\nuseful for dealing with malleability.\n\nnSequence is currently the *only* thing in CTxIn's that the signature\nsigns that can be freely changed; I won't be surprised if we find other\nuses for it.\n\nOf course, all of the above is assuming this proposal is useful; that's\nnot clear to me yet and won't be without fleshed out examples.\n\n-- \n'peter'[:-1]@petertodd.org\n000000000000000008464a6a19387029fa99edace15996d06a6343a8345d6167\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 650 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/b3fb4c20/attachment.sig>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-28T13:35:57",
                "message_text_only": "On Thu, May 28, 2015 at 1:04 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> For that matter, we probably don't want to treat this as a *version*\n> change, but rather a *feature* flag.\n\n\nI think it is still a version change.  At the moment, the 4 bytes refer to\nthe sequence number and afterwards they mean something else.\n\nFor relative locktime verify, I think most use cases could be block count\nbased and don't need to be able to count very high.\n\nI think the main benefit is that protocols can have one party trigger a\nstep while giving the other party guaranteed time to respond.\n\n\n*Fast Channel Close*\n\nThis assumes that malleability is fixed.\n\nAlice creates\n\nTXA:\noutput (x) to [multisig A1 & B1]\n\nRefund:\ninput TXA (signed by Alice)\nOutput [(A2 & relative_check_locktime(150)) OR (multisig A3 &  B2)]\n\nAlice sends Refund to Bob\n\nBob signs it and sends it back to Alice\n\nAlice verifies the signature, adds her own and sends it to Bob.\n\nShe broadcasts TXA (would wait until Bob confirms acceptance).\n\nThis means that both Alice and Bob have the refund transaction and can use\nit to close the channel (assuming TXA is not mutated).\n\nAlice can send money to Bob by creating a transaction which spends the\noutput of the refund transaction (splitting the output x-b for Alice and b\nfor Bob), signing it and sending it to Bob.\n\nAlice can force Bob to close the channel by broadcasting the refund\ntransaction.  150 blocks later, she gets the channel deposit if he doesn't\nact.\n\nIf she had sent some money to Bob, he has 150 blocks to sign the\ntransaction that pays him the most money and broadcast it.  Alice gets the\nremainder of the deposit.\n\nAlice cannot broadcast earlier version, since Bob doesn't send her the\nsigned versions.\n\nThis means that the channel doesn't need a defined end date.  Either party\ncan close the channel whenever they want.\n\nTXA could be protected against malleability by adding a locktime path.\nThis would only be for use if the transaction is mutated.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/c68865b7/attachment.html>"
            },
            {
                "author": "s7r",
                "date": "2015-05-28T16:22:46",
                "message_text_only": "On 5/28/2015 4:35 PM, Tier Nolan wrote:\n> On Thu, May 28, 2015 at 1:04 PM, Peter Todd <pete at petertodd.org\n> <mailto:pete at petertodd.org>> wrote:\n> \n>     For that matter, we probably don't want to treat this as a *version*\n>     change, but rather a *feature* flag. \n> \n> \n> I think it is still a version change.  At the moment, the 4 bytes refer\n> to the sequence number and afterwards they mean something else.\n> \n> For relative locktime verify, I think most use cases could be block\n> count based and don't need to be able to count very high. \n> \n> I think the main benefit is that protocols can have one party trigger a\n> step while giving the other party guaranteed time to respond.\n> \n> *Fast Channel Close\n> *\n> \n> This assumes that malleability is fixed.\n> \n\nIndeed. This is very important for refunds.\n\n> Alice creates\n> \n> TXA:\n> output (x) to [multisig A1 & B1]\n> \n> Refund:\n> input TXA (signed by Alice)\n> Output [(A2 & relative_check_locktime(150)) OR (multisig A3 &  B2)]\n> \n> Alice sends Refund to Bob\n> \n> Bob signs it and sends it back to Alice\n> \n> Alice verifies the signature, adds her own and sends it to Bob.\n> \n> She broadcasts TXA (would wait until Bob confirms acceptance).\n> \n> This means that both Alice and Bob have the refund transaction and can\n> use it to close the channel (assuming TXA is not mutated).\n> \n\nIn this scenario, if channel is closed, Alice is the only one who can\ntake the coins back after a relative locktime of 150 blocks. Bob is not\nable to do this.\n\n> Alice can send money to Bob by creating a transaction which spends the\n> output of the refund transaction (splitting the output x-b for Alice and\n> b for Bob), signing it and sending it to Bob.\n> \n> Alice can force Bob to close the channel by broadcasting the refund\n> transaction.  150 blocks later, she gets the channel deposit if he\n> doesn't act.\n> \n\nHow is Bob protected in this scenario? If Alice sings a transaction\nwhich spends the output of the refund transaction and gives it to Bob,\nBob can just add its signature and claim his slice of the output,\nwithout necessarily shipping the goods or delivering the services to Alice.\n\n> If she had sent some money to Bob, he has 150 blocks to sign the\n> transaction that pays him the most money and broadcast it.  Alice gets\n> the remainder of the deposit.\n> \nCan you be more explicit here? It doesn't make sense for me.\n\n> Alice cannot broadcast earlier version, since Bob doesn't send her the\n> signed versions.\n> \n> This means that the channel doesn't need a defined end date.  Either\n> party can close the channel whenever they want.\n> \nWith some risks.\n\n> TXA could be protected against malleability by adding a locktime path. \n> This would only be for use if the transaction is mutated.\n> \nHow do you apply a locktime path to a tx in the current network consensus?\n\n> \n> ------------------------------------------------------------------------------\n> \n> \n> \n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-28T17:21:34",
                "message_text_only": "On Thu, May 28, 2015 at 5:22 PM, s7r <s7r at sky-ip.org> wrote:\n\n> In this scenario, if channel is closed, Alice is the only one who can\n> take the coins back after a relative locktime of 150 blocks. Bob is not\n> able to do this.\n>\n\nYes, Alice is assumed to be the one who funded the channel.  It is a single\ndirection channel (Alice to Bob).\n\n\n> How is Bob protected in this scenario?\n\n\nAssuming the deposit is 1 BTC.\n\nWhen the channel is created, Alice can broadcast the refund transaction\nimmediately and the get her money back 150 blocks later.\n\nThe full scriptPubKey for the refund transaction would be\n\nOP_IF\n    <150> OP_RELATIVE_CHECKLOCKTIME_VERIFY OP_DROP <Alice's public key 2>\nOP_CHECKSIGVERIFY\nOP_ELSE\n    OP_2 <Alice's public key 3> <Bob's public key 2> OP_2\nOP_CHECKMULTISIGVERIFY\nOP_ENDIF\n\nThis means that Alice can spend the output after 150 blocks but with both\nsignatures Bob and Alice can spend the output without the delay.\n\nShe can send money to Bob by spending the non-locked output of the refund\ntransaction (0.01BTC for Bob and 0.99BTC for Alice).\n\nBob has a transaction that pays him 0.01BTC and pays Alice 0.99BTC from the\nrefund transaction and is signed by Alice, but still requires his\nsignature.  Only Bob can make the transaction valid.\n\nIt can be spent as soon as the refund transaction is broadcast.\n\nHe has the refund transaction, so he can start the process whenever he\nwishes.\n\nAssume the channel runs for a while, and Alice sends 0.3BTC total.\n\nBob has a transaction which pays him 0.3BTC and Alice 0.7BTC.  He also has\nsome that pay him less than 0.3, but there is no point in him using those\nones.\n\nAlice decides she wants to close the channel, so asks bob to sign his final\ntransaction and broadcast it and the refund transaction.\n\nIf Bob refuses to do that, then Alice can just broadcast the refund\ntransaction.\n\nIf Bob still refuses to broadcast his final transaction, then Alice gets\n1BTC and he gets nothing, after 150 blocks.\n\nThis means he will send his final transaction before the 150 blocks have\npassed.  This gets him 0.3 and Alice 0.7.\n\nBob can close the channel immediately and Alice can force it to be closed\nwithin 150 blocks (~1 day).\n\n\n> If Alice sings a transaction\n> which spends the output of the refund transaction and gives it to Bob,\n> Bob can just add its signature and claim his slice of the output,\n> without necessarily shipping the goods or delivering the services to Alice.\n>\n\nProtection against that type of fraud isn't covered by channels.  They are\njust to make sure money is handed over.\n\n\n>  Can you be more explicit here? It doesn't make sense for me.\n>\n\nDoes the explanation above help?\n\nWith some risks.\n>\n\nAs long as Bob is online and sees the refund transaction being broadcast by\nAlice, then there is no risk to him.\n\nAlice can close the transaction whenever she wants, so there is no holdup\nrisk for her.\n\n\n> How do you apply a locktime path to a tx in the current network consensus?\n>\n\nI mean with OP_CHECKLOCKTIMEVERIFY.\n\nShe could say that TXA pays to her in 6 months.\n\nIf TXA ends up mutated after being broadcast, then she would have to wait\nthe 6 months.  It's better than nothing and maybe Bob would sign the\nmutated transaction.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/00a2a35f/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-28T14:59:13",
                "message_text_only": "Why 3? Do we have a version 2?\n\nAs for doing it in serialization, that would alter the txid making it a\nhard fork change.\nOn May 28, 2015 03:30, \"Tier Nolan\" <tier.nolan at gmail.com> wrote:\n\n> Can you update it so that it only applies to transactions with version\n> number 3 and higher.  Changing the meaning of a field is exactly what the\n> version numbers are for.\n>\n> You could even decode version 3 transactions like that.\n>\n> Version 3 transactions have a sequence number of 0xFFFFFFFF and the\n> sequence number field is re-purposed for relative lock time.\n>\n> This means that legacy transactions that have already been signed but have\n> a locktime in the future will still be able to enter the blockchain\n> (without having to wait significantly longer than expected).\n>\n> On Thu, May 28, 2015 at 10:56 AM, Mark Friedenbach <mark at friedenbach.org>\n> wrote:\n>\n>> I have no problem with modifying the proposal to have the most\n>> significant bit signal use of the nSequence field as a relative lock-time.\n>> That leaves a full 31 bits for experimentation when relative lock-time is\n>> not in use. I have adjusted the code appropriately:\n>>\n>> https://github.com/maaku/bitcoin/tree/sequencenumbers\n>>\n>> On Wed, May 27, 2015 at 10:39 AM, Mike Hearn <mike at plan99.net> wrote:\n>>\n>>> Mike, this proposal was purposefully constructed to maintain as well as\n>>>> possible the semantics of Satoshi's original construction. Higher sequence\n>>>> numbers -- chronologically later transactions -- are able to hit the chain\n>>>> earlier, and therefore it can be reasonably argued will be selected by\n>>>> miners before the later transactions mature. Did I fail in some way to\n>>>> capture that original intent?\n>>>>\n>>>\n>>> Right, but the original protocol allowed for e.g. millions of revisions\n>>> of the transaction, hence for high frequency trading (that's actually how\n>>> Satoshi originally explained it to me - as a way to do HFT - back then the\n>>> channel concept didn't exist).\n>>>\n>>> As you point out, with a careful construction of channels you should\n>>> only need to bump the sequence number when the channel reverses direction.\n>>> If your app only needs to do that rarely, it's a fine approach.And your\n>>> proposal does sounds better than sequence numbers being useless like at the\n>>> moment. I'm just wondering if we can get back to the original somehow or at\n>>> least leave a path open to it, as it seems to be a superset of all other\n>>> proposals, features-wise.\n>>>\n>>\n>>\n>>\n>> ------------------------------------------------------------------------------\n>>\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/0d4e5a2a/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-28T15:18:05",
                "message_text_only": "On Thu, May 28, 2015 at 3:59 PM, Mark Friedenbach <mark at friedenbach.org>\nwrote:\n\n> Why 3? Do we have a version 2?\n>\nI meant whatever the next version is, so you are right, it's version 2.\n\n> As for doing it in serialization, that would alter the txid making it a\n> hard fork change.\n>\nThe change is backwards compatible (since there is no restrictions on\nsequence numbers).   This makes it a soft fork.\n\nThat doesn't change the fact that you are changing what a field in the\ntransaction represents.\n\nYou could say that the sequence number is no longer encoded in the\nserialization, it is assumed to be 0xFFFFFFFF for all version 2+\ntransactions and the relative locktime is a whole new field that is the\nsame size (and position).\n\nI think keeping some of the bytes for other uses is a good idea.  The\nentire top 2 bytes could be ignored when working out relative locktime\nverify.  That leaves them fully free to be set to anything.\n\nIt could be that if the MSB of the bottom 2 bytes is set, then that\nactivates the rule and the top 2 bytes are ignored.\n\nAre there any use-cases which need a RLTV of more than 8191 blocks delay\n(that can't be covered by the absolute version)?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/c5be8db5/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2015-05-28T15:38:43",
                "message_text_only": "Oh ok you mean a semantic difference for the purpose of explaining. It\ndoesn't actually change the code.\n\nRegarding saving more bits, there really isn't much room if you consider\ntime-based relative locktimes and long-lived channels on the order of a\nyear or more.\n\nOn Thu, May 28, 2015 at 8:18 AM, Tier Nolan <tier.nolan at gmail.com> wrote:\n\n> On Thu, May 28, 2015 at 3:59 PM, Mark Friedenbach <mark at friedenbach.org>\n> wrote:\n>\n>> Why 3? Do we have a version 2?\n>>\n> I meant whatever the next version is, so you are right, it's version 2.\n>\n>> As for doing it in serialization, that would alter the txid making it a\n>> hard fork change.\n>>\n> The change is backwards compatible (since there is no restrictions on\n> sequence numbers).   This makes it a soft fork.\n>\n> That doesn't change the fact that you are changing what a field in the\n> transaction represents.\n>\n> You could say that the sequence number is no longer encoded in the\n> serialization, it is assumed to be 0xFFFFFFFF for all version 2+\n> transactions and the relative locktime is a whole new field that is the\n> same size (and position).\n>\n> I think keeping some of the bytes for other uses is a good idea.  The\n> entire top 2 bytes could be ignored when working out relative locktime\n> verify.  That leaves them fully free to be set to anything.\n>\n> It could be that if the MSB of the bottom 2 bytes is set, then that\n> activates the rule and the top 2 bytes are ignored.\n>\n> Are there any use-cases which need a RLTV of more than 8191 blocks delay\n> (that can't be covered by the absolute version)?\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/37f27f81/attachment.html>"
            },
            {
                "author": "Tier Nolan",
                "date": "2015-05-28T15:57:15",
                "message_text_only": "What are the use cases for relative lock time verify?  I have 1 and I think\nthat is the kind of thing it is useful for.\n\nI think that most cases are just to guarantee that the other party has a\nchance to react.  This means that 8191 blocks should be more than enough\n(and most would set it lower).\n\nFor long term, the absolute version is just as good.  That depends on use\ncases.  \"You can't take step 4 until 3 months after step 3 has completed\"\ndoesn't seem useful.\n\nOn Thu, May 28, 2015 at 4:38 PM, Mark Friedenbach <mark at friedenbach.org>\nwrote:\n\n> Oh ok you mean a semantic difference for the purpose of explaining. It\n> doesn't actually change the code.\n>\n> Regarding saving more bits, there really isn't much room if you consider\n> time-based relative locktimes and long-lived channels on the order of a\n> year or more.\n>\n> On Thu, May 28, 2015 at 8:18 AM, Tier Nolan <tier.nolan at gmail.com> wrote:\n>\n>> On Thu, May 28, 2015 at 3:59 PM, Mark Friedenbach <mark at friedenbach.org>\n>> wrote:\n>>\n>>> Why 3? Do we have a version 2?\n>>>\n>> I meant whatever the next version is, so you are right, it's version 2.\n>>\n>>> As for doing it in serialization, that would alter the txid making it a\n>>> hard fork change.\n>>>\n>> The change is backwards compatible (since there is no restrictions on\n>> sequence numbers).   This makes it a soft fork.\n>>\n>> That doesn't change the fact that you are changing what a field in the\n>> transaction represents.\n>>\n>> You could say that the sequence number is no longer encoded in the\n>> serialization, it is assumed to be 0xFFFFFFFF for all version 2+\n>> transactions and the relative locktime is a whole new field that is the\n>> same size (and position).\n>>\n>> I think keeping some of the bytes for other uses is a good idea.  The\n>> entire top 2 bytes could be ignored when working out relative locktime\n>> verify.  That leaves them fully free to be set to anything.\n>>\n>> It could be that if the MSB of the bottom 2 bytes is set, then that\n>> activates the rule and the top 2 bytes are ignored.\n>>\n>> Are there any use-cases which need a RLTV of more than 8191 blocks delay\n>> (that can't be covered by the absolute version)?\n>>\n>>\n>> ------------------------------------------------------------------------------\n>>\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150528/066bb1f3/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Consensus-enforced transaction replacement via sequence numbers",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Mike Hearn",
                "s7r",
                "Peter Todd",
                "Tier Nolan",
                "Telephone Lemien",
                "Jorge Tim\u00f3n",
                "Gregory Maxwell",
                "Mark Friedenbach"
            ],
            "messages_count": 21,
            "total_messages_chars_count": 43363
        }
    },
    {
        "title": "[Bitcoin-development] soft-fork block size increase (extension blocks) Re: Proposed alternatives to the 20MB stepfunction",
        "thread_messages": [
            {
                "author": "Adam Back",
                "date": "2015-05-30T00:00:28",
                "message_text_only": "I discussed the extension block idea on wizards a while back and it is\na way to soft-fork an opt-in block-size increase.  Like everything\nhere there are pros and cons.\n\nThe security is better than Raylstonn inferred from Tier's explanation\nI think..  It works as Tier described - there is an extension block\n(say 10MB) and the existing 1MB block.  The extension block is\ncommitted to in the 1MB chain.  Users can transfer bitcoin into the\nextension block, and they can transfer them out.\n\nThe interesting thing is this makes block sizes changes opt-in and\ngives users choice.  Choice is good.  Bitcoin has a one-size-fits-all\nblocksize at present hence the block size debate.  If a bigger\nblock-size were an opt-in choice, and some people wanted 10MB or even\n100MB blocks for low value transactions I expect it would be far\neasier a discussion - people who think 100MB blocks are dangerously\ncentralising, would not opt to use them (or would put only small\nvalues they can afford to lose in them).  There are some security\nimplications though, so this also is nuanced, and more on that in a\nbit.\n\nFee pressure still exists for blocks of difference size as the\nsecurity assurances are not the same.  It is plausible that some\npeople would pay more for transactions in the 1MB block.\n\nNow there are three choices of validation level, rather than the\nnormal 2-levels of SPV or full-node, with extension blocks we get a\nchoice: A) a user could run a full node for both 1MB and 10MB blocks,\nand get full security for both 1MB and 10MB block transactions (but at\nhigher bandwidth cost), or B) a user could run a full node on the 1MB\nblock, but operate as an SPV node for the 10MB block, or C) run in SPV\nmode for both 1MB and 10MB blocks.\n\nSimilarly for mining - miners could validate 1MB and 10MB transactions\n(solo mine or GBT-style), or they could self-validate 1MB transactions\nand pool mine 10MB transactions (have a pool validate those).\n\n1MB full node users who do not upgrade to software that understands\nextension blocks, could run in SPV mode with respect to 10MB blocks.\nHere lies the risk - this imposes a security downgrade on the 1MB\nnon-upgraded users, and also on users who upgrade but dont have the\nbandwidth to validate 10MB blocks.\n\n\nWe could defend non-upgrade users by making receiving funds that came\nvia the extension block opt-in also, eg an optional to use new address\nversion and construct the extension block so that payments out of it\ncan only go to new version addresses.\n\nWe could harden 1MB block SPV security (when receiving weaker\nextension block transactions) in a number of ways: UTXO commitments,\nfraud proofs (and fraud bounties) for moving from the extension block\nto the 1MB block.  We could optionally require coins moving via the\nextension block to the 1MB block to be matured (eg 100 blocks delay)\n\n\nAnyway something else to evaluate.  Not as simple to code as a\nhard-fork, but way safer transition than a hard-fork, and opt-in - if\nyou prefer the higher decentralisation of 1MB blocks, keep using them;\nif you prefer 10MB blocks you can opt-in to them.\n\nAdam\n\nOn 29 May 2015 at 17:39, Raystonn . <raystonn at hotmail.com> wrote:\n> Regarding Tier\u2019s proposal: The lower security you mention for extended\n> blocks would delay, possibly forever, the larger blocks maximum block size\n> that we want for the entire network.  That doesn\u2019t sound like an optimal\n> solution.\n>\n> Regarding consensus for larger maximum block size, what we are seeing on\n> this list is typical of what we see in the U.S. Congress.  Support for\n> changes by the stakeholders (support for bills by the citizens as a whole)\n> has become irrelevant to the probability of these changes being adopted.\n> Lobbyists have all the sway in getting their policies enacted.  In our case,\n> I would bet on some lobbying of core developers by wealthy miners.\n>\n> Someone recently proposed that secret ballots could help eliminate the power\n> of lobbyists in Congress.  Nobody invests in that which cannot be confirmed.\n> Secret ballots mean the vote you are buying cannot be confirmed.  Perhaps\n> this will work for Bitcoin Core as well.\n>\n>\n> From: Tier Nolan\n> Sent: Friday, May 29, 2015 7:22 AM\n> Cc: Bitcoin Dev\n> Subject: Re: [Bitcoin-development] Proposed alternatives to the 20MB\n> stepfunction\n>\n> On Fri, May 29, 2015 at 3:09 PM, Tier Nolan <tier.nolan at gmail.com> wrote:\n>>\n>>\n>>\n>> On Fri, May 29, 2015 at 1:39 PM, Gavin Andresen <gavinandresen at gmail.com>\n>> wrote:\n>>>\n>>> But if there is still no consensus among developers but the \"bigger\n>>> blocks now\" movement is successful, I'll ask for help getting big miners to\n>>> do the same, and use the soft-fork block version voting mechanism to\n>>> (hopefully) get a majority and then a super-majority willing to produce\n>>> bigger blocks. The purpose of that process is to prove to any doubters that\n>>> they'd better start supporting bigger blocks or they'll be left behind, and\n>>> to give them a chance to upgrade before that happens.\n>>\n>>\n>> How do you define that the movement is successful?\n>\n>\n> Sorry again, I keep auto-sending from gmail when trying to delete.\n>\n> In theory, using the \"nuclear option\", the block size can be increased via\n> soft fork.\n>\n> Version 4 blocks would contain the hash of the a valid extended block in the\n> coinbase.\n>\n> <block height> <32 byte extended hash>\n>\n> To send coins to the auxiliary block, you send them to some template.\n>\n> OP_P2SH_EXTENDED <scriptPubKey hash> OP_TRUE\n>\n> This transaction can be spent by anyone (under the current rules).  The soft\n> fork would lock the transaction output unless it transferred money from the\n> extended block.\n>\n> To unlock the transaction output, you need to include the txid of\n> transaction(s) in the extended block and signature(s) in the scriptSig.\n>\n> The transaction output can be spent in the extended block using P2SH against\n> the scriptPubKey hash.\n>\n> This means that people can choose to move their money to the extended block.\n> It might have lower security than leaving it in the root chain.\n>\n> The extended chain could use the updated script language too.\n>\n> This is obviously more complex than just increasing the size though, but it\n> could be a fallback option if no consensus is reached.  It has the advantage\n> of giving people a choice.  They can move their money to the extended chain\n> or not, as they wish."
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-30T00:16:01",
                "message_text_only": "RE: soft-forking an \"extension block\":\n\nSo... go for it, code it up. Implement it in the Bitcoin Core wallet.\n\nThen ask the various wallet developer how long it would take them to update\ntheir software to support something like this, and do some UI mockups of\nwhat the experience would look like for users.\n\nIf there are two engineering solutions to a problem, one really simple, and\none complex, why would you pick the complex one?\n\nEspecially if the complex solution has all of the problems of the simple\none (20MB extension blocks are just as \"dangerous\" as 20MB main blocks,\nyes? If not, why not?)\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/9b9298b2/attachment.html>"
            },
            {
                "author": "Raystonn",
                "date": "2015-05-30T01:36:50",
                "message_text_only": "An HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150529/19cf34ff/attachment.html>"
            },
            {
                "author": "Andrew",
                "date": "2015-05-30T03:27:36",
                "message_text_only": "Hello Adam\n\nFirst of all, thank you for inventing hashcash, which is basically what\nbitcoin is!\n\nSome people have said that my proposal, subject line \"Scaling Bitcoin with\nSubchains\" is essentially the idea of blockchain extensions. Though, I\nthink there is quite a difference between what I propose and what you\npropose. You want to add one optional 10 MB blockchain that synchronizes\nwith the 1 MB blockchain, while I want to add ten 1 MB\n blockchains that each synchronize with the 1 MB chain (and you can\ncontinue like that). I think, as long as we want to keep using blockchains\nfor our cryptocurrency, we will need a tree structure of blockchains in\norder to scale for an arbitrary number of transactions. With just one 10 MB\nblockchain, someone who wants to do the lower valued transactions will need\nto validate all 10 MB, while with ten 1 MB chains, they can choose just the\nchain or chains that are of interest to them. With a tree structure you get\nO(a^(n-1)) MB of transactions in the network while each participant only\nhas to validate O(n) MB of transactions (a is just the number of children\nchains per parent divided by 2, so 5 in the case of 10 children as I\ndescribed). With just one child chain, you don't get this scaling, and it\nis pretty much equivalent to increasing the blocksize, though with a\nsoft-fork instead of a hard-fork.\n\nI think the actual way that the blockchains interact can be still worked\nout (Recently I was thinking of maybe creating a contract system or even a\ndecentralized market between chains). But still, everyone should agree that\nyou need this kind of tree structure. Even if you want to only run a pruned\nnode, the CPU usage and memory scales just as bad. The tree structure also\nhas good privacy and miner decentralization properties, as I can write\nabout later.\n\nBut another thing that I recommend is an \"exit plan\". What if we go with\nsome kind of soft fork and then in the future some better idea comes along?\nThen we should have a way to reverse the soft fork. If people already have\ncoins tied up in sidechains, it can be problematic. So perhaps, in case\npeople want to later ditch the soft fork, nodes in the parent chain can\nallow only old transactions inside the child chains to be accepted back up,\nwhile new transactions are not recognized anymore. That way you can limit\nthe amount of useless transaction traffic that results in case we want\nsomething else.\n\nOn Sat, May 30, 2015 at 1:36 AM, Raystonn <raystonn at hotmail.com> wrote:\n\n> My fear now is too much unnecessary complexity.  More complex means\n> brittle code, but also fewer programmers working on this, which is a risk.\n>\n> We shouldn't delay forever until every potential solution has been\n> explored.  There's always going to be one more thing to explore.\n>  On 29 May 2015 5:16 pm, Gavin Andresen <gavinandresen at gmail.com> wrote:\n>\n> RE: soft-forking an \"extension block\":\n>\n> So... go for it, code it up. Implement it in the Bitcoin Core wallet.\n>\n> Then ask the various wallet developer how long it would take them to\n> update their software to support something like this, and do some UI\n> mockups of what the experience would look like for users.\n>\n> If there are two engineering solutions to a problem, one really simple,\n> and one complex, why would you pick the complex one?\n>\n> Especially if the complex solution has all of the problems of the simple\n> one (20MB extension blocks are just as \"dangerous\" as 20MB main blocks,\n> yes? If not, why not?)\n>\n>\n> --\n> --\n> Gavin Andresen\n>\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150530/8252af9b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "soft-fork block size increase (extension blocks) Re: Proposed alternatives to the 20MB stepfunction",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Adam Back",
                "Andrew",
                "Raystonn",
                "Gavin Andresen"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 11387
        }
    },
    {
        "title": "[Bitcoin-development] Proposal: A measured response to save Bitcoin Core",
        "thread_messages": [
            {
                "author": "Matt Whitlock",
                "date": "2015-05-31T00:29:25",
                "message_text_only": "Greg, Pieter, Jeff, and Wladimir,\n\nI'll try to be brief to respect your time.\n\n1. I don't want to see Bitcoin die.\n\n2. As has been discussed on this list and elsewhere: Bitcoin could potentially die due to economic and/or game-theoretic complications arising from raising the block size limit, but Bitcoin could also die due to usability complications arising from NOT raising the block size limit. Strong, personally held opinions by various members of this community notwithstanding, it is not clear which of these scenarios is more likely.\n\n3. What *is* clear at this point is that Gavin will move ahead with his proposal, regardless of whether the remainder of the Bitcoin Core committers agree with him. If he has to commit his changes to Bitcoin XT and then rally the miners to switch, then that's what he'll do. He believes that he is working in the best interests of Bitcoin (as I would hope we all do), and so I do not fault him for his intentions. However, I think his proposal is too risky.\n\n4. I also think that ignoring the immediate problem is too risky. If allowing significantly larger blocks will cause a serious problem for Bitcoin (which is a possibility that we cannot rule out, as we lack omniscience), then NOT making any change to Bitcoin Core will virtually *assure* that we cause exactly this problem, as the popular (non-technical) consensus appears to be in favor of Bitcoin XT and a larger block size limit. If we do nothing, then there's a very real chance that Bitcoin XT takes over, for better or worse.\n\n5. I'd like to propose a way that we can have our cake and eat it too. My proposal attempts to satisfy both those who want larger blocks AND those who want to be extremely cautious about changing the fundamental economic parameters of Bitcoin.\n\n6. Something I've never understood about Gavin's (et al.) proposal is why there is a massive step right up front. Assuming we accept his argument that we're critically close to running out of capacity, I still must ask: why do we need a 20x increase all at once?\n\n7. It's not a given that blocks will immediately expand to meet the hard limit. In fact, there are strong and compelling arguments why this will NOT happen. But in any software system, if a given scenario is *possible*, then one MUST assume that it will happen and must have a plan to handle it.\n\n8. My primary objection is not to raising the block size limit; my objection is to raising it *suddenly*. You can argue that, because we'll have plenty of time before March 2016, it's not \"sudden,\" but, whether we do it now or a year from now or a decade from now, a step function is, by definition, sudden.\n\n9. My proposal is that we raise the block size limit *gradually*, using an approximately smooth function, without a step discontinuity. We can employ a linear growth function to adjust the block size limit *smoothly* from 1 MB to 20 MB over the course of several years, beginning next March.\n\n10. This is the difference between cannonballing into the deep end of the pool and walking gingerly down the steps into the shallow end. Both get you to the eventual goal, but one is reckless while the other is measured and deliberate. If there's a problem that larger blocks will enable, then I'd prefer to see the problem crop up gradually rather than all at once. If it's gradual, then we'll have time to discuss and fix it without panicking.\n\n11. I am offering to implement this proposal and submit a pull request to Bitcoin Core. However, if another dev who is more familiar with the internals would like to step forward, then that would be superior.\n\nRespectfully submitted,\nMatt Whitlock"
            },
            {
                "author": "s7r",
                "date": "2015-05-31T09:32:08",
                "message_text_only": "Hi,\n\nFor the less crypto engineering experts but highly interested in Bitcoin\nand working with Bitcoin on daily basis reading the list, what would be\nan easy to understand explanation about how does this solution represent\na good fix?\n\nSo, we have a hard cap of 1 MB block currently. This is not enough\nbecause more and more people use Bitcoin and the transaction volume\nincreased (yeey, good news). So, rather than fixing the issue for good,\nwe just increase the block size hard cap to 20 MB. I will not discuss if\nthis causes problems or not. But what are the future plans, when the 20\nMB hard cap will be reached? Increase it again? This doesn't sound like\na fix, it sounds more like pushing the can down the road. Obviously if 1\nMB is not enough now, we have the reasonable suspicion that 20 MB could\nnot be enough in few years.\n\nWhat is the explanation that 20 MB blocks will be sufficient for life\ntime? Is it because 'probably other solutions will appear, such as\nmicropayment channels and offchain transactions'?  If this is the case,\nthose can easily function with 1 MB blocks as well, and we should see\nthose in action sooner rather than later.\n\nI run multiple full nodes, including one with Bitcoin XT and I don't\nwant to see Bitcoin XT and Bitcoin Core divide into different consensus\nand create 2 altcoins instead of one Bitcoin.\n\nOn 5/31/2015 3:29 AM, Matt Whitlock wrote:\n> Greg, Pieter, Jeff, and Wladimir,\n> \n> I'll try to be brief to respect your time.\n> \n> 1. I don't want to see Bitcoin die.\n> \n> 2. As has been discussed on this list and elsewhere: Bitcoin could potentially die due to economic and/or game-theoretic complications arising from raising the block size limit, but Bitcoin could also die due to usability complications arising from NOT raising the block size limit. Strong, personally held opinions by various members of this community notwithstanding, it is not clear which of these scenarios is more likely.\n> \n> 3. What *is* clear at this point is that Gavin will move ahead with his proposal, regardless of whether the remainder of the Bitcoin Core committers agree with him. If he has to commit his changes to Bitcoin XT and then rally the miners to switch, then that's what he'll do. He believes that he is working in the best interests of Bitcoin (as I would hope we all do), and so I do not fault him for his intentions. However, I think his proposal is too risky.\n> \n> 4. I also think that ignoring the immediate problem is too risky. If allowing significantly larger blocks will cause a serious problem for Bitcoin (which is a possibility that we cannot rule out, as we lack omniscience), then NOT making any change to Bitcoin Core will virtually *assure* that we cause exactly this problem, as the popular (non-technical) consensus appears to be in favor of Bitcoin XT and a larger block size limit. If we do nothing, then there's a very real chance that Bitcoin XT takes over, for better or worse.\n> \n> 5. I'd like to propose a way that we can have our cake and eat it too. My proposal attempts to satisfy both those who want larger blocks AND those who want to be extremely cautious about changing the fundamental economic parameters of Bitcoin.\n> \n> 6. Something I've never understood about Gavin's (et al.) proposal is why there is a massive step right up front. Assuming we accept his argument that we're critically close to running out of capacity, I still must ask: why do we need a 20x increase all at once?\n> \n> 7. It's not a given that blocks will immediately expand to meet the hard limit. In fact, there are strong and compelling arguments why this will NOT happen. But in any software system, if a given scenario is *possible*, then one MUST assume that it will happen and must have a plan to handle it.\n> \n> 8. My primary objection is not to raising the block size limit; my objection is to raising it *suddenly*. You can argue that, because we'll have plenty of time before March 2016, it's not \"sudden,\" but, whether we do it now or a year from now or a decade from now, a step function is, by definition, sudden.\n> \n> 9. My proposal is that we raise the block size limit *gradually*, using an approximately smooth function, without a step discontinuity. We can employ a linear growth function to adjust the block size limit *smoothly* from 1 MB to 20 MB over the course of several years, beginning next March.\n> \n> 10. This is the difference between cannonballing into the deep end of the pool and walking gingerly down the steps into the shallow end. Both get you to the eventual goal, but one is reckless while the other is measured and deliberate. If there's a problem that larger blocks will enable, then I'd prefer to see the problem crop up gradually rather than all at once. If it's gradual, then we'll have time to discuss and fix it without panicking.\n> \n> 11. I am offering to implement this proposal and submit a pull request to Bitcoin Core. However, if another dev who is more familiar with the internals would like to step forward, then that would be superior.\n> \n> Respectfully submitted,\n> Matt Whitlock\n> \n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Btc Drak",
                "date": "2015-05-31T09:35:20",
                "message_text_only": "On Sun, May 31, 2015 at 1:29 AM, Matt Whitlock <bip at mattwhitlock.name>\nwrote:\n\n> 3. What *is* clear at this point is that Gavin will move ahead with his\n> proposal, regardless of whether the remainder of the Bitcoin Core\n> committers agree with him. If he has to commit his changes to Bitcoin XT\n> and then rally the miners to switch, then that's what he'll do. He believes\n> that he is working in the best interests of Bitcoin (as I would hope we all\n> do), and so I do not fault him for his intentions. However, I think his\n> proposal is too risky.\n>\n\nI seriously doubt if miners and merchants who's income depends on bitcoin\nare going to risk a network split. Gavin isn't pedalling some mempool\npolicy which doesn't affect consensus. The changes have to be universally\nadopted by miners and full nodes. If there is any uncertainty about that\nglobal acceptance, those financially dependent on bitcoin will not take the\nrisk just to be political. You can see how conservative the mining\ncommunity is already by their slow upgrade of Bitcoin Core as it is. Even\nif some miners and merchants generally support the idea of bigger blocks,\nthey most certainly are not going to take the risk of leading a hard fork\nwhen there is substantial risk of it failing.\n\nUntil there is actual consensus among the technical community I wouldn't be\ntoo concerned.\n\n\n> 4. I also think that ignoring the immediate problem is too risky. If\n> allowing significantly larger blocks will cause a serious problem for\n> Bitcoin (which is a possibility that we cannot rule out, as we lack\n> omniscience), then NOT making any change to Bitcoin Core will virtually\n> *assure* that we cause exactly this problem, as the popular (non-technical)\n> consensus appears to be in favor of Bitcoin XT and a larger block size\n> limit. If we do nothing, then there's a very real chance that Bitcoin XT\n> takes over, for better or worse.\n>\n\nI don't think anyone is ignoring the issues, nor that everyone accepts that\nblocksize may have to eventually change. The overwhelming technical\nmajority do not agree there is a problem that needs to be immediately\naddressed. It would be far more helpful if we focused on stuff that helps\nenable level 2 technologies so that bitcoin can actually scale, (like\nR/CLTV and malleability fixes which are being delayed by BIP66 rollout and\npending the new \"concurrent soft-forks\" proposal).\n\n\n> 7. It's not a given that blocks will immediately expand to meet the hard\n> limit. In fact, there are strong and compelling arguments why this will NOT\n> happen. But in any software system, if a given scenario is *possible*, then\n> one MUST assume that it will happen and must have a plan to handle it.\n>\n\nBut of course it would be dealt with if and when it becomes necessary. It's\nnot like there is blanket opposition to increasing the blocksize ever, it's\nthe matter of if, when and how; but when is defintely not now.\n\n9. My proposal is that we raise the block size limit *gradually*, using an\n> approximately smooth function, without a step discontinuity. We can employ\n> a linear growth function to adjust the block size limit *smoothly* from 1\n> MB to 20 MB over the course of several years, beginning next March.\n>\n\nAutomatic or dynamic blocksize increase risks being very difficult to shut\ndown if later we find it is negatively impacting the ecosystem... and\nthat's part of the reluctance with bigger blocks because we still have not\nstudied the potential downsides enough beyond some sketchy and disputed\ncalculations and overall it's not addressing scalability at all.\n\n\n> 10. This is the difference between cannonballing into the deep end of the\n> pool and walking gingerly down the steps into the shallow end. Both get you\n> to the eventual goal, but one is reckless while the other is measured and\n> deliberate. If there's a problem that larger blocks will enable, then I'd\n> prefer to see the problem crop up gradually rather than all at once. If\n> it's gradual, then we'll have time to discuss and fix it without panicking.\n\n\nExtending blocksize now would be nothing more than a political move. I have\nno idea what will be decided in the end, but I do know that in order for\nbitcoin to survive, changes must be based on well thought out and discussed\ntechnical merits and not the result of political pressure. Politics and\ngood software do not mix.\n\nDrak\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/c680bfb1/attachment.html>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2015-05-31T10:01:53",
                "message_text_only": "Drak,\n\nI mostly agree with your assessment...except for your last claim.\n\nNot that I wouldn't like to find a way to avoid politics, but like I've\nargued before, it is inevitable that sooner or later any consensus protocol\nthat seeks dynamism will encounter politics.\n\nThe block size discussion, while ultimately necessary, for now is in the\nbest case merely serving as an example of the kind of political issues we\n*really* need to be finding some solution for...and in the worst case is a\ndistraction and evasion.\n\nSome protocol updates will be merely technical optimizations or feature\nenhancements that are fairly uncontroversial...but some will inevitably be\nhighly controversial with real-world economic consequences, winners and\nlosers. We lack a process for deciding these issues. No matter how\nsophistocated we make the protocol, somethings will inevitably require\nexternal input to make these issues decidable...it is a Goedelian\nimplication. This external input could be some sort of vote (of which\nhashing power is a particular kind) or perhaps something else.\n\nThere's something to be said for building the dynamics of hard forks *into*\nour model rather than avoiding it at all costs.  However, forks are the\neasy part. The difficulty is in merging different branches. Perhaps we\nshould learn a thing or two from git. Perhaps the question we should be\nasking is not \"how do we avoid hard forks\" but \"how can we design the\nnetwork to allow for merging?\"\n\n- Eric Lombrozo\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/142f646d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: A measured response to save Bitcoin Core",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Matt Whitlock",
                "Eric Lombrozo",
                "s7r",
                "Btc Drak"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 15212
        }
    },
    {
        "title": "[Bitcoin-development] Fwd:  Block Size Increase Requirements",
        "thread_messages": [
            {
                "author": "Chun Wang",
                "date": "2015-05-31T01:31:45",
                "message_text_only": "On Sat, May 30, 2015 at 9:57 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n>> Bad miners could attack us and the network with artificial\n>> big blocks.\n>\n>\n> How?\n>\n> I ran some simulations, and I could not find a network topology where a big\n> miner producing big blocks could cause a loss of profit to another miner\n> (big or small) producing smaller blocks:\n>\n> http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners\n>\n> (the 0.3% advantage I DID find was for the situation where EVERYBODY was\n> producing big blocks).\n\nIf someone propagate a 20MB block, it will take at best 6 seconds for\nus to receive to verify it at current configuration, result of one\npercent orphan rate increase. Or, we can mine the next block only on\nthe previous block's header, in this case, the network would see many\nmore transaction-less blocks.\n\nOur orphan rate is about 0.5% over the past few months. If the network\nfloods 20MB blocks, it can be well above 2%. Besides bandwidth, A 20MB\nblock could contain an average of 50000 transactions, hundred of\nthousands of sigops, Do you have an estimate how long it takes on the\nsubmitblock rpccall?\n\nFor references, our 30Mbps bandwidth in Beijing costs us 1350 dollars\nper month. We also use Aliyun and Linode cloud services for block\npropagation. As of May 2015, the price is 0.13 U.S. dollars per GB for\n100Mbps connectivity at Aliyun. For a single cross-border TCP\nconnection, it would be certainly far slower than 12.5 MB/s.\n\nI think we can accept 5MB block at most.\n\n(sorry forgot to cc to the mailing list)"
            }
        ],
        "thread_summary": {
            "title": "Fwd:  Block Size Increase Requirements",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Chun Wang"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1566
        }
    },
    {
        "title": "[Bitcoin-development] Fwd: Block Size Increase Requirements",
        "thread_messages": [
            {
                "author": "Pindar Wong",
                "date": "2015-05-31T02:20:42",
                "message_text_only": "Thank you very much Chun Wang for the details below.\n\nWhile I'm based in HK, but I'd like to propose that the miners in China\nwork together with Gavin and others to run an experiment of sorts next\nmonth to gather more details for the community.\n\np.\n\n\n\n\nOn Sun, May 31, 2015 at 9:31 AM, Chun Wang <1240902 at gmail.com> wrote:\n\n> On Sat, May 30, 2015 at 9:57 PM, Gavin Andresen <gavinandresen at gmail.com>\n> wrote:\n> >> Bad miners could attack us and the network with artificial\n> >> big blocks.\n> >\n> >\n> > How?\n> >\n> > I ran some simulations, and I could not find a network topology where a\n> big\n> > miner producing big blocks could cause a loss of profit to another miner\n> > (big or small) producing smaller blocks:\n> >\n> > http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners\n> >\n> > (the 0.3% advantage I DID find was for the situation where EVERYBODY was\n> > producing big blocks).\n>\n> If someone propagate a 20MB block, it will take at best 6 seconds for\n> us to receive to verify it at current configuration, result of one\n> percent orphan rate increase. Or, we can mine the next block only on\n> the previous block's header, in this case, the network would see many\n> more transaction-less blocks.\n>\n> Our orphan rate is about 0.5% over the past few months. If the network\n> floods 20MB blocks, it can be well above 2%. Besides bandwidth, A 20MB\n> block could contain an average of 50000 transactions, hundred of\n> thousands of sigops, Do you have an estimate how long it takes on the\n> submitblock rpccall?\n>\n> For references, our 30Mbps bandwidth in Beijing costs us 1350 dollars\n> per month. We also use Aliyun and Linode cloud services for block\n> propagation. As of May 2015, the price is 0.13 U.S. dollars per GB for\n> 100Mbps connectivity at Aliyun. For a single cross-border TCP\n> connection, it would be certainly far slower than 12.5 MB/s.\n>\n> I think we can accept 5MB block at most.\n>\n> (sorry forgot to cc to the mailing list)\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/0a109f73/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T12:40:35",
                "message_text_only": "On Sat, May 30, 2015 at 9:31 PM, Chun Wang <1240902 at gmail.com> wrote:\n>\n> If someone propagate a 20MB block, it will take at best 6 seconds for\n> us to receive to verify it at current configuration, result of one\n> percent orphan rate increase.\n\n\nThat orphan rate increase will go to whoever is producing the 20MB blocks,\nNOT you.\n\n\nOr, we can mine the next block only on\n> the previous block's header, in this case, the network would see many\n> more transaction-less blocks.\n>\n\nAre you sure that is the best strategy? If a big block is slow to\npropagate, I suspect it will be better to punish the miner that created it\nby refusing to build on it until it has been fully validated.\n\nI'll try to find time to run a couple of simulations.\n\n\n\n>\n> Our orphan rate is about 0.5% over the past few months. If the network\n> floods 20MB blocks, it can be well above 2%. Besides bandwidth, A 20MB\n> block could contain an average of 50000 transactions, hundred of\n> thousands of sigops, Do you have an estimate how long it takes on the\n> submitblock rpccall?\n>\n\nI can benchmark it. It should be pretty fast, and sipa has a couple of\npatches pending to make the UTXO cache much faster.\n\nIt can be fast because the vast majority of the work of validating all\nthose transactions can happen as they are received into the memory pool.\n\n\n> For references, our 30Mbps bandwidth in Beijing costs us 1350 dollars\n> per month.\n\n\nYou should be able to handle 20MB blocks no problem; if I round up to 100MB\nper block that works out to 1.3Mbps.\n\nWe also use Aliyun and Linode cloud services for block\n> propagation. As of May 2015, the price is 0.13 U.S. dollars per GB for\n> 100Mbps connectivity at Aliyun.\n\n\nThat speed will handle 20MB blocks no problem.\n\nIf each 20MB block is 100MB of data up/down the wire (I'm vastly\nover-estimating, after optimization it should be 40MB) then you'll be\npaying...uhhh:\n\n0.1 GB / block-data-on-wire * 144 blocks/day * 30.5 days/month * 0.13 $ /\nGB = $57\n\nLess than $2 per day in bandwidth, surely you can afford that.\n\n\n> For a single cross-border TCP\n> connection, it would be certainly far slower than 12.5 MB/s.\n\n\nThat's OK, you'll 1.3Mbps or less.\n\n\n> I think we can accept 5MB block at most.\n>\n\nAre you worried about paying too much, or do 20MB blocks \"feel like too\nmuch\" ?\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/41d199dc/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-31T13:45:25",
                "message_text_only": "> That orphan rate increase will go to whoever is producing the 20MB blocks,\n> NOT you.\n>\n\nThis depends on how miners are connected.\n\nE.g. suppose there are three miners, A and B have fast connectivity between\nthen, and C has a slow network.\nSuppose that A miners a block and B receives it in 1 second. C receives it\nin 6 seconds.\nThis means that blocks mined by C during these ~5 seconds will be orphaned\nbecause B gets A's block first.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/500f47c7/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T14:54:10",
                "message_text_only": "On Sun, May 31, 2015 at 9:45 AM, Alex Mizrahi <alex.mizrahi at gmail.com>\nwrote:\n\n>\n>\n>> That orphan rate increase will go to whoever is producing the 20MB\n>> blocks, NOT you.\n>>\n>\n> This depends on how miners are connected.\n>\n> E.g. suppose there are three miners, A and B have fast connectivity\n> between then, and C has a slow network.\n> Suppose that A miners a block and B receives it in 1 second. C receives it\n> in 6 seconds.\n> This means that blocks mined by C during these ~5 seconds will be orphaned\n> because B gets A's block first.\n>\n\nYes, if you are on a slow network then you are at a (slight) disadvantage.\nSo?\n\nThere are lots of equations that go into the \"is mining profitable\"\nequation: cost of power, Internet cost and connectivity, cost of capital,\naccess to technology other miners don't have, inexpensive labor or rent,\ninexpensive cooling, ability to use waste heat...\n\nThat's good. An equation with lots of variables has lots of different\nmaximum solutions, and that means better decentralization -- there is less\nlikely to be one perfect place or way to mine.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/11fb2d62/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2015-05-31T22:55:06",
                "message_text_only": ">\n> Yes, if you are on a slow network then you are at a (slight) disadvantage.\n> So?\n>\n\nChun mentioned that his pool is on a slow network, and thus bigger blocks\ngive it an disadvantage. (Orphan rate is proportional to block size.)\nYou said that no, on contrary those who make big blocks have a disadvantage.\nAnd now you say that yes, this disadvantage exist.\n\nDid you just lie to Chun?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150601/28766322/attachment.html>"
            },
            {
                "author": "Ricardo Filipe",
                "date": "2015-05-31T23:23:23",
                "message_text_only": "He also said that the equation for miners has many variables, as it\nshould. There is no disadvantage if the network speed is the same\nbetween the miners. If there is a difference in network speed, the\nminer is incentivized to invest in their network infrastructure.\n\n2015-05-31 23:55 GMT+01:00 Alex Mizrahi <alex.mizrahi at gmail.com>:\n>> Yes, if you are on a slow network then you are at a (slight) disadvantage.\n>> So?\n>\n>\n> Chun mentioned that his pool is on a slow network, and thus bigger blocks\n> give it an disadvantage. (Orphan rate is proportional to block size.)\n> You said that no, on contrary those who make big blocks have a disadvantage.\n> And now you say that yes, this disadvantage exist.\n>\n> Did you just lie to Chun?\n>\n>\n> ------------------------------------------------------------------------------\n>\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Pindar Wong",
                "date": "2015-05-31T23:40:30",
                "message_text_only": "On Mon, Jun 1, 2015 at 7:23 AM, Ricardo Filipe <ricardojdfilipe at gmail.com>\nwrote:\n\n> He also said that the equation for miners has many variables, as it\n> should. There is no disadvantage if the network speed is the same\n> between the miners.\n\n\nHi,\n\nIs that an assumption?\n\nIf there is a difference in network speed, the\n> miner is incentivized to invest in their network infrastructure.\n>\n\nPerhaps it's best not to  assume that investing in Internet network\ninfrastructure's a free or open market everywhere.\n\np.\n\n\n>\n> 2015-05-31 23:55 GMT+01:00 Alex Mizrahi <alex.mizrahi at gmail.com>:\n> >> Yes, if you are on a slow network then you are at a (slight)\n> disadvantage.\n> >> So?\n> >\n> >\n> > Chun mentioned that his pool is on a slow network, and thus bigger blocks\n> > give it an disadvantage. (Orphan rate is proportional to block size.)\n> > You said that no, on contrary those who make big blocks have a\n> disadvantage.\n> > And now you say that yes, this disadvantage exist.\n> >\n> > Did you just lie to Chun?\n> >\n> >\n> >\n> ------------------------------------------------------------------------------\n> >\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150601/913e7526/attachment.html>"
            },
            {
                "author": "Ricardo Filipe",
                "date": "2015-05-31T23:58:13",
                "message_text_only": "2015-06-01 0:40 GMT+01:00 Pindar Wong <pindar.wong at gmail.com>:\n>\n>\n> On Mon, Jun 1, 2015 at 7:23 AM, Ricardo Filipe <ricardojdfilipe at gmail.com>\n> wrote:\n>>\n>> He also said that the equation for miners has many variables, as it\n>> should. There is no disadvantage if the network speed is the same\n>> between the miners.\n>\n>\n> Hi,\n>\n> Is that an assumption?\nno, let me rephrase: The disadvantage alex refers to only exists if\nminers do not have the same network speed.\n\n>\n>> If there is a difference in network speed, the\n>> miner is incentivized to invest in their network infrastructure.\n>\n>\n> Perhaps it's best not to  assume that investing in Internet network\n> infrastructure's a free or open market everywhere.\nJust like easy ASIC access, low price electricity, etc are not a free\nand open market.\n\n>\n> p.\n>\n>>\n>>\n>> 2015-05-31 23:55 GMT+01:00 Alex Mizrahi <alex.mizrahi at gmail.com>:\n>> >> Yes, if you are on a slow network then you are at a (slight)\n>> >> disadvantage.\n>> >> So?\n>> >\n>> >\n>> > Chun mentioned that his pool is on a slow network, and thus bigger\n>> > blocks\n>> > give it an disadvantage. (Orphan rate is proportional to block size.)\n>> > You said that no, on contrary those who make big blocks have a\n>> > disadvantage.\n>> > And now you say that yes, this disadvantage exist.\n>> >\n>> > Did you just lie to Chun?\n>> >\n>> >\n>> >\n>> > ------------------------------------------------------------------------------\n>> >\n>> > _______________________________________________\n>> > Bitcoin-development mailing list\n>> > Bitcoin-development at lists.sourceforge.net\n>> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>> >\n>>\n>>\n>> ------------------------------------------------------------------------------\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T12:52:45",
                "message_text_only": "On Sat, May 30, 2015 at 9:31 PM, Chun Wang <1240902 at gmail.com> wrote:\n>\n> If someone propagate a 20MB block, it will take at best 6 seconds for\n> us to receive to verify it at current configuration, result of one\n> percent orphan rate increase.\n\n\nThat orphan rate increase will go to whoever is producing the 20MB blocks,\nNOT you.\n\n\nOr, we can mine the next block only on\n> the previous block's header, in this case, the network would see many\n> more transaction-less blocks.\n>\n\nAre you sure that is the best strategy? If a big block is slow to\npropagate, I suspect it will be better to punish the miner that created it\nby refusing to build on it until it has been fully validated.\n\nI'll try to find time to run a couple of simulations.\n\n\n\n>\n> Our orphan rate is about 0.5% over the past few months. If the network\n> floods 20MB blocks, it can be well above 2%. Besides bandwidth, A 20MB\n> block could contain an average of 50000 transactions, hundred of\n> thousands of sigops, Do you have an estimate how long it takes on the\n> submitblock rpccall?\n>\n\nI can benchmark it. It should be pretty fast, and sipa has a couple of\npatches pending to make the UTXO cache much faster.\n\nIt can be fast because the vast majority of the work of validating all\nthose transactions can happen as they are received into the memory pool.\n\n\n> For references, our 30Mbps bandwidth in Beijing costs us 1350 dollars\n> per month.\n\n\nYou should be able to handle 20MB blocks no problem; if I round up to 100MB\nper block that works out to 1.3Mbps.\n\nWe also use Aliyun and Linode cloud services for block\n> propagation. As of May 2015, the price is 0.13 U.S. dollars per GB for\n> 100Mbps connectivity at Aliyun.\n\n\nThat speed will handle 20MB blocks no problem.\n\nIf each 20MB block is 100MB of data up/down the wire (I'm vastly\nover-estimating, after optimization it should be 40MB) then you'll be\npaying...uhhh:\n\n0.1 GB / block-data-on-wire * 144 blocks/day * 30.5 days/month * 0.13 $ /\nGB = $57\n\nLess than $2 per day in bandwidth.\n\n\n> For a single cross-border TCP\n> connection, it would be certainly far slower than 12.5 MB/s.\n\n\nThat's OK, you'll 1.3Mbps or less.\n\n\n> I think we can accept 5MB block at most.\n>\n\nAre you worried about paying too much, or do 20MB blocks \"feel like too\nmuch\" ?\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/79089b53/attachment.html>"
            },
            {
                "author": "Dave Hudson",
                "date": "2015-05-31T14:17:10",
                "message_text_only": "> On 31 May 2015, at 13:52, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> \n> On Sat, May 30, 2015 at 9:31 PM, Chun Wang <1240902 at gmail.com <mailto:1240902 at gmail.com>> wrote:\n> If someone propagate a 20MB block, it will take at best 6 seconds for\n> us to receive to verify it at current configuration, result of one\n> percent orphan rate increase.\n> \n> That orphan rate increase will go to whoever is producing the 20MB blocks, NOT you.\n\nThere's an interesting incentives question if the mining fees ever become large enough to be interesting. Given two potential blocks on which to build then for the best interests of the system we'd want miners to select the block that confirmed the largest number of transactions since that puts less pressure on the network later. This is at odds with the incentives for our would-be block maker though because the incentive for mining would be to use whichever block left the largest potential fees available; that's generally going to be the smaller of the two.\n\nThis, of course, only gets worse as the block reward reduces and fees become the dominant way for miners to be paid (and my hypothesis that eventually this could lead to miners trying to deliberately orphan earlier blocks to \"steal\" fees because the fixed block reward is no longer the dominant part of their income).\n\nWhen coupled with the block propagation delay problem increasing the risk of orphan races I'm pretty sure that this actually leads to miners having an incentive to continually mine smaller blocks, and that's aside from the question of whether smaller blocks will push up fees (which also benefits miners). \n\n\nCheers,\nDave\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/2622fb5b/attachment.html>"
            },
            {
                "author": "Yifu Guo",
                "date": "2015-05-31T14:34:21",
                "message_text_only": "I will abstain on this wrangle of \"when\",\n\nInstead I'd like to address some of the network topology health issues\nthat's been brought up in this debate.\n\nDue to how blocks are being broadcast by miners at the moment, it is not\ndifficult to find the origin node of these blocks. These more influential\norigin nodes are a minority, about <100 out of ~6000, <2%. These data\npoints are important to certain attack vectors. It is highly recommended\nthat pools adopt broadcast logic that rotates broadcasting nodes and\nincrease their node count.. Eloipool has this implanted for those seeking\nto adopt/see it in action in the wild.\n\nChina is a particular worse-case due to the sporadic nature of their\ninternet infrastructure, especially connecting from/to outside of gfw, on a\naverage node-walk I can get up to a 10% difference while I know for a fact\nsome of the nodes shown to be down are up.\n\nIn F2Pool's case, I see 6 replay nodes, I don't know if that's enough or\nthat's all the nodes F2Pool runs, but it may be beneficial to set up\nmulti-homing with shadowsocks over mptcp to increase the stability. also\nsee if you can get a CERNET connection to be part of your rotations since\ntheir backbone is quite good.\n\ncomments, question and grievances welcome.\n\nOn Sat, May 30, 2015 at 9:31 PM, Chun Wang <1240902 at gmail.com> wrote:\n\n> On Sat, May 30, 2015 at 9:57 PM, Gavin Andresen <gavinandresen at gmail.com>\n> wrote:\n> >> Bad miners could attack us and the network with artificial\n> >> big blocks.\n> >\n> >\n> > How?\n> >\n> > I ran some simulations, and I could not find a network topology where a\n> big\n> > miner producing big blocks could cause a loss of profit to another miner\n> > (big or small) producing smaller blocks:\n> >\n> > http://gavinandresen.ninja/are-bigger-blocks-better-for-bigger-miners\n> >\n> > (the 0.3% advantage I DID find was for the situation where EVERYBODY was\n> > producing big blocks).\n>\n> If someone propagate a 20MB block, it will take at best 6 seconds for\n> us to receive to verify it at current configuration, result of one\n> percent orphan rate increase. Or, we can mine the next block only on\n> the previous block's header, in this case, the network would see many\n> more transaction-less blocks.\n>\n> Our orphan rate is about 0.5% over the past few months. If the network\n> floods 20MB blocks, it can be well above 2%. Besides bandwidth, A 20MB\n> block could contain an average of 50000 transactions, hundred of\n> thousands of sigops, Do you have an estimate how long it takes on the\n> submitblock rpccall?\n>\n> For references, our 30Mbps bandwidth in Beijing costs us 1350 dollars\n> per month. We also use Aliyun and Linode cloud services for block\n> propagation. As of May 2015, the price is 0.13 U.S. dollars per GB for\n> 100Mbps connectivity at Aliyun. For a single cross-border TCP\n> connection, it would be certainly far slower than 12.5 MB/s.\n>\n> I think we can accept 5MB block at most.\n>\n> (sorry forgot to cc to the mailing list)\n>\n>\n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n\n\n\n-- \n*Yifu Guo*\n*\"Life is an everlasting self-improvement.\"*\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/6cbb311b/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T14:47:10",
                "message_text_only": "On Sun, May 31, 2015 at 10:34 AM, Yifu Guo <yifu at coinapex.com> wrote:\n\n> comments, question and grievances welcome.\n>\n\nThanks for chiming in with facts, Yifu!\n\nDo you have any real-world data on latency/bandwidth/cost through the gfw ?\nChung Wang's post was very helpful to get away from hypotheticals to \"what\nwould it actually cost.\"\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/69330aa7/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd: Block Size Increase Requirements",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Yifu Guo",
                "Dave Hudson",
                "Ricardo Filipe",
                "Alex Mizrahi",
                "Gavin Andresen",
                "Pindar Wong"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 20546
        }
    },
    {
        "title": "[Bitcoin-development] [Bulk] Re: Fwd: Block Size Increase Requirements",
        "thread_messages": [
            {
                "author": "gb",
                "date": "2015-05-31T13:31:25",
                "message_text_only": "Aren't you calculating bandwidth for a singly-connected node? A \"highly\nconnected\" miner could have 30-100 node connections so you probably need\nto increase your traffic estimates by that factor.\n\nI.e. For 100MB blocks, 30-100 Mbps and $60-$100 per day data costs.\n\n> You should be able to handle 20MB blocks no problem; if I round up to\n> 100MB per block that works out to 1.3Mbps.\n> \n> \n>         We also use Aliyun and Linode cloud services for block\n>         propagation. As of May 2015, the price is 0.13 U.S. dollars\n>         per GB for\n>         100Mbps connectivity at Aliyun.\n> \n> \n> That speed will handle 20MB blocks no problem.\n> \n> \n> If each 20MB block is 100MB of data up/down the wire (I'm vastly\n> over-estimating, after optimization it should be 40MB) then you'll be\n> paying...uhhh:\n> \n> \n> 0.1 GB / block-data-on-wire * 144 blocks/day * 30.5 days/month * 0.13\n> $ / GB = $57\n> \n> \n> Less than $2 per day in bandwidth.\n>  \n>         For a single cross-border TCP\n>         connection, it would be certainly far slower than 12.5 MB/s. \n> \n> \n> That's OK, you'll 1.3Mbps or less\n\n\n> -- \n> --\n> Gavin Andresen\n> \n> ------------------------------------------------------------------------------\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Gavin Andresen",
                "date": "2015-05-31T19:49:05",
                "message_text_only": "On Sun, May 31, 2015 at 9:31 AM, gb <kiwigb at yahoo.com> wrote:\n\n> Aren't you calculating bandwidth for a singly-connected node? A \"highly\n> connected\" miner could have 30-100 node connections so you probably need\n> to increase your traffic estimates by that factor.\n>\n> I.e. For 100MB blocks, 30-100 Mbps and $60-$100 per day data costs.\n>\n\nNo, randomly connected gossip networks (which is what the Bitcoin p2p\nnetwork is) don't work that way, bandwidth is (roughly) O(N) where N is the\nnumber of bytes relayed to everybody.\n\n(it is actually a small multiple of N, because of the overhead of 'inv'\nmessages, and if we ever get really serious about scaling up we'll need to\nfix the protocol to reduce that overhead, but that won't be a problem for\nyears).\n\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/a4940550/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Re: Fwd: Block Size Increase Requirements",
            "categories": [
                "Bitcoin-development",
                "Bulk"
            ],
            "authors": [
                "Gavin Andresen",
                "gb"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2373
        }
    },
    {
        "title": "[Bitcoin-development] Max Block Size: Simple Voting Procedure",
        "thread_messages": [
            {
                "author": "Stephen Morse",
                "date": "2015-05-31T19:04:32",
                "message_text_only": "This is likely very similar to other proposals, but I want to bring voting\nprocedures back into the discussion. The goal here is to create a voting\nprocedure that is as simple as possible to increase the block size limit.\n\nVotes are aggregated over each 2016 block period. Each coinbase transaction\nmay have an output at tx.vout[0] with OP_RETURN data in it of the format:\n\n  OP_RETURN {OP_1 or OP_2}\n\nOP_2 means the miner votes to increase the block size limit. OP_1 means the\nminer votes to not increase the block size limit. *Not including such a\nvote is equivalent to voting to NOT increase the block size. *I first\nthought that not voting should mean that you vote with your block size, but\nthen decided that it would be too gameable by others broadcasting\ntransactions to affect your block size.\n\nIf in a 2016 block round there were more than 1008 blocks that voted to\nincrease the block size limit, then the max block size increases by 500 kb.\nThe votes can start when there is a supermajority of miners signaling\nsupport for the voting procedure.\n\nA few important properties of this simple voting:\n\n   - It's not gameable via broadcasting transactions (assuming miners don't\n   set their votes to be automatic, based on the size of recent blocks).\n   - Miners don't have to bloat their blocks artificially just to place a\n   vote for larger block sizes, and, similarly, don't need to exclude\n   transactions even when they think the block size does not need to be raised.\n   - The chain up until the point that this goes into effect may be\n   interpreted as just lacking votes to increase the block size.\n\nWe can't trust all miners, but we have to trust that >50% of them are\nhonest for the system to work. This system makes it so that altering the\nmaximum block size requires >50% of miners (hash power) to vote to increase\nthe consensus-limit.\n\nThanks for your time. I think this is an important time in Bitcoin's\nhistory. I'm not married to this proposal, but I think it would work. I\nthink a lot of the proposals mentioned on this mailing list would work. I\nthink it's time we just pick one and run with it.\n\nPlease let me know your thoughts. I will start working on a pull request if\nthis receives any support from miners/core devs/community members, unless\nsomeone with more experience volunteers.\n\nBest,\nStephen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20150531/eaca8eaa/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Max Block Size: Simple Voting Procedure",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Stephen Morse"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2509
        }
    }
]