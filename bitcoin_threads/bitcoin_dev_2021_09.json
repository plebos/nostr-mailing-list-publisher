[
    {
        "title": "[bitcoin-dev] Proposal for a few IANA mime-types related to Bitcoin",
        "thread_messages": [
            {
                "author": "Peter D. Gray",
                "date": "2021-09-01T13:39:30",
                "message_text_only": "> ... I tried doing this with \"application/bitcoin-psbt\" back in\n> 2019 but it was not accepted...\n\nThanks for this background.\n\nBased on your experience, we should probably ignore the IANA then,\nand just declare a few useful \"mime types\" (note the quotes) in a\nnew BIP. We can then agree inside the Bitcoin community on their\nusage and meaning.\n\nAnyone want to write that BIP and shepherd it? I can support you\nbut I'd rather write code.\n\n---\n@DocHEX  ||  Coinkite  ||  PGP: A3A31BAD 5A2A5B10\n\nOn Tue, Aug 31, 2021 at 07:46:55PM +0000, Andrew Chow wrote:\n> Hi Peter,\n> \n> It would be nice to have mime types registered for Bitcoin things, but\n> I'm not sure that it will be possible, at least not in the way that we\n> would like. I tried doing this with \"application/bitcoin-psbt\" back in\n> 2019 but it was not accepted. From that attempt, here is what I have\n> learned:\n> \n> There are only a few accepted top level types, so we would not be able\n> to use \"bitcoin\" as the top level (unless you want to submit an RFC to\n> add a \"bitcoin\" top level). Of the available top level types,\n> \"application\" is the most appropriate for Bitcoin.\n> \n> Next is the tree that the mime type should be in. The best would be the\n> Standards tree, but it has some requirements that Bitcoin doesn't really\n> meet. In order to be in the standards tree, the registration must be\n> either associated with an IETF specification (so a RFC) or registered by\n> a recognized standards related organization. Unfortunately the closest\n> thing to a standards organization that Bitcoin has is the BIPs process,\n> and that is not a really a standards organization nor is it recognized\n> by IANA. So in order to register the mimetypes as Standards tree types,\n> we would need to write an RFC, but this could be an independent\n> submission (https://www.rfc-editor.org/about/independent/) rather than\n> IETF-stream submission. I did not continue to pursue this because I\n> didn't have the time.\n> \n> Another alternative would be to use the Vendor tree, but that would\n> prefix the mimetype with \"vnd.\" so it would end up being something like\n> \"application/vnd.bitcoin.psbt\". I did not think this was an reasonable\n> so I did not continue to pursue this avenue.\n> \n> \n> Andrew Chow\n> \n> On 8/31/21 2:27 PM, Peter D. Gray via bitcoin-dev wrote:\n> > Hi list!\n> >\n> > I am proposing to register the following MIME (RFC 2046) media types with the IANA:\n> >\n> >\n> > bitcoin/psbt\n> >\n> >      - aka. a BIP-174 file, in binary\n> >      - does not make any claims about signed/unsigned status; lets leave that to the file\n> >\n> > bitcoin/txn\n> >\n> >      - aka. wire-ready fully-signed transaction in binary\n> >\n> > bitcoin/uri\n> >\n> >      - aka [BIP-21](https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki)\n> >      - could be just a bare bech32 or base58 payment address\n> >      - but can also encode amount, comments in URL args\n> >      - potentially interesting as a response to 402 - Payment required\n> >\n> >\n> > Other thoughts\n> >\n> > - some mime-types are proposed in BIP-71 but those are unrelated to above, and never\n> >    seem to have been registered\n> >\n> > - for those who like to encode their binary as base64 or hex, that can be indicated\n> >    as \"encoding=hex\" or \"encoding=base64\" in the optional parameters, just like\n> >    \"text/plain; encoding=utf-8\" does. However, the default must be binary.\n> >\n> > - although the above are useful for web servers, they are also useful elsewhere and I\n> >    intend to use them in NFC (NDEF records) where a shorter length is critical.\n> >\n> > - I have no idea how easily IANA will accept these proposals.\n> >\n> > - current approved mime types: https://www.iana.org/assignments/media-types/media-types.xhtml\n> >\n> > Thoughts?\n> >\n> > ---\n> > @DocHEX  ||  Coinkite  ||  PGP: A3A31BAD 5A2A5B10\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210901/c6f95528/attachment.sig>"
            },
            {
                "author": "Dr Maxim Orlovsky",
                "date": "2021-09-02T10:52:13",
                "message_text_only": "Hi Peter,\n\nYep, I think it is a good idea just to do new BIP and ignore IANA opinion on the matter. I can write it since I was going to propose pretty much the same thing some time ago. I will share the draft in this mail list in a week.\n\nKind regards,\nMaxim Orlovsky\nLNP/BP Standards Association\nhttp://lnp-bp.org\ngithub.com/dr-orlovsky\n\n-------- Original Message --------\nOn 1 Sep 2021, 15:39, Peter D. Gray via bitcoin-dev wrote:\n\n>> ... I tried doing this with \"application/bitcoin-psbt\" back in\n>> 2019 but it was not accepted...\n>\n> Thanks for this background.\n>\n> Based on your experience, we should probably ignore the IANA then,\n> and just declare a few useful \"mime types\" (note the quotes) in a\n> new BIP. We can then agree inside the Bitcoin community on their\n> usage and meaning.\n>\n> Anyone want to write that BIP and shepherd it? I can support you\n> but I'd rather write code.\n>\n> ---\n> @DocHEX || Coinkite || PGP: A3A31BAD 5A2A5B10\n>\n> On Tue, Aug 31, 2021 at 07:46:55PM +0000, Andrew Chow wrote:\n>> Hi Peter,\n>>\n>> It would be nice to have mime types registered for Bitcoin things, but\n>> I'm not sure that it will be possible, at least not in the way that we\n>> would like. I tried doing this with \"application/bitcoin-psbt\" back in\n>> 2019 but it was not accepted. From that attempt, here is what I have\n>> learned:\n>>\n>> There are only a few accepted top level types, so we would not be able\n>> to use \"bitcoin\" as the top level (unless you want to submit an RFC to\n>> add a \"bitcoin\" top level). Of the available top level types,\n>> \"application\" is the most appropriate for Bitcoin.\n>>\n>> Next is the tree that the mime type should be in. The best would be the\n>> Standards tree, but it has some requirements that Bitcoin doesn't really\n>> meet. In order to be in the standards tree, the registration must be\n>> either associated with an IETF specification (so a RFC) or registered by\n>> a recognized standards related organization. Unfortunately the closest\n>> thing to a standards organization that Bitcoin has is the BIPs process,\n>> and that is not a really a standards organization nor is it recognized\n>> by IANA. So in order to register the mimetypes as Standards tree types,\n>> we would need to write an RFC, but this could be an independent\n>> submission (https://www.rfc-editor.org/about/independent/) rather than\n>> IETF-stream submission. I did not continue to pursue this because I\n>> didn't have the time.\n>>\n>> Another alternative would be to use the Vendor tree, but that would\n>> prefix the mimetype with \"vnd.\" so it would end up being something like\n>> \"application/vnd.bitcoin.psbt\". I did not think this was an reasonable\n>> so I did not continue to pursue this avenue.\n>>\n>>\n>> Andrew Chow\n>>\n>> On 8/31/21 2:27 PM, Peter D. Gray via bitcoin-dev wrote:\n>> > Hi list!\n>> >\n>> > I am proposing to register the following MIME (RFC 2046) media types with the IANA:\n>> >\n>> >\n>> > bitcoin/psbt\n>> >\n>> > - aka. a BIP-174 file, in binary\n>> > - does not make any claims about signed/unsigned status; lets leave that to the file\n>> >\n>> > bitcoin/txn\n>> >\n>> > - aka. wire-ready fully-signed transaction in binary\n>> >\n>> > bitcoin/uri\n>> >\n>> > - aka [BIP-21](https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki)\n>> > - could be just a bare bech32 or base58 payment address\n>> > - but can also encode amount, comments in URL args\n>> > - potentially interesting as a response to 402 - Payment required\n>> >\n>> >\n>> > Other thoughts\n>> >\n>> > - some mime-types are proposed in BIP-71 but those are unrelated to above, and never\n>> > seem to have been registered\n>> >\n>> > - for those who like to encode their binary as base64 or hex, that can be indicated\n>> > as \"encoding=hex\" or \"encoding=base64\" in the optional parameters, just like\n>> > \"text/plain; encoding=utf-8\" does. However, the default must be binary.\n>> >\n>> > - although the above are useful for web servers, they are also useful elsewhere and I\n>> > intend to use them in NFC (NDEF records) where a shorter length is critical.\n>> >\n>> > - I have no idea how easily IANA will accept these proposals.\n>> >\n>> > - current approved mime types: https://www.iana.org/assignments/media-types/media-types.xhtml\n>> >\n>> > Thoughts?\n>> >\n>> > ---\n>> > @DocHEX || Coinkite || PGP: A3A31BAD 5A2A5B10\n>> >\n>> > _______________________________________________\n>> > bitcoin-dev mailing list\n>> > bitcoin-dev at lists.linuxfoundation.org\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210902/cb571cb2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal for a few IANA mime-types related to Bitcoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Peter D. Gray",
                "Dr Maxim Orlovsky"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 9176
        }
    },
    {
        "title": "[bitcoin-dev] Exploring: limiting transaction output amount as a function of total input value",
        "thread_messages": [
            {
                "author": "Zac Greenwood",
                "date": "2021-09-01T15:15:30",
                "message_text_only": "Hi ZmnSCPxj,\n\nThe rate-limiting algorithm would be relatively straightforward. I\ndocumented the rate-limiting part of the algorithm below, perhaps they can\nevoke new ideas of how to make this MAST-able or otherwise implement this\nin a privacy preserving way.\n\nSomething like the following:\n\n=> Create an output at block height [h0] with the following properties:\n\nServing as input at any block height, the maximum amount is limited to\n[limit] sats;  // This rule introduces [limit] and is permanent and always\ncopied over to a change output\nServing as input at a block height < [h0 + window], the maximum amount is\nlimited to [limit - 0] sats;  // [limit - 0] to emphasize that nothing was\nspent yet and no window has started.\n\n=> A transaction occurs at block height [h1], spending [h1_spent].\nThe payment output created at [h1] is not encumbered and of value\n[h1_spent]; // Note, this is the first encumbered transaction so [h1] is\nthe first block of the first window\n\nThe change output created at block height [h1] must be encumbered as\nfollows:\nServing as input at any block height, the maximum amount is limited to\n[limit] sats;  // Permanent rule repeats\nServing as input at a block height < [h1 + window], the maximum amount is\nlimited to [limit - h1_spent]  // Second permanent rule reduces spendable\namount until height [h1 + window] by [h1_spent]\n\n=> A second transaction occurs at block height [h2], spending [h2_spent].\nThe payment output created at [h2] is not encumbered and of value\n[h2_spent]; // Second transaction, so a second window starts at [h2]\n\nThe change output created at block height [h2] must be encumbered as\nfollows:\nServing as input at any block height, the maximum amount is limited to\n[limit] sats;  // Permanent rule repeats\nServing as input at a block height < [h1 + window], the max amount is\nlimited to [limit - h1_spent - h2_spent] // Reduce spendable amount between\n[h1] and [h1 + window] by an additional [h2_spent]\nServing as input in range [h1 + window] <= block height < [h2 + window],\nthe max amount is limited to [limit - h2_spent]  // First payment no longer\ninside this window so [h1_spent] no longer subtracted\n\n... and so on. A rule that pertains to a block height < the current block\nheight can be abandoned, keeping the number of rules equal to the number of\ntransactions that exist within the oldest still active window.\n\nZac\n\n\nOn Tue, Aug 31, 2021 at 4:22 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Zac,\n>\n> > Hi ZmnSCPxj,\n> >\n> > Thank you for your helpful response. We're on the same page concerning\n> privacy so I'll focus on that. I understand from your mail that privacy\n> would be reduced by this proposal because:\n> >\n> > * It requires the introduction of a new type of transaction that is\n> different from a \"standard\" transaction (would that be P2TR in the\n> future?), reducing the anonymity set for everyone;\n> > * The payment and change output will be identifiable because the change\n> output must be marked encumbered on-chain;\n> > * The specifics of how the output is encumbered must be visible on-chain\n> as well reducing privacy even further.\n> >\n> > I don't have the technical skills to judge whether these issues can\n> somehow be resolved. In functional terms, the output should be spendable in\n> a way that does not reveal that the output is encumbered, and produce a\n> change output that cannot be distinguished from a non-change output while\n> still being encumbered. Perhaps some clever MAST-fu could somehow help?\n>\n> I believe some of the covenant efforts may indeed have such clever MAST-fu\n> integrated into them, which is why I pointed you to them --- the people\n> developing these (aj I think? RubenSomsen?) might be able to accommodate\n> this or some subset of the desired feature in a sufficiently clever\n> covenant scheme.\n>\n> There are a number of such proposals, though, so I cannot really point you\n> to one that seems likely to have a lot of traction.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210901/cf80c2eb/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Exploring: limiting transaction output amount as a function of total input value",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Zac Greenwood"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4167
        }
    },
    {
        "title": "[bitcoin-dev] Braidpool: Proposal for a decentralised mining pool",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2021-09-02T06:46:55",
                "message_text_only": "How would you compare this to Stratum v2?\n\nOn Sun, Aug 29, 2021 at 1:02 AM pool2win via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> We have been working on a peer to peer mining pool that overcomes the\n> problems faced by P2Pool and enables building a futures market for\n> hashrate.\n>\n> The proposal can be found here:\n> https://github.com/pool2win/braidpool/raw/main/proposal/proposal.pdf\n>\n> The key features of the pool are:\n>\n> 1. Lower variance for smaller miners, even when large miners join\n>   the pool.\n> 2. Miners build their own blocks, just like in P2Pool.\n> 3. Payouts require a constant size blockspace, independent of the\n>   number of miners in the pool.\n> 4. Provide building blocks for enabling a futures market of hash\n>   rates.\n>\n> Braidpool: Decentralised Mining Pool for Bitcoin\n>\n> Abstract. Bitcoin P2Pool's usage has steadily declined over the years,\n> negatively impacting bitcoin's decentralisation. The variance in\n> earnings for miners increases with total hashrate participating in\n> P2Pool, and payouts require a linearly increasing block space with the\n> number of miners participating in the pool. We present a solution that\n> uses a DAG of shares replicated at all miners. The DAG is then used to\n> compute rewards for miners. Rewards are paid out using one-way payment\n> channels by an anonymous hub communicating with the miners using Tor's\n> hidden services. Using the payment channels construction, neither the\n> hub nor the miners can cheat.\n>\n> Full proposal at\n> https://github.com/pool2win/braidpool/raw/main/proposal/proposal.pdf\n>\n> Details on trading hashrate are here:\n>\n> https://pool2win.github.io/braidpool/2021/08/18/deliver-hashrate-to-market-makers.html\n>\n> @pool2win\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210901/6bc8a1b5/attachment-0001.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2021-09-06T06:23:41",
                "message_text_only": "On Wed, Sep 01, 2021 at 11:46:55PM -0700, Billy Tetrud via bitcoin-dev wrote:\n> How would you compare this to Stratum v2?\n\nSpecifically, I'd be interested in learning what advantages this has\nover a centralized mining pool using BetterHash or StratumV2 with\npayouts made via LN (perhaps immediately after each submitted share is\nvalidated).\n\n-Dave\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/a2d81ede/attachment.sig>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2021-09-06T07:29:01",
                "message_text_only": "It doesn\u2019t centralize payment, which ultimately controls transaction selection (censorship).\n\ne\n\n> On Sep 6, 2021, at 08:25, David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeffOn Wed, Sep 01, 2021 at 11:46:55PM -0700, Billy Tetrud via bitcoin-dev wrote:\n>> How would you compare this to Stratum v2?\n> \n> Specifically, I'd be interested in learning what advantages this has\n> over a centralized mining pool using BetterHash or StratumV2 with\n> payouts made via LN (perhaps immediately after each submitted share is\n> validated).\n> \n> -Dave\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "David A. Harding",
                "date": "2021-09-06T07:54:30",
                "message_text_only": "On Mon, Sep 06, 2021 at 09:29:01AM +0200, Eric Voskuil wrote:\n> It doesn\u2019t centralize payment, which ultimately controls transaction selection (censorship).\n\nYeah, but if you get paid after each share via LN and you can switch\npools instantly, then the worst case with centralized pools is that \nyou don't get paid for one share.  If the hasher sets their share\ndifficulty low enough, that shouldn't be a big deal.\n\nI'm interested in whether braidpool offers any significant benefits over\nan idealized version of centralized mining with independent transaction\nselection.\n\n-Dave\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/26c8c9d4/attachment-0001.sig>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2021-09-06T08:26:00",
                "message_text_only": "Switching pools has always been possible. But the largest pool is the most profitable, and centralized pools are easily controlled. Decoupling selection without decoupling payout is an engineering change without a pooling pressure change.\n\ne\n\n> On Sep 6, 2021, at 10:01, David A. Harding <dave at dtrt.org> wrote:\n> \n> \ufeffOn Mon, Sep 06, 2021 at 09:29:01AM +0200, Eric Voskuil wrote:\n>> It doesn\u2019t centralize payment, which ultimately controls transaction selection (censorship).\n> \n> Yeah, but if you get paid after each share via LN and you can switch\n> pools instantly, then the worst case with centralized pools is that\n> you don't get paid for one share.  If the hasher sets their share\n> difficulty low enough, that shouldn't be a big deal.\n> \n> I'm interested in whether braidpool offers any significant benefits over\n> an idealized version of centralized mining with independent transaction\n> selection.\n> \n> -Dave"
            },
            {
                "author": "pool2win",
                "date": "2021-09-06T09:03:06",
                "message_text_only": "I see Braidpool as an improvement to P2Pool - i.e. make a peer to peer pool work at scale.\n\nThis is in contrast to Stratum v2, which brings some very good and much needed engineering improvements to centralised pools.\n\nSpecifically about transaction selection in Stratum V2, as far as I understand, the pool still controls both accepting the proposed block and also as Eric says, they still could refuse payouts. Here's a quote from the Stratum V2 docs[1]:\n\n\"The name Job \u2018Negotiation\u2019 Protocol is telling, as job selection is indeed a negotiation process between a miner and a pool. The miner proposes a block template, and it is up to a pool to accept or reject it.\"\n\nAs David says, a miner is free to hop pools, but generally pool hopping can be detrimental to a pool [2].\n\nFurther still, the immediate payouts to miners will work if they opt for PPS. But most centralised pools still use PPLNS(*) or equivalent.\n\nI'd like to highlight an additional problem with centralised pools using PPLNS. These pools are opaque, at least to smaller miners, who can't view the shares received by the pool. Miners are forced to simply trust centralised pools to be honest and compute rewards fairly. A bug in their share tracking or reward calculation protocol could go unnoticed for a long time.\n\nWith Braidpool you get:\n1. Transparent view of the shares received by the pool - thus have the ability to verify reward calculation, even with a PPLNS like scheme. This is the same advantage as P2Pool.\n2. Payouts over one-way channel, so we don't consume block space for miner rewards payouts. This is different from P2Pool.\n3. Using the transparent view of shares, we can build delivery of such shares to market makers providing futures contracts for hashrate. This is nigh impossible with opaque centralised pools.\n4. We prepare for any attacks on centralised mining pools in the future - which we want to keep as the central aim of Braidpool. All the other advantages attract miners to Braidpool now, while preparing our defense against future attacks.\n\n[1] Stratum V2: https://braiins.com/stratum-v2\n[2] Analysis of Bitcoin Pooled Mining Reward Systems: https://arxiv.org/abs/1112.4980\n\n(*) Starting a new PPS based pool requires a lot of funds. The probability of bankruptcy for pools providing PPS is pretty high.\n\n---------- Original Message ----------\nOn Mon, September 6, 2021 at 8:01 AM,  David A. Harding via bitcoin-dev<bitcoin-dev at lists.linuxfoundation.org> wrote:\nOn Mon, Sep 06, 2021 at 09:29:01AM +0200, Eric Voskuil wrote:\n> It doesn\u2019t centralize payment, which ultimately controls transaction selection (censorship).\n\nYeah, but if you get paid after each share via LN and you can switch\npools instantly, then the worst case with centralized pools is that \nyou don't get paid for one share.  If the hasher sets their share\ndifficulty low enough, that shouldn't be a big deal.\n\nI'm interested in whether braidpool offers any significant benefits over\nan idealized version of centralized mining with independent transaction\nselection.\n\n-Dave\n _______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Prayank",
                "date": "2021-09-06T10:15:17",
                "message_text_only": ">\u00a0How would you compare this to Stratum v2?\n\nStratum v2 will help miners with encryption, broadcasting new blocks, signalling bits, choose transactions set, however the mining pools can still reject negotiations and censor payments.\n\nMaybe Stratum v2 can be used in combination with other things like discreet log contracts:\u00a0https://mailmanlists.org/pipermail/dlc-dev/2021-May/000073.html\n\nI think Braidpool does this in a better way.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210906/f0ab2026/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-09-07T23:38:42",
                "message_text_only": "Good morning all,\n\nA thing I just realized about Braidpool is that the payout server is still a single central point-of-failure.\n\nAlthough the paper claims to use Tor hidden service to protect against DDoS attacks, its centrality still cannot protect against sheer accident.\nWhat happens if some clumsy human (all humans are clumsy, right?) fumbles the cables in the datacenter the hub is hosted in?\nWhat happens if the country the datacenter is in is plunged into war or anarchy, because you humans love war and chaos so much?\nWhat happens if Zeus has a random affair (like all those other times), Hera gets angry, and they get into a domestic, and then a random thrown lightning bolt hits the datacenter the hub is in?\n\nThe paper relies on economic arguments (\"such an action will end the pool and the stream of future profits for the hub\"), but economic arguments tend to be a lot less powerful in a monopoly, and the hub effectively has a monopoly on all Braidpool miners.\nHashers might be willing to tolerate minor peccadilloes of the hub, simply to let the pool continue (their other choices would be even worse).\n\nSo it seems to me that it would still be nicer, if it were at all possible, to use multiple hubs.\nI am uncertain how easily this can be done.\n\nPerhaps a Lightning model can be considered.\nMultiple hubs may exist which offer liquidity to the Braidpool network, hashers measure uptime and timeliness of payouts, and the winning hasher elects one of the hubs.\nThe hub gets paid on the coinbase, and should send payouts, minus fees, on the LN to the miners.\n\nHowever, this probably complicates the design too much, and it may be more beneficial to get *something* working now.\nLet not the perfect be the enemy of the good.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "pool2win",
                "date": "2021-09-08T10:03:05",
                "message_text_only": "> A thing I just realized about Braidpool is that the payout server is still a single central point-of-failure.\n\n> However, this probably complicates the design too much, and it may be more beneficial to get *something* working now.\n\nYou have hit the nail on the head here and Chris Belcher's original proposal for using payment channels does provide a construction for multiple hubs [1]. In the Braidpool proposal however, the focus is on a single hub to describe the plan for an MVP.\n\nDecentralising hubs is the end goal here, and either Belcher's multiple hubs construction or a leadership election based construction along the lines you propose might be a good way forward. Belcher's idea has the added advantage that the required liquidity at each hub is reduced as more hubs join, with the cost that in case of a hubs defecting, it takes longer for miners to do cascading close on channels to all hubs. TBH, it might be a cost worth paying in the absence of better ideas. But as braidpool is built, more ideas will be appear as well.\n\n[1] Payment Channel Payouts: An Idea for Improving P2Pool Scalability: https://bitcointalk.org/index.php?topic=2135429.0\n\n---------- Original Message ----------\nOn Tue, September 7, 2021 at 11:39 PM,  ZmnSCPxj via bitcoin-dev<bitcoin-dev at lists.linuxfoundation.org> wrote:\nGood morning all,\n\nA thing I just realized about Braidpool is that the payout server is still a single central point-of-failure.\n\nAlthough the paper claims to use Tor hidden service to protect against DDoS attacks, its centrality still cannot protect against sheer accident.\nWhat happens if some clumsy human (all humans are clumsy, right?) fumbles the cables in the datacenter the hub is hosted in?\nWhat happens if the country the datacenter is in is plunged into war or anarchy, because you humans love war and chaos so much?\nWhat happens if Zeus has a random affair (like all those other times), Hera gets angry, and they get into a domestic, and then a random thrown lightning bolt hits the datacenter the hub is in?\n\nThe paper relies on economic arguments (\"such an action will end the pool and the stream of future profits for the hub\"), but economic arguments tend to be a lot less powerful in a monopoly, and the hub effectively has a monopoly on all Braidpool miners.\nHashers might be willing to tolerate minor peccadilloes of the hub, simply to let the pool continue (their other choices would be even worse).\n\nSo it seems to me that it would still be nicer, if it were at all possible, to use multiple hubs.\nI am uncertain how easily this can be done.\n\nPerhaps a Lightning model can be considered.\nMultiple hubs may exist which offer liquidity to the Braidpool network, hashers measure uptime and timeliness of payouts, and the winning hasher elects one of the hubs.\nThe hub gets paid on the coinbase, and should send payouts, minus fees, on the LN to the miners.\n\nHowever, this probably complicates the design too much, and it may be more beneficial to get *something* working now.\nLet not the perfect be the enemy of the good.\n\nRegards,\nZmnSCPxj\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Filippo Merli",
                "date": "2021-09-10T09:30:31",
                "message_text_only": "Hi!\n\n>From the proposal it is not clear why a miner must reference other miners'\nshares in his shares.\nWhat I mean is that there is a huge incentive for a rogue miner to not\nreference any share from\nother miner so he won't share the reward with anyone, but it will be paid\nfor the share that he\ncreate because good miners will reference his shares.\nThe pool will probably become unprofitable for good miners.\n\nAnother thing that I do not understand is how to resolve conflicts. For\nexample, using figure 1 at\npage 1, a node could be receive this 2 valid states:\n\n1. L -> a1 -> a2 -> a3 -> R\n2. L -> a1* -> a2* -> R\n\nTo resolve the above fork the only two method that comes to my mind are:\n\n1. use the one that has more work\n2. use the longest one\n\nBtw both methods present an issue IMHO.\n\nIf the longest chain is used:\nWhen a block (L) is find, a miner (a) could easily create a lot of share\nwith low difficulty\n(L -> a1* -> a2* -> ... -> an*), then start to mine shares with his real\nhashrate (L -> a1 -> a2)\nand publish them so they get referenced. If someone else finds a block he\ngets the reward cause he\nhas been referenced. If he finds the block he just attaches the funded\nblock to the longest chain\n(that reference no one) and publishes it without sharing the reward\n(L -> a1* -> a2* -> ... -> an* -> R).\n\nIf is used the one with more work:\nA miner that has published the shares (L -> a1 -> a2 -> a3) when find a\nblock R that alone has more\nwork than a1 + a2 + a3 it just publish (L -> R) and he do not share the\nreward with anyone.\n\nOn Wed, Sep 8, 2021 at 1:15 PM pool2win via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > A thing I just realized about Braidpool is that the payout server is\n> still a single central point-of-failure.\n>\n> > However, this probably complicates the design too much, and it may be\n> more beneficial to get *something* working now.\n>\n> You have hit the nail on the head here and Chris Belcher's original\n> proposal for using payment channels does provide a construction for\n> multiple hubs [1]. In the Braidpool proposal however, the focus is on a\n> single hub to describe the plan for an MVP.\n>\n> Decentralising hubs is the end goal here, and either Belcher's multiple\n> hubs construction or a leadership election based construction along the\n> lines you propose might be a good way forward. Belcher's idea has the added\n> advantage that the required liquidity at each hub is reduced as more hubs\n> join, with the cost that in case of a hubs defecting, it takes longer for\n> miners to do cascading close on channels to all hubs. TBH, it might be a\n> cost worth paying in the absence of better ideas. But as braidpool is\n> built, more ideas will be appear as well.\n>\n> [1] Payment Channel Payouts: An Idea for Improving P2Pool Scalability:\n> https://bitcointalk.org/index.php?topic=2135429.0\n>\n> ---------- Original Message ----------\n> On Tue, September 7, 2021 at 11:39 PM,  ZmnSCPxj via bitcoin-dev<\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Good morning all,\n>\n> A thing I just realized about Braidpool is that the payout server is still\n> a single central point-of-failure.\n>\n> Although the paper claims to use Tor hidden service to protect against\n> DDoS attacks, its centrality still cannot protect against sheer accident.\n> What happens if some clumsy human (all humans are clumsy, right?) fumbles\n> the cables in the datacenter the hub is hosted in?\n> What happens if the country the datacenter is in is plunged into war or\n> anarchy, because you humans love war and chaos so much?\n> What happens if Zeus has a random affair (like all those other times),\n> Hera gets angry, and they get into a domestic, and then a random thrown\n> lightning bolt hits the datacenter the hub is in?\n>\n> The paper relies on economic arguments (\"such an action will end the pool\n> and the stream of future profits for the hub\"), but economic arguments tend\n> to be a lot less powerful in a monopoly, and the hub effectively has a\n> monopoly on all Braidpool miners.\n> Hashers might be willing to tolerate minor peccadilloes of the hub, simply\n> to let the pool continue (their other choices would be even worse).\n>\n> So it seems to me that it would still be nicer, if it were at all\n> possible, to use multiple hubs.\n> I am uncertain how easily this can be done.\n>\n> Perhaps a Lightning model can be considered.\n> Multiple hubs may exist which offer liquidity to the Braidpool network,\n> hashers measure uptime and timeliness of payouts, and the winning hasher\n> elects one of the hubs.\n> The hub gets paid on the coinbase, and should send payouts, minus fees, on\n> the LN to the miners.\n>\n> However, this probably complicates the design too much, and it may be more\n> beneficial to get *something* working now.\n> Let not the perfect be the enemy of the good.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210910/00bba50d/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-09-11T01:09:30",
                "message_text_only": "Good morning Filippo,\n\n> Hi!\n>\n> From the proposal it is not clear why a miner must reference other miners' shares in his shares.\n> What I mean is that there is a huge incentive for a rogue miner to not reference any share from\n> other miner so he won't share the reward with anyone, but it will be paid for the share that he\n> create because good miners will reference his shares.\n> The pool will probably become unprofitable for good miners.\n>\n> Another thing that I do not understand is how to resolve conflicts. For example, using figure 1 at\n> page 1, a node could be receive this 2 valid states:\n>\n> 1. L -> a1 -> a2 -> a3 -> R\n> 2. L -> a1* -> a2* -> R\n>\n> To resolve the above fork the only two method that comes to my mind are:\n>\n> 1. use the one that has more work\n> 2. use the longest one\n> Btw both methods present an issue IMHO.\n>\n> If the longest chain is used:\n> When a block (L) is find, a miner (a) could easily create a lot of share with low difficulty\n> (L -> a1* -> a2* -> ... -> an*), then start to mine shares with his real hashrate (L -> a1 -> a2)\n> and publish them so they get referenced. If someone else finds a block he gets the reward cause he\n> has been referenced. If he finds the block he just attaches the funded block to the longest chain\n> (that reference no one) and publishes it without sharing the reward\n> (L -> a1* -> a2* -> ... -> an* -> R).\n>\n> If is used the one with more work:\n> A miner that has published the shares (L -> a1 -> a2 -> a3) when find a block R that alone has more\n> work than a1 + a2 + a3 it just publish (L -> R) and he do not share the reward with anyone.\n\n\nMy understanding from the \"Braid\" in braidpool is that every share can reference more than one previous share.\n\nIn your proposed attack, a single hasher refers only to shares that the hasher itself makes.\n\nHowever, a good hasher will refer not only to its own shares, but also to shares of the \"bad\" hasher.\n\nAnd all honest hashers will be based, not on a single chain, but on the share that refers to the most total work.\n\nSo consider these shares from a bad hasher:\n\n     BAD1 <- BAD2 <- BAD3\n\nA good hasher will refer to those, and also to its own shares:\n\n     BAD1 <- BAD2 <- BAD3\n       ^       ^       ^\n       |       |       |\n       |       |       +------+\n       |       +-----+        |\n       |             |        |\n       +--- GOOD1 <- GOOD2 <- GOOD3\n\n`GOOD3` refers to 5 other shares, whereas `BAD3` refers to only 2 shares, so `GOOD3` will be considered weightier, thus removing this avenue of attack and resolving the issue.\nEven if measured in terms of total work, `GOOD3` also contains the work that `BAD3` does, so it would still win.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Filippo Merli",
                "date": "2021-09-11T07:54:58",
                "message_text_only": ">From my understanding of the posted proposal, a share to get rewarded must\n\"prove\" to be created before the rewarded share. (between L and R)\nIf GOOD3 refers to BAD2 the only thing that I can prove is that BAD2 has\nbeen mined before GOOD2.\n\nSo if  BAD3 is a valid block (I call it R like in the pdf) the only thing\nthat I can certainly know is that between L and R there is BAD1 and BAD2\nand they should be the ones that get the rewards.\nBtw rereading the proposal I'm not sure about how the rewards are\ncalculated, what let me think that is the way that I illustrate above is\nthe figure 2: c3 is referring to a3 but is not included in the first reward.\n\nTo clarify I'm talking about two things in my previous email, the first one\nis: what if a bad miner does not refer to good miners' shares?\nThe second thing is: what if a bad miner publishes two (or more)\nconflicting versions of the DAG?\n\n\nOn Sat, Sep 11, 2021 at 3:09 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Filippo,\n>\n> > Hi!\n> >\n> > From the proposal it is not clear why a miner must reference other\n> miners' shares in his shares.\n> > What I mean is that there is a huge incentive for a rogue miner to not\n> reference any share from\n> > other miner so he won't share the reward with anyone, but it will be\n> paid for the share that he\n> > create because good miners will reference his shares.\n> > The pool will probably become unprofitable for good miners.\n> >\n> > Another thing that I do not understand is how to resolve conflicts. For\n> example, using figure 1 at\n> > page 1, a node could be receive this 2 valid states:\n> >\n> > 1. L -> a1 -> a2 -> a3 -> R\n> > 2. L -> a1* -> a2* -> R\n> >\n> > To resolve the above fork the only two method that comes to my mind are:\n> >\n> > 1. use the one that has more work\n> > 2. use the longest one\n> > Btw both methods present an issue IMHO.\n> >\n> > If the longest chain is used:\n> > When a block (L) is find, a miner (a) could easily create a lot of share\n> with low difficulty\n> > (L -> a1* -> a2* -> ... -> an*), then start to mine shares with his real\n> hashrate (L -> a1 -> a2)\n> > and publish them so they get referenced. If someone else finds a block\n> he gets the reward cause he\n> > has been referenced. If he finds the block he just attaches the funded\n> block to the longest chain\n> > (that reference no one) and publishes it without sharing the reward\n> > (L -> a1* -> a2* -> ... -> an* -> R).\n> >\n> > If is used the one with more work:\n> > A miner that has published the shares (L -> a1 -> a2 -> a3) when find a\n> block R that alone has more\n> > work than a1 + a2 + a3 it just publish (L -> R) and he do not share the\n> reward with anyone.\n>\n>\n> My understanding from the \"Braid\" in braidpool is that every share can\n> reference more than one previous share.\n>\n> In your proposed attack, a single hasher refers only to shares that the\n> hasher itself makes.\n>\n> However, a good hasher will refer not only to its own shares, but also to\n> shares of the \"bad\" hasher.\n>\n> And all honest hashers will be based, not on a single chain, but on the\n> share that refers to the most total work.\n>\n> So consider these shares from a bad hasher:\n>\n>      BAD1 <- BAD2 <- BAD3\n>\n> A good hasher will refer to those, and also to its own shares:\n>\n>      BAD1 <- BAD2 <- BAD3\n>        ^       ^       ^\n>        |       |       |\n>        |       |       +------+\n>        |       +-----+        |\n>        |             |        |\n>        +--- GOOD1 <- GOOD2 <- GOOD3\n>\n> `GOOD3` refers to 5 other shares, whereas `BAD3` refers to only 2 shares,\n> so `GOOD3` will be considered weightier, thus removing this avenue of\n> attack and resolving the issue.\n> Even if measured in terms of total work, `GOOD3` also contains the work\n> that `BAD3` does, so it would still win.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210911/4012ae1d/attachment.html>"
            },
            {
                "author": "pool2win",
                "date": "2021-09-13T08:03:42",
                "message_text_only": "Hi Filippo,\n\nIf a malicious miner, M broadcasts {m1, m2 ... mn} at a regular interval, *and* also broadcasts {m1*, mn*} where mn* is  bitcoin block then M will cheat all other miners of their reward. You correctly identified this attack. The problem stems from the fact that I wanted to use the bitcoin block as the sentinel to mark the shares from the DAG that need to be rewarded. There's a few approaches we can take here, but I think the best one is that the hub broadcasts a \"sentinel\" to mark out the point in logical time up to which shares will be rewarded.\n\nm1* <-------------------- mn*<--------+\n                                      |\nm1 <----m2 <---m3 <-------------------+\n^        ^      ^                     |\n|        |      |                     |\n|        |      +-----------+         |\n|        |                  |         |\n|        +--------+         |    SENTINEL\n+-----+           |         |         |\n      |           |         |         |\n      a1  <------ a2 <-----a3  <------+\n\n\nIn the above diagram, when hub receives mn*, the bitcoin block to be rewarded, the hub has also received {m1...m3, a1...a3} and therefore rewards all those shares and broadcasts this logical time to the p2p by sending a sentinel announcement.\n\nThis solution will also scale to the multiple hubs construction, as each hub will define their own sentinel and the miners working with each hub can independently verify their shares are being correctly rewarded. The solution also handles the case where M is not referencing any other shares.\n\nThe above alternative, might also answer your question about why we need to build a DAG. With a DAG we can capture logical time. Without a DAG, the above solution will require the hub to announce the hash of shares from each miner that have been rewarded.\n\nI really appreciate you taking the time to go through the proposal and pointing out the attack. I hope the above solution addresses your concerns.\n\nThanks and best regards\npool2win"
            }
        ],
        "thread_summary": {
            "title": "Braidpool: Proposal for a decentralised mining pool",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Filippo Merli",
                "Eric Voskuil",
                "pool2win",
                "David A. Harding",
                "Billy Tetrud",
                "ZmnSCPxj",
                "Prayank"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 28245
        }
    },
    {
        "title": "[bitcoin-dev] Drivechain: BIP 300 and 301",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-09-02T17:11:22",
                "message_text_only": "printf(\"Hello, World!\");\n\nWhat are your thoughts on Drivechain and associated BIPs?\n\nThis article compares Liquid and Lightning:\u00a0https://blog.liquid.net/six-differences-between-liquid-and-lightning/. Two things from it that I am interested in while evaluating Drivechain:\n\n1.Trust model\n2.On-Ramps and Off-Ramps\n\nOther things:\n\n1.Security of Bitcoin (Layer 1)\n2.Bitcoin transactions and fees expected on layer 1 because of Drivechain\n\nSimilarities and Differences between RSK and Ethereum:\u00a0https://medium.com/iovlabs-innovation-stories/similarities-and-differences-between-rsk-and-ethereum-e480655eff37\n\nPaul Sztorc had mentioned few things about fees in this video:\u00a0https://youtu.be/oga8Pwbq9M0?t=481\u00a0I am interested to know same for LN, Liquid and Rootstock as well so asked a question on Bitcoin Stackexchange today:\u00a0https://bitcoin.stackexchange.com/questions/109466/bitcoin-transactions-associated-with-layer-2-projects\n\nTwo critiques are mentioned here:\u00a0https://www.drivechain.info/peer-review/peer-review-new/\u00a0with lot of names. I don't agree with everything mentioned on project website although any comments on technical things that can help Bitcoin and Bitcoin projects will be great.\n\nWhy discuss here and not on Twitter?\n\n1.Twitter is not the best place for such discussions. There are some interesting threads but Its mostly used for followers, likes, retweets etc. and people can write anything for it.\n2.Avoid misinformation, controversies etc.\u00a0\n\nMy personal opinion:\n\nWe should encourage sidechain projects. I don't know much about Drivechain to form a strong opinion but concept looks good which can help in making better sidechains.\n\n----------------------------------------------------------------------------------------------------------------------\n\n\nThe website used in the slides of above YouTube video is misleading for few reasons:\n\n1.Blocks mined everyday (in MB) for Bitcoin is ~150 MB. It is ~600 MB for Ethereum. Block limits for Bitcoin is ~4 MB per 10 minutes and ~500 MB for Ethereum. If full nodes will be run by few organizations on AWS we can basically do everything on chain. However the main goal isn't too make money and create an illusion to do something innovative, primary goal was/is decentralized network that allows settlement of payments.\n\n2.Bitcoin uses UTXO model while Ethereum uses Account model. Basic difference in transactions for two is explained in an article\u00a0https://coinmetrics.io/on-data-and-certainty/. Irony is the website in the slides for screenshot is using Coinmetrics API and this misleading website is even shared by Coinmetrics team on Twitter. So in some cases you are doing more transactions, paying more fees for work which could have been done with less. Inefficiency.\n\n3.Failed transactions paying fees on Ethereum everyday, no such transactions on Bitcoin.\n\n4.Other improvements that affect fees: Segwit, Layer 2, Batching, UTXO consolidation, Fee estimation, Coin selection, Exchanges, Wallets etc.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210902/6638e6fe/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2021-09-02T20:20:21",
                "message_text_only": "drivechain is a cool proposal.   i don't think there's a ton of\nobvious risk to the network itself (not slow, not too much work for\nnodes, etc), but it seems to encourage \"bad behavior\", not sure the\nincentives line up to prevent thefts, and not sure that won't turn\naround and bite bitcoin's main chain.\n\nof course stacks can do this even without drivechain, so not sure what\nwe're hiding from there\n\nif you're talking about extensions there's lightning-compatible\nmimblewimble, which is probably more important, since it gets bitcoin\nto global-scale payments, while improving fungibility, and probably\ncan't be implemented safely via drivechain\n\n\n\nOn Thu, Sep 2, 2021 at 2:24 PM Prayank via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> printf(\"Hello, World!\");\n>\n> What are your thoughts on Drivechain and associated BIPs?\n>\n> This article compares Liquid and Lightning: https://blog.liquid.net/six-differences-between-liquid-and-lightning/. Two things from it that I am interested in while evaluating Drivechain:\n>\n> 1.Trust model\n> 2.On-Ramps and Off-Ramps\n>\n> Other things:\n>\n> 1.Security of Bitcoin (Layer 1)\n> 2.Bitcoin transactions and fees expected on layer 1 because of Drivechain\n>\n> Similarities and Differences between RSK and Ethereum: https://medium.com/iovlabs-innovation-stories/similarities-and-differences-between-rsk-and-ethereum-e480655eff37\n>\n> Paul Sztorc had mentioned few things about fees in this video: https://youtu.be/oga8Pwbq9M0?t=481 I am interested to know same for LN, Liquid and Rootstock as well so asked a question on Bitcoin Stackexchange today: https://bitcoin.stackexchange.com/questions/109466/bitcoin-transactions-associated-with-layer-2-projects\n>\n> Two critiques are mentioned here: https://www.drivechain.info/peer-review/peer-review-new/ with lot of names. I don't agree with everything mentioned on project website although any comments on technical things that can help Bitcoin and Bitcoin projects will be great.\n>\n> Why discuss here and not on Twitter?\n>\n> 1.Twitter is not the best place for such discussions. There are some interesting threads but Its mostly used for followers, likes, retweets etc. and people can write anything for it.\n> 2.Avoid misinformation, controversies etc.\n>\n> My personal opinion:\n>\n> We should encourage sidechain projects. I don't know much about Drivechain to form a strong opinion but concept looks good which can help in making better sidechains.\n>\n> ----------------------------------------------------------------------------------------------------------------------\n>\n>\n> The website used in the slides of above YouTube video is misleading for few reasons:\n>\n> 1.Blocks mined everyday (in MB) for Bitcoin is ~150 MB. It is ~600 MB for Ethereum. Block limits for Bitcoin is ~4 MB per 10 minutes and ~500 MB for Ethereum. If full nodes will be run by few organizations on AWS we can basically do everything on chain. However the main goal isn't too make money and create an illusion to do something innovative, primary goal was/is decentralized network that allows settlement of payments.\n>\n> 2.Bitcoin uses UTXO model while Ethereum uses Account model. Basic difference in transactions for two is explained in an article https://coinmetrics.io/on-data-and-certainty/. Irony is the website in the slides for screenshot is using Coinmetrics API and this misleading website is even shared by Coinmetrics team on Twitter. So in some cases you are doing more transactions, paying more fees for work which could have been done with less. Inefficiency.\n>\n> 3.Failed transactions paying fees on Ethereum everyday, no such transactions on Bitcoin.\n>\n> 4.Other improvements that affect fees: Segwit, Layer 2, Batching, UTXO consolidation, Fee estimation, Coin selection, Exchanges, Wallets etc.\n>\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-09-02T21:02:55",
                "message_text_only": "Good morning Prayank,\n\nJust to be clear, neither Liquid nor RSK, as of my current knowledge, are Drivechain systems.\n\nInstead, they are both federated sidechains.\nThe money owned by a federated sidechain is, as far s the Bitcoin blockchain is concerned, really owned by the federation that.runs the sidechain.\n\nBasically, a mainchain->sidechain transfer is done by paying to a federation k-of-n address and a coordination signal of some kind (details depending on federated sidechain) to create the equivalent coins on the sidechain.\nA sidechain->mainchain transfer is done by requesting some coins on the sidechain to be destroyed, and then the federation will send some of its mainchain k-of-n coins into whatever address you indicate you want to use on the mainchain.\n\nIn theory, a sufficient quorum of the federation can decide to ignore the sidechain data entirely and spend the mainchain money arbitrarily, and the mainchain layer will allow this (being completely ignorant of he sidechain).\n\nIn such federated sidechains, the federation is often a fixed predetermined signing set, and changes to that federation are expected to be rare.\n\nFederated sidechains are ultimately custodial; as noted above, the federation could in theory abscond with the funds completely, and the mainchain would not care if the sidechain federation executes its final exit strategy and you lose your funds.\nOne can consider federated sidechains to be a custodian with multiple personality disorder, that happens to use a blockchain to keep its individual sub-personalities coordinated with each other, but ultimately control of the money is contingent on the custodian following the dictates of the supposed owners of the coin.\n>From a certain point of view, it is actually immaterial that there is a separate blockchain called the \"sidechain\" --- it is simply that a blockchain is used to coordinate the custodians of the coin, but in principle any other coordination mechanism can be used between them, including a plain database.\n\n\nWith Drivechains, custody of the sidechain funds is held by mainchain miners.\nAgain, this is still a custodial setup.\nA potential issue here is that the mainchain miners cannot be identified (the entire point is anonymity of miners is possible), which may be of concern.\n\nIn particular, note that solely on mainchain, all that miners determine is the *ordering* and *timing* of transactions.\nLet us suppose that there is a major 51% attack attempt on the Bitcoin blockchain.\nWe expect that such an attack will be temporary --- individuals currently not mining may find that their HODLings are under threat of the 51% attack, and may find it more economic to run miners at a loss, in order to protect their stacks rather than lose it.\nThus, we expect that a 51% attack will be temporary, as other miners will arise inevitably to take back control of transaction processing.\nhttps://github.com/libbitcoin/libbitcoin-system/wiki/Threat-Level-Paradox\n\nIn particular, on the mainchain, 51% miners cannot reverse deep history.\nIf you have coins you have not moved since 2017, for example, the 51% attack is expected to take about 4 years before it can begin to threaten your ownership of those coins (hopefully, in those 4 years, you will get a clue and start mining at a loss to protect your funds from outright loss, thus helping evict the 51% attacker).\n51% miners can, in practice, only prevent transfers (censorship), not force transfer of funds (confiscation).\nOnce the 51% attacker is evicted (and they will in general be evicted), then coins you owned that were deeply confirmed remain under your control.\n\nWith Drivechains, however, sidechain funds can be confiscated by a 51% attacker, by forcing a bogus sidechain->mainchain withdrawal.\nThe amount of time it takes is simply the security parameter of the Drivechain spec.\nIt does not matter if you were holding those funds in the sidechain for several years without moving them --- a 51% attacker that is able to keep control of the mainchain blockchain, for the Drivechain security parameter, will be capable of confiscating sidechain funds outright.\nThus, even if the 51% attacker is evicted, then your coins in the sidechain can be confiscated and no longer under your control.\n\nIncreasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.\nWhile exchanges may exist that allow sidechain->mainchain withdrawal faster, those can only operate if the number of coins exiting the sidechain is approximately equal to coins entering the sidechain (remember, it is an *exchange*, coins are not actually moved from one to the other).\nIf there is a \"thundering herd\" problem, then exchanges will saturate and the sidechain->mainchain withdrawal mechanism has to come into play, and if the Drivechain security parameter (which secures sidechains from 51% attack confiscation)\nIn a \"thundering herd\" situation, the peg can be lost, meaning that sidechain coins become devalued relative to mainchain coins they are purportedly equivalent to.\n\nA \"thundering herd\" exiting the sidechain can happen, for example, if the sidechain is primarily used to prototype a new feature, and the feature is demonstrably so desirable that Bitcoin Core actually adds it.\nIn that case, the better security of the mainchain becomes desirable, and the sidechain no longer has a unique feature to incentivize keeping your funds there (since mainchain has/will have that feature).\nIn that case, the sidechain coin value can transiently drop due to the sidechain->mainchain withdrawal bottleneck caused by the Drivechain security parameter.\nAnd if the value can temporarily drop, well, it is not much of a peg, then.\n\n* If the Drivechain security parameter is too low, then a short 51% attack is enough to confiscate all sidechain coins.\n* If the Drivechain seucrity parameter is too large, then a coincidental large number of sidechain->mainchain exits risks triggering a thundering herd that temporarily devalues the sidechain value relative to mainchain.\n\nAgainst 51% attack confiscation, Paul Sztorc I believe proposes a \"nuclear option\" where mainchain fullnodes are upgraded to ignore historical blocks created by the 51% attacker.\nThe point is that a 51% attacker takes on the risk that confiscation will simply cause everyone to evict all miners and possibly destroy Bitcoin entirely, and rational 51% attackers will not do so, since then their mining hardware becomes useless.\nI believe this leads to a situation where a controversial chainsplit of a sidechain can effectively \"infect\" mainchain, with competing mainchain miners with different views of the sidechain censoring each other, thus removing isolation of the sidechain from the mainchain.\n\n--\n\nMore to the point: what are sidechains **for**?\n\n* If sidechains are for prototyping new features, then you are probably better off getting a bunch of developer friends together and creating a federation that runs the sidechain so you can tinker on new features with friends.\n  * This is how SegWit was prototyped in Elements Alpha, the predecessor of Liquid.\n* If sidechains are for scaling, then:\n  * We already ***know*** that blockchains cannot scale.\n  * Your plan for scaling is to make ***more*** blockchains?\n    Which we know cannot scale, right?\n  * Good luck.\n\nNow, if we were to consider scaling...\n\nAs I pointed out above, in principle a federated sidechain simply decided to use a blockchain to coordinate the federation members.\nNothing really prevents the federation from using a different mechanims.\n\nIn addition, federations (whether signer federations like in RSK or Liquid, or miner federations like in Drivechains) have custodial risk if you put your funds in them.\nThe only way to avoid the custodial risk is if ***you*** were one of the signatories of the federation, and the federation was an n-of-n.\n\nNow, let us consider a 2-of-2 federation, the smallest possible federation.\nAs long as *you* are one of the two signatories, you have no custodial risk in putting funds in this federation --- nothing can happen to the mainchain funds without your say-so, so the federation cannot confiscate your funds.\n\nAnd again, there is no real need to use a big, inefficient data structure like a **blockchain**.\nIn fact, in a 2-of-2 federation, there are only two members, so a lot of the blockchain overhead can be reduced to just a bunch of fairly simple protocol messages you send to each other, no need for a heavy history-retaining append-only data structure.\n\nOf course, only you and the other signatory in this 2-of-2 federation can safely keep funds in that federation.\nYou cannot pay a third party with those funds, because that third party now takes on custodial risk, you and your coutnerparty can collude to steal the funds of the third party.\nHowever, suppose your counterparty was a member of another 2-of-2 federation, this time with the third party you want to pay.\nYou can use an atomic swap mechanism of some kind so that you pay your couterparty if that couterparty pays the third party.\n\nAnd guess what?\nThat is just Lightning Network.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Prayank",
                "date": "2021-09-03T09:47:45",
                "message_text_only": "Good morning\u00a0ZmnSCPxj,\n\nThanks for sharing all the details. One thing that I am not sure about:\n\n>\u00a0* We already ***know*** that blockchains cannot scale\n> * Your plan for scaling is to make ***more*** blockchains?\n\nScaling Bitcoin can be different from scaling Bitcoin sidechains. You can experiment with lot of things on sidechains to scale which isn't true for Bitcoin. Most important thing is requirements for running a node differ. Its easy to run a node for LN, Liquid and Rootstock right now. Will it remain the same? I am not sure.\n\nLND:\u00a0https://github.com/lightningnetwork/lnd/blob/master/docs/INSTALL.md\n\nLiquid:\u00a0https://help.blockstream.com/hc/en-us/articles/900002026026-How-do-I-set-up-a-Liquid-node-\n\nRootstock:\u00a0https://developers.rsk.co/rsk/node/install/\n\n>\u00a0More to the point: what are sidechains **for**?\n\nSmart contracts are possible on Bitcoin but with limited functionality so lot of applications are not possible using Bitcoin (Layer1). Some of these don't even make sense on Layer 1 and create other issues like MEV however deploying them on sidechains should not affect base layer.\n\n>\u00a0Increasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.\n\nI think 'withdrawals' is the part which can be improved in Drivechain. Not sure about any solution at this point or trade-offs involved but making few changes can help Drivechain and Bitcoin.\nI agree with everything else you explained and emails like these will be helpful for everyone trying to understand what's going on with Layer 2 on Bitcoin.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nSep 3, 2021, 02:32 by ZmnSCPxj at protonmail.com:\n\n> Good morning Prayank,\n>\n> Just to be clear, neither Liquid nor RSK, as of my current knowledge, are Drivechain systems.\n>\n> Instead, they are both federated sidechains.\n> The money owned by a federated sidechain is, as far s the Bitcoin blockchain is concerned, really owned by the federation that.runs the sidechain.\n>\n> Basically, a mainchain->sidechain transfer is done by paying to a federation k-of-n address and a coordination signal of some kind (details depending on federated sidechain) to create the equivalent coins on the sidechain.\n> A sidechain->mainchain transfer is done by requesting some coins on the sidechain to be destroyed, and then the federation will send some of its mainchain k-of-n coins into whatever address you indicate you want to use on the mainchain.\n>\n> In theory, a sufficient quorum of the federation can decide to ignore the sidechain data entirely and spend the mainchain money arbitrarily, and the mainchain layer will allow this (being completely ignorant of he sidechain).\n>\n> In such federated sidechains, the federation is often a fixed predetermined signing set, and changes to that federation are expected to be rare.\n>\n> Federated sidechains are ultimately custodial; as noted above, the federation could in theory abscond with the funds completely, and the mainchain would not care if the sidechain federation executes its final exit strategy and you lose your funds.\n> One can consider federated sidechains to be a custodian with multiple personality disorder, that happens to use a blockchain to keep its individual sub-personalities coordinated with each other, but ultimately control of the money is contingent on the custodian following the dictates of the supposed owners of the coin.\n> From a certain point of view, it is actually immaterial that there is a separate blockchain called the \"sidechain\" --- it is simply that a blockchain is used to coordinate the custodians of the coin, but in principle any other coordination mechanism can be used between them, including a plain database.\n>\n>\n> With Drivechains, custody of the sidechain funds is held by mainchain miners.\n> Again, this is still a custodial setup.\n> A potential issue here is that the mainchain miners cannot be identified (the entire point is anonymity of miners is possible), which may be of concern.\n>\n> In particular, note that solely on mainchain, all that miners determine is the *ordering* and *timing* of transactions.\n> Let us suppose that there is a major 51% attack attempt on the Bitcoin blockchain.\n> We expect that such an attack will be temporary --- individuals currently not mining may find that their HODLings are under threat of the 51% attack, and may find it more economic to run miners at a loss, in order to protect their stacks rather than lose it.\n> Thus, we expect that a 51% attack will be temporary, as other miners will arise inevitably to take back control of transaction processing.\n> https://github.com/libbitcoin/libbitcoin-system/wiki/Threat-Level-Paradox\n>\n> In particular, on the mainchain, 51% miners cannot reverse deep history.\n> If you have coins you have not moved since 2017, for example, the 51% attack is expected to take about 4 years before it can begin to threaten your ownership of those coins (hopefully, in those 4 years, you will get a clue and start mining at a loss to protect your funds from outright loss, thus helping evict the 51% attacker).\n> 51% miners can, in practice, only prevent transfers (censorship), not force transfer of funds (confiscation).\n> Once the 51% attacker is evicted (and they will in general be evicted), then coins you owned that were deeply confirmed remain under your control.\n>\n> With Drivechains, however, sidechain funds can be confiscated by a 51% attacker, by forcing a bogus sidechain->mainchain withdrawal.\n> The amount of time it takes is simply the security parameter of the Drivechain spec.\n> It does not matter if you were holding those funds in the sidechain for several years without moving them --- a 51% attacker that is able to keep control of the mainchain blockchain, for the Drivechain security parameter, will be capable of confiscating sidechain funds outright.\n> Thus, even if the 51% attacker is evicted, then your coins in the sidechain can be confiscated and no longer under your control.\n>\n> Increasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.\n> While exchanges may exist that allow sidechain->mainchain withdrawal faster, those can only operate if the number of coins exiting the sidechain is approximately equal to coins entering the sidechain (remember, it is an *exchange*, coins are not actually moved from one to the other).\n> If there is a \"thundering herd\" problem, then exchanges will saturate and the sidechain->mainchain withdrawal mechanism has to come into play, and if the Drivechain security parameter (which secures sidechains from 51% attack confiscation)\n> In a \"thundering herd\" situation, the peg can be lost, meaning that sidechain coins become devalued relative to mainchain coins they are purportedly equivalent to.\n>\n> A \"thundering herd\" exiting the sidechain can happen, for example, if the sidechain is primarily used to prototype a new feature, and the feature is demonstrably so desirable that Bitcoin Core actually adds it.\n> In that case, the better security of the mainchain becomes desirable, and the sidechain no longer has a unique feature to incentivize keeping your funds there (since mainchain has/will have that feature).\n> In that case, the sidechain coin value can transiently drop due to the sidechain->mainchain withdrawal bottleneck caused by the Drivechain security parameter.\n> And if the value can temporarily drop, well, it is not much of a peg, then.\n>\n> * If the Drivechain security parameter is too low, then a short 51% attack is enough to confiscate all sidechain coins.\n> * If the Drivechain seucrity parameter is too large, then a coincidental large number of sidechain->mainchain exits risks triggering a thundering herd that temporarily devalues the sidechain value relative to mainchain.\n>\n> Against 51% attack confiscation, Paul Sztorc I believe proposes a \"nuclear option\" where mainchain fullnodes are upgraded to ignore historical blocks created by the 51% attacker.\n> The point is that a 51% attacker takes on the risk that confiscation will simply cause everyone to evict all miners and possibly destroy Bitcoin entirely, and rational 51% attackers will not do so, since then their mining hardware becomes useless.\n> I believe this leads to a situation where a controversial chainsplit of a sidechain can effectively \"infect\" mainchain, with competing mainchain miners with different views of the sidechain censoring each other, thus removing isolation of the sidechain from the mainchain.\n>\n> --\n>\n> More to the point: what are sidechains **for**?\n>\n> * If sidechains are for prototyping new features, then you are probably better off getting a bunch of developer friends together and creating a federation that runs the sidechain so you can tinker on new features with friends.\n>  * This is how SegWit was prototyped in Elements Alpha, the predecessor of Liquid.\n> * If sidechains are for scaling, then:\n>  * We already ***know*** that blockchains cannot scale.\n>  * Your plan for scaling is to make ***more*** blockchains?\n>  Which we know cannot scale, right?\n>  * Good luck.\n>\n> Now, if we were to consider scaling...\n>\n> As I pointed out above, in principle a federated sidechain simply decided to use a blockchain to coordinate the federation members.\n> Nothing really prevents the federation from using a different mechanims.\n>\n> In addition, federations (whether signer federations like in RSK or Liquid, or miner federations like in Drivechains) have custodial risk if you put your funds in them.\n> The only way to avoid the custodial risk is if ***you*** were one of the signatories of the federation, and the federation was an n-of-n.\n>\n> Now, let us consider a 2-of-2 federation, the smallest possible federation.\n> As long as *you* are one of the two signatories, you have no custodial risk in putting funds in this federation --- nothing can happen to the mainchain funds without your say-so, so the federation cannot confiscate your funds.\n>\n> And again, there is no real need to use a big, inefficient data structure like a **blockchain**.\n> In fact, in a 2-of-2 federation, there are only two members, so a lot of the blockchain overhead can be reduced to just a bunch of fairly simple protocol messages you send to each other, no need for a heavy history-retaining append-only data structure.\n>\n> Of course, only you and the other signatory in this 2-of-2 federation can safely keep funds in that federation.\n> You cannot pay a third party with those funds, because that third party now takes on custodial risk, you and your coutnerparty can collude to steal the funds of the third party.\n> However, suppose your counterparty was a member of another 2-of-2 federation, this time with the third party you want to pay.\n> You can use an atomic swap mechanism of some kind so that you pay your couterparty if that couterparty pays the third party.\n>\n> And guess what?\n> That is just Lightning Network.\n>\n> Regards,\n> ZmnSCPxj\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/2499e8b6/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-09-07T09:37:43",
                "message_text_only": "Good morning Prayank,\n\n\n> Thanks for sharing all the details. One thing that I am not sure about:\n>\n> >\u00a0* We already ***know*** that blockchains cannot scale\n> > * Your plan for scaling is to make ***more*** blockchains?\n>\n> Scaling Bitcoin can be different from scaling Bitcoin sidechains. You can experiment with lot of things on sidechains to scale which isn't true for Bitcoin.\n\nI would classify this as \"prototyping new features\" (i.e. it just happens to be a feature that theoretically improves blockchain scaling, with the sidechain as a demonstration and the goal eventually to get something like it into Bitcoin blockchain proper), not really scaling-by-sidechains/shards, so I think this is a fine example of \"just make a federated sidechain\" solution for the prototyping bit.\n\nDo note that the above idea is a kernel for the argument that Drivechains simply allow for miner-controlled block size increases, an argument I have seen elsewhere but have no good links for, so take it is hearsay.\n\n> Most important thing is requirements for running a node differ. Its easy to run a node for LN, Liquid and Rootstock right now. Will it remain the same? I am not sure.\n>\n> LND:\u00a0https://github.com/lightningnetwork/lnd/blob/master/docs/INSTALL.md\n>\n> Liquid:\u00a0https://help.blockstream.com/hc/en-us/articles/900002026026-How-do-I-set-up-a-Liquid-node-\n>\n> Rootstock:\u00a0https://developers.rsk.co/rsk/node/install/\n\nLN will likely remain easy to install and maintain, especially if you use C-Lightning and CLBOSS *cough*.\n\n> >\u00a0More to the point: what are sidechains **for**?\n>\n> Smart contracts are possible on Bitcoin but with limited functionality so lot of applications are not possible using Bitcoin (Layer1). Some of these don't even make sense on Layer 1 and create other issues like MEV however deploying them on sidechains should not affect base layer.\n\nKey being \"should\" --- as noted, part of the Drivechains security argument from Paul Sztorc is that a nuclear option can be deployed, which *possibly* means that issues in the sidechain may infect the mainchain.\n\nAlso see stuff like \"smart contracts unchained\": https://zmnscpxj.github.io/bitcoin/unchained.html\nThis allows creation of small federations which are *not* coordinated via inefficient blockchain structures.\n\nSo, really, my main point is: before going for the big heavy blockchain hammer, maybe other constructions are possible for any specific application?\n\n>\n> >\u00a0Increasing the Drivechain security parameter leads to slower sidechain->mainchin withdrawals, effectively a bottleneck on how much can be transferred sidechain->mainchain.\n>\n> I think 'withdrawals' is the part which can be improved in Drivechain. Not sure about any solution at this point or trade-offs involved but making few changes can help Drivechain and Bitcoin.\n\nIt is precisely due to the fact that the mainchain cannot validate the sidechain rules, that side->main transfers must be bottlenecked, so that sidechain miners have an opportunity to gainsay any theft attempts that violate the sidechain rules.\nConsider a similar parameter in Lightning when exiting non-cooperatively from a channel, which allows the other side to gainsay any theft attempts, a parameter which will still exist even in Decker-Russell-Osuntokun.\n\nThis parameter existed even in the old Blockstream sidechains proposal from sipa et al.\nFor the old Blockstream proposal the parameter is measured in sidechain blocks, and the sidechain has its own miners instead of riding off mainchain, but ultimately there exists a parameter that restricts the rate at which side->main transfers can be performed.\n\nAt least LN does not require any changes at the base layer (at least not anymore, after SegWit).\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Prayank",
                "date": "2021-09-03T10:07:55",
                "message_text_only": "> of course stacks can do this even without drivechain, so not sure whatwe're hiding from there\n\nStacks is not a Bitcoin sidechain IMO. It has its own native token which isn't pegged to BTC. Premined.\u00a0 It uses Bitcoin as a storage and broadcast medium for recording all blocks. Marketing with lot of misinformation. None of these things really help Bitcoin.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/4d1737c1/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Drivechain: BIP 300 and 301",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Prayank",
                "Erik Aronesty"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 31988
        }
    },
    {
        "title": "[bitcoin-dev] Human readable checksum (verification code) to avoid errors on BTC public addresses",
        "thread_messages": [
            {
                "author": "ts",
                "date": "2021-09-03T05:08:42",
                "message_text_only": "Hi Marek,\n\nMarek Palatinus wrote on 8/31/21 3:47 AM:\n> I fully agree with sipa and his reasoning that this proposal is not solving any particular \n> problem, but making it actually a bit worse.\nOk, I understand. I'm just trying to find ways to reduce the risk of sending to the wrong \naddress and to make the transaction process a bit more user friendly, specially for \ninexperienced users. I am sure that it can be implemented in a way without making it \"worse\". \nFor example, if there is the risk that the user looks ONLY at the code and not at the address, \nthen the code should have enough entropy to account for it. If looking at 6 characters is \nconsidered to be enough, then the code should also be 6 characters long. As I mentioned in my \nfollowing message, the code could be made from specific characters of the address instead of a \nchecksum (e.g. first 4 and last 2 characters). By showing these characters to the user \nseparately and in a bigger font, he will be encouraged to verify all of these characters.\n\n> Also, do you know what I hate more than copy&pasting bitcoin addresses? Copy pasting zillion \n> random fields for SEPA/wire transfers. And I believe that a single copy pasta of a bitcoin \n> address is a much better user experience after all.\n\nI totally agree with this :)\n\nCheers,\nTS\n\n\n> Best,\n> slush\n>\n> On Tue, Aug 31, 2021 at 9:08 AM ts via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org \n> <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>     Pieter, thanks for your comments. Here my thoughts:\n>\n>     Pieter Wuille wrote on 8/29/21 9:24 AM:\n>     > On Saturday, August 28th, 2021 at 5:17 PM, ts via bitcoin-dev\n>     <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>>\n>     wrote:\n>     >\n>     >> Following up on my original proposal, I would like to get some more feedback of the\n>     community\n>     >>\n>     >> to see if this could be realized at some point. Also, any recommendations as to who\n>     to contact\n>     >>\n>     >> to get things rolling?\n>     >\n>     > I honestly don't understand the point of what you're suggesting.\n>\n>     It is about creating a simple technical assistance that makes it more user friendly and\n>     less\n>     error prone to verify the entered address. For all types of users, including those who are\n>     less tech savvy.\n>\n>\n>     > * If you're concerned about random typos, this is something already automatically\n>     protected against through the checksum (both base58check or bech32/bech32m).\n>\n>     I agree, but as mentioned in the original proposal, it is not about random typos (although\n>     this would help for other coins without integrated checksum of course), but rather about\n>     copy&paste errors (both technical or user caused).\n>\n>\n>     > * If you're concerned about accidentally entering the wrong - but honestly created -\n>     address, comparing any few characters of the address is just as good as any other. It\n>     doesn't even require the presence of a checksum. Looking at the last N characters, or\n>     the middle N, or anything except the first few, will do, and is just as good as an\n>     \"external\" checksum added at the end. For randomly-generated addresses (as honest ones\n>     are), each of those has exactly as much entropy.\n>\n>     Correct. However, I believe that ADDITIONALLY to looking at N characters, a quick check\n>     of a 3\n>     or 4 digit code in bigger font next to the address would make for a better user experience.\n>     This gives the user the reassurance that there is definitely no error. I agree that most\n>     users\n>     with technical background including most of us here will routinely check the first/last N\n>     characters. I usually check the first 3 + last 3 characters. But I don't think this is very\n>     user friendly. More importantly, I once had the case that two addresses were very\n>     similar at\n>     precisely those 6 characters, and only a more close and concentrated look made me see the\n>     difference. Moreover, some inexperienced users that are not aware of the consequences of\n>     entering a wrong address (much worse than entering the wrong bank account in an online bank\n>     transfer) might forget to look at the characters altogether.\n>\n>\n>     > * If you're concerned about maliciously constructed addresses, which are designed to\n>     look similar in specific places, an attacker can just as easily make the external\n>     checksum collide (and having one might even worsen this, as now the attacker can focus\n>     on exactly that, rather than needing to focus on every other character).\n>\n>     Not so concerned about this case, since this is a very special case that can only occur\n>     under\n>     certain circumstances. But taking this case also into consideration, this is why the user\n>     should use the verification code ADDITIONALLY to the normal way of verifying, not\n>     instead. If\n>     the attacker only focuses on the verification code, he will only be successful with\n>     users that\n>     ONLY look at this code. But if the attacker intends to be more successful, he now needs to\n>     create a valid address that is both similar in specific places AND produces the same\n>     verification code, which is way more difficult to achieve.\n>\n>\n>     > Things would be different if you'd suggest a checksum in another medium than text\n>     (e.g. a visual/drawing/colorcoding one). But I don't see any added value for an\n>     additional text-based checksum when addresses are already text themselves.\n>\n>     Yes, a visual checksum could also work. Christopher Allen proposed to use LifeHash as an\n>     alternative. It would be a matter of balancing the more complex implementation and need of\n>     space in the app's layout with the usability and advantages of use. One advantage of the\n>     digit\n>     verification code is that it can be spoken in a call or written in a message.\n>\n>     > This is even disregarding the difficulty of getting the ecosystem to adopt such changes.\n>\n>     No changes are needed, only an agreement or recommendation on which algorithm for the code\n>     generation should be used. Once this is done, it is up to the developers of wallets and\n>     exchanges to implement this feature as they see fit.\n>\n>     Greetings,\n>     TS\n>     _______________________________________________\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>     <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/4f4ab95b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Human readable checksum (verification code) to avoid errors on BTC public addresses",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ts"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6832
        }
    },
    {
        "title": "[bitcoin-dev] Note on Sequence Lock Upgrades Defect",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-09-04T03:32:19",
                "message_text_only": "Hi Bitcoin Devs,\n\nI recently noticed a flaw in the Sequence lock implementation with respect\nto upgradability. It might be the case that this is protected against by\nsome transaction level policy (didn't see any in policy.cpp, but if not,\nI've put up a blogpost explaining the defect and patching it\nhttps://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/\n\nI've proposed patching it here https://github.com/bitcoin/bitcoin/pull/22871,\nit is proper to widely survey the community before patching to ensure no\none is depending on the current semantics in any live application lest this\ntightening of standardness rules engender a confiscatory effect.\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210903/1d2d16a4/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-09-05T03:19:57",
                "message_text_only": "In working on resolving this issue, one issue that has come up is what\nsequence values get used by wallet implementations?\n\nE.g., in Bitcoin Core a script test says\n\nBIP125_SEQUENCE_NUMBER = 0xfffffffd  # Sequence number that is rbf-opt-in\n(BIP 125) and csv-opt-out (BIP 68)\n\nAre any other numbers currently expected by any wallet software to be\nbroadcastable with the DISABLE flag set? Does anyone use *this* number? Is\nthere any advantage of this number v.s. just 0? Do people commonly use\n0xfffffffd? 0xfffffffe is special, but it seems the former has the\nalternative of either 0 valued sequence lock (1<<22 or 0).\n\nAre there any other sequence numbers that are not defined in a BIP that\nmight be used somewhere?\n\nCheers,\n\nJeremy\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Fri, Sep 3, 2021 at 8:32 PM Jeremy <jlrubin at mit.edu> wrote:\n\n> Hi Bitcoin Devs,\n>\n> I recently noticed a flaw in the Sequence lock implementation with respect\n> to upgradability. It might be the case that this is protected against by\n> some transaction level policy (didn't see any in policy.cpp, but if not,\n> I've put up a blogpost explaining the defect and patching it\n> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/\n>\n> I've proposed patching it here\n> https://github.com/bitcoin/bitcoin/pull/22871, it is proper to widely\n> survey the community before patching to ensure no one is depending on the\n> current semantics in any live application lest this tightening of\n> standardness rules engender a confiscatory effect.\n>\n> Best,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210904/036089ac/attachment.html>"
            },
            {
                "author": "David A. Harding",
                "date": "2021-09-06T02:35:25",
                "message_text_only": "On Fri, Sep 03, 2021 at 08:32:19PM -0700, Jeremy via bitcoin-dev wrote:\n> Hi Bitcoin Devs,\n> \n> I recently noticed a flaw in the Sequence lock implementation with respect\n> to upgradability. It might be the case that this is protected against by\n> some transaction level policy (didn't see any in policy.cpp, but if not,\n> I've put up a blogpost explaining the defect and patching it\n> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/\n\nIsn't this why BIP68 requires using tx.version=2?  Wouldn't we just\ndeploy any new nSequence rules with tx.version>2?\n\n-Dave\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/2f55d419/attachment.sig>"
            },
            {
                "author": "Jeremy",
                "date": "2021-09-06T03:17:17",
                "message_text_only": "BIP 68 says >= 2:\n*This specification defines the meaning of sequence numbers for\ntransactions with an nVersion greater than or equal to 2 for which the rest\nof this specification relies on.*\nBIP-112 says not < 2\n// Fail if the transaction's version number is not set high\n// enough to trigger BIP 68 rules.\nif (static_cast<uint32_t>(txTo->nVersion) < 2) return false;\n\nA further proof that this needs fix: the flawed upgradable semantic exists\nin script as well as in the transaction nSeqeunce. we can't really control\nthe transaction version an output will be spent with in the future, so it\nwould be weird/bad to change the semantic in transaction version 3.\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Sun, Sep 5, 2021 at 7:36 PM David A. Harding <dave at dtrt.org> wrote:\n\n> On Fri, Sep 03, 2021 at 08:32:19PM -0700, Jeremy via bitcoin-dev wrote:\n> > Hi Bitcoin Devs,\n> >\n> > I recently noticed a flaw in the Sequence lock implementation with\n> respect\n> > to upgradability. It might be the case that this is protected against by\n> > some transaction level policy (didn't see any in policy.cpp, but if not,\n> > I've put up a blogpost explaining the defect and patching it\n> > https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/\n>\n> Isn't this why BIP68 requires using tx.version=2?  Wouldn't we just\n> deploy any new nSequence rules with tx.version>2?\n>\n> -Dave\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210905/036b421e/attachment.html>"
            },
            {
                "author": "darosior",
                "date": "2021-09-06T06:16:44",
                "message_text_only": "Hi Jeremy,\n\nI think it would be nice to have and suggested something similar (enforce minimality) in the context of\nMiniscript a few months ago [0].\n\nHowever your code:\n\nconst bool seq_is_reserved = (txin.nSequence < CTxIn::SEQUENCE_FINAL-2) && (\n// when sequence is set to disabled, it is reserved for future use\n((txin.nSequence & CTxIn::SEQUENCE_LOCKTIME_DISABLE_FLAG) != 0) ||\n// when sequence has bits set outside of the type flag and locktime mask,\n// it is reserved for future use.\n((~(CTxIn::SEQUENCE_LOCKTIME_TYPE_FLAG | CTxIn::SEQUENCE_LOCKTIME_MASK) &\ntxin.nSequence) != 0)\n);\n\nWould effectively prevent Lightning Network commitment transactions from relaying. The protocol uses\na hack encoding the commitment transaction numbering in the part of nSequence (and nLockTime)\nwithout consensus meaning. This both sets the LOCKTIME_DISABLE_FLAG and uses bits outside of\nthe mask.\n\n[0] https://github.com/rust-bitcoin/rust-miniscript/pull/246#issue-671512626\n[1] https://github.com/lightningnetwork/lightning-rfc/blob/master/03-transactions.md#commitment-transaction\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nLe lundi 6 septembre 2021 \u00e0 5:17 AM, Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> BIP 68 says >= 2:This specification defines the meaning of sequence numbers for transactions with an nVersion greater than or equal to 2 for which the rest of this specification relies on.\n> BIP-112 says not < 2\n> // Fail if the transaction's version number is not set high\n> // enough to trigger BIP 68 rules.\n> if (static_cast<uint32_t>(txTo->nVersion) < 2) return false;\n>\n> A further proof that this needs fix: the flawed upgradable semantic exists in script as well as in the transaction nSeqeunce. we can't really control the transaction version an output will be spent with in the future, so it would be weird/bad to change the semantic in transaction version 3.\n>\n> --\n> [@JeremyRubin](https://twitter.com/JeremyRubin)https://twitter.com/JeremyRubin\n>\n> On Sun, Sep 5, 2021 at 7:36 PM David A. Harding <dave at dtrt.org> wrote:\n>\n>> On Fri, Sep 03, 2021 at 08:32:19PM -0700, Jeremy via bitcoin-dev wrote:\n>>> Hi Bitcoin Devs,\n>>>\n>>> I recently noticed a flaw in the Sequence lock implementation with respect\n>>> to upgradability. It might be the case that this is protected against by\n>>> some transaction level policy (didn't see any in policy.cpp, but if not,\n>>> I've put up a blogpost explaining the defect and patching it\n>>> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/\n>>\n>> Isn't this why BIP68 requires using tx.version=2? Wouldn't we just\n>> deploy any new nSequence rules with tx.version>2?\n>>\n>> -Dave\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210906/ff395880/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-09T00:02:45",
                "message_text_only": "Hi Jeremy,\n\nAnswering here from #22871 discussions.\n\nI agree on the general principle to not blur mempool policies signaling in\ncommitted transaction data. Beyond preserving upgradeability, another good\nargument is to let L2 nodes update the mempool policies signaling their\npre-signed transactions non-interactively. If one of the transaction fields\nis assigned mempool semantics, in case of tightening policy changes, you\nwill need to re-sign or bear the risks of having non-propagating\ntransactions which opens the door for exploitation by a malicious\ncounterparty. I think this point is kinda relevant if we have future\ncross-layer coordinated safety fixes to deal with a la CVE-2021-31876.\n\nEven further, a set of L2 counterparties would like to pick up divergent\ntx-relay/mempool policies, having the signaling fields as part of the\nsignature force them to come to consensus.\n\nI think we can take the opportunity of p2p packages to introduce a new\nfield to signal policy. Of course, a malicious tx-relay peer could modify\nits content to jam your transaction's propagation but in that case it is\neasier to just drop it.\n\nOne issue with taking back the `nSequence` field for consensus-semantic\nsounds is depriving the application-layer from a discrete, zero-cost\npayload (e.g the LN obfuscated commitment number watermark). This might be\ncontroversial as we'll increase the price of such applications if they're\nstill willingly to relay application specific data through the p2p network\n(e.g force them to use a costly OP_RETURN output or payer/payee\ninteractions to setup a pay-to-contract)\n\nW.r.t flag day activation to smooth policy deployment, I think that's\nsomething we might rely on in the future though we could distinguish few\ntypes of policy deployments :\n1) loosening changes (e.g full-rbf/dust threshold removal), a transaction\nwhich was relaying under\nthe former policy should relay under the new one\n2) tightening changes (e.g #22871), a transaction which was relaying under\nthe former policy\nmight not relay under the new one\n3) new feature introduced (e.g packages), a transaction is offered a new\nmode of relay\n\nI think 1) doesn't need that level of ecosystem coordination as\napplications/second-layers should always benefit from such changes. Maybe\nwith the exception of full-rbf, where we have historical 0-conf softwares,\nwith (broken) security assumptions made on the opt-out RBF mechanism. Same\nwith 3), better to have new features deployed gradually, a flag day\nactivation day in this case won't mean that all higher stacks will jump to\nuse package-relay ?\n\nWhere a flag day might make sense would be for 2) ? It would create a\nhigher level of commitment by the base layer software instead of a pure\ncommunication on the ML/GH, which might not be concretized in the announced\nrelease due to slow review process/feature freeze/rebase conflicts...\nReversing the process and asking for Bitcoin applications/higher layers to\nupdate first might get us in the trap of never doing the change, as someone\nmight have a small use-case in the corner relying on a given policy\nbehavior.\n\nThat said, w.r.t to the proposed policy change in #22871, I think it's\nbetter to deploy full-rbf first, then give a time buffer to higher\napplications to free up the `nSequence` field and finally start to\ndiscourage the usage. Otherwise, by introducing new discouragement waivers,\ne.g not rejecting the usage of the top 8 bits, I think we're moving away\nfrom the policy design principle we're trying to establish (separation of\nmempool policies signaling from consensus data)\n\nLe ven. 3 sept. 2021 \u00e0 23:32, Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi Bitcoin Devs,\n>\n> I recently noticed a flaw in the Sequence lock implementation with respect\n> to upgradability. It might be the case that this is protected against by\n> some transaction level policy (didn't see any in policy.cpp, but if not,\n> I've put up a blogpost explaining the defect and patching it\n> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/\n>\n> I've proposed patching it here\n> https://github.com/bitcoin/bitcoin/pull/22871, it is proper to widely\n> survey the community before patching to ensure no one is depending on the\n> current semantics in any live application lest this tightening of\n> standardness rules engender a confiscatory effect.\n>\n> Best,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210908/e40cd7f3/attachment-0001.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-09-09T01:04:53",
                "message_text_only": "See the current patchset proposed:\nhttps://github.com/bitcoin/bitcoin/pull/22871/commits\n\nTwo things are happening that are separate:\n\n1) Fixing the semantics of arg in <arg> OP_CHECKSEQUENCEVERIFY\n2) Fixing the semantics on nSequence in each tx input\n\nThere is no sense in conditioning part 1 on RBF or anything else, since\nit's only loosely related to 2. I think it should be a class-2 rollout as\nyou describe above since it's a rule tightening.\n\nFor part 2, I think the way the patches handle it currently (which is\ndefining 1 byte type prefix followed by 3 bytes application data) is\nsufficient for immediate deployment.\n\nI agree with you that a class-2 rollout might be appropriate for it, but\nthat can be followed by removing the SEQUENCE_ROOT_TYPE::SPECIAL field\nlater as a class-1 rollout. However, so long as it's not being used for any\nparticular constants, there is no need to deallocate\nSEQUENCE_ROOT_TYPE::SPECIAL tag as long as no new use case must overlap\nit's range.\n\nWith respect to the SEQUENCE_ROOT_TYPE::UNCHECKED_METADATA, it is in fact\n*not* mempool data, but is a special type of metadata which is required for\nthe counterparty to efficiently respond to a unilateral channel closure (see\nbolt-3 This obscures the number of commitments made on the channel in the\ncase of unilateral close, yet still provides a useful index for both nodes\n(who know the payment_basepoints) to quickly find a revoked commitment\ntransaction.)\n\nI understand wanting to remove full-rbf, but I think that fixing the\nupgradability of sequences is much less controversial among the\nuserbase and worth doing expediently. That part 1 is doable now -- albeit\nas a class 2 -- means that it would not be unreasonable to bundle parts 1\nand 2 so that we don't double burden the community with an upgrade effort.\nFurther, RBF can be disabled on a purely ad-hoc node-by-node policy layer,\nwhereas this restriction requires more community coordination/awareness.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\n\nOn Wed, Sep 8, 2021 at 5:03 PM Antoine Riard <antoine.riard at gmail.com>\nwrote:\n\n> Hi Jeremy,\n>\n> Answering here from #22871 discussions.\n>\n> I agree on the general principle to not blur mempool policies signaling in\n> committed transaction data. Beyond preserving upgradeability, another good\n> argument is to let L2 nodes update the mempool policies signaling their\n> pre-signed transactions non-interactively. If one of the transaction fields\n> is assigned mempool semantics, in case of tightening policy changes, you\n> will need to re-sign or bear the risks of having non-propagating\n> transactions which opens the door for exploitation by a malicious\n> counterparty. I think this point is kinda relevant if we have future\n> cross-layer coordinated safety fixes to deal with a la CVE-2021-31876.\n>\n> Even further, a set of L2 counterparties would like to pick up divergent\n> tx-relay/mempool policies, having the signaling fields as part of the\n> signature force them to come to consensus.\n>\n> I think we can take the opportunity of p2p packages to introduce a new\n> field to signal policy. Of course, a malicious tx-relay peer could modify\n> its content to jam your transaction's propagation but in that case it is\n> easier to just drop it.\n>\n> One issue with taking back the `nSequence` field for consensus-semantic\n> sounds is depriving the application-layer from a discrete, zero-cost\n> payload (e.g the LN obfuscated commitment number watermark). This might be\n> controversial as we'll increase the price of such applications if they're\n> still willingly to relay application specific data through the p2p network\n> (e.g force them to use a costly OP_RETURN output or payer/payee\n> interactions to setup a pay-to-contract)\n>\n> W.r.t flag day activation to smooth policy deployment, I think that's\n> something we might rely on in the future though we could distinguish few\n> types of policy deployments :\n> 1) loosening changes (e.g full-rbf/dust threshold removal), a transaction\n> which was relaying under\n> the former policy should relay under the new one\n> 2) tightening changes (e.g #22871), a transaction which was relaying under\n> the former policy\n> might not relay under the new one\n> 3) new feature introduced (e.g packages), a transaction is offered a new\n> mode of relay\n>\n> I think 1) doesn't need that level of ecosystem coordination as\n> applications/second-layers should always benefit from such changes. Maybe\n> with the exception of full-rbf, where we have historical 0-conf softwares,\n> with (broken) security assumptions made on the opt-out RBF mechanism. Same\n> with 3), better to have new features deployed gradually, a flag day\n> activation day in this case won't mean that all higher stacks will jump to\n> use package-relay ?\n>\n> Where a flag day might make sense would be for 2) ? It would create a\n> higher level of commitment by the base layer software instead of a pure\n> communication on the ML/GH, which might not be concretized in the announced\n> release due to slow review process/feature freeze/rebase conflicts...\n> Reversing the process and asking for Bitcoin applications/higher layers to\n> update first might get us in the trap of never doing the change, as someone\n> might have a small use-case in the corner relying on a given policy\n> behavior.\n>\n> That said, w.r.t to the proposed policy change in #22871, I think it's\n> better to deploy full-rbf first, then give a time buffer to higher\n> applications to free up the `nSequence` field and finally start to\n> discourage the usage. Otherwise, by introducing new discouragement waivers,\n> e.g not rejecting the usage of the top 8 bits, I think we're moving away\n> from the policy design principle we're trying to establish (separation of\n> mempool policies signaling from consensus data)\n>\n> Le ven. 3 sept. 2021 \u00e0 23:32, Jeremy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi Bitcoin Devs,\n>>\n>> I recently noticed a flaw in the Sequence lock implementation with\n>> respect to upgradability. It might be the case that this is protected\n>> against by some transaction level policy (didn't see any in policy.cpp, but\n>> if not, I've put up a blogpost explaining the defect and patching it\n>> https://rubin.io/bitcoin/2021/09/03/upgradable-nops-flaw/\n>>\n>> I've proposed patching it here\n>> https://github.com/bitcoin/bitcoin/pull/22871, it is proper to widely\n>> survey the community before patching to ensure no one is depending on the\n>> current semantics in any live application lest this tightening of\n>> standardness rules engender a confiscatory effect.\n>>\n>> Best,\n>>\n>> Jeremy\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>> <https://twitter.com/JeremyRubin>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210908/24501572/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Note on Sequence Lock Upgrades Defect",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "darosior",
                "Jeremy",
                "Antoine Riard",
                "David A. Harding"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 20048
        }
    },
    {
        "title": "[bitcoin-dev] BIP process meeting - Tuesday September 14th 23:00 UTC on #bitcoin-dev Libera IRC",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2021-09-07T11:45:42",
                "message_text_only": "With a new BIP editor (Kalle Alm) in place and Taproot activation\nlocked in it is probably/possibly as good time as any to revisit the\nBIP process and see if we can bolster it, improve it or at least\ninform why certain things operate the way they do.\n\nHence two IRC meetings are being organized, one on Tuesday September\n14th (23:00 UTC) and one on Wednesday September 29th (23:00 UTC), both\non the Libera IRC channel #bitcoin-dev.\n\nPossible discussion topics range from the relatively mundane (should\nBIP champions need to ACK basic spelling change PRs) to the possibly\ncontentious (what role if any do the BIPs have in informing the\ncommunity of soft fork activation parameters). At the very least it\nwould be good to address some common misunderstandings and subtleties\nof the BIP process that many (including myself) are lacking context\non.\n\nSo if you are interested in the BIP process or certainly if you have\nexperienced frustrations with the BIP process in the past as a BIP\nchampion or BIP contributor please attend and we\u2019ll see what progress\nwe can make.\n\nThere is a BIP process wishlist that some have already contributed\nideas too: https://github.com/bitcoin/bips/wiki/BIP-Process-wishlist\n\nAnd of course BIP 2 outlines the current BIP process:\nhttps://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki\n\nThis is being organized in the spirit of seeking to improve a process\nand a resource that we as a community all rely on so please engage in\nthe spirit that is intended. I will keep the mailing list informed of\nanything that comes out of the meetings and the #bitcoin-dev channel\nis open for discussion outside of the meetings.\n\n-- \nMichael Folkson\nEmail: michaelfolkson at gmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            },
            {
                "author": "Prayank",
                "date": "2021-09-14T12:17:57",
                "message_text_only": "Hi Michael,\n\nThanks for sharing the details about the meeting.\n\nWishlist has some interesting points. I would like to suggest few things:\n\n1.BIP process: \n\nA. Plan and document a proposal\n\n B. Open PR in\u00a0https://github.com/bitcoin/bips and edit everything properly\n\n C. BIP is assigned a number and merged\n\n D. Share the proposal on bitcoin dev mailing list\n\nbitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.\n\nWho implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.\n\n2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.\n\n3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it:\u00a0https://prayank23.github.io/BIPsGallery/\u00a0however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.\u00a0\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210914/cee843bc/attachment.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2021-09-14T14:07:46",
                "message_text_only": "Hey Prayank\n\nThanks for the suggestions.\n\n> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.\n\nA mailing list post is static and a BIP will go normally go through\nmultiple edits and revisions so you do need to take advantage of the\nGit version control system. It gets quite unwieldy to attempt to do\nthat via a mailing list with every minor suggested edit getting sent\nto all subscribers. Also allowing the entire global population\n(billions of people) to be able to create a directory doesn't sound\nlike a good idea to me :)\n\n> This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.\n\nI can only speak for myself here but I am not particularly concerned\nabout this perception of authority. We need a central repo that we can\nall refer to (rather than BIPs being distributed across a large number\nof repos) and that central repo needs to managed and maintained by\nsomebody (in this case the two BIP editors Kalle and Luke). In the\nsame way as there are limits on the ability of Core maintainers to\nunilaterally merge in contentious code changes there are similar\nlimits on the ability of BIP editors. Ultimately anyone merging a PR\nhas to consider process/consensus and concerns can (and have been in\nthe past) be raised on this mailing list or elsewhere.\n\n> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.\n\nI'm not sure where you are suggesting a bot should be. On IRC? There\nis a BIP merges bot on Mastodon[0] that I'm aware of and obviously you\ncan subscribe to GitHub repo notification emails.\n\n> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.\n\nThis looks cool. I think we can definitely do better in encouraging\nmore people to engage with the BIP process especially as the ideas\nstart flowing in post Taproot activation brainstorming what should be\nin the \"next soft fork\" (trademark!). Some of the BIPs (e.g. the\nTaproot BIPs 340-342) are quite technically dense so someone on IRC\nsuggested making greater use of informational BIPs to supplement the\nstandard BIPs for new implementers or even casual readers.\n\n[0] https://x0f.org/@bipmerges\n\nOn Tue, Sep 14, 2021 at 1:17 PM Prayank <prayank at tutanota.de> wrote:\n>\n> Hi Michael,\n>\n> Thanks for sharing the details about the meeting.\n>\n> Wishlist has some interesting points. I would like to suggest few things:\n>\n> 1.BIP process:\n>\n> A. Plan and document a proposal\n>\n> B. Open PR in https://github.com/bitcoin/bips and edit everything properly\n>\n> C. BIP is assigned a number and merged\n>\n> D. Share the proposal on bitcoin dev mailing list\n>\n> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.\n>\n> Who implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.\n>\n> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.\n>\n> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.\n>\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n\n\n\n-- \nMichael Folkson\nEmail: michaelfolkson at gmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            },
            {
                "author": "Prayank",
                "date": "2021-09-14T14:50:16",
                "message_text_only": "> A mailing list post is static and a BIP will go normally go through multiple edits and revisions so you do need to take advantage of the Git version control system. It gets quite unwieldy to attempt to do that via a mailing list with every minor suggested edit getting sent to all subscribers.\n\n Mailing list post will have the link to BIP documentation. Post itself doesn't need to be updated but same link can be used to share updated information. Example: https://gist.github.com/prayank23/95b4804777fefd015d7cc4f847675d7f\u00a0(Image can be changed in gist when required or add new information)\nMailing list post will help in reading discussions related to proposal.\n\n>Also allowing the entire global population\n(billions of people) to be able to create a directory doesn't sound\nlike a good idea to me :)\n\nThere is nothing to allow/disallow. That's the whole point. People are free to save links and organize things which can be called a BIP directory.\n\n> I can only speak for myself here but I am not particularly concerned about this perception of authority. \n\nThis perception affects Bitcoin. \n\n> In the same way as there are limits on the ability of Core maintainers to unilaterally merge in contentious code changes there are similar limits on the ability of BIP editors. Ultimately anyone merging a PR has to consider process/consensus and concerns can (and have been in the past) be raised on this mailing list or elsewhere.\n\nBitcoin Core is an implementation (used by most of the nodes right now). BIPs are proposals for Bitcoin. Using same organization on GitHub and such comparisons can be misleading for many. I don't think we need ACKs/NACKs in BIPs repository and I feel weird to be a part of discussions, ACKing this pull request:\u00a0https://github.com/bitcoin/bips/pull/1104. Not sure any Bitcoin project needs a pull request merged in this repository to implement a proposal.\n\n>\u00a0I'm not sure where you are suggesting a bot should be.\n\nA bot similar to DrahtBot in Bitcoin Core repository.\u00a0\nFew other developers had suggested similar thing earlier:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018859.html\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018868.html\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018869.html\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018871.html\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nSep 14, 2021, 19:37 by michaelfolkson at gmail.com:\n\n> Hey Prayank\n>\n> Thanks for the suggestions.\n>\n>> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.\n>>\n>\n> A mailing list post is static and a BIP will go normally go through\n> multiple edits and revisions so you do need to take advantage of the\n> Git version control system. It gets quite unwieldy to attempt to do\n> that via a mailing list with every minor suggested edit getting sent\n> to all subscribers. Also allowing the entire global population\n> (billions of people) to be able to create a directory doesn't sound\n> like a good idea to me :)\n>\n>> This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.\n>>\n>\n> I can only speak for myself here but I am not particularly concerned\n> about this perception of authority. We need a central repo that we can\n> all refer to (rather than BIPs being distributed across a large number\n> of repos) and that central repo needs to managed and maintained by\n> somebody (in this case the two BIP editors Kalle and Luke). In the\n> same way as there are limits on the ability of Core maintainers to\n> unilaterally merge in contentious code changes there are similar\n> limits on the ability of BIP editors. Ultimately anyone merging a PR\n> has to consider process/consensus and concerns can (and have been in\n> the past) be raised on this mailing list or elsewhere.\n>\n>> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.\n>>\n>\n> I'm not sure where you are suggesting a bot should be. On IRC? There\n> is a BIP merges bot on Mastodon[0] that I'm aware of and obviously you\n> can subscribe to GitHub repo notification emails.\n>\n>> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.\n>>\n>\n> This looks cool. I think we can definitely do better in encouraging\n> more people to engage with the BIP process especially as the ideas\n> start flowing in post Taproot activation brainstorming what should be\n> in the \"next soft fork\" (trademark!). Some of the BIPs (e.g. the\n> Taproot BIPs 340-342) are quite technically dense so someone on IRC\n> suggested making greater use of informational BIPs to supplement the\n> standard BIPs for new implementers or even casual readers.\n>\n> [0] https://x0f.org/@bipmerges\n>\n> On Tue, Sep 14, 2021 at 1:17 PM Prayank <prayank at tutanota.de> wrote:\n>\n>>\n>> Hi Michael,\n>>\n>> Thanks for sharing the details about the meeting.\n>>\n>> Wishlist has some interesting points. I would like to suggest few things:\n>>\n>> 1.BIP process:\n>>\n>> A. Plan and document a proposal\n>>\n>> B. Open PR in https://github.com/bitcoin/bips and edit everything properly\n>>\n>> C. BIP is assigned a number and merged\n>>\n>> D. Share the proposal on bitcoin dev mailing list\n>>\n>> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.\n>>\n>> Who implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.\n>>\n>> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.\n>>\n>> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.\n>>\n>>\n>> --\n>> Prayank\n>>\n>> A3B1 E430 2298 178F\n>>\n>\n>\n>\n> -- \n> Michael Folkson\n> Email: michaelfolkson at gmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210914/261a6313/attachment-0001.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2021-09-14T15:43:59",
                "message_text_only": ">> I can only speak for myself here but I am not particularly concerned about this perception of authority.\n\n> This perception affects Bitcoin.\n\nPersonally I would rather have an optimal process that provides\nclarity and helps us build better software than be sensitive to\ninaccurate perceptions that hinder that ultimate goal.\n\n> Bitcoin Core is an implementation (used by most of the nodes right now). BIPs are proposals for Bitcoin.\n\nIndeed, thanks for pointing this out. I take it as a given that\neveryone knows this but yeah when making such a comparison it is good\nto make this clear.\n\n> Using same organization on GitHub and such comparisons can be misleading for many.\n\nI think there's an argument that BIPs could be under a different\nGitHub organization but it would be pretty low on my list or\npriorities. There is a clear divide between the group of Core\nmaintainers and the group of BIP editors and in the absence of a\nreason to change that I would rather maintain the status quo.\n\n> I don't think we need ACKs/NACKs in BIPs repository and I feel weird to be a part of discussions, ACKing this pull request: https://github.com/bitcoin/bips/pull/1104.\n\nWith BIP champions having more latitude in getting their BIP PR merged\nthan they would for example getting their Core PR merged I agree\nACK/NACKs on BIP PRs are less relevant. However, I still think some\nBIP champions would like to have their changes reviewed especially by\nsubject matter experts. And if there are strong disagreements over the\nchanges made an alternative BIP is always an option. I don't see the\nharm in having discussion with reviewers on BIP PRs and reviewers\nregistering an ACK/NACK as long as we are all clear on what the BIP\nprocess is.\n\n> Not sure any Bitcoin project needs a pull request merged in this repository to implement a proposal.\n\nI agree it is optional for some/many Bitcoin projects whether they are\nBIPed or not. Would you be comfortable with a soft fork/consensus code\nchange going into Bitcoin Core without a BIP? I personally wouldn't.\n\nWe should probably leave it at that to ensure we are not spamming the\nemail list but hope to see you at the meeting later :)\n\nOn Tue, Sep 14, 2021 at 3:50 PM Prayank <prayank at tutanota.de> wrote:\n>\n> > A mailing list post is static and a BIP will go normally go through multiple edits and revisions so you do need to take advantage of the Git version control system. It gets quite unwieldy to attempt to do that via a mailing list with every minor suggested edit getting sent to all subscribers.\n>\n> Mailing list post will have the link to BIP documentation. Post itself doesn't need to be updated but same link can be used to share updated information. Example: https://gist.github.com/prayank23/95b4804777fefd015d7cc4f847675d7f (Image can be changed in gist when required or add new information)\n>\n> Mailing list post will help in reading discussions related to proposal.\n>\n> >Also allowing the entire global population\n> (billions of people) to be able to create a directory doesn't sound\n> like a good idea to me :)\n>\n> There is nothing to allow/disallow. That's the whole point. People are free to save links and organize things which can be called a BIP directory.\n>\n> > I can only speak for myself here but I am not particularly concerned about this perception of authority.\n>\n> This perception affects Bitcoin.\n>\n> > In the same way as there are limits on the ability of Core maintainers to unilaterally merge in contentious code changes there are similar limits on the ability of BIP editors. Ultimately anyone merging a PR has to consider process/consensus and concerns can (and have been in the past) be raised on this mailing list or elsewhere.\n>\n> Bitcoin Core is an implementation (used by most of the nodes right now). BIPs are proposals for Bitcoin. Using same organization on GitHub and such comparisons can be misleading for many. I don't think we need ACKs/NACKs in BIPs repository and I feel weird to be a part of discussions, ACKing this pull request: https://github.com/bitcoin/bips/pull/1104. Not sure any Bitcoin project needs a pull request merged in this repository to implement a proposal.\n>\n> > I'm not sure where you are suggesting a bot should be.\n>\n> A bot similar to DrahtBot in Bitcoin Core repository.\n>\n> Few other developers had suggested similar thing earlier:\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018859.html\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018868.html\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018869.html\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018871.html\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n>\n>\n> Sep 14, 2021, 19:37 by michaelfolkson at gmail.com:\n>\n> Hey Prayank\n>\n> Thanks for the suggestions.\n>\n> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.\n>\n>\n> A mailing list post is static and a BIP will go normally go through\n> multiple edits and revisions so you do need to take advantage of the\n> Git version control system. It gets quite unwieldy to attempt to do\n> that via a mailing list with every minor suggested edit getting sent\n> to all subscribers. Also allowing the entire global population\n> (billions of people) to be able to create a directory doesn't sound\n> like a good idea to me :)\n>\n> This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.\n>\n>\n> I can only speak for myself here but I am not particularly concerned\n> about this perception of authority. We need a central repo that we can\n> all refer to (rather than BIPs being distributed across a large number\n> of repos) and that central repo needs to managed and maintained by\n> somebody (in this case the two BIP editors Kalle and Luke). In the\n> same way as there are limits on the ability of Core maintainers to\n> unilaterally merge in contentious code changes there are similar\n> limits on the ability of BIP editors. Ultimately anyone merging a PR\n> has to consider process/consensus and concerns can (and have been in\n> the past) be raised on this mailing list or elsewhere.\n>\n> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.\n>\n>\n> I'm not sure where you are suggesting a bot should be. On IRC? There\n> is a BIP merges bot on Mastodon[0] that I'm aware of and obviously you\n> can subscribe to GitHub repo notification emails.\n>\n> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.\n>\n>\n> This looks cool. I think we can definitely do better in encouraging\n> more people to engage with the BIP process especially as the ideas\n> start flowing in post Taproot activation brainstorming what should be\n> in the \"next soft fork\" (trademark!). Some of the BIPs (e.g. the\n> Taproot BIPs 340-342) are quite technically dense so someone on IRC\n> suggested making greater use of informational BIPs to supplement the\n> standard BIPs for new implementers or even casual readers.\n>\n> [0] https://x0f.org/@bipmerges\n>\n> On Tue, Sep 14, 2021 at 1:17 PM Prayank <prayank at tutanota.de> wrote:\n>\n>\n> Hi Michael,\n>\n> Thanks for sharing the details about the meeting.\n>\n> Wishlist has some interesting points. I would like to suggest few things:\n>\n> 1.BIP process:\n>\n> A. Plan and document a proposal\n>\n> B. Open PR in https://github.com/bitcoin/bips and edit everything properly\n>\n> C. BIP is assigned a number and merged\n>\n> D. Share the proposal on bitcoin dev mailing list\n>\n> bitcoin-dev mailing list link can be considered a BIP and saved in a BIP directory. Anyone can create such directories. So BIP is nothing but a proposal shared on bitcoin-dev mailing list.\n>\n> Who implements the BIP? When is it implemented? How is it implemented? Opinions on proposal etc. will be different for each BIP. This will avoid the 'bitcoin/bips' repository being considered as some BIP authority that approves BIPs and proposals can improve Bitcoin without using the repository. Repository will only be helpful in documenting BIP correctly.\n>\n> 2. Bot in `bitcoin/bips` repository that notifies about pull requests based on different things. This will help maintainer(s) and contributors.\n>\n> 3. BIP Gallery: I tried sharing things in a different way so that newbies can understand importance of BIPs in Bitcoin and relate to it: https://prayank23.github.io/BIPsGallery/ however couldn't complete it with all the BIPs because not many people considered it helpful. There were few suggestions to improve it by adding some text for each BIP and better image gallery. Maybe someone else can create a better project.\n>\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n>\n>\n>\n> --\n> Michael Folkson\n> Email: michaelfolkson at gmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n>\n\n\n-- \nMichael Folkson\nEmail: michaelfolkson at gmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            }
        ],
        "thread_summary": {
            "title": "BIP process meeting - Tuesday September 14th 23:00 UTC on #bitcoin-dev Libera IRC",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "Michael Folkson"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 25052
        }
    },
    {
        "title": "[bitcoin-dev] Reorgs on SigNet - Looking for feedback on approach and parameters",
        "thread_messages": [
            {
                "author": "0xB10C",
                "date": "2021-09-07T16:07:47",
                "message_text_only": "Hello,\n\ntl;dr: We want to make reorgs on SigNet a reality and are looking for\nfeedback on approach and parameters.\n\nOne of the ideas for SigNet is the possibility for it to be reliably\nunreliable, for example, planned chain reorganizations. These have not\nbeen implemented yet.\n\nMy summerofbitcoin.org mentee Nikhil Bartwal and I have been looking at\nimplementing support for reorgs on SigNet. We are looking for feedback\non which approach and parameters to use. Please consider answering the\nquestions below if you or your company is interested in chain\nreorganizations on SigNet.\n\nWith feedback from AJ and Kalle Alm (thanks again!), we came up with two\nscenarios that could be implemented in the current SigNet miner script\n[0]. Both would trigger automatically in a fixed block interval.\nScenario 1 simulates a race scenario where two chains compete for D\nblocks. Scenario 2 simulates a chain rollback where the top D blocks get\nreplaced by a chain that outgrows the earlier branch.\n\nAJ proposed to allow SigNet users to opt-out of reorgs in case they\nexplicitly want to remain unaffected. This can be done by setting a\nto-be-reorged version bit flag on the blocks that won't end up in the\nmost work chain. Node operators could choose not to accept to-be-reorged\nSigNet blocks with this flag set via a configuration argument.\n\nThe reorg-interval X very much depends on the user's needs. One could\nargue that there should be, for example, three reorgs per day, each 48\nblocks apart. Such a short reorg interval allows developers in all time\nzones to be awake during one or two reorgs per day. Developers don't\nneed to wait for, for example, a week until they can test their reorgs\nnext. However, too frequent reorgs could hinder other SigNet users.\n\nWe propose that the reorg depth D is deterministically random between a\nminimum and a maximum based on, e.g., the block hash or the nonce of the\nlast block before the reorg. Compared to a local randint() based\nimplementation, this allows reorg-handling tests and external tools to\ncalculate the expected reorg depth.\n\n# Scenario 1: Race between two chains\n\nFor this scenario, at least two nodes and miner scripts need to be\nrunning. An always-miner A continuously produces blocks and rejects\nblocks with the to-be-reorged version bit flag set. And a race-miner R\nthat only mines D blocks at the start of each interval and then waits X\nblocks. A and R both have the same hash rate. Assuming both are well\nconnected to the network, it's random which miner will first mine and\npropagate a block. In the end, the A miner chain will always win the race.\n\n# Scenario 2: Chain rollback\n\nThis scenario only requires one miner and Bitcoin Core node but also\nworks in a multiminer setup. The miners mine D blocks with the\nto-be-reorged version bit flag set at the start of the interval. After\nallowing the block at height X+D to propagate, they invalidate the block\nat height X+1 and start mining on block X again. This time without\nsetting the to-be-reorged version bit flag. Non-miner nodes will reorg\nto the new tip at height X+D+1, and the first-seen branch stalls.\n\n# Questions\n\n\u00a0\u00a0\u00a0 1. How do you currently test your applications reorg handling? Do\n       the two discussed scenarios (race and chain rollback) cover your\n       needs? Are we missing something you'd find helpful?\n\n\u00a0\u00a0\u00a0 2. How often should reorgs happen on the default SigNet? Should\n       there be multiple reorgs a day (e.g., every 48 or 72 blocks\n       assuming 144 blocks per day) as your engineers need to be awake?\n       Do you favor less frequent reorgs (once per week or month)? Why?\n\n    3. How deep should the reorgs be on average? Do you want to test\n       deeper reorgs (10+ blocks) too?\n\n\n# Next Steps\n\nWe will likely implement Scenario 1, the race between two chains, first.\nWe'll set up a public test-SigNet along with a faucet, block explorer,\nand a block tree visualization. If there is interest in the second\napproach, chain rollbacks can be implemented too. Future work will add\nthe possibility to include conflicting transactions in the two branches.\nAfter enough testing, the default SigNet can start to do periodical\nreorgs, too.\n\nThanks,\n0xB10C\n\n[0]: https://github.com/bitcoin/bitcoin/blob/master/contrib/signet/miner"
            },
            {
                "author": "Jeremy",
                "date": "2021-09-07T16:44:17",
                "message_text_only": "If you make the to be reorged flag 2 bits, 1 bit can mark final block and\nthe other can mark to be reorged.\n\nThat way the nodes opting into reorg can see the reorg and ignore the final\nblocks (until a certain time? Or until it's via a reorg?), and the nodes\nwanting not to see reorgs get continuous service without disruption\n\nOn Tue, Sep 7, 2021, 9:12 AM 0xB10C via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello,\n>\n> tl;dr: We want to make reorgs on SigNet a reality and are looking for\n> feedback on approach and parameters.\n>\n> One of the ideas for SigNet is the possibility for it to be reliably\n> unreliable, for example, planned chain reorganizations. These have not\n> been implemented yet.\n>\n> My summerofbitcoin.org mentee Nikhil Bartwal and I have been looking at\n> implementing support for reorgs on SigNet. We are looking for feedback\n> on which approach and parameters to use. Please consider answering the\n> questions below if you or your company is interested in chain\n> reorganizations on SigNet.\n>\n> With feedback from AJ and Kalle Alm (thanks again!), we came up with two\n> scenarios that could be implemented in the current SigNet miner script\n> [0]. Both would trigger automatically in a fixed block interval.\n> Scenario 1 simulates a race scenario where two chains compete for D\n> blocks. Scenario 2 simulates a chain rollback where the top D blocks get\n> replaced by a chain that outgrows the earlier branch.\n>\n> AJ proposed to allow SigNet users to opt-out of reorgs in case they\n> explicitly want to remain unaffected. This can be done by setting a\n> to-be-reorged version bit flag on the blocks that won't end up in the\n> most work chain. Node operators could choose not to accept to-be-reorged\n> SigNet blocks with this flag set via a configuration argument.\n>\n> The reorg-interval X very much depends on the user's needs. One could\n> argue that there should be, for example, three reorgs per day, each 48\n> blocks apart. Such a short reorg interval allows developers in all time\n> zones to be awake during one or two reorgs per day. Developers don't\n> need to wait for, for example, a week until they can test their reorgs\n> next. However, too frequent reorgs could hinder other SigNet users.\n>\n> We propose that the reorg depth D is deterministically random between a\n> minimum and a maximum based on, e.g., the block hash or the nonce of the\n> last block before the reorg. Compared to a local randint() based\n> implementation, this allows reorg-handling tests and external tools to\n> calculate the expected reorg depth.\n>\n> # Scenario 1: Race between two chains\n>\n> For this scenario, at least two nodes and miner scripts need to be\n> running. An always-miner A continuously produces blocks and rejects\n> blocks with the to-be-reorged version bit flag set. And a race-miner R\n> that only mines D blocks at the start of each interval and then waits X\n> blocks. A and R both have the same hash rate. Assuming both are well\n> connected to the network, it's random which miner will first mine and\n> propagate a block. In the end, the A miner chain will always win the race.\n>\n> # Scenario 2: Chain rollback\n>\n> This scenario only requires one miner and Bitcoin Core node but also\n> works in a multiminer setup. The miners mine D blocks with the\n> to-be-reorged version bit flag set at the start of the interval. After\n> allowing the block at height X+D to propagate, they invalidate the block\n> at height X+1 and start mining on block X again. This time without\n> setting the to-be-reorged version bit flag. Non-miner nodes will reorg\n> to the new tip at height X+D+1, and the first-seen branch stalls.\n>\n> # Questions\n>\n>     1. How do you currently test your applications reorg handling? Do\n>        the two discussed scenarios (race and chain rollback) cover your\n>        needs? Are we missing something you'd find helpful?\n>\n>     2. How often should reorgs happen on the default SigNet? Should\n>        there be multiple reorgs a day (e.g., every 48 or 72 blocks\n>        assuming 144 blocks per day) as your engineers need to be awake?\n>        Do you favor less frequent reorgs (once per week or month)? Why?\n>\n>     3. How deep should the reorgs be on average? Do you want to test\n>        deeper reorgs (10+ blocks) too?\n>\n>\n> # Next Steps\n>\n> We will likely implement Scenario 1, the race between two chains, first.\n> We'll set up a public test-SigNet along with a faucet, block explorer,\n> and a block tree visualization. If there is interest in the second\n> approach, chain rollbacks can be implemented too. Future work will add\n> the possibility to include conflicting transactions in the two branches.\n> After enough testing, the default SigNet can start to do periodical\n> reorgs, too.\n>\n> Thanks,\n> 0xB10C\n>\n> [0]: https://github.com/bitcoin/bitcoin/blob/master/contrib/signet/miner\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210907/4f2f687d/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-08T07:59:04",
                "message_text_only": "On Tue, Sep 07, 2021 at 06:07:47PM +0200, 0xB10C via bitcoin-dev wrote:\n> The reorg-interval X very much depends on the user's needs. One could\n> argue that there should be, for example, three reorgs per day, each 48\n> blocks apart.\n\nOh, wow, I think the last suggestion was every 100 blocks (every\n~16h40m). Once every ~8h sounds very convenient.\n\n> Such a short reorg interval allows developers in all time\n> zones to be awake during one or two reorgs per day.\n\nAnd also for there to reliably be reorgs when they're not awake, which\nmight be a useful thing to be able to handle, too :)\n\n> Developers don't\n> need to wait for, for example, a week until they can test their reorgs\n> next. However, too frequent reorgs could hinder other SigNet users.\n\nBeing able to run `bitcoind -signet -signetacceptreorg=0` and never\nseeing any reorgs should presumably make this not a problem?\n\nFor people who do see reorgs, having an average of 2 or 3 additional\nblocks every 48 blocks is perhaps a 6% increase in storage/traffic.\n\n> # Scenario 1: Race between two chains\n> \n> For this scenario, at least two nodes and miner scripts need to be\n> running. An always-miner A continuously produces blocks and rejects\n> blocks with the to-be-reorged version bit flag set. And a race-miner R\n> that only mines D blocks at the start of each interval and then waits X\n> blocks. A and R both have the same hash rate. Assuming both are well\n> connected to the network, it's random which miner will first mine and\n> propagate a block. In the end, the A miner chain will always win the race.\n\nI think this description is missing that all the blocks R mines have\nthe to-be-reorged flag set.\n\n>     3. How deep should the reorgs be on average? Do you want to test\n>        deeper reorgs (10+ blocks) too?\n\nSuper interested in input on this -- perhaps we should get optech to\nsend a survey out to their members, or so?\n\nMy feeling is:\n\n - 1 block reorgs: these are a regular feature on mainnet, everyone\n   should cope with them; having them happen multiple times a day to\n   make testing easier should be great\n\n - 2-3 block reorgs: good for testing the \"your tx didn't get enough\n   confirms to be credited to your account\" case, even though it barely\n   ever happens on mainnet\n\n - 4-6 block reorgs: likely to violate business assumptions, but\n   completely technically plausible, especially if there's an attack\n   against the network\n\n - 7-100 block reorgs: for this to happen on mainnet, it would probably\n   mean there was a bug and pools/miners agree the chain has to\n   be immediately reverted -- eg, someone discovers and exploits an\n   inflation bug, minting themselves free bitcoins and breaking the 21M\n   limit (eg, the 51 block reorg in Aug 2010); or someone discovers a\n   bug that splits the chain, and the less compatible chain is reverted\n   (eg, the 24 block reorg due to the bdb lock limit in Mar 2013);\n   or something similar. Obviously the bug would have to have been\n   discovered pretty quickly after it was exploited for the reorg to be\n   under a day's worth of blocks.\n\n - 100-2000+ block reorgs: severe bug that wasn't found quickly, or where\n   getting >50% of miners organised took more than a few hours. This will\n   start breaking protocol assumptions, like pool payouts, lightning's\n   relative locktimes, or liquid's peg-in confirmation requirements, and\n   result in hundres of MBs of changes to the utxo set\n\nMaybe it would be good to do reorgs of 15, 150 or 1500 blocks as a\nspecial fire-drill event, perhaps once a month/quarter/year or so,\nin some pre-announced window?\n\nI think sticking to 1-6 block reorgs initially is a fine way to start\nthough.\n\n> After enough testing, the default SigNet can start to do periodical\n> reorgs, too.\n\nFWIW, the only thing that concerns me about doing this on the default\nsignet is making sure that nodes that set -signetacceptreorg=0 don't\nend up partitioning the p2p network due to either rejecting a higher\nwork chain or rejecting txs due to double-spends across the two chains.\n\nA quick draft of code for -signetacceptreorg=0 is available at \n\n  https://github.com/ajtowns/bitcoin/commits/202108-signetreorg\n\nCheers,\naj"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-09-12T14:29:08",
                "message_text_only": "> - 1 block reorgs: these are a regular feature on mainnet, everyone\n   should cope with them; having them happen multiple times a day to\n   make testing easier should be great\n\nAnyone can do 1 block reorg, because nonce is not signed, so anyone can replace that with better value. For example, if you have block 00000086d6b2636cb2a392d45edc4ec544a10024d30141c9adf4bfd9de533b53 with 0x0007f4cc nonce, you can replace that with 0x00110241 nonce and get 000000096a1c4239d994547185c80308a552cba85d5bd28a51e9dc583ae5eadb block, where everything is identical, except the nonce.\n\nSometimes that reorg could be deeper if you would be lucky enough to get a block with more work than N following blocks combined.\n\nOn 2021-09-08 09:59:29 user Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Tue, Sep 07, 2021 at 06:07:47PM +0200, 0xB10C via bitcoin-dev wrote:\n> The reorg-interval X very much depends on the user's needs. One could\n> argue that there should be, for example, three reorgs per day, each 48\n> blocks apart.\n\nOh, wow, I think the last suggestion was every 100 blocks (every\n~16h40m). Once every ~8h sounds very convenient.\n\n> Such a short reorg interval allows developers in all time\n> zones to be awake during one or two reorgs per day.\n\nAnd also for there to reliably be reorgs when they're not awake, which\nmight be a useful thing to be able to handle, too :)\n\n> Developers don't\n> need to wait for, for example, a week until they can test their reorgs\n> next. However, too frequent reorgs could hinder other SigNet users.\n\nBeing able to run `bitcoind -signet -signetacceptreorg=0` and never\nseeing any reorgs should presumably make this not a problem?\n\nFor people who do see reorgs, having an average of 2 or 3 additional\nblocks every 48 blocks is perhaps a 6% increase in storage/traffic.\n\n> # Scenario 1: Race between two chains\n> \n> For this scenario, at least two nodes and miner scripts need to be\n> running. An always-miner A continuously produces blocks and rejects\n> blocks with the to-be-reorged version bit flag set. And a race-miner R\n> that only mines D blocks at the start of each interval and then waits X\n> blocks. A and R both have the same hash rate. Assuming both are well\n> connected to the network, it's random which miner will first mine and\n> propagate a block. In the end, the A miner chain will always win the race.\n\nI think this description is missing that all the blocks R mines have\nthe to-be-reorged flag set.\n\n>     3. How deep should the reorgs be on average? Do you want to test\n>        deeper reorgs (10+ blocks) too?\n\nSuper interested in input on this -- perhaps we should get optech to\nsend a survey out to their members, or so?\n\nMy feeling is:\n\n - 1 block reorgs: these are a regular feature on mainnet, everyone\n   should cope with them; having them happen multiple times a day to\n   make testing easier should be great\n\n - 2-3 block reorgs: good for testing the \"your tx didn't get enough\n   confirms to be credited to your account\" case, even though it barely\n   ever happens on mainnet\n\n - 4-6 block reorgs: likely to violate business assumptions, but\n   completely technically plausible, especially if there's an attack\n   against the network\n\n - 7-100 block reorgs: for this to happen on mainnet, it would probably\n   mean there was a bug and pools/miners agree the chain has to\n   be immediately reverted -- eg, someone discovers and exploits an\n   inflation bug, minting themselves free bitcoins and breaking the 21M\n   limit (eg, the 51 block reorg in Aug 2010); or someone discovers a\n   bug that splits the chain, and the less compatible chain is reverted\n   (eg, the 24 block reorg due to the bdb lock limit in Mar 2013);\n   or something similar. Obviously the bug would have to have been\n   discovered pretty quickly after it was exploited for the reorg to be\n   under a day's worth of blocks.\n\n - 100-2000+ block reorgs: severe bug that wasn't found quickly, or where\n   getting >50% of miners organised took more than a few hours. This will\n   start breaking protocol assumptions, like pool payouts, lightning's\n   relative locktimes, or liquid's peg-in confirmation requirements, and\n   result in hundres of MBs of changes to the utxo set\n\nMaybe it would be good to do reorgs of 15, 150 or 1500 blocks as a\nspecial fire-drill event, perhaps once a month/quarter/year or so,\nin some pre-announced window?\n\nI think sticking to 1-6 block reorgs initially is a fine way to start\nthough.\n\n> After enough testing, the default SigNet can start to do periodical\n> reorgs, too.\n\nFWIW, the only thing that concerns me about doing this on the default\nsignet is making sure that nodes that set -signetacceptreorg=0 don't\nend up partitioning the p2p network due to either rejecting a higher\nwork chain or rejecting txs due to double-spends across the two chains.\n\nA quick draft of code for -signetacceptreorg=0 is available at \n\n  https://github.com/ajtowns/bitcoin/commits/202108-signetreorg\n\nCheers,\naj\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Greg Sanders",
                "date": "2021-09-12T14:54:33",
                "message_text_only": "> Sometimes that reorg could be deeper if you would be lucky enough to get\na block with more work than N following blocks combined\n\nEach block is credited for its contribution to total chainwork by the\ndifficulty target, not the hash of the block itself.\n\nOn Sun, Sep 12, 2021 at 10:42 PM vjudeu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > - 1 block reorgs: these are a regular feature on mainnet, everyone\n>    should cope with them; having them happen multiple times a day to\n>    make testing easier should be great\n>\n> Anyone can do 1 block reorg, because nonce is not signed, so anyone can\n> replace that with better value. For example, if you have block\n> 00000086d6b2636cb2a392d45edc4ec544a10024d30141c9adf4bfd9de533b53 with\n> 0x0007f4cc nonce, you can replace that with 0x00110241 nonce and get\n> 000000096a1c4239d994547185c80308a552cba85d5bd28a51e9dc583ae5eadb block,\n> where everything is identical, except the nonce.\n>\n> Sometimes that reorg could be deeper if you would be lucky enough to get a\n> block with more work than N following blocks combined.\n>\n> On 2021-09-08 09:59:29 user Anthony Towns via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > On Tue, Sep 07, 2021 at 06:07:47PM +0200, 0xB10C via bitcoin-dev wrote:\n> > The reorg-interval X very much depends on the user's needs. One could\n> > argue that there should be, for example, three reorgs per day, each 48\n> > blocks apart.\n>\n> Oh, wow, I think the last suggestion was every 100 blocks (every\n> ~16h40m). Once every ~8h sounds very convenient.\n>\n> > Such a short reorg interval allows developers in all time\n> > zones to be awake during one or two reorgs per day.\n>\n> And also for there to reliably be reorgs when they're not awake, which\n> might be a useful thing to be able to handle, too :)\n>\n> > Developers don't\n> > need to wait for, for example, a week until they can test their reorgs\n> > next. However, too frequent reorgs could hinder other SigNet users.\n>\n> Being able to run `bitcoind -signet -signetacceptreorg=0` and never\n> seeing any reorgs should presumably make this not a problem?\n>\n> For people who do see reorgs, having an average of 2 or 3 additional\n> blocks every 48 blocks is perhaps a 6% increase in storage/traffic.\n>\n> > # Scenario 1: Race between two chains\n> >\n> > For this scenario, at least two nodes and miner scripts need to be\n> > running. An always-miner A continuously produces blocks and rejects\n> > blocks with the to-be-reorged version bit flag set. And a race-miner R\n> > that only mines D blocks at the start of each interval and then waits X\n> > blocks. A and R both have the same hash rate. Assuming both are well\n> > connected to the network, it's random which miner will first mine and\n> > propagate a block. In the end, the A miner chain will always win the\n> race.\n>\n> I think this description is missing that all the blocks R mines have\n> the to-be-reorged flag set.\n>\n> >     3. How deep should the reorgs be on average? Do you want to test\n> >        deeper reorgs (10+ blocks) too?\n>\n> Super interested in input on this -- perhaps we should get optech to\n> send a survey out to their members, or so?\n>\n> My feeling is:\n>\n>  - 1 block reorgs: these are a regular feature on mainnet, everyone\n>    should cope with them; having them happen multiple times a day to\n>    make testing easier should be great\n>\n>  - 2-3 block reorgs: good for testing the \"your tx didn't get enough\n>    confirms to be credited to your account\" case, even though it barely\n>    ever happens on mainnet\n>\n>  - 4-6 block reorgs: likely to violate business assumptions, but\n>    completely technically plausible, especially if there's an attack\n>    against the network\n>\n>  - 7-100 block reorgs: for this to happen on mainnet, it would probably\n>    mean there was a bug and pools/miners agree the chain has to\n>    be immediately reverted -- eg, someone discovers and exploits an\n>    inflation bug, minting themselves free bitcoins and breaking the 21M\n>    limit (eg, the 51 block reorg in Aug 2010); or someone discovers a\n>    bug that splits the chain, and the less compatible chain is reverted\n>    (eg, the 24 block reorg due to the bdb lock limit in Mar 2013);\n>    or something similar. Obviously the bug would have to have been\n>    discovered pretty quickly after it was exploited for the reorg to be\n>    under a day's worth of blocks.\n>\n>  - 100-2000+ block reorgs: severe bug that wasn't found quickly, or where\n>    getting >50% of miners organised took more than a few hours. This will\n>    start breaking protocol assumptions, like pool payouts, lightning's\n>    relative locktimes, or liquid's peg-in confirmation requirements, and\n>    result in hundres of MBs of changes to the utxo set\n>\n> Maybe it would be good to do reorgs of 15, 150 or 1500 blocks as a\n> special fire-drill event, perhaps once a month/quarter/year or so,\n> in some pre-announced window?\n>\n> I think sticking to 1-6 block reorgs initially is a fine way to start\n> though.\n>\n> > After enough testing, the default SigNet can start to do periodical\n> > reorgs, too.\n>\n> FWIW, the only thing that concerns me about doing this on the default\n> signet is making sure that nodes that set -signetacceptreorg=0 don't\n> end up partitioning the p2p network due to either rejecting a higher\n> work chain or rejecting txs due to double-spends across the two chains.\n>\n> A quick draft of code for -signetacceptreorg=0 is available at\n>\n>   https://github.com/ajtowns/bitcoin/commits/202108-signetreorg\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/22dac873/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-09-10T00:50:08",
                "message_text_only": "On 9/7/21 09:07, 0xB10C via bitcoin-dev wrote:\n> Hello,\n> \n> tl;dr: We want to make reorgs on SigNet a reality and are looking for\n> feedback on approach and parameters.\n\nAwesome!\n\n> AJ proposed to allow SigNet users to opt-out of reorgs in case they\n> explicitly want to remain unaffected. This can be done by setting a\n> to-be-reorged version bit flag on the blocks that won't end up in the\n> most work chain. Node operators could choose not to accept to-be-reorged\n> SigNet blocks with this flag set via a configuration argument.\n\nWhy bother with a version bit? This seems substantially more complicated than the original proposal \nthat surfaced many times before signet launched to just have a different reorg signing key. Thus, \nusers who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not \nfollow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid \nwithout touching any header bits that non-full clients will ever see.\n\n> The reorg-interval X very much depends on the user's needs. One could\n> argue that there should be, for example, three reorgs per day, each 48\n> blocks apart. Such a short reorg interval allows developers in all time\n> zones to be awake during one or two reorgs per day. Developers don't\n> need to wait for, for example, a week until they can test their reorgs\n> next. However, too frequent reorgs could hinder other SigNet users.\n\nI see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users \nopt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to \ntest with reorgs.\n\n> We propose that the reorg depth D is deterministically random between a\n> minimum and a maximum based on, e.g., the block hash or the nonce of the\n> last block before the reorg. Compared to a local randint() based\n> implementation, this allows reorg-handling tests and external tools to\n> calculate the expected reorg depth.\n> \n> # Scenario 1: Race between two chains\n> \n> For this scenario, at least two nodes and miner scripts need to be\n> running. An always-miner A continuously produces blocks and rejects\n> blocks with the to-be-reorged version bit flag set. And a race-miner R\n> that only mines D blocks at the start of each interval and then waits X\n> blocks. A and R both have the same hash rate. Assuming both are well\n> connected to the network, it's random which miner will first mine and\n> propagate a block. In the end, the A miner chain will always win the race.\n> \n> # Scenario 2: Chain rollback\n> \n> This scenario only requires one miner and Bitcoin Core node but also\n> works in a multiminer setup. The miners mine D blocks with the\n> to-be-reorged version bit flag set at the start of the interval. After\n> allowing the block at height X+D to propagate, they invalidate the block\n> at height X+1 and start mining on block X again. This time without\n> setting the to-be-reorged version bit flag. Non-miner nodes will reorg\n> to the new tip at height X+D+1, and the first-seen branch stalls.\n\nBoth seem reasonable. I'm honestly not sure what software cases would be hit differently between one \nor the other, as long as reorgs happen regularly and at random depth. Nodes should presumably only \never be following one chain.\n\n> # Questions\n> \n>  \u00a0\u00a0\u00a0 1. How do you currently test your applications reorg handling? Do\n>         the two discussed scenarios (race and chain rollback) cover your\n>         needs? Are we missing something you'd find helpful?\n> \n>  \u00a0\u00a0\u00a0 2. How often should reorgs happen on the default SigNet? Should\n>         there be multiple reorgs a day (e.g., every 48 or 72 blocks\n>         assuming 144 blocks per day) as your engineers need to be awake?\n>         Do you favor less frequent reorgs (once per week or month)? Why?\n> \n>      3. How deep should the reorgs be on average? Do you want to test\n>         deeper reorgs (10+ blocks) too?\n\n6 is the \"standard\" confirmation window for mainnet. Its arguably much too low, but for testing \npurposes we've gotta pick something, so that seems reasonable?\n\nMatt"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-12T07:53:05",
                "message_text_only": "On Thu, Sep 09, 2021 at 05:50:08PM -0700, Matt Corallo via bitcoin-dev wrote:\n> > AJ proposed to allow SigNet users to opt-out of reorgs in case they\n> > explicitly want to remain unaffected. This can be done by setting a\n> > to-be-reorged version bit [...]\n> Why bother with a version bit? This seems substantially more complicated\n> than the original proposal that surfaced many times before signet launched\n> to just have a different reorg signing key.\n\nYeah, that was the original idea, but there ended up being two problems\nwith that approach. The simplest is that the signet block signature\nencodes the signet challenge, so if you have two different challenges, eg\n\n  \"<normal> CHECKSIG\"\n  \"0 SWAP 1 <normal> <reorg> 2 CHECKMULTISIG\"\n\nthen while both challenges will accept a signature by normal as the\nblock solution, the signature by \"normal\" will be different between the\ntwo. This is a fairly natural result of reusing the tx-signing code for\nthe block signatures and not having a noinput/anyprevout tx-signing mode.\n\nMore generally, though, this would mean that a node that's opting out\nof reorgs will see the to-be-reorged blocks as simply invalid due to a\nbad signature, and will follow the \"this node sent me an invalid block\"\npath in the p2p code, and start marking peers that are following reorgs\nas discouraged and worth disconnecting. I think that would make it pretty\nhard to avoid partitioning the network between peers that do and don't\naccept reorgs, and generally be a pain.\n\nSo using the RECENT_CONSENSUS_CHANGE behaviour that avoids the\ndiscourage/disconnect logic seems the way to avoid that problem, and that\nmeans making it so that nodes that that opt-out of reorgs can distinguish\nvalid-but-will-become-stale blocks from invalid blocks. Using a versionbit\nseems like the easiest way of doing that.\n\n> > The reorg-interval X very much depends on the user's needs. One could\n> > argue that there should be, for example, three reorgs per day, each 48\n> > blocks apart. Such a short reorg interval allows developers in all time\n> > zones to be awake during one or two reorgs per day. Developers don't\n> > need to wait for, for example, a week until they can test their reorgs\n> > next. However, too frequent reorgs could hinder other SigNet users.\n> I see zero reason whatsoever to not simply reorg ~every block, or as often\n> as is practical. If users opt in to wanting to test with reorgs, they should\n> be able to test with reorgs, not wait a day to test with reorgs.\n\nBlocks on signet get mined at a similar rate to mainnet, so you'll always\nhave to wait a little bit (up to an hour) -- if you don't want to wait\nat all, that's what regtest (or perhaps a custom signet) is for.\n\nI guess it would be super easy to say something like:\n\n - miner 1 ignores blocks marked for reorg\n - miner 2 marks its blocks for reorg, mines on top of the most work\n   block\n - miner 2 never mines a block which would have (height % 10 == 1)\n - miner 1 and miner 2 have the same hashrate, but mine at randomly\n   different times\n\nwhich would mean there's almost always a reorg being mined, people that\nfollow reorgs will see fewer than 1.9x as many blocks as non-reorg nodes,\nand reorgs won't go on for more than 10 blocks.\n\nCheers,\naj"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-09-13T05:33:24",
                "message_text_only": "> On Sep 12, 2021, at 00:53, Anthony Towns <aj at erisian.com.au> wrote:\n> \n> \ufeffOn Thu, Sep 09, 2021 at 05:50:08PM -0700, Matt Corallo via bitcoin-dev wrote:\n>>> AJ proposed to allow SigNet users to opt-out of reorgs in case they\n>>> explicitly want to remain unaffected. This can be done by setting a\n>>> to-be-reorged version bit [...]\n>> Why bother with a version bit? This seems substantially more complicated\n>> than the original proposal that surfaced many times before signet launched\n>> to just have a different reorg signing key.\n> \n> Yeah, that was the original idea, but there ended up being two problems\n> with that approach. The simplest is that the signet block signature\n> encodes the signet challenge,\n\nBut if that was the originally proposal, why is the challenge committed to in the block? :)\n\n> So using the RECENT_CONSENSUS_CHANGE behaviour that avoids the\n> discourage/disconnect logic seems the way to avoid that problem, and that\n> means making it so that nodes that that opt-out of reorgs can distinguish\n> valid-but-will-become-stale blocks from invalid blocks. Using a versionbit\n> seems like the easiest way of doing that.\n\nSure, you could set that for invalid block signatures as well though. It\u2019s not really a material DoS protection one way or the other.\n\n>>> The reorg-interval X very much depends on the user's needs. One could\n>>> argue that there should be, for example, three reorgs per day, each 48\n>>> blocks apart. Such a short reorg interval allows developers in all time\n>>> zones to be awake during one or two reorgs per day. Developers don't\n>>> need to wait for, for example, a week until they can test their reorgs\n>>> next. However, too frequent reorgs could hinder other SigNet users.\n>> I see zero reason whatsoever to not simply reorg ~every block, or as often\n>> as is practical. If users opt in to wanting to test with reorgs, they should\n>> be able to test with reorgs, not wait a day to test with reorgs.\n> \n> Blocks on signet get mined at a similar rate to mainnet, so you'll always\n> have to wait a little bit (up to an hour) -- if you don't want to wait\n> at all, that's what regtest (or perhaps a custom signet) is for.\n\nCan you explain the motivation for this? From where I sit, as far as I know, I should basically be a prime example of the target market for public signet - someone developing bitcoin applications with regular requirements to test those applications with other developers without jumping through hoops to configure software the same across the globe and set up miners. With blocks being slow and irregular, I\u2019m basically not benefited at all by signet and will stick with testnet3/mainnet testing, which both suck."
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-14T04:56:10",
                "message_text_only": "On Sun, Sep 12, 2021 at 10:33:24PM -0700, Matt Corallo via bitcoin-dev wrote:\n> > On Sep 12, 2021, at 00:53, Anthony Towns <aj at erisian.com.au> wrote:\n> >> Why bother with a version bit? This seems substantially more complicated\n> >> than the original proposal that surfaced many times before signet launched\n> >> to just have a different reorg signing key.\n> > Yeah, that was the original idea, but there ended up being two problems\n> > with that approach. The simplest is that the signet block signature\n> > encodes the signet challenge,\n> But if that was the originally proposal, why is the challenge committed to in the block? :)\n\nThe answer to your question was in the text after the comma, that you\ndeleted...\n\n> > Blocks on signet get mined at a similar rate to mainnet, so you'll always\n> > have to wait a little bit (up to an hour) -- if you don't want to wait\n> > at all, that's what regtest (or perhaps a custom signet) is for.\n> Can you explain the motivation for this? \n\nI'm not sure that's really the question you want answered? Mostly\nit's just \"this is how mainnet works\" plus \"these are the smallest\nchanges to have blocks be chosen by a signature, rather than entirely\nby PoW competition\".\n\nFor integration testing across many services, I think a ten-minute-average\nbetween blocks still makes sense -- protocols relying on CSV/CLTV to\nensure there's a delay they can use to recover funds, if they specify\nthat in blocks (as lightning's to_self_delay does), then significant\nsurges of blocks will cause uninteresting bugs. \n\nIt would be easy enough to change things to target an average of 2 or\n5 minutes, I suppose, but then you'd probably need to propogate that\nlogic back into your apps that would otherwise think 144 blocks is around\nabout a day.\n\nWe could switch back to doing blocks exactly every 10 minutes, rather\nthan a poisson-ish distribution in the range of 1min to 60min, but that\ndoesn't seem like that huge a win, and makes it hard to test that things\nbehave properly when blocks arrive in bursts.\n\n> From where I sit, as far as I know, I should basically be a prime\n> example of the target market for public signet - someone developing\n> bitcoin applications with regular requirements to test those applications\n> with other developers without jumping through hoops to configure software\n> the same across the globe and set up miners. With blocks being slow and\n> irregular, I\u2019m basically not benefited at all by signet and will stick\n> with testnet3/mainnet testing, which both suck.\n\nBest of luck to you then? Nobody's trying to sell you on a subscription\nplan to using signet. Signet's less expensive in fees (or risk) than\nmainnet, and takes far less time for IBD than testnet, but if those\naren't blockers for you, that's great.\n\nCheers,\naj"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-09-15T15:24:43",
                "message_text_only": "> On Sep 13, 2021, at 21:56, Anthony Towns <aj at erisian.com.au> wrote:\n> I'm not sure that's really the question you want answered?\n\nOf course it is? I\u2019d like to understand the initial thinking and design analysis that went into this decision. That seems like an important question to ask when seeking changes in an existing system :).\n\n> Mostly\n> it's just \"this is how mainnet works\" plus \"these are the smallest\n> changes to have blocks be chosen by a signature, rather than entirely\n> by PoW competition\".\n> \n> For integration testing across many services, I think a ten-minute-average\n> between blocks still makes sense -- protocols relying on CSV/CLTV to\n> ensure there's a delay they can use to recover funds, if they specify\n> that in blocks (as lightning's to_self_delay does), then significant\n> surges of blocks will cause uninteresting bugs. \n\nHmm, why would blocks coming quicker lead to a bug? I certainly hope no one has a bug if their block time is faster than per ten minutes. I presume here, you mean something like \u201cif the node can\u2019t keep up with the block rate\u201d, but I certainly hope the benchmark for may isn\u2019t 10 minutes, or really even one.\n\n> It would be easy enough to change things to target an average of 2 or\n> 5 minutes, I suppose, but then you'd probably need to propogate that\n> logic back into your apps that would otherwise think 144 blocks is around\n> about a day.\n\nWhy? One useful thing for testing is compressing real time. More broadly, the only issues that I\u2019ve heard around block times in testnet3 are the inconsistency and, rarely software failing to keep up at all.\n\n> We could switch back to doing blocks exactly every 10 minutes, rather\n> than a poisson-ish distribution in the range of 1min to 60min, but that\n> doesn't seem like that huge a win, and makes it hard to test that things\n> behave properly when blocks arrive in bursts.\n\nHmm, I suppose? If you want to test that the upper bound doesn\u2019t need to be 100 minutes, though, it could be 10.\n\n> Best of luck to you then? Nobody's trying to sell you on a subscription\n> plan to using signet.\n\n\nlol, yes, I\u2019m aware of that, nor did I mean to imply that anything has to be targeted at a specific person\u2019s requirements. Rather, my point here is that I\u2019m really confused as to who  the target user *is*, because we should be building products with target users in mind, even if those targets are often \u201cme\u201d for open source projects."
            },
            {
                "author": "Michael Folkson",
                "date": "2021-09-10T13:05:40",
                "message_text_only": "> I see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users opt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to test with reorgs.\n\nOne of the goals of the default Signet was to make the default Signet\nresemble mainnet as much as possible. (You can do whatever you want on\na custom signet you set up yourself including manufacturing a re-org\nevery block if you wish.) Hence I'm a bit wary of making the behavior\non the default Signet deviate significantly from what you might\nexperience on mainnet. Given re-orgs don't occur that often on mainnet\nI can see the argument for making them more regular (every 8 hours\nseems reasonable to me) on the default Signet but every block seems\nexcessive. It makes the default Signet into an environment for purely\ntesting whether your application can withstand various flavors of edge\ncase re-orgs. You may want to test whether your application can\nwithstand normal mainnet behavior (no re-orgs for long periods of\ntime) first before you concern yourself with re-orgs.\n\n> Why bother with a version bit? This seems substantially more complicated than the original proposal that surfaced many times before signet launched to just have a different reorg signing key. Thus, users who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not follow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid without touching any header bits that non-full clients will ever see.\n\nIf I understand this correctly this is introducing a need for users to\nsign blocks when currently with the default Signet the user does not\nneed to concern themselves with signing blocks. That is entirely left\nto the network block signers of the default Signet (who were AJ and\nKalle last time I checked). Again I don't think this additional\ncomplexity is needed on the default Signet when you can set up your\nown custom Signet if you want to test edge case scenarios that deviate\nsignificantly from what you are likely to experience on mainnet. A\nflag set via a configuration argument (the AJ, 0xB10C proposal) with\nno-reorgs (or 8 hour re-orgs) as the default seems to me like it would\nintroduce no additional complexity to the casual (or alpha stage)\ntester experience though of course it introduces implementation\ncomplexity.\n\nTo move the default Signet in the direction of resembling mainnet even\ncloser would be to randomly generate batches of transactions to fill\nup blocks and create a fee market. It would be great to be able to\ntest features like RBF and Lightning unhappy paths (justice\ntransactions, perhaps even pinning attacks etc) on the default Signet\nin future.\n\n-- \nMichael Folkson\nEmail: michaelfolkson at gmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-09-10T18:24:15",
                "message_text_only": "On 9/10/21 06:05, Michael Folkson wrote:\n>> I see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users opt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to test with reorgs.\n> \n> One of the goals of the default Signet was to make the default Signet\n> resemble mainnet as much as possible. (You can do whatever you want on\n> a custom signet you set up yourself including manufacturing a re-org\n> every block if you wish.) Hence I'm a bit wary of making the behavior\n> on the default Signet deviate significantly from what you might\n> experience on mainnet. Given re-orgs don't occur that often on mainnet\n> I can see the argument for making them more regular (every 8 hours\n> seems reasonable to me) on the default Signet but every block seems\n> excessive. It makes the default Signet into an environment for purely\n> testing whether your application can withstand various flavors of edge\n> case re-orgs. You may want to test whether your application can\n> withstand normal mainnet behavior (no re-orgs for long periods of\n> time) first before you concern yourself with re-orgs.\n\nHuh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to \nuse SigNet without modification *to make testing simpler* - keep the header format the same to let \nSPV clients function without (significant) modification, etc. The point of the whole thing is to \nmake testing as easy as possible, why would we do otherwise.\n\nFurther, because one goal here is to enable clients to opt in or out of the reorg chain at will \n(presumably by just changing one config flag in bitcoin.conf), why would we worry about making it \n\"similar to mainnet\". If users want an experience \"similar to mainnet\", they can simply turn off \nreorgs and they'll see a consistent chain moving forward which never reorgs, similar to the \npractical experience of mainnet.\n\nOnce you've opted into reorgs, you almost certainly are looking to *test* reorgs - you just \nrestarted Bitcoin Core with the reorg flag set, waiting around for a reorg after doing that seems \nlike the experience of testnet3 today, and the whole reason why we wanted signet to begin with - \nthings happen sporadically and inconsistently, making developers wait around forever. Please lets \nnot replicate the \"gotta wait for blocks before I can go to lunch\" experience of testnet today on \nsignet, I'm tired of eating lunch late.\n\n>> Why bother with a version bit? This seems substantially more complicated than the original proposal that surfaced many times before signet launched to just have a different reorg signing key. Thus, users who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not follow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid without touching any header bits that non-full clients will ever see.\n> \n> If I understand this correctly this is introducing a need for users to\n> sign blocks when currently with the default Signet the user does not\n> need to concern themselves with signing blocks. That is entirely left\n> to the network block signers of the default Signet (who were AJ and\n> Kalle last time I checked). Again I don't think this additional\n> complexity is needed on the default Signet when you can set up your\n> own custom Signet if you want to test edge case scenarios that deviate\n> significantly from what you are likely to experience on mainnet. A\n> flag set via a configuration argument (the AJ, 0xB10C proposal) with\n> no-reorgs (or 8 hour re-orgs) as the default seems to me like it would\n> introduce no additional complexity to the casual (or alpha stage)\n> tester experience though of course it introduces implementation\n> complexity.\n> \n> To move the default Signet in the direction of resembling mainnet even\n> closer would be to randomly generate batches of transactions to fill\n> up blocks and create a fee market. It would be great to be able to\n> test features like RBF and Lightning unhappy paths (justice\n> transactions, perhaps even pinning attacks etc) on the default Signet\n> in future.\n\nI believe my suggestion was not correctly understood. I'm not suggesting *users* sign blocks or \notherwise do anything manually here, only that the existing block producers each generate a new key, \nand we then only sign reorgs with *those* keys. Users will be able to set a flag to indicate \"I want \nto accept sigs from either sets of keys, and see reorgs\" or \"I only want sigs from the non-reorg \nkeys, and will consider the reorg keys-signed blocks invalid\"\n\nMatt"
            },
            {
                "author": "Michael Folkson",
                "date": "2021-09-10T19:00:39",
                "message_text_only": "> Huh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to\nuse SigNet without modification *to make testing simpler* - keep the\nheader format the same to let\nSPV clients function without (significant) modification, etc. The\npoint of the whole thing is to\nmake testing as easy as possible, why would we do otherwise.\n\nI guess Kalle (and AJ) can answer this question better than me but my\nunderstanding is that the motivation for Signet was that testnet\ndeviated erratically from mainnet behavior (e.g. long delays before\nany blocks were mined followed by a multitude of blocks mined in a\nshort period of time) which meant it wasn't conducive to normal\ntesting of applications. Why would you want a mainnet like chain? To\ncheck if your application works on a mainnet like chain without\nrisking any actual value before moving to mainnet. The same purpose as\ntestnet but more reliably resembling mainnet behavior. You are well\nwithin your rights to demand more than that but my preference would be\nto push some of those demands to custom signets rather than the\ndefault Signet.\n\nTesting out proposed soft forks in advance of them being considered\nfor activation would already be introducing a dimension of complexity\nthat is going to be hard to manage [0]. I'm generally of the view that\nif you are going to introduce a complexity dimension, keep the other\ndimensions as vanilla as possible. Otherwise you are battling\ncomplexity in multiple different dimensions and it becomes hard or\nimpossible to maintain it and meet your initial objectives.\n\nBut if this feature of extremely regular re-orgs is an in demand\nfeature for testers I think the question then becomes what the default\nbe (I would suggest re-orgs every 8 hours rather than no re-orgs at\nall) and then the alternative which you can switch to, re-orgs every\nblock or every 6 blocks or whatever.\n\n> I believe my suggestion was not correctly understood. I'm not suggesting *users* sign blocks or\notherwise do anything manually here, only that the existing block\nproducers each generate a new key,\nand we then only sign reorgs with *those* keys. Users will be able to\nset a flag to indicate \"I want\nto accept sigs from either sets of keys, and see reorgs\" or \"I only\nwant sigs from the non-reorg\nkeys, and will consider the reorg keys-signed blocks invalid\"\n\nAh I did misunderstand, yes this makes more sense. Thanks for the correction.\n\n[0] https://bitcoin.stackexchange.com/questions/98642/can-we-experiment-on-signet-with-multiple-proposed-soft-forks-whilst-maintaining\n\nOn Fri, Sep 10, 2021 at 7:24 PM Matt Corallo <lf-lists at mattcorallo.com> wrote:\n>\n>\n>\n> On 9/10/21 06:05, Michael Folkson wrote:\n> >> I see zero reason whatsoever to not simply reorg ~every block, or as often as is practical. If users opt in to wanting to test with reorgs, they should be able to test with reorgs, not wait a day to test with reorgs.\n> >\n> > One of the goals of the default Signet was to make the default Signet\n> > resemble mainnet as much as possible. (You can do whatever you want on\n> > a custom signet you set up yourself including manufacturing a re-org\n> > every block if you wish.) Hence I'm a bit wary of making the behavior\n> > on the default Signet deviate significantly from what you might\n> > experience on mainnet. Given re-orgs don't occur that often on mainnet\n> > I can see the argument for making them more regular (every 8 hours\n> > seems reasonable to me) on the default Signet but every block seems\n> > excessive. It makes the default Signet into an environment for purely\n> > testing whether your application can withstand various flavors of edge\n> > case re-orgs. You may want to test whether your application can\n> > withstand normal mainnet behavior (no re-orgs for long periods of\n> > time) first before you concern yourself with re-orgs.\n>\n> Huh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to\n> use SigNet without modification *to make testing simpler* - keep the header format the same to let\n> SPV clients function without (significant) modification, etc. The point of the whole thing is to\n> make testing as easy as possible, why would we do otherwise.\n>\n> Further, because one goal here is to enable clients to opt in or out of the reorg chain at will\n> (presumably by just changing one config flag in bitcoin.conf), why would we worry about making it\n> \"similar to mainnet\". If users want an experience \"similar to mainnet\", they can simply turn off\n> reorgs and they'll see a consistent chain moving forward which never reorgs, similar to the\n> practical experience of mainnet.\n>\n> Once you've opted into reorgs, you almost certainly are looking to *test* reorgs - you just\n> restarted Bitcoin Core with the reorg flag set, waiting around for a reorg after doing that seems\n> like the experience of testnet3 today, and the whole reason why we wanted signet to begin with -\n> things happen sporadically and inconsistently, making developers wait around forever. Please lets\n> not replicate the \"gotta wait for blocks before I can go to lunch\" experience of testnet today on\n> signet, I'm tired of eating lunch late.\n>\n> >> Why bother with a version bit? This seems substantially more complicated than the original proposal that surfaced many times before signet launched to just have a different reorg signing key. Thus, users who wish to follow reorgs can use a 1-of-2 (or higher multisig) and users who wish to not follow reorgs would use a 1-of-1 (or higher multisig), simply marking the reorg blocks as invalid without touching any header bits that non-full clients will ever see.\n> >\n> > If I understand this correctly this is introducing a need for users to\n> > sign blocks when currently with the default Signet the user does not\n> > need to concern themselves with signing blocks. That is entirely left\n> > to the network block signers of the default Signet (who were AJ and\n> > Kalle last time I checked). Again I don't think this additional\n> > complexity is needed on the default Signet when you can set up your\n> > own custom Signet if you want to test edge case scenarios that deviate\n> > significantly from what you are likely to experience on mainnet. A\n> > flag set via a configuration argument (the AJ, 0xB10C proposal) with\n> > no-reorgs (or 8 hour re-orgs) as the default seems to me like it would\n> > introduce no additional complexity to the casual (or alpha stage)\n> > tester experience though of course it introduces implementation\n> > complexity.\n> >\n> > To move the default Signet in the direction of resembling mainnet even\n> > closer would be to randomly generate batches of transactions to fill\n> > up blocks and create a fee market. It would be great to be able to\n> > test features like RBF and Lightning unhappy paths (justice\n> > transactions, perhaps even pinning attacks etc) on the default Signet\n> > in future.\n>\n> I believe my suggestion was not correctly understood. I'm not suggesting *users* sign blocks or\n> otherwise do anything manually here, only that the existing block producers each generate a new key,\n> and we then only sign reorgs with *those* keys. Users will be able to set a flag to indicate \"I want\n> to accept sigs from either sets of keys, and see reorgs\" or \"I only want sigs from the non-reorg\n> keys, and will consider the reorg keys-signed blocks invalid\"\n>\n> Matt\n\n\n\n-- \nMichael Folkson\nEmail: michaelfolkson at gmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-09-10T19:22:00",
                "message_text_only": "Fwiw, your email client is broken and does not properly quote in the plaintext copy. I believe this \nis a known gmail bug, but I'd recommend avoiding gmail's web interface for list posting :).\n\nOn 9/10/21 12:00, Michael Folkson wrote:\n>> Huh? Why would the goal be to match mainnet? The goal, as I understand it, is to allow software to\n> use SigNet without modification *to make testing simpler* - keep the\n> header format the same to let\n> SPV clients function without (significant) modification, etc. The\n> point of the whole thing is to\n> make testing as easy as possible, why would we do otherwise.\n> \n> I guess Kalle (and AJ) can answer this question better than me but my\n> understanding is that the motivation for Signet was that testnet\n> deviated erratically from mainnet behavior (e.g. long delays before\n> any blocks were mined followed by a multitude of blocks mined in a\n> short period of time) which meant it wasn't conducive to normal\n> testing of applications. Why would you want a mainnet like chain? To\n> check if your application works on a mainnet like chain without\n> risking any actual value before moving to mainnet. The same purpose as\n> testnet but more reliably resembling mainnet behavior. You are well\n> within your rights to demand more than that but my preference would be\n> to push some of those demands to custom signets rather than the\n> default Signet.\n\nHuh? You haven't made an argument here as to why such a chain is easier to test with, only that we \nshould \"match mainnet\". Testing on mainnet sucks, 99% of the time testing on mainnet involves no \nreorgs, which *doesn't* match in-the-field reality of mainnet, with occasional reorgs. Matching \nmainnet's behavior is, in fact, a terrible way to test if your application will run fine on mainnet.\n\nMy point is that the goal should be making it easier to test. I'm not entirely sure why there's \ndebate here.  I *regularly* have lunch late because I'm waiting for blocks either on mainnet or \ntestnet3, and would quite like to avoid that in the future. It takes *forever* to test things on \nmainnet and testnet3, matching their behavior would mean its equally impossible to test things on \nmainnet and testnet3, why is that something we should stirve for?\n\n\n> Testing out proposed soft forks in advance of them being considered\n> for activation would already be introducing a dimension of complexity\n> that is going to be hard to manage [0]. I'm generally of the view that\n> if you are going to introduce a complexity dimension, keep the other\n> dimensions as vanilla as possible. Otherwise you are battling\n> complexity in multiple different dimensions and it becomes hard or\n> impossible to maintain it and meet your initial objectives.\n\nYep! Great reason to not have any probabilistic nonsense or try to match mainnet or something on \nsignet, just make it deterministic, reorg once a block or twice an our or whatever and call it a day!\n\nMatt"
            },
            {
                "author": "David A. Harding",
                "date": "2021-09-10T20:00:05",
                "message_text_only": "On Fri, Sep 10, 2021 at 11:24:15AM -0700, Matt Corallo via bitcoin-dev wrote:\n> I'm [...] suggesting [...] that the existing block producers each\n> generate a new key, and we then only sign reorgs with *those* keys.\n> Users will be able to set a flag to indicate \"I want to accept sigs\n> from either sets of keys, and see reorgs\" or \"I only want sigs from\n> the non-reorg keys, and will consider the reorg keys-signed blocks\n> invalid\"\n\nThis seems pretty useful to me.  I think we might want multiple sets of\nkeys:\n\n0. No reorgs\n\n1. Periodic reorgs of small to moderate depth for ongoing testing\nwithout excessive disruption (e.g. the every 8 hours proposal).  I think\nthis probably ought to be the default-default `-signet` in Bitcoin Core\nand other nodes.\n\n2. Either frequent reorgs (e.g. every block) or a webapp that generates\nreorgs on demand to further reduce testing delays.\n\nIf we can only have two, I'd suggest dropping 0.  I think it's already\nthe case that too few people test their software with reorgs.\n\n-Dave\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210910/4dc3cf45/attachment.sig>"
            },
            {
                "author": "Michael Folkson",
                "date": "2021-09-13T12:30:31",
                "message_text_only": "> Can you explain the motivation for this? From where I sit, as far as I know, I should basically be > a prime example of the target market for public signet - someone developing bitcoin applications > with regular requirements to test those applications with other developers without\n> jumping through hoops to configure software the same across the globe and set up miners.\n> With blocks > being slow and irregular, I\u2019m basically not benefited at all by signet and will\n> stick with testnet3/mainnet testing, which both suck.\n\nOn testnet3 you can realistically go days without blocks being found\n(and conversely thousands of blocks can be found in a day), the block\ndiscovery time variance is huge. Of course this is probabilistically\npossible on mainnet too but the probability of this happening is close\nto zero. Here[0] is an example of 16,000 blocks being found in a day\non testnet3.\n\nOn signet block discovery time variance mirrors mainnet.\n\nOn mainnet you are risking Bitcoin with actual monetary value. If you\ndon't mind doing this then you don't need testnet3, signet or anything\nelse. In addition proposed soft forks may be activated on signet (and\ncould also be on testnet3) well before they are considered for\nactivation on mainnet for testing and experimentation purposes.\n\n[0] https://web.archive.org/web/20160910173004/https://blog.blocktrail.com/2015/04/in-the-darkest-depths-of-testnet3-16k-blocks-were-found-in-1-day/\n\n-- \nMichael Folkson\nEmail: michaelfolkson at gmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-09-13T16:24:29",
                "message_text_only": "> On Sep 13, 2021, at 05:30, Michael Folkson <michaelfolkson at gmail.com> wrote:\n> \n> \ufeff\n>> \n>> Can you explain the motivation for this? From where I sit, as far as I know, I should basically be > a prime example of the target market for public signet - someone developing bitcoin applications > with regular requirements to test those applications with other developers without\n>> jumping through hoops to configure software the same across the globe and set up miners.\n>> With blocks > being slow and irregular, I\u2019m basically not benefited at all by signet and will\n>> stick with testnet3/mainnet testing, which both suck.\n> \n> On testnet3 you can realistically go days without blocks being found\n> (and conversely thousands of blocks can be found in a day), the block\n> discovery time variance is huge. Of course this is probabilistically\n> possible on mainnet too but the probability of this happening is close\n> to zero. Here[0] is an example of 16,000 blocks being found in a day\n> on testnet3.\n\nBlocks too fast isn\u2019t generally an issue when waiting for blocks to test, and hooking up a miner is probably less work on testnet3 than creating a multi-party private signet with miners. In any case, you didn\u2019t address the substance of the point - we can do better to make it a good platform for testing\u2026. Why aren\u2019t we?"
            }
        ],
        "thread_summary": {
            "title": "Reorgs on SigNet - Looking for feedback on approach and parameters",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Michael Folkson",
                "David A. Harding",
                "Anthony Towns",
                "0xB10C",
                "vjudeu at gazeta.pl",
                "Matt Corallo",
                "Greg Sanders"
            ],
            "messages_count": 17,
            "total_messages_chars_count": 62321
        }
    },
    {
        "title": "[bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2021-09-09T06:41:38",
                "message_text_only": "Hello world,\n\nA couple of years ago I had a flight of fancy [0] imagining how it\nmight be possible for everyone on the planet to use bitcoin in a\nmostly decentralised/untrusted way, without requiring a block size\nincrease. It was a bit ridiculous and probably doesn't quite hold up,\nand beyond needing all the existing proposals to be implemented (taproot,\nANYPREVOUT, CTV, eltoo, channel factories), it also needed a covenant\nopcode [1]. I came up with something that I thought fit well with taproot,\nbut couldn't quite figure out how to use it for anything other than my\nridiculous scheme, so left it at that.\n\nBut recently [2] Greg Maxwell emailed me about his own cool idea for a\ncovenant opcode, which turned out to basically be a reinvention of the\nsame idea but with more functionality, a better name and a less fanciful\nuse case; and with that inspiration, I think I've also now figured out\nhow to use it for a basic vault, so it seems worth making the idea a\nbit more public.\n\nI'll split this into two emails, this one's the handwavy overview,\nthe followup will go into some of the implementation complexities.\n\n\n\nThe basic idea is to think about \"updating\" a utxo by changing the\ntaproot tree.\n\nAs you might recall, a taproot address is made up from an internal public\nkey (P) and a merkle tree of scripts (S) combined via the formula Q=P+H(P,\nS)*G to calculate the scriptPubKey (Q). When spending using a script,\nyou provide the path to the merkle leaf that has the script you want\nto use in the control block. The BIP has an example [3] with 5 scripts\narranged as ((A,B), ((C,D), E)), so if you were spending with E, you'd\nreveal a path of two hashes, one for (AB), then one for (CD), then you'd\nreveal your script E and satisfy it.\n\nSo that makes it relatively easy to imagine creating a new taproot address\nbased on the input you're spending by doing some or all of the following:\n\n * Updating the internal public key (ie from P to P' = P + X)\n * Trimming the merkle path (eg, removing CD)\n * Removing the script you're currently executing (ie E)\n * Adding a new step to the end of the merkle path (eg F)\n\nOnce you've done those things, you can then calculate the new merkle\nroot by resolving the updated merkle path (eg, S' = MerkleRootFor(AB,\nF, H_TapLeaf(E))), and then calculate a new scriptPubKey based on that\nand the updated internal public key (Q' = P' + H(P', S')).\n\nSo the idea is to do just that via a new opcode \"TAPLEAF_UPDATE_VERIFY\"\n(TLUV) that takes three inputs: one that specifies how to update the\ninternal public key (X), one that specifies a new step for the merkle path\n(F), and one that specifies whether to remove the current script and/or\nhow many merkle path steps to remove. The opcode then calculates the\nscriptPubKey that matches that, and verifies that the output corresponding\nto the current input spends to that scriptPubKey.\n\nThat's useless without some way of verifying that the new utxo retains\nthe bitcoin that was in the old utxo, so also include a new opcode\nIN_OUT_AMOUNT that pushes two items onto the stack: the amount from this\ninput's utxo, and the amount in the corresponding output, and then expect\nanyone using TLUV to use maths operators to verify that funds are being\nappropriately retained in the updated scriptPubKey.\n\n\n\nHere's two examples of how you might use this functionality.\n\nFirst, a basic vault. The idea is that funds are ultimately protected\nby a cold wallet key (COLD) that's inconvenient to access but is as\nsafe from theft as possible. In order to make day to day transactions\nmore convenient, a hot wallet key (HOT) is also available, which is\nmore vulnerable to theft. The vault design thus limits the hot wallet\nto withdrawing at most L satoshis every D blocks, so that if funds are\nstolen, you lose at most L, and have D blocks to use your cold wallet\nkey to re-secure the funds and prevent further losses.\n\nTo set this up with TLUV, you construct a taproot output with COLD as\nthe internal public key, and a script that specifies:\n\n * The tx is signed via HOT\n * <D> CSV -- there's a relative time lock since the last spend\n * If the input amount is less than L + dust threshold, fine, all done,\n   the vault can be emptied.\n * Otherwise, the output amount must be at least (the input amount -\n   L), and do a TLUV check that the resulting sPK is unchanged\n\nSo you can spend up to \"L\" satoshis via the hot wallet as long as you\nwait D blocks since the last spend, and can do whatever you want via a\nkey path spend with the cold wallet.\n\nYou could extend this to have a two phase protocol for spending, where\nfirst you use the hot wallet to say \"in D blocks, allow spending up to\nL satoshis\", and only after that can you use the hot wallet to actually\nspend funds. In that case supply a taproot sPK with COLD as the internal\npublic key and two scripts, the \"release\" script, which specifies:\n\n * The tx is signed via HOT\n * Output amount is greater or equal to the input amount.\n * Use TLUV to check:\n   + the output sPK has the same internal public key (ie COLD)\n   + the merkle path has one element trimmed\n   + the current script is included\n   + a new step is added that matches either H_LOCKED or H_AVAILABLE as\n     described below (depending on whether 0 or 1 was provided as\n     witness info)\n\nThe other script is either \"locked\" (which is just \"OP_RETURN\") or\n\"available\" which specifies:\n\n * The tx is signed via HOT\n * <D> CSV -- there's a relative time lock since the last spend (ie,\n   when the \"release\" script above was used)\n * If the input amount is less than L, fine, all done, the vault can\n   be emptied\n * Otherwise, the output amount must be at least (the input amount minus\n   L), and via TLUV, check the resulting sPK keeps the internal pubkey\n   unchanged, keeps the merkle path, drops the current script, and adds\n   H_LOCKED as the new step.\n\nH_LOCKED and H_AVAILABLE are just the TapLeaf hash corresponding to the\n\"locked\" and \"available\" scripts.\n\nI believe this latter setup matches the design Bryan Bishop talked about\na couple of years ago [4], with the benefit that it's fully recursive,\nallows withdrawals to vary rather than be the fixed amount L (due to not\nrelying on pre-signed transactions), and generally seems a bit simpler\nto work with.\n\n\n\nThe second scheme is allowing for a utxo to represent a group's pooled\nfunds. The idea being that as long as everyone's around you can use\nthe taproot key path to efficiently move money around within the pool,\nor use a single transaction and signature for many people in the pool\nto make payments. But key path spends only work if everyone's available\nto sign -- what happens if someone disappears, or loses access to their\nkeys, or similar? For that, we want to have script paths to allow other\npeople to reclaim their funds even if everyone else disappears. So we\nsetup scripts for each participant, eg for Alice:\n\n * The tx is signed by Alice\n * The output value must be at least the input value minus Alice's balance\n * Must pass TLUV such that:\n   + the internal public key is the old internal pubkey minus Alice's key\n   + the currently executing script is dropped from the merkle path\n   + no steps are otherwise removed or added\n\nThe neat part here is that if you have many participants in the pool,\nthe pool continues to operate normally even if someone makes use of the\nescape hatch -- the remaining participants can still use the key path to\nspend efficiently, and they can each unilaterally withdraw their balance\nvia their own script path. If everyone decides to exit, whoever is last\ncan spend the remaining balance directly via the key path.\n\nCompared to having on-chain transactions using non-pooled funds, this\nis more efficient and private: a single one-in, one-out transaction\nsuffices for any number of transfers within the pool, and there's no\non-chain information about who was sending/receiving the transfers, or\nhow large the transfers were; and for transfers out of the pool, there's\nno on-chain indication which member of the pool is sending the funds,\nand multiple members of the pool can send funds to multiple destinations\nwith only a single signature. The major constraint is that you need\neveryone in the pool to be online in order to sign via the key path,\nwhich provides a practical limit to how many people can reasonably be\nincluded in a pool before there's a breakdown.\n\nCompared to lightning (eg eltoo channel factories with multiple\nparticipants), the drawback is that no transfer is final without an\nupdated state being committed on chain, however there are also benefits\nincluding that if one member of the pool unilaterally exits, that\ndoesn't reveal the state of anyone remaining in the pool (eg an eltoo\nfactory would likely reveal the balances of everyone else's channels at\nthat point).\n\nA simpler case for something like this might be for funding a joint\nventure -- suppose you're joining with some other early bitcoiners to\nbuy land to build a citadel, so you each put 20 BTC into a pooled utxo,\nready to finalise the land purchase in a few months, but you also want\nto make sure you can reclaim the funds if the deal falls through. So\nyou might include scripts like the above that allow you to reclaim your\nbalance, but add a CLTV condition preventing anyone from doing that until\nthe deal's deadline has passed. If the deal goes ahead, you all transfer\nthe funds to the vendor via the keypath; if it doesn't work out, you\nhopefully return your funds via the keypath, but if things turn really\nsour, you can still just directly reclaim your 20 BTC yourself via the\nscript path.\n\n\n\nI think a nice thing about this particular approach to recursive covenants\nat a conceptual level is that it automatically leaves the key path as an\nescape mechanism -- rather than having to build a base case manually,\nand have the risk that it might not work because of some bug, locking\nyour funds into the covenant permanently; the escape path is free, easy,\nand also the optimal way of spending things when everything is working\nright. (Of course, you could set the internal public key to a NUMS point\nand shoot yourself in the foot that way anyway)\n\n\n\nI think there's two limitations of this method that are worth pointing out.\n\nFirst it can't tweak scripts in areas of the merkle tree that it can't\nsee -- I don't see a way of doing that particularly efficiently, so maybe\nit's best just to leave that as something for the people responsible for\nthe funds to negotiate via the keypath, in which case it's automatically\nboth private and efficient since all the details stay off-chain, anyway\n\nAnd second, it doesn't provide a way for utxos to \"interact\", which is\nsomething that is interesting for automated market makers [5], but perhaps\nonly interesting for chains aiming to support multiple asset types,\nand not bitcoin directly. On the other hand, perhaps combining it with\nCTV might be enough to solve that, particularly if the hash passed to\nCTV is constructed via script/CAT/etc.\n\n\n\n(I think everything described here could be simulated with CAT and\nCHECKSIGFROMSTACK (and 64bit maths operators and some way to access\nthe internal public key), the point of introducing dedicated opcodes\nfor this functionality rather than (just) having more generic opcodes\nwould be to make the feature easy to use correctly, and, presuming it\nactually has a wide set of use cases, to make it cheap and efficient\nboth to use in wallets, and for nodes to validate)\n\nCheers,\naj\n\n[0] https://gist.github.com/ajtowns/dc9a59cf0a200bd1f9e6fb569f76f7a0\n\n[1] Roughly, the idea was that if you have ~9 billion people using\n    bitcoin, but can only have ~1000 transactions per block, then you\n    need have each utxo represent a significant number of people. That\n    means that you need a way of allowing the utxo's to be efficiently\n    spent, but need to introduce some level of trust since expecting\n    many people to constantly be online seems unreliable, but to remain\n    mostly decentralised/untrusted, you want to have some way of limiting\n    how much trust you're introducing, and that's where covenants come in.\n\n[2] Recently in covid-adjusted terms, or on the bitcoin consensus\n    change scale anyway...\n    https://mobile.twitter.com/ajtowns/status/1385091604357124100 \n\n[3] https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki#Constructing_and_spending_Taproot_outputs \n\n[4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017231.html\n\n[5] The idea behind an automated market maker being that you setup a\n    script that says \"you can withdraw x BTC if you deposit f(x) units of\n    USDT, or you can withdraw g(x) units of USDT if you deposit x units\n    of BTC\", with f(x)/x giving the buy price, and f(x)>g(x) meaning\n    you make a profit. Being able to specify a covenant that links the\n    change in value to the BTC utxo (+/-x) and the change in value to\n    the USDT utxo (+f(x) or -g(x)) is what you'd need to support this\n    sort of use case, but TLUV doesn't provide a way to do that linkage."
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-09T06:53:30",
                "message_text_only": "On Thu, Sep 09, 2021 at 04:41:38PM +1000, Anthony Towns wrote:\n> I'll split this into two emails, this one's the handwavy overview,\n> the followup will go into some of the implementation complexities.\n\n(This is informed by discussions with Greg, Matt Corallo, David Harding\nand Jeremy Rubin; opinions and mistakes my own, of course)\n\n\n\nFirst, let's talk quickly about IN_OUT_AMOUNT. I think the easiest way to\ndeal with it is just a single opcode that pushes two values to the stack;\nhowever it could be two opcodes, or it could even accept a parameter\nletting you specify which input (and hence which corresponding output)\nyou're talking about (-1 meaning the current input perhaps). \n\nAnyway, a big complication here is that amounts in satoshis require up\nto 51 bits to represent them, but script only allows you to do 32 bit\nmaths. However introducing IN_OUT_AMOUNT already means using an OP_SUCCESS\nopcode, which in turn allows us to arbitrarily redefine the behaviour\nof other opcodes -- so we can use the presence of IN_OUT_AMOUNT in the\nscript to upgrade ADD, SUB, and the comparison operators to support 64\nbit values. Enabling MUL, DIV and MOD might also be worthwhile.\n\n\n\nMoving onto TLUV. My theory is that it pops three items off the stack. The\ntop of the stack is \"C\" the control integer; next is \"H\" the\nadditional path step; and finally \"X\" the tweak for the internal\npubkey. If \"H\" is the empty vector, no additional path step is\nadded; otherwise it must be 32 bytes. If \"X\" is the empty vector,\nthe internal pubkey is not tweaked; otherwise it must be a 32 byte\nx-only pubkey.\n\nThe low bit of C indicates the parity of X; if it's 0, X has even y,\nif it's 1, X has odd y.\n\nThe next bit of C indicates whether the current script is dropped from\nthe merkle path, if it's 0, the current script is kept, if it's 1 the\ncurrent script is dropped.\n\nThe remaining bits of C (ie C >> 2) are the number of steps in the merkle\npath that are dropped. (If C is negative, behaviour is to be determined\n-- either always fail, or always succeed and left for definition via\nfuture soft-fork)\n\nFor example, suppose we have a taproot utxo that had 5 scripts\n(A,B,C,D,E), calculated as per the example in BIP 341 as:\n\n    AB = H_TapBranch(A, B)\n    CD = H_TapBranch(C, D)\n    CDE = H_TapBranch(CD, E)\n    ABCDE = H_TapBranch(AB, CDE)\n\nAnd we're spending using script E, in that case the control block includes\nthe script E, and the merkle path to it, namely (AB, CD).\n\nSo here's some examples of what you could do with TLUV to control how\nthe spending scripts can change, between the input sPK and the output sPK.\n\nAt it's simplest, if we used the script \"0 0 0 TLUV\", then that says we\nkeep the current script, keep all steps in the merkle path, don't add\nany new ones, and don't change the internal public key -- that is that\nwe want to resulting sPK to be exactly the same as the one we're spending.\n\nIf we used the script \"0 F 0 TLUV\" (H=F, C=0) then we keep the current\nscript, keep all the steps in the merkle path (AB and CD), and add\na new step to the merkle path (F), giving us:\n\n    EF = H_TapBranch(E, F)\n    CDEF =H_TapBranch(CD, EF)\n    ABCDEF = H_TapBranch(AB, CDEF)\n\nIf we used the script \"0 F 2 TLUV\" (H=F, C=2) then we drop the current\nscript, but keep all the other steps, and add a new step (effectively\nreplacing the current script with a new one):\n\n    CDF = H_TapBranch(CD, F)\n    ABCDF = H_TapBranch(AB, CDF)\n\nIf we used the script \"0 F 4 TLUV\" (H=F, C=4) then we keep the current\nscript, but drop the last step in the merkle path, and add a new step\n(effectively replacing the *sibling* of the current script):\n\n    EF = H_TapBranch(E, F)\n    ABEF = H_TapBranch(AB, EF)\n\nIf we used the script \"0 0 4 TLUV\" (H=empty, C=4) then we keep the current\nscript, drop the last step in the merkle path, and don't add anything new\n(effectively dropping the sibling), giving just:\n\n    ABE = H_TapBranch(AB, E)\n\n\n\nImplementing the release/locked/available vault construct would then\nlook something like this:\n\nLocked script = \"OP_RETURN\"\nAvailable script = \"<HOT> CHECKSIGVERIFY IN_OUT_AMOUNT SWAP <X> SUB DUP 0 GREATERTHAN IF GREATERTHANOREQUAL VERIFY 0 <H_LOCKED> 2 TLUV ELSE 2DROP ENDIF <D> CSV\"\nRelease script = \"<HOT> CHECKSIGVERIFY IF <H_LOCKED> ELSE <H_AVAILABLE> ENDIF 0 SWAP 4 TLUV  INPUTAMOUNT OUTPUTAMOUNT LESSTHANOREQUAL\"\nHOT = 32B hot wallet pubkey\nX = maximum amount spendable via hot wallet at any time\nD = compulsory delay between releasing funds and being able to spend them\nH_LOCKED = H_TapLeaf(locked script)\nH_AVAILABLE= H_TapLeaf(available script)\nInternal public key = 32B cold wallet pubkey\n\n\n\nMoving on to the pooled scheme and actually updating the internal pubkey\nis, unfortunately, where things start to come apart. In particular,\nsince taproot uses 32-byte x-only pubkeys (with implicit even-y) for the\nscriptPubKey and the internal public key, we have to worry about what\nhappens if, eg, A,B,C and A+B+C all have even-y, but (A+B)=(A+B+C)-C does\nnot have even-y. In that case allowing C to remove herself from the pool,\nmight result in switching from the scriptPubKey Qabc to the scriptPubKey\nQab as follows:\n\n     Qabc = (A+B+C) + H(A+B+C, (Sa, (Sb, Sc)))*G\n     Qab = -(A+B) + H( -(A+B), (Sa, Sb)*G\n\nThat's fine so far, but what happens if B then removes himself from the\npool? You take the internal public key, which turns out to be -(A+B)\nsince (A+B) did not have even y, and then subtract B, but that gives you\n-A-2B instead of just A. So B obtains his funds, but B's signature hasn't\nbeen cancelled out from the internal public key, so is still required\nin order to do key path spends, which is definitely not what we want.\n\nIf we ignore that caveat (eg, having TLUV consider it to be an error if\nyou end up an internal public key that has odd-y) then the scripts for\nexiting the pool are straightforward (if your balance is BAL and your\nkey is KEY):\n\n    <KEY> DUP \"\" 1 TLUV\n    CHECKSIGVERIFY \n    IN_OUT_AMOUNT SUB <BAL> GREATERTHANOREQUAL\n\nIt seems like \"just ignore it\" might be feasible for modest sized pools --\njust choose A, B, C, D.. so that every combination of them (A+B+C, A+D,\netc) sums to a point that happens to have even-y and have each participant\nin the pool verify that prior to using the pool. If I got my maths right,\nyou'll need to do about (2**n) trials to find a set of lucky points,\nbut each unlucky set will tend to fail quickly, leading to amortized\nconstant time for each test, so something like 3*(2**n) work overall. So\nas long as n is no more than 20 or 30, that should be reasonably feasible.\n\nTo deal with it properly, you need to have the utxo commit to the parity\nof the internal public key and have some way to find out that value when\nusing TLUV. There are probably three plausible ways of doing this.\n\nThe straightforward way is just to commit to it in the scriptPubKey --\nthat is, rather than taproot's approach of setting Q = P + H(P, S)*G where\nP is a 32 byte x-only pubkey, also commit to the parity of P in the H(P,\nS) step, and reveal the parity of the internal public key as part of the\ncontrol block when spending via the script path, in addition to revealing\nthe parity of the scriptPubKey point as we do already. Since taproot is\nalready locked in for activation, it's too late to change this behaviour\nfor taproot addresses, but we could include this in a future soft-fork\nthat enabled entroot or similar, or we could make this the behaviour of\n(eg) 33B segwit v1 addresses that begin with 0x00, or similar.\n\nIf we don't commit to the parity in the scriptPubKey, there are two other\nways to commit to it in the utxo: either by having script ensure it is\ncommitted to it in the value, or by extending the data that's saved in\nthe utxo database.\n\nTo commit to it in the value, you might do something like:\n\n    <P> <H> IN_OUT_AMOUNT 2 MOD SWAP 2 MOD TUCK EQUAL 2 MUL ADD TLUV\n\nand change TLUV's control parameter to be: C&1 = add/subtract the point,\nC&2 = require the result to be even/odd y (with C&4 and C>>3 controlling\nwhether the current script and how many merkle paths are dropped). The\nidea being to require that, if the utxo's value in satoshis is 0 mod\n2, you subtract the point, and if it's 1 mod 2, you add the point,\nand that the *output* amount's value in satoshis is different (mod 2)\nfrom the input amount's value (mod 2), exactly when the resulting point\nends up with odd y.  Combined with a rule to ensure the output amount\ndoesn't decrease by more than your balance, this would effectively mean\nthat if half the time when you withdraw your balance you'll have to pay\na 1 satoshi fee to the remaining pool members so the the parity of the\nremaining value is correct, which is inelegant, but seems like workable.\n\nThe other approach sits somewhere between those two, and would involve\nadding a flag to each entry in the utxo database to say whether the\ninternal public key had been inverted. This would only be set if the\nutxo had been created via a spending script that invoked TLUV, and TLUV\nwould use the flag to determine whether to add/subtract the provided\npoint. That seems quite complicated to implement to me, particularly if\nyou want to allow the flag to be able to be set by future opcodes that\nwe haven't thought of yet.\n\n\n\nAll of this so far assumed that the hashes for any new merkle steps are\nfixed when the contract is created. If \"OP_CAT\" or similar were enabled,\nhowever, you could construct those hashes programmatically in script,\nwhich might lead to some interesting behaviour. For example, you could\nconstruct a script that says \"allow anyone to add themselves to the\nbuy-a-citadel pool, as long as they're contributing at least 10 BTC\",\nwhich would then verify they have control of the pubkey they're adding,\nand allow them to add a script that lets them pull their 10 BTC back\nout via that pubkey, and participate in key path spends in the same\nway as everyone else. Of course, that sort of feature probably also\nnaturally extends to many of the \"covenants considered harmful\" cases,\neg a dollar-auction-like-contract: \"Alice can spend this utxo after 1000\nconfirmations\" or \"anyone who increases the balance by 0.1 BTC can swap\nAlice's pubkey for their own in the sibling script to this\".\n\nAn interesting thing to note is that constructing the script can sometimes\nbe more efficient than hardcoding it, eg, I think\n\n    \"TapLeaf\" SHA256 DUP CAT [0xc0016a] CAT SHA256\n\nis correct for calculating the hash for the \"OP_RETURN\" script, and at\n~17 bytes should be cheaper than the ~33 bytes it would take to hardcode\nthe hash.\n\nTo construct a new script programmatically you almost certainly need to\nuse templates, eg\n\n    SIZE 32 EQUALVERIFY [0xc02220] SWAP CAT [0xac] CAT\n    \"TapLeaf\" SHA256 DUP CAT SWAP CAT SHA256\n\nmight take a public key off the stack and turn it into the hash for a\nscript that expects a signature from that pubkey. I believe you could\nconstruct multiple scripts and combine them via\n\n    CAT \"TapBranch\" SHA256 DUP CAT SWAP CAT SHA256\n\nor similar as well.\n\nThere's a serious caveat with doing that in practice though: if you allow\npeople to add in arbitrary opcodes when constructing the new script,\nthey could choose to have that opcode be one of the \"OP_SUCCESS\" opcodes,\nand, if they're a miner, use that to bypass the covenant constraints\nentirely. So if you want to think about this, the template being filled\nin probably has to be very strict, eg including the specific PUSH opcode\nfor the data being provided in the witness, and checking that the length\nof the witness data exactly matches the PUSH opcode being used.\n\nCheers,\naj"
            },
            {
                "author": "darosior",
                "date": "2021-09-09T12:56:10",
                "message_text_only": "Hi Anthony,\n\n\nThis post is a follow-up to your insight on Twitter [0], sent here for better\nposterity, accessibility and readability than Twitter. And also to motivate this\nidea by giving another concrete [1] usecase for it.\n\nRevault [2] is a multi-party \"vault\" protocol. It involves 2 sets of participants\nthat may or may not intersect (although i expect the second one to often be a subset\nof the first one). The stakeholders, analogous to the \"cold keys\", receive coins on\na (large) N-of-N multisig and presign an Unvault transaction which creates an Unvault\noutput which pays to either the (small) K-of-M multisig of the managers after a timelock\nor to the N-of-N immediately (allowing for a Cancel transaction).\n\nThis allows for partial delegation of the funds, and some automated policies (can't\nbroadcast the Unvault outside business hours, can't unvault more than <limit> BTC a\nweek, etc..) that can be enforced by watchtowers. That's nice, but it would be even\nnicer if we could have policies on the Spend transaction (the one created by the\nmanagers to spend the Unvault output) itself to further restrict how the coin can move [3].\n\nBut in order to do so, you'd need the managers to disclaim the Spend transaction they\nare going to use before broadcasting the Unvault and somehow commit to it at unvaulting\ntime. Apart from stupid hacks [4] i could not find a reasonable covenant design as a\nsolution to this issue.\nIt think TLUV fixes this.\n\nThe idea (your idea, actually) is to receive coins not to a N-of-N anymore but to a\nTaproot with a branch which contains the manager multisig + a TLUV which would replace\nthe current branch being executed by a CSV + CTV which input hash value will be taken\nfrom the witness stack at Unvault broadcast. Therefore chosen by the managers at spending\ntime, and available for the entire duration of the timelock.\n\nSo, the scripts would be something like (assuming CAT, CTV, TLUV):\nV = max acceptable fees\nD = \"CTV <X> CSV DROP 1\"\nC = \"<32 bytes> D\"\nB = \"\n<pk man 1> CHECKSIG <pk man 2> CHECKSIGADD ... <pk man M> CHECKSIGADD <K> EQUALVERIFY\nIN_OUT_AMOUNT SUB <V> LESSTHANOREQUAL DUP VERIFY\nSIZE 32 EQUALVERIFY <0xc0 | len(D) + 32 + 1 | 0x20> SWAP CAT \"Tapleaf\" SHA256 DUP CAT SWAP CAT SHA256 0 SWAP 2 TLUV\n\"\nA = \"<pk stk 1> CHECKSIGVERIFY <pk stk 2> CHECKSIGVERIFY ... <pk stk N> CHECKSIG\"\n\nThe deposit output ScriptPubKey would be Taproot(A, B) [5].\nThe unvault output ScriptPubKey would be Taproot(A, C).\nThis also allows for a lot more flexibility (batching at the Unvault level [7], use RBF\ninstead of more wasteful CPFP, etc..) and creates a number of problems [6] on which\ni won't expand on. But it does the most important part: it enables it.\n\nLooking forward to more feedback on your proposal!\n\n\nThanks,\nAntoine\n\n\n\n[0] https://twitter.com/ajtowns/status/1435884659146059776?s=20\n[1] we've proposed Revault a year and a half ago, have been building it since. We\n    should have a first version released soon (tm).\n[2] https://github.com/revault\n[3] technically we do optionally offer this at the moment, but at the expense of a\n    reduction of security and a pretty ugly hack: by using \"anti-replay\" oracles\n    (cosigning servers that are only going to sign only once for a given prevout)\n[4] the last bad idea to date is \"have ANYPREVOUT, presign the Unvault with\n    SIGHASH_SINGLE, enforce that the Unvault output is only spent with a transaction\n    spending <same txid>:1 and have managers append an output to the Unvault enforcing\n    a covenant just before broadcast\"\n[5] as a branch because i don't know how to use the keypath spend for a multisig with\n    cold keys (yet).\n[6] as such you'd need a sig for canceling but not for unvaulting, so it reverses the\n    security model from \"can't do anything til everyone signed\" to \"can steal until\n    everyone has signed\" so you'd need a TLUV for the cancel spending path as well, but\n    then how to make this covenant non-replayable, flexible enough to feebump but not\n    enough be vulnerable to pining, etc..\n[7] Note that this means all Cancel must be confirmed to recover the funds but a single\n    one needs to in order to prevent a spending.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nLe jeudi 9 septembre 2021 \u00e0 8:53 AM, Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> On Thu, Sep 09, 2021 at 04:41:38PM +1000, Anthony Towns wrote:\n>\n> > I'll split this into two emails, this one's the handwavy overview,\n> >\n> > the followup will go into some of the implementation complexities.\n>\n> (This is informed by discussions with Greg, Matt Corallo, David Harding\n>\n> and Jeremy Rubin; opinions and mistakes my own, of course)\n>\n> First, let's talk quickly about IN_OUT_AMOUNT. I think the easiest way to\n>\n> deal with it is just a single opcode that pushes two values to the stack;\n>\n> however it could be two opcodes, or it could even accept a parameter\n>\n> letting you specify which input (and hence which corresponding output)\n>\n> you're talking about (-1 meaning the current input perhaps).\n>\n> Anyway, a big complication here is that amounts in satoshis require up\n>\n> to 51 bits to represent them, but script only allows you to do 32 bit\n>\n> maths. However introducing IN_OUT_AMOUNT already means using an OP_SUCCESS\n>\n> opcode, which in turn allows us to arbitrarily redefine the behaviour\n>\n> of other opcodes -- so we can use the presence of IN_OUT_AMOUNT in the\n>\n> script to upgrade ADD, SUB, and the comparison operators to support 64\n>\n> bit values. Enabling MUL, DIV and MOD might also be worthwhile.\n>\n> Moving onto TLUV. My theory is that it pops three items off the stack. The\n>\n> top of the stack is \"C\" the control integer; next is \"H\" the\n>\n> additional path step; and finally \"X\" the tweak for the internal\n>\n> pubkey. If \"H\" is the empty vector, no additional path step is\n>\n> added; otherwise it must be 32 bytes. If \"X\" is the empty vector,\n>\n> the internal pubkey is not tweaked; otherwise it must be a 32 byte\n>\n> x-only pubkey.\n>\n> The low bit of C indicates the parity of X; if it's 0, X has even y,\n>\n> if it's 1, X has odd y.\n>\n> The next bit of C indicates whether the current script is dropped from\n>\n> the merkle path, if it's 0, the current script is kept, if it's 1 the\n>\n> current script is dropped.\n>\n> The remaining bits of C (ie C >> 2) are the number of steps in the merkle\n>\n> path that are dropped. (If C is negative, behaviour is to be determined\n>\n> -- either always fail, or always succeed and left for definition via\n>\n> future soft-fork)\n>\n> For example, suppose we have a taproot utxo that had 5 scripts\n>\n> (A,B,C,D,E), calculated as per the example in BIP 341 as:\n>\n> AB = H_TapBranch(A, B)\n>\n> CD = H_TapBranch(C, D)\n>\n> CDE = H_TapBranch(CD, E)\n>\n> ABCDE = H_TapBranch(AB, CDE)\n>\n> And we're spending using script E, in that case the control block includes\n>\n> the script E, and the merkle path to it, namely (AB, CD).\n>\n> So here's some examples of what you could do with TLUV to control how\n>\n> the spending scripts can change, between the input sPK and the output sPK.\n>\n> At it's simplest, if we used the script \"0 0 0 TLUV\", then that says we\n>\n> keep the current script, keep all steps in the merkle path, don't add\n>\n> any new ones, and don't change the internal public key -- that is that\n>\n> we want to resulting sPK to be exactly the same as the one we're spending.\n>\n> If we used the script \"0 F 0 TLUV\" (H=F, C=0) then we keep the current\n>\n> script, keep all the steps in the merkle path (AB and CD), and add\n>\n> a new step to the merkle path (F), giving us:\n>\n> EF = H_TapBranch(E, F)\n>\n> CDEF =H_TapBranch(CD, EF)\n>\n> ABCDEF = H_TapBranch(AB, CDEF)\n>\n> If we used the script \"0 F 2 TLUV\" (H=F, C=2) then we drop the current\n>\n> script, but keep all the other steps, and add a new step (effectively\n>\n> replacing the current script with a new one):\n>\n> CDF = H_TapBranch(CD, F)\n>\n> ABCDF = H_TapBranch(AB, CDF)\n>\n> If we used the script \"0 F 4 TLUV\" (H=F, C=4) then we keep the current\n>\n> script, but drop the last step in the merkle path, and add a new step\n>\n> (effectively replacing the sibling of the current script):\n>\n> EF = H_TapBranch(E, F)\n>\n> ABEF = H_TapBranch(AB, EF)\n>\n> If we used the script \"0 0 4 TLUV\" (H=empty, C=4) then we keep the current\n>\n> script, drop the last step in the merkle path, and don't add anything new\n>\n> (effectively dropping the sibling), giving just:\n>\n> ABE = H_TapBranch(AB, E)\n>\n> Implementing the release/locked/available vault construct would then\n>\n> look something like this:\n>\n> Locked script = \"OP_RETURN\"\n>\n> Available script = \"<HOT> CHECKSIGVERIFY IN_OUT_AMOUNT SWAP <X> SUB DUP 0 GREATERTHAN IF GREATERTHANOREQUAL VERIFY 0 <H_LOCKED> 2 TLUV ELSE 2DROP ENDIF <D> CSV\"\n>\n> Release script = \"<HOT> CHECKSIGVERIFY IF <H_LOCKED> ELSE <H_AVAILABLE> ENDIF 0 SWAP 4 TLUV INPUTAMOUNT OUTPUTAMOUNT LESSTHANOREQUAL\"\n>\n> HOT = 32B hot wallet pubkey\n>\n> X = maximum amount spendable via hot wallet at any time\n>\n> D = compulsory delay between releasing funds and being able to spend them\n>\n> H_LOCKED = H_TapLeaf(locked script)\n>\n> H_AVAILABLE= H_TapLeaf(available script)\n>\n> Internal public key = 32B cold wallet pubkey\n>\n> Moving on to the pooled scheme and actually updating the internal pubkey\n>\n> is, unfortunately, where things start to come apart. In particular,\n>\n> since taproot uses 32-byte x-only pubkeys (with implicit even-y) for the\n>\n> scriptPubKey and the internal public key, we have to worry about what\n>\n> happens if, eg, A,B,C and A+B+C all have even-y, but (A+B)=(A+B+C)-C does\n>\n> not have even-y. In that case allowing C to remove herself from the pool,\n>\n> might result in switching from the scriptPubKey Qabc to the scriptPubKey\n>\n> Qab as follows:\n>\n> Qabc = (A+B+C) + H(A+B+C, (Sa, (Sb, Sc)))*G\n>\n> Qab = -(A+B) + H( -(A+B), (Sa, Sb)*G\n>\n> That's fine so far, but what happens if B then removes himself from the\n>\n> pool? You take the internal public key, which turns out to be -(A+B)\n>\n> since (A+B) did not have even y, and then subtract B, but that gives you\n>\n> -A-2B instead of just A. So B obtains his funds, but B's signature hasn't\n>\n> been cancelled out from the internal public key, so is still required\n>\n> in order to do key path spends, which is definitely not what we want.\n>\n> If we ignore that caveat (eg, having TLUV consider it to be an error if\n>\n> you end up an internal public key that has odd-y) then the scripts for\n>\n> exiting the pool are straightforward (if your balance is BAL and your\n>\n> key is KEY):\n>\n> <KEY> DUP \"\" 1 TLUV\n>\n>     CHECKSIGVERIFY\n>     IN_OUT_AMOUNT SUB <BAL> GREATERTHANOREQUAL\n>\n>\n> It seems like \"just ignore it\" might be feasible for modest sized pools --\n>\n> just choose A, B, C, D.. so that every combination of them (A+B+C, A+D,\n>\n> etc) sums to a point that happens to have even-y and have each participant\n>\n> in the pool verify that prior to using the pool. If I got my maths right,\n>\n> you'll need to do about (2n) trials to find a set of lucky points,\n>\n> but each unlucky set will tend to fail quickly, leading to amortized\n>\n> constant time for each test, so something like 3*(2n) work overall. So\n>\n> as long as n is no more than 20 or 30, that should be reasonably feasible.\n>\n> To deal with it properly, you need to have the utxo commit to the parity\n>\n> of the internal public key and have some way to find out that value when\n>\n> using TLUV. There are probably three plausible ways of doing this.\n>\n> The straightforward way is just to commit to it in the scriptPubKey --\n>\n> that is, rather than taproot's approach of setting Q = P + H(P, S)*G where\n>\n> P is a 32 byte x-only pubkey, also commit to the parity of P in the H(P,\n>\n> S) step, and reveal the parity of the internal public key as part of the\n>\n> control block when spending via the script path, in addition to revealing\n>\n> the parity of the scriptPubKey point as we do already. Since taproot is\n>\n> already locked in for activation, it's too late to change this behaviour\n>\n> for taproot addresses, but we could include this in a future soft-fork\n>\n> that enabled entroot or similar, or we could make this the behaviour of\n>\n> (eg) 33B segwit v1 addresses that begin with 0x00, or similar.\n>\n> If we don't commit to the parity in the scriptPubKey, there are two other\n>\n> ways to commit to it in the utxo: either by having script ensure it is\n>\n> committed to it in the value, or by extending the data that's saved in\n>\n> the utxo database.\n>\n> To commit to it in the value, you might do something like:\n>\n> <P> <H> IN_OUT_AMOUNT 2 MOD SWAP 2 MOD TUCK EQUAL 2 MUL ADD TLUV\n>\n> and change TLUV's control parameter to be: C&1 = add/subtract the point,\n>\n> C&2 = require the result to be even/odd y (with C&4 and C>>3 controlling\n>\n> whether the current script and how many merkle paths are dropped). The\n>\n> idea being to require that, if the utxo's value in satoshis is 0 mod\n>\n> 2, you subtract the point, and if it's 1 mod 2, you add the point,\n>\n> and that the output amount's value in satoshis is different (mod 2)\n>\n> from the input amount's value (mod 2), exactly when the resulting point\n>\n> ends up with odd y. Combined with a rule to ensure the output amount\n>\n> doesn't decrease by more than your balance, this would effectively mean\n>\n> that if half the time when you withdraw your balance you'll have to pay\n>\n> a 1 satoshi fee to the remaining pool members so the the parity of the\n>\n> remaining value is correct, which is inelegant, but seems like workable.\n>\n> The other approach sits somewhere between those two, and would involve\n>\n> adding a flag to each entry in the utxo database to say whether the\n>\n> internal public key had been inverted. This would only be set if the\n>\n> utxo had been created via a spending script that invoked TLUV, and TLUV\n>\n> would use the flag to determine whether to add/subtract the provided\n>\n> point. That seems quite complicated to implement to me, particularly if\n>\n> you want to allow the flag to be able to be set by future opcodes that\n>\n> we haven't thought of yet.\n>\n> All of this so far assumed that the hashes for any new merkle steps are\n>\n> fixed when the contract is created. If \"OP_CAT\" or similar were enabled,\n>\n> however, you could construct those hashes programmatically in script,\n>\n> which might lead to some interesting behaviour. For example, you could\n>\n> construct a script that says \"allow anyone to add themselves to the\n>\n> buy-a-citadel pool, as long as they're contributing at least 10 BTC\",\n>\n> which would then verify they have control of the pubkey they're adding,\n>\n> and allow them to add a script that lets them pull their 10 BTC back\n>\n> out via that pubkey, and participate in key path spends in the same\n>\n> way as everyone else. Of course, that sort of feature probably also\n>\n> naturally extends to many of the \"covenants considered harmful\" cases,\n>\n> eg a dollar-auction-like-contract: \"Alice can spend this utxo after 1000\n>\n> confirmations\" or \"anyone who increases the balance by 0.1 BTC can swap\n>\n> Alice's pubkey for their own in the sibling script to this\".\n>\n> An interesting thing to note is that constructing the script can sometimes\n>\n> be more efficient than hardcoding it, eg, I think\n>\n> \"TapLeaf\" SHA256 DUP CAT [0xc0016a] CAT SHA256\n>\n> is correct for calculating the hash for the \"OP_RETURN\" script, and at\n>\n> ~17 bytes should be cheaper than the ~33 bytes it would take to hardcode\n>\n> the hash.\n>\n> To construct a new script programmatically you almost certainly need to\n>\n> use templates, eg\n>\n> SIZE 32 EQUALVERIFY [0xc02220] SWAP CAT [0xac] CAT\n>\n> \"TapLeaf\" SHA256 DUP CAT SWAP CAT SHA256\n>\n> might take a public key off the stack and turn it into the hash for a\n>\n> script that expects a signature from that pubkey. I believe you could\n>\n> construct multiple scripts and combine them via\n>\n> CAT \"TapBranch\" SHA256 DUP CAT SWAP CAT SHA256\n>\n> or similar as well.\n>\n> There's a serious caveat with doing that in practice though: if you allow\n>\n> people to add in arbitrary opcodes when constructing the new script,\n>\n> they could choose to have that opcode be one of the \"OP_SUCCESS\" opcodes,\n>\n> and, if they're a miner, use that to bypass the covenant constraints\n>\n> entirely. So if you want to think about this, the template being filled\n>\n> in probably has to be very strict, eg including the specific PUSH opcode\n>\n> for the data being provided in the witness, and checking that the length\n>\n> of the witness data exactly matches the PUSH opcode being used.\n>\n> Cheers,\n>\n> aj\n>\n> bitcoin-dev mailing list\n>\n> bitcoin-dev at lists.linuxfoundation.org\n>\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jeremy",
                "date": "2021-09-09T15:54:25",
                "message_text_only": "I like this proposal, I think it has interesting use cases! I'm quick to\ncharitably Matt's comment, \"I\u2019ve been saying we need more covenants\nresearch and proposals before we move forward with one\", as before we move\nforward with *any.* I don't think that these efforts are rival -- different\nopcodes for different nodes as they say.\n\nI've previously done some analysis comparing Coin / Payment Pools with CTV\nto TapLeafUpdate which make CTV out favorably in terms of chain load and\nprivacy.\n\nOn the \"anyone can withdraw themselves in O(1) transactions\" front, is that\nif you contrast a CTV-style tree, the withdraws are O(log(n)) but E[O(1)]\nfor all participants, e.g. summing over the entire tree as it splits to\nevict a bad actor ends up being O(N) total work over N participants, so you\ndo have to look at the exact transactions that come out w.r.t. script size\nto determine which Payment Pool has overall less chain work to trustlessly\nwithdraw. This is compounded by the fact that a Taproot for N participants\nuses a O(log N) witness.\n\n\nLet's do out that basic math. First, let's assume we have 30 participants.\nThe basic script for each node would be:\n\nTLUV: Taproot(Tweaked Key, {<KEY> DUP \"\" 1 TLUV\n    CHECKSIGVERIFY\n    IN_OUT_AMOUNT SUB <BAL> GREATERTHANOREQUAL, ...})\n\nUnder this, the first withdraw for TLUV would require in witnesses stack:\nAssume average amount is 0.005BTC, so we have 4.2 B users = 18.9 bits =3\nbytes\n\n1 signature (1+64 bytes) + (1 Script = (+ 1 1 32 1 1 1 1 1 1 1 3 1 1) = 46\nbytes) + (1 taproot path = 2 + 33 + log2(N)*32)\n= 146+log2(N)*32.\n\nnow, because we delete the key, we need to sum this from N=0 to N=30:\n\n>>> sum([65+46+35+math.log(N,2)*32 for N in range(1, 31)])\n7826.690154943152 bytes of witness data\n\nEach transaction should have 1 input (40 bytes), 2 outputs (2* (34+8) =\n84), 4 bytes locktime, 4 bytes version, 2 byte witness flag, 1 byte in\ncounter 1 byte out counter  = 136 bytes (we already count witnesses above)\n\n\n136 * 30 + 7827 = 11907 bytes to withdraw all trustlessly\n\nNow for CTV:\n-CTV: Taproot(MuSigKey(subparties), <H splits pool with radix 4> CTV)\n\n*sidebar: **why radix 4? A while ago, I did the math out and a radix of 4\nor 5 was optimal for bare script... assuming this result holds with\ntaproot.*\n\n\nbalance holders: 0..30\nyou have a base set of transactions paying out: 0..4 4..8 8..12 12..16\n16..20 20..24 24..27 27..30\ninterior nodes covering: 0..16 16..30\nroot node covering: 0..30\n\nThe witness for each of these looks like:\n\n(Taproot Script = 1+1+32+1) + (Taproot Control = 33) = 68 bytes\n\nA transaction with two outputs should have 1 input (40 bytes), 2 outputs\n(2* (34+8) = 84), 4 bytes locktime, 4 bytes version, 2 byte witness flag, 1\nbyte in counter 1 byte out counter  = 136 bytes + 68 bytes witness = 204\nA transaction with three outputs should have 1 input (40 bytes), 3 outputs\n(3* (34+8) = 126), 4 bytes locktime, 4 bytes version, 2 byte witness flag,\n1 byte in counter 1 byte out counter  = 178 bytes + 68 bytes witness = 246\nA transaction with 4 outputs should have 1 input (40 bytes), 4 outputs (4*\n(34+8) = 126), 4 bytes locktime, 4 bytes version, 2 byte witness flag, 1\nbyte in counter 1 byte out counter  = 220 bytes + 68 bytes witness = 288\n\n204 + 288*6 + 246*2 = 2424 bytes\n\nTherefore the CTV style pool is, in this example, about 5x more efficient\nin block space utilization as compared to TLUV at trustlessly withdrawing\nall participants. This extra space leaves lots of headroom to e.g.\nincluding things like OP_TRUE anchor outputs (12*10) = 120 bytes total for\nCPFP; an optional script path with 2 inputs for a gas-paying input (cost is\naround 32 bytes for taproot?). The design also scales beyond 30\nparticipants, where the advantage grows further (iirc, sum i = 0 to n log i\nis relatively close to n log n).\n\nIn the single withdrawal case, the cost to eject a single participant with\nCTV is 204+288 = 492 bytes, compared to 65+46+35+math.log(30,2)*32+136 =\n439 bytes. The cost to eject a second participant in CTV is much smaller as\nit amortizes -- worst case is 288, best case is 0 (already expanded),\nwhereas in TLUV there is limited amortization so it would be about 438\nbytes.\n\nThe protocols are identical in the cooperative case.\n\nIn terms of privacy, the CTV version is a little bit worse. At every\nsplitting, radix of the root nodes total value gets broadcast. So to eject\na participant, you end up leaking a bit more information. However, it might\nbe a reasonable assumption that if one of your counterparties is\nuncooperative, they might dox you anyways. CTV trees are also superior\nduring updates for privacy in the cooperative case. With the TLUV pool, you\nmust know all tapleafs and the corresponding balances. Whereas in CTV\ntrees, you only need to know the balances of the nodes above you. E.g., we\ncan update the balances\n\nfrom: [[1 Alice, 2 Bob], [3 Carol, 4 Dave]]\nto: [[2.5 Alice, 0.5 Bob], [3 Carol, 4 Dave]]\n\nwithout informing Carol or Dave about the updates in our subtree, just that\nour slice of participants signed off on it. So even in the 1 party\nuncooperative case, 3/4 the pool has update privacy against them, and the\nsubtrees that share a branch with the uncooperative also have this privacy\nrecursively to an extent.\n\nCTV's TXID stability also lets you embed payment channels a bit easier, but\nperhaps TLUV would be in an ANYPREVOUT universe w.r.t. channel protocols so\nthat might matter less across the designs. Nevertheless, I think it's\nexciting to think about these payment pools where every node is an\nupdatable channel.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210909/b7a5c0bd/attachment-0001.html>"
            },
            {
                "author": "Jeremy",
                "date": "2021-09-09T19:26:37",
                "message_text_only": "I'm a bit skeptical of the safety of the control byte. Have you considered\nthe following issues?\n\n\n\n> The low bit of C indicates the parity of X; if it's 0, X has even y,\n> if it's 1, X has odd y.\n>\n> The next bit of C indicates whether the current script is dropped from\n> the merkle path, if it's 0, the current script is kept, if it's 1 the\n> current script is dropped.\n>\n> The remaining bits of C (ie C >> 2) are the number of steps in the merkle\n> path that are dropped. (If C is negative, behaviour is to be determined\n> -- either always fail, or always succeed and left for definition via\n> future soft-fork)\n>\n> For example, suppose we have a taproot utxo that had 5 scripts\n> (A,B,C,D,E), calculated as per the example in BIP 341 as:\n>\n>     AB = H_TapBranch(A, B)\n>     CD = H_TapBranch(C, D)\n>     CDE = H_TapBranch(CD, E)\n>     ABCDE = H_TapBranch(AB, CDE)\n>\n> And we're spending using script E, in that case the control block includes\n> the script E, and the merkle path to it, namely (AB, CD).\n>\n> So here's some examples of what you could do with TLUV to control how\n> the spending scripts can change, between the input sPK and the output sPK.\n>\n> At it's simplest, if we used the script \"0 0 0 TLUV\", then that says we\n> keep the current script, keep all steps in the merkle path, don't add\n> any new ones, and don't change the internal public key -- that is that\n> we want to resulting sPK to be exactly the same as the one we're spending.\n>\n> If we used the script \"0 F 0 TLUV\" (H=F, C=0) then we keep the current\n> script, keep all the steps in the merkle path (AB and CD), and add\n> a new step to the merkle path (F), giving us:\n>\n>     EF = H_TapBranch(E, F)\n>     CDEF =H_TapBranch(CD, EF)\n>     ABCDEF = H_TapBranch(AB, CDEF)\n>\n> If we used the script \"0 F 2 TLUV\" (H=F, C=2) then we drop the current\n> script, but keep all the other steps, and add a new step (effectively\n> replacing the current script with a new one):\n>\n>     CDF = H_TapBranch(CD, F)\n>     ABCDF = H_TapBranch(AB, CDF)\n>\n\nIf we recursively apply this rule, would it not be possible to repeatedly\napply it and end up burning out path E beyond the 128 Taproot depth limit?\n\nSuppose we protect against this by checking that after adding F the depth\nis not more than 128 for E.\n\nThe E path that adds F could also be burned for future use once the depth\nis hit, and if adding F is necessary for correctness, then we're burned\nanyways.\n\nI don't see a way to protect against this generically.\n\nPerhaps it's OK: E can always approve burning E?\n\n\n\n\n>\n> If we used the script \"0 F 4 TLUV\" (H=F, C=4) then we keep the current\n> script, but drop the last step in the merkle path, and add a new step\n> (effectively replacing the *sibling* of the current script):\n>\n>     EF = H_TapBranch(E, F)\n>     ABEF = H_TapBranch(AB, EF)\n\n\n> If we used the script \"0 0 4 TLUV\" (H=empty, C=4) then we keep the current\n> script, drop the last step in the merkle path, and don't add anything new\n> (effectively dropping the sibling), giving just:\n>\n>     ABE = H_TapBranch(AB, E)\n>\n>\n>\nIs C = 4 stable across all state transitions? I may be missing something,\nbut it seems that the location of C would not be stable across transitions.\n\n\nE.g., What happens when, C and E are similar scripts and C adds some\nclauses F1, F2, F3, then what does this sibling replacement do? Should a\nsibling not be able to specify (e.g., by leaf version?) a NOREPLACE flag\nthat prevents siblings from modifying it?\n\nWhat happens when E adds a bunch of F's F1 F2 F3, is C still in the same\nposition as when E was created?\n\nEspecially since nodes are lexicographically sorted, it seems hard to\ncreate stable path descriptors even if you index from the root downwards.\n\nIdentifying nodes by Hash is also not acceptable because of hash cycles,\nunless you want to restrict the tree structure accordingly (maybe OK\ntradeoff?).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210909/f906f558/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-10T07:42:19",
                "message_text_only": "On Thu, Sep 09, 2021 at 12:26:37PM -0700, Jeremy wrote:\n> I'm a bit skeptical of the safety of the control byte. Have you considered the\n> following issues?\n\n>     If we used the script \"0 F 0 TLUV\" (H=F, C=0) then we keep the current\n>     script, keep all the steps in the merkle path (AB and CD), and add\n>     a new step to the merkle path (F), giving us:\n>     \u00a0 \u00a0 EF = H_TapBranch(E, F)\n>     \u00a0 \u00a0 CDEF =H_TapBranch(CD, EF)\n>     \u00a0 \u00a0 ABCDEF = H_TapBranch(AB, CDEF)\n> \n> If we recursively apply this rule, would it not be possible to repeatedly apply\n> it and end up burning out path E beyond the 128 Taproot depth limit?\n\nSure. Suppose you had a script X which allows adding a new script A[0..n]\nas its sibling. You'd start with X and then go to (A0, X), then (A0,\n(A1, X)), then (A0, (A1, (A2, X))) and by the time you added A127 TLUV\nwould fail because it'd be trying to add a path longer than 128 elements.\n\nBut this would be bad anyway -- you'd already have a maximally unbalanced\ntree. So the fix for both these things would be to do a key path spend\nand rebalance the tree. With taproot, you always want to do key path\nspends if possible.\n\nAnother approach would be to have X replace itself not with (X, A) but\nwith (X, (X, A)) -- that way you go from:\n\n   /\\\n  A  X\n\nto \n     /\\\n    A /\\\n     X /\\\n      B  X\n  \nto \n      /\\\n     /  \\\n    A   /\\\n       /  \\\n      /    \\\n     /\\    /\\\n    C  X  B  X\n\nand can keep the tree height at O(log(n)) of the number of members.\n\nThis means the script X would need a way to reference its own hash, but\nyou could do that by invoking TLUV twice, once to check that your new\nsPK is adding a sibling (X', B) to the current script X, and a second\ntime to check that you're replacing the current script with (X', (X',\nB)). Executing it twice ensures that you've verified X' = X, so you can\nprovide X' on the stack, rather than trying to include the script's on\nhash in itself.\n\n> Perhaps it's OK: E can always approve burning E?\n\nAs long as you've got the key path, then I think that's the thing to do.\n\n>     If we used the script \"0 F 4 TLUV\" (H=F, C=4) then we keep the current\n>     script, but drop the last step in the merkle path, and add a new step\n>     (effectively replacing the *sibling* of the current script):\n>     \u00a0 \u00a0 EF = H_TapBranch(E, F)\n>     \u00a0 \u00a0 ABEF = H_TapBranch(AB, EF)\u00a0\n>     If we used the script \"0 0 4 TLUV\" (H=empty, C=4) then we keep the current\n>     script, drop the last step in the merkle path, and don't add anything new\n>     (effectively dropping the sibling), giving just:\n>     \u00a0 \u00a0 ABE = H_TapBranch(AB, E)\n> \n> Is C = 4 stable across all state transitions? I may be missing something, but\n> it seems that the location of C would not be stable across transitions.\n\nDropping a sibling without replacing it or dropping the current script\nwould mean you could re-execute the same script on the new utxo, and\nrepeat that enough times and the only remaining ways of spending would\nbe that script and the key path.\n\n> E.g., What happens when, C and E are similar scripts and C adds some clauses\n> F1, F2, F3, then what does this sibling replacement do? Should a sibling not be\n> able to specify (e.g., by leaf version?) a NOREPLACE flag that prevents\n> siblings from modifying it?\n\nIf you want a utxo where some script paths are constant, don't construct\nthe utxo with script paths that can modify them.\n\n> What happens when E adds a bunch of F's F1 F2 F3, is C still in the same\n> position as when E was created?\n\nThat depends how you define \"position\". If you have:\n\n    \n   /\\\n  R  S\n\nand\n\n   /\\\n  R /\\\n   S  T\n\nthen I'd say that \"R\" has stayed in the same position, while \"S\" has\nbeen lowered to allow for a new sibling \"T\". But the merkle path to\nR will have changed (from \"H(S)\" to \"H(H(S),H(T))\"). \n\n> Especially since nodes are lexicographically sorted, it seems hard to create\n> stable path descriptors even if you index from the root downwards.\n\nThe merkle path will always change unless you have the exact same set\nof scripts, so that doesn't seem like a very interesting way to define\n\"position\" when you're adding/removing/replacing scripts.\n\nThe \"lexical ordering\" is just a modification to how the hash is\ncalculated that makes it commutative, so that H(A,B) = H(B,A), with\nthe result being that the merkle path for any script in the the R,(S,T)\ntree above is the same for the corresponding script in the tree:\n\n   /\\\n  /\\ R\n T  S\n\nCheers,\naj"
            },
            {
                "author": "Matt Corallo",
                "date": "2021-09-09T09:16:12",
                "message_text_only": "Thanks for taking the time to write this up!\n\nTo wax somewhat broadly here, I\u2019m very excited about this as a direction for bitcoin covenants. Other concrete proposals seem significantly more limited, which worries me greatly. Further, this feels very \u201ctaproot-native\u201d in a way that encourages utilizing taproot\u2019s features fully while building covenants, saving fees on chain and at least partially improving privacy.\n\nI\u2019ve been saying we need more covenants research and proposals before we move forward with one and this is a huge step in that direction, IMO. With Taproot activating soon, I\u2019m excited for what coming forks bring.\n\nMatt\n\n> On Sep 8, 2021, at 23:42, Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> \ufeffHello world,\n> \n> A couple of years ago I had a flight of fancy [0] imagining how it\n> might be possible for everyone on the planet to use bitcoin in a\n> mostly decentralised/untrusted way, without requiring a block size\n> increase. It was a bit ridiculous and probably doesn't quite hold up,\n> and beyond needing all the existing proposals to be implemented (taproot,\n> ANYPREVOUT, CTV, eltoo, channel factories), it also needed a covenant\n> opcode [1]. I came up with something that I thought fit well with taproot,\n> but couldn't quite figure out how to use it for anything other than my\n> ridiculous scheme, so left it at that.\n> \n> But recently [2] Greg Maxwell emailed me about his own cool idea for a\n> covenant opcode, which turned out to basically be a reinvention of the\n> same idea but with more functionality, a better name and a less fanciful\n> use case; and with that inspiration, I think I've also now figured out\n> how to use it for a basic vault, so it seems worth making the idea a\n> bit more public.\n> \n> I'll split this into two emails, this one's the handwavy overview,\n> the followup will go into some of the implementation complexities.\n> \n> \n> \n> The basic idea is to think about \"updating\" a utxo by changing the\n> taproot tree.\n> \n> As you might recall, a taproot address is made up from an internal public\n> key (P) and a merkle tree of scripts (S) combined via the formula Q=P+H(P,\n> S)*G to calculate the scriptPubKey (Q). When spending using a script,\n> you provide the path to the merkle leaf that has the script you want\n> to use in the control block. The BIP has an example [3] with 5 scripts\n> arranged as ((A,B), ((C,D), E)), so if you were spending with E, you'd\n> reveal a path of two hashes, one for (AB), then one for (CD), then you'd\n> reveal your script E and satisfy it.\n> \n> So that makes it relatively easy to imagine creating a new taproot address\n> based on the input you're spending by doing some or all of the following:\n> \n> * Updating the internal public key (ie from P to P' = P + X)\n> * Trimming the merkle path (eg, removing CD)\n> * Removing the script you're currently executing (ie E)\n> * Adding a new step to the end of the merkle path (eg F)\n> \n> Once you've done those things, you can then calculate the new merkle\n> root by resolving the updated merkle path (eg, S' = MerkleRootFor(AB,\n> F, H_TapLeaf(E))), and then calculate a new scriptPubKey based on that\n> and the updated internal public key (Q' = P' + H(P', S')).\n> \n> So the idea is to do just that via a new opcode \"TAPLEAF_UPDATE_VERIFY\"\n> (TLUV) that takes three inputs: one that specifies how to update the\n> internal public key (X), one that specifies a new step for the merkle path\n> (F), and one that specifies whether to remove the current script and/or\n> how many merkle path steps to remove. The opcode then calculates the\n> scriptPubKey that matches that, and verifies that the output corresponding\n> to the current input spends to that scriptPubKey.\n> \n> That's useless without some way of verifying that the new utxo retains\n> the bitcoin that was in the old utxo, so also include a new opcode\n> IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this\n> input's utxo, and the amount in the corresponding output, and then expect\n> anyone using TLUV to use maths operators to verify that funds are being\n> appropriately retained in the updated scriptPubKey.\n> \n> \n> \n> Here's two examples of how you might use this functionality.\n> \n> First, a basic vault. The idea is that funds are ultimately protected\n> by a cold wallet key (COLD) that's inconvenient to access but is as\n> safe from theft as possible. In order to make day to day transactions\n> more convenient, a hot wallet key (HOT) is also available, which is\n> more vulnerable to theft. The vault design thus limits the hot wallet\n> to withdrawing at most L satoshis every D blocks, so that if funds are\n> stolen, you lose at most L, and have D blocks to use your cold wallet\n> key to re-secure the funds and prevent further losses.\n> \n> To set this up with TLUV, you construct a taproot output with COLD as\n> the internal public key, and a script that specifies:\n> \n> * The tx is signed via HOT\n> * <D> CSV -- there's a relative time lock since the last spend\n> * If the input amount is less than L + dust threshold, fine, all done,\n>   the vault can be emptied.\n> * Otherwise, the output amount must be at least (the input amount -\n>   L), and do a TLUV check that the resulting sPK is unchanged\n> \n> So you can spend up to \"L\" satoshis via the hot wallet as long as you\n> wait D blocks since the last spend, and can do whatever you want via a\n> key path spend with the cold wallet.\n> \n> You could extend this to have a two phase protocol for spending, where\n> first you use the hot wallet to say \"in D blocks, allow spending up to\n> L satoshis\", and only after that can you use the hot wallet to actually\n> spend funds. In that case supply a taproot sPK with COLD as the internal\n> public key and two scripts, the \"release\" script, which specifies:\n> \n> * The tx is signed via HOT\n> * Output amount is greater or equal to the input amount.\n> * Use TLUV to check:\n>   + the output sPK has the same internal public key (ie COLD)\n>   + the merkle path has one element trimmed\n>   + the current script is included\n>   + a new step is added that matches either H_LOCKED or H_AVAILABLE as\n>     described below (depending on whether 0 or 1 was provided as\n>     witness info)\n> \n> The other script is either \"locked\" (which is just \"OP_RETURN\") or\n> \"available\" which specifies:\n> \n> * The tx is signed via HOT\n> * <D> CSV -- there's a relative time lock since the last spend (ie,\n>   when the \"release\" script above was used)\n> * If the input amount is less than L, fine, all done, the vault can\n>   be emptied\n> * Otherwise, the output amount must be at least (the input amount minus\n>   L), and via TLUV, check the resulting sPK keeps the internal pubkey\n>   unchanged, keeps the merkle path, drops the current script, and adds\n>   H_LOCKED as the new step.\n> \n> H_LOCKED and H_AVAILABLE are just the TapLeaf hash corresponding to the\n> \"locked\" and \"available\" scripts.\n> \n> I believe this latter setup matches the design Bryan Bishop talked about\n> a couple of years ago [4], with the benefit that it's fully recursive,\n> allows withdrawals to vary rather than be the fixed amount L (due to not\n> relying on pre-signed transactions), and generally seems a bit simpler\n> to work with.\n> \n> \n> \n> The second scheme is allowing for a utxo to represent a group's pooled\n> funds. The idea being that as long as everyone's around you can use\n> the taproot key path to efficiently move money around within the pool,\n> or use a single transaction and signature for many people in the pool\n> to make payments. But key path spends only work if everyone's available\n> to sign -- what happens if someone disappears, or loses access to their\n> keys, or similar? For that, we want to have script paths to allow other\n> people to reclaim their funds even if everyone else disappears. So we\n> setup scripts for each participant, eg for Alice:\n> \n> * The tx is signed by Alice\n> * The output value must be at least the input value minus Alice's balance\n> * Must pass TLUV such that:\n>   + the internal public key is the old internal pubkey minus Alice's key\n>   + the currently executing script is dropped from the merkle path\n>   + no steps are otherwise removed or added\n> \n> The neat part here is that if you have many participants in the pool,\n> the pool continues to operate normally even if someone makes use of the\n> escape hatch -- the remaining participants can still use the key path to\n> spend efficiently, and they can each unilaterally withdraw their balance\n> via their own script path. If everyone decides to exit, whoever is last\n> can spend the remaining balance directly via the key path.\n> \n> Compared to having on-chain transactions using non-pooled funds, this\n> is more efficient and private: a single one-in, one-out transaction\n> suffices for any number of transfers within the pool, and there's no\n> on-chain information about who was sending/receiving the transfers, or\n> how large the transfers were; and for transfers out of the pool, there's\n> no on-chain indication which member of the pool is sending the funds,\n> and multiple members of the pool can send funds to multiple destinations\n> with only a single signature. The major constraint is that you need\n> everyone in the pool to be online in order to sign via the key path,\n> which provides a practical limit to how many people can reasonably be\n> included in a pool before there's a breakdown.\n> \n> Compared to lightning (eg eltoo channel factories with multiple\n> participants), the drawback is that no transfer is final without an\n> updated state being committed on chain, however there are also benefits\n> including that if one member of the pool unilaterally exits, that\n> doesn't reveal the state of anyone remaining in the pool (eg an eltoo\n> factory would likely reveal the balances of everyone else's channels at\n> that point).\n> \n> A simpler case for something like this might be for funding a joint\n> venture -- suppose you're joining with some other early bitcoiners to\n> buy land to build a citadel, so you each put 20 BTC into a pooled utxo,\n> ready to finalise the land purchase in a few months, but you also want\n> to make sure you can reclaim the funds if the deal falls through. So\n> you might include scripts like the above that allow you to reclaim your\n> balance, but add a CLTV condition preventing anyone from doing that until\n> the deal's deadline has passed. If the deal goes ahead, you all transfer\n> the funds to the vendor via the keypath; if it doesn't work out, you\n> hopefully return your funds via the keypath, but if things turn really\n> sour, you can still just directly reclaim your 20 BTC yourself via the\n> script path.\n> \n> \n> \n> I think a nice thing about this particular approach to recursive covenants\n> at a conceptual level is that it automatically leaves the key path as an\n> escape mechanism -- rather than having to build a base case manually,\n> and have the risk that it might not work because of some bug, locking\n> your funds into the covenant permanently; the escape path is free, easy,\n> and also the optimal way of spending things when everything is working\n> right. (Of course, you could set the internal public key to a NUMS point\n> and shoot yourself in the foot that way anyway)\n> \n> \n> \n> I think there's two limitations of this method that are worth pointing out.\n> \n> First it can't tweak scripts in areas of the merkle tree that it can't\n> see -- I don't see a way of doing that particularly efficiently, so maybe\n> it's best just to leave that as something for the people responsible for\n> the funds to negotiate via the keypath, in which case it's automatically\n> both private and efficient since all the details stay off-chain, anyway\n> \n> And second, it doesn't provide a way for utxos to \"interact\", which is\n> something that is interesting for automated market makers [5], but perhaps\n> only interesting for chains aiming to support multiple asset types,\n> and not bitcoin directly. On the other hand, perhaps combining it with\n> CTV might be enough to solve that, particularly if the hash passed to\n> CTV is constructed via script/CAT/etc.\n> \n> \n> \n> (I think everything described here could be simulated with CAT and\n> CHECKSIGFROMSTACK (and 64bit maths operators and some way to access\n> the internal public key), the point of introducing dedicated opcodes\n> for this functionality rather than (just) having more generic opcodes\n> would be to make the feature easy to use correctly, and, presuming it\n> actually has a wide set of use cases, to make it cheap and efficient\n> both to use in wallets, and for nodes to validate)\n> \n> Cheers,\n> aj\n> \n> [0] https://gist.github.com/ajtowns/dc9a59cf0a200bd1f9e6fb569f76f7a0\n> \n> [1] Roughly, the idea was that if you have ~9 billion people using\n>    bitcoin, but can only have ~1000 transactions per block, then you\n>    need have each utxo represent a significant number of people. That\n>    means that you need a way of allowing the utxo's to be efficiently\n>    spent, but need to introduce some level of trust since expecting\n>    many people to constantly be online seems unreliable, but to remain\n>    mostly decentralised/untrusted, you want to have some way of limiting\n>    how much trust you're introducing, and that's where covenants come in.\n> \n> [2] Recently in covid-adjusted terms, or on the bitcoin consensus\n>    change scale anyway...\n>    https://mobile.twitter.com/ajtowns/status/1385091604357124100 \n> \n> [3] https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki#Constructing_and_spending_Taproot_outputs \n> \n> [4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017231.html\n> \n> [5] The idea behind an automated market maker being that you setup a\n>    script that says \"you can withdraw x BTC if you deposit f(x) units of\n>    USDT, or you can withdraw g(x) units of USDT if you deposit x units\n>    of BTC\", with f(x)/x giving the buy price, and f(x)>g(x) meaning\n>    you make a profit. Being able to specify a covenant that links the\n>    change in value to the BTC utxo (+/-x) and the change in value to\n>    the USDT utxo (+f(x) or -g(x)) is what you'd need to support this\n>    sort of use case, but TLUV doesn't provide a way to do that linkage.\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-10T04:12:24",
                "message_text_only": "Hi AJ,\n\nThanks for finally putting the pieces together! [0]\n\nWe've been hacking with Gleb on a paper for the CoinPool protocol [1]\nduring the last weeks and it should be public soon, hopefully highlighting\nwhat kind of scheme, TAPLEAF_UPDATE_VERIFY-style of covenant enable :)\n\nHere few early feedbacks on this specific proposal,\n\n> So that makes it relatively easy to imagine creating a new taproot address\n> based on the input you're spending by doing some or all of the following:\n>\n>  * Updating the internal public key (ie from P to P' = P + X)\n>  * Trimming the merkle path (eg, removing CD)\n>  * Removing the script you're currently executing (ie E)\n>  * Adding a new step to the end of the merkle path (eg F)\n\n\"Talk is cheap. Show me the code\" :p\n\n    case OP_MERKLESUB:\n    {\n        if (!(flags & SCRIPT_VERIFY_MERKLESUB)) {\n            break;\n        }\n\n        if (stack.size() < 2) {\n            return set_error(serror, SCRIPT_ERR_INVALID_STACK_OPERATION);\n        }\n\n        valtype& vchPubKey = stacktop(-1);\n\n        if (vchPubKey.size() != 32) {\n            break;\n        }\n\n        const std::vector<unsigned char>& vch = stacktop(-2);\n        int nOutputPos = CScriptNum(stacktop(-2), fRequireMinimal).getint();\n\n        if (nOutputPos < 0) {\n            return set_error(serror, SCRIPT_ERR_NEGATIVE_MERKLEVOUT);\n        }\n\n        if (!checker.CheckMerkleUpdate(*execdata.m_control, nOutputPos,\nvchPubKey)) {\n            return set_error(serror, SCRIPT_ERR_UNSATISFIED_MERKLESUB);\n        }\n        break;\n    }\n\n    case OP_NOP1: case OP_NOP5:\n\n\n\n    template <class T>\n    bool GenericTransactionSignatureChecker<T>::CheckMerkleUpdate(const\nstd::vector<unsigned char>& control, unsigned int out_pos, const\nstd::vector<unsigned char>& point) const\n    {\n        //! The internal pubkey (x-only, so no Y coordinate parity).\n        XOnlyPubKey p{uint256(std::vector<unsigned char>(control.begin() +\n1, control.begin() + TAPROOT_CONTROL_BASE_SIZE))};\n        //! Update the internal key by subtracting the point.\n        XOnlyPubKey s{uint256(point)};\n        XOnlyPubKey u;\n        try {\n            u = p.UpdateInternalKey(s).value();\n        } catch (const std::bad_optional_access& e) {\n            return false;\n        }\n\n        //! The first control node is made the new tapleaf hash.\n        //! TODO: what if there is no control node ?\n        uint256 updated_tapleaf_hash;\n        updated_tapleaf_hash = uint256(std::vector<unsigned\nchar>(control.data() + TAPROOT_CONTROL_BASE_SIZE, control.data() +\nTAPROOT_CONTROL_BASE_SIZE + TAPROOT_CONTROL_NODE_SIZE));\n\n        //! The committed-to output must be in the spent transaction vout\nrange.\n        if (out_pos >= txTo->vout.size()) return false;\n        int witnessversion;\n        std::vector<unsigned char> witnessprogram;\n        txTo->vout[out_pos].scriptPubKey.IsWitnessProgram(witnessversion,\nwitnessprogram);\n        //! The committed to output must be a witness v1 program at least\n        if (witnessversion == 0) {\n            return false;\n        } else if (witnessversion == 1) {\n            //! The committed-to output.\n            const XOnlyPubKey q{uint256(witnessprogram)};\n            //! Compute the Merkle root from the leaf and the incremented\nby one path.\n            const uint256 merkle_root = ComputeTaprootMerkleRoot(control,\nupdated_tapleaf_hash, 1);\n            //! TODO modify MERKLESUB design\n            bool parity_ret = q.CheckTapTweak(u, merkle_root, true);\n            bool no_parity_ret = q.CheckTapTweak(u, merkle_root, false);\n            if (!parity_ret && !no_parity_ret) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n\nHere the main chunks for an \"<n> <point> OP_MERKLESUB\" opcode, with `n` the\noutput position which is checked for update and `point` the x-only pubkey\nwhich must be subtracted from the internal key.\n\nI think one design advantage of explicitly passing the output position as a\nstack element is giving more flexibility to your contract dev. The first\noutput could be SIGHASH_ALL locked-down. e.g \"you have to pay Alice on\noutput 1 while pursuing the contract semantic on output 2\".\n\nOne could also imagine a list of output positions to force the taproot\nupdate on multiple outputs (\"OP_MULTIMERKLESUB\"). Taking back your citadel\njoint venture example, partners could decide to split the funds in 3\nequivalent amounts *while* conserving the pre-negotiated script policies [2]\n\nFor the merkle branches extension, I was thinking of introducing a separate\nOP_MERKLEADD, maybe to *add* a point to the internal pubkey group signer.\nIf you're only interested in leaf pruning, using OP_MERKLESUB only should\nsave you one byte of empty vector ?\n\nWe can also explore more fancy opcodes where the updated merkle branch is\npushed on the stack for deep manipulations. Or even n-dimensions\ninspections if combined with your G'root [3] ?\n\nNote, this current OP_MERKLESUB proposal doesn't deal with committing the\nparity of the internal pubkey as part of the spent utxo. As you highlighted\nwell in your other mail, if we want to conserve the updated key-path across\na sequence of TLUV-covenanted transactions, we need either\na) to select a set of initial points, where whatever combination of\nadd/sub, it yields an even-y point. Or b) have the even/odd bit\nre-committed at each update. Otherwise, we're not guaranteed to cancel the\npoint from the aggregated key.\n\nThis property is important for CoinPool. Let's say you have A+B+C+D, after\nthe D withdraw transaction has been confirmed on-chain, you want A+B+C to\nretain the ability to use the key-path and update the off-chain state,\nwithout forcing a script path spend to a new setup.\n\nIf we put the updated internal key parity bit in the first control byte, we\nneed to have a  redundant commitment somewhere else as we can't trust the\nspender to not be willingly to break the key-path spend of the remaining\ngroup of signers.\n\nOne solution I was thinking about was introducing a new tapscript version\n(`TAPROOT_INTERNAL_TAPSCRIPT`) signaling that VerifyTaprootCommitment must\ncompute the TapTweak with a new TapTweak=(internal_pubkey || merkle_root ||\nparity_bit). A malicious participant wouldn't be able to interfere with the\nupdated internal key as it would break its own spending taproot commitment\nverification ?\n\n> That's useless without some way of verifying that the new utxo retains\n> the bitcoin that was in the old utxo, so also include a new opcode\n> IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this\n> input's utxo, and the amount in the corresponding output, and then expect\n> anyone using TLUV to use maths operators to verify that funds are being\n> appropriately retained in the updated scriptPubKey.\n\nCredit to you for the SIGHASH_GROUP design, here the code, with\nSIGHASH_ANYPUBKEY/ANYAMOUNT extensions.\n\n    if ((output_type & SIGHASH_GROUP) == SIGHASH_GROUP) {\n        // Verify the output group bounds\n        if (execdata.m_bundle->first == execdata.m_bundle->second ||\nexecdata.m_bundle->second >= tx_to.vout.size()) return false;\n\n        // Verify the value commitment\n        if (VerifyOutputsGroup(tx_to, cache.m_spent_outputs[in_pos].nValue,\nexecdata.m_bundle->first, execdata.m_bundle->second)) return false;\n\n\n\n        for (unsigned int out_pos = execdata.m_bundle->first; out_pos <\nexecdata.m_bundle->second + 1; out_pos++) {\n            bool anypubkey_flag = false;\n            bool anyamount_flag = false;\n            std::map<unsigned int, char>::const_iterator it;\n\n            if ((output_type & SIGHASH_GROUP_ANYPUBKEY) ==\nSIGHASH_GROUP_ANYPUBKEY) {\n                it = execdata.m_anypubkeys.find(out_pos);\n                if (it != execdata.m_anypubkeys.end() && it->second == 1) {\n                    anypubkey_flag = true;\n                }\n            }\n\n            if ((output_type & SIGHASH_GROUP_ANYAMOUNT) ==\nSIGHASH_GROUP_ANYAMOUNT) {\n                it = execdata.m_anyamounts.find(out_pos);\n                if (it != execdata.m_anyamounts.end() && it->second == 1) {\n                    anyamount_flag = true;\n                }\n            }\n\n            if (!anypubkey_flag) {\n                ss << tx_to.vout[out_pos].scriptPubKey;\n            }\n            if (!anyamount_flag) {\n                ss << tx_to.vout[out_pos].nValue;\n            }\n\n        }\n    }\n\nI think it's achieving the same effect as IN_OUT_AMOUNT, at least for\nCoinPool use-case. A MuSig  `contract_pubkey` can commit to the\n`to_withdraw` output while allowing a wildcard for the `to_pool` output\nnValue/scriptPubKey. The nValue correctness will be ensured by the\ngroup-value-lock validation rule (`VerifyOutputsGroup`) and scriptPubkey by\nOP_MERKLESUB commitment.\n\nI think witness data size it's roughly equivalent as the annex fields must\nbe occupied by the output group commitment. SIGHASH_GROUP might be more\nflexible than IN_OUT_AMOUNT for a range of use-cases, see my point on AMM.\n\n> The second scheme is allowing for a utxo to represent a group's pooled\n> funds. The idea being that as long as everyone's around you can use\n> the taproot key path to efficiently move money around within the pool,\n> or use a single transaction and signature for many people in the pool\n> to make payments. But key path spends only work if everyone's available\n> to sign -- what happens if someone disappears, or loses access to their\n> keys, or similar? For that, we want to have script paths to allow other\n> people to reclaim their funds even if everyone else disappears. So we\n> setup scripts for each participant, eg for Alice:\n>\n>  * The tx is signed by Alice\n>  * The output value must be at least the input value minus Alice's balance\n>  * Must pass TLUV such that:\n>    + the internal public key is the old internal pubkey minus Alice's key\n>    + the currently executing script is dropped from the merkle path\n>    + no steps are otherwise removed or added\n\nYes the security model is roughly similar to the LN one. Instead of a\ncounter-signed commitment transaction which can be broadcast at any point\nduring channel lifetime, you have a pre-signed withdraw transaction sending\nto {`to_withdraw`,`to_pool`} outputs. Former is your off-chain balance, the\nlatter one is the pool balance, and one grieved with the updated Taproot\noutput. The withdraw tapscript force the point subtraction with the\nfollowing format (`<n> <withdraw_point> <OP_MERKLESUB> <33-byte\ncontract_pubkey> OP_CHECKSIG)\n\n> A simpler case for something like this might be for funding a joint\n> venture -- suppose you're joining with some other early bitcoiners to\n> buy land to build a citadel, so you each put 20 BTC into a pooled utxo,\n> ready to finalise the land purchase in a few months, but you also want\n> to make sure you can reclaim the funds if the deal falls through. So\n> you might include scripts like the above that allow you to reclaim your\n> balance, but add a CLTV condition preventing anyone from doing that until\n> the deal's deadline has passed. If the deal goes ahead, you all transfer\n> the funds to the vendor via the keypath; if it doesn't work out, you\n> hopefully return your funds via the keypath, but if things turn really\n> sour, you can still just directly reclaim your 20 BTC yourself via the\n> script path.\n\nYes, that kind of blockchain validation semantic extension is vaudoo-magic\nif we want to enable smart corporation/scalable multi-event contracts. I\ngave a presentation on advanced bitcoin contracts two years ago, mentioning\nwe would need covenants to solve the factorial complexity on edge-case [4]\n\nBitcoin ledger would fit perfectly well to host international commerce law\nstyle of contracts, where you have a lot of usual fancy provisions (e.g\nhardship, delay penalty, ...) :)\n\n> First it can't tweak scripts in areas of the merkle tree that it can't\n> see -- I don't see a way of doing that particularly efficiently, so maybe\n> it's best just to leave that as something for the people responsible for\n> the funds to negotiate via the keypath, in which case it's automatically\n> both private and efficient since all the details stay off-chain, anyway\n\nYeah, in that kind of case, we might want to push the merkle root as a\nstack element but still update the internal pubkey from the spent utxo ?\nThis new merkle_root would be the tree of tweaked scripts as you expect\nthem if you execute *this* tapscript. And you can still this new tree with\na tapbranch inherited from the taproot output.\n\n(I think I could come with some use-case from lex mercatoria where if you\nplay out a hardship provision you want to tweak all the other provisions by\na CSV delay while conserving the rest of their policy)\n\n> And second, it doesn't provide a way for utxos to \"interact\", which is\n> something that is interesting for automated market makers [5], but perhaps\n> only interesting for chains aiming to support multiple asset types,\n> and not bitcoin directly. On the other hand, perhaps combining it with\n> CTV might be enough to solve that, particularly if the hash passed to\n> CTV is constructed via script/CAT/etc.\n\nThat's where SIGHASH_GROUP might be more interesting as you could generate\ntransaction \"puzzles\".\n\nIIUC, the problem is how to have a set of ratios between x/f(x). I think it\ncan be simplified to just generate pairs of input btc-amount/output\nusdt-amount for the whole range of strike price you want to cover.\n\nEach transaction puzzle has 1-input/2-outputs. The first output is signed\nwith SIGHASH_ANYPUBKEY but committed to a USDT amount. The second output is\nsigned with SIGHASH_ANYAMOUNT but committed to the maker pubkey. The input\ncommits to the spent BTC amount but not the spent txid/scriptPubKey.\nThe maker generates a Taproot tree where each leaf is committing to a\ndifferent \"strike price\".\n\nA taker is finalizing the puzzle by inserting its withdraw scriptPubKey for\nthe first output and the maker amount for the second output. The\ntransitivity value output group rule guarantees that a malicious taker\ncan't siphon the fund.\n\n> (I think everything described here could be simulated with CAT and\n> CHECKSIGFROMSTACK (and 64bit maths operators and some way to access\n> the internal public key), the point of introducing dedicated opcodes\n> for this functionality rather than (just) having more generic opcodes\n> would be to make the feature easy to use correctly, and, presuming it\n> actually has a wide set of use cases, to make it cheap and efficient\n> both to use in wallets, and for nodes to validate)\n\nYeah, I think CHECKSIGFROMSTACK is a no-go if we want to emulate\nTAPLEAF_UPDATE_VERIFY functionality. If you want to update the 100th\ntapscript, I believe we'll have to throw on the stack the corresponding\nmerkle branch and it sounds inefficient in terms of witness space ? Though\nofc, in both cases we bear the tree traversal computational cost ?\n\nReally really excited to see progress on more powerful covenants for\nBitcoin :)\n\nCheers,\nAntoine\n\n[0] For the ideas genealogy, I think Greg's OP_MERKLE_UPDATE has been\ncirculating for a while and we chatted with Jeremy last year about the\ncurrent limitation of the script interpreter w.r.t expressing the factorial\ncomplexity of advanced off-chain systems. I also remember Matt's artistic\ndrawing of a TAPLEAF_UPDATE_VERIFY ancestor on a Chaincode whiteboard :)\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-June/017964.html\n\n[2]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-July/016249.html\n\n[3] A legal construction well-spread in the real-world. Known as\n\"indivision\" in civil law\".\n\n[4] https://github.com/ariard/talk-slides/blob/master/advanced-contracts.pdf\n\nLe jeu. 9 sept. 2021 \u00e0 02:42, Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hello world,\n>\n> A couple of years ago I had a flight of fancy [0] imagining how it\n> might be possible for everyone on the planet to use bitcoin in a\n> mostly decentralised/untrusted way, without requiring a block size\n> increase. It was a bit ridiculous and probably doesn't quite hold up,\n> and beyond needing all the existing proposals to be implemented (taproot,\n> ANYPREVOUT, CTV, eltoo, channel factories), it also needed a covenant\n> opcode [1]. I came up with something that I thought fit well with taproot,\n> but couldn't quite figure out how to use it for anything other than my\n> ridiculous scheme, so left it at that.\n>\n> But recently [2] Greg Maxwell emailed me about his own cool idea for a\n> covenant opcode, which turned out to basically be a reinvention of the\n> same idea but with more functionality, a better name and a less fanciful\n> use case; and with that inspiration, I think I've also now figured out\n> how to use it for a basic vault, so it seems worth making the idea a\n> bit more public.\n>\n> I'll split this into two emails, this one's the handwavy overview,\n> the followup will go into some of the implementation complexities.\n>\n>\n>\n> The basic idea is to think about \"updating\" a utxo by changing the\n> taproot tree.\n>\n> As you might recall, a taproot address is made up from an internal public\n> key (P) and a merkle tree of scripts (S) combined via the formula Q=P+H(P,\n> S)*G to calculate the scriptPubKey (Q). When spending using a script,\n> you provide the path to the merkle leaf that has the script you want\n> to use in the control block. The BIP has an example [3] with 5 scripts\n> arranged as ((A,B), ((C,D), E)), so if you were spending with E, you'd\n> reveal a path of two hashes, one for (AB), then one for (CD), then you'd\n> reveal your script E and satisfy it.\n>\n> So that makes it relatively easy to imagine creating a new taproot address\n> based on the input you're spending by doing some or all of the following:\n>\n>  * Updating the internal public key (ie from P to P' = P + X)\n>  * Trimming the merkle path (eg, removing CD)\n>  * Removing the script you're currently executing (ie E)\n>  * Adding a new step to the end of the merkle path (eg F)\n>\n> Once you've done those things, you can then calculate the new merkle\n> root by resolving the updated merkle path (eg, S' = MerkleRootFor(AB,\n> F, H_TapLeaf(E))), and then calculate a new scriptPubKey based on that\n> and the updated internal public key (Q' = P' + H(P', S')).\n>\n> So the idea is to do just that via a new opcode \"TAPLEAF_UPDATE_VERIFY\"\n> (TLUV) that takes three inputs: one that specifies how to update the\n> internal public key (X), one that specifies a new step for the merkle path\n> (F), and one that specifies whether to remove the current script and/or\n> how many merkle path steps to remove. The opcode then calculates the\n> scriptPubKey that matches that, and verifies that the output corresponding\n> to the current input spends to that scriptPubKey.\n>\n> That's useless without some way of verifying that the new utxo retains\n> the bitcoin that was in the old utxo, so also include a new opcode\n> IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this\n> input's utxo, and the amount in the corresponding output, and then expect\n> anyone using TLUV to use maths operators to verify that funds are being\n> appropriately retained in the updated scriptPubKey.\n>\n>\n>\n> Here's two examples of how you might use this functionality.\n>\n> First, a basic vault. The idea is that funds are ultimately protected\n> by a cold wallet key (COLD) that's inconvenient to access but is as\n> safe from theft as possible. In order to make day to day transactions\n> more convenient, a hot wallet key (HOT) is also available, which is\n> more vulnerable to theft. The vault design thus limits the hot wallet\n> to withdrawing at most L satoshis every D blocks, so that if funds are\n> stolen, you lose at most L, and have D blocks to use your cold wallet\n> key to re-secure the funds and prevent further losses.\n>\n> To set this up with TLUV, you construct a taproot output with COLD as\n> the internal public key, and a script that specifies:\n>\n>  * The tx is signed via HOT\n>  * <D> CSV -- there's a relative time lock since the last spend\n>  * If the input amount is less than L + dust threshold, fine, all done,\n>    the vault can be emptied.\n>  * Otherwise, the output amount must be at least (the input amount -\n>    L), and do a TLUV check that the resulting sPK is unchanged\n>\n> So you can spend up to \"L\" satoshis via the hot wallet as long as you\n> wait D blocks since the last spend, and can do whatever you want via a\n> key path spend with the cold wallet.\n>\n> You could extend this to have a two phase protocol for spending, where\n> first you use the hot wallet to say \"in D blocks, allow spending up to\n> L satoshis\", and only after that can you use the hot wallet to actually\n> spend funds. In that case supply a taproot sPK with COLD as the internal\n> public key and two scripts, the \"release\" script, which specifies:\n>\n>  * The tx is signed via HOT\n>  * Output amount is greater or equal to the input amount.\n>  * Use TLUV to check:\n>    + the output sPK has the same internal public key (ie COLD)\n>    + the merkle path has one element trimmed\n>    + the current script is included\n>    + a new step is added that matches either H_LOCKED or H_AVAILABLE as\n>      described below (depending on whether 0 or 1 was provided as\n>      witness info)\n>\n> The other script is either \"locked\" (which is just \"OP_RETURN\") or\n> \"available\" which specifies:\n>\n>  * The tx is signed via HOT\n>  * <D> CSV -- there's a relative time lock since the last spend (ie,\n>    when the \"release\" script above was used)\n>  * If the input amount is less than L, fine, all done, the vault can\n>    be emptied\n>  * Otherwise, the output amount must be at least (the input amount minus\n>    L), and via TLUV, check the resulting sPK keeps the internal pubkey\n>    unchanged, keeps the merkle path, drops the current script, and adds\n>    H_LOCKED as the new step.\n>\n> H_LOCKED and H_AVAILABLE are just the TapLeaf hash corresponding to the\n> \"locked\" and \"available\" scripts.\n>\n> I believe this latter setup matches the design Bryan Bishop talked about\n> a couple of years ago [4], with the benefit that it's fully recursive,\n> allows withdrawals to vary rather than be the fixed amount L (due to not\n> relying on pre-signed transactions), and generally seems a bit simpler\n> to work with.\n>\n>\n>\n> The second scheme is allowing for a utxo to represent a group's pooled\n> funds. The idea being that as long as everyone's around you can use\n> the taproot key path to efficiently move money around within the pool,\n> or use a single transaction and signature for many people in the pool\n> to make payments. But key path spends only work if everyone's available\n> to sign -- what happens if someone disappears, or loses access to their\n> keys, or similar? For that, we want to have script paths to allow other\n> people to reclaim their funds even if everyone else disappears. So we\n> setup scripts for each participant, eg for Alice:\n>\n>  * The tx is signed by Alice\n>  * The output value must be at least the input value minus Alice's balance\n>  * Must pass TLUV such that:\n>    + the internal public key is the old internal pubkey minus Alice's key\n>    + the currently executing script is dropped from the merkle path\n>    + no steps are otherwise removed or added\n>\n> The neat part here is that if you have many participants in the pool,\n> the pool continues to operate normally even if someone makes use of the\n> escape hatch -- the remaining participants can still use the key path to\n> spend efficiently, and they can each unilaterally withdraw their balance\n> via their own script path. If everyone decides to exit, whoever is last\n> can spend the remaining balance directly via the key path.\n>\n> Compared to having on-chain transactions using non-pooled funds, this\n> is more efficient and private: a single one-in, one-out transaction\n> suffices for any number of transfers within the pool, and there's no\n> on-chain information about who was sending/receiving the transfers, or\n> how large the transfers were; and for transfers out of the pool, there's\n> no on-chain indication which member of the pool is sending the funds,\n> and multiple members of the pool can send funds to multiple destinations\n> with only a single signature. The major constraint is that you need\n> everyone in the pool to be online in order to sign via the key path,\n> which provides a practical limit to how many people can reasonably be\n> included in a pool before there's a breakdown.\n>\n> Compared to lightning (eg eltoo channel factories with multiple\n> participants), the drawback is that no transfer is final without an\n> updated state being committed on chain, however there are also benefits\n> including that if one member of the pool unilaterally exits, that\n> doesn't reveal the state of anyone remaining in the pool (eg an eltoo\n> factory would likely reveal the balances of everyone else's channels at\n> that point).\n>\n> A simpler case for something like this might be for funding a joint\n> venture -- suppose you're joining with some other early bitcoiners to\n> buy land to build a citadel, so you each put 20 BTC into a pooled utxo,\n> ready to finalise the land purchase in a few months, but you also want\n> to make sure you can reclaim the funds if the deal falls through. So\n> you might include scripts like the above that allow you to reclaim your\n> balance, but add a CLTV condition preventing anyone from doing that until\n> the deal's deadline has passed. If the deal goes ahead, you all transfer\n> the funds to the vendor via the keypath; if it doesn't work out, you\n> hopefully return your funds via the keypath, but if things turn really\n> sour, you can still just directly reclaim your 20 BTC yourself via the\n> script path.\n>\n>\n>\n> I think a nice thing about this particular approach to recursive covenants\n> at a conceptual level is that it automatically leaves the key path as an\n> escape mechanism -- rather than having to build a base case manually,\n> and have the risk that it might not work because of some bug, locking\n> your funds into the covenant permanently; the escape path is free, easy,\n> and also the optimal way of spending things when everything is working\n> right. (Of course, you could set the internal public key to a NUMS point\n> and shoot yourself in the foot that way anyway)\n>\n>\n>\n> I think there's two limitations of this method that are worth pointing out.\n>\n> First it can't tweak scripts in areas of the merkle tree that it can't\n> see -- I don't see a way of doing that particularly efficiently, so maybe\n> it's best just to leave that as something for the people responsible for\n> the funds to negotiate via the keypath, in which case it's automatically\n> both private and efficient since all the details stay off-chain, anyway\n>\n> And second, it doesn't provide a way for utxos to \"interact\", which is\n> something that is interesting for automated market makers [5], but perhaps\n> only interesting for chains aiming to support multiple asset types,\n> and not bitcoin directly. On the other hand, perhaps combining it with\n> CTV might be enough to solve that, particularly if the hash passed to\n> CTV is constructed via script/CAT/etc.\n>\n>\n>\n> (I think everything described here could be simulated with CAT and\n> CHECKSIGFROMSTACK (and 64bit maths operators and some way to access\n> the internal public key), the point of introducing dedicated opcodes\n> for this functionality rather than (just) having more generic opcodes\n> would be to make the feature easy to use correctly, and, presuming it\n> actually has a wide set of use cases, to make it cheap and efficient\n> both to use in wallets, and for nodes to validate)\n>\n> Cheers,\n> aj\n>\n> [0] https://gist.github.com/ajtowns/dc9a59cf0a200bd1f9e6fb569f76f7a0\n>\n> [1] Roughly, the idea was that if you have ~9 billion people using\n>     bitcoin, but can only have ~1000 transactions per block, then you\n>     need have each utxo represent a significant number of people. That\n>     means that you need a way of allowing the utxo's to be efficiently\n>     spent, but need to introduce some level of trust since expecting\n>     many people to constantly be online seems unreliable, but to remain\n>     mostly decentralised/untrusted, you want to have some way of limiting\n>     how much trust you're introducing, and that's where covenants come in.\n>\n> [2] Recently in covid-adjusted terms, or on the bitcoin consensus\n>     change scale anyway...\n>     https://mobile.twitter.com/ajtowns/status/1385091604357124100\n>\n> [3]\n> https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki#Constructing_and_spending_Taproot_outputs\n>\n> [4]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-August/017231.html\n>\n> [5] The idea behind an automated market maker being that you setup a\n>     script that says \"you can withdraw x BTC if you deposit f(x) units of\n>     USDT, or you can withdraw g(x) units of USDT if you deposit x units\n>     of BTC\", with f(x)/x giving the buy price, and f(x)>g(x) meaning\n>     you make a profit. Being able to specify a covenant that links the\n>     change in value to the BTC utxo (+/-x) and the change in value to\n>     the USDT utxo (+f(x) or -g(x)) is what you'd need to support this\n>     sort of use case, but TLUV doesn't provide a way to do that linkage.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210910/0a78717c/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-11T03:26:44",
                "message_text_only": "On Fri, Sep 10, 2021 at 12:12:24AM -0400, Antoine Riard wrote:\n> \"Talk is cheap. Show me the code\" :p\n> \u00a0 \u00a0 case OP_MERKLESUB:\n\nI'm not entirely clear on what your opcode there is trying to do. I\nthink it's taking\n\n   <N> <P> MERKLESUB\n\nand checking that output N has the same scripts as the current input\nexcept with the current script removed, and with its internal pubkey as\nthe current input's internal pubkey plus P.\n\n> \u00a0 \u00a0 \u00a0 \u00a0 txTo->vout[out_pos].scriptPubKey.IsWitnessProgram(witnessversion,\n> witnessprogram);\n> \u00a0 \u00a0 \u00a0 \u00a0 //! The committed to output must be a witness v1 program at least\n\nThat would mean anyone who could do a valid spend of the tx could\nviolate the covenant by spending to an unencumbered witness v2 output\nand (by collaborating with a miner) steal the funds. I don't think\nthere's a reasonable way to have existing covenants be forward\ncompatible with future destination addresses (beyond something like CTV\nthat strictly hardcodes them).\n\n> One could also imagine a list of output positions to force the taproot update\n> on multiple outputs (\"OP_MULTIMERKLESUB\").\n\nHaving the output position parameter might be an interesting way to\nmerge/split a vault/pool, but it's not clear to me how much sense it\nmakes sense to optimise for that, rather than just doing that via the key\npath. For pools, you want the key path to be common anyway (for privacy\nand efficiency), so it shouldn't be a problem; but even for vaults,\nyou want the cold wallet accessible enough to be useful for the case\nwhere theft is attempted, and maybe that's also accessible enough for\nthe ocassional merge/split to keep your utxo count/sizes reasonable.\n\n> For the merkle branches extension, I was thinking of introducing a separate\n> OP_MERKLEADD, maybe to *add* a point to the internal pubkey group signer. If\n> you're only interested in leaf pruning, using OP_MERKLESUB only should save you\n> one byte of empty vector ?\n\nSaving a byte of witness data at the cost of specifying additional\nopcodes seems like optimising the wrong thing to me.\n\n> One solution I was thinking about was introducing a new tapscript version\n> (`TAPROOT_INTERNAL_TAPSCRIPT`) signaling that VerifyTaprootCommitment must\n> compute the TapTweak with a new TapTweak=(internal_pubkey || merkle_root ||\n> parity_bit). A malicious participant wouldn't be able to interfere with the\n> updated internal key as it would break its own spending taproot commitment\n> verification ?\n\nI don't think that works, because different scripts in the same merkle\ntree can have different script versions, which would here indicate\ndifferent parities for the same internal pub key.\n\n> > That's useless without some way of verifying that the new utxo retains\n> > the bitcoin that was in the old utxo, so also include a new opcode\n> > IN_OUT_AMOUNT that pushes two items onto the stack: the amount from this\n> > input's utxo, and the amount in the corresponding output, and then expect\n> > anyone using TLUV to use maths operators to verify that funds are being\n> > appropriately retained in the updated scriptPubKey.\n> Credit to you for the SIGHASH_GROUP design, here the code, with\n> SIGHASH_ANYPUBKEY/ANYAMOUNT extensions.\n> \n> I think it's achieving the same effect as IN_OUT_AMOUNT, at least for CoinPool\n> use-case.\n\nThe IN_OUT_AMOUNT opcode lets you do maths on the values, so you can\nspecify \"hot wallets can withdraw up to X\" rather than \"hot wallets\nmust withdraw exactly X\". I don't think there's a way of doing that with\nSIGHASH_GROUP, even with a modifier like ANYPUBKEY?\n\n> (I think I could come with some use-case from lex mercatoria where if you play\n> out a hardship provision you want to tweak all the other provisions by a CSV\n> delay while conserving the rest of their policy)\n\nIf you want to tweak all the scripts, I think you should be using the\nkey path.\n\nOne way you could do somthing like that without changing the scripts\nthough, is have the timelock on most of the scripts be something like\n\"[3 months] CSV\", and have a \"delay\" script that doesn't require a CSV,\ndoes require a signature from someone able to authorise the delay,\nand requires the output to have the same scriptPubKey and amount. Then\nyou can use that path to delay resolution by 3 months however often,\neven if you can't coordinate a key path spend.\n\n> > And second, it doesn't provide a way for utxos to \"interact\", which is\n> > something that is interesting for automated market makers [5], but perhaps\n> > only interesting for chains aiming to support multiple asset types,\n> > and not bitcoin directly. On the other hand, perhaps combining it with\n> > CTV might be enough to solve that, particularly if the hash passed to\n> > CTV is constructed via script/CAT/etc.\n> That's where SIGHASH_GROUP might be more interesting as you could generate\n> transaction \"puzzles\".\n> IIUC, the problem is how to have a set of ratios between x/f(x).\n\nNormal way to do it is specify a formula, eg\n\n   outBTC * outUSDT >= inBTC * inUSDT\n\nthat's a constant product market maker without a profit margin. There's\nlots of research in the ethereum world about doing these things, and\nbitmatrix is trying to do it on liquid. It's not clear to me if there's\nanywhere in bitcoin per se that it would make sense.\n\nThen your relative balances of each token imply a price, and traders will\nrebalance anytime that price is out of whack with the rest of the market.\n\nYou can tweak the formula so that you make a profit, which also ends up\nmeaning the fund pool becomes more liquid overtime. But that means that\nyou want to cope with 100 BTC and 5M USDT at $50k, but also 200 BTC and\n10M USDT at $50k, and many values in between. So I don't think:\n\n> The maker generates a Taproot tree where each leaf is committing to a different\n> \"strike price\".\n\nreally works that well.\n\nOne irritating thing I realised while reading Jeremy's mail is that\n\n  CAT \"TapBranch\" SHA256 DUP CAT SWAP CAT SHA256\n\ndoesn't actually work -- the first CAT needs to sort the two branches\nfirst, and \"LESSTHAN\" etc want to compare values numerically rather\nthan lexically. So maybe it would make more sense to introduce an opcode\nthat builds a merkle root from tagged hashes directly, rather than one\nthat lets you compare to 32B strings so that you can do the TapBranch\nlogic manually.\n\nCheers,\naj"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-12T23:37:56",
                "message_text_only": "Sorry for the lack of clarity, sometimes it sounds easier to explain ideas\nwith code.\n\nWhile MERKLESUB is still WIP, here the semantic. If the input spent is a\nSegWit v1 Taproot output, and the script path spending is used, the top\nstack item is interpreted as an output position of the spending\ntransaction. The second top stack item is interpreted as a 32-byte x-only\npubkey to be negated and added to the spent internal pubkey.\n\nThe spent tapscript is removed from the merkle tree of tapscripts and a new\nmerkle root is recomputed with the first node element of the spending\ncontrol block as the tapleaf hash. From then, this new merkle root is added\nas the taproot tweak to the updated internal pubkey, while correcting for\nparity. This new tweaked pubkey is interpreted as a v1 witness program and\nmust match the scriptPubKey of the spending transaction output as the\npassed position. Otherwise, MERKLESUB returns a failure.\n\nI believe this is matching your description and the main difference\ncompared to your TLUV proposal is the lack of merkle tree extension, where\na new merkle path is added in place of the removed tapscript. Motivation is\nsaving up the one byte of the new merkle path step, which is not necessary\nfor our CoinPool use-case.\n\n> That would mean anyone who could do a valid spend of the tx could\n> violate the covenant by spending to an unencumbered witness v2 output\n> and (by collaborating with a miner) steal the funds. I don't think\n> there's a reasonable way to have existing covenants be forward\n> compatible with future destination addresses (beyond something like CTV\n> that strictly hardcodes them).\n\nThat's a good catch, thanks for raising it :)\n\nDepends how you define reasonable, but I think one straightforward fix is\nto extend the signature digest algorithm to encompass the segwit version\n(and maybe program-size ?) of the spending transaction outputs.\n\nThen you add a \"contract\" aggregated-key in every tapscript where a\nTLUV/MERKLESUB covenant is present. The off-chain contract participant can\nexchange signatures at initial setup committing to the segwit version. I\nthink this addresses the sent-to-unknown-witness-output point ?\n\nWhen future destination addresses are deployed, assuming a new round of\ninteractivity, the participants can send the fund to a v1+ by exchanging\nsignatures with SIGHASH_ALL, that way authorizing the bypass of\nTLUV/MERKLESUB.\n\nOf course, in case of v1+ deployment, the key path could be used. Though\nthis path could have been \"burnt\" by picking up an internal point with an\nunknown scalar following the off-chain contract/use-case semantic ?\n\n> Having the output position parameter might be an interesting way to\n> merge/split a vault/pool, but it's not clear to me how much sense it\n> makes sense to optimise for that, rather than just doing that via the key\n> path. For pools, you want the key path to be common anyway (for privacy\n> and efficiency), so it shouldn't be a problem; but even for vaults,\n> you want the cold wallet accessible enough to be useful for the case\n> where theft is attempted, and maybe that's also accessible enough for\n> the ocassional merge/split to keep your utxo count/sizes reasonable.\n\nI think you can come up with interesting contract policies. Let's say you\nwant to authorize the emergency path of your pool/vault balances if X\nhappens (e.g a massive drop in USDT price signed by DLC oracles). You have\n(A+B+C+D) forking into (A+B) and (C+D) pooled funds. To conserve the\ncontracts pre-negotiated economic equilibrium, all the participants would\nlike the emergency path to be inherited on both forks. Without relying on\nthe key path interactivity, which is ultimately a trust on the post-fork\ncooperation of your counterparty ?\n\n> Saving a byte of witness data at the cost of specifying additional\n> opcodes seems like optimising the wrong thing to me.\n\nI think we should keep in mind that any overhead cost in the usage of a\nscript primitive is echoed to the user of off-chain contract/payment\nchannels. If the tapscripts are bigger, your average on-chain spends in\ncase of non-cooperative scenarios are increased in consequence, and as such\nyour fee-bumping reserve. Thus making those systems less economically\naccessible.\n\nIf we really envision having billions of Bitcoin users owning a utxo or\nshards of them, we should also think that those users might have limited\nmeans to pay on-chain fees. Where should be the line between resource\noptimizations and protocol/implementation complexity ? Hard to tell.\n\n> I don't think that works, because different scripts in the same merkle\n> tree can have different script versions, which would here indicate\n> different parities for the same internal pub key.\n\nLet me make it clearer. We introduce a new tapscript version 0x20, forcing\na new bit in the first byte of the control block to be interpreted as the\nparity bit of the spent internal pubkey. To ensure this parity bit is\nfaithful and won't break the updated key path, it's committed in the spent\ntaptweak. A malicious counterparty while having malleability on the control\nblock, by setting the parity bit to the wrong value will break the taptweak\nand fail the taproot commitment verification ?\n\nI think the correct commitment of different script versions in the merkle\ntree can be verified by tree participants at setup ?\n\n> The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can\n> specify \"hot wallets can withdraw up to X\" rather than \"hot wallets\n> must withdraw exactly X\". I don't think there's a way of doing that with\n> SIGHASH_GROUP, even with a modifier like ANYPUBKEY?\n\nYou can exchange signatures for withdraw outputs with multiples `nValue`\ncovering the authorized range, assuming the ANYAMOUNT modifier ? One\nadvantage of leveraging sighash is the ability to update a withdraw policy\nin real-time. Vaults participants might be willing to bump the withdraw\npolicy beyond X, assuming you have N-of-M consents.\n\n> If you want to tweak all the scripts, I think you should be using the\n> key path.\n>\n> One way you could do somthing like that without changing the scripts\n> though, is have the timelock on most of the scripts be something like\n> \"[3 months] CSV\", and have a \"delay\" script that doesn't require a CSV,\n> does require a signature from someone able to authorise the delay,\n> and requires the output to have the same scriptPubKey and amount. Then\n> you can use that path to delay resolution by 3 months however often,\n> even if you can't coordinate a key path spend\n\nI think I would like to express the following contract policy. Let's say\nyou have 1) a one-time conditional script path to withdraw fund (\"a put on\nstrike price X\"), 2) a conditional script path to tweak by 3 months all the\nusual withdraw path and 3) those remaining withdraw paths. Once played out,\nyou would like the one-time path to be removed from your merkle tree. And\nthis removal to be inherited on the tweaked tree if 2) plays out.\n\nI agree that's advanced Bitcoin contracting and we might not require from\none script primitive to cover the whole expressivity we're aiming to.\n\n> that's a constant product market maker without a profit margin. There's\n> lots of research in the ethereum world about doing these things, and\n> bitmatrix is trying to do it on liquid. It's not clear to me if there's\n> anywhere in bitcoin per se that it would make sense.\n\nGood with the more detailed explanation. Yeah I know it's widely deployed\non the ethereum-side, still late on catching up with literature/resources\non that. Assuming we have a widely-deployed token protocol on the\nbitcoin-side, you could couple it with a DLC-style of security model and\nthat might be enough to bootstrap a fruitful token trading ecosystem ?\nThough I agree, expressing an AMM in bitcoin primitives is an interesting\ndesign challenge!\n\n> So maybe it would make more sense to introduce an opcode\n> that builds a merkle root from tagged hashes directly, rather than one\n> that lets you compare to 32B strings so that you can do the TapBranch\n> logic manually.\n\nIIUC, you would like an opcode to edit the spent merkle root or build a new\none from stack elements ? E.g adding new withdraw tapleaf if the input\namount is over X. I think that the design description gives more\nflexibility but I'm worried you will need more than one opcode. Like\nOP_TWEAKADD, to add the tweak on the updated internal key and\nOP_SCRIPTPUBKEY_VERIFY (or at least OP_CSFS though more expensive) ?\n\nLe ven. 10 sept. 2021 \u00e0 23:26, Anthony Towns <aj at erisian.com.au> a \u00e9crit :\n\n> On Fri, Sep 10, 2021 at 12:12:24AM -0400, Antoine Riard wrote:\n> > \"Talk is cheap. Show me the code\" :p\n> >     case OP_MERKLESUB:\n>\n> I'm not entirely clear on what your opcode there is trying to do. I\n> think it's taking\n>\n>    <N> <P> MERKLESUB\n>\n> and checking that output N has the same scripts as the current input\n> except with the current script removed, and with its internal pubkey as\n> the current input's internal pubkey plus P.\n>\n> >         txTo->vout[out_pos].scriptPubKey.IsWitnessProgram(witnessversion,\n> > witnessprogram);\n> >         //! The committed to output must be a witness v1 program at least\n>\n> That would mean anyone who could do a valid spend of the tx could\n> violate the covenant by spending to an unencumbered witness v2 output\n> and (by collaborating with a miner) steal the funds. I don't think\n> there's a reasonable way to have existing covenants be forward\n> compatible with future destination addresses (beyond something like CTV\n> that strictly hardcodes them).\n>\n> > One could also imagine a list of output positions to force the taproot\n> update\n> > on multiple outputs (\"OP_MULTIMERKLESUB\").\n>\n> Having the output position parameter might be an interesting way to\n> merge/split a vault/pool, but it's not clear to me how much sense it\n> makes sense to optimise for that, rather than just doing that via the key\n> path. For pools, you want the key path to be common anyway (for privacy\n> and efficiency), so it shouldn't be a problem; but even for vaults,\n> you want the cold wallet accessible enough to be useful for the case\n> where theft is attempted, and maybe that's also accessible enough for\n> the ocassional merge/split to keep your utxo count/sizes reasonable.\n>\n> > For the merkle branches extension, I was thinking of introducing a\n> separate\n> > OP_MERKLEADD, maybe to *add* a point to the internal pubkey group\n> signer. If\n> > you're only interested in leaf pruning, using OP_MERKLESUB only should\n> save you\n> > one byte of empty vector ?\n>\n> Saving a byte of witness data at the cost of specifying additional\n> opcodes seems like optimising the wrong thing to me.\n>\n> > One solution I was thinking about was introducing a new tapscript version\n> > (`TAPROOT_INTERNAL_TAPSCRIPT`) signaling that VerifyTaprootCommitment\n> must\n> > compute the TapTweak with a new TapTweak=(internal_pubkey || merkle_root\n> ||\n> > parity_bit). A malicious participant wouldn't be able to interfere with\n> the\n> > updated internal key as it would break its own spending taproot\n> commitment\n> > verification ?\n>\n> I don't think that works, because different scripts in the same merkle\n> tree can have different script versions, which would here indicate\n> different parities for the same internal pub key.\n>\n> > > That's useless without some way of verifying that the new utxo retains\n> > > the bitcoin that was in the old utxo, so also include a new opcode\n> > > IN_OUT_AMOUNT that pushes two items onto the stack: the amount from\n> this\n> > > input's utxo, and the amount in the corresponding output, and then\n> expect\n> > > anyone using TLUV to use maths operators to verify that funds are being\n> > > appropriately retained in the updated scriptPubKey.\n> > Credit to you for the SIGHASH_GROUP design, here the code, with\n> > SIGHASH_ANYPUBKEY/ANYAMOUNT extensions.\n> >\n> > I think it's achieving the same effect as IN_OUT_AMOUNT, at least for\n> CoinPool\n> > use-case.\n>\n> The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can\n> specify \"hot wallets can withdraw up to X\" rather than \"hot wallets\n> must withdraw exactly X\". I don't think there's a way of doing that with\n> SIGHASH_GROUP, even with a modifier like ANYPUBKEY?\n>\n> > (I think I could come with some use-case from lex mercatoria where if\n> you play\n> > out a hardship provision you want to tweak all the other provisions by a\n> CSV\n> > delay while conserving the rest of their policy)\n>\n> If you want to tweak all the scripts, I think you should be using the\n> key path.\n>\n> One way you could do somthing like that without changing the scripts\n> though, is have the timelock on most of the scripts be something like\n> \"[3 months] CSV\", and have a \"delay\" script that doesn't require a CSV,\n> does require a signature from someone able to authorise the delay,\n> and requires the output to have the same scriptPubKey and amount. Then\n> you can use that path to delay resolution by 3 months however often,\n> even if you can't coordinate a key path spend.\n>\n> > > And second, it doesn't provide a way for utxos to \"interact\", which is\n> > > something that is interesting for automated market makers [5], but\n> perhaps\n> > > only interesting for chains aiming to support multiple asset types,\n> > > and not bitcoin directly. On the other hand, perhaps combining it with\n> > > CTV might be enough to solve that, particularly if the hash passed to\n> > > CTV is constructed via script/CAT/etc.\n> > That's where SIGHASH_GROUP might be more interesting as you could\n> generate\n> > transaction \"puzzles\".\n> > IIUC, the problem is how to have a set of ratios between x/f(x).\n>\n> Normal way to do it is specify a formula, eg\n>\n>    outBTC * outUSDT >= inBTC * inUSDT\n>\n> that's a constant product market maker without a profit margin. There's\n> lots of research in the ethereum world about doing these things, and\n> bitmatrix is trying to do it on liquid. It's not clear to me if there's\n> anywhere in bitcoin per se that it would make sense.\n>\n> Then your relative balances of each token imply a price, and traders will\n> rebalance anytime that price is out of whack with the rest of the market.\n>\n> You can tweak the formula so that you make a profit, which also ends up\n> meaning the fund pool becomes more liquid overtime. But that means that\n> you want to cope with 100 BTC and 5M USDT at $50k, but also 200 BTC and\n> 10M USDT at $50k, and many values in between. So I don't think:\n>\n> > The maker generates a Taproot tree where each leaf is committing to a\n> different\n> > \"strike price\".\n>\n> really works that well.\n>\n> One irritating thing I realised while reading Jeremy's mail is that\n>\n>   CAT \"TapBranch\" SHA256 DUP CAT SWAP CAT SHA256\n>\n> doesn't actually work -- the first CAT needs to sort the two branches\n> first, and \"LESSTHAN\" etc want to compare values numerically rather\n> than lexically. So maybe it would make more sense to introduce an opcode\n> that builds a merkle root from tagged hashes directly, rather than one\n> that lets you compare to 32B strings so that you can do the TapBranch\n> logic manually.\n>\n> Cheers,\n> aj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/63905a99/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-15T06:50:51",
                "message_text_only": "On Sun, Sep 12, 2021 at 07:37:56PM -0400, Antoine Riard via bitcoin-dev wrote:\n> While MERKLESUB is still WIP, here the semantic. [...]\n> I believe this is matching your description and the main difference compared to\n> your TLUV proposal is the lack of merkle tree extension, where a new merkle\n> path is added in place of the removed tapscript.\n\nI think \"<I> <P> MERKLESUB\" is the same as \"<P> OP_0 2 TLUV\", provided\n<I> happens to be the same index as the current input. So it misses the\nability to add branches (replacing OP_0 with a hash), the ability to\npreserve the current script (replacing 2 with 0), and the ability to\nremove some of the parent paths (replacing 2 with 4*n); but gains the\nability to refer to non-corresponding outputs.\n\n> > That would mean anyone who could do a valid spend of the tx could\n> > violate the covenant by spending to an unencumbered witness v2 output\n> > and (by collaborating with a miner) steal the funds. I don't think\n> > there's a reasonable way to have existing covenants be forward\n> > compatible with future destination addresses (beyond something like CTV\n> > that strictly hardcodes them).\n> That's a good catch, thanks for raising it :)\n> Depends how you define reasonable, but I think one straightforward fix is to\n> extend the signature digest algorithm to encompass the segwit version (and\n> maybe program-size ?) of the spending transaction outputs.\n\nThat... doesn't sound very straightforward to me; it's basically\nintroducing a new covenant approach, that's getting fixed into a\nsignature, rather than being a separate opcode.\n\nI think a better approach for that would be to introduce the opcode (eg,\nPUSH_OUTPUT_SCRIPTPUBKEY, and SUBSTR to be able to analyse the segwit\nversion), and make use of graftroot to allow a signature to declare that\nit's conditional on some extra script code. But it feels like it's going\na bit off topic.\n\n> > Having the output position parameter might be an interesting way to\n> > merge/split a vault/pool, but it's not clear to me how much sense it\n> > makes sense to optimise for that, rather than just doing that via the key\n> > path. For pools, you want the key path to be common anyway (for privacy\n> > and efficiency), so it shouldn't be a problem; but even for vaults,\n> > you want the cold wallet accessible enough to be useful for the case\n> > where theft is attempted, and maybe that's also accessible enough for\n> > the ocassional merge/split to keep your utxo count/sizes reasonable.\n> I think you can come up with interesting contract policies. Let's say you want\n> to authorize the emergency path of your pool/vault balances if X happens (e.g a\n> massive drop in USDT price signed by DLC oracles). You have (A+B+C+D) forking\n> into (A+B) and (C+D) pooled funds. To conserve the contracts pre-negotiated\n> economic equilibrium, all the participants would like the emergency path to be\n> inherited on both forks. Without relying on the key path interactivity, which\n> is ultimately a trust on the post-fork cooperation of your counterparty ?\n\nI'm not really sure what you're saying there; is that any different to a\npool of (A and B) where A suddenly wants to withdraw funds ASAP and can't\nwait for a key path signature? In that case A authorises the withdrawal\nand does whatever she wants with the funds (including form a new pool),\nand B remains in the pool.\n\nI don't think you can reliably have some arbitrary subset of the pool\nable to withdraw atomically without using the key path -- if A,B,C,D have\nindividual scripts allowing withdrawal, then there's no way of setting\nthe tree up so that every pair of members can have their scripts cut\noff without also cutting off one or both of the other members withdrawal\nscripts.\n\nIf you know in advance which groups want to stick together, you could\nset things up as:\n\n  (((A, B), AB), C)\n\nwhere:\n\n  A =   \"A DUP H(B') 10 TLUV CHECKSIG\"  -> (B', C)\n  B =   \"B DUP H(A') 10 TLUV CHECKSIG\"  -> (A', C)\n  A' =  \"A DUP 0 2 TLUV CHECKSIG\"   -> (C)\n  B' =  \"B DUP 0 2 TLUV CHECKSIG\"   -> (C)\n  AB =  \"(A+B) DUP 6 TLUV CHECKSIG  -> (C)\n  C  =  \"C DUP 0 2 TLUV CHECKSIG\"   -> ((A,B), AB)\n\n(10 = 2+4*2 = drop my script, my sibling and my uncle; 6 = 2+4*1 =\ndrop my script and my sibling; 2 = drop my script only)\n\nWhich would let A and B exit together in a single tx rather than needing two\ntransactions to exit separately.\n\n> > Saving a byte of witness data at the cost of specifying additional\n> > opcodes seems like optimising the wrong thing to me.\n> I think we should keep in mind that any overhead cost in the usage of a script\n> primitive is echoed to the user of off-chain contract/payment channels. If the\n> tapscripts are bigger, your average on-chain spends in case of non-cooperative\n> scenarios are increased in consequence, and as such your fee-bumping reserve.\n> Thus making those systems less economically accessible.\n\nIf you're worried about the cost of a single byte of witness data you\nprobably can't afford to do script path spends at all -- certainly\nhaving to do 64 bytes of witness data to add a signature that commits\nto an amount and the like will be infeasible in that case.\n\n> > I don't think that works, because different scripts in the same merkle\n> > tree can have different script versions, which would here indicate\n> > different parities for the same internal pub key.\n> Let me make it clearer. We introduce a new tapscript version 0x20, forcing a\n> new bit in the first byte of the control block to be interpreted as the parity\n> bit of the spent internal pubkey.\n\nThat doesn't work. Suppose you start off with an even internal pubkey,\nwith three scripts, (A, (B,C)). All of those scripts have tapscript\nversion 0xc0 because the internal pubkey is even. You spend using A and\ncalculate the new internal pubkey which turns out to be odd. You then\nneed to change B and C's script version from 0xc0 to 0x20, but you can't\ndo that (at least, you can't do it without revealing every script).\n\n> To ensure this parity bit is faithful and\n> won't break the updated key path, it's committed in the spent taptweak.\n\nChanging the TapTweak calculation is a hard fork; existing software\nalready verifies the calculation even if the script version is unknown.\n\n> > The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can\n> > specify \"hot wallets can withdraw up to X\" rather than \"hot wallets\n> > must withdraw exactly X\". I don't think there's a way of doing that with\n> > SIGHASH_GROUP, even with a modifier like ANYPUBKEY?\n> You can exchange signatures for withdraw outputs with multiples `nValue`\n> covering the authorized range, assuming the ANYAMOUNT modifier ?\n\nIf you want your hotwallet to be able to withdraw up to $2000, that's\naround 4,000,000 sats, so you'd be doing up to 4M signatures there if you\nwanted to get the exact value you're trying to send, without having to\neither overpay, or first pay yourself then have another tx that splits\nyour withdrawal into what you're spending and change that's no longer\nin your vault.\n\n> One advantage\n> of leveraging sighash is the ability to update a withdraw policy in real-time.\n> Vaults participants might be willing to bump the withdraw policy beyond X,\n> assuming you have N-of-M consents.\n\nI mean, maybe? It seems like a very heavy weight construct where a more\ngeneral approach would probably be better (eg, graftroot to attach a\nscript to a signature; or checkdatasig or whatever so you push a value\nto the stack then check it's signature, then reuse the authenticated\ndata against other checks) so that you only have to supply a signature\nwhen you want to be able to approve things after the fact.\n\n> I think I would like to express the following contract policy. Let's say you\n> have 1) a one-time conditional script path to withdraw fund (\"a put on strike\n> price X\"), 2) a conditional script path to tweak by 3 months all the usual\n> withdraw path and 3) those remaining withdraw paths. Once played out, you would\n> like the one-time path to be removed from your merkle tree. And this removal to\n> be inherited on the tweaked tree if 2) plays out.\n\nOkay, so I think that means we've got the unconditional withdraw path\n\"U\" (your 1), the delay path \"D\" (your 2) and some normal path(s) \"N\"\n(your 3). I think you can get that behaviour with:\n\n   S1 = Merkle( U, (D, N) )\n   S2 = Merkle( U, W )\n   S3 = Merkle( N )\n\nthat is, you start off with the funds in scriptPubKey S1, then spend\nusing D to get to S2, then spend using W to get to S3, then presumably\nspend using N at some point.\n\nThe script for W is just:\n\n   \"IN_OUT_AMOUNT EQUALVERIFY 0 <N> 6 TLUV <3 months> CSV\"   \n       (drop the script, drop its sibling, add N, wait 3 months)\n\nThe script for D is:\n\n   \"IN_OUT_AMOUNT EQUALVERIFY 0 <W> 6 TLUV <sigcheck...>\"\n       (drop the script, drop its sibling, add W, extra conditions\n        to avoid anyone being able to delay things)\n\nThat is, the strategy isn't \"tweak the scripts by delaying them 3 months\"\nit's \"tweak the merkle tree, to replace the scripts that would be delayed\nwith a new script that has a delay and then allows itself to be replaced\nby the original scripts that we now want back\".\n\nCheers,\naj"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-18T14:11:10",
                "message_text_only": "> I think \"<I> <P> MERKLESUB\" is the same as \"<P> OP_0 2 TLUV\", provided\n> <I> happens to be the same index as the current input. So it misses the\n> ability to add branches (replacing OP_0 with a hash), the ability to\n> preserve the current script (replacing 2 with 0), and the ability to\n> remove some of the parent paths (replacing 2 with 4*n); but gains the\n> ability to refer to non-corresponding outputs.\n\nYes, I agree.\n\n> That... doesn't sound very straightforward to me; it's basically\n> introducing a new covenant approach, that's getting fixed into a\n> signature, rather than being a separate opcode.\n\nI think one design advantage of combining scope-minimal opcodes like\nMERKLESUB with sighash malleability is the ability to update a subset of\nthe off-chain contract transactions fields after the funding phase. With a\nlower level of cooperation than required by the key path. I think not an\nability offered by templated covenants.\n\n> I'm not really sure what you're saying there; is that any different to a\n> pool of (A and B) where A suddenly wants to withdraw funds ASAP and can't\n> wait for a key path signature? In that case A authorises the withdrawal\n> and does whatever she wants with the funds (including form a new pool),\n> and B remains in the pool.\n\nYes this is a different contract policy that I would like to set up.\n\nLet's say you would like to express the following set of capabilities.\n\nC0=\"Split the 4 BTC funds between Alice/Bob and Caroll/Dave\"\nC1=\"Alice can withdraw 1 BTC after 2 weeks\"\nC2=\"Bob can withdraw 1 BTC after 2 weeks\"\nC3=\"Caroll can withdraw 1 BTC after 2 weeks\"\nC4=\"Dave can withdraw 1 BTC after 2 weeks\"\nC5=\"If USDT price=X, Alice can withdraw 2 BTC or Caroll can withdraw 2 BTC\"\n\nIf C4 is exercised, to avoid trust in the remaining counterparty, both\nAlice or Caroll should be able to conserve the C5 option, without relying\non the updated key path.\n\nAs you're saying, as we know the group in advance, one way to setup the tree\ncould be:\n\n       (A, (((((B, C), BC), D), BCD), ((((E, F), EF), G), EFG)))\n\nwhere:\nA=\"1 <alice> <caroll> 2 CHECKMULTISIG <usdt_oracle> CHECKSIG\"\nB=\"<alice> DUP 0 2 TLUV CHECKSIG\"\nC=\"<bob> DUP 0 2 TLUV CHECKSIG\"\nD=\"<alice+bob> 0 6 TLUV 1 <caroll> <dave> 2 CHECKMULTISIG\"\nE=\"<caroll> DUP 0 2 TLUV CHECKSIG\"\nF=\"<dave> DUP 0 2 TLUV CHECKSIG\"\nG=\"<caroll+dave> 0 6 TLUV 1 <alice> <bob> 2 CHECKMULTISIG\"\n\nE.g, if D is exercised, B+C+D are removed from the tree and A, E, F, G are\nconserved in the Caroll/Dave fork. Then Caroll can exercise the USDT option\nwithout trusting Dave.\n\nNote, this solution isn't really satisfying as the G path isn't neutralized\non the Caroll/Dave fork and could be replayed by Alice or Bob... One\nimprovement could be to have the \"withdraw\" script path (C,D,F,G) expressed\nredundantly. That way when a \"split\" script path is exercised the uncle\nsplit path and all the siblings \"withdraw\" paths can be removed.\n\nEchoing your point about the difficulty of reliably composing arbitrary\nsubsets of the pool, I lean to agree that merkle trees aren't the most\nstraightforward way to encode that kind of contract policy.\n\n> If you're worried about the cost of a single byte of witness data you\n> probably can't afford to do script path spends at all -- certainly\n> having to do 64 bytes of witness data to add a signature that commits\n> to an amount and the like will be infeasible in that case.\n\nYes, I agree fully templated covenants are more efficient to save witness\ndata.\n\nI still like the idea of inserting a key as you might have an interesting\nability.\nLike a N-of-M, a subset of the vault/pool able to update the withdraw\npubkey.\n\n> That doesn't work. Suppose you start off with an even internal pubkey,\n> with three scripts, (A, (B,C)). All of those scripts have tapscript\n> version 0xc0 because the internal pubkey is even. You spend using A and\n> calculate the new internal pubkey which turns out to be odd. You then\n> need to change B and C's script version from 0xc0 to 0x20, but you can't\n> do that (at least, you can't do it without revealing every script).\n\nI'm not sure we're aligned on the mechanism.\n\nWe introduce a new tapscript version 0x20.\n\nAt spent taproot commitment verification, if the tapscript version=0x20,\nthe second-lowest bit of the first byte of the control block is interpreted\nas the parity bit of the spent internal pubkey (i.e control[0] & 0x2).\n\nThis parity bit is used to compute a new format of TapTweakV2=H(p || m ||\nbit) and commitment verification keep proceeding unmodified.\n\nAs the leaf version is committed as part of every TapLeaf, I think any\nusage of MERKLESUB would require to use tapscript version 0x20 for the\nwhole set of leaves.\n\nIf you build a tree blurring 0xc0 leaves and TapTweakV2, I think those\nleaves will be unspendable as they will always fail the commitment\nverification.\n\n> Changing the TapTweak calculation is a hard fork; existing software\n> already verifies the calculation even if the script version is unknown.\n\nThinking more, you're right...\n\nIn case of TapTweakV2, non-upgraded nodes won't be able to pass the\nvalidation of unknown script version (0x20), and the failure will provoke a\nfork.\n\nCould we commit the spent internal pubkey parity bit as a one-more-tweak\ntransparent to non-upgrades nodes ?\n\nFor upgraded, P = R + (t2 * G) and Q = P + (t1 * G)\nFor non-upgraded, Q = P + (t1 * G).\n\nCould we add a new validation rule (e.g VerifyInternalPubkeyCommitment)\nconditional on a newer tapscript version just before\nVerifyTaprootCommitment ?\n\n> That is, the strategy isn't \"tweak the scripts by delaying them 3 months\"\n> it's \"tweak the merkle tree, to replace the scripts that would be delayed\n> with a new script that has a delay and then allows itself to be replaced\n> by the original scripts that we now want back\".\n\nYes, that's a good strategy to have logically equivalent subtree embedded\nin the modifying tapscript.\n\nIf you have multiple modifying scripts and you can't predict the order, I\nthink the tree complexity will be quickly too high and grafroot-like\napproaches are likely better\n\nLe mer. 15 sept. 2021 \u00e0 02:51, Anthony Towns <aj at erisian.com.au> a \u00e9crit :\n\n> On Sun, Sep 12, 2021 at 07:37:56PM -0400, Antoine Riard via bitcoin-dev\n> wrote:\n> > While MERKLESUB is still WIP, here the semantic. [...]\n> > I believe this is matching your description and the main difference\n> compared to\n> > your TLUV proposal is the lack of merkle tree extension, where a new\n> merkle\n> > path is added in place of the removed tapscript.\n>\n> I think \"<I> <P> MERKLESUB\" is the same as \"<P> OP_0 2 TLUV\", provided\n> <I> happens to be the same index as the current input. So it misses the\n> ability to add branches (replacing OP_0 with a hash), the ability to\n> preserve the current script (replacing 2 with 0), and the ability to\n> remove some of the parent paths (replacing 2 with 4*n); but gains the\n> ability to refer to non-corresponding outputs.\n>\n> > > That would mean anyone who could do a valid spend of the tx could\n> > > violate the covenant by spending to an unencumbered witness v2 output\n> > > and (by collaborating with a miner) steal the funds. I don't think\n> > > there's a reasonable way to have existing covenants be forward\n> > > compatible with future destination addresses (beyond something like CTV\n> > > that strictly hardcodes them).\n> > That's a good catch, thanks for raising it :)\n> > Depends how you define reasonable, but I think one straightforward fix\n> is to\n> > extend the signature digest algorithm to encompass the segwit version\n> (and\n> > maybe program-size ?) of the spending transaction outputs.\n>\n> That... doesn't sound very straightforward to me; it's basically\n> introducing a new covenant approach, that's getting fixed into a\n> signature, rather than being a separate opcode.\n>\n> I think a better approach for that would be to introduce the opcode (eg,\n> PUSH_OUTPUT_SCRIPTPUBKEY, and SUBSTR to be able to analyse the segwit\n> version), and make use of graftroot to allow a signature to declare that\n> it's conditional on some extra script code. But it feels like it's going\n> a bit off topic.\n>\n> > > Having the output position parameter might be an interesting way to\n> > > merge/split a vault/pool, but it's not clear to me how much sense it\n> > > makes sense to optimise for that, rather than just doing that via the\n> key\n> > > path. For pools, you want the key path to be common anyway (for privacy\n> > > and efficiency), so it shouldn't be a problem; but even for vaults,\n> > > you want the cold wallet accessible enough to be useful for the case\n> > > where theft is attempted, and maybe that's also accessible enough for\n> > > the ocassional merge/split to keep your utxo count/sizes reasonable.\n> > I think you can come up with interesting contract policies. Let's say\n> you want\n> > to authorize the emergency path of your pool/vault balances if X happens\n> (e.g a\n> > massive drop in USDT price signed by DLC oracles). You have (A+B+C+D)\n> forking\n> > into (A+B) and (C+D) pooled funds. To conserve the contracts\n> pre-negotiated\n> > economic equilibrium, all the participants would like the emergency path\n> to be\n> > inherited on both forks. Without relying on the key path interactivity,\n> which\n> > is ultimately a trust on the post-fork cooperation of your counterparty ?\n>\n> I'm not really sure what you're saying there; is that any different to a\n> pool of (A and B) where A suddenly wants to withdraw funds ASAP and can't\n> wait for a key path signature? In that case A authorises the withdrawal\n> and does whatever she wants with the funds (including form a new pool),\n> and B remains in the pool.\n>\n> I don't think you can reliably have some arbitrary subset of the pool\n> able to withdraw atomically without using the key path -- if A,B,C,D have\n> individual scripts allowing withdrawal, then there's no way of setting\n> the tree up so that every pair of members can have their scripts cut\n> off without also cutting off one or both of the other members withdrawal\n> scripts.\n>\n> If you know in advance which groups want to stick together, you could\n> set things up as:\n>\n>   (((A, B), AB), C)\n>\n> where:\n>\n>   A =   \"A DUP H(B') 10 TLUV CHECKSIG\"  -> (B', C)\n>   B =   \"B DUP H(A') 10 TLUV CHECKSIG\"  -> (A', C)\n>   A' =  \"A DUP 0 2 TLUV CHECKSIG\"   -> (C)\n>   B' =  \"B DUP 0 2 TLUV CHECKSIG\"   -> (C)\n>   AB =  \"(A+B) DUP 6 TLUV CHECKSIG  -> (C)\n>   C  =  \"C DUP 0 2 TLUV CHECKSIG\"   -> ((A,B), AB)\n>\n> (10 = 2+4*2 = drop my script, my sibling and my uncle; 6 = 2+4*1 =\n> drop my script and my sibling; 2 = drop my script only)\n>\n> Which would let A and B exit together in a single tx rather than needing\n> two\n> transactions to exit separately.\n>\n> > > Saving a byte of witness data at the cost of specifying additional\n> > > opcodes seems like optimising the wrong thing to me.\n> > I think we should keep in mind that any overhead cost in the usage of a\n> script\n> > primitive is echoed to the user of off-chain contract/payment channels.\n> If the\n> > tapscripts are bigger, your average on-chain spends in case of\n> non-cooperative\n> > scenarios are increased in consequence, and as such your fee-bumping\n> reserve.\n> > Thus making those systems less economically accessible.\n>\n> If you're worried about the cost of a single byte of witness data you\n> probably can't afford to do script path spends at all -- certainly\n> having to do 64 bytes of witness data to add a signature that commits\n> to an amount and the like will be infeasible in that case.\n>\n> > > I don't think that works, because different scripts in the same merkle\n> > > tree can have different script versions, which would here indicate\n> > > different parities for the same internal pub key.\n> > Let me make it clearer. We introduce a new tapscript version 0x20,\n> forcing a\n> > new bit in the first byte of the control block to be interpreted as the\n> parity\n> > bit of the spent internal pubkey.\n>\n> That doesn't work. Suppose you start off with an even internal pubkey,\n> with three scripts, (A, (B,C)). All of those scripts have tapscript\n> version 0xc0 because the internal pubkey is even. You spend using A and\n> calculate the new internal pubkey which turns out to be odd. You then\n> need to change B and C's script version from 0xc0 to 0x20, but you can't\n> do that (at least, you can't do it without revealing every script).\n>\n> > To ensure this parity bit is faithful and\n> > won't break the updated key path, it's committed in the spent taptweak.\n>\n> Changing the TapTweak calculation is a hard fork; existing software\n> already verifies the calculation even if the script version is unknown.\n>\n> > > The IN_OUT_AMOUNT opcode lets you do maths on the values, so you can\n> > > specify \"hot wallets can withdraw up to X\" rather than \"hot wallets\n> > > must withdraw exactly X\". I don't think there's a way of doing that\n> with\n> > > SIGHASH_GROUP, even with a modifier like ANYPUBKEY?\n> > You can exchange signatures for withdraw outputs with multiples `nValue`\n> > covering the authorized range, assuming the ANYAMOUNT modifier ?\n>\n> If you want your hotwallet to be able to withdraw up to $2000, that's\n> around 4,000,000 sats, so you'd be doing up to 4M signatures there if you\n> wanted to get the exact value you're trying to send, without having to\n> either overpay, or first pay yourself then have another tx that splits\n> your withdrawal into what you're spending and change that's no longer\n> in your vault.\n>\n> > One advantage\n> > of leveraging sighash is the ability to update a withdraw policy in\n> real-time.\n> > Vaults participants might be willing to bump the withdraw policy beyond\n> X,\n> > assuming you have N-of-M consents.\n>\n> I mean, maybe? It seems like a very heavy weight construct where a more\n> general approach would probably be better (eg, graftroot to attach a\n> script to a signature; or checkdatasig or whatever so you push a value\n> to the stack then check it's signature, then reuse the authenticated\n> data against other checks) so that you only have to supply a signature\n> when you want to be able to approve things after the fact.\n>\n> > I think I would like to express the following contract policy. Let's say\n> you\n> > have 1) a one-time conditional script path to withdraw fund (\"a put on\n> strike\n> > price X\"), 2) a conditional script path to tweak by 3 months all the\n> usual\n> > withdraw path and 3) those remaining withdraw paths. Once played out,\n> you would\n> > like the one-time path to be removed from your merkle tree. And this\n> removal to\n> > be inherited on the tweaked tree if 2) plays out.\n>\n> Okay, so I think that means we've got the unconditional withdraw path\n> \"U\" (your 1), the delay path \"D\" (your 2) and some normal path(s) \"N\"\n> (your 3). I think you can get that behaviour with:\n>\n>    S1 = Merkle( U, (D, N) )\n>    S2 = Merkle( U, W )\n>    S3 = Merkle( N )\n>\n> that is, you start off with the funds in scriptPubKey S1, then spend\n> using D to get to S2, then spend using W to get to S3, then presumably\n> spend using N at some point.\n>\n> The script for W is just:\n>\n>    \"IN_OUT_AMOUNT EQUALVERIFY 0 <N> 6 TLUV <3 months> CSV\"\n>        (drop the script, drop its sibling, add N, wait 3 months)\n>\n> The script for D is:\n>\n>    \"IN_OUT_AMOUNT EQUALVERIFY 0 <W> 6 TLUV <sigcheck...>\"\n>        (drop the script, drop its sibling, add W, extra conditions\n>         to avoid anyone being able to delay things)\n>\n> That is, the strategy isn't \"tweak the scripts by delaying them 3 months\"\n> it's \"tweak the merkle tree, to replace the scripts that would be delayed\n> with a new script that has a delay and then allows itself to be replaced\n> by the original scripts that we now want back\".\n>\n> Cheers,\n> aj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210918/1e644c1b/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-20T14:52:46",
                "message_text_only": "On Sat, Sep 18, 2021 at 10:11:10AM -0400, Antoine Riard wrote:\n> I think one design advantage of combining scope-minimal opcodes like MERKLESUB\n> with sighash malleability is the ability to update a subset of the off-chain\n> contract transactions fields after the funding phase.\n\nNote that it's not \"update\" so much as \"add to\"; and I mostly think\ngraftroot (and friends), or just updating the utxo onchain, are a better\ngeneral purpose way of doing that. It's definitely a tradeoff though.\n\n> Yes this is a different contract policy that I would like to set up.\n> Let's say you would like to express the following set of capabilities.\n> C0=\"Split the 4 BTC funds between Alice/Bob and Caroll/Dave\"\n> C1=\"Alice can withdraw 1 BTC after 2 weeks\"\n> C2=\"Bob can withdraw 1 BTC after 2 weeks\"\n> C3=\"Caroll can withdraw 1 BTC after 2 weeks\"\n> C4=\"Dave can withdraw 1 BTC after 2 weeks\"\n> C5=\"If USDT price=X, Alice can withdraw 2 BTC or Caroll can withdraw 2 BTC\"\n\nHmm, I'm reading C5 as \"If an oracle says X, and Alice and Carol agree,\nthey can distribute all the remaining funds as they see fit\".\n\n> If C4 is exercised, to avoid trust in the remaining counterparty, both Alice or\n> Caroll should be able to conserve the C5 option, without relying on the updated\n> key path.\n\n> As you're saying, as we know the group in advance, one way to setup the tree\n> could be:\n> \u00a0 \u00a0 \u00a0 \u00a0(A, (((((B, C), BC), D), BCD), ((((E, F), EF), G), EFG)))\n\nMake it:\n\n  (((AB, (A,B)), (CD, (C,D))), ACO)\n\nAB = DROP <alice+bob> DUP 0 6 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 2BTC LESSTHAN\nCD = same but for carol+dave\nA = <alice> DUP <B'> 10 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN\nB' = <bob> DUP 0 2 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN\nB,C,D = same as A but for bob, etc\nA',C',D' = same as B' but for alice, etc\nACO = <alice+carol> CHECKSIGVERIFY <oracle> CHECKSIG\n\nProbably AB, CD, A..D, A'..D' all want a CLTV delay in there as well.\n(Relative timelocks would probably be annoying for everyone who wasn't\nthe first to exit the pool)\n\n> Note, this solution isn't really satisfying as the G path isn't neutralized on\n> the Caroll/Dave fork and could be replayed by Alice or Bob...\n\nI think the above fixes that -- when AB is spent it deletes itself and\nthe (A,B) pair; when A is spent, it deletes (A, B and AB) and replaces\nthem with B'; when B' is spent it just deletes itself.\n\nCheers,\naj"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-22T01:40:16",
                "message_text_only": "> Hmm, I'm reading C5 as \"If an oracle says X, and Alice and Carol agree,\n> they can distribute all the remaining funds as they see fit\".\n\nShould be read as an OR:\n\n        IF 2 <oracle_sig> <alice_sig> 2 CHECKMULTISIG\n        ELSE 2 <oracle_sig> <bob_sig> 2 CHECKMULTISIG\n        ENDIF\n        <> 2 IN_OUT_AMOUNT\n\nThe empty vector is a wildcard on the spent amount, as this tapscript may\nbe executed before/\nafter the split or any withdraw option.\n\n> (Relative timelocks would probably be annoying for everyone who wasn't\n> the first to exit the pool)\n\nAnd I think unsafe, if you're wrapping a time-sensitive output in your\nwithdraw scriptPubkey.\n\n> I think the above fixes that -- when AB is spent it deletes itself and\n> the (A,B) pair; when A is spent, it deletes (A, B and AB) and replaces\n> them with B'; when B' is spent it just deletes itself.\n\nRight, here the subtlety in reading the scripts is about the B'\nsubstitution tapscript in the\nA one. And it sounds correct to me that AB exercise deletes the withdraw\npair (A, B).\n\nLe lun. 20 sept. 2021 \u00e0 10:52, Anthony Towns <aj at erisian.com.au> a \u00e9crit :\n\n> On Sat, Sep 18, 2021 at 10:11:10AM -0400, Antoine Riard wrote:\n> > I think one design advantage of combining scope-minimal opcodes like\n> MERKLESUB\n> > with sighash malleability is the ability to update a subset of the\n> off-chain\n> > contract transactions fields after the funding phase.\n>\n> Note that it's not \"update\" so much as \"add to\"; and I mostly think\n> graftroot (and friends), or just updating the utxo onchain, are a better\n> general purpose way of doing that. It's definitely a tradeoff though.\n>\n> > Yes this is a different contract policy that I would like to set up.\n> > Let's say you would like to express the following set of capabilities.\n> > C0=\"Split the 4 BTC funds between Alice/Bob and Caroll/Dave\"\n> > C1=\"Alice can withdraw 1 BTC after 2 weeks\"\n> > C2=\"Bob can withdraw 1 BTC after 2 weeks\"\n> > C3=\"Caroll can withdraw 1 BTC after 2 weeks\"\n> > C4=\"Dave can withdraw 1 BTC after 2 weeks\"\n> > C5=\"If USDT price=X, Alice can withdraw 2 BTC or Caroll can withdraw 2\n> BTC\"\n>\n> Hmm, I'm reading C5 as \"If an oracle says X, and Alice and Carol agree,\n> they can distribute all the remaining funds as they see fit\".\n>\n> > If C4 is exercised, to avoid trust in the remaining counterparty, both\n> Alice or\n> > Caroll should be able to conserve the C5 option, without relying on the\n> updated\n> > key path.\n>\n> > As you're saying, as we know the group in advance, one way to setup the\n> tree\n> > could be:\n> >        (A, (((((B, C), BC), D), BCD), ((((E, F), EF), G), EFG)))\n>\n> Make it:\n>\n>   (((AB, (A,B)), (CD, (C,D))), ACO)\n>\n> AB = DROP <alice+bob> DUP 0 6 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 2BTC\n> LESSTHAN\n> CD = same but for carol+dave\n> A = <alice> DUP <B'> 10 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN\n> B' = <bob> DUP 0 2 TLUV CHECKSIGVERIFY IN_OUT_AMOUNT SUB 1BTC LESSTHAN\n> B,C,D = same as A but for bob, etc\n> A',C',D' = same as B' but for alice, etc\n> ACO = <alice+carol> CHECKSIGVERIFY <oracle> CHECKSIG\n>\n> Probably AB, CD, A..D, A'..D' all want a CLTV delay in there as well.\n> (Relative timelocks would probably be annoying for everyone who wasn't\n> the first to exit the pool)\n>\n> > Note, this solution isn't really satisfying as the G path isn't\n> neutralized on\n> > the Caroll/Dave fork and could be replayed by Alice or Bob...\n>\n> I think the above fixes that -- when AB is spent it deletes itself and\n> the (A,B) pair; when A is spent, it deletes (A, B and AB) and replaces\n> them with B'; when B' is spent it just deletes itself.\n>\n> Cheers,\n> aj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/a563fe77/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2021-09-23T00:29:18",
                "message_text_only": "Hi AJ,\n\nHappy to see that this proposal has finally seen the light of day! I've been\nhearing about it in hinted background convos over the past few months, so\nhappy I can finally dig into the specifics of its operation.\n\n> So the idea is to do just that via a new opcode \"TAPLEAF_UPDATE_VERIFY\"\n> (TLUV) that takes three inputs: one that specifies how to update the\n> internal public key (X), one that specifies a new step for the merkle path\n> (F), and one that specifies whether to remove the current script and/or\n> how many merkle path steps to remove\n\nWhat if instead, it obtained the script from the _annex_? I think this small\nmodification would make the op code even _more_ powerful. Consider that this\nallows a new script to be passed _dynamically_ after the output has been\ncreated, possibly by a threshold of parties that control the output, or them\nall (mu sig, etc, etc). This serves to create a generic \"upgrade\" mechanism\nfor any tapscript output (covenant or not). Functionally, this is similar to\nthe existence of \"admin keys\" or voted DAO upgrades that exists in chains\nthat utilize an account based systems. This is really useful as it allows a\nscript any given output to optional add in graftroot like behavior (leaf in\ntree that accepts script updates), and also allows contract developers to\nprogressively upgrade or fix issues in prior versions of their deployed\ncontracts.\n\nThis little trick is secure since unlike the witness itself, the annex is\nactually _signed_ within the sighash like everything else. Incorporating\nthis proposal would require the addition of an OP_PUSH_ANNEX op code, which\nby itself seems expertly useful. If one views the annex as a sort of\nauthenticated associated data that can be passed into the script execution\ncontext, then this actually serves to absorb _some_ uses cases of a\nhypothetical OP_CHECKSIG_FROM_STACK opcode. A push annex op code also makes\ndoing things like output delegation to a given key passed into the witness\nsecure since the prior \"owner\" of the output commits to the key within the\nsighash.\n\nEven assuming a more powerful type of covenant that allows partial\napplication of binding logic, something like this is still super useful\nsince the action of re-creating a new tapscript tree based in dynamic input\ndata would generate a rather large witness if only something like OP_CAT was\navailable. The unique \"update\" nature of this appears to augment any other\ntype of covenant, which is pretty cool. Consider that it would allow you\n(with the annex addition above), take something like a CTV congestion tree,\nand add in _new_ users at the tree is already being unrolled (just a toy\nexample).\n\nIt would also allow an individual to _join_ the payment pool construct\ndescribed earlier which makes it 1000x more useful (vs just supporting\nunrolling). I haven't written it all down yet, but I think this along with\nsomething like CTV or CSFS makes it possible to implement a Plasma Cash [4]\nlike Commit Chain [5], which is super exciting (assume a counter is embedded\nin the main script that tracks the next free leaf slot(s). With this model\nan \"operator\" is able to include a single transaction in the chain that\nstamps a batch of updates in the payment tree.  Users then get a\ncontestation period where they can refute a modification to the tree in\norder to withdraw their funds.\n\n> And second, it doesn't provide a way for utxos to \"interact\",\n\nThis is due to the fact that the op code doesn't allow any sort of late\nbinding or pattern matching then constraining _where_ (or whence?) the coins\ncan Be sent to. There's a group of developers that are attempting to make an\nAMM-like system on Liquid [1] using more generic stack based covenants [2]\n(see the `OP_INSPECTINPUT` op code, which seems very much inspired by\njl2012's old proposal). However one challenge that still need to be tackled\nin the UTXO model is allowing multiple participants to easily interact w/\nthe\ncontract in a single block w/o a coordination layer to synchronize the\naccess.\n\nOne solution to this concurrency issue, that I believe is already employed\nby Chia is to allow \"contracts\" to be identified via a fixed ID (as long as\ntheir active in the chain) [3]. This lets transactions spend/interact with a\ncontract, without always needing to know the set of active UTXOs where that\ncontract lives. Transactions then specify their contract and \"regular\"\ninputs, with the requirement that every transaction spends at least a single\nregular input.\n\nThe trade-off here is that nodes need to maintain this extra index into the\nUTXO set. However, this can be alleviated by applying a utreexo like\nsolution: nodes maintain some merklized data structure over the index and\nrequire that spending transactions provide an _inclusion_ proof of the\nactive contract. Nodes then only need to maintain root hashes of the UTXO\nand contract set.\n\nI'm super happy w.r.t how the covenant space has been processing over the\npast few years. IMO its the single most important (along with the utreexo\ntype stateless stuff mentioned above) missing component to allow the\ncreation of more decentralized self-custodial applications built on top of\nBitcoin.\n\n-- Laolu\n\n[1]: https://medium.com/bit-matrix\n[2]:\nhttps://github.com/sanket1729/elements/blob/84339ba5e5dc65328d98afe2b1b33dcb69ba4311/doc/tapscript_opcodes.md\n[3]: https://forum.celestia.org/t/accounts-strict-access-lists-and-utxos/37\n[4]: https://www.learnplasma.org/en/learn/cash.html\n[5]: https://eprint.iacr.org/2018/642\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210922/4ccd6220/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "TAPLEAF_UPDATE_VERIFY covenant opcode",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Antoine Riard",
                "Anthony Towns",
                "darosior",
                "Olaoluwa Osuntokun",
                "Matt Corallo"
            ],
            "messages_count": 15,
            "total_messages_chars_count": 158028
        }
    },
    {
        "title": "[bitcoin-dev] Clarification on the use of getblocktemplate RPC method.",
        "thread_messages": [
            {
                "author": "Mike Rosset",
                "date": "2021-09-09T12:54:18",
                "message_text_only": "Hello all,\n\nI recently went down the bitcoin protocol rabbit hole. I wanted to use\nGNU guile scheme to experiment with bitcoin. I initially started by\ncreating a toy bitcoin miner but I've run into some inconsistencies with\nthe documentation found on\nhttps://en.bitcoin.it/wiki/Getblocktemplate. Namely with creating the\ntemplates merkle root.\n\n>From my understanding a coinbase transaction should have the\ntransactions data concatenated before creating the merkle root. But\ngetblocktemplate does not have a json cointbasetxn field. So I'm not\nsure how to create a coinbase transaction without that.\n\nI have a test template response data found here.\nhttps://raw.githubusercontent.com/mrosset/prospect/master/test-suite/data.json\nand using a modified version of the merkle python reference script found\non the wiki page. see\nhttps://github.com/mrosset/prospect/blob/master/scripts/merkle.py . I'm\nable to create a merkle root with the hash\nc5fff939f628a04428c080ed5bd7cd9bc0b4722b2522743049adb18213adf28a but\nthat's minus the coinbase transaction.\n\nSo far I'm able to replicate this hash using the test data in guile. But\nI'd like to sanitize this so that I'm using a coinbase transaction and\nmaking sure the python and guile merkle roots match.\n\nIn short how do I get the coinbase transaction without the coinbasetxn\nfield existing?\n\nMike"
            },
            {
                "author": "Luke Dashjr",
                "date": "2021-09-09T17:36:17",
                "message_text_only": "https://github.com/bitcoin/libblkmaker/blob/master/blkmaker.c#L172\n\nOn Thursday 09 September 2021 12:54:18 Mike Rosset via bitcoin-dev wrote:\n> Hello all,\n>\n> I recently went down the bitcoin protocol rabbit hole. I wanted to use\n> GNU guile scheme to experiment with bitcoin. I initially started by\n> creating a toy bitcoin miner but I've run into some inconsistencies with\n> the documentation found on\n> https://en.bitcoin.it/wiki/Getblocktemplate. Namely with creating the\n> templates merkle root.\n>\n> From my understanding a coinbase transaction should have the\n> transactions data concatenated before creating the merkle root. But\n> getblocktemplate does not have a json cointbasetxn field. So I'm not\n> sure how to create a coinbase transaction without that.\n>\n> I have a test template response data found here.\n> https://raw.githubusercontent.com/mrosset/prospect/master/test-suite/data.j\n>son and using a modified version of the merkle python reference script found\n> on the wiki page. see\n> https://github.com/mrosset/prospect/blob/master/scripts/merkle.py . I'm\n> able to create a merkle root with the hash\n> c5fff939f628a04428c080ed5bd7cd9bc0b4722b2522743049adb18213adf28a but\n> that's minus the coinbase transaction.\n>\n> So far I'm able to replicate this hash using the test data in guile. But\n> I'd like to sanitize this so that I'm using a coinbase transaction and\n> making sure the python and guile merkle roots match.\n>\n> In short how do I get the coinbase transaction without the coinbasetxn\n> field existing?\n>\n> Mike\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Clarification on the use of getblocktemplate RPC method.",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Mike Rosset",
                "Luke Dashjr"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3058
        }
    },
    {
        "title": "[bitcoin-dev] Storing the Merkle Tree in a compact way",
        "thread_messages": [
            {
                "author": "shymaa arafat",
                "date": "2021-09-11T03:00:12",
                "message_text_only": "Allow me to introduce this simple idea that could be useful ...\n\n-The Intuition was some discussion on Utreexo project about storage saving\nand some traversing issues in handling the UTXOS Merkle Tree/ forest; that\nis  N internal nodes need to be stored along with 2N pointers (left&right),\n+ maybe 1 more pointer in the leaves special nodes to handle different\ntraversing options (insert, delete, & differently proof fetch that traverse\naunt or niece node according to your implementation\nhttps://github.com/mit-dci/utreexo/discussions/316)\n.\nThen, I thought of a simple idea that gets rid of all the pointers;\nspecially appealing when we have all trees are full (complete) in the\nforest, but can work for any Merkle Tree:\n\n- 2D array with variable row size; R[j] is of length (N/2^j)\n-For example when N=8 nodes\nR[0]=0,1,2,...,7\nR[1]=8,9,10,11\nR[2]=12,13\nR[3]=14\n.\n-We can see that total storage is just 2N-1 nodes,\nno need for pointers, and traversing could be neat in any direction with\nthe right formula:\n\n-Pseudo code to fetch proof[i] ...\n\n//direction to know + or -\nIf ((i mod 2)==0) drct=1;\n            else drct=-1;\n// first, the sibling node\nproof[i]=R[0,i+drct]\n\n//add the rest thru loop\nFor(j=1; j\u2264logN; j++)\n { index= i/(2^j)+drct;\n    proof[i]=Add(R[j,index]);\n }\n\n-In fact it's just the simple primitive approach of transforming a\nrecursion to an iteration, and even if Utreexo team solved their problem\ndifferently I thought it is worth telling as it can work for any Merkle Tree\n.\nThanks for your time,\nShymaa M Arafat\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210911/1ca4abf1/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Storing the Merkle Tree in a compact way",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "shymaa arafat"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1724
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] Storing the Merkle Tree in a compact way",
        "thread_messages": [
            {
                "author": "shymaa arafat",
                "date": "2021-09-16T15:05:24",
                "message_text_only": "It could be viewed as the simple complete tree to 1D array  with no\npointers described in lecture 8 here\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-spring-2020/lecture-notes/index.htm\nstarting from min 15 in this video\nhttps://youtu.be/Xnpo1atN-Iw\n\nSince all trees in Utreexo forest are full binary trees, this is perfect to\nuse, and we can save *76*10\u2076*2*size of pointer(probably4bytes)*\n*~600MB *with almost no effort.\n\nHowever, I suggest to put it in a 2D array to make it more easy to handle\n(the indexing math) as we, different than the lecture, traverse in many\nways ( normally to delete or insert, and the parent siblings for the proofs)\n\nI wrote more details here\nhttps://bitcointalk.org/index.php?topic=5360009.0\n\nOn Thu, Sep 16, 2021, 14:37 Vincent <vincent.palazzo at protonmail.com> wrote:\n\n> Hi.\n>\n> Thanks for the reference, but I missed where you want save space with this\n> compression on the Merkle Tree.\n>\n> Regards.\n>\n> Vincent.\n> vincenzo.palazzo at protonmail.com\n> https://github.com/vincenzopalazzo\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Thursday, September 16th, 2021 at 5:15 AM, shymaa arafat <\n> shymaa.arafat at gmail.com> wrote:\n>\n> Allow me to introduce this simple idea that could be useful ...\n>\n> -The Intuition was some discussion on Utreexo project about storage saving\n> and some traversing issues in handling the UTXOS Merkle Tree/ forest; that\n> is  N internal nodes need to be stored along with 2N pointers (left&right),\n> + maybe 1 more pointer in the leaves special nodes to handle different\n> traversing options (insert, delete, & differently proof fetch that traverse\n> aunt or niece node according to your implementation\n> https://github.com/mit-dci/utreexo/discussions/316)\n> .\n> Then, I thought of a simple idea that gets rid of all the pointers;\n> specially appealing when we have all trees are full (complete) in the\n> forest, but can work for any Merkle Tree:\n>\n> - 2D array with variable row size; R[j] is of length (N/2^j)\n> -For example when N=8 nodes\n> R[0]=0,1,2,...,7\n> R[1]=8,9,10,11\n> R[2]=12,13\n> R[3]=14\n> .\n> -We can see that total storage is just 2N-1 nodes,\n> no need for pointers, and traversing could be neat in any direction with\n> the right formula:\n>\n> -Pseudo code to fetch proof[i] ...\n>\n> //direction to know + or -\n> If ((i mod 2)==0) drct=1;\n>             else drct=-1;\n> // first, the sibling node\n> proof[i]=R[0,i+drct]\n>\n> //add the rest thru loop\n> For(j=1; j\u2264logN; j++)\n>  { index= i/(2^j)+drct;\n>     proof[i]=Add(R[j,index]);\n>  }\n>\n> -In fact it's just the simple primitive approach of transforming a\n> recursion to an iteration, and even if Utreexo team solved their problem\n> differently I thought it is worth telling as it can work for any Merkle Tree\n> .\n> Thanks for your time,\n> Shymaa M Arafat\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/c0fa90ad/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: IMG_20210913_194256.jpg\nType: image/jpeg\nSize: 279763 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/c0fa90ad/attachment-0002.jpg>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: IMG_20210913_193322.jpg\nType: image/jpeg\nSize: 164592 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/c0fa90ad/attachment-0003.jpg>"
            }
        ],
        "thread_summary": {
            "title": "Storing the Merkle Tree in a compact way",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "shymaa arafat"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3588
        }
    },
    {
        "title": "[bitcoin-dev] Proposal: Auto-shutdown as 5-year fork window",
        "thread_messages": [
            {
                "author": "James Lu",
                "date": "2021-09-12T15:26:44",
                "message_text_only": "If MTP-11 is greater than 5 years after the release date of the current\nsoftware version, the full node should shut down automatically.\n\nThis would allow writing code that gives the community ~5 years to upgrade\nto a version that executes a new hard fork while keeping everyone in\nconsensus, provided the change is non-controversial.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/9e083ba8/attachment-0001.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-09-12T19:38:47",
                "message_text_only": "You can do that kind of change in your own Bitcoin-compatible client, but you cannot be sure that other people will run that version and that it will shut down when you want. Many miners use their own custom software for mining blocks, the same for mining pools. There are many clients that are compatible with consensus, but different than Bitcoin Core.\nAlso you should notice that Bitcoin community make changes by using soft-forks, not hard-forks. Backward compatibility is preserved as often as possible and there is no reason to change that. Any change can be deployed in a soft-fork way, even \"evil soft-forks\" are possible, as described in https://petertodd.org/2016/forced-soft-forks. I think that kind of soft-fork is still better than hard-fork.\nOn 2021-09-12 21:16:00 user James Lu via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nIf MTP-11 is greater than 5 years after the release date of the current software version, the full node should shut down automatically.\n\u00a0\nThis would\u00a0allow writing code that\u00a0gives the community ~5 years to upgrade to a version that executes a new hard fork while keeping everyone in consensus, provided the change is non-controversial.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210912/4805d7b0/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: Auto-shutdown as 5-year fork window",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "James Lu",
                "vjudeu at gazeta.pl"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1897
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Core 22.0 released",
        "thread_messages": [
            {
                "author": "W. J. van der Laan",
                "date": "2021-09-13T14:31:16",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\n22.0 Release Notes\n==================\n\nBitcoin Core version 22.0 is now available from:\n\n  <https://bitcoincore.org/bin/bitcoin-core-22.0/>\n\nOr through bittorrent:\n\n  magnet:?xt=urn:btih:1538a3b3962215f12e0e5f60105457332cf8fee4&dn=bitcoin-core-22.0&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Fexplodie.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.torrent.eu.org%3A451%2Fannounce&tr=udp%3A%2F%2Ftracker.bitcoin.sprovoost.nl%3A6969\n\nThis release includes new features, various bug fixes and performance\nimprovements, as well as updated translations.\n\nPlease report bugs using the issue tracker at GitHub:\n\n  <https://github.com/bitcoin/bitcoin/issues>\n\nTo receive security and update notifications, please subscribe to:\n\n  <https://bitcoincore.org/en/list/announcements/join/>\n\nHow to Upgrade\n==============\n\nIf you are running an older version, shut it down. Wait until it has completely\nshut down (which might take a few minutes in some cases), then run the\ninstaller (on Windows) or just copy over `/Applications/Bitcoin-Qt` (on Mac)\nor `bitcoind`/`bitcoin-qt` (on Linux).\n\nUpgrading directly from a version of Bitcoin Core that has reached its EOL is\npossible, but it might take some time if the data directory needs to be migrated. Old\nwallet versions of Bitcoin Core are generally supported.\n\nCompatibility\n==============\n\nBitcoin Core is supported and extensively tested on operating systems\nusing the Linux kernel, macOS 10.14+, and Windows 7 and newer.  Bitcoin\nCore should also work on most other Unix-like systems but is not as\nfrequently tested on them.  It is not recommended to use Bitcoin Core on\nunsupported systems.\n\n- From Bitcoin Core 22.0 onwards, macOS versions earlier than 10.14 are no longer supported.\n\nNotable changes\n===============\n\nP2P and network changes\n- -----------------------\n- - Added support for running Bitcoin Core as an\n  [I2P (Invisible Internet Project)](https://en.wikipedia.org/wiki/I2P) service\n  and connect to such services. See [i2p.md](https://github.com/bitcoin/bitcoin/blob/22.x/doc/i2p.md) for details. (#20685)\n- - This release removes support for Tor version 2 hidden services in favor of Tor\n  v3 only, as the Tor network [dropped support for Tor\n  v2](https://blog.torproject.org/v2-deprecation-timeline) with the release of\n  Tor version 0.4.6.  Henceforth, Bitcoin Core ignores Tor v2 addresses; it\n  neither rumors them over the network to other peers, nor stores them in memory\n  or to `peers.dat`.  (#22050)\n\n- - Added NAT-PMP port mapping support via\n  [`libnatpmp`](https://miniupnp.tuxfamily.org/libnatpmp.html). (#18077)\n\nNew and Updated RPCs\n- --------------------\n\n- - Due to [BIP 350](https://github.com/bitcoin/bips/blob/master/bip-0350.mediawiki)\n  being implemented, behavior for all RPCs that accept addresses is changed when\n  a native witness version 1 (or higher) is passed. These now require a Bech32m\n  encoding instead of a Bech32 one, and Bech32m encoding will be used for such\n  addresses in RPC output as well. No version 1 addresses should be created\n  for mainnet until consensus rules are adopted that give them meaning\n  (as will happen through [BIP 341](https://github.com/bitcoin/bips/blob/master/bip-0341.mediawiki)).\n  Once that happens, Bech32m is expected to be used for them, so this shouldn't\n  affect any production systems, but may be observed on other networks where such\n  addresses already have meaning (like signet). (#20861)\n\n- - The `getpeerinfo` RPC returns two new boolean fields, `bip152_hb_to` and\n  `bip152_hb_from`, that respectively indicate whether we selected a peer to be\n  in compact blocks high-bandwidth mode or whether a peer selected us as a\n  compact blocks high-bandwidth peer. High-bandwidth peers send new block\n  announcements via a `cmpctblock` message rather than the usual inv/headers\n  announcements. See BIP 152 for more details. (#19776)\n\n- - `getpeerinfo` no longer returns the following fields: `addnode`, `banscore`,\n  and `whitelisted`, which were previously deprecated in 0.21. Instead of\n  `addnode`, the `connection_type` field returns manual. Instead of\n  `whitelisted`, the `permissions` field indicates if the peer has special\n  privileges. The `banscore` field has simply been removed. (#20755)\n\n- - The following RPCs:  `gettxout`, `getrawtransaction`, `decoderawtransaction`,\n  `decodescript`, `gettransaction`, and REST endpoints: `/rest/tx`,\n  `/rest/getutxos`, `/rest/block` deprecated the following fields (which are no\n  longer returned in the responses by default): `addresses`, `reqSigs`.\n  The `-deprecatedrpc=addresses` flag must be passed for these fields to be\n  included in the RPC response. This flag/option will be available only for this major release, after which\n  the deprecation will be removed entirely. Note that these fields are attributes of\n  the `scriptPubKey` object returned in the RPC response. However, in the response\n  of `decodescript` these fields are top-level attributes, and included again as attributes\n  of the `scriptPubKey` object. (#20286)\n\n- - When creating a hex-encoded bitcoin transaction using the `bitcoin-tx` utility\n  with the `-json` option set, the following fields: `addresses`, `reqSigs` are no longer\n  returned in the tx output of the response. (#20286)\n\n- - The `listbanned` RPC now returns two new numeric fields: `ban_duration` and `time_remaining`.\n  Respectively, these new fields indicate the duration of a ban and the time remaining until a ban expires,\n  both in seconds. Additionally, the `ban_created` field is repositioned to come before `banned_until`. (#21602)\n\n- - The `setban` RPC can ban onion addresses again. This fixes a regression introduced in version 0.21.0. (#20852)\n\n- - The `getnodeaddresses` RPC now returns a \"network\" field indicating the\n  network type (ipv4, ipv6, onion, or i2p) for each address.  (#21594)\n\n- - `getnodeaddresses` now also accepts a \"network\" argument (ipv4, ipv6, onion,\n  or i2p) to return only addresses of the specified network.  (#21843)\n\n- - The `testmempoolaccept` RPC now accepts multiple transactions (still experimental at the moment,\n  API may be unstable). This is intended for testing transaction packages with dependency\n  relationships; it is not recommended for batch-validating independent transactions. In addition to\n  mempool policy, package policies apply: the list cannot contain more than 25 transactions or have a\n  total size exceeding 101K virtual bytes, and cannot conflict with (spend the same inputs as) each other or\n  the mempool, even if it would be a valid BIP125 replace-by-fee. There are some known limitations to\n  the accuracy of the test accept: it's possible for `testmempoolaccept` to return \"allowed\"=True for a\n  group of transactions, but \"too-long-mempool-chain\" if they are actually submitted. (#20833)\n\n- - `addmultisigaddress` and `createmultisig` now support up to 20 keys for\n  Segwit addresses. (#20867)\n\nChanges to Wallet or GUI related RPCs can be found in the GUI or Wallet section below.\n\nBuild System\n- ------------\n\n- - Release binaries are now produced using the new `guix`-based build system.\n  The [/doc/release-process.md](https://github.com/bitcoin/bitcoin/blob/master/doc/release-process.md) document has been updated accordingly.\n\nFiles\n- -----\n\n- - The list of banned hosts and networks (via `setban` RPC) is now saved on disk\n  in JSON format in `banlist.json` instead of `banlist.dat`. `banlist.dat` is\n  only read on startup if `banlist.json` is not present. Changes are only written to the new\n  `banlist.json`. A future version of Bitcoin Core may completely ignore\n  `banlist.dat`. (#20966)\n\nNew settings\n- ------------\n\n- - The `-natpmp` option has been added to use NAT-PMP to map the listening port.\n  If both UPnP and NAT-PMP are enabled, a successful allocation from UPnP\n  prevails over one from NAT-PMP. (#18077)\n\nUpdated settings\n- ----------------\n\nChanges to Wallet or GUI related settings can be found in the GUI or Wallet section below.\n\n- - Passing an invalid `-rpcauth` argument now cause bitcoind to fail to start.  (#20461)\n\nTools and Utilities\n- -------------------\n\n- - A new CLI `-addrinfo` command returns the number of addresses known to the\n  node per network type (including Tor v2 versus v3) and total. This can be\n  useful to see if the node knows enough addresses in a network to use options\n  like `-onlynet=<network>` or to upgrade to this release of Bitcoin Core 22.0\n  that supports Tor v3 only.  (#21595)\n\n- - A new `-rpcwaittimeout` argument to `bitcoin-cli` sets the timeout\n  in seconds to use with `-rpcwait`. If the timeout expires,\n  `bitcoin-cli` will report a failure. (#21056)\n\nWallet\n- ------\n\n- - External signers such as hardware wallets can now be used through the new RPC methods `enumeratesigners` and `displayaddress`. Support is also added to the `send` RPC call. This feature is experimental. See [external-signer.md](https://github.com/bitcoin/bitcoin/blob/22.x/doc/external-signer.md) for details. (#16546)\n\n- - A new `listdescriptors` RPC is available to inspect the contents of descriptor-enabled wallets.\n  The RPC returns public versions of all imported descriptors, including their timestamp and flags.\n  For ranged descriptors, it also returns the range boundaries and the next index to generate addresses from. (#20226)\n\n- - The `bumpfee` RPC is not available with wallets that have private keys\n  disabled. `psbtbumpfee` can be used instead. (#20891)\n\n- - The `fundrawtransaction`, `send` and `walletcreatefundedpsbt` RPCs now support an `include_unsafe` option\n  that when `true` allows using unsafe inputs to fund the transaction.\n  Note that the resulting transaction may become invalid if one of the unsafe inputs disappears.\n  If that happens, the transaction must be funded with different inputs and republished. (#21359)\n\n- - We now support up to 20 keys in `multi()` and `sortedmulti()` descriptors\n  under `wsh()`. (#20867)\n\n- - Taproot descriptors can be imported into the wallet only after activation has occurred on the network (e.g. mainnet, testnet, signet) in use. See [descriptors.md](https://github.com/bitcoin/bitcoin/blob/22.x/doc/descriptors.md) for supported descriptors.\n\nGUI changes\n- -----------\n\n- - External signers such as hardware wallets can now be used. These require an external tool such as [HWI](https://github.com/bitcoin-core/HWI) to be installed and configured under Options -> Wallet. When creating a new wallet a new option \"External signer\" will appear in the dialog. If the device is detected, its name is suggested as the wallet name. The watch-only keys are then automatically imported. Receive addresses can be verified on the device. The send dialog will automatically use the connected device. This feature is experimental and the UI may freeze for a few seconds when performing these actions.\n\nLow-level changes\n=================\n\nRPC\n- ---\n\n- - The RPC server can process a limited number of simultaneous RPC requests.\n  Previously, if this limit was exceeded, the RPC server would respond with\n  [status code 500 (`HTTP_INTERNAL_SERVER_ERROR`)](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#5xx_server_errors).\n  Now it returns status code 503 (`HTTP_SERVICE_UNAVAILABLE`). (#18335)\n\n- - Error codes have been updated to be more accurate for the following error cases (#18466):\n  - `signmessage` now returns RPC_INVALID_ADDRESS_OR_KEY (-5) if the\n    passed address is invalid. Previously returned RPC_TYPE_ERROR (-3).\n  - `verifymessage` now returns RPC_INVALID_ADDRESS_OR_KEY (-5) if the\n    passed address is invalid. Previously returned RPC_TYPE_ERROR (-3).\n  - `verifymessage` now returns RPC_TYPE_ERROR (-3) if the passed signature\n    is malformed. Previously returned RPC_INVALID_ADDRESS_OR_KEY (-5).\n\nTests\n- -----\n\n22.0 change log\n===============\n\nA detailed list of changes in this version follows. To keep the list to a manageable length, small refactors and typo fixes are not included, and similar changes are sometimes condensed into one line.\n\n### Consensus\n- - bitcoin/bitcoin#19438 Introduce deploymentstatus (ajtowns)\n- - bitcoin/bitcoin#20207 Follow-up extra comments on taproot code and tests (sipa)\n- - bitcoin/bitcoin#21330 Deal with missing data in signature hashes more consistently (sipa)\n\n### Policy\n- - bitcoin/bitcoin#18766 Disable fee estimation in blocksonly mode (by removing the fee estimates global) (darosior)\n- - bitcoin/bitcoin#20497 Add `MAX_STANDARD_SCRIPTSIG_SIZE` to policy (sanket1729)\n- - bitcoin/bitcoin#20611 Move `TX_MAX_STANDARD_VERSION` to policy (MarcoFalke)\n\n### Mining\n- - bitcoin/bitcoin#19937, bitcoin/bitcoin#20923 Signet mining utility (ajtowns)\n\n### Block and transaction handling\n- - bitcoin/bitcoin#14501 Fix possible data race when committing block files (luke-jr)\n- - bitcoin/bitcoin#15946 Allow maintaining the blockfilterindex when using prune (jonasschnelli)\n- - bitcoin/bitcoin#18710 Add local thread pool to CCheckQueue (hebasto)\n- - bitcoin/bitcoin#19521 Coinstats Index (fjahr)\n- - bitcoin/bitcoin#19806 UTXO snapshot activation (jamesob)\n- - bitcoin/bitcoin#19905 Remove dead CheckForkWarningConditionsOnNewFork (MarcoFalke)\n- - bitcoin/bitcoin#19935 Move SaltedHashers to separate file and add some new ones (achow101)\n- - bitcoin/bitcoin#20054 Remove confusing and useless \"unexpected version\" warning (MarcoFalke)\n- - bitcoin/bitcoin#20519 Handle rename failure in `DumpMempool(\u2026)` by using the `RenameOver(\u2026)` return value (practicalswift)\n- - bitcoin/bitcoin#20749, bitcoin/bitcoin#20750, bitcoin/bitcoin#21055, bitcoin/bitcoin#21270, bitcoin/bitcoin#21525, bitcoin/bitcoin#21391, bitcoin/bitcoin#21767, bitcoin/bitcoin#21866 Prune `g_chainman` usage (dongcarl)\n- - bitcoin/bitcoin#20833 rpc/validation: enable packages through testmempoolaccept (glozow)\n- - bitcoin/bitcoin#20834 Locks and docs in ATMP and CheckInputsFromMempoolAndCache (glozow)\n- - bitcoin/bitcoin#20854 Remove unnecessary try-block (amitiuttarwar)\n- - bitcoin/bitcoin#20868 Remove redundant check on pindex (jarolrod)\n- - bitcoin/bitcoin#20921 Don't try to invalidate genesis block in CChainState::InvalidateBlock (theStack)\n- - bitcoin/bitcoin#20972 Locks: Annotate CTxMemPool::check to require `cs_main` (dongcarl)\n- - bitcoin/bitcoin#21009 Remove RewindBlockIndex logic (dhruv)\n- - bitcoin/bitcoin#21025 Guard chainman chainstates with `cs_main` (dongcarl)\n- - bitcoin/bitcoin#21202 Two small clang lock annotation improvements (amitiuttarwar)\n- - bitcoin/bitcoin#21523 Run VerifyDB on all chainstates (jamesob)\n- - bitcoin/bitcoin#21573 Update libsecp256k1 subtree to latest master (sipa)\n- - bitcoin/bitcoin#21582, bitcoin/bitcoin#21584, bitcoin/bitcoin#21585 Fix assumeutxo crashes (MarcoFalke)\n- - bitcoin/bitcoin#21681 Fix ActivateSnapshot to use hardcoded nChainTx (jamesob)\n- - bitcoin/bitcoin#21796 index: Avoid async shutdown on init error (MarcoFalke)\n- - bitcoin/bitcoin#21946 Document and test lack of inherited signaling in RBF policy (ariard)\n- - bitcoin/bitcoin#22084 Package testmempoolaccept followups (glozow)\n- - bitcoin/bitcoin#22102 Remove `Warning:` from warning message printed for unknown new rules (prayank23)\n- - bitcoin/bitcoin#22112 Force port 0 in I2P (vasild)\n- - bitcoin/bitcoin#22135 CRegTestParams: Use `args` instead of `gArgs` (kiminuo)\n- - bitcoin/bitcoin#22146 Reject invalid coin height and output index when loading assumeutxo (MarcoFalke)\n- - bitcoin/bitcoin#22253 Distinguish between same tx and same-nonwitness-data tx in mempool (glozow)\n- - bitcoin/bitcoin#22261 Two small fixes to node broadcast logic (jnewbery)\n- - bitcoin/bitcoin#22415 Make `m_mempool` optional in CChainState (jamesob)\n- - bitcoin/bitcoin#22499 Update assumed chain params (sriramdvt)\n- - bitcoin/bitcoin#22589 net, doc: update I2P hardcoded seeds and docs for 22.0 (jonatack)\n\n### P2P protocol and network code\n- - bitcoin/bitcoin#18077 Add NAT-PMP port forwarding support (hebasto)\n- - bitcoin/bitcoin#18722 addrman: improve performance by using more suitable containers (vasild)\n- - bitcoin/bitcoin#18819 Replace `cs_feeFilter` with simple std::atomic (MarcoFalke)\n- - bitcoin/bitcoin#19203 Add regression fuzz harness for CVE-2017-18350. Add FuzzedSocket (practicalswift)\n- - bitcoin/bitcoin#19288 fuzz: Add fuzzing harness for TorController (practicalswift)\n- - bitcoin/bitcoin#19415 Make DNS lookup mockable, add fuzzing harness (practicalswift)\n- - bitcoin/bitcoin#19509 Per-Peer Message Capture (troygiorshev)\n- - bitcoin/bitcoin#19763 Don't try to relay to the address' originator (vasild)\n- - bitcoin/bitcoin#19771 Replace enum CConnMan::NumConnections with enum class ConnectionDirection (luke-jr)\n- - bitcoin/bitcoin#19776 net, rpc: expose high bandwidth mode state via getpeerinfo (theStack)\n- - bitcoin/bitcoin#19832 Put disconnecting logs into BCLog::NET category (hebasto)\n- - bitcoin/bitcoin#19858 Periodically make block-relay connections and sync headers (sdaftuar)\n- - bitcoin/bitcoin#19884 No delay in adding fixed seeds if -dnsseed=0 and peers.dat is empty (dhruv)\n- - bitcoin/bitcoin#20079 Treat handshake misbehavior like unknown message (MarcoFalke)\n- - bitcoin/bitcoin#20138 Assume that SetCommonVersion is called at most once per peer (MarcoFalke)\n- - bitcoin/bitcoin#20162 p2p: declare Announcement::m_state as uint8_t, add getter/setter (jonatack)\n- - bitcoin/bitcoin#20197 Protect onions in AttemptToEvictConnection(), add eviction protection test coverage (jonatack)\n- - bitcoin/bitcoin#20210 assert `CNode::m_inbound_onion` is inbound in ctor, add getter, unit tests (jonatack)\n- - bitcoin/bitcoin#20228 addrman: Make addrman a top-level component (jnewbery)\n- - bitcoin/bitcoin#20234 Don't bind on 0.0.0.0 if binds are restricted to Tor (vasild)\n- - bitcoin/bitcoin#20477 Add unit testing of node eviction logic (practicalswift)\n- - bitcoin/bitcoin#20516 Well-defined CAddress disk serialization, and addrv2 anchors.dat (sipa)\n- - bitcoin/bitcoin#20557 addrman: Fix new table bucketing during unserialization (jnewbery)\n- - bitcoin/bitcoin#20561 Periodically clear `m_addr_known` (sdaftuar)\n- - bitcoin/bitcoin#20599 net processing: Tolerate sendheaders and sendcmpct messages before verack (jnewbery)\n- - bitcoin/bitcoin#20616 Check CJDNS address is valid (lontivero)\n- - bitcoin/bitcoin#20617 Remove `m_is_manual_connection` from CNodeState (ariard)\n- - bitcoin/bitcoin#20624 net processing: Remove nStartingHeight check from block relay (jnewbery)\n- - bitcoin/bitcoin#20651 Make p2p recv buffer timeout 20 minutes for all peers (jnewbery)\n- - bitcoin/bitcoin#20661 Only select from addrv2-capable peers for torv3 address relay (sipa)\n- - bitcoin/bitcoin#20685 Add I2P support using I2P SAM (vasild)\n- - bitcoin/bitcoin#20690 Clean up logging of outbound connection type (sdaftuar)\n- - bitcoin/bitcoin#20721 Move ping data to `net_processing` (jnewbery)\n- - bitcoin/bitcoin#20724 Cleanup of -debug=net log messages (ajtowns)\n- - bitcoin/bitcoin#20747 net processing: Remove dropmessagestest (jnewbery)\n- - bitcoin/bitcoin#20764 cli -netinfo peer connections dashboard updates \ud83c\udf84 \u2728 (jonatack)\n- - bitcoin/bitcoin#20788 add RAII socket and use it instead of bare SOCKET (vasild)\n- - bitcoin/bitcoin#20791 remove unused legacyWhitelisted in AcceptConnection() (jonatack)\n- - bitcoin/bitcoin#20816 Move RecordBytesSent() call out of `cs_vSend` lock (jnewbery)\n- - bitcoin/bitcoin#20845 Log to net debug in MaybeDiscourageAndDisconnect except for noban and manual peers (MarcoFalke)\n- - bitcoin/bitcoin#20864 Move SocketSendData lock annotation to header (MarcoFalke)\n- - bitcoin/bitcoin#20965 net, rpc:  return `NET_UNROUTABLE` as `not_publicly_routable`, automate helps (jonatack)\n- - bitcoin/bitcoin#20966 banman: save the banlist in a JSON format on disk (vasild)\n- - bitcoin/bitcoin#21015 Make all of `net_processing` (and some of net) use std::chrono types (dhruv)\n- - bitcoin/bitcoin#21029 bitcoin-cli: Correct docs (no \"generatenewaddress\" exists) (luke-jr)\n- - bitcoin/bitcoin#21148 Split orphan handling from `net_processing` into txorphanage (ajtowns)\n- - bitcoin/bitcoin#21162 Net Processing: Move RelayTransaction() into PeerManager (jnewbery)\n- - bitcoin/bitcoin#21167 make `CNode::m_inbound_onion` public, initialize explicitly (jonatack)\n- - bitcoin/bitcoin#21186 net/net processing: Move addr data into `net_processing` (jnewbery)\n- - bitcoin/bitcoin#21187 Net processing: Only call PushAddress() from `net_processing` (jnewbery)\n- - bitcoin/bitcoin#21198 Address outstanding review comments from PR20721 (jnewbery)\n- - bitcoin/bitcoin#21222 log: Clarify log message when file does not exist (MarcoFalke)\n- - bitcoin/bitcoin#21235 Clarify disconnect log message in ProcessGetBlockData, remove send bool (MarcoFalke)\n- - bitcoin/bitcoin#21236 Net processing: Extract `addr` send functionality into MaybeSendAddr() (jnewbery)\n- - bitcoin/bitcoin#21261 update inbound eviction protection for multiple networks, add I2P peers (jonatack)\n- - bitcoin/bitcoin#21328 net, refactor: pass uint16 CService::port as uint16 (jonatack)\n- - bitcoin/bitcoin#21387 Refactor sock to add I2P fuzz and unit tests (vasild)\n- - bitcoin/bitcoin#21395 Net processing: Remove unused CNodeState.address member (jnewbery)\n- - bitcoin/bitcoin#21407 i2p: limit the size of incoming messages (vasild)\n- - bitcoin/bitcoin#21506 p2p, refactor: make NetPermissionFlags an enum class (jonatack)\n- - bitcoin/bitcoin#21509 Don't send FEEFILTER in blocksonly mode (mzumsande)\n- - bitcoin/bitcoin#21560 Add Tor v3 hardcoded seeds (laanwj)\n- - bitcoin/bitcoin#21563 Restrict period when `cs_vNodes` mutex is locked (hebasto)\n- - bitcoin/bitcoin#21564 Avoid calling getnameinfo when formatting IPv4 addresses in CNetAddr::ToStringIP (practicalswift)\n- - bitcoin/bitcoin#21631 i2p: always check the return value of Sock::Wait() (vasild)\n- - bitcoin/bitcoin#21644 p2p, bugfix: use NetPermissions::HasFlag() in CConnman::Bind() (jonatack)\n- - bitcoin/bitcoin#21659 flag relevant Sock methods with [[nodiscard]] (vasild)\n- - bitcoin/bitcoin#21750 remove unnecessary check of `CNode::cs_vSend` (vasild)\n- - bitcoin/bitcoin#21756 Avoid calling `getnameinfo` when formatting IPv6 addresses in `CNetAddr::ToStringIP` (practicalswift)\n- - bitcoin/bitcoin#21775 Limit `m_block_inv_mutex` (MarcoFalke)\n- - bitcoin/bitcoin#21825 Add I2P hardcoded seeds (jonatack)\n- - bitcoin/bitcoin#21843 p2p, rpc: enable GetAddr, GetAddresses, and getnodeaddresses by network (jonatack)\n- - bitcoin/bitcoin#21845 net processing: Don't require locking `cs_main` before calling RelayTransactions() (jnewbery)\n- - bitcoin/bitcoin#21872 Sanitize message type for logging (laanwj)\n- - bitcoin/bitcoin#21914 Use stronger AddLocal() for our I2P address (vasild)\n- - bitcoin/bitcoin#21985 Return IPv6 scope id in `CNetAddr::ToStringIP()` (laanwj)\n- - bitcoin/bitcoin#21992 Remove -feefilter option (amadeuszpawlik)\n- - bitcoin/bitcoin#21996 Pass strings to NetPermissions::TryParse functions by const ref (jonatack)\n- - bitcoin/bitcoin#22013 ignore block-relay-only peers when skipping DNS seed (ajtowns)\n- - bitcoin/bitcoin#22050 Remove tor v2 support (jonatack)\n- - bitcoin/bitcoin#22096 AddrFetch - don't disconnect on self-announcements (mzumsande)\n- - bitcoin/bitcoin#22141 net processing: Remove hash and fValidatedHeaders from QueuedBlock (jnewbery)\n- - bitcoin/bitcoin#22144 Randomize message processing peer order (sipa)\n- - bitcoin/bitcoin#22147 Protect last outbound HB compact block peer (sdaftuar)\n- - bitcoin/bitcoin#22179 Torv2 removal followups (vasild)\n- - bitcoin/bitcoin#22211 Relay I2P addresses even if not reachable (by us) (vasild)\n- - bitcoin/bitcoin#22284 Performance improvements to ProtectEvictionCandidatesByRatio() (jonatack)\n- - bitcoin/bitcoin#22387 Rate limit the processing of rumoured addresses (sipa)\n- - bitcoin/bitcoin#22455 addrman: detect on-disk corrupted nNew and nTried during unserialization (vasild)\n\n### Wallet\n- - bitcoin/bitcoin#15710 Catch `ios_base::failure` specifically (Bushstar)\n- - bitcoin/bitcoin#16546 External signer support - Wallet Box edition (Sjors)\n- - bitcoin/bitcoin#17331 Use effective values throughout coin selection (achow101)\n- - bitcoin/bitcoin#18418 Increase `OUTPUT_GROUP_MAX_ENTRIES` to 100 (fjahr)\n- - bitcoin/bitcoin#18842 Mark replaced tx to not be in the mempool anymore (MarcoFalke)\n- - bitcoin/bitcoin#19136 Add `parent_desc` to `getaddressinfo` (achow101)\n- - bitcoin/bitcoin#19137 wallettool: Add dump and createfromdump commands (achow101)\n- - bitcoin/bitcoin#19651 `importdescriptor`s update existing (S3RK)\n- - bitcoin/bitcoin#20040 Refactor OutputGroups to handle fees and spending eligibility on grouping (achow101)\n- - bitcoin/bitcoin#20202 Make BDB support optional (achow101)\n- - bitcoin/bitcoin#20226, bitcoin/bitcoin#21277, - bitcoin/bitcoin#21063 Add `listdescriptors` command (S3RK)\n- - bitcoin/bitcoin#20267 Disable and fix tests for when BDB is not compiled (achow101)\n- - bitcoin/bitcoin#20275 List all wallets in non-SQLite and non-BDB builds (ryanofsky)\n- - bitcoin/bitcoin#20365 wallettool: Add parameter to create descriptors wallet (S3RK)\n- - bitcoin/bitcoin#20403 `upgradewallet` fixes, improvements, test coverage (jonatack)\n- - bitcoin/bitcoin#20448 `unloadwallet`: Allow specifying `wallet_name` param matching RPC endpoint wallet (luke-jr)\n- - bitcoin/bitcoin#20536 Error with \"Transaction too large\" if the funded tx will end up being too large after signing (achow101)\n- - bitcoin/bitcoin#20687 Add missing check for -descriptors wallet tool option (MarcoFalke)\n- - bitcoin/bitcoin#20952 Add BerkeleyDB version sanity check at init time (laanwj)\n- - bitcoin/bitcoin#21127 Load flags before everything else (Sjors)\n- - bitcoin/bitcoin#21141 Add new format string placeholders for walletnotify (maayank)\n- - bitcoin/bitcoin#21238 A few descriptor improvements to prepare for Taproot support (sipa)\n- - bitcoin/bitcoin#21302 `createwallet` examples for descriptor wallets (S3RK)\n- - bitcoin/bitcoin#21329 descriptor wallet: Cache last hardened xpub and use in normalized descriptors (achow101)\n- - bitcoin/bitcoin#21365 Basic Taproot signing support for descriptor wallets (sipa)\n- - bitcoin/bitcoin#21417 Misc external signer improvement and HWI 2 support (Sjors)\n- - bitcoin/bitcoin#21467 Move external signer out of wallet module (Sjors)\n- - bitcoin/bitcoin#21572 Fix wrong wallet RPC context set after #21366 (ryanofsky)\n- - bitcoin/bitcoin#21574 Drop JSONRPCRequest constructors after #21366 (ryanofsky)\n- - bitcoin/bitcoin#21666 Miscellaneous external signer changes (fanquake)\n- - bitcoin/bitcoin#21759 Document coin selection code (glozow)\n- - bitcoin/bitcoin#21786 Ensure sat/vB feerates are in range (mantissa of 3) (jonatack)\n- - bitcoin/bitcoin#21944 Fix issues when `walletdir` is root directory (prayank23)\n- - bitcoin/bitcoin#22042 Replace size/weight estimate tuple with struct for named fields (instagibbs)\n- - bitcoin/bitcoin#22051 Basic Taproot derivation support for descriptors (sipa)\n- - bitcoin/bitcoin#22154 Add OutputType::BECH32M and related wallet support for fetching bech32m addresses (achow101)\n- - bitcoin/bitcoin#22156 Allow tr() import only when Taproot is active (achow101)\n- - bitcoin/bitcoin#22166 Add support for inferring tr() descriptors (sipa)\n- - bitcoin/bitcoin#22173 Do not load external signers wallets when unsupported (achow101)\n- - bitcoin/bitcoin#22308 Add missing BlockUntilSyncedToCurrentChain (MarcoFalke)\n- - bitcoin/bitcoin#22334 Do not spam about non-existent spk managers (S3RK)\n- - bitcoin/bitcoin#22379 Erase spkmans rather than setting to nullptr (achow101)\n- - bitcoin/bitcoin#22421 Make IsSegWitOutput return true for taproot outputs (sipa)\n- - bitcoin/bitcoin#22461 Change ScriptPubKeyMan::Upgrade default to True (achow101)\n- - bitcoin/bitcoin#22492 Reorder locks in dumpwallet to avoid lock order assertion (achow101)\n- - bitcoin/bitcoin#22686 Use GetSelectionAmount in ApproximateBestSubset (achow101)\n\n### RPC and other APIs\n- - bitcoin/bitcoin#18335, bitcoin/bitcoin#21484 cli: Print useful error if bitcoind rpc work queue exceeded (LarryRuane)\n- - bitcoin/bitcoin#18466 Fix invalid parameter error codes for `{sign,verify}message` RPCs (theStack)\n- - bitcoin/bitcoin#18772 Calculate fees in `getblock` using BlockUndo data (robot-visions)\n- - bitcoin/bitcoin#19033 http: Release work queue after event base finish (promag)\n- - bitcoin/bitcoin#19055 Add MuHash3072 implementation (fjahr)\n- - bitcoin/bitcoin#19145 Add `hash_type` MUHASH for gettxoutsetinfo (fjahr)\n- - bitcoin/bitcoin#19847 Avoid duplicate set lookup in `gettxoutproof` (promag)\n- - bitcoin/bitcoin#20286 Deprecate `addresses` and `reqSigs` from RPC outputs (mjdietzx)\n- - bitcoin/bitcoin#20459 Fail to return undocumented return values (MarcoFalke)\n- - bitcoin/bitcoin#20461 Validate `-rpcauth` arguments (promag)\n- - bitcoin/bitcoin#20556 Properly document return values (`submitblock`, `gettxout`, `getblocktemplate`, `scantxoutset`) (MarcoFalke)\n- - bitcoin/bitcoin#20755 Remove deprecated fields from `getpeerinfo` (amitiuttarwar)\n- - bitcoin/bitcoin#20832 Better error messages for invalid addresses (eilx2)\n- - bitcoin/bitcoin#20867 Support up to 20 keys for multisig under Segwit context (darosior)\n- - bitcoin/bitcoin#20877 cli: `-netinfo` user help and argument parsing improvements (jonatack)\n- - bitcoin/bitcoin#20891 Remove deprecated bumpfee behavior (achow101)\n- - bitcoin/bitcoin#20916 Return wtxid from `testmempoolaccept` (MarcoFalke)\n- - bitcoin/bitcoin#20917 Add missing signet mentions in network name lists (theStack)\n- - bitcoin/bitcoin#20941 Document `RPC_TRANSACTION_ALREADY_IN_CHAIN` exception (jarolrod)\n- - bitcoin/bitcoin#20944 Return total fee in `getmempoolinfo` (MarcoFalke)\n- - bitcoin/bitcoin#20964 Add specific error code for \"wallet already loaded\" (laanwj)\n- - bitcoin/bitcoin#21053 Document {previous,next}blockhash as optional (theStack)\n- - bitcoin/bitcoin#21056 Add a `-rpcwaittimeout` parameter to limit time spent waiting (cdecker)\n- - bitcoin/bitcoin#21192 cli: Treat high detail levels as maximum in `-netinfo` (laanwj)\n- - bitcoin/bitcoin#21311 Document optional fields for `getchaintxstats` result (theStack)\n- - bitcoin/bitcoin#21359 `include_unsafe` option for fundrawtransaction (t-bast)\n- - bitcoin/bitcoin#21426 Remove `scantxoutset` EXPERIMENTAL warning (jonatack)\n- - bitcoin/bitcoin#21544 Missing doc updates for bumpfee psbt update (MarcoFalke)\n- - bitcoin/bitcoin#21594 Add `network` field to `getnodeaddresses` (jonatack)\n- - bitcoin/bitcoin#21595, bitcoin/bitcoin#21753 cli: Create `-addrinfo` (jonatack)\n- - bitcoin/bitcoin#21602 Add additional ban time fields to `listbanned` (jarolrod)\n- - bitcoin/bitcoin#21679 Keep default argument value in correct type (promag)\n- - bitcoin/bitcoin#21718 Improve error message for `getblock` invalid datatype (klementtan)\n- - bitcoin/bitcoin#21913 RPCHelpMan fixes (kallewoof)\n- - bitcoin/bitcoin#22021 `bumpfee`/`psbtbumpfee` fixes and updates (jonatack)\n- - bitcoin/bitcoin#22043 `addpeeraddress` test coverage, code simplify/constness (jonatack)\n- - bitcoin/bitcoin#22327 cli: Avoid truncating `-rpcwaittimeout` (MarcoFalke)\n\n### GUI\n- - bitcoin/bitcoin#18948 Call setParent() in the parent's context (hebasto)\n- - bitcoin/bitcoin#20482 Add depends qt fix for ARM macs (jonasschnelli)\n- - bitcoin/bitcoin#21836 scripted-diff: Replace three dots with ellipsis in the ui strings (hebasto)\n- - bitcoin/bitcoin#21935 Enable external signer support for GUI builds (Sjors)\n- - bitcoin/bitcoin#22133 Make QWindowsVistaStylePlugin available again (regression) (hebasto)\n- - bitcoin-core/gui#4 UI external signer support (e.g. hardware wallet) (Sjors)\n- - bitcoin-core/gui#13 Hide peer detail view if multiple are selected (promag)\n- - bitcoin-core/gui#18 Add peertablesortproxy module (hebasto)\n- - bitcoin-core/gui#21 Improve pruning tooltip (fluffypony, BitcoinErrorLog)\n- - bitcoin-core/gui#72 Log static plugins meta data and used style (hebasto)\n- - bitcoin-core/gui#79 Embed monospaced font (hebasto)\n- - bitcoin-core/gui#85 Remove unused \"What's This\" button in dialogs on Windows OS (hebasto)\n- - bitcoin-core/gui#115 Replace \"Hide tray icon\" option with positive \"Show tray icon\" one (hebasto)\n- - bitcoin-core/gui#118 Remove BDB version from the Information tab (hebasto)\n- - bitcoin-core/gui#121 Early subscribe core signals in transaction table model (promag)\n- - bitcoin-core/gui#123 Do not accept command while executing another one (hebasto)\n- - bitcoin-core/gui#125 Enable changing the autoprune block space size in intro dialog (luke-jr)\n- - bitcoin-core/gui#138 Unlock encrypted wallet \"OK\" button bugfix (mjdietzx)\n- - bitcoin-core/gui#139 doc: Improve gui/src/qt README.md (jarolrod)\n- - bitcoin-core/gui#154 Support macOS Dark mode (goums, Uplab)\n- - bitcoin-core/gui#162 Add network to peers window and peer details (jonatack)\n- - bitcoin-core/gui#163, bitcoin-core/gui#180 Peer details: replace Direction with Connection Type (jonatack)\n- - bitcoin-core/gui#164 Handle peer addition/removal in a right way (hebasto)\n- - bitcoin-core/gui#165 Save QSplitter state in QSettings (hebasto)\n- - bitcoin-core/gui#173 Follow Qt docs when implementing rowCount and columnCount (hebasto)\n- - bitcoin-core/gui#179 Add Type column to peers window, update peer details name/tooltip (jonatack)\n- - bitcoin-core/gui#186 Add information to \"Confirm fee bump\" window (prayank23)\n- - bitcoin-core/gui#189 Drop workaround for QTBUG-42503 which was fixed in Qt 5.5.0 (prusnak)\n- - bitcoin-core/gui#194 Save/restore RPCConsole geometry only for window (hebasto)\n- - bitcoin-core/gui#202 Fix right panel toggle in peers tab (RandyMcMillan)\n- - bitcoin-core/gui#203 Display plain \"Inbound\" in peer details (jonatack)\n- - bitcoin-core/gui#204 Drop buggy TableViewLastColumnResizingFixer class (hebasto)\n- - bitcoin-core/gui#205, bitcoin-core/gui#229 Save/restore TransactionView and recentRequestsView tables column sizes (hebasto)\n- - bitcoin-core/gui#206 Display fRelayTxes and `bip152_highbandwidth_{to, from}` in peer details (jonatack)\n- - bitcoin-core/gui#213 Add Copy Address Action to Payment Requests (jarolrod)\n- - bitcoin-core/gui#214 Disable requests context menu actions when appropriate (jarolrod)\n- - bitcoin-core/gui#217 Make warning label look clickable (jarolrod)\n- - bitcoin-core/gui#219 Prevent the main window popup menu (hebasto)\n- - bitcoin-core/gui#220 Do not translate file extensions (hebasto)\n- - bitcoin-core/gui#221 RPCConsole translatable string fixes and improvements (jonatack)\n- - bitcoin-core/gui#226 Add \"Last Block\" and \"Last Tx\" rows to peer details area (jonatack)\n- - bitcoin-core/gui#233 qt test: Don't bind to regtest port (achow101)\n- - bitcoin-core/gui#243 Fix issue when disabling the auto-enabled blank wallet checkbox (jarolrod)\n- - bitcoin-core/gui#246 Revert \"qt: Use \"fusion\" style on macOS Big Sur with old Qt\" (hebasto)\n- - bitcoin-core/gui#248 For values of \"Bytes transferred\" and \"Bytes/s\" with 1000-based prefix names use 1000-based divisor instead of 1024-based (wodry)\n- - bitcoin-core/gui#251 Improve URI/file handling message (hebasto)\n- - bitcoin-core/gui#256 Save/restore column sizes of the tables in the Peers tab (hebasto)\n- - bitcoin-core/gui#260 Handle exceptions isntead of crash (hebasto)\n- - bitcoin-core/gui#263 Revamp context menus (hebasto)\n- - bitcoin-core/gui#271 Don't clear console prompt when font resizing (jarolrod)\n- - bitcoin-core/gui#275 Support runtime appearance adjustment on macOS (hebasto)\n- - bitcoin-core/gui#276 Elide long strings in their middle in the Peers tab (hebasto)\n- - bitcoin-core/gui#281 Set shortcuts for console's resize buttons (jarolrod)\n- - bitcoin-core/gui#293 Enable wordWrap for Services (RandyMcMillan)\n- - bitcoin-core/gui#296 Do not use QObject::tr plural syntax for numbers with a unit symbol (hebasto)\n- - bitcoin-core/gui#297 Avoid unnecessary translations (hebasto)\n- - bitcoin-core/gui#298 Peertableview alternating row colors (RandyMcMillan)\n- - bitcoin-core/gui#300 Remove progress bar on modal overlay (brunoerg)\n- - bitcoin-core/gui#309 Add access to the Peers tab from the network icon (hebasto)\n- - bitcoin-core/gui#311 Peers Window rename 'Peer id' to 'Peer' (jarolrod)\n- - bitcoin-core/gui#313 Optimize string concatenation by default (hebasto)\n- - bitcoin-core/gui#325 Align numbers in the \"Peer Id\" column to the right (hebasto)\n- - bitcoin-core/gui#329 Make console buttons look clickable (jarolrod)\n- - bitcoin-core/gui#330 Allow prompt icon to be colorized (jarolrod)\n- - bitcoin-core/gui#331 Make RPC console welcome message translation-friendly (hebasto)\n- - bitcoin-core/gui#332 Replace disambiguation strings with translator comments (hebasto)\n- - bitcoin-core/gui#335 test: Use QSignalSpy instead of QEventLoop (jarolrod)\n- - bitcoin-core/gui#343 Improve the GUI responsiveness when progress dialogs are used (hebasto)\n- - bitcoin-core/gui#361 Fix GUI segfault caused by bitcoin/bitcoin#22216 (ryanofsky)\n- - bitcoin-core/gui#362 Add keyboard shortcuts to context menus (luke-jr)\n- - bitcoin-core/gui#366 Dark Mode fixes/portability (luke-jr)\n- - bitcoin-core/gui#375 Emit dataChanged signal to dynamically re-sort Peers table (hebasto)\n- - bitcoin-core/gui#393 Fix regression in \"Encrypt Wallet\" menu item (hebasto)\n- - bitcoin-core/gui#396 Ensure external signer option remains disabled without signers (achow101)\n- - bitcoin-core/gui#406 Handle new added plurals in `bitcoin_en.ts` (hebasto)\n\n### Build system\n- - bitcoin/bitcoin#17227 Add Android packaging support (icota)\n- - bitcoin/bitcoin#17920 guix: Build support for macOS (dongcarl)\n- - bitcoin/bitcoin#18298 Fix Qt processing of configure script for depends with DEBUG=1 (hebasto)\n- - bitcoin/bitcoin#19160 multiprocess: Add basic spawn and IPC support (ryanofsky)\n- - bitcoin/bitcoin#19504 Bump minimum python version to 3.6 (ajtowns)\n- - bitcoin/bitcoin#19522 fix building libconsensus with reduced exports for Darwin targets (fanquake)\n- - bitcoin/bitcoin#19683 Pin clang search paths for darwin host (dongcarl)\n- - bitcoin/bitcoin#19764 Split boost into build/host packages + bump + cleanup (dongcarl)\n- - bitcoin/bitcoin#19817 libtapi 1100.0.11 (fanquake)\n- - bitcoin/bitcoin#19846 enable unused member function diagnostic (Zero-1729)\n- - bitcoin/bitcoin#19867 Document and cleanup Qt hacks (fanquake)\n- - bitcoin/bitcoin#20046 Set `CMAKE_INSTALL_RPATH` for native packages (ryanofsky)\n- - bitcoin/bitcoin#20223 Drop the leading 0 from the version number (achow101)\n- - bitcoin/bitcoin#20333 Remove `native_biplist` dependency (fanquake)\n- - bitcoin/bitcoin#20353 configure: Support -fdebug-prefix-map and -fmacro-prefix-map (ajtowns)\n- - bitcoin/bitcoin#20359 Various config.site.in improvements and linting (dongcarl)\n- - bitcoin/bitcoin#20413 Require C++17 compiler (MarcoFalke)\n- - bitcoin/bitcoin#20419 Set minimum supported macOS to 10.14 (fanquake)\n- - bitcoin/bitcoin#20421 miniupnpc 2.2.2 (fanquake)\n- - bitcoin/bitcoin#20422 Mac deployment unification (fanquake)\n- - bitcoin/bitcoin#20424 Update univalue subtree (MarcoFalke)\n- - bitcoin/bitcoin#20449 Fix Windows installer build (achow101)\n- - bitcoin/bitcoin#20468 Warn when generating man pages for binaries built from a dirty branch (tylerchambers)\n- - bitcoin/bitcoin#20469 Avoid secp256k1.h include from system (dergoegge)\n- - bitcoin/bitcoin#20470 Replace genisoimage with xorriso (dongcarl)\n- - bitcoin/bitcoin#20471 Use C++17 in depends (fanquake)\n- - bitcoin/bitcoin#20496 Drop unneeded macOS framework dependencies (hebasto)\n- - bitcoin/bitcoin#20520 Do not force Precompiled Headers (PCH) for building Qt on Linux (hebasto)\n- - bitcoin/bitcoin#20549 Support make src/bitcoin-node and src/bitcoin-gui (promag)\n- - bitcoin/bitcoin#20565 Ensure PIC build for bdb on Android (BlockMechanic)\n- - bitcoin/bitcoin#20594 Fix getauxval calls in randomenv.cpp (jonasschnelli)\n- - bitcoin/bitcoin#20603 Update crc32c subtree (MarcoFalke)\n- - bitcoin/bitcoin#20609 configure: output notice that test binary is disabled by fuzzing (apoelstra)\n- - bitcoin/bitcoin#20619 guix: Quality of life improvements (dongcarl)\n- - bitcoin/bitcoin#20629 Improve id string robustness (dongcarl)\n- - bitcoin/bitcoin#20641 Use Qt top-level build facilities (hebasto)\n- - bitcoin/bitcoin#20650 Drop workaround for a fixed bug in Qt build system (hebasto)\n- - bitcoin/bitcoin#20673 Use more legible qmake commands in qt package (hebasto)\n- - bitcoin/bitcoin#20684 Define .INTERMEDIATE target once only (hebasto)\n- - bitcoin/bitcoin#20720 more robustly check for fcf-protection support (fanquake)\n- - bitcoin/bitcoin#20734 Make platform-specific targets available for proper platform builds only (hebasto)\n- - bitcoin/bitcoin#20936 build fuzz tests by default (danben)\n- - bitcoin/bitcoin#20937 guix: Make nsis reproducible by respecting SOURCE-DATE-EPOCH (dongcarl)\n- - bitcoin/bitcoin#20938 fix linking against -latomic when building for riscv (fanquake)\n- - bitcoin/bitcoin#20939 fix `RELOC_SECTION` security check for bitcoin-util (fanquake)\n- - bitcoin/bitcoin#20963 gitian-linux: Build binaries for 64-bit POWER (continued) (laanwj)\n- - bitcoin/bitcoin#21036 gitian: Bump descriptors to focal for 22.0 (fanquake)\n- - bitcoin/bitcoin#21045 Adds switch to enable/disable randomized base address in MSVC builds (EthanHeilman)\n- - bitcoin/bitcoin#21065 make macOS HOST in download-osx generic (fanquake)\n- - bitcoin/bitcoin#21078 guix: only download sources for hosts being built (fanquake)\n- - bitcoin/bitcoin#21116 Disable --disable-fuzz-binary for gitian/guix builds (hebasto)\n- - bitcoin/bitcoin#21182 remove mostly pointless `BOOST_PROCESS` macro (fanquake)\n- - bitcoin/bitcoin#21205 actually fail when Boost is missing (fanquake)\n- - bitcoin/bitcoin#21209 use newer source for libnatpmp (fanquake)\n- - bitcoin/bitcoin#21226 Fix fuzz binary compilation under windows (danben)\n- - bitcoin/bitcoin#21231 Add /opt/homebrew to path to look for boost libraries (fyquah)\n- - bitcoin/bitcoin#21239 guix: Add codesignature attachment support for osx+win (dongcarl)\n- - bitcoin/bitcoin#21250 Make `HAVE_O_CLOEXEC` available outside LevelDB (bugfix) (theStack)\n- - bitcoin/bitcoin#21272 guix: Passthrough `SDK_PATH` into container (dongcarl)\n- - bitcoin/bitcoin#21274 assumptions:  Assume C++17 (fanquake)\n- - bitcoin/bitcoin#21286 Bump minimum Qt version to 5.9.5 (hebasto)\n- - bitcoin/bitcoin#21298 guix: Bump time-machine, glibc, and linux-headers (dongcarl)\n- - bitcoin/bitcoin#21304 guix: Add guix-clean script + establish gc-root for container profiles (dongcarl)\n- - bitcoin/bitcoin#21320 fix libnatpmp macos cross compile (fanquake)\n- - bitcoin/bitcoin#21321 guix: Add curl to required tool list (hebasto)\n- - bitcoin/bitcoin#21333 set Unicode true for NSIS installer (fanquake)\n- - bitcoin/bitcoin#21339 Make `AM_CONDITIONAL([ENABLE_EXTERNAL_SIGNER])` unconditional (hebasto)\n- - bitcoin/bitcoin#21349 Fix fuzz-cuckoocache cross-compiling with DEBUG=1 (hebasto)\n- - bitcoin/bitcoin#21354 build, doc: Drop no longer required packages from macOS cross-compiling dependencies (hebasto)\n- - bitcoin/bitcoin#21363 build, qt: Improve Qt static plugins/libs check code (hebasto)\n- - bitcoin/bitcoin#21375 guix: Misc feedback-based fixes + hier restructuring (dongcarl)\n- - bitcoin/bitcoin#21376 Qt 5.12.10 (fanquake)\n- - bitcoin/bitcoin#21382 Clean remnants of QTBUG-34748 fix (hebasto)\n- - bitcoin/bitcoin#21400 Fix regression introduced in #21363 (hebasto)\n- - bitcoin/bitcoin#21403 set --build when configuring packages in depends (fanquake)\n- - bitcoin/bitcoin#21421 don't try and use -fstack-clash-protection on Windows (fanquake)\n- - bitcoin/bitcoin#21423 Cleanups and follow ups after bumping Qt to 5.12.10 (hebasto)\n- - bitcoin/bitcoin#21427 Fix `id_string` invocations (dongcarl)\n- - bitcoin/bitcoin#21430 Add -Werror=implicit-fallthrough compile flag (hebasto)\n- - bitcoin/bitcoin#21457 Split libtapi and clang out of `native_cctools` (fanquake)\n- - bitcoin/bitcoin#21462 guix: Add guix-{attest,verify} scripts (dongcarl)\n- - bitcoin/bitcoin#21495 build, qt: Fix static builds on macOS Big Sur (hebasto)\n- - bitcoin/bitcoin#21497 Do not opt-in unused CoreWLAN stuff in depends for macOS (hebasto)\n- - bitcoin/bitcoin#21543 Enable safe warnings for msvc builds (hebasto)\n- - bitcoin/bitcoin#21565 Make `bitcoin_qt.m4` more generic (fanquake)\n- - bitcoin/bitcoin#21610 remove -Wdeprecated-register from NOWARN flags (fanquake)\n- - bitcoin/bitcoin#21613 enable -Wdocumentation (fanquake)\n- - bitcoin/bitcoin#21629 Fix configuring when building depends with `NO_BDB=1` (fanquake)\n- - bitcoin/bitcoin#21654 build, qt: Make Qt rcc output always deterministic (hebasto)\n- - bitcoin/bitcoin#21655 build, qt: No longer need to set `QT_RCC_TEST=1` for determinism (hebasto)\n- - bitcoin/bitcoin#21658 fix make deploy for arm64-darwin (sgulls)\n- - bitcoin/bitcoin#21694 Use XLIFF file to provide more context to Transifex translators (hebasto)\n- - bitcoin/bitcoin#21708, bitcoin/bitcoin#21593 Drop pointless sed commands (hebasto)\n- - bitcoin/bitcoin#21731 Update msvc build to use Qt5.12.10 binaries (sipsorcery)\n- - bitcoin/bitcoin#21733 Re-add command to install vcpkg (dplusplus1024)\n- - bitcoin/bitcoin#21793 Use `-isysroot` over `--sysroot` on macOS (fanquake)\n- - bitcoin/bitcoin#21869 Add missing `-D_LIBCPP_DEBUG=1` to debug flags (MarcoFalke)\n- - bitcoin/bitcoin#21889 macho: check for control flow instrumentation (fanquake)\n- - bitcoin/bitcoin#21920 Improve macro for testing -latomic requirement (MarcoFalke)\n- - bitcoin/bitcoin#21991 libevent 2.1.12-stable (fanquake)\n- - bitcoin/bitcoin#22054 Bump Qt version to 5.12.11 (hebasto)\n- - bitcoin/bitcoin#22063 Use Qt archive of the same version as the compiled binaries (hebasto)\n- - bitcoin/bitcoin#22070 Don't use cf-protection when targeting arm-apple-darwin (fanquake)\n- - bitcoin/bitcoin#22071 Latest config.guess and config.sub (fanquake)\n- - bitcoin/bitcoin#22075 guix: Misc leftover usability improvements (dongcarl)\n- - bitcoin/bitcoin#22123 Fix qt.mk for mac arm64 (promag)\n- - bitcoin/bitcoin#22174 build, qt: Fix libraries linking order for Linux hosts (hebasto)\n- - bitcoin/bitcoin#22182 guix: Overhaul how guix-{attest,verify} works and hierarchy (dongcarl)\n- - bitcoin/bitcoin#22186 build, qt: Fix compiling qt package in depends with GCC 11 (hebasto)\n- - bitcoin/bitcoin#22199 macdeploy: minor fixups and simplifications (fanquake)\n- - bitcoin/bitcoin#22230 Fix MSVC linker /SubSystem option for bitcoin-qt.exe (hebasto)\n- - bitcoin/bitcoin#22234 Mark print-% target as phony (dgoncharov)\n- - bitcoin/bitcoin#22238 improve detection of eBPF support (fanquake)\n- - bitcoin/bitcoin#22258 Disable deprecated-copy warning only when external warnings are enabled (MarcoFalke)\n- - bitcoin/bitcoin#22320 set minimum required Boost to 1.64.0 (fanquake)\n- - bitcoin/bitcoin#22348 Fix cross build for Windows with Boost Process (hebasto)\n- - bitcoin/bitcoin#22365 guix: Avoid relying on newer symbols by rebasing our cross toolchains on older glibcs (dongcarl)\n- - bitcoin/bitcoin#22381 guix: Test security-check sanity before performing them (with macOS) (fanquake)\n- - bitcoin/bitcoin#22405 Remove --enable-glibc-back-compat from Guix build (fanquake)\n- - bitcoin/bitcoin#22406 Remove --enable-determinism configure option (fanquake)\n- - bitcoin/bitcoin#22410 Avoid GCC 7.1 ABI change warning in guix build (sipa)\n- - bitcoin/bitcoin#22436 use aarch64 Clang if cross-compiling for darwin on aarch64 (fanquake)\n- - bitcoin/bitcoin#22465 guix: Pin kernel-header version, time-machine to upstream 1.3.0 commit (dongcarl)\n- - bitcoin/bitcoin#22511 guix: Silence `getent(1)` invocation, doc fixups (dongcarl)\n- - bitcoin/bitcoin#22531 guix: Fixes to guix-{attest,verify} (achow101)\n- - bitcoin/bitcoin#22642 release: Release with separate sha256sums and sig files (dongcarl)\n- - bitcoin/bitcoin#22685 clientversion: No suffix `#if CLIENT_VERSION_IS_RELEASE` (dongcarl)\n- - bitcoin/bitcoin#22713 Fix build with Boost 1.77.0 (sizeofvoid)\n\n### Tests and QA\n- - bitcoin/bitcoin#14604 Add test and refactor `feature_block.py` (sanket1729)\n- - bitcoin/bitcoin#17556 Change `feature_config_args.py` not to rely on strange regtest=0 behavior (ryanofsky)\n- - bitcoin/bitcoin#18795 wallet issue with orphaned rewards (domob1812)\n- - bitcoin/bitcoin#18847 compressor: Use a prevector in CompressScript serialization (jb55)\n- - bitcoin/bitcoin#19259 fuzz: Add fuzzing harness for LoadMempool(\u2026) and DumpMempool(\u2026) (practicalswift)\n- - bitcoin/bitcoin#19315 Allow outbound & block-relay-only connections in functional tests. (amitiuttarwar)\n- - bitcoin/bitcoin#19698 Apply strict verification flags for transaction tests and assert backwards compatibility (glozow)\n- - bitcoin/bitcoin#19801 Check for all possible `OP_CLTV` fail reasons in `feature_cltv.py` (BIP 65) (theStack)\n- - bitcoin/bitcoin#19893 Remove or explain syncwithvalidationinterfacequeue (MarcoFalke)\n- - bitcoin/bitcoin#19972 fuzz: Add fuzzing harness for node eviction logic (practicalswift)\n- - bitcoin/bitcoin#19982 Fix inconsistent lock order in `wallet_tests/CreateWallet` (hebasto)\n- - bitcoin/bitcoin#20000 Fix creation of \"std::string\"s with \\0s (vasild)\n- - bitcoin/bitcoin#20047 Use `wait_for_{block,header}` helpers in `p2p_fingerprint.py` (theStack)\n- - bitcoin/bitcoin#20171 Add functional test `test_txid_inv_delay` (ariard)\n- - bitcoin/bitcoin#20189 Switch to BIP341's suggested scheme for outputs without script (sipa)\n- - bitcoin/bitcoin#20248 Fix length of R check in `key_signature_tests` (dgpv)\n- - bitcoin/bitcoin#20276, bitcoin/bitcoin#20385, bitcoin/bitcoin#20688, bitcoin/bitcoin#20692 Run various mempool tests even with wallet disabled (mjdietzx)\n- - bitcoin/bitcoin#20323 Create or use existing properly initialized NodeContexts (dongcarl)\n- - bitcoin/bitcoin#20354 Add `feature_taproot.py --previous_release` (MarcoFalke)\n- - bitcoin/bitcoin#20370 fuzz: Version handshake (MarcoFalke)\n- - bitcoin/bitcoin#20377 fuzz: Fill various small fuzzing gaps (practicalswift)\n- - bitcoin/bitcoin#20425 fuzz: Make CAddrMan fuzzing harness deterministic (practicalswift)\n- - bitcoin/bitcoin#20430 Sanitizers: Add suppression for unsigned-integer-overflow in libstdc++ (jonasschnelli)\n- - bitcoin/bitcoin#20437 fuzz: Avoid time-based \"non-determinism\" in fuzzing harnesses by using mocked GetTime() (practicalswift)\n- - bitcoin/bitcoin#20458 Add `is_bdb_compiled` helper (Sjors)\n- - bitcoin/bitcoin#20466 Fix intermittent `p2p_fingerprint` issue (MarcoFalke)\n- - bitcoin/bitcoin#20472 Add testing of ParseInt/ParseUInt edge cases with leading +/-/0:s (practicalswift)\n- - bitcoin/bitcoin#20507 sync: print proper lock order location when double lock is detected (vasild)\n- - bitcoin/bitcoin#20522 Fix sync issue in `disconnect_p2ps` (amitiuttarwar)\n- - bitcoin/bitcoin#20524 Move `MIN_VERSION_SUPPORTED` to p2p.py (jnewbery)\n- - bitcoin/bitcoin#20540 Fix `wallet_multiwallet` issue on windows (MarcoFalke)\n- - bitcoin/bitcoin#20560 fuzz: Link all targets once (MarcoFalke)\n- - bitcoin/bitcoin#20567 Add option to git-subtree-check to do full check, add help (laanwj)\n- - bitcoin/bitcoin#20569 Fix intermittent `wallet_multiwallet` issue with `got_loading_error` (MarcoFalke)\n- - bitcoin/bitcoin#20613 Use Popen.wait instead of RPC in `assert_start_raises_init_error` (MarcoFalke)\n- - bitcoin/bitcoin#20663 fuzz: Hide `script_assets_test_minimizer` (MarcoFalke)\n- - bitcoin/bitcoin#20674 fuzz: Call SendMessages after ProcessMessage to increase coverage (MarcoFalke)\n- - bitcoin/bitcoin#20683 Fix restart node race (MarcoFalke)\n- - bitcoin/bitcoin#20686 fuzz: replace CNode code with fuzz/util.h::ConsumeNode() (jonatack)\n- - bitcoin/bitcoin#20733 Inline non-member functions with body in fuzzing headers (pstratem)\n- - bitcoin/bitcoin#20737 Add missing assignment in `mempool_resurrect.py` (MarcoFalke)\n- - bitcoin/bitcoin#20745 Correct `epoll_ctl` data race suppression (hebasto)\n- - bitcoin/bitcoin#20748 Add race:SendZmqMessage tsan suppression (MarcoFalke)\n- - bitcoin/bitcoin#20760 Set correct nValue for multi-op-return policy check (MarcoFalke)\n- - bitcoin/bitcoin#20761 fuzz: Check that `NULL_DATA` is unspendable (MarcoFalke)\n- - bitcoin/bitcoin#20765 fuzz: Check that certain script TxoutType are nonstandard (mjdietzx)\n- - bitcoin/bitcoin#20772 fuzz: Bolster ExtractDestination(s) checks (mjdietzx)\n- - bitcoin/bitcoin#20789 fuzz: Rework strong and weak net enum fuzzing (MarcoFalke)\n- - bitcoin/bitcoin#20828 fuzz: Introduce CallOneOf helper to replace switch-case (MarcoFalke)\n- - bitcoin/bitcoin#20839 fuzz: Avoid extraneous copy of input data, using Span<> (MarcoFalke)\n- - bitcoin/bitcoin#20844 Add sanitizer suppressions for AMD EPYC CPUs (MarcoFalke)\n- - bitcoin/bitcoin#20857 Update documentation in `feature_csv_activation.py` (PiRK)\n- - bitcoin/bitcoin#20876 Replace getmempoolentry with testmempoolaccept in MiniWallet (MarcoFalke)\n- - bitcoin/bitcoin#20881 fuzz: net permission flags in net processing (MarcoFalke)\n- - bitcoin/bitcoin#20882 fuzz: Add missing muhash registration (MarcoFalke)\n- - bitcoin/bitcoin#20908 fuzz: Use mocktime in `process_message*` fuzz targets (MarcoFalke)\n- - bitcoin/bitcoin#20915 fuzz: Fail if message type is not fuzzed (MarcoFalke)\n- - bitcoin/bitcoin#20946 fuzz: Consolidate fuzzing TestingSetup initialization (dongcarl)\n- - bitcoin/bitcoin#20954 Declare `nodes` type `in test_framework.py` (kiminuo)\n- - bitcoin/bitcoin#20955 Fix `get_previous_releases.py` for aarch64 (MarcoFalke)\n- - bitcoin/bitcoin#20969 check that getblockfilter RPC fails without block filter index (theStack)\n- - bitcoin/bitcoin#20971 Work around libFuzzer deadlock (MarcoFalke)\n- - bitcoin/bitcoin#20993 Store subversion (user agent) as string in `msg_version` (theStack)\n- - bitcoin/bitcoin#20995 fuzz: Avoid initializing version to less than `MIN_PEER_PROTO_VERSION` (MarcoFalke)\n- - bitcoin/bitcoin#20998 Fix BlockToJsonVerbose benchmark (martinus)\n- - bitcoin/bitcoin#21003 Move MakeNoLogFileContext to `libtest_util`, and use it in bench (MarcoFalke)\n- - bitcoin/bitcoin#21008 Fix zmq test flakiness, improve speed (theStack)\n- - bitcoin/bitcoin#21023 fuzz: Disable shuffle when merge=1 (MarcoFalke)\n- - bitcoin/bitcoin#21037 fuzz: Avoid designated initialization (C++20) in fuzz tests (practicalswift)\n- - bitcoin/bitcoin#21042 doc, test: Improve `setup_clean_chain` documentation (fjahr)\n- - bitcoin/bitcoin#21080 fuzz: Configure check for main function (take 2) (MarcoFalke)\n- - bitcoin/bitcoin#21084 Fix timeout decrease in `feature_assumevalid` (brunoerg)\n- - bitcoin/bitcoin#21096 Re-add dead code detection (flack)\n- - bitcoin/bitcoin#21100 Remove unused function `xor_bytes` (theStack)\n- - bitcoin/bitcoin#21115 Fix Windows cross build (hebasto)\n- - bitcoin/bitcoin#21117 Remove `assert_blockchain_height` (MarcoFalke)\n- - bitcoin/bitcoin#21121 Small unit test improvements, including helper to make mempool transaction (amitiuttarwar)\n- - bitcoin/bitcoin#21124 Remove unnecessary assignment in bdb (brunoerg)\n- - bitcoin/bitcoin#21125 Change `BOOST_CHECK` to `BOOST_CHECK_EQUAL` for paths (kiminuo)\n- - bitcoin/bitcoin#21142, bitcoin/bitcoin#21512 fuzz: Add `tx_pool` fuzz target (MarcoFalke)\n- - bitcoin/bitcoin#21165 Use mocktime in `test_seed_peers` (dhruv)\n- - bitcoin/bitcoin#21169 fuzz: Add RPC interface fuzzing. Increase fuzzing coverage from 65% to 70% (practicalswift)\n- - bitcoin/bitcoin#21170 bench: Add benchmark to write json into a string (martinus)\n- - bitcoin/bitcoin#21178 Run `mempool_reorg.py` even with wallet disabled (DariusParvin)\n- - bitcoin/bitcoin#21185 fuzz: Remove expensive and redundant muhash from crypto fuzz target (MarcoFalke)\n- - bitcoin/bitcoin#21200 Speed up `rpc_blockchain.py` by removing miniwallet.generate() (MarcoFalke)\n- - bitcoin/bitcoin#21211 Move `P2WSH_OP_TRUE` to shared test library (MarcoFalke)\n- - bitcoin/bitcoin#21228 Avoid comparision of integers with different signs (jonasschnelli)\n- - bitcoin/bitcoin#21230 Fix `NODE_NETWORK_LIMITED_MIN_BLOCKS` disconnection (MarcoFalke)\n- - bitcoin/bitcoin#21252 Add missing wait for sync to `feature_blockfilterindex_prune` (MarcoFalke)\n- - bitcoin/bitcoin#21254 Avoid connecting to real network when running tests (MarcoFalke)\n- - bitcoin/bitcoin#21264 fuzz: Two scripted diff renames (MarcoFalke)\n- - bitcoin/bitcoin#21280 Bug fix in `transaction_tests` (glozow)\n- - bitcoin/bitcoin#21293 Replace accidentally placed bit-OR with logical-OR (hebasto)\n- - bitcoin/bitcoin#21297 `feature_blockfilterindex_prune.py` improvements (jonatack)\n- - bitcoin/bitcoin#21310 zmq test: fix sync-up by matching notification to generated block (theStack)\n- - bitcoin/bitcoin#21334 Additional BIP9 tests (Sjors)\n- - bitcoin/bitcoin#21338 Add functional test for anchors.dat (brunoerg)\n- - bitcoin/bitcoin#21345 Bring `p2p_leak.py` up to date (mzumsande)\n- - bitcoin/bitcoin#21357 Unconditionally check for fRelay field in test framework (jarolrod)\n- - bitcoin/bitcoin#21358 fuzz: Add missing include (`test/util/setup_common.h`) (MarcoFalke)\n- - bitcoin/bitcoin#21371 fuzz: fix gcc Woverloaded-virtual build warnings (jonatack)\n- - bitcoin/bitcoin#21373 Generate fewer blocks in `feature_nulldummy` to fix timeouts, speed up (jonatack)\n- - bitcoin/bitcoin#21390 Test improvements for UTXO set hash tests (fjahr)\n- - bitcoin/bitcoin#21410 increase `rpc_timeout` for fundrawtx `test_transaction_too_large` (jonatack)\n- - bitcoin/bitcoin#21411 add logging, reduce blocks, move `sync_all` in `wallet_` groups (jonatack)\n- - bitcoin/bitcoin#21438 Add ParseUInt8() test coverage (jonatack)\n- - bitcoin/bitcoin#21443 fuzz: Implement `fuzzed_dns_lookup_function` as a lambda (practicalswift)\n- - bitcoin/bitcoin#21445 cirrus: Use SSD cluster for speedup (MarcoFalke)\n- - bitcoin/bitcoin#21477 Add test for CNetAddr::ToString IPv6 address formatting (RFC 5952) (practicalswift)\n- - bitcoin/bitcoin#21487 fuzz: Use ConsumeWeakEnum in addrman for service flags (MarcoFalke)\n- - bitcoin/bitcoin#21488 Add ParseUInt16() unit test and fuzz coverage (jonatack)\n- - bitcoin/bitcoin#21491 test: remove duplicate assertions in util_tests (jonatack)\n- - bitcoin/bitcoin#21522 fuzz: Use PickValue where possible (MarcoFalke)\n- - bitcoin/bitcoin#21531 remove qt byteswap compattests (fanquake)\n- - bitcoin/bitcoin#21557 small cleanup in RPCNestedTests tests (fanquake)\n- - bitcoin/bitcoin#21586 Add missing suppression for signed-integer-overflow:txmempool.cpp (MarcoFalke)\n- - bitcoin/bitcoin#21592 Remove option to make TestChain100Setup non-deterministic (MarcoFalke)\n- - bitcoin/bitcoin#21597 Document `race:validation_chainstatemanager_tests` suppression (MarcoFalke)\n- - bitcoin/bitcoin#21599 Replace file level integer overflow suppression with function level suppression (practicalswift)\n- - bitcoin/bitcoin#21604 Document why no symbol names can be used for suppressions (MarcoFalke)\n- - bitcoin/bitcoin#21606 fuzz: Extend psbt fuzz target a bit (MarcoFalke)\n- - bitcoin/bitcoin#21617 fuzz: Fix uninitialized read in i2p test (MarcoFalke)\n- - bitcoin/bitcoin#21630 fuzz: split FuzzedSock interface and implementation (vasild)\n- - bitcoin/bitcoin#21634 Skip SQLite fsyncs while testing (achow101)\n- - bitcoin/bitcoin#21669 Remove spurious double lock tsan suppressions by bumping to clang-12 (MarcoFalke)\n- - bitcoin/bitcoin#21676 Use mocktime to avoid intermittent failure in `rpc_tests` (MarcoFalke)\n- - bitcoin/bitcoin#21677 fuzz: Avoid use of low file descriptor ids (which may be in use) in FuzzedSock (practicalswift)\n- - bitcoin/bitcoin#21678 Fix TestPotentialDeadLockDetected suppression (hebasto)\n- - bitcoin/bitcoin#21689 Remove intermittently failing and not very meaningful `BOOST_CHECK` in `cnetaddr_basic` (practicalswift)\n- - bitcoin/bitcoin#21691 Check that no versionbits are re-used (MarcoFalke)\n- - bitcoin/bitcoin#21707 Extend functional tests for addr relay (mzumsande)\n- - bitcoin/bitcoin#21712 Test default `include_mempool` value of gettxout (promag)\n- - bitcoin/bitcoin#21738 Use clang-12 for ASAN, Add missing suppression (MarcoFalke)\n- - bitcoin/bitcoin#21740 add new python linter to check file names and permissions (windsok)\n- - bitcoin/bitcoin#21749 Bump shellcheck version (hebasto)\n- - bitcoin/bitcoin#21754 Run `feature_cltv` with MiniWallet (MarcoFalke)\n- - bitcoin/bitcoin#21762 Speed up `mempool_spend_coinbase.py` (MarcoFalke)\n- - bitcoin/bitcoin#21773 fuzz: Ensure prevout is consensus-valid (MarcoFalke)\n- - bitcoin/bitcoin#21777 Fix `feature_notifications.py` intermittent issue (MarcoFalke)\n- - bitcoin/bitcoin#21785 Fix intermittent issue in `p2p_addr_relay.py` (MarcoFalke)\n- - bitcoin/bitcoin#21787 Fix off-by-ones in `rpc_fundrawtransaction` assertions (jonatack)\n- - bitcoin/bitcoin#21792 Fix intermittent issue in `p2p_segwit.py` (MarcoFalke)\n- - bitcoin/bitcoin#21795 fuzz: Terminate immediately if a fuzzing harness tries to perform a DNS lookup (belt and suspenders) (practicalswift)\n- - bitcoin/bitcoin#21798 fuzz: Create a block template in `tx_pool` targets (MarcoFalke)\n- - bitcoin/bitcoin#21804 Speed up `p2p_segwit.py` (jnewbery)\n- - bitcoin/bitcoin#21810 fuzz: Various RPC fuzzer follow-ups (practicalswift)\n- - bitcoin/bitcoin#21814 Fix `feature_config_args.py` intermittent issue (MarcoFalke)\n- - bitcoin/bitcoin#21821 Add missing test for empty P2WSH redeem (MarcoFalke)\n- - bitcoin/bitcoin#21822 Resolve bug in `interface_bitcoin_cli.py` (klementtan)\n- - bitcoin/bitcoin#21846 fuzz: Add `-fsanitize=integer` suppression needed for RPC fuzzer (`generateblock`) (practicalswift)\n- - bitcoin/bitcoin#21849 fuzz: Limit toxic test globals to their respective scope (MarcoFalke)\n- - bitcoin/bitcoin#21867 use MiniWallet for `p2p_blocksonly.py` (theStack)\n- - bitcoin/bitcoin#21873 minor fixes & improvements for files linter test (windsok)\n- - bitcoin/bitcoin#21874 fuzz: Add `WRITE_ALL_FUZZ_TARGETS_AND_ABORT` (MarcoFalke)\n- - bitcoin/bitcoin#21884 fuzz: Remove unused --enable-danger-fuzz-link-all option (MarcoFalke)\n- - bitcoin/bitcoin#21890 fuzz: Limit ParseISO8601DateTime fuzzing to 32-bit (MarcoFalke)\n- - bitcoin/bitcoin#21891 fuzz: Remove strprintf test cases that are known to fail (MarcoFalke)\n- - bitcoin/bitcoin#21892 fuzz: Avoid excessively large min fee rate in `tx_pool` (MarcoFalke)\n- - bitcoin/bitcoin#21895 Add TSA annotations to the WorkQueue class members (hebasto)\n- - bitcoin/bitcoin#21900 use MiniWallet for `feature_csv_activation.py` (theStack)\n- - bitcoin/bitcoin#21909 fuzz: Limit max insertions in timedata fuzz test (MarcoFalke)\n- - bitcoin/bitcoin#21922 fuzz: Avoid timeout in EncodeBase58 (MarcoFalke)\n- - bitcoin/bitcoin#21927 fuzz: Run const CScript member functions only once (MarcoFalke)\n- - bitcoin/bitcoin#21929 fuzz: Remove incorrect float round-trip serialization test (MarcoFalke)\n- - bitcoin/bitcoin#21936 fuzz: Terminate immediately if a fuzzing harness tries to create a TCP socket (belt and suspenders) (practicalswift)\n- - bitcoin/bitcoin#21941 fuzz: Call const member functions in addrman fuzz test only once (MarcoFalke)\n- - bitcoin/bitcoin#21945 add P2PK support to MiniWallet (theStack)\n- - bitcoin/bitcoin#21948 Fix off-by-one in mockscheduler test RPC (MarcoFalke)\n- - bitcoin/bitcoin#21953 fuzz: Add `utxo_snapshot` target (MarcoFalke)\n- - bitcoin/bitcoin#21970 fuzz: Add missing CheckTransaction before CheckTxInputs (MarcoFalke)\n- - bitcoin/bitcoin#21989 Use `COINBASE_MATURITY` in functional tests (kiminuo)\n- - bitcoin/bitcoin#22003 Add thread safety annotations (ajtowns)\n- - bitcoin/bitcoin#22004 fuzz: Speed up transaction fuzz target (MarcoFalke)\n- - bitcoin/bitcoin#22005 fuzz: Speed up banman fuzz target (MarcoFalke)\n- - bitcoin/bitcoin#22029 [fuzz] Improve transport deserialization fuzz test coverage (dhruv)\n- - bitcoin/bitcoin#22048 MiniWallet: introduce enum type for output mode (theStack)\n- - bitcoin/bitcoin#22057 use MiniWallet (P2PK mode) for `feature_dersig.py` (theStack)\n- - bitcoin/bitcoin#22065 Mark `CheckTxInputs` `[[nodiscard]]`. Avoid UUM in fuzzing harness `coins_view` (practicalswift)\n- - bitcoin/bitcoin#22069 fuzz: don't try and use fopencookie() when building for Android (fanquake)\n- - bitcoin/bitcoin#22082 update nanobench from release 4.0.0 to 4.3.4 (martinus)\n- - bitcoin/bitcoin#22086 remove BasicTestingSetup from unit tests that don't need it (fanquake)\n- - bitcoin/bitcoin#22089 MiniWallet: fix fee calculation for P2PK and check tx vsize (theStack)\n- - bitcoin/bitcoin#21107, bitcoin/bitcoin#22092 Convert documentation into type annotations (fanquake)\n- - bitcoin/bitcoin#22095 Additional BIP32 test vector for hardened derivation with leading zeros (kristapsk)\n- - bitcoin/bitcoin#22103 Fix IPv6 check on BSD systems (n-thumann)\n- - bitcoin/bitcoin#22118 check anchors.dat when node starts for the first time (brunoerg)\n- - bitcoin/bitcoin#22120 `p2p_invalid_block`: Check that a block rejected due to too-new tim\u2026 (willcl-ark)\n- - bitcoin/bitcoin#22153 Fix `p2p_leak.py` intermittent failure (mzumsande)\n- - bitcoin/bitcoin#22169 p2p, rpc, fuzz: various tiny follow-ups (jonatack)\n- - bitcoin/bitcoin#22176 Correct outstanding -Werror=sign-compare errors (Empact)\n- - bitcoin/bitcoin#22180 fuzz: Increase branch coverage of the float fuzz target (MarcoFalke)\n- - bitcoin/bitcoin#22187 Add `sync_blocks` in `wallet_orphanedreward.py` (domob1812)\n- - bitcoin/bitcoin#22201 Fix TestShell to allow running in Jupyter Notebook (josibake)\n- - bitcoin/bitcoin#22202 Add temporary coinstats suppressions (MarcoFalke)\n- - bitcoin/bitcoin#22203 Use ConnmanTestMsg from test lib in `denialofservice_tests` (MarcoFalke)\n- - bitcoin/bitcoin#22210 Use MiniWallet in `test_no_inherited_signaling` RBF test (MarcoFalke)\n- - bitcoin/bitcoin#22224 Update msvc and appveyor builds to use Qt5.12.11 binaries (sipsorcery)\n- - bitcoin/bitcoin#22249 Kill process group to avoid dangling processes when using `--failfast` (S3RK)\n- - bitcoin/bitcoin#22267 fuzz: Speed up crypto fuzz target (MarcoFalke)\n- - bitcoin/bitcoin#22270 Add bitcoin-util tests (+refactors) (MarcoFalke)\n- - bitcoin/bitcoin#22271 fuzz: Assert roundtrip equality for `CPubKey` (theStack)\n- - bitcoin/bitcoin#22279 fuzz: add missing ECCVerifyHandle to `base_encode_decode` (apoelstra)\n- - bitcoin/bitcoin#22292 bench, doc: benchmarking updates and fixups (jonatack)\n- - bitcoin/bitcoin#22306 Improvements to `p2p_addr_relay.py` (amitiuttarwar)\n- - bitcoin/bitcoin#22310 Add functional test for replacement relay fee check (ariard)\n- - bitcoin/bitcoin#22311 Add missing syncwithvalidationinterfacequeue in `p2p_blockfilters` (MarcoFalke)\n- - bitcoin/bitcoin#22313 Add missing `sync_all` to `feature_coinstatsindex` (MarcoFalke)\n- - bitcoin/bitcoin#22322 fuzz: Check banman roundtrip (MarcoFalke)\n- - bitcoin/bitcoin#22363 Use `script_util` helpers for creating P2{PKH,SH,WPKH,WSH} scripts (theStack)\n- - bitcoin/bitcoin#22399 fuzz: Rework CTxDestination fuzzing (MarcoFalke)\n- - bitcoin/bitcoin#22408 add tests for `bad-txns-prevout-null` reject reason (theStack)\n- - bitcoin/bitcoin#22445 fuzz: Move implementations of non-template fuzz helpers from util.h to util.cpp (sriramdvt)\n- - bitcoin/bitcoin#22446 Fix `wallet_listdescriptors.py` if bdb is not compiled (hebasto)\n- - bitcoin/bitcoin#22447 Whitelist `rpc_rawtransaction` peers to speed up tests (jonatack)\n- - bitcoin/bitcoin#22742 Use proper target in `do_fund_send` (S3RK)\n\n### Miscellaneous\n- - bitcoin/bitcoin#19337 sync: Detect double lock from the same thread (vasild)\n- - bitcoin/bitcoin#19809 log: Prefix log messages with function name and source code location if -logsourcelocations is set (practicalswift)\n- - bitcoin/bitcoin#19866 eBPF Linux tracepoints (jb55)\n- - bitcoin/bitcoin#20024 init: Fix incorrect warning \"Reducing -maxconnections from N to N-1, because of system limitations\" (practicalswift)\n- - bitcoin/bitcoin#20145 contrib: Add getcoins.py script to get coins from (signet) faucet (kallewoof)\n- - bitcoin/bitcoin#20255 util: Add assume() identity function (MarcoFalke)\n- - bitcoin/bitcoin#20288 script, doc: Contrib/seeds updates (jonatack)\n- - bitcoin/bitcoin#20358 src/randomenv.cpp: Fix build on uclibc (ffontaine)\n- - bitcoin/bitcoin#20406 util: Avoid invalid integer negation in formatmoney and valuefromamount (practicalswift)\n- - bitcoin/bitcoin#20434 contrib: Parse elf directly for symbol and security checks (laanwj)\n- - bitcoin/bitcoin#20451 lint: Run mypy over contrib/devtools (fanquake)\n- - bitcoin/bitcoin#20476 contrib: Add test for elf symbol-check (laanwj)\n- - bitcoin/bitcoin#20530 lint: Update cppcheck linter to c++17 and improve explicit usage (fjahr)\n- - bitcoin/bitcoin#20589 log: Clarify that failure to read/write `fee_estimates.dat` is non-fatal (MarcoFalke)\n- - bitcoin/bitcoin#20602 util: Allow use of c++14 chrono literals (MarcoFalke)\n- - bitcoin/bitcoin#20605 init: Signal-safe instant shutdown (laanwj)\n- - bitcoin/bitcoin#20608 contrib: Add symbol check test for PE binaries (fanquake)\n- - bitcoin/bitcoin#20689 contrib: Replace binary verification script verify.sh with python rewrite (theStack)\n- - bitcoin/bitcoin#20715 util: Add argsmanager::getcommand() and use it in bitcoin-wallet (MarcoFalke)\n- - bitcoin/bitcoin#20735 script: Remove outdated extract-osx-sdk.sh (hebasto)\n- - bitcoin/bitcoin#20817 lint: Update list of spelling linter false positives, bump to codespell 2.0.0 (theStack)\n- - bitcoin/bitcoin#20884 script: Improve robustness of bitcoind.service on startup (hebasto)\n- - bitcoin/bitcoin#20906 contrib: Embed c++11 patch in `install_db4.sh` (gruve-p)\n- - bitcoin/bitcoin#21004 contrib: Fix docker args conditional in gitian-build (setpill)\n- - bitcoin/bitcoin#21007 bitcoind: Add -daemonwait option to wait for initialization (laanwj)\n- - bitcoin/bitcoin#21041 log: Move \"Pre-allocating up to position 0x[\u2026] in [\u2026].dat\" log message to debug category (practicalswift)\n- - bitcoin/bitcoin#21059 Drop boost/preprocessor dependencies (hebasto)\n- - bitcoin/bitcoin#21087 guix: Passthrough `BASE_CACHE` into container (dongcarl)\n- - bitcoin/bitcoin#21088 guix: Jump forwards in time-machine and adapt (dongcarl)\n- - bitcoin/bitcoin#21089 guix: Add support for powerpc64{,le} (dongcarl)\n- - bitcoin/bitcoin#21110 util: Remove boost `posix_time` usage from `gettime*` (fanquake)\n- - bitcoin/bitcoin#21111 Improve OpenRC initscript (parazyd)\n- - bitcoin/bitcoin#21123 code style: Add EditorConfig file (kiminuo)\n- - bitcoin/bitcoin#21173 util: Faster hexstr => 13% faster blocktojson (martinus)\n- - bitcoin/bitcoin#21221 tools: Allow argument/parameter bin packing in clang-format (jnewbery)\n- - bitcoin/bitcoin#21244 Move GetDataDir to ArgsManager (kiminuo)\n- - bitcoin/bitcoin#21255 contrib: Run test-symbol-check for risc-v (fanquake)\n- - bitcoin/bitcoin#21271 guix: Explicitly set umask in build container (dongcarl)\n- - bitcoin/bitcoin#21300 script: Add explanatory comment to tc.sh (dscotese)\n- - bitcoin/bitcoin#21317 util: Make assume() usable as unary expression (MarcoFalke)\n- - bitcoin/bitcoin#21336 Make .gitignore ignore src/test/fuzz/fuzz.exe (hebasto)\n- - bitcoin/bitcoin#21337 guix: Update darwin native packages dependencies (hebasto)\n- - bitcoin/bitcoin#21405 compat: remove memcpy -> memmove backwards compatibility alias (fanquake)\n- - bitcoin/bitcoin#21418 contrib: Make systemd invoke dependencies only when ready (laanwj)\n- - bitcoin/bitcoin#21447 Always add -daemonwait to known command line arguments (hebasto)\n- - bitcoin/bitcoin#21471 bugfix: Fix `bech32_encode` calls in `gen_key_io_test_vectors.py` (sipa)\n- - bitcoin/bitcoin#21615 script: Add trusted key for hebasto (hebasto)\n- - bitcoin/bitcoin#21664 contrib: Use lief for macos and windows symbol & security checks (fanquake)\n- - bitcoin/bitcoin#21695 contrib: Remove no longer used contrib/bitcoin-qt.pro (hebasto)\n- - bitcoin/bitcoin#21711 guix: Add full installation and usage documentation (dongcarl)\n- - bitcoin/bitcoin#21799 guix: Use `gcc-8` across the board (dongcarl)\n- - bitcoin/bitcoin#21802 Avoid UB in util/asmap (advance a dereferenceable iterator outside its valid range) (MarcoFalke)\n- - bitcoin/bitcoin#21823 script: Update reviewers (jonatack)\n- - bitcoin/bitcoin#21850 Remove `GetDataDir(net_specific)` function (kiminuo)\n- - bitcoin/bitcoin#21871 scripts: Add checks for minimum required os versions (fanquake)\n- - bitcoin/bitcoin#21966 Remove double serialization; use software encoder for fee estimation (sipa)\n- - bitcoin/bitcoin#22060 contrib: Add torv3 seed nodes for testnet, drop v2 ones (laanwj)\n- - bitcoin/bitcoin#22244 devtools: Correctly extract symbol versions in symbol-check (laanwj)\n- - bitcoin/bitcoin#22533 guix/build: Remove vestigial SKIPATTEST.TAG (dongcarl)\n- - bitcoin/bitcoin#22643 guix-verify: Non-zero exit code when anything fails (dongcarl)\n- - bitcoin/bitcoin#22654 guix: Don't include directory name in SHA256SUMS (achow101)\n\n### Documentation\n- - bitcoin/bitcoin#15451 clarify getdata limit after #14897 (HashUnlimited)\n- - bitcoin/bitcoin#15545 Explain why CheckBlock() is called before AcceptBlock (Sjors)\n- - bitcoin/bitcoin#17350 Add developer documentation to isminetype (HAOYUatHZ)\n- - bitcoin/bitcoin#17934 Use `CONFIG_SITE` variable instead of --prefix option (hebasto)\n- - bitcoin/bitcoin#18030 Coin::IsSpent() can also mean never existed (Sjors)\n- - bitcoin/bitcoin#18096 IsFinalTx comment about nSequence & `OP_CLTV` (nothingmuch)\n- - bitcoin/bitcoin#18568 Clarify developer notes about constant naming (ryanofsky)\n- - bitcoin/bitcoin#19961 doc: tor.md updates (jonatack)\n- - bitcoin/bitcoin#19968 Clarify CRollingBloomFilter size estimate (robot-dreams)\n- - bitcoin/bitcoin#20200 Rename CODEOWNERS to REVIEWERS (adamjonas)\n- - bitcoin/bitcoin#20329 docs/descriptors.md: Remove hardened marker in the path after xpub (dgpv)\n- - bitcoin/bitcoin#20380 Add instructions on how to fuzz the P2P layer using Honggfuzz NetDriver (practicalswift)\n- - bitcoin/bitcoin#20414 Remove generated manual pages from master branch (laanwj)\n- - bitcoin/bitcoin#20473 Document current boost dependency as 1.71.0 (laanwj)\n- - bitcoin/bitcoin#20512 Add bash as an OpenBSD dependency (emilengler)\n- - bitcoin/bitcoin#20568 Use FeeModes doc helper in estimatesmartfee (MarcoFalke)\n- - bitcoin/bitcoin#20577 libconsensus: add missing error code description, fix NBitcoin link (theStack)\n- - bitcoin/bitcoin#20587 Tidy up Tor doc (more stringent) (wodry)\n- - bitcoin/bitcoin#20592 Update wtxidrelay documentation per BIP339 (jonatack)\n- - bitcoin/bitcoin#20601 Update for FreeBSD 12.2, add GUI Build Instructions (jarolrod)\n- - bitcoin/bitcoin#20635 fix misleading comment about call to non-existing function (pox)\n- - bitcoin/bitcoin#20646 Refer to BIPs 339/155 in feature negotiation (jonatack)\n- - bitcoin/bitcoin#20653 Move addr relay comment in net to correct place (MarcoFalke)\n- - bitcoin/bitcoin#20677 Remove shouty enums in `net_processing` comments (sdaftuar)\n- - bitcoin/bitcoin#20741 Update 'Secure string handling' (prayank23)\n- - bitcoin/bitcoin#20757 tor.md and -onlynet help updates (jonatack)\n- - bitcoin/bitcoin#20829 Add -netinfo help (jonatack)\n- - bitcoin/bitcoin#20830 Update developer notes with signet (jonatack)\n- - bitcoin/bitcoin#20890 Add explicit macdeployqtplus dependencies install step (hebasto)\n- - bitcoin/bitcoin#20913 Add manual page generation for bitcoin-util (laanwj)\n- - bitcoin/bitcoin#20985 Add xorriso to macOS depends packages (fanquake)\n- - bitcoin/bitcoin#20986 Update developer notes to discourage very long lines (jnewbery)\n- - bitcoin/bitcoin#20987 Add instructions for generating RPC docs (ben-kaufman)\n- - bitcoin/bitcoin#21026 Document use of make-tag script to make tags (laanwj)\n- - bitcoin/bitcoin#21028 doc/bips: Add BIPs 43, 44, 49, and 84 (luke-jr)\n- - bitcoin/bitcoin#21049 Add release notes for listdescriptors RPC (S3RK)\n- - bitcoin/bitcoin#21060 More precise -debug and -debugexclude doc (wodry)\n- - bitcoin/bitcoin#21077 Clarify -timeout and -peertimeout config options (glozow)\n- - bitcoin/bitcoin#21105 Correctly identify script type (niftynei)\n- - bitcoin/bitcoin#21163 Guix is shipped in Debian and Ubuntu (MarcoFalke)\n- - bitcoin/bitcoin#21210 Rework internal and external links (MarcoFalke)\n- - bitcoin/bitcoin#21246 Correction for VerifyTaprootCommitment comments (roconnor-blockstream)\n- - bitcoin/bitcoin#21263 Clarify that squashing should happen before review (MarcoFalke)\n- - bitcoin/bitcoin#21323 guix, doc: Update default HOSTS value (hebasto)\n- - bitcoin/bitcoin#21324 Update build instructions for Fedora (hebasto)\n- - bitcoin/bitcoin#21343 Revamp macOS build doc (jarolrod)\n- - bitcoin/bitcoin#21346 install qt5 when building on macOS (fanquake)\n- - bitcoin/bitcoin#21384 doc: add signet to bitcoin.conf documentation (jonatack)\n- - bitcoin/bitcoin#21394 Improve comment about protected peers (amitiuttarwar)\n- - bitcoin/bitcoin#21398 Update fuzzing docs for afl-clang-lto (MarcoFalke)\n- - bitcoin/bitcoin#21444 net, doc: Doxygen updates and fixes in netbase.{h,cpp} (jonatack)\n- - bitcoin/bitcoin#21481 Tell howto install clang-format on Debian/Ubuntu (wodry)\n- - bitcoin/bitcoin#21567 Fix various misleading comments (glozow)\n- - bitcoin/bitcoin#21661 Fix name of script guix-build (Emzy)\n- - bitcoin/bitcoin#21672 Remove boostrap info from `GUIX_COMMON_FLAGS` doc (fanquake)\n- - bitcoin/bitcoin#21688 Note on SDK for macOS depends cross-compile (jarolrod)\n- - bitcoin/bitcoin#21709 Update reduce-memory.md and bitcoin.conf -maxconnections info (jonatack)\n- - bitcoin/bitcoin#21710 update helps for addnode rpc and -addnode/-maxconnections config options (jonatack)\n- - bitcoin/bitcoin#21752 Clarify that feerates are per virtual size (MarcoFalke)\n- - bitcoin/bitcoin#21811 Remove Visual Studio 2017 reference from readme (sipsorcery)\n- - bitcoin/bitcoin#21818 Fixup -coinstatsindex help, update bitcoin.conf and files.md (jonatack)\n- - bitcoin/bitcoin#21856 add OSS-Fuzz section to fuzzing.md doc (adamjonas)\n- - bitcoin/bitcoin#21912 Remove mention of priority estimation (MarcoFalke)\n- - bitcoin/bitcoin#21925 Update bips.md for 0.21.1 (MarcoFalke)\n- - bitcoin/bitcoin#21942 improve make with parallel jobs description (klementtan)\n- - bitcoin/bitcoin#21947 Fix OSS-Fuzz links (MarcoFalke)\n- - bitcoin/bitcoin#21988 note that brew installed qt is not supported (jarolrod)\n- - bitcoin/bitcoin#22056 describe in fuzzing.md how to reproduce a CI crash (jonatack)\n- - bitcoin/bitcoin#22080 add maxuploadtarget to bitcoin.conf example (jarolrod)\n- - bitcoin/bitcoin#22088 Improve note on choosing posix mingw32 (jarolrod)\n- - bitcoin/bitcoin#22109 Fix external links (IRC, \u2026) (MarcoFalke)\n- - bitcoin/bitcoin#22121 Various validation doc fixups (MarcoFalke)\n- - bitcoin/bitcoin#22172 Update tor.md, release notes with removal of tor v2 support (jonatack)\n- - bitcoin/bitcoin#22204 Remove obsolete `okSafeMode` RPC guideline from developer notes (theStack)\n- - bitcoin/bitcoin#22208 Update `REVIEWERS` (practicalswift)\n- - bitcoin/bitcoin#22250 add basic I2P documentation (vasild)\n- - bitcoin/bitcoin#22296 Final merge of release notes snippets, mv to wiki (MarcoFalke)\n- - bitcoin/bitcoin#22335 recommend `--disable-external-signer` in OpenBSD build guide (theStack)\n- - bitcoin/bitcoin#22339 Document minimum required libc++ version (hebasto)\n- - bitcoin/bitcoin#22349 Repository IRC updates (jonatack)\n- - bitcoin/bitcoin#22360 Remove unused section from release process (MarcoFalke)\n- - bitcoin/bitcoin#22369 Add steps for Transifex to release process (jonatack)\n- - bitcoin/bitcoin#22393 Added info to bitcoin.conf doc (bliotti)\n- - bitcoin/bitcoin#22402 Install Rosetta on M1-macOS for qt in depends (hebasto)\n- - bitcoin/bitcoin#22432 Fix incorrect `testmempoolaccept` doc (glozow)\n- - bitcoin/bitcoin#22648 doc, test: improve i2p/tor docs and i2p reachable unit tests (jonatack)\n\nCredits\n=======\n\nThanks to everyone who directly contributed to this release:\n\n- - Aaron Clauson\n- - Adam Jonas\n- - amadeuszpawlik\n- - Amiti Uttarwar\n- - Andrew Chow\n- - Andrew Poelstra\n- - Anthony Towns\n- - Antoine Poinsot\n- - Antoine Riard\n- - apawlik\n- - apitko\n- - Ben Carman\n- - Ben Woosley\n- - benk10\n- - Bezdrighin\n- - Block Mechanic\n- - Brian Liotti\n- - Bruno Garcia\n- - Carl Dong\n- - Christian Decker\n- - coinforensics\n- - Cory Fields\n- - Dan Benjamin\n- - Daniel Kraft\n- - Darius Parvin\n- - Dhruv Mehta\n- - Dmitry Goncharov\n- - Dmitry Petukhov\n- - dplusplus1024\n- - dscotese\n- - Duncan Dean\n- - Elle Mouton\n- - Elliott Jin\n- - Emil Engler\n- - Ethan Heilman\n- - eugene\n- - Evan Klitzke\n- - Fabian Jahr\n- - Fabrice Fontaine\n- - fanquake\n- - fdov\n- - flack\n- - Fotis Koutoupas\n- - Fu Yong Quah\n- - fyquah\n- - glozow\n- - Gregory Sanders\n- - Guido Vranken\n- - Gunar C. Gessner\n- - h\n- - HAOYUatHZ\n- - Hennadii Stepanov\n- - Igor Cota\n- - Ikko Ashimine\n- - Ivan Metlushko\n- - jackielove4u\n- - James O'Beirne\n- - Jarol Rodriguez\n- - Joel Klabo\n- - John Newbery\n- - Jon Atack\n- - Jonas Schnelli\n- - Jo\u00e3o Barbosa\n- - Josiah Baker\n- - Karl-Johan Alm\n- - Kiminuo\n- - Klement Tan\n- - Kristaps Kaupe\n- - Larry Ruane\n- - lisa neigut\n- - Lucas Ontivero\n- - Luke Dashjr\n- - Maayan Keshet\n- - MarcoFalke\n- - Martin Ankerl\n- - Martin Zumsande\n- - Michael Dietz\n- - Michael Polzer\n- - Michael Tidwell\n- - Niklas G\u00f6gge\n- - nthumann\n- - Oliver Gugger\n- - parazyd\n- - Patrick Strateman\n- - Pavol Rusnak\n- - Peter Bushnell\n- - Pierre K\n- - Pieter Wuille\n- - PiRK\n- - pox\n- - practicalswift\n- - Prayank\n- - R E Broadley\n- - Rafael Sadowski\n- - randymcmillan\n- - Raul Siles\n- - Riccardo Spagni\n- - Russell O'Connor\n- - Russell Yanofsky\n- - S3RK\n- - saibato\n- - Samuel Dobson\n- - sanket1729\n- - Sawyer Billings\n- - Sebastian Falbesoner\n- - setpill\n- - sgulls\n- - sinetek\n- - Sjors Provoost\n- - Sriram\n- - Stephan Oeste\n- - Suhas Daftuar\n- - Sylvain Goumy\n- - t-bast\n- - Troy Giorshev\n- - Tushar Singla\n- - Tyler Chambers\n- - Uplab\n- - Vasil Dimov\n- - W. J. van der Laan\n- - willcl-ark\n- - William Bright\n- - William Casarin\n- - windsok\n- - wodry\n- - Yerzhan Mazhkenov\n- - Yuval Kogman\n- - Zero\n\nAs well as to everyone that helped with translations on\n[Transifex](https://www.transifex.com/bitcoin/bitcoin/).\n-----BEGIN PGP SIGNATURE-----\n\niQEzBAEBCgAdFiEEnerg3HBjJJ+wVHRoHkrtYphs0l0FAmE/X9UACgkQHkrtYphs\n0l2ezQf+JD5g0NVVNdLuvNf+bz59zBMf7seNi385h6sd74hRDHYYN/whYLZwRl+w\n0zvCvzbDy3AFULI/laoaUHAP1sz7/5H01je+BH/hzAKCflGQfZYz3y+fBftye6ag\nOKesWunMJdmU58nj1AQjcueXu8JmolH73GeJFRlNsVYyiYRndyA+5osF2oqBUTdP\nc9rtUIOQx6O/YjEhFZeIXnER2YhLIYaVf06FkGRUTS6coYI6dhhYbFrv3NcD1+rf\nU2XMeTaiDoDnQrSaSxd/czJ85oNdF8kjsZpfbVwcRU0M2HW8AqBpp+AgDIdmznCt\nNac3q+5b0JHiZ2tNfyUiixto+OMSVA==\n=DIZ5\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 22.0 released",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "W. J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 82526
        }
    },
    {
        "title": "[bitcoin-dev] BIP extensions",
        "thread_messages": [
            {
                "author": "Karl-Johan Alm",
                "date": "2021-09-15T06:14:31",
                "message_text_only": "BIPs are proposals.\n\nThey begin as ideas, are formulated and discussed on this list, and\nassuming no glaring flaws are observed, turned into pull requests to\nthe bips repository, assigned a BIP number by the editors, and merged.\n\nIt is then organically incorporated into the various entities that\nexist in the Bitcoin space. At this point, it is not merely a\nproposal, but a standard. As entities place their weight behind a BIP,\nit makes less and less sense to consider its author the \"maintainer\"\nof the BIP, with rights to modify it at their whim. Someone may have\nagreed to the proposal in its original form, but they may disagree\nwith it if it is altered from under their feet.\n\nBIPs are modified for primarily three reasons:\n\n1. Because of spelling errors, or to otherwise improve on their\ndescription without changing what is actually proposed.\n2. To improve the proposal in some way, e.g. after discussion or after\ngetting feedback on the proposed approach.\n3. To add missing content, such as activation strategy.\n\nI propose that changes of the second and third type, unless they are\nabsolutely free from contention, are done as BIP extensions.\n\nBIP extensions are separate BIPs that extend on or an existing BIP.\nBIP extensions do not require the approval of the extended-upon BIP's\nauthor, and are considered independent proposals entirely. A BIP that\nextends on BIP XXX is referred to as BIP-XXX-Y, e.g. BIP-123-1, and\ntheir introductory section must include the wording \"\n\nThis BIP extends on (link: BIP-XXX).\n\n\".\n\nBy making extensions to BIPs, rather than modifying them long after\nreview, we are giving the community\n1. the assurance that a BIP will mostly remain in its form forever,\nexcept if an obvious win is discovered,\n2. the ability to judge modifications to BIPs, such as activation\nparameters, on their merits alone, and\n3. the path to propose modifications to BIPs even if their authors\nhave gone inactive and cease to provide feedback, as is the case for\nmany BIPs today, as BIP extensions do not require the approval of the\nextended-upon BIP.\n\n(Apologies if this has been proposed already. If so, feel free to\nignore this message, and sorry to have wasted your time.)"
            },
            {
                "author": "Federico Berrone",
                "date": "2021-09-15T05:47:57",
                "message_text_only": "Hi Karl-Johan,\nI fully agree with your proposal. In order to de-clutter BIPs and make a \nmore understandable proposal, we can add the additional information in a \nseparate piece. Also, this would maintain the original proposal without \nany modifications, showing the original spirit of it.\nLet me know how can I help you with your proposal.\n\nRegards,\nFederico Berrone.\n\nP/D: This is my first participation in the bitcoin-dev list, sorry if I \nam breaking any rule, I would be glad to know if that is the case.\n\nEl 15/09/2021 a las 8:14, Karl-Johan Alm via bitcoin-dev escribi\u00f3:\n> BIPs are proposals.\n>\n> They begin as ideas, are formulated and discussed on this list, and\n> assuming no glaring flaws are observed, turned into pull requests to\n> the bips repository, assigned a BIP number by the editors, and merged.\n>\n> It is then organically incorporated into the various entities that\n> exist in the Bitcoin space. At this point, it is not merely a\n> proposal, but a standard. As entities place their weight behind a BIP,\n> it makes less and less sense to consider its author the \"maintainer\"\n> of the BIP, with rights to modify it at their whim. Someone may have\n> agreed to the proposal in its original form, but they may disagree\n> with it if it is altered from under their feet.\n>\n> BIPs are modified for primarily three reasons:\n>\n> 1. Because of spelling errors, or to otherwise improve on their\n> description without changing what is actually proposed.\n> 2. To improve the proposal in some way, e.g. after discussion or after\n> getting feedback on the proposed approach.\n> 3. To add missing content, such as activation strategy.\n>\n> I propose that changes of the second and third type, unless they are\n> absolutely free from contention, are done as BIP extensions.\n>\n> BIP extensions are separate BIPs that extend on or an existing BIP.\n> BIP extensions do not require the approval of the extended-upon BIP's\n> author, and are considered independent proposals entirely. A BIP that\n> extends on BIP XXX is referred to as BIP-XXX-Y, e.g. BIP-123-1, and\n> their introductory section must include the wording \"\n>\n> This BIP extends on (link: BIP-XXX).\n>\n> \".\n>\n> By making extensions to BIPs, rather than modifying them long after\n> review, we are giving the community\n> 1. the assurance that a BIP will mostly remain in its form forever,\n> except if an obvious win is discovered,\n> 2. the ability to judge modifications to BIPs, such as activation\n> parameters, on their merits alone, and\n> 3. the path to propose modifications to BIPs even if their authors\n> have gone inactive and cease to provide feedback, as is the case for\n> many BIPs today, as BIP extensions do not require the approval of the\n> extended-upon BIP.\n>\n> (Apologies if this has been proposed already. If so, feel free to\n> ignore this message, and sorry to have wasted your time.)\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: OpenPGP_0xB4B16B2D677120AF.asc\nType: application/pgp-keys\nSize: 3147 bytes\nDesc: OpenPGP public key\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210915/f2ca346f/attachment.bin>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: OpenPGP_signature\nType: application/pgp-signature\nSize: 840 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210915/f2ca346f/attachment.sig>"
            },
            {
                "author": "Karl-Johan Alm",
                "date": "2021-09-15T10:18:41",
                "message_text_only": "Hi Frederico,\n\nWelcome to the bitcoin-dev list. :)\n\nMichael Folkson is currently pushing for a revision to BIP 2, which is\ndiscussed in the \"BIP process meeting\" thread here. You could help out\nby participating in that process. There's a wiki page with ideas for\nthis in [1] and the current plan is to modify [2] or some other pull\nrequest to reflect what everyone decides.\n\n[1] https://github.com/bitcoin/bips/wiki/BIP-Process-wishlist\n[2] https://github.com/bitcoin/bips/pull/1015\n\n-Kalle.\n\nOn Wed, 15 Sept 2021 at 17:29, Federico Berrone via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Hi Karl-Johan,\n> I fully agree with your proposal. In order to de-clutter BIPs and make a\n> more understandable proposal, we can add the additional information in a\n> separate piece. Also, this would maintain the original proposal without\n> any modifications, showing the original spirit of it.\n> Let me know how can I help you with your proposal.\n>\n> Regards,\n> Federico Berrone.\n>\n> P/D: This is my first participation in the bitcoin-dev list, sorry if I\n> am breaking any rule, I would be glad to know if that is the case.\n>\n> El 15/09/2021 a las 8:14, Karl-Johan Alm via bitcoin-dev escribi\u00f3:\n> > BIPs are proposals.\n> >\n> > They begin as ideas, are formulated and discussed on this list, and\n> > assuming no glaring flaws are observed, turned into pull requests to\n> > the bips repository, assigned a BIP number by the editors, and merged.\n> >\n> > It is then organically incorporated into the various entities that\n> > exist in the Bitcoin space. At this point, it is not merely a\n> > proposal, but a standard. As entities place their weight behind a BIP,\n> > it makes less and less sense to consider its author the \"maintainer\"\n> > of the BIP, with rights to modify it at their whim. Someone may have\n> > agreed to the proposal in its original form, but they may disagree\n> > with it if it is altered from under their feet.\n> >\n> > BIPs are modified for primarily three reasons:\n> >\n> > 1. Because of spelling errors, or to otherwise improve on their\n> > description without changing what is actually proposed.\n> > 2. To improve the proposal in some way, e.g. after discussion or after\n> > getting feedback on the proposed approach.\n> > 3. To add missing content, such as activation strategy.\n> >\n> > I propose that changes of the second and third type, unless they are\n> > absolutely free from contention, are done as BIP extensions.\n> >\n> > BIP extensions are separate BIPs that extend on or an existing BIP.\n> > BIP extensions do not require the approval of the extended-upon BIP's\n> > author, and are considered independent proposals entirely. A BIP that\n> > extends on BIP XXX is referred to as BIP-XXX-Y, e.g. BIP-123-1, and\n> > their introductory section must include the wording \"\n> >\n> > This BIP extends on (link: BIP-XXX).\n> >\n> > \".\n> >\n> > By making extensions to BIPs, rather than modifying them long after\n> > review, we are giving the community\n> > 1. the assurance that a BIP will mostly remain in its form forever,\n> > except if an obvious win is discovered,\n> > 2. the ability to judge modifications to BIPs, such as activation\n> > parameters, on their merits alone, and\n> > 3. the path to propose modifications to BIPs even if their authors\n> > have gone inactive and cease to provide feedback, as is the case for\n> > many BIPs today, as BIP extensions do not require the approval of the\n> > extended-upon BIP.\n> >\n> > (Apologies if this has been proposed already. If so, feel free to\n> > ignore this message, and sorry to have wasted your time.)\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-15T14:34:54",
                "message_text_only": "On Wed, Sep 15, 2021 at 03:14:31PM +0900, Karl-Johan Alm via bitcoin-dev wrote:\n> BIPs are proposals.\n\n> It is then organically incorporated into the various entities that\n> exist in the Bitcoin space. At this point, it is not merely a\n> proposal, but a standard.\n\nThinking of BIPs that have reach \"Final\" status as a \"standard\" might\nbe reasonable, but I'd be pretty careful about even going that far,\nlet alone further.\n\nBut as you said, \"BIPs are proposals\". If your conclusion is somehow\nthat a BIP \"is not merely a proposal\", you're reached a contradiction,\nwhich means you've made a logic error somewhere in between...\n\n> Someone may have\n> agreed to the proposal in its original form, but they may disagree\n> with it if it is altered from under their feet.\n\n> 2. To improve the proposal in some way, e.g. after discussion or after\n> getting feedback on the proposed approach.\n> 3. To add missing content, such as activation strategy.\n\n> I propose that changes of the second and third type, unless they are\n> absolutely free from contention, are done as BIP extensions.\n\nIf you were proposing this just for BIPs that are marked final, then\nsure, maybe, I guess -- though why mark them final if you still want\nto add missing content or make further improvements? But if you want to\napply it as soon as a BIP number is assigned or text is merged into the\nrepo, I think that just means requesting number assignment gets delayed\nuntil the end of the development process rather than near the beginning,\nwhich doesn't sound particularly helpful.\n\nThat's essentially how the lightning BOLTs are set up -- you only get to\npublish a BOLT after you've got support from multiple implementations\n[0]; but that has meant they don't have published docs for the various\nthings individual teams have implemented, making interoperability harder\nrather than easier. There's been talk about creating bLIPs [1] to remedy\nthis lack.\n\n> BIP extensions are separate BIPs that extend on or an existing BIP.\n\nSo as an alternative, how about more clearly separating out draft BIPs\nfrom those in Active/Final state? ie:\n\n * brand new BIP draft comes in from its authors/champions/whatever\n * number xxx gets assigned, it becomes \"Draft BIP xxx\"\n * authors modify it as they see fit\n * once the authors are happy with the text, they can move it\n   to Final status, at which point it is no longer a draft and is\n   just \"BIP xxx\", and doesn't get modified anymore\n * go to step 1\n\n(I'm doubtful that it's very useful to have an \"Active\" state as distinct\nfrom \"Final\"; that just gives the editors an excuse to play favourites\nby deciding whose objections count and whose don't (or perhaps which\nimplementations count and which ones don't). It's currently only used for\nBIPs about the BIP process, which makes it seem particularly pointless...)\n\n> By making extensions to BIPs, rather than modifying them long after\n> review, we are giving the community [...]\n\nAs described, I think you would be giving people an easy way to actively\nobstruct the BIP process by making it harder to \"improve the proposal\"\nand \"add missing content\", and encouraging contentiousness as a result.\n\nFor adding on to BIPs that have reached Final status, I think just\nassigning completely new numbers is fine, as occurred with bech32 and\nbech32m (BIPs 173 and 350).\n\nEven beyond that, having BIP maintainers exercising judgement by trying\nto reserve/assign \"pretty\" numbers (like \"BIP 3\" for the new BIP process)\nseems like a mistake to me. If it were up to me, I'd make the setup be\nsomething like:\n\n * new BIP? make a PR, putting the text into\n   \"drafts/bip-authorname-description.mediawiki\" (with corresponding\n   directory for images etc). Have the word \"Draft\" appear in the \"BIP:\n   xxx\" header as well as in the Status: header.\n\n * if that passes CI and isn't incoherent, it gets merged\n\n * only after the draft is already merged is a BIP number assigned.\n   the number is chosen by a script, and the BIP maintainers rename it\n   to \"drafts/bip-xxx.mediawiki\" in a followup commit including internal\n   links to bip-authorname-description/foo.png and add it to the README\n   (automatically at the same time as the merge, ideally)\n\n * when a BIP becomes Final, it gets moved from drafts/ into\n   the main directory [2], and to avoid breaking external links,\n   drafts/bip-xxx.mediawiki is changed to just have a link to the\n   main doc.\n\n * likewise when a BIP becomes rejected/deprecated/whatever, it's moved\n   into historical/ and drafts/bip-xxx.mediawiki and bip-xxx.mediawiki\n   are updated with a link to the new location\n\n * otherwise, don't allow any modifications to bips outside of\n   drafts/, with the possible exception of adding additional info in\n   Acknowledgements or See also section or similar, adding Superseded-By:\n   links, and updating additional tables that are deliberately designed\n   to be updated, eg bip-0009/assignments.mediawiki\n\nIt's better to remove incentives to introduce friction rather than\nadd more.\n\nCheers,\naj\n\n[0] https://github.com/lightningnetwork/lightning-rfc/blob/master/CONTRIBUTING.md\n\n[1] https://github.com/ryanthegentry/lightning-rfc/blob/blip-0001/blips/blip-0001.md\n\n[2] Maybe moving the files between directories is too much, but I think\n    having \"drafts/\" in the URL is likely to help ensure people referring\n    to draft BIPs actually realise they're drafts, and thus subject to\n    large changes, cf\n    https://twitter.com/BobMcElrath/status/1281606259863629824\n\n    Likewise, people probably might not want to implement/deploy BIPs\n    marked \"draft\", which is a good reason for the authors to mark them\n    final, which in turn might help ensure they're actually complete\n    and finished before they're deployed, all of which seems like a\n    good thing."
            }
        ],
        "thread_summary": {
            "title": "BIP extensions",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Federico Berrone",
                "Karl-Johan Alm"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 15552
        }
    },
    {
        "title": "[bitcoin-dev] Reminder on the Purpose of BIPs",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-09-15T09:50:34",
                "message_text_only": "> I like the idea of decentralizing the BIPs process. It is a historical artifact that the bips repository is part of the same organization that bitcoin core is part of. But there shouldn't be the perception that standardization is driven by that, or that there is any kind of (non-trivial) gatekeeping.\n\nI had suggested few changes in BIP process and repository yesterday. Meeting was disappointing because of few reasons: \n\n1.Its been 12 years since Bitcoin came in to existence and I am surprised that during such important conversations I still see only 4 people out of which 2 are maintainers.\n\n2.None of the people who participated in meeting agree that we need to create multiple BIP directories and let people decide what works best for them. Reduce dependency on one repository or few people. At the end of the day these are just proposals, implementations are more important and there are so many ways to document things online, archive etc.\n\nPlaying ACK/NACK game in 'bitcoin/bips' repository will be a waste of time so I created this as an example:\n\nhttps://github.com/prayank23/bips/blob/master/README.md\n\nhttps://prayank23.github.io/bips/\n\nI respect everyone involved in Bitcoin development however neither I trust anyone nor I expect anyone to trust me. Bitcoin is not just another open source software. Its a protocol for decentralized network which can be used to settle payments. We are trying to redefine MONEY, many cypherpunks, activists, hacktivists, privacy advocates etc. are involved and trying to separate money from state. The same money that is needed for almost everything you do in this world from birth to death, love to war and same money that makes some people more powerful. So, I won't be surprised with anything in future and will be prepared for everything.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210915/a1478d8f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Reminder on the Purpose of BIPs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2011
        }
    },
    {
        "title": "[bitcoin-dev] Proposal: Package Mempool Accept and Package RBF",
        "thread_messages": [
            {
                "author": "Gloria Zhao",
                "date": "2021-09-16T07:51:25",
                "message_text_only": "Hi there,\n\nI'm writing to propose a set of mempool policy changes to enable package\nvalidation (in preparation for package relay) in Bitcoin Core. These would\nnot\nbe consensus or P2P protocol changes. However, since mempool policy\nsignificantly affects transaction propagation, I believe this is relevant\nfor\nthe mailing list.\n\nMy proposal enables packages consisting of multiple parents and 1 child. If\nyou\ndevelop software that relies on specific transaction relay assumptions\nand/or\nare interested in using package relay in the future, I'm very interested to\nhear\nyour feedback on the utility or restrictiveness of these package policies\nfor\nyour use cases.\n\nA draft implementation of this proposal can be found in [Bitcoin Core\nPR#22290][1].\n\nAn illustrated version of this post can be found at\nhttps://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\nI have also linked the images below.\n\n## Background\n\nFeel free to skip this section if you are already familiar with mempool\npolicy\nand package relay terminology.\n\n### Terminology Clarifications\n\n* Package = an ordered list of related transactions, representable by a\nDirected\n  Acyclic Graph.\n* Package Feerate = the total modified fees divided by the total virtual\nsize of\n  all transactions in the package.\n    - Modified fees = a transaction's base fees + fee delta applied by the\nuser\n      with `prioritisetransaction`. As such, we expect this to vary across\nmempools.\n    - Virtual Size = the maximum of virtual sizes calculated using [BIP141\n      virtual size][2] and sigop weight. [Implemented here in Bitcoin\nCore][3].\n    - Note that feerate is not necessarily based on the base fees and\nserialized\n      size.\n\n* Fee-Bumping = user/wallet actions that take advantage of miner incentives\nto\n  boost a transaction's candidacy for inclusion in a block, including Child\nPays\nfor Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in\nmempool policy is to recognize when the new transaction is more economical\nto\nmine than the original one(s) but not open DoS vectors, so there are some\nlimitations.\n\n### Policy\n\nThe purpose of the mempool is to store the best (to be most\nincentive-compatible\nwith miners, highest feerate) candidates for inclusion in a block. Miners\nuse\nthe mempool to build block templates. The mempool is also useful as a cache\nfor\nboosting block relay and validation performance, aiding transaction relay,\nand\ngenerating feerate estimations.\n\nIdeally, all consensus-valid transactions paying reasonable fees should\nmake it\nto miners through normal transaction relay, without any special\nconnectivity or\nrelationships with miners. On the other hand, nodes do not have unlimited\nresources, and a P2P network designed to let any honest node broadcast their\ntransactions also exposes the transaction validation engine to DoS attacks\nfrom\nmalicious peers.\n\nAs such, for unconfirmed transactions we are considering for our mempool, we\napply a set of validation rules in addition to consensus, primarily to\nprotect\nus from resource exhaustion and aid our efforts to keep the highest fee\ntransactions. We call this mempool _policy_: a set of (configurable,\nnode-specific) rules that transactions must abide by in order to be accepted\ninto our mempool. Transaction \"Standardness\" rules and mempool restrictions\nsuch\nas \"too-long-mempool-chain\" are both examples of policy.\n\n### Package Relay and Package Mempool Accept\n\nIn transaction relay, we currently consider transactions one at a time for\nsubmission to the mempool. This creates a limitation in the node's ability\nto\ndetermine which transactions have the highest feerates, since we cannot take\ninto account descendants (i.e. cannot use CPFP) until all the transactions\nare\nin the mempool. Similarly, we cannot use a transaction's descendants when\nconsidering it for RBF. When an individual transaction does not meet the\nmempool\nminimum feerate and the user isn't able to create a replacement transaction\ndirectly, it will not be accepted by mempools.\n\nThis limitation presents a security issue for applications and users\nrelying on\ntime-sensitive transactions. For example, Lightning and other protocols\ncreate\nUTXOs with multiple spending paths, where one counterparty's spending path\nopens\nup after a timelock, and users are protected from cheating scenarios as\nlong as\nthey redeem on-chain in time. A key security assumption is that all parties'\ntransactions will propagate and confirm in a timely manner. This assumption\ncan\nbe broken if fee-bumping does not work as intended.\n\nThe end goal for Package Relay is to consider multiple transactions at the\nsame\ntime, e.g. a transaction with its high-fee child. This may help us better\ndetermine whether transactions should be accepted to our mempool,\nespecially if\nthey don't meet fee requirements individually or are better RBF candidates\nas a\npackage. A combination of changes to mempool validation logic, policy, and\ntransaction relay allows us to better propagate the transactions with the\nhighest package feerates to miners, and makes fee-bumping tools more\npowerful\nfor users.\n\nThe \"relay\" part of Package Relay suggests P2P messaging changes, but a\nlarge\npart of the changes are in the mempool's package validation logic. We call\nthis\n*Package Mempool Accept*.\n\n### Previous Work\n\n* Given that mempool validation is DoS-sensitive and complex, it would be\n  dangerous to haphazardly tack on package validation logic. Many efforts\nhave\nbeen made to make mempool validation less opaque (see [#16400][4],\n[#21062][5],\n[#22675][6], [#22796][7]).\n* [#20833][8] Added basic capabilities for package validation, test accepts\nonly\n  (no submission to mempool).\n* [#21800][9] Implemented package ancestor/descendant limit checks for\narbitrary\n  packages. Still test accepts only.\n* Previous package relay proposals (see [#16401][10], [#19621][11]).\n\n### Existing Package Rules\n\nThese are in master as introduced in [#20833][8] and [#21800][9]. I'll\nconsider\nthem as \"given\" in the rest of this document, though they can be changed,\nsince\npackage validation is test-accept only right now.\n\n1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n`MAX_PACKAGE_SIZE=101KvB` total size [8]\n\n   *Rationale*: This is already enforced as mempool ancestor/descendant\nlimits.\nPresumably, transactions in a package are all related, so exceeding this\nlimit\nwould mean that the package can either be split up or it wouldn't pass this\nmempool policy.\n\n2. Packages must be topologically sorted: if any dependencies exist between\ntransactions, parents must appear somewhere before children. [8]\n\n3. A package cannot have conflicting transactions, i.e. none of them can\nspend\nthe same inputs. This also means there cannot be duplicate transactions. [8]\n\n4. When packages are evaluated against ancestor/descendant limits in a test\naccept, the union of all of their descendants and ancestors is considered.\nThis\nis essentially a \"worst case\" heuristic where every transaction in the\npackage\nis treated as each other's ancestor and descendant. [8]\nPackages for which ancestor/descendant limits are accurately captured by\nthis\nheuristic: [19]\n\nThere are also limitations such as the fact that CPFP carve out is not\napplied\nto package transactions. #20833 also disables RBF in package validation;\nthis\nproposal overrides that to allow packages to use RBF.\n\n## Proposed Changes\n\nThe next step in the Package Mempool Accept project is to implement\nsubmission\nto mempool, initially through RPC only. This allows us to test the\nsubmission\nlogic before exposing it on P2P.\n\n### Summary\n\n- Packages may contain already-in-mempool transactions.\n- Packages are 2 generations, Multi-Parent-1-Child.\n- Fee-related checks use the package feerate. This means that wallets can\ncreate a package that utilizes CPFP.\n- Parents are allowed to RBF mempool transactions with a set of rules\nsimilar\n  to BIP125. This enables a combination of CPFP and RBF, where a\ntransaction's descendant fees pay for replacing mempool conflicts.\n\nThere is a draft implementation in [#22290][1]. It is WIP, but feedback is\nalways welcome.\n\n### Details\n\n#### Packages May Contain Already-in-Mempool Transactions\n\nA package may contain transactions that are already in the mempool. We\nremove\n(\"deduplicate\") those transactions from the package for the purposes of\npackage\nmempool acceptance. If a package is empty after deduplication, we do\nnothing.\n\n*Rationale*: Mempools vary across the network. It's possible for a parent\nto be\naccepted to the mempool of a peer on its own due to differences in policy\nand\nfee market fluctuations. We should not reject or penalize the entire\npackage for\nan individual transaction as that could be a censorship vector.\n\n#### Packages Are Multi-Parent-1-Child\n\nOnly packages of a specific topology are permitted. Namely, a package is\nexactly\n1 child with all of its unconfirmed parents. After deduplication, the\npackage\nmay be exactly the same, empty, 1 child, 1 child with just some of its\nunconfirmed parents, etc. Note that it's possible for the parents to be\nindirect\ndescendants/ancestors of one another, or for parent and child to share a\nparent,\nso we cannot make any other topology assumptions.\n\n*Rationale*: This allows for fee-bumping by CPFP. Allowing multiple parents\nmakes it possible to fee-bump a batch of transactions. Restricting packages\nto a\ndefined topology is also easier to reason about and simplifies the\nvalidation\nlogic greatly. Multi-parent-1-child allows us to think of the package as\none big\ntransaction, where:\n\n- Inputs = all the inputs of parents + inputs of the child that come from\n  confirmed UTXOs\n- Outputs = all the outputs of the child + all outputs of the parents that\n  aren't spent by other transactions in the package\n\nExamples of packages that follow this rule (variations of example A show\nsome\npossibilities after deduplication): ![image][15]\n\n#### Fee-Related Checks Use Package Feerate\n\nPackage Feerate = the total modified fees divided by the total virtual size\nof\nall transactions in the package.\n\nTo meet the two feerate requirements of a mempool, i.e., the pre-configured\nminimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\nfeerate, the\ntotal package feerate is used instead of the individual feerate. The\nindividual\ntransactions are allowed to be below feerate requirements if the package\nmeets\nthe feerate requirements. For example, the parent(s) in the package can\nhave 0\nfees but be paid for by the child.\n\n*Rationale*: This can be thought of as \"CPFP within a package,\" solving the\nissue of a parent not meeting minimum fees on its own. This allows L2\napplications to adjust their fees at broadcast time instead of overshooting\nor\nrisking getting stuck/pinned.\n\nWe use the package feerate of the package *after deduplication*.\n\n*Rationale*:  It would be incorrect to use the fees of transactions that are\nalready in the mempool, as we do not want a transaction's fees to be\ndouble-counted for both its individual RBF and package RBF.\n\nExamples F and G [14] show the same package, but P1 is submitted\nindividually before\nthe package in example G. In example F, we can see that the 300vB package\npays\nan additional 200sat in fees, which is not enough to pay for its own\nbandwidth\n(BIP125#4). In example G, we can see that P1 pays enough to replace M1, but\nusing P1's fees again during package submission would make it look like a\n300sat\nincrease for a 200vB package. Even including its fees and size would not be\nsufficient in this example, since the 300sat looks like enough for the 300vB\npackage. The calculcation after deduplication is 100sat increase for a\npackage\nof size 200vB, which correctly fails BIP125#4. Assume all transactions have\na\nsize of 100vB.\n\n#### Package RBF\n\nIf a package meets feerate requirements as a package, the parents in the\ntransaction are allowed to replace-by-fee mempool transactions. The child\ncannot\nreplace mempool transactions. Multiple transactions can replace the same\ntransaction, but in order to be valid, none of the transactions can try to\nreplace an ancestor of another transaction in the same package (which would\nthus\nmake its inputs unavailable).\n\n*Rationale*: Even if we are using package feerate, a package will not\npropagate\nas intended if RBF still requires each individual transaction to meet the\nfeerate requirements.\n\nWe use a set of rules slightly modified from BIP125 as follows:\n\n##### Signaling (Rule #1)\n\nAll mempool transactions to be replaced must signal replaceability.\n\n*Rationale*: Package RBF signaling logic should be the same for package RBF\nand\nsingle transaction acceptance. This would be updated if single transaction\nvalidation moves to full RBF.\n\n##### New Unconfirmed Inputs (Rule #2)\n\nA package may include new unconfirmed inputs, but the ancestor feerate of\nthe\nchild must be at least as high as the ancestor feerates of every transaction\nbeing replaced. This is contrary to BIP125#2, which states \"The replacement\ntransaction may only include an unconfirmed input if that input was\nincluded in\none of the original transactions. (An unconfirmed input spends an output\nfrom a\ncurrently-unconfirmed transaction.)\"\n\n*Rationale*: The purpose of BIP125#2 is to ensure that the replacement\ntransaction has a higher ancestor score than the original transaction(s)\n(see\n[comment][13]). Example H [16] shows how adding a new unconfirmed input can\nlower the\nancestor score of the replacement transaction. P1 is trying to replace M1,\nand\nspends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and M2\npays\n100sat. Assume all transactions have a size of 100vB. While, in isolation,\nP1\nlooks like a better mining candidate than M1, it must be mined with M2, so\nits\nancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor\nfeerate, which is 6sat/vB.\n\nIn package RBF, the rule analogous to BIP125#2 would be \"none of the\ntransactions in the package can spend new unconfirmed inputs.\" Example J\n[17] shows\nwhy, if any of the package transactions have ancestors, package feerate is\nno\nlonger accurate. Even though M2 and M3 are not ancestors of P1 (which is the\nreplacement transaction in an RBF), we're actually interested in the entire\npackage. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1, P2,\nand\nP3, which is only 4sat/vB. The Package RBF rule cannot be loosened to only\nallow\nthe child to have new unconfirmed inputs, either, because it can still\ncause us\nto overestimate the package's ancestor score.\n\nHowever, enforcing a rule analogous to BIP125#2 would not only make Package\nRBF\nless useful, but would also break Package RBF for packages with parents\nalready\nin the mempool: if a package parent has already been submitted, it would\nlook\nlike the child is spending a \"new\" unconfirmed input. In example K [18],\nwe're\nlooking to replace M1 with the entire package including P1, P2, and P3. We\nmust\nconsider the case where one of the parents is already in the mempool (in\nthis\ncase, P2), which means we must allow P3 to have new unconfirmed inputs.\nHowever,\nM2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace M1\nwith this package.\n\nThus, the package RBF rule regarding new unconfirmed inputs is less strict\nthan\nBIP125#2. However, we still achieve the same goal of requiring the\nreplacement\ntransactions to have a ancestor score at least as high as the original\nones. As\na result, the entire package is required to be a higher feerate mining\ncandidate\nthan each of the replaced transactions.\n\nAnother note: the [comment][13] above the BIP125#2 code in the original RBF\nimplementation suggests that the rule was intended to be temporary.\n\n##### Absolute Fee (Rule #3)\n\nThe package must increase the absolute fee of the mempool, i.e. the total\nfees\nof the package must be higher than the absolute fees of the mempool\ntransactions\nit replaces. Combined with the CPFP rule above, this differs from BIP125\nRule #3\n- an individual transaction in the package may have lower fees than the\n  transaction(s) it is replacing. In fact, it may have 0 fees, and the child\npays for RBF.\n\n##### Feerate (Rule #4)\n\nThe package must pay for its own bandwidth; the package feerate must be\nhigher\nthan the replaced transactions by at least minimum relay feerate\n(`incrementalRelayFee`). Combined with the CPFP rule above, this differs\nfrom\nBIP125 Rule #4 - an individual transaction in the package can have a lower\nfeerate than the transaction(s) it is replacing. In fact, it may have 0\nfees,\nand the child pays for RBF.\n\n##### Total Number of Replaced Transactions (Rule #5)\n\nThe package cannot replace more than 100 mempool transactions. This is\nidentical\nto BIP125 Rule #5.\n\n### Expected FAQs\n\n1. Is it possible for only some of the package to make it into the mempool?\n\n   Yes, it is. However, since we evict transactions from the mempool by\ndescendant score and the package child is supposed to be sponsoring the\nfees of\nits parents, the most common scenario would be all-or-nothing. This is\nincentive-compatible. In fact, to be conservative, package validation should\nbegin by trying to submit all of the transactions individually, and only\nuse the\npackage mempool acceptance logic if the parents fail due to low feerate.\n\n2. Should we allow packages to contain already-confirmed transactions?\n\n    No, for practical reasons. In mempool validation, we actually aren't\nable to\ntell with 100% confidence if we are looking at a transaction that has\nalready\nconfirmed, because we look up inputs using a UTXO set. If we have historical\nblock data, it's possible to look for it, but this is inefficient, not\nalways\npossible for pruning nodes, and unnecessary because we're not going to do\nanything with the transaction anyway. As such, we already have the\nexpectation\nthat transaction relay is somewhat \"stateful\" i.e. nobody should be relaying\ntransactions that have already been confirmed. Similarly, we shouldn't be\nrelaying packages that contain already-confirmed transactions.\n\n[1]: https://github.com/bitcoin/bitcoin/pull/22290\n[2]:\nhttps://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n[3]:\nhttps://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n[4]: https://github.com/bitcoin/bitcoin/pull/16400\n[5]: https://github.com/bitcoin/bitcoin/pull/21062\n[6]: https://github.com/bitcoin/bitcoin/pull/22675\n[7]: https://github.com/bitcoin/bitcoin/pull/22796\n[8]: https://github.com/bitcoin/bitcoin/pull/20833\n[9]: https://github.com/bitcoin/bitcoin/pull/21800\n[10]: https://github.com/bitcoin/bitcoin/pull/16401\n[11]: https://github.com/bitcoin/bitcoin/pull/19621\n[12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n[13]:\nhttps://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n[14]:\nhttps://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n[15]:\nhttps://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n[16]:\nhttps://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n[17]:\nhttps://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n[18]:\nhttps://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n[19]:\nhttps://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n[20]:\nhttps://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/d71208a2/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-19T23:16:45",
                "message_text_only": "Hi Gloria,\n\n> A package may contain transactions that are already in the mempool. We\n> remove\n> (\"deduplicate\") those transactions from the package for the purposes of\n> package\n> mempool acceptance. If a package is empty after deduplication, we do\n> nothing.\n\nIIUC, you have a package A+B+C submitted for acceptance and A is already in\nyour mempool. You trim out A from the package and then evaluate B+C.\n\nI think this might be an issue if A is the higher-fee element of the ABC\npackage. B+C package fees might be under the mempool min fee and will be\nrejected, potentially breaking the acceptance expectations of the package\nissuer ?\n\nFurther, I think the dedup should be done on wtxid, as you might have\nmultiple valid witnesses. Though with varying vsizes and as such offering\ndifferent feerates.\n\nE.g you're going to evaluate the package A+B and A' is already in your\nmempool with a bigger valid witness. You trim A based on txid, then you\nevaluate A'+B, which fails the fee checks. However, evaluating A+B would\nhave been a success.\n\nAFAICT, the dedup rationale would be to save on CPU time/IO disk, to avoid\nrepeated signatures verification and parent UTXOs fetches ? Can we achieve\nthe same goal by bypassing tx-level checks for already-in txn while\nconserving the package integrity for package-level checks ?\n\n> Note that it's possible for the parents to be\n> indirect\n> descendants/ancestors of one another, or for parent and child to share a\n> parent,\n> so we cannot make any other topology assumptions.\n\nI'm not clearly understanding the accepted topologies. By \"parent and child\nto share a parent\", do you mean the set of transactions A, B, C, where B is\nspending A and C is spending A and B would be correct ?\n\nIf yes, is there a width-limit introduced or we fallback on\nMAX_PACKAGE_COUNT=25 ?\n\nIIRC, one rationale to come with this topology limitation was to lower the\nDoS risks when potentially deploying p2p packages.\n\nConsidering the current Core's mempool acceptance rules, I think CPFP\nbatching is unsafe for LN time-sensitive closure. A malicious tx-relay\njamming successful on one channel commitment transaction would contamine\nthe remaining commitments sharing the same package.\n\nE.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\ntransactions and E a shared CPFP. If a malicious A' transaction has a\nbetter feerate than A, the whole package acceptance will fail. Even if A'\nconfirms in the following block,\nthe propagation and confirmation of B+C+D have been delayed. This could\ncarry on a loss of funds.\n\nThat said, if you're broadcasting commitment transactions without\ntime-sensitive HTLC outputs, I think the batching is effectively a fee\nsaving as you don't have to duplicate the CPFP.\n\nIMHO, I'm leaning towards deploying during a first phase 1-parent/1-child.\nI think it's the most conservative step still improving second-layer safety.\n\n> *Rationale*:  It would be incorrect to use the fees of transactions that\nare\n> already in the mempool, as we do not want a transaction's fees to be\n> double-counted for both its individual RBF and package RBF.\n\nI'm unsure about the logical order of the checks proposed.\n\nIf A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats and\nA' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\nfails. For this reason I think the individual RBF should be bypassed and\nonly the package RBF apply ?\n\nNote this situation is plausible, with current LN design, your counterparty\ncan have a commitment transaction with a better fee just by selecting a\nhigher `dust_limit_satoshis` than yours.\n\n> Examples F and G [14] show the same package, but P1 is submitted\n> individually before\n> the package in example G. In example F, we can see that the 300vB package\n> pays\n> an additional 200sat in fees, which is not enough to pay for its own\n> bandwidth\n> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\nbut\n> using P1's fees again during package submission would make it look like a\n> 300sat\n> increase for a 200vB package. Even including its fees and size would not\nbe\n> sufficient in this example, since the 300sat looks like enough for the\n300vB\n> package. The calculcation after deduplication is 100sat increase for a\n> package\n> of size 200vB, which correctly fails BIP125#4. Assume all transactions\nhave\n> a\n> size of 100vB.\n\nWhat problem are you trying to solve by the package feerate *after* dedup\nrule ?\n\nMy understanding is that an in-package transaction might be already in the\nmempool. Therefore, to compute a correct RBF penalty replacement, the vsize\nof this transaction could be discarded lowering the cost of package RBF.\n\nIf we keep a \"safe\" dedup mechanism (see my point above), I think this\ndiscount is justified, as the validation cost of node operators is paid for\n?\n\n> The child cannot replace mempool transactions.\n\nLet's say you issue package A+B, then package C+B', where B' is a child of\nboth A and C. This rule fails the acceptance of C+B' ?\n\nI think this is a footgunish API, as if a package issuer send the\nmultiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\nThen try to broadcast the higher-feerate C'+D' package, it should be\nrejected. So it's breaking the naive broadcaster assumption that a\nhigher-feerate/higher-fee package always replaces ? And it might be unsafe\nin protocols where states are symmetric. E.g a malicious counterparty\nbroadcasts first S+A, then you honestly broadcast S+B, where B pays better\nfees.\n\n> All mempool transactions to be replaced must signal replaceability.\n\nI think this is unsafe for L2s if counterparties have malleability of the\nchild transaction. They can block your package replacement by opting-out\nfrom RBF signaling. IIRC, LN's \"anchor output\" presents such an ability.\n\nI think it's better to either fix inherited signaling or move towards\nfull-rbf.\n\n> if a package parent has already been submitted, it would\n> look\n>like the child is spending a \"new\" unconfirmed input.\n\nI think this is an issue brought by the trimming during the dedup phase. If\nwe preserve the package integrity, only re-using the tx-level checks\nresults of already in-mempool transactions to gain in CPU time we won't\nhave this issue. Package childs can add unconfirmed inputs as long as\nthey're in-package, the bip125 rule2 is only evaluated against parents ?\n\n> However, we still achieve the same goal of requiring the\n> replacement\n> transactions to have a ancestor score at least as high as the original\n> ones.\n\nI'm not sure if this holds...\n\nLet's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes\nand B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D\nspending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1\nsat/vb for 1000 vbytes.\n\nPackage A + B ancestor score is 10 sat/vb.\n\nD has a higher feerate/absolute fee than B.\n\nPackage A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000\nsats + D's 1500 sats) /\nA's 100 vb + C's 1000 vb + D's 100 vb)\n\nOverall, this is a review through the lenses of LN requirements. I think\nother L2 protocols/applications\ncould be candidates to using package accept/relay such as:\n* https://github.com/lightninglabs/pool\n* https://github.com/discreetlogcontracts/dlcspecs\n* https://github.com/bitcoin-teleport/teleport-transactions/\n* https://github.com/sapio-lang/sapio\n* https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n* https://github.com/revault/practical-revault\n\nThanks for rolling forward the ball on this subject.\n\nAntoine\n\nLe jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi there,\n>\n> I'm writing to propose a set of mempool policy changes to enable package\n> validation (in preparation for package relay) in Bitcoin Core. These would\n> not\n> be consensus or P2P protocol changes. However, since mempool policy\n> significantly affects transaction propagation, I believe this is relevant\n> for\n> the mailing list.\n>\n> My proposal enables packages consisting of multiple parents and 1 child.\n> If you\n> develop software that relies on specific transaction relay assumptions\n> and/or\n> are interested in using package relay in the future, I'm very interested\n> to hear\n> your feedback on the utility or restrictiveness of these package policies\n> for\n> your use cases.\n>\n> A draft implementation of this proposal can be found in [Bitcoin Core\n> PR#22290][1].\n>\n> An illustrated version of this post can be found at\n> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n> I have also linked the images below.\n>\n> ## Background\n>\n> Feel free to skip this section if you are already familiar with mempool\n> policy\n> and package relay terminology.\n>\n> ### Terminology Clarifications\n>\n> * Package = an ordered list of related transactions, representable by a\n> Directed\n>   Acyclic Graph.\n> * Package Feerate = the total modified fees divided by the total virtual\n> size of\n>   all transactions in the package.\n>     - Modified fees = a transaction's base fees + fee delta applied by the\n> user\n>       with `prioritisetransaction`. As such, we expect this to vary across\n> mempools.\n>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141\n>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n> Core][3].\n>     - Note that feerate is not necessarily based on the base fees and\n> serialized\n>       size.\n>\n> * Fee-Bumping = user/wallet actions that take advantage of miner\n> incentives to\n>   boost a transaction's candidacy for inclusion in a block, including\n> Child Pays\n> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in\n> mempool policy is to recognize when the new transaction is more economical\n> to\n> mine than the original one(s) but not open DoS vectors, so there are some\n> limitations.\n>\n> ### Policy\n>\n> The purpose of the mempool is to store the best (to be most\n> incentive-compatible\n> with miners, highest feerate) candidates for inclusion in a block. Miners\n> use\n> the mempool to build block templates. The mempool is also useful as a\n> cache for\n> boosting block relay and validation performance, aiding transaction relay,\n> and\n> generating feerate estimations.\n>\n> Ideally, all consensus-valid transactions paying reasonable fees should\n> make it\n> to miners through normal transaction relay, without any special\n> connectivity or\n> relationships with miners. On the other hand, nodes do not have unlimited\n> resources, and a P2P network designed to let any honest node broadcast\n> their\n> transactions also exposes the transaction validation engine to DoS attacks\n> from\n> malicious peers.\n>\n> As such, for unconfirmed transactions we are considering for our mempool,\n> we\n> apply a set of validation rules in addition to consensus, primarily to\n> protect\n> us from resource exhaustion and aid our efforts to keep the highest fee\n> transactions. We call this mempool _policy_: a set of (configurable,\n> node-specific) rules that transactions must abide by in order to be\n> accepted\n> into our mempool. Transaction \"Standardness\" rules and mempool\n> restrictions such\n> as \"too-long-mempool-chain\" are both examples of policy.\n>\n> ### Package Relay and Package Mempool Accept\n>\n> In transaction relay, we currently consider transactions one at a time for\n> submission to the mempool. This creates a limitation in the node's ability\n> to\n> determine which transactions have the highest feerates, since we cannot\n> take\n> into account descendants (i.e. cannot use CPFP) until all the transactions\n> are\n> in the mempool. Similarly, we cannot use a transaction's descendants when\n> considering it for RBF. When an individual transaction does not meet the\n> mempool\n> minimum feerate and the user isn't able to create a replacement transaction\n> directly, it will not be accepted by mempools.\n>\n> This limitation presents a security issue for applications and users\n> relying on\n> time-sensitive transactions. For example, Lightning and other protocols\n> create\n> UTXOs with multiple spending paths, where one counterparty's spending path\n> opens\n> up after a timelock, and users are protected from cheating scenarios as\n> long as\n> they redeem on-chain in time. A key security assumption is that all\n> parties'\n> transactions will propagate and confirm in a timely manner. This\n> assumption can\n> be broken if fee-bumping does not work as intended.\n>\n> The end goal for Package Relay is to consider multiple transactions at the\n> same\n> time, e.g. a transaction with its high-fee child. This may help us better\n> determine whether transactions should be accepted to our mempool,\n> especially if\n> they don't meet fee requirements individually or are better RBF candidates\n> as a\n> package. A combination of changes to mempool validation logic, policy, and\n> transaction relay allows us to better propagate the transactions with the\n> highest package feerates to miners, and makes fee-bumping tools more\n> powerful\n> for users.\n>\n> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n> large\n> part of the changes are in the mempool's package validation logic. We call\n> this\n> *Package Mempool Accept*.\n>\n> ### Previous Work\n>\n> * Given that mempool validation is DoS-sensitive and complex, it would be\n>   dangerous to haphazardly tack on package validation logic. Many efforts\n> have\n> been made to make mempool validation less opaque (see [#16400][4],\n> [#21062][5],\n> [#22675][6], [#22796][7]).\n> * [#20833][8] Added basic capabilities for package validation, test\n> accepts only\n>   (no submission to mempool).\n> * [#21800][9] Implemented package ancestor/descendant limit checks for\n> arbitrary\n>   packages. Still test accepts only.\n> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>\n> ### Existing Package Rules\n>\n> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n> consider\n> them as \"given\" in the rest of this document, though they can be changed,\n> since\n> package validation is test-accept only right now.\n>\n> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>\n>    *Rationale*: This is already enforced as mempool ancestor/descendant\n> limits.\n> Presumably, transactions in a package are all related, so exceeding this\n> limit\n> would mean that the package can either be split up or it wouldn't pass this\n> mempool policy.\n>\n> 2. Packages must be topologically sorted: if any dependencies exist between\n> transactions, parents must appear somewhere before children. [8]\n>\n> 3. A package cannot have conflicting transactions, i.e. none of them can\n> spend\n> the same inputs. This also means there cannot be duplicate transactions.\n> [8]\n>\n> 4. When packages are evaluated against ancestor/descendant limits in a test\n> accept, the union of all of their descendants and ancestors is considered.\n> This\n> is essentially a \"worst case\" heuristic where every transaction in the\n> package\n> is treated as each other's ancestor and descendant. [8]\n> Packages for which ancestor/descendant limits are accurately captured by\n> this\n> heuristic: [19]\n>\n> There are also limitations such as the fact that CPFP carve out is not\n> applied\n> to package transactions. #20833 also disables RBF in package validation;\n> this\n> proposal overrides that to allow packages to use RBF.\n>\n> ## Proposed Changes\n>\n> The next step in the Package Mempool Accept project is to implement\n> submission\n> to mempool, initially through RPC only. This allows us to test the\n> submission\n> logic before exposing it on P2P.\n>\n> ### Summary\n>\n> - Packages may contain already-in-mempool transactions.\n> - Packages are 2 generations, Multi-Parent-1-Child.\n> - Fee-related checks use the package feerate. This means that wallets can\n> create a package that utilizes CPFP.\n> - Parents are allowed to RBF mempool transactions with a set of rules\n> similar\n>   to BIP125. This enables a combination of CPFP and RBF, where a\n> transaction's descendant fees pay for replacing mempool conflicts.\n>\n> There is a draft implementation in [#22290][1]. It is WIP, but feedback is\n> always welcome.\n>\n> ### Details\n>\n> #### Packages May Contain Already-in-Mempool Transactions\n>\n> A package may contain transactions that are already in the mempool. We\n> remove\n> (\"deduplicate\") those transactions from the package for the purposes of\n> package\n> mempool acceptance. If a package is empty after deduplication, we do\n> nothing.\n>\n> *Rationale*: Mempools vary across the network. It's possible for a parent\n> to be\n> accepted to the mempool of a peer on its own due to differences in policy\n> and\n> fee market fluctuations. We should not reject or penalize the entire\n> package for\n> an individual transaction as that could be a censorship vector.\n>\n> #### Packages Are Multi-Parent-1-Child\n>\n> Only packages of a specific topology are permitted. Namely, a package is\n> exactly\n> 1 child with all of its unconfirmed parents. After deduplication, the\n> package\n> may be exactly the same, empty, 1 child, 1 child with just some of its\n> unconfirmed parents, etc. Note that it's possible for the parents to be\n> indirect\n> descendants/ancestors of one another, or for parent and child to share a\n> parent,\n> so we cannot make any other topology assumptions.\n>\n> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple parents\n> makes it possible to fee-bump a batch of transactions. Restricting\n> packages to a\n> defined topology is also easier to reason about and simplifies the\n> validation\n> logic greatly. Multi-parent-1-child allows us to think of the package as\n> one big\n> transaction, where:\n>\n> - Inputs = all the inputs of parents + inputs of the child that come from\n>   confirmed UTXOs\n> - Outputs = all the outputs of the child + all outputs of the parents that\n>   aren't spent by other transactions in the package\n>\n> Examples of packages that follow this rule (variations of example A show\n> some\n> possibilities after deduplication): ![image][15]\n>\n> #### Fee-Related Checks Use Package Feerate\n>\n> Package Feerate = the total modified fees divided by the total virtual\n> size of\n> all transactions in the package.\n>\n> To meet the two feerate requirements of a mempool, i.e., the pre-configured\n> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n> feerate, the\n> total package feerate is used instead of the individual feerate. The\n> individual\n> transactions are allowed to be below feerate requirements if the package\n> meets\n> the feerate requirements. For example, the parent(s) in the package can\n> have 0\n> fees but be paid for by the child.\n>\n> *Rationale*: This can be thought of as \"CPFP within a package,\" solving the\n> issue of a parent not meeting minimum fees on its own. This allows L2\n> applications to adjust their fees at broadcast time instead of\n> overshooting or\n> risking getting stuck/pinned.\n>\n> We use the package feerate of the package *after deduplication*.\n>\n> *Rationale*:  It would be incorrect to use the fees of transactions that\n> are\n> already in the mempool, as we do not want a transaction's fees to be\n> double-counted for both its individual RBF and package RBF.\n>\n> Examples F and G [14] show the same package, but P1 is submitted\n> individually before\n> the package in example G. In example F, we can see that the 300vB package\n> pays\n> an additional 200sat in fees, which is not enough to pay for its own\n> bandwidth\n> (BIP125#4). In example G, we can see that P1 pays enough to replace M1, but\n> using P1's fees again during package submission would make it look like a\n> 300sat\n> increase for a 200vB package. Even including its fees and size would not be\n> sufficient in this example, since the 300sat looks like enough for the\n> 300vB\n> package. The calculcation after deduplication is 100sat increase for a\n> package\n> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n> have a\n> size of 100vB.\n>\n> #### Package RBF\n>\n> If a package meets feerate requirements as a package, the parents in the\n> transaction are allowed to replace-by-fee mempool transactions. The child\n> cannot\n> replace mempool transactions. Multiple transactions can replace the same\n> transaction, but in order to be valid, none of the transactions can try to\n> replace an ancestor of another transaction in the same package (which\n> would thus\n> make its inputs unavailable).\n>\n> *Rationale*: Even if we are using package feerate, a package will not\n> propagate\n> as intended if RBF still requires each individual transaction to meet the\n> feerate requirements.\n>\n> We use a set of rules slightly modified from BIP125 as follows:\n>\n> ##### Signaling (Rule #1)\n>\n> All mempool transactions to be replaced must signal replaceability.\n>\n> *Rationale*: Package RBF signaling logic should be the same for package\n> RBF and\n> single transaction acceptance. This would be updated if single transaction\n> validation moves to full RBF.\n>\n> ##### New Unconfirmed Inputs (Rule #2)\n>\n> A package may include new unconfirmed inputs, but the ancestor feerate of\n> the\n> child must be at least as high as the ancestor feerates of every\n> transaction\n> being replaced. This is contrary to BIP125#2, which states \"The replacement\n> transaction may only include an unconfirmed input if that input was\n> included in\n> one of the original transactions. (An unconfirmed input spends an output\n> from a\n> currently-unconfirmed transaction.)\"\n>\n> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n> transaction has a higher ancestor score than the original transaction(s)\n> (see\n> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n> can lower the\n> ancestor score of the replacement transaction. P1 is trying to replace M1,\n> and\n> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and M2\n> pays\n> 100sat. Assume all transactions have a size of 100vB. While, in isolation,\n> P1\n> looks like a better mining candidate than M1, it must be mined with M2, so\n> its\n> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor\n> feerate, which is 6sat/vB.\n>\n> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n> transactions in the package can spend new unconfirmed inputs.\" Example J\n> [17] shows\n> why, if any of the package transactions have ancestors, package feerate is\n> no\n> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is\n> the\n> replacement transaction in an RBF), we're actually interested in the entire\n> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n> P2, and\n> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to only\n> allow\n> the child to have new unconfirmed inputs, either, because it can still\n> cause us\n> to overestimate the package's ancestor score.\n>\n> However, enforcing a rule analogous to BIP125#2 would not only make\n> Package RBF\n> less useful, but would also break Package RBF for packages with parents\n> already\n> in the mempool: if a package parent has already been submitted, it would\n> look\n> like the child is spending a \"new\" unconfirmed input. In example K [18],\n> we're\n> looking to replace M1 with the entire package including P1, P2, and P3. We\n> must\n> consider the case where one of the parents is already in the mempool (in\n> this\n> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n> However,\n> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace\n> M1\n> with this package.\n>\n> Thus, the package RBF rule regarding new unconfirmed inputs is less strict\n> than\n> BIP125#2. However, we still achieve the same goal of requiring the\n> replacement\n> transactions to have a ancestor score at least as high as the original\n> ones. As\n> a result, the entire package is required to be a higher feerate mining\n> candidate\n> than each of the replaced transactions.\n>\n> Another note: the [comment][13] above the BIP125#2 code in the original RBF\n> implementation suggests that the rule was intended to be temporary.\n>\n> ##### Absolute Fee (Rule #3)\n>\n> The package must increase the absolute fee of the mempool, i.e. the total\n> fees\n> of the package must be higher than the absolute fees of the mempool\n> transactions\n> it replaces. Combined with the CPFP rule above, this differs from BIP125\n> Rule #3\n> - an individual transaction in the package may have lower fees than the\n>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n> child\n> pays for RBF.\n>\n> ##### Feerate (Rule #4)\n>\n> The package must pay for its own bandwidth; the package feerate must be\n> higher\n> than the replaced transactions by at least minimum relay feerate\n> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs\n> from\n> BIP125 Rule #4 - an individual transaction in the package can have a lower\n> feerate than the transaction(s) it is replacing. In fact, it may have 0\n> fees,\n> and the child pays for RBF.\n>\n> ##### Total Number of Replaced Transactions (Rule #5)\n>\n> The package cannot replace more than 100 mempool transactions. This is\n> identical\n> to BIP125 Rule #5.\n>\n> ### Expected FAQs\n>\n> 1. Is it possible for only some of the package to make it into the mempool?\n>\n>    Yes, it is. However, since we evict transactions from the mempool by\n> descendant score and the package child is supposed to be sponsoring the\n> fees of\n> its parents, the most common scenario would be all-or-nothing. This is\n> incentive-compatible. In fact, to be conservative, package validation\n> should\n> begin by trying to submit all of the transactions individually, and only\n> use the\n> package mempool acceptance logic if the parents fail due to low feerate.\n>\n> 2. Should we allow packages to contain already-confirmed transactions?\n>\n>     No, for practical reasons. In mempool validation, we actually aren't\n> able to\n> tell with 100% confidence if we are looking at a transaction that has\n> already\n> confirmed, because we look up inputs using a UTXO set. If we have\n> historical\n> block data, it's possible to look for it, but this is inefficient, not\n> always\n> possible for pruning nodes, and unnecessary because we're not going to do\n> anything with the transaction anyway. As such, we already have the\n> expectation\n> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n> relaying\n> transactions that have already been confirmed. Similarly, we shouldn't be\n> relaying packages that contain already-confirmed transactions.\n>\n> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n> [2]:\n> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n> [3]:\n> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n> [13]:\n> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n> [14]:\n> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n> [15]:\n> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n> [16]:\n> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n> [17]:\n> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n> [18]:\n> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n> [19]:\n> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n> [20]:\n> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210919/f11e1976/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2021-09-20T15:10:14",
                "message_text_only": "Hi Antoine,\n\nFirst of all, thank you for the thorough review. I appreciate your insight\non LN requirements.\n\n> IIUC, you have a package A+B+C submitted for acceptance and A is already\nin your mempool. You trim out A from the package and then evaluate B+C.\n\n> I think this might be an issue if A is the higher-fee element of the ABC\npackage. B+C package fees might be under the mempool min fee and will be\nrejected, potentially breaking the acceptance expectations of the package\nissuer ?\n\nCorrect, if B+C is too low feerate to be accepted, we will reject it. I\nprefer this because it is incentive compatible: A can be mined by itself,\nso there's no reason to prefer A+B+C instead of A.\nAs another way of looking at this, consider the case where we do accept\nA+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\ncapacity, we evict the lowest descendant feerate transactions, which are\nB+C in this case. This gives us the same resulting mempool, with A and not\nB+C.\n\n\n> Further, I think the dedup should be done on wtxid, as you might have\nmultiple valid witnesses. Though with varying vsizes and as such offering\ndifferent feerates.\n\nI agree that variations of the same package with different witnesses is a\ncase that must be handled. I consider witness replacement to be a project\nthat can be done in parallel to package mempool acceptance because being\nable to accept packages does not worsen the problem of a\nsame-txid-different-witness \"pinning\" attack.\n\nIf or when we have witness replacement, the logic is: if the individual\ntransaction is enough to replace the mempool one, the replacement will\nhappen during the preceding individual transaction acceptance, and\ndeduplication logic will work. Otherwise, we will try to deduplicate by\nwtxid, see that we need a package witness replacement, and use the package\nfeerate to evaluate whether this is economically rational.\n\nSee the #22290 \"handle package transactions already in mempool\" commit (\nhttps://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),\nwhich handles the case of same-txid-different-witness by simply using the\ntransaction in the mempool for now, with TODOs for what I just described.\n\n\n> I'm not clearly understanding the accepted topologies. By \"parent and\nchild to share a parent\", do you mean the set of transactions A, B, C,\nwhere B is spending A and C is spending A and B would be correct ?\n\nYes, that is what I meant. Yes, that would a valid package under these\nrules.\n\n> If yes, is there a width-limit introduced or we fallback on\nMAX_PACKAGE_COUNT=25 ?\n\nNo, there is no limit on connectivity other than \"child with all\nunconfirmed parents.\" We will enforce MAX_PACKAGE_COUNT=25 and child's\nin-mempool + in-package ancestor limits.\n\n\n> Considering the current Core's mempool acceptance rules, I think CPFP\nbatching is unsafe for LN time-sensitive closure. A malicious tx-relay\njamming successful on one channel commitment transaction would contamine\nthe remaining commitments sharing the same package.\n\n> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\ntransactions and E a shared CPFP. If a malicious A' transaction has a\nbetter feerate than A, the whole package acceptance will fail. Even if A'\nconfirms in the following block,\nthe propagation and confirmation of B+C+D have been delayed. This could\ncarry on a loss of funds.\n\nPlease note that A may replace A' even if A' has higher fees than A\nindividually, because the proposed package RBF utilizes the fees and size\nof the entire package. This just requires E to pay enough fees, although\nthis can be pretty high if there are also potential B' and C' competing\ncommitment transactions that we don't know about.\n\n\n> IMHO, I'm leaning towards deploying during a first phase\n1-parent/1-child. I think it's the most conservative step still improving\nsecond-layer safety.\n\nSo far, my understanding is that multi-parent-1-child is desired for\nbatched fee-bumping (\nhttps://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289) and\nI've also seen your response which I have less context on (\nhttps://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202). That\nbeing said, I am happy to create a new proposal for 1 parent + 1 child\n(which would be slightly simpler) and plan for moving to\nmulti-parent-1-child later if that is preferred. I am very interested in\nhearing feedback on that approach.\n\n\n> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats\nand A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\nfails. For this reason I think the individual RBF should be bypassed and\nonly the package RBF apply ?\n\nI think there is a misunderstanding here - let me describe what I'm\nproposing we'd do in this situation: we'll try individual submission for A,\nsee that it fails due to \"insufficient fees.\" Then, we'll try package\nvalidation for A+B and use package RBF. If A+B pays enough, it can still\nreplace A'. If A fails for a bad signature, we won't look at B or A+B. Does\nthis meet your expectations?\n\n\n> What problem are you trying to solve by the package feerate *after* dedup\nrule ?\n> My understanding is that an in-package transaction might be already in\nthe mempool. Therefore, to compute a correct RBF penalty replacement, the\nvsize of this transaction could be discarded lowering the cost of package\nRBF.\n\nI'm proposing that, when a transaction has already been submitted to\nmempool, we would ignore both its fees and vsize when calculating package\nfeerate. In example G2, we shouldn't count M1 fees after its submission to\nmempool, since M1's fees have already been used to pay for its individual\nbandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.\nWe also shouldn't count its vsize, since it has already been paid for.\n\n\n> I think this is a footgunish API, as if a package issuer send the\nmultiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\nThen try to broadcast the higher-feerate C'+D' package, it should be\nrejected. So it's breaking the naive broadcaster assumption that a\nhigher-feerate/higher-fee package always replaces ?\n\nNote that, if C' conflicts with C, it also conflicts with D, since D is a\ndescendant of C and would thus need to be evicted along with it.\nImplicitly, D' would not be in conflict with D.\nMore generally, this example is surprising to me because I didn't think\npackages would be used to fee-bump replaceable transactions. Do we want the\nchild to be able to replace mempool transactions as well? This can be\nimplemented with a bit of additional logic.\n\n> I think this is unsafe for L2s if counterparties have malleability of the\nchild transaction. They can block your package replacement by opting-out\nfrom RBF signaling. IIRC, LN's \"anchor output\" presents such an ability.\n\nI'm not sure what you mean? Let's say we have a package of parent A + child\nB, where A is supposed to replace a mempool transaction A'. Are you saying\nthat counterparties are able to malleate the package child B, or a child of\nA'? If they can malleate a child of A', that shouldn't matter as long as A'\nis signaling replacement. This would be handled identically with full RBF\nand what Core currently implements.\n\n> I think this is an issue brought by the trimming during the dedup phase.\nIf we preserve the package integrity, only re-using the tx-level checks\nresults of already in-mempool transactions to gain in CPU time we won't\nhave this issue. Package childs can add unconfirmed inputs as long as\nthey're in-package, the bip125 rule2 is only evaluated against parents ?\n\nSorry, I don't understand what you mean by \"preserve the package\nintegrity?\" Could you elaborate?\n\n> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes\nand B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D\nspending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1\nsat/vb for 1000 vbytes.\n\n> Package A + B ancestor score is 10 sat/vb.\n\n> D has a higher feerate/absolute fee than B.\n\n> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000\nsats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)\n\nI am in agreement with your calculations but unsure if we disagree on the\nexpected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\nancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\nit fails the proposed package RBF Rule #2, so this package would be\nrejected. Does this meet your expectations?\n\nThank you for linking to projects that might be interested in package relay\n:)\n\nThanks,\nGloria\n\nOn Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>\nwrote:\n\n> Hi Gloria,\n>\n> > A package may contain transactions that are already in the mempool. We\n> > remove\n> > (\"deduplicate\") those transactions from the package for the purposes of\n> > package\n> > mempool acceptance. If a package is empty after deduplication, we do\n> > nothing.\n>\n> IIUC, you have a package A+B+C submitted for acceptance and A is already\n> in your mempool. You trim out A from the package and then evaluate B+C.\n>\n> I think this might be an issue if A is the higher-fee element of the ABC\n> package. B+C package fees might be under the mempool min fee and will be\n> rejected, potentially breaking the acceptance expectations of the package\n> issuer ?\n>\n> Further, I think the dedup should be done on wtxid, as you might have\n> multiple valid witnesses. Though with varying vsizes and as such offering\n> different feerates.\n>\n> E.g you're going to evaluate the package A+B and A' is already in your\n> mempool with a bigger valid witness. You trim A based on txid, then you\n> evaluate A'+B, which fails the fee checks. However, evaluating A+B would\n> have been a success.\n>\n> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to avoid\n> repeated signatures verification and parent UTXOs fetches ? Can we achieve\n> the same goal by bypassing tx-level checks for already-in txn while\n> conserving the package integrity for package-level checks ?\n>\n> > Note that it's possible for the parents to be\n> > indirect\n> > descendants/ancestors of one another, or for parent and child to share a\n> > parent,\n> > so we cannot make any other topology assumptions.\n>\n> I'm not clearly understanding the accepted topologies. By \"parent and\n> child to share a parent\", do you mean the set of transactions A, B, C,\n> where B is spending A and C is spending A and B would be correct ?\n>\n> If yes, is there a width-limit introduced or we fallback on\n> MAX_PACKAGE_COUNT=25 ?\n>\n> IIRC, one rationale to come with this topology limitation was to lower the\n> DoS risks when potentially deploying p2p packages.\n>\n> Considering the current Core's mempool acceptance rules, I think CPFP\n> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n> jamming successful on one channel commitment transaction would contamine\n> the remaining commitments sharing the same package.\n>\n> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n> transactions and E a shared CPFP. If a malicious A' transaction has a\n> better feerate than A, the whole package acceptance will fail. Even if A'\n> confirms in the following block,\n> the propagation and confirmation of B+C+D have been delayed. This could\n> carry on a loss of funds.\n>\n> That said, if you're broadcasting commitment transactions without\n> time-sensitive HTLC outputs, I think the batching is effectively a fee\n> saving as you don't have to duplicate the CPFP.\n>\n> IMHO, I'm leaning towards deploying during a first phase 1-parent/1-child.\n> I think it's the most conservative step still improving second-layer safety.\n>\n> > *Rationale*:  It would be incorrect to use the fees of transactions that\n> are\n> > already in the mempool, as we do not want a transaction's fees to be\n> > double-counted for both its individual RBF and package RBF.\n>\n> I'm unsure about the logical order of the checks proposed.\n>\n> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats\n> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\n> fails. For this reason I think the individual RBF should be bypassed and\n> only the package RBF apply ?\n>\n> Note this situation is plausible, with current LN design, your\n> counterparty can have a commitment transaction with a better fee just by\n> selecting a higher `dust_limit_satoshis` than yours.\n>\n> > Examples F and G [14] show the same package, but P1 is submitted\n> > individually before\n> > the package in example G. In example F, we can see that the 300vB package\n> > pays\n> > an additional 200sat in fees, which is not enough to pay for its own\n> > bandwidth\n> > (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n> but\n> > using P1's fees again during package submission would make it look like a\n> > 300sat\n> > increase for a 200vB package. Even including its fees and size would not\n> be\n> > sufficient in this example, since the 300sat looks like enough for the\n> 300vB\n> > package. The calculcation after deduplication is 100sat increase for a\n> > package\n> > of size 200vB, which correctly fails BIP125#4. Assume all transactions\n> have\n> > a\n> > size of 100vB.\n>\n> What problem are you trying to solve by the package feerate *after* dedup\n> rule ?\n>\n> My understanding is that an in-package transaction might be already in the\n> mempool. Therefore, to compute a correct RBF penalty replacement, the vsize\n> of this transaction could be discarded lowering the cost of package RBF.\n>\n> If we keep a \"safe\" dedup mechanism (see my point above), I think this\n> discount is justified, as the validation cost of node operators is paid for\n> ?\n>\n> > The child cannot replace mempool transactions.\n>\n> Let's say you issue package A+B, then package C+B', where B' is a child of\n> both A and C. This rule fails the acceptance of C+B' ?\n>\n> I think this is a footgunish API, as if a package issuer send the\n> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n> Then try to broadcast the higher-feerate C'+D' package, it should be\n> rejected. So it's breaking the naive broadcaster assumption that a\n> higher-feerate/higher-fee package always replaces ? And it might be unsafe\n> in protocols where states are symmetric. E.g a malicious counterparty\n> broadcasts first S+A, then you honestly broadcast S+B, where B pays better\n> fees.\n>\n> > All mempool transactions to be replaced must signal replaceability.\n>\n> I think this is unsafe for L2s if counterparties have malleability of the\n> child transaction. They can block your package replacement by opting-out\n> from RBF signaling. IIRC, LN's \"anchor output\" presents such an ability.\n>\n> I think it's better to either fix inherited signaling or move towards\n> full-rbf.\n>\n> > if a package parent has already been submitted, it would\n> > look\n> >like the child is spending a \"new\" unconfirmed input.\n>\n> I think this is an issue brought by the trimming during the dedup phase.\n> If we preserve the package integrity, only re-using the tx-level checks\n> results of already in-mempool transactions to gain in CPU time we won't\n> have this issue. Package childs can add unconfirmed inputs as long as\n> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>\n> > However, we still achieve the same goal of requiring the\n> > replacement\n> > transactions to have a ancestor score at least as high as the original\n> > ones.\n>\n> I'm not sure if this holds...\n>\n> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes\n> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D\n> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1\n> sat/vb for 1000 vbytes.\n>\n> Package A + B ancestor score is 10 sat/vb.\n>\n> D has a higher feerate/absolute fee than B.\n>\n> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000\n> sats + D's 1500 sats) /\n> A's 100 vb + C's 1000 vb + D's 100 vb)\n>\n> Overall, this is a review through the lenses of LN requirements. I think\n> other L2 protocols/applications\n> could be candidates to using package accept/relay such as:\n> * https://github.com/lightninglabs/pool\n> * https://github.com/discreetlogcontracts/dlcspecs\n> * https://github.com/bitcoin-teleport/teleport-transactions/\n> * https://github.com/sapio-lang/sapio\n> * https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n> * https://github.com/revault/practical-revault\n>\n> Thanks for rolling forward the ball on this subject.\n>\n> Antoine\n>\n> Le jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi there,\n>>\n>> I'm writing to propose a set of mempool policy changes to enable package\n>> validation (in preparation for package relay) in Bitcoin Core. These\n>> would not\n>> be consensus or P2P protocol changes. However, since mempool policy\n>> significantly affects transaction propagation, I believe this is relevant\n>> for\n>> the mailing list.\n>>\n>> My proposal enables packages consisting of multiple parents and 1 child.\n>> If you\n>> develop software that relies on specific transaction relay assumptions\n>> and/or\n>> are interested in using package relay in the future, I'm very interested\n>> to hear\n>> your feedback on the utility or restrictiveness of these package policies\n>> for\n>> your use cases.\n>>\n>> A draft implementation of this proposal can be found in [Bitcoin Core\n>> PR#22290][1].\n>>\n>> An illustrated version of this post can be found at\n>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>> I have also linked the images below.\n>>\n>> ## Background\n>>\n>> Feel free to skip this section if you are already familiar with mempool\n>> policy\n>> and package relay terminology.\n>>\n>> ### Terminology Clarifications\n>>\n>> * Package = an ordered list of related transactions, representable by a\n>> Directed\n>>   Acyclic Graph.\n>> * Package Feerate = the total modified fees divided by the total virtual\n>> size of\n>>   all transactions in the package.\n>>     - Modified fees = a transaction's base fees + fee delta applied by\n>> the user\n>>       with `prioritisetransaction`. As such, we expect this to vary across\n>> mempools.\n>>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141\n>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>> Core][3].\n>>     - Note that feerate is not necessarily based on the base fees and\n>> serialized\n>>       size.\n>>\n>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>> incentives to\n>>   boost a transaction's candidacy for inclusion in a block, including\n>> Child Pays\n>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in\n>> mempool policy is to recognize when the new transaction is more\n>> economical to\n>> mine than the original one(s) but not open DoS vectors, so there are some\n>> limitations.\n>>\n>> ### Policy\n>>\n>> The purpose of the mempool is to store the best (to be most\n>> incentive-compatible\n>> with miners, highest feerate) candidates for inclusion in a block. Miners\n>> use\n>> the mempool to build block templates. The mempool is also useful as a\n>> cache for\n>> boosting block relay and validation performance, aiding transaction\n>> relay, and\n>> generating feerate estimations.\n>>\n>> Ideally, all consensus-valid transactions paying reasonable fees should\n>> make it\n>> to miners through normal transaction relay, without any special\n>> connectivity or\n>> relationships with miners. On the other hand, nodes do not have unlimited\n>> resources, and a P2P network designed to let any honest node broadcast\n>> their\n>> transactions also exposes the transaction validation engine to DoS\n>> attacks from\n>> malicious peers.\n>>\n>> As such, for unconfirmed transactions we are considering for our mempool,\n>> we\n>> apply a set of validation rules in addition to consensus, primarily to\n>> protect\n>> us from resource exhaustion and aid our efforts to keep the highest fee\n>> transactions. We call this mempool _policy_: a set of (configurable,\n>> node-specific) rules that transactions must abide by in order to be\n>> accepted\n>> into our mempool. Transaction \"Standardness\" rules and mempool\n>> restrictions such\n>> as \"too-long-mempool-chain\" are both examples of policy.\n>>\n>> ### Package Relay and Package Mempool Accept\n>>\n>> In transaction relay, we currently consider transactions one at a time for\n>> submission to the mempool. This creates a limitation in the node's\n>> ability to\n>> determine which transactions have the highest feerates, since we cannot\n>> take\n>> into account descendants (i.e. cannot use CPFP) until all the\n>> transactions are\n>> in the mempool. Similarly, we cannot use a transaction's descendants when\n>> considering it for RBF. When an individual transaction does not meet the\n>> mempool\n>> minimum feerate and the user isn't able to create a replacement\n>> transaction\n>> directly, it will not be accepted by mempools.\n>>\n>> This limitation presents a security issue for applications and users\n>> relying on\n>> time-sensitive transactions. For example, Lightning and other protocols\n>> create\n>> UTXOs with multiple spending paths, where one counterparty's spending\n>> path opens\n>> up after a timelock, and users are protected from cheating scenarios as\n>> long as\n>> they redeem on-chain in time. A key security assumption is that all\n>> parties'\n>> transactions will propagate and confirm in a timely manner. This\n>> assumption can\n>> be broken if fee-bumping does not work as intended.\n>>\n>> The end goal for Package Relay is to consider multiple transactions at\n>> the same\n>> time, e.g. a transaction with its high-fee child. This may help us better\n>> determine whether transactions should be accepted to our mempool,\n>> especially if\n>> they don't meet fee requirements individually or are better RBF\n>> candidates as a\n>> package. A combination of changes to mempool validation logic, policy, and\n>> transaction relay allows us to better propagate the transactions with the\n>> highest package feerates to miners, and makes fee-bumping tools more\n>> powerful\n>> for users.\n>>\n>> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n>> large\n>> part of the changes are in the mempool's package validation logic. We\n>> call this\n>> *Package Mempool Accept*.\n>>\n>> ### Previous Work\n>>\n>> * Given that mempool validation is DoS-sensitive and complex, it would be\n>>   dangerous to haphazardly tack on package validation logic. Many efforts\n>> have\n>> been made to make mempool validation less opaque (see [#16400][4],\n>> [#21062][5],\n>> [#22675][6], [#22796][7]).\n>> * [#20833][8] Added basic capabilities for package validation, test\n>> accepts only\n>>   (no submission to mempool).\n>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>> arbitrary\n>>   packages. Still test accepts only.\n>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>\n>> ### Existing Package Rules\n>>\n>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>> consider\n>> them as \"given\" in the rest of this document, though they can be changed,\n>> since\n>> package validation is test-accept only right now.\n>>\n>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>\n>>    *Rationale*: This is already enforced as mempool ancestor/descendant\n>> limits.\n>> Presumably, transactions in a package are all related, so exceeding this\n>> limit\n>> would mean that the package can either be split up or it wouldn't pass\n>> this\n>> mempool policy.\n>>\n>> 2. Packages must be topologically sorted: if any dependencies exist\n>> between\n>> transactions, parents must appear somewhere before children. [8]\n>>\n>> 3. A package cannot have conflicting transactions, i.e. none of them can\n>> spend\n>> the same inputs. This also means there cannot be duplicate transactions.\n>> [8]\n>>\n>> 4. When packages are evaluated against ancestor/descendant limits in a\n>> test\n>> accept, the union of all of their descendants and ancestors is\n>> considered. This\n>> is essentially a \"worst case\" heuristic where every transaction in the\n>> package\n>> is treated as each other's ancestor and descendant. [8]\n>> Packages for which ancestor/descendant limits are accurately captured by\n>> this\n>> heuristic: [19]\n>>\n>> There are also limitations such as the fact that CPFP carve out is not\n>> applied\n>> to package transactions. #20833 also disables RBF in package validation;\n>> this\n>> proposal overrides that to allow packages to use RBF.\n>>\n>> ## Proposed Changes\n>>\n>> The next step in the Package Mempool Accept project is to implement\n>> submission\n>> to mempool, initially through RPC only. This allows us to test the\n>> submission\n>> logic before exposing it on P2P.\n>>\n>> ### Summary\n>>\n>> - Packages may contain already-in-mempool transactions.\n>> - Packages are 2 generations, Multi-Parent-1-Child.\n>> - Fee-related checks use the package feerate. This means that wallets can\n>> create a package that utilizes CPFP.\n>> - Parents are allowed to RBF mempool transactions with a set of rules\n>> similar\n>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>> transaction's descendant fees pay for replacing mempool conflicts.\n>>\n>> There is a draft implementation in [#22290][1]. It is WIP, but feedback is\n>> always welcome.\n>>\n>> ### Details\n>>\n>> #### Packages May Contain Already-in-Mempool Transactions\n>>\n>> A package may contain transactions that are already in the mempool. We\n>> remove\n>> (\"deduplicate\") those transactions from the package for the purposes of\n>> package\n>> mempool acceptance. If a package is empty after deduplication, we do\n>> nothing.\n>>\n>> *Rationale*: Mempools vary across the network. It's possible for a parent\n>> to be\n>> accepted to the mempool of a peer on its own due to differences in policy\n>> and\n>> fee market fluctuations. We should not reject or penalize the entire\n>> package for\n>> an individual transaction as that could be a censorship vector.\n>>\n>> #### Packages Are Multi-Parent-1-Child\n>>\n>> Only packages of a specific topology are permitted. Namely, a package is\n>> exactly\n>> 1 child with all of its unconfirmed parents. After deduplication, the\n>> package\n>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>> unconfirmed parents, etc. Note that it's possible for the parents to be\n>> indirect\n>> descendants/ancestors of one another, or for parent and child to share a\n>> parent,\n>> so we cannot make any other topology assumptions.\n>>\n>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>> parents\n>> makes it possible to fee-bump a batch of transactions. Restricting\n>> packages to a\n>> defined topology is also easier to reason about and simplifies the\n>> validation\n>> logic greatly. Multi-parent-1-child allows us to think of the package as\n>> one big\n>> transaction, where:\n>>\n>> - Inputs = all the inputs of parents + inputs of the child that come from\n>>   confirmed UTXOs\n>> - Outputs = all the outputs of the child + all outputs of the parents that\n>>   aren't spent by other transactions in the package\n>>\n>> Examples of packages that follow this rule (variations of example A show\n>> some\n>> possibilities after deduplication): ![image][15]\n>>\n>> #### Fee-Related Checks Use Package Feerate\n>>\n>> Package Feerate = the total modified fees divided by the total virtual\n>> size of\n>> all transactions in the package.\n>>\n>> To meet the two feerate requirements of a mempool, i.e., the\n>> pre-configured\n>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>> feerate, the\n>> total package feerate is used instead of the individual feerate. The\n>> individual\n>> transactions are allowed to be below feerate requirements if the package\n>> meets\n>> the feerate requirements. For example, the parent(s) in the package can\n>> have 0\n>> fees but be paid for by the child.\n>>\n>> *Rationale*: This can be thought of as \"CPFP within a package,\" solving\n>> the\n>> issue of a parent not meeting minimum fees on its own. This allows L2\n>> applications to adjust their fees at broadcast time instead of\n>> overshooting or\n>> risking getting stuck/pinned.\n>>\n>> We use the package feerate of the package *after deduplication*.\n>>\n>> *Rationale*:  It would be incorrect to use the fees of transactions that\n>> are\n>> already in the mempool, as we do not want a transaction's fees to be\n>> double-counted for both its individual RBF and package RBF.\n>>\n>> Examples F and G [14] show the same package, but P1 is submitted\n>> individually before\n>> the package in example G. In example F, we can see that the 300vB package\n>> pays\n>> an additional 200sat in fees, which is not enough to pay for its own\n>> bandwidth\n>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n>> but\n>> using P1's fees again during package submission would make it look like a\n>> 300sat\n>> increase for a 200vB package. Even including its fees and size would not\n>> be\n>> sufficient in this example, since the 300sat looks like enough for the\n>> 300vB\n>> package. The calculcation after deduplication is 100sat increase for a\n>> package\n>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>> have a\n>> size of 100vB.\n>>\n>> #### Package RBF\n>>\n>> If a package meets feerate requirements as a package, the parents in the\n>> transaction are allowed to replace-by-fee mempool transactions. The child\n>> cannot\n>> replace mempool transactions. Multiple transactions can replace the same\n>> transaction, but in order to be valid, none of the transactions can try to\n>> replace an ancestor of another transaction in the same package (which\n>> would thus\n>> make its inputs unavailable).\n>>\n>> *Rationale*: Even if we are using package feerate, a package will not\n>> propagate\n>> as intended if RBF still requires each individual transaction to meet the\n>> feerate requirements.\n>>\n>> We use a set of rules slightly modified from BIP125 as follows:\n>>\n>> ##### Signaling (Rule #1)\n>>\n>> All mempool transactions to be replaced must signal replaceability.\n>>\n>> *Rationale*: Package RBF signaling logic should be the same for package\n>> RBF and\n>> single transaction acceptance. This would be updated if single transaction\n>> validation moves to full RBF.\n>>\n>> ##### New Unconfirmed Inputs (Rule #2)\n>>\n>> A package may include new unconfirmed inputs, but the ancestor feerate of\n>> the\n>> child must be at least as high as the ancestor feerates of every\n>> transaction\n>> being replaced. This is contrary to BIP125#2, which states \"The\n>> replacement\n>> transaction may only include an unconfirmed input if that input was\n>> included in\n>> one of the original transactions. (An unconfirmed input spends an output\n>> from a\n>> currently-unconfirmed transaction.)\"\n>>\n>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>> transaction has a higher ancestor score than the original transaction(s)\n>> (see\n>> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n>> can lower the\n>> ancestor score of the replacement transaction. P1 is trying to replace\n>> M1, and\n>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and\n>> M2 pays\n>> 100sat. Assume all transactions have a size of 100vB. While, in\n>> isolation, P1\n>> looks like a better mining candidate than M1, it must be mined with M2,\n>> so its\n>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor\n>> feerate, which is 6sat/vB.\n>>\n>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>> transactions in the package can spend new unconfirmed inputs.\" Example J\n>> [17] shows\n>> why, if any of the package transactions have ancestors, package feerate\n>> is no\n>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is\n>> the\n>> replacement transaction in an RBF), we're actually interested in the\n>> entire\n>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n>> P2, and\n>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>> only allow\n>> the child to have new unconfirmed inputs, either, because it can still\n>> cause us\n>> to overestimate the package's ancestor score.\n>>\n>> However, enforcing a rule analogous to BIP125#2 would not only make\n>> Package RBF\n>> less useful, but would also break Package RBF for packages with parents\n>> already\n>> in the mempool: if a package parent has already been submitted, it would\n>> look\n>> like the child is spending a \"new\" unconfirmed input. In example K [18],\n>> we're\n>> looking to replace M1 with the entire package including P1, P2, and P3.\n>> We must\n>> consider the case where one of the parents is already in the mempool (in\n>> this\n>> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n>> However,\n>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace\n>> M1\n>> with this package.\n>>\n>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>> strict than\n>> BIP125#2. However, we still achieve the same goal of requiring the\n>> replacement\n>> transactions to have a ancestor score at least as high as the original\n>> ones. As\n>> a result, the entire package is required to be a higher feerate mining\n>> candidate\n>> than each of the replaced transactions.\n>>\n>> Another note: the [comment][13] above the BIP125#2 code in the original\n>> RBF\n>> implementation suggests that the rule was intended to be temporary.\n>>\n>> ##### Absolute Fee (Rule #3)\n>>\n>> The package must increase the absolute fee of the mempool, i.e. the total\n>> fees\n>> of the package must be higher than the absolute fees of the mempool\n>> transactions\n>> it replaces. Combined with the CPFP rule above, this differs from BIP125\n>> Rule #3\n>> - an individual transaction in the package may have lower fees than the\n>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>> child\n>> pays for RBF.\n>>\n>> ##### Feerate (Rule #4)\n>>\n>> The package must pay for its own bandwidth; the package feerate must be\n>> higher\n>> than the replaced transactions by at least minimum relay feerate\n>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs\n>> from\n>> BIP125 Rule #4 - an individual transaction in the package can have a lower\n>> feerate than the transaction(s) it is replacing. In fact, it may have 0\n>> fees,\n>> and the child pays for RBF.\n>>\n>> ##### Total Number of Replaced Transactions (Rule #5)\n>>\n>> The package cannot replace more than 100 mempool transactions. This is\n>> identical\n>> to BIP125 Rule #5.\n>>\n>> ### Expected FAQs\n>>\n>> 1. Is it possible for only some of the package to make it into the\n>> mempool?\n>>\n>>    Yes, it is. However, since we evict transactions from the mempool by\n>> descendant score and the package child is supposed to be sponsoring the\n>> fees of\n>> its parents, the most common scenario would be all-or-nothing. This is\n>> incentive-compatible. In fact, to be conservative, package validation\n>> should\n>> begin by trying to submit all of the transactions individually, and only\n>> use the\n>> package mempool acceptance logic if the parents fail due to low feerate.\n>>\n>> 2. Should we allow packages to contain already-confirmed transactions?\n>>\n>>     No, for practical reasons. In mempool validation, we actually aren't\n>> able to\n>> tell with 100% confidence if we are looking at a transaction that has\n>> already\n>> confirmed, because we look up inputs using a UTXO set. If we have\n>> historical\n>> block data, it's possible to look for it, but this is inefficient, not\n>> always\n>> possible for pruning nodes, and unnecessary because we're not going to do\n>> anything with the transaction anyway. As such, we already have the\n>> expectation\n>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>> relaying\n>> transactions that have already been confirmed. Similarly, we shouldn't be\n>> relaying packages that contain already-confirmed transactions.\n>>\n>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>> [2]:\n>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>> [3]:\n>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>> [13]:\n>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>> [14]:\n>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>> [15]:\n>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>> [16]:\n>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>> [17]:\n>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>> [18]:\n>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>> [19]:\n>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>> [20]:\n>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210920/09a51edf/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-23T04:29:39",
                "message_text_only": "> Correct, if B+C is too low feerate to be accepted, we will reject it. I\n> prefer this because it is incentive compatible: A can be mined by itself,\n> so there's no reason to prefer A+B+C instead of A.\n> As another way of looking at this, consider the case where we do accept\n> A+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\n> capacity, we evict the lowest descendant feerate transactions, which are\n> B+C in this case. This gives us the same resulting mempool, with A and not\n> B+C.\n\nI agree here. Doing otherwise, we might evict other transactions mempool in\n`MempoolAccept::Finalize` with a higher-feerate than B+C while those\nevicted transactions are the most compelling for block construction.\n\nI thought at first missing this acceptance requirement would break a\nfee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is\nattached to a child signed with SIGHASH_ANYONECANPAY but in this case the\nchild fee is capturing the parent value. I can't think of other fee-bumping\nschemes potentially affected. If they do exist I would say they're wrong in\ntheir design assumptions.\n\n> If or when we have witness replacement, the logic is: if the individual\n> transaction is enough to replace the mempool one, the replacement will\n> happen during the preceding individual transaction acceptance, and\n> deduplication logic will work. Otherwise, we will try to deduplicate by\n> wtxid, see that we need a package witness replacement, and use the package\n> feerate to evaluate whether this is economically rational.\n\nIIUC, you have package A+B, during the dedup phase early in\n`AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\nand A' is higher feerate than A, you trim A and replace by A' ?\n\nI think this approach is safe, the one who appears unsafe to me is when A'\nhas a _lower_ feerate, even if A' is already accepted by our mempool ? In\nthat case iirc that would be a pinning.\n\nGood to see progress on witness replacement before we see usage of Taproot\ntree in the context of multi-party, where a malicious counterparty inflates\nits witness to jam a honest spending.\n\n(Note, the commit linked currently points nowhere :))\n\n\n> Please note that A may replace A' even if A' has higher fees than A\n> individually, because the proposed package RBF utilizes the fees and size\n> of the entire package. This just requires E to pay enough fees, although\n> this can be pretty high if there are also potential B' and C' competing\n> commitment transactions that we don't know about.\n\nAh right, if the package acceptance waives `PaysMoreThanConflicts` for the\nindividual check on A, the honest package should replace the pinning\nattempt. I've not fully parsed the proposed implementation yet.\n\nThough note, I think it's still unsafe for a Lightning\nmulti-commitment-broadcast-as-one-package as a malicious A' might have an\nabsolute fee higher than E. It sounds uneconomical for\nan attacker but I think it's not when you consider than you can \"batch\"\nattack against multiple honest counterparties. E.g, Mallory broadcast A' +\nB' + C' + D' where A' conflicts with Alice's honest package P1, B'\nconflicts with Bob's honest package P2, C' conflicts with Caroll's honest\npackage P3. And D' is a high-fee child of A' + B' + C'.\n\nIf D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs\nconfirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n\n> So far, my understanding is that multi-parent-1-child is desired for\n> batched fee-bumping (\n> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289) and\n> I've also seen your response which I have less context on (\n> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\nThat\n> being said, I am happy to create a new proposal for 1 parent + 1 child\n> (which would be slightly simpler) and plan for moving to\n> multi-parent-1-child later if that is preferred. I am very interested in\n> hearing feedback on that approach.\n\nI think batched fee-bumping is okay as long as you don't have\ntime-sensitive outputs encumbering your commitment transactions. For the\nreasons mentioned above, I think that's unsafe.\n\nWhat I'm worried about is  L2 developers, potentially not aware about all\nthe mempool subtleties blurring the difference and always batching their\nbroadcast by default.\n\nIMO, a good thing by restraining to 1-parent + 1 child,  we artificially\nconstraint L2 design space for now and minimize risks of unsafe usage of\nthe package API :)\n\nI think that's a point where it would be relevant to have the opinion of\nmore L2 devs.\n\n> I think there is a misunderstanding here - let me describe what I'm\n> proposing we'd do in this situation: we'll try individual submission for\nA,\n> see that it fails due to \"insufficient fees.\" Then, we'll try package\n> validation for A+B and use package RBF. If A+B pays enough, it can still\n> replace A'. If A fails for a bad signature, we won't look at B or A+B.\nDoes\n> this meet your expectations?\n\nYes there was a misunderstanding, I think this approach is correct, it's\nmore a question of performance. Do we assume that broadcasted packages are\n\"honest\" by default and that the parent(s) always need the child to pass\nthe fee checks, that way saving the processing of individual transactions\nwhich are expected to fail in 99% of cases or more ad hoc composition of\npackages at relay ?\n\nI think this point is quite dependent on the p2p packages format/logic\nwe'll end up on and that we should feel free to revisit it later ?\n\n\n> What problem are you trying to solve by the package feerate *after* dedup\nrule ?\n> My understanding is that an in-package transaction might be already in\nthe mempool. Therefore, to compute a correct RBF penalty replacement, the\nvsize of this transaction could be discarded lowering the cost of package\nRBF.\n\n> I'm proposing that, when a transaction has already been submitted to\n> mempool, we would ignore both its fees and vsize when calculating package\n> feerate.\n\nYes, if you receive A+B, and A is already in-mempoo, I agree you can\ndiscard its feerate as B should pay for all fees checked on its own. Where\nI'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\nhave a fee high enough to cover the bandwidth penalty replacement\n(`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n\nIf you have a second-layer like current Lightning, you might have a\ncounterparty commitment to replace and should always expect to have to pay\nfor parent replacement bandwidth.\n\nWhere a potential discount sounds interesting is when you have an univoque\nstate on the first-stage of transactions. E.g DLC's funding transaction\nwhich might be CPFP by any participant iirc.\n\n> Note that, if C' conflicts with C, it also conflicts with D, since D is a\n> descendant of C and would thus need to be evicted along with it.\n\nAh once again I think it's a misunderstanding without the code under my\neyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark\nfor potential eviction D and don't consider it for future conflicts in the\nrest of the package, I think D' `PreChecks` should be good ?\n\n> More generally, this example is surprising to me because I didn't think\n> packages would be used to fee-bump replaceable transactions. Do we want\nthe\n> child to be able to replace mempool transactions as well?\n\nIf we mean when you have replaceable A+B then A'+B' try to replace with a\nhigher-feerate ? I think that's exactly the case we need for Lightning as\nA+B is coming from Alice and A'+B' is coming from Bob :/\n\n> I'm not sure what you mean? Let's say we have a package of parent A +\nchild\n> B, where A is supposed to replace a mempool transaction A'. Are you saying\n> that counterparties are able to malleate the package child B, or a child\nof\n> A'?\n\nThe second option, a child of A', In the LN case I think the CPFP is\nattached on one's anchor output.\n\nI think it's good if we assume the\nsolve-conflicts-after-parent's`'PreChecks` mentioned above or fixing\ninherited signaling or full-rbf ?\n\n> Sorry, I don't understand what you mean by \"preserve the package\n> integrity?\" Could you elaborate?\n\nAfter thinking the relaxation about the \"new\" unconfirmed input is not\nlinked to trimming but I would say more to the multi-parent support.\n\nLet's say you have A+B trying to replace C+D where B is also spending\nalready in-mempool E. To succeed, you need to waive the no-new-unconfirmed\ninput as D isn't spending E.\n\nSo good, I think we agree on the problem description here.\n\n> I am in agreement with your calculations but unsure if we disagree on the\n> expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n> it fails the proposed package RBF Rule #2, so this package would be\n> rejected. Does this meet your expectations?\n\nWell what sounds odd to me, in my example, we fail D even if it has a\nhigher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\nfees are 4500 sats ?\n\nIs this compatible with a model where a miner prioritizes absolute fees\nover ancestor score, in the case that mempools aren't full-enough to\nfulfill a block ?\n\nLet me know if I can clarify a point.\n\nAntoine\n\nLe lun. 20 sept. 2021 \u00e0 11:10, Gloria Zhao <gloriajzhao at gmail.com> a \u00e9crit :\n\n>\n> Hi Antoine,\n>\n> First of all, thank you for the thorough review. I appreciate your insight\n> on LN requirements.\n>\n> > IIUC, you have a package A+B+C submitted for acceptance and A is already\n> in your mempool. You trim out A from the package and then evaluate B+C.\n>\n> > I think this might be an issue if A is the higher-fee element of the ABC\n> package. B+C package fees might be under the mempool min fee and will be\n> rejected, potentially breaking the acceptance expectations of the package\n> issuer ?\n>\n> Correct, if B+C is too low feerate to be accepted, we will reject it. I\n> prefer this because it is incentive compatible: A can be mined by itself,\n> so there's no reason to prefer A+B+C instead of A.\n> As another way of looking at this, consider the case where we do accept\n> A+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\n> capacity, we evict the lowest descendant feerate transactions, which are\n> B+C in this case. This gives us the same resulting mempool, with A and not\n> B+C.\n>\n>\n> > Further, I think the dedup should be done on wtxid, as you might have\n> multiple valid witnesses. Though with varying vsizes and as such offering\n> different feerates.\n>\n> I agree that variations of the same package with different witnesses is a\n> case that must be handled. I consider witness replacement to be a project\n> that can be done in parallel to package mempool acceptance because being\n> able to accept packages does not worsen the problem of a\n> same-txid-different-witness \"pinning\" attack.\n>\n> If or when we have witness replacement, the logic is: if the individual\n> transaction is enough to replace the mempool one, the replacement will\n> happen during the preceding individual transaction acceptance, and\n> deduplication logic will work. Otherwise, we will try to deduplicate by\n> wtxid, see that we need a package witness replacement, and use the package\n> feerate to evaluate whether this is economically rational.\n>\n> See the #22290 \"handle package transactions already in mempool\" commit (\n> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),\n> which handles the case of same-txid-different-witness by simply using the\n> transaction in the mempool for now, with TODOs for what I just described.\n>\n>\n> > I'm not clearly understanding the accepted topologies. By \"parent and\n> child to share a parent\", do you mean the set of transactions A, B, C,\n> where B is spending A and C is spending A and B would be correct ?\n>\n> Yes, that is what I meant. Yes, that would a valid package under these\n> rules.\n>\n> > If yes, is there a width-limit introduced or we fallback on\n> MAX_PACKAGE_COUNT=25 ?\n>\n> No, there is no limit on connectivity other than \"child with all\n> unconfirmed parents.\" We will enforce MAX_PACKAGE_COUNT=25 and child's\n> in-mempool + in-package ancestor limits.\n>\n>\n> > Considering the current Core's mempool acceptance rules, I think CPFP\n> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n> jamming successful on one channel commitment transaction would contamine\n> the remaining commitments sharing the same package.\n>\n> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n> transactions and E a shared CPFP. If a malicious A' transaction has a\n> better feerate than A, the whole package acceptance will fail. Even if A'\n> confirms in the following block,\n> the propagation and confirmation of B+C+D have been delayed. This could\n> carry on a loss of funds.\n>\n> Please note that A may replace A' even if A' has higher fees than A\n> individually, because the proposed package RBF utilizes the fees and size\n> of the entire package. This just requires E to pay enough fees, although\n> this can be pretty high if there are also potential B' and C' competing\n> commitment transactions that we don't know about.\n>\n>\n> > IMHO, I'm leaning towards deploying during a first phase\n> 1-parent/1-child. I think it's the most conservative step still improving\n> second-layer safety.\n>\n> So far, my understanding is that multi-parent-1-child is desired for\n> batched fee-bumping (\n> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289) and\n> I've also seen your response which I have less context on (\n> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n> That being said, I am happy to create a new proposal for 1 parent + 1 child\n> (which would be slightly simpler) and plan for moving to\n> multi-parent-1-child later if that is preferred. I am very interested in\n> hearing feedback on that approach.\n>\n>\n> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats\n> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\n> fails. For this reason I think the individual RBF should be bypassed and\n> only the package RBF apply ?\n>\n> I think there is a misunderstanding here - let me describe what I'm\n> proposing we'd do in this situation: we'll try individual submission for A,\n> see that it fails due to \"insufficient fees.\" Then, we'll try package\n> validation for A+B and use package RBF. If A+B pays enough, it can still\n> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does\n> this meet your expectations?\n>\n>\n> > What problem are you trying to solve by the package feerate *after*\n> dedup rule ?\n> > My understanding is that an in-package transaction might be already in\n> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n> vsize of this transaction could be discarded lowering the cost of package\n> RBF.\n>\n> I'm proposing that, when a transaction has already been submitted to\n> mempool, we would ignore both its fees and vsize when calculating package\n> feerate. In example G2, we shouldn't count M1 fees after its submission to\n> mempool, since M1's fees have already been used to pay for its individual\n> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.\n> We also shouldn't count its vsize, since it has already been paid for.\n>\n>\n> > I think this is a footgunish API, as if a package issuer send the\n> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n> Then try to broadcast the higher-feerate C'+D' package, it should be\n> rejected. So it's breaking the naive broadcaster assumption that a\n> higher-feerate/higher-fee package always replaces ?\n>\n> Note that, if C' conflicts with C, it also conflicts with D, since D is a\n> descendant of C and would thus need to be evicted along with it.\n> Implicitly, D' would not be in conflict with D.\n> More generally, this example is surprising to me because I didn't think\n> packages would be used to fee-bump replaceable transactions. Do we want the\n> child to be able to replace mempool transactions as well? This can be\n> implemented with a bit of additional logic.\n>\n> > I think this is unsafe for L2s if counterparties have malleability of\n> the child transaction. They can block your package replacement by\n> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n> ability.\n>\n> I'm not sure what you mean? Let's say we have a package of parent A +\n> child B, where A is supposed to replace a mempool transaction A'. Are you\n> saying that counterparties are able to malleate the package child B, or a\n> child of A'? If they can malleate a child of A', that shouldn't matter as\n> long as A' is signaling replacement. This would be handled identically with\n> full RBF and what Core currently implements.\n>\n> > I think this is an issue brought by the trimming during the dedup phase.\n> If we preserve the package integrity, only re-using the tx-level checks\n> results of already in-mempool transactions to gain in CPU time we won't\n> have this issue. Package childs can add unconfirmed inputs as long as\n> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>\n> Sorry, I don't understand what you mean by \"preserve the package\n> integrity?\" Could you elaborate?\n>\n> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes\n> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D\n> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1\n> sat/vb for 1000 vbytes.\n>\n> > Package A + B ancestor score is 10 sat/vb.\n>\n> > D has a higher feerate/absolute fee than B.\n>\n> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)\n>\n> I am in agreement with your calculations but unsure if we disagree on the\n> expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n> it fails the proposed package RBF Rule #2, so this package would be\n> rejected. Does this meet your expectations?\n>\n> Thank you for linking to projects that might be interested in package\n> relay :)\n>\n> Thanks,\n> Gloria\n>\n> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>\n> wrote:\n>\n>> Hi Gloria,\n>>\n>> > A package may contain transactions that are already in the mempool. We\n>> > remove\n>> > (\"deduplicate\") those transactions from the package for the purposes of\n>> > package\n>> > mempool acceptance. If a package is empty after deduplication, we do\n>> > nothing.\n>>\n>> IIUC, you have a package A+B+C submitted for acceptance and A is already\n>> in your mempool. You trim out A from the package and then evaluate B+C.\n>>\n>> I think this might be an issue if A is the higher-fee element of the ABC\n>> package. B+C package fees might be under the mempool min fee and will be\n>> rejected, potentially breaking the acceptance expectations of the package\n>> issuer ?\n>>\n>> Further, I think the dedup should be done on wtxid, as you might have\n>> multiple valid witnesses. Though with varying vsizes and as such offering\n>> different feerates.\n>>\n>> E.g you're going to evaluate the package A+B and A' is already in your\n>> mempool with a bigger valid witness. You trim A based on txid, then you\n>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would\n>> have been a success.\n>>\n>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to\n>> avoid repeated signatures verification and parent UTXOs fetches ? Can we\n>> achieve the same goal by bypassing tx-level checks for already-in txn while\n>> conserving the package integrity for package-level checks ?\n>>\n>> > Note that it's possible for the parents to be\n>> > indirect\n>> > descendants/ancestors of one another, or for parent and child to share a\n>> > parent,\n>> > so we cannot make any other topology assumptions.\n>>\n>> I'm not clearly understanding the accepted topologies. By \"parent and\n>> child to share a parent\", do you mean the set of transactions A, B, C,\n>> where B is spending A and C is spending A and B would be correct ?\n>>\n>> If yes, is there a width-limit introduced or we fallback on\n>> MAX_PACKAGE_COUNT=25 ?\n>>\n>> IIRC, one rationale to come with this topology limitation was to lower\n>> the DoS risks when potentially deploying p2p packages.\n>>\n>> Considering the current Core's mempool acceptance rules, I think CPFP\n>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>> jamming successful on one channel commitment transaction would contamine\n>> the remaining commitments sharing the same package.\n>>\n>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>> better feerate than A, the whole package acceptance will fail. Even if A'\n>> confirms in the following block,\n>> the propagation and confirmation of B+C+D have been delayed. This could\n>> carry on a loss of funds.\n>>\n>> That said, if you're broadcasting commitment transactions without\n>> time-sensitive HTLC outputs, I think the batching is effectively a fee\n>> saving as you don't have to duplicate the CPFP.\n>>\n>> IMHO, I'm leaning towards deploying during a first phase\n>> 1-parent/1-child. I think it's the most conservative step still improving\n>> second-layer safety.\n>>\n>> > *Rationale*:  It would be incorrect to use the fees of transactions\n>> that are\n>> > already in the mempool, as we do not want a transaction's fees to be\n>> > double-counted for both its individual RBF and package RBF.\n>>\n>> I'm unsure about the logical order of the checks proposed.\n>>\n>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats\n>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\n>> fails. For this reason I think the individual RBF should be bypassed and\n>> only the package RBF apply ?\n>>\n>> Note this situation is plausible, with current LN design, your\n>> counterparty can have a commitment transaction with a better fee just by\n>> selecting a higher `dust_limit_satoshis` than yours.\n>>\n>> > Examples F and G [14] show the same package, but P1 is submitted\n>> > individually before\n>> > the package in example G. In example F, we can see that the 300vB\n>> package\n>> > pays\n>> > an additional 200sat in fees, which is not enough to pay for its own\n>> > bandwidth\n>> > (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n>> but\n>> > using P1's fees again during package submission would make it look like\n>> a\n>> > 300sat\n>> > increase for a 200vB package. Even including its fees and size would\n>> not be\n>> > sufficient in this example, since the 300sat looks like enough for the\n>> 300vB\n>> > package. The calculcation after deduplication is 100sat increase for a\n>> > package\n>> > of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>> have\n>> > a\n>> > size of 100vB.\n>>\n>> What problem are you trying to solve by the package feerate *after* dedup\n>> rule ?\n>>\n>> My understanding is that an in-package transaction might be already in\n>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>> vsize of this transaction could be discarded lowering the cost of package\n>> RBF.\n>>\n>> If we keep a \"safe\" dedup mechanism (see my point above), I think this\n>> discount is justified, as the validation cost of node operators is paid for\n>> ?\n>>\n>> > The child cannot replace mempool transactions.\n>>\n>> Let's say you issue package A+B, then package C+B', where B' is a child\n>> of both A and C. This rule fails the acceptance of C+B' ?\n>>\n>> I think this is a footgunish API, as if a package issuer send the\n>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>> rejected. So it's breaking the naive broadcaster assumption that a\n>> higher-feerate/higher-fee package always replaces ? And it might be unsafe\n>> in protocols where states are symmetric. E.g a malicious counterparty\n>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better\n>> fees.\n>>\n>> > All mempool transactions to be replaced must signal replaceability.\n>>\n>> I think this is unsafe for L2s if counterparties have malleability of the\n>> child transaction. They can block your package replacement by opting-out\n>> from RBF signaling. IIRC, LN's \"anchor output\" presents such an ability.\n>>\n>> I think it's better to either fix inherited signaling or move towards\n>> full-rbf.\n>>\n>> > if a package parent has already been submitted, it would\n>> > look\n>> >like the child is spending a \"new\" unconfirmed input.\n>>\n>> I think this is an issue brought by the trimming during the dedup phase.\n>> If we preserve the package integrity, only re-using the tx-level checks\n>> results of already in-mempool transactions to gain in CPU time we won't\n>> have this issue. Package childs can add unconfirmed inputs as long as\n>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>\n>> > However, we still achieve the same goal of requiring the\n>> > replacement\n>> > transactions to have a ancestor score at least as high as the original\n>> > ones.\n>>\n>> I'm not sure if this holds...\n>>\n>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes\n>> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D\n>> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1\n>> sat/vb for 1000 vbytes.\n>>\n>> Package A + B ancestor score is 10 sat/vb.\n>>\n>> D has a higher feerate/absolute fee than B.\n>>\n>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's 1000\n>> sats + D's 1500 sats) /\n>> A's 100 vb + C's 1000 vb + D's 100 vb)\n>>\n>> Overall, this is a review through the lenses of LN requirements. I think\n>> other L2 protocols/applications\n>> could be candidates to using package accept/relay such as:\n>> * https://github.com/lightninglabs/pool\n>> * https://github.com/discreetlogcontracts/dlcspecs\n>> * https://github.com/bitcoin-teleport/teleport-transactions/\n>> * https://github.com/sapio-lang/sapio\n>> * https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n>> * https://github.com/revault/practical-revault\n>>\n>> Thanks for rolling forward the ball on this subject.\n>>\n>> Antoine\n>>\n>> Le jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi there,\n>>>\n>>> I'm writing to propose a set of mempool policy changes to enable package\n>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>> would not\n>>> be consensus or P2P protocol changes. However, since mempool policy\n>>> significantly affects transaction propagation, I believe this is\n>>> relevant for\n>>> the mailing list.\n>>>\n>>> My proposal enables packages consisting of multiple parents and 1 child.\n>>> If you\n>>> develop software that relies on specific transaction relay assumptions\n>>> and/or\n>>> are interested in using package relay in the future, I'm very interested\n>>> to hear\n>>> your feedback on the utility or restrictiveness of these package\n>>> policies for\n>>> your use cases.\n>>>\n>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>> PR#22290][1].\n>>>\n>>> An illustrated version of this post can be found at\n>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>> I have also linked the images below.\n>>>\n>>> ## Background\n>>>\n>>> Feel free to skip this section if you are already familiar with mempool\n>>> policy\n>>> and package relay terminology.\n>>>\n>>> ### Terminology Clarifications\n>>>\n>>> * Package = an ordered list of related transactions, representable by a\n>>> Directed\n>>>   Acyclic Graph.\n>>> * Package Feerate = the total modified fees divided by the total virtual\n>>> size of\n>>>   all transactions in the package.\n>>>     - Modified fees = a transaction's base fees + fee delta applied by\n>>> the user\n>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>> across\n>>> mempools.\n>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>> [BIP141\n>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>> Core][3].\n>>>     - Note that feerate is not necessarily based on the base fees and\n>>> serialized\n>>>       size.\n>>>\n>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>> incentives to\n>>>   boost a transaction's candidacy for inclusion in a block, including\n>>> Child Pays\n>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in\n>>> mempool policy is to recognize when the new transaction is more\n>>> economical to\n>>> mine than the original one(s) but not open DoS vectors, so there are some\n>>> limitations.\n>>>\n>>> ### Policy\n>>>\n>>> The purpose of the mempool is to store the best (to be most\n>>> incentive-compatible\n>>> with miners, highest feerate) candidates for inclusion in a block.\n>>> Miners use\n>>> the mempool to build block templates. The mempool is also useful as a\n>>> cache for\n>>> boosting block relay and validation performance, aiding transaction\n>>> relay, and\n>>> generating feerate estimations.\n>>>\n>>> Ideally, all consensus-valid transactions paying reasonable fees should\n>>> make it\n>>> to miners through normal transaction relay, without any special\n>>> connectivity or\n>>> relationships with miners. On the other hand, nodes do not have unlimited\n>>> resources, and a P2P network designed to let any honest node broadcast\n>>> their\n>>> transactions also exposes the transaction validation engine to DoS\n>>> attacks from\n>>> malicious peers.\n>>>\n>>> As such, for unconfirmed transactions we are considering for our\n>>> mempool, we\n>>> apply a set of validation rules in addition to consensus, primarily to\n>>> protect\n>>> us from resource exhaustion and aid our efforts to keep the highest fee\n>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>> node-specific) rules that transactions must abide by in order to be\n>>> accepted\n>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>> restrictions such\n>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>\n>>> ### Package Relay and Package Mempool Accept\n>>>\n>>> In transaction relay, we currently consider transactions one at a time\n>>> for\n>>> submission to the mempool. This creates a limitation in the node's\n>>> ability to\n>>> determine which transactions have the highest feerates, since we cannot\n>>> take\n>>> into account descendants (i.e. cannot use CPFP) until all the\n>>> transactions are\n>>> in the mempool. Similarly, we cannot use a transaction's descendants when\n>>> considering it for RBF. When an individual transaction does not meet the\n>>> mempool\n>>> minimum feerate and the user isn't able to create a replacement\n>>> transaction\n>>> directly, it will not be accepted by mempools.\n>>>\n>>> This limitation presents a security issue for applications and users\n>>> relying on\n>>> time-sensitive transactions. For example, Lightning and other protocols\n>>> create\n>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>> path opens\n>>> up after a timelock, and users are protected from cheating scenarios as\n>>> long as\n>>> they redeem on-chain in time. A key security assumption is that all\n>>> parties'\n>>> transactions will propagate and confirm in a timely manner. This\n>>> assumption can\n>>> be broken if fee-bumping does not work as intended.\n>>>\n>>> The end goal for Package Relay is to consider multiple transactions at\n>>> the same\n>>> time, e.g. a transaction with its high-fee child. This may help us better\n>>> determine whether transactions should be accepted to our mempool,\n>>> especially if\n>>> they don't meet fee requirements individually or are better RBF\n>>> candidates as a\n>>> package. A combination of changes to mempool validation logic, policy,\n>>> and\n>>> transaction relay allows us to better propagate the transactions with the\n>>> highest package feerates to miners, and makes fee-bumping tools more\n>>> powerful\n>>> for users.\n>>>\n>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n>>> large\n>>> part of the changes are in the mempool's package validation logic. We\n>>> call this\n>>> *Package Mempool Accept*.\n>>>\n>>> ### Previous Work\n>>>\n>>> * Given that mempool validation is DoS-sensitive and complex, it would be\n>>>   dangerous to haphazardly tack on package validation logic. Many\n>>> efforts have\n>>> been made to make mempool validation less opaque (see [#16400][4],\n>>> [#21062][5],\n>>> [#22675][6], [#22796][7]).\n>>> * [#20833][8] Added basic capabilities for package validation, test\n>>> accepts only\n>>>   (no submission to mempool).\n>>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>>> arbitrary\n>>>   packages. Still test accepts only.\n>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>\n>>> ### Existing Package Rules\n>>>\n>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>>> consider\n>>> them as \"given\" in the rest of this document, though they can be\n>>> changed, since\n>>> package validation is test-accept only right now.\n>>>\n>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>\n>>>    *Rationale*: This is already enforced as mempool ancestor/descendant\n>>> limits.\n>>> Presumably, transactions in a package are all related, so exceeding this\n>>> limit\n>>> would mean that the package can either be split up or it wouldn't pass\n>>> this\n>>> mempool policy.\n>>>\n>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>> between\n>>> transactions, parents must appear somewhere before children. [8]\n>>>\n>>> 3. A package cannot have conflicting transactions, i.e. none of them can\n>>> spend\n>>> the same inputs. This also means there cannot be duplicate transactions.\n>>> [8]\n>>>\n>>> 4. When packages are evaluated against ancestor/descendant limits in a\n>>> test\n>>> accept, the union of all of their descendants and ancestors is\n>>> considered. This\n>>> is essentially a \"worst case\" heuristic where every transaction in the\n>>> package\n>>> is treated as each other's ancestor and descendant. [8]\n>>> Packages for which ancestor/descendant limits are accurately captured by\n>>> this\n>>> heuristic: [19]\n>>>\n>>> There are also limitations such as the fact that CPFP carve out is not\n>>> applied\n>>> to package transactions. #20833 also disables RBF in package validation;\n>>> this\n>>> proposal overrides that to allow packages to use RBF.\n>>>\n>>> ## Proposed Changes\n>>>\n>>> The next step in the Package Mempool Accept project is to implement\n>>> submission\n>>> to mempool, initially through RPC only. This allows us to test the\n>>> submission\n>>> logic before exposing it on P2P.\n>>>\n>>> ### Summary\n>>>\n>>> - Packages may contain already-in-mempool transactions.\n>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>> - Fee-related checks use the package feerate. This means that wallets can\n>>> create a package that utilizes CPFP.\n>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>> similar\n>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>\n>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback\n>>> is\n>>> always welcome.\n>>>\n>>> ### Details\n>>>\n>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>\n>>> A package may contain transactions that are already in the mempool. We\n>>> remove\n>>> (\"deduplicate\") those transactions from the package for the purposes of\n>>> package\n>>> mempool acceptance. If a package is empty after deduplication, we do\n>>> nothing.\n>>>\n>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>> parent to be\n>>> accepted to the mempool of a peer on its own due to differences in\n>>> policy and\n>>> fee market fluctuations. We should not reject or penalize the entire\n>>> package for\n>>> an individual transaction as that could be a censorship vector.\n>>>\n>>> #### Packages Are Multi-Parent-1-Child\n>>>\n>>> Only packages of a specific topology are permitted. Namely, a package is\n>>> exactly\n>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>> package\n>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>> unconfirmed parents, etc. Note that it's possible for the parents to be\n>>> indirect\n>>> descendants/ancestors of one another, or for parent and child to share a\n>>> parent,\n>>> so we cannot make any other topology assumptions.\n>>>\n>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>> parents\n>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>> packages to a\n>>> defined topology is also easier to reason about and simplifies the\n>>> validation\n>>> logic greatly. Multi-parent-1-child allows us to think of the package as\n>>> one big\n>>> transaction, where:\n>>>\n>>> - Inputs = all the inputs of parents + inputs of the child that come from\n>>>   confirmed UTXOs\n>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>> that\n>>>   aren't spent by other transactions in the package\n>>>\n>>> Examples of packages that follow this rule (variations of example A show\n>>> some\n>>> possibilities after deduplication): ![image][15]\n>>>\n>>> #### Fee-Related Checks Use Package Feerate\n>>>\n>>> Package Feerate = the total modified fees divided by the total virtual\n>>> size of\n>>> all transactions in the package.\n>>>\n>>> To meet the two feerate requirements of a mempool, i.e., the\n>>> pre-configured\n>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>> feerate, the\n>>> total package feerate is used instead of the individual feerate. The\n>>> individual\n>>> transactions are allowed to be below feerate requirements if the package\n>>> meets\n>>> the feerate requirements. For example, the parent(s) in the package can\n>>> have 0\n>>> fees but be paid for by the child.\n>>>\n>>> *Rationale*: This can be thought of as \"CPFP within a package,\" solving\n>>> the\n>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>> applications to adjust their fees at broadcast time instead of\n>>> overshooting or\n>>> risking getting stuck/pinned.\n>>>\n>>> We use the package feerate of the package *after deduplication*.\n>>>\n>>> *Rationale*:  It would be incorrect to use the fees of transactions that\n>>> are\n>>> already in the mempool, as we do not want a transaction's fees to be\n>>> double-counted for both its individual RBF and package RBF.\n>>>\n>>> Examples F and G [14] show the same package, but P1 is submitted\n>>> individually before\n>>> the package in example G. In example F, we can see that the 300vB\n>>> package pays\n>>> an additional 200sat in fees, which is not enough to pay for its own\n>>> bandwidth\n>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n>>> but\n>>> using P1's fees again during package submission would make it look like\n>>> a 300sat\n>>> increase for a 200vB package. Even including its fees and size would not\n>>> be\n>>> sufficient in this example, since the 300sat looks like enough for the\n>>> 300vB\n>>> package. The calculcation after deduplication is 100sat increase for a\n>>> package\n>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>>> have a\n>>> size of 100vB.\n>>>\n>>> #### Package RBF\n>>>\n>>> If a package meets feerate requirements as a package, the parents in the\n>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>> child cannot\n>>> replace mempool transactions. Multiple transactions can replace the same\n>>> transaction, but in order to be valid, none of the transactions can try\n>>> to\n>>> replace an ancestor of another transaction in the same package (which\n>>> would thus\n>>> make its inputs unavailable).\n>>>\n>>> *Rationale*: Even if we are using package feerate, a package will not\n>>> propagate\n>>> as intended if RBF still requires each individual transaction to meet the\n>>> feerate requirements.\n>>>\n>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>\n>>> ##### Signaling (Rule #1)\n>>>\n>>> All mempool transactions to be replaced must signal replaceability.\n>>>\n>>> *Rationale*: Package RBF signaling logic should be the same for package\n>>> RBF and\n>>> single transaction acceptance. This would be updated if single\n>>> transaction\n>>> validation moves to full RBF.\n>>>\n>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>\n>>> A package may include new unconfirmed inputs, but the ancestor feerate\n>>> of the\n>>> child must be at least as high as the ancestor feerates of every\n>>> transaction\n>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>> replacement\n>>> transaction may only include an unconfirmed input if that input was\n>>> included in\n>>> one of the original transactions. (An unconfirmed input spends an output\n>>> from a\n>>> currently-unconfirmed transaction.)\"\n>>>\n>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>> transaction has a higher ancestor score than the original transaction(s)\n>>> (see\n>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n>>> can lower the\n>>> ancestor score of the replacement transaction. P1 is trying to replace\n>>> M1, and\n>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and\n>>> M2 pays\n>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>> isolation, P1\n>>> looks like a better mining candidate than M1, it must be mined with M2,\n>>> so its\n>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor\n>>> feerate, which is 6sat/vB.\n>>>\n>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>> transactions in the package can spend new unconfirmed inputs.\" Example J\n>>> [17] shows\n>>> why, if any of the package transactions have ancestors, package feerate\n>>> is no\n>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is\n>>> the\n>>> replacement transaction in an RBF), we're actually interested in the\n>>> entire\n>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n>>> P2, and\n>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>> only allow\n>>> the child to have new unconfirmed inputs, either, because it can still\n>>> cause us\n>>> to overestimate the package's ancestor score.\n>>>\n>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>> Package RBF\n>>> less useful, but would also break Package RBF for packages with parents\n>>> already\n>>> in the mempool: if a package parent has already been submitted, it would\n>>> look\n>>> like the child is spending a \"new\" unconfirmed input. In example K [18],\n>>> we're\n>>> looking to replace M1 with the entire package including P1, P2, and P3.\n>>> We must\n>>> consider the case where one of the parents is already in the mempool (in\n>>> this\n>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n>>> However,\n>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>> replace M1\n>>> with this package.\n>>>\n>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>> strict than\n>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>> replacement\n>>> transactions to have a ancestor score at least as high as the original\n>>> ones. As\n>>> a result, the entire package is required to be a higher feerate mining\n>>> candidate\n>>> than each of the replaced transactions.\n>>>\n>>> Another note: the [comment][13] above the BIP125#2 code in the original\n>>> RBF\n>>> implementation suggests that the rule was intended to be temporary.\n>>>\n>>> ##### Absolute Fee (Rule #3)\n>>>\n>>> The package must increase the absolute fee of the mempool, i.e. the\n>>> total fees\n>>> of the package must be higher than the absolute fees of the mempool\n>>> transactions\n>>> it replaces. Combined with the CPFP rule above, this differs from BIP125\n>>> Rule #3\n>>> - an individual transaction in the package may have lower fees than the\n>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>>> child\n>>> pays for RBF.\n>>>\n>>> ##### Feerate (Rule #4)\n>>>\n>>> The package must pay for its own bandwidth; the package feerate must be\n>>> higher\n>>> than the replaced transactions by at least minimum relay feerate\n>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs\n>>> from\n>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>> lower\n>>> feerate than the transaction(s) it is replacing. In fact, it may have 0\n>>> fees,\n>>> and the child pays for RBF.\n>>>\n>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>\n>>> The package cannot replace more than 100 mempool transactions. This is\n>>> identical\n>>> to BIP125 Rule #5.\n>>>\n>>> ### Expected FAQs\n>>>\n>>> 1. Is it possible for only some of the package to make it into the\n>>> mempool?\n>>>\n>>>    Yes, it is. However, since we evict transactions from the mempool by\n>>> descendant score and the package child is supposed to be sponsoring the\n>>> fees of\n>>> its parents, the most common scenario would be all-or-nothing. This is\n>>> incentive-compatible. In fact, to be conservative, package validation\n>>> should\n>>> begin by trying to submit all of the transactions individually, and only\n>>> use the\n>>> package mempool acceptance logic if the parents fail due to low feerate.\n>>>\n>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>\n>>>     No, for practical reasons. In mempool validation, we actually aren't\n>>> able to\n>>> tell with 100% confidence if we are looking at a transaction that has\n>>> already\n>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>> historical\n>>> block data, it's possible to look for it, but this is inefficient, not\n>>> always\n>>> possible for pruning nodes, and unnecessary because we're not going to do\n>>> anything with the transaction anyway. As such, we already have the\n>>> expectation\n>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>> relaying\n>>> transactions that have already been confirmed. Similarly, we shouldn't be\n>>> relaying packages that contain already-confirmed transactions.\n>>>\n>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>> [2]:\n>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>> [3]:\n>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>> [13]:\n>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>> [14]:\n>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>> [15]:\n>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>> [16]:\n>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>> [17]:\n>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>> [18]:\n>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>> [19]:\n>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>> [20]:\n>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210923/f8611b91/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2021-09-23T15:36:02",
                "message_text_only": "Hi Antoine,\n\nThanks as always for your input. I'm glad we agree on so much!\n\nIn summary, it seems that the decisions that might still need\nattention/input from devs on this mailing list are:\n1. Whether we should start with multiple-parent-1-child or 1-parent-1-child.\n2. Whether it's ok to require that the child not have conflicts with\nmempool transactions.\n\nResponding to your comments...\n\n> IIUC, you have package A+B, during the dedup phase early in\n`AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\nand A' is higher feerate than A, you trim A and replace by A' ?\n\n> I think this approach is safe, the one who appears unsafe to me is when\nA' has a _lower_ feerate, even if A' is already accepted by our mempool ?\nIn that case iirc that would be a pinning.\n\nRight, the fact that we essentially always choose the first-seen witness is\nan unfortunate limitation that exists already. Adding package mempool\naccept doesn't worsen this, but the procedure in the future is to replace\nthe witness when it makes sense economically. We can also add logic to\nallow package feerate to pay for witness replacements as well. This is\npretty far into the future, though.\n\n> It sounds uneconomical for an attacker but I think it's not when you\nconsider than you can \"batch\" attack against multiple honest\ncounterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts\nwith Alice's honest package P1, B' conflicts with Bob's honest package P2,\nC' conflicts with Caroll's honest package P3. And D' is a high-fee child of\nA' + B' + C'.\n\n> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs\nconfirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n\nI could be misunderstanding, but an attacker wouldn't be able to\nbatch-attack like this. Alice's package only conflicts with A' + D', not A'\n+ B' + C' + D'. She only needs to pay for evicting 2 transactions.\n\n> Do we assume that broadcasted packages are \"honest\" by default and that\nthe parent(s) always need the child to pass the fee checks, that way saving\nthe processing of individual transactions which are expected to fail in 99%\nof cases or more ad hoc composition of packages at relay ?\n> I think this point is quite dependent on the p2p packages format/logic\nwe'll end up on and that we should feel free to revisit it later ?\n\nI think it's the opposite; there's no way for us to assume that p2p\npackages will be \"honest.\" I'd like to have two things before we expose on\nP2P: (1) ensure that the amount of resources potentially allocated for\npackage validation isn't disproportionately higher than that of single\ntransaction validation and (2) only use package validation when we're\nunsatisifed with the single validation result, e.g. we might get better\nfees.\nYes, let's revisit this later :)\n\n > Yes, if you receive A+B, and A is already in-mempoo, I agree you can\ndiscard its feerate as B should pay for all fees checked on its own. Where\nI'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\nhave a fee high enough to cover the bandwidth penalty replacement\n(`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n\n B' only needs to pay for itself in this case.\n\n> > Do we want the child to be able to replace mempool transactions as well?\n\n> If we mean when you have replaceable A+B then A'+B' try to replace with a\nhigher-feerate ? I think that's exactly the case we need for Lightning as\nA+B is coming from Alice and A'+B' is coming from Bob :/\n\nLet me clarify this because I can see that my wording was ambiguous, and\nthen please let me know if it fits Lightning's needs?\n\nIn my proposal, I wrote \"If a package meets feerate requirements as a\npackage, the parents in the transaction are allowed to replace-by-fee\nmempool transactions. The child cannot replace mempool transactions.\" What\nI meant was: the package can replace mempool transactions if any of the\nparents conflict with mempool transactions. The child cannot not conflict\nwith any mempool transactions.\nThe Lightning use case this attempts to address is: Alice and Mallory are\nLN counterparties, and have packages A+B and A'+B', respectively. A and A'\nare their commitment transactions and conflict with each other; they have\nshared inputs and different txids.\nB spends Alice's anchor output from A. B' spends Mallory's anchor output\nfrom A'. Thus, B and B' do not conflict with each other.\nAlice can broadcast her package, A+B, to replace Mallory's package, A'+B',\nsince B doesn't conflict with the mempool.\n\nWould this be ok?\n\n> The second option, a child of A', In the LN case I think the CPFP is\nattached on one's anchor output.\n\nWhile it would be nice to have full RBF, malleability of the child won't\nblock RBF here. If we're trying to replace A', we only require that A'\nsignals replaceability, and don't mind if its child doesn't.\n\n> > B has an ancestor score of 10sat/vb and D has an\n> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\nB's,\n> > it fails the proposed package RBF Rule #2, so this package would be\n> > rejected. Does this meet your expectations?\n\n> Well what sounds odd to me, in my example, we fail D even if it has a\nhigher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\nfees are 4500 sats ?\n\nYes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A miner\nshould prefer to utilize their block space more effectively.\n\n> Is this compatible with a model where a miner prioritizes absolute fees\nover ancestor score, in the case that mempools aren't full-enough to\nfulfill a block ?\n\nNo, because we don't use that model.\n\nThanks,\nGloria\n\nOn Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>\nwrote:\n\n> > Correct, if B+C is too low feerate to be accepted, we will reject it. I\n> > prefer this because it is incentive compatible: A can be mined by itself,\n> > so there's no reason to prefer A+B+C instead of A.\n> > As another way of looking at this, consider the case where we do accept\n> > A+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\n> > capacity, we evict the lowest descendant feerate transactions, which are\n> > B+C in this case. This gives us the same resulting mempool, with A and\n> not\n> > B+C.\n>\n> I agree here. Doing otherwise, we might evict other transactions mempool\n> in `MempoolAccept::Finalize` with a higher-feerate than B+C while those\n> evicted transactions are the most compelling for block construction.\n>\n> I thought at first missing this acceptance requirement would break a\n> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is\n> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the\n> child fee is capturing the parent value. I can't think of other fee-bumping\n> schemes potentially affected. If they do exist I would say they're wrong in\n> their design assumptions.\n>\n> > If or when we have witness replacement, the logic is: if the individual\n> > transaction is enough to replace the mempool one, the replacement will\n> > happen during the preceding individual transaction acceptance, and\n> > deduplication logic will work. Otherwise, we will try to deduplicate by\n> > wtxid, see that we need a package witness replacement, and use the\n> package\n> > feerate to evaluate whether this is economically rational.\n>\n> IIUC, you have package A+B, during the dedup phase early in\n> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n> and A' is higher feerate than A, you trim A and replace by A' ?\n>\n> I think this approach is safe, the one who appears unsafe to me is when A'\n> has a _lower_ feerate, even if A' is already accepted by our mempool ? In\n> that case iirc that would be a pinning.\n>\n> Good to see progress on witness replacement before we see usage of Taproot\n> tree in the context of multi-party, where a malicious counterparty inflates\n> its witness to jam a honest spending.\n>\n> (Note, the commit linked currently points nowhere :))\n>\n>\n> > Please note that A may replace A' even if A' has higher fees than A\n> > individually, because the proposed package RBF utilizes the fees and size\n> > of the entire package. This just requires E to pay enough fees, although\n> > this can be pretty high if there are also potential B' and C' competing\n> > commitment transactions that we don't know about.\n>\n> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for the\n> individual check on A, the honest package should replace the pinning\n> attempt. I've not fully parsed the proposed implementation yet.\n>\n> Though note, I think it's still unsafe for a Lightning\n> multi-commitment-broadcast-as-one-package as a malicious A' might have an\n> absolute fee higher than E. It sounds uneconomical for\n> an attacker but I think it's not when you consider than you can \"batch\"\n> attack against multiple honest counterparties. E.g, Mallory broadcast A' +\n> B' + C' + D' where A' conflicts with Alice's honest package P1, B'\n> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest\n> package P3. And D' is a high-fee child of A' + B' + C'.\n>\n> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs\n> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>\n> > So far, my understanding is that multi-parent-1-child is desired for\n> > batched fee-bumping (\n> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n> and\n> > I've also seen your response which I have less context on (\n> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n> That\n> > being said, I am happy to create a new proposal for 1 parent + 1 child\n> > (which would be slightly simpler) and plan for moving to\n> > multi-parent-1-child later if that is preferred. I am very interested in\n> > hearing feedback on that approach.\n>\n> I think batched fee-bumping is okay as long as you don't have\n> time-sensitive outputs encumbering your commitment transactions. For the\n> reasons mentioned above, I think that's unsafe.\n>\n> What I'm worried about is  L2 developers, potentially not aware about all\n> the mempool subtleties blurring the difference and always batching their\n> broadcast by default.\n>\n> IMO, a good thing by restraining to 1-parent + 1 child,  we artificially\n> constraint L2 design space for now and minimize risks of unsafe usage of\n> the package API :)\n>\n> I think that's a point where it would be relevant to have the opinion of\n> more L2 devs.\n>\n> > I think there is a misunderstanding here - let me describe what I'm\n> > proposing we'd do in this situation: we'll try individual submission for\n> A,\n> > see that it fails due to \"insufficient fees.\" Then, we'll try package\n> > validation for A+B and use package RBF. If A+B pays enough, it can still\n> > replace A'. If A fails for a bad signature, we won't look at B or A+B.\n> Does\n> > this meet your expectations?\n>\n> Yes there was a misunderstanding, I think this approach is correct, it's\n> more a question of performance. Do we assume that broadcasted packages are\n> \"honest\" by default and that the parent(s) always need the child to pass\n> the fee checks, that way saving the processing of individual transactions\n> which are expected to fail in 99% of cases or more ad hoc composition of\n> packages at relay ?\n>\n> I think this point is quite dependent on the p2p packages format/logic\n> we'll end up on and that we should feel free to revisit it later ?\n>\n>\n> > What problem are you trying to solve by the package feerate *after* dedup\n> rule ?\n> > My understanding is that an in-package transaction might be already in\n> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n> vsize of this transaction could be discarded lowering the cost of package\n> RBF.\n>\n> > I'm proposing that, when a transaction has already been submitted to\n> > mempool, we would ignore both its fees and vsize when calculating package\n> > feerate.\n>\n> Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n> discard its feerate as B should pay for all fees checked on its own. Where\n> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n> have a fee high enough to cover the bandwidth penalty replacement\n> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>\n> If you have a second-layer like current Lightning, you might have a\n> counterparty commitment to replace and should always expect to have to pay\n> for parent replacement bandwidth.\n>\n> Where a potential discount sounds interesting is when you have an univoque\n> state on the first-stage of transactions. E.g DLC's funding transaction\n> which might be CPFP by any participant iirc.\n>\n> > Note that, if C' conflicts with C, it also conflicts with D, since D is a\n> > descendant of C and would thus need to be evicted along with it.\n>\n> Ah once again I think it's a misunderstanding without the code under my\n> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark\n> for potential eviction D and don't consider it for future conflicts in the\n> rest of the package, I think D' `PreChecks` should be good ?\n>\n> > More generally, this example is surprising to me because I didn't think\n> > packages would be used to fee-bump replaceable transactions. Do we want\n> the\n> > child to be able to replace mempool transactions as well?\n>\n> If we mean when you have replaceable A+B then A'+B' try to replace with a\n> higher-feerate ? I think that's exactly the case we need for Lightning as\n> A+B is coming from Alice and A'+B' is coming from Bob :/\n>\n> > I'm not sure what you mean? Let's say we have a package of parent A +\n> child\n> > B, where A is supposed to replace a mempool transaction A'. Are you\n> saying\n> > that counterparties are able to malleate the package child B, or a child\n> of\n> > A'?\n>\n> The second option, a child of A', In the LN case I think the CPFP is\n> attached on one's anchor output.\n>\n> I think it's good if we assume the\n> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing\n> inherited signaling or full-rbf ?\n>\n> > Sorry, I don't understand what you mean by \"preserve the package\n> > integrity?\" Could you elaborate?\n>\n> After thinking the relaxation about the \"new\" unconfirmed input is not\n> linked to trimming but I would say more to the multi-parent support.\n>\n> Let's say you have A+B trying to replace C+D where B is also spending\n> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed\n> input as D isn't spending E.\n>\n> So good, I think we agree on the problem description here.\n>\n> > I am in agreement with your calculations but unsure if we disagree on the\n> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n> > it fails the proposed package RBF Rule #2, so this package would be\n> > rejected. Does this meet your expectations?\n>\n> Well what sounds odd to me, in my example, we fail D even if it has a\n> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n> fees are 4500 sats ?\n>\n> Is this compatible with a model where a miner prioritizes absolute fees\n> over ancestor score, in the case that mempools aren't full-enough to\n> fulfill a block ?\n>\n> Let me know if I can clarify a point.\n>\n> Antoine\n>\n> Le lun. 20 sept. 2021 \u00e0 11:10, Gloria Zhao <gloriajzhao at gmail.com> a\n> \u00e9crit :\n>\n>>\n>> Hi Antoine,\n>>\n>> First of all, thank you for the thorough review. I appreciate your\n>> insight on LN requirements.\n>>\n>> > IIUC, you have a package A+B+C submitted for acceptance and A is\n>> already in your mempool. You trim out A from the package and then evaluate\n>> B+C.\n>>\n>> > I think this might be an issue if A is the higher-fee element of the\n>> ABC package. B+C package fees might be under the mempool min fee and will\n>> be rejected, potentially breaking the acceptance expectations of the\n>> package issuer ?\n>>\n>> Correct, if B+C is too low feerate to be accepted, we will reject it. I\n>> prefer this because it is incentive compatible: A can be mined by itself,\n>> so there's no reason to prefer A+B+C instead of A.\n>> As another way of looking at this, consider the case where we do accept\n>> A+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\n>> capacity, we evict the lowest descendant feerate transactions, which are\n>> B+C in this case. This gives us the same resulting mempool, with A and not\n>> B+C.\n>>\n>>\n>> > Further, I think the dedup should be done on wtxid, as you might have\n>> multiple valid witnesses. Though with varying vsizes and as such offering\n>> different feerates.\n>>\n>> I agree that variations of the same package with different witnesses is a\n>> case that must be handled. I consider witness replacement to be a project\n>> that can be done in parallel to package mempool acceptance because being\n>> able to accept packages does not worsen the problem of a\n>> same-txid-different-witness \"pinning\" attack.\n>>\n>> If or when we have witness replacement, the logic is: if the individual\n>> transaction is enough to replace the mempool one, the replacement will\n>> happen during the preceding individual transaction acceptance, and\n>> deduplication logic will work. Otherwise, we will try to deduplicate by\n>> wtxid, see that we need a package witness replacement, and use the package\n>> feerate to evaluate whether this is economically rational.\n>>\n>> See the #22290 \"handle package transactions already in mempool\" commit (\n>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),\n>> which handles the case of same-txid-different-witness by simply using the\n>> transaction in the mempool for now, with TODOs for what I just described.\n>>\n>>\n>> > I'm not clearly understanding the accepted topologies. By \"parent and\n>> child to share a parent\", do you mean the set of transactions A, B, C,\n>> where B is spending A and C is spending A and B would be correct ?\n>>\n>> Yes, that is what I meant. Yes, that would a valid package under these\n>> rules.\n>>\n>> > If yes, is there a width-limit introduced or we fallback on\n>> MAX_PACKAGE_COUNT=25 ?\n>>\n>> No, there is no limit on connectivity other than \"child with all\n>> unconfirmed parents.\" We will enforce MAX_PACKAGE_COUNT=25 and child's\n>> in-mempool + in-package ancestor limits.\n>>\n>>\n>> > Considering the current Core's mempool acceptance rules, I think CPFP\n>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>> jamming successful on one channel commitment transaction would contamine\n>> the remaining commitments sharing the same package.\n>>\n>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>> better feerate than A, the whole package acceptance will fail. Even if A'\n>> confirms in the following block,\n>> the propagation and confirmation of B+C+D have been delayed. This could\n>> carry on a loss of funds.\n>>\n>> Please note that A may replace A' even if A' has higher fees than A\n>> individually, because the proposed package RBF utilizes the fees and size\n>> of the entire package. This just requires E to pay enough fees, although\n>> this can be pretty high if there are also potential B' and C' competing\n>> commitment transactions that we don't know about.\n>>\n>>\n>> > IMHO, I'm leaning towards deploying during a first phase\n>> 1-parent/1-child. I think it's the most conservative step still improving\n>> second-layer safety.\n>>\n>> So far, my understanding is that multi-parent-1-child is desired for\n>> batched fee-bumping (\n>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>> and I've also seen your response which I have less context on (\n>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>> That being said, I am happy to create a new proposal for 1 parent + 1 child\n>> (which would be slightly simpler) and plan for moving to\n>> multi-parent-1-child later if that is preferred. I am very interested in\n>> hearing feedback on that approach.\n>>\n>>\n>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats\n>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\n>> fails. For this reason I think the individual RBF should be bypassed and\n>> only the package RBF apply ?\n>>\n>> I think there is a misunderstanding here - let me describe what I'm\n>> proposing we'd do in this situation: we'll try individual submission for A,\n>> see that it fails due to \"insufficient fees.\" Then, we'll try package\n>> validation for A+B and use package RBF. If A+B pays enough, it can still\n>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does\n>> this meet your expectations?\n>>\n>>\n>> > What problem are you trying to solve by the package feerate *after*\n>> dedup rule ?\n>> > My understanding is that an in-package transaction might be already in\n>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>> vsize of this transaction could be discarded lowering the cost of package\n>> RBF.\n>>\n>> I'm proposing that, when a transaction has already been submitted to\n>> mempool, we would ignore both its fees and vsize when calculating package\n>> feerate. In example G2, we shouldn't count M1 fees after its submission to\n>> mempool, since M1's fees have already been used to pay for its individual\n>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.\n>> We also shouldn't count its vsize, since it has already been paid for.\n>>\n>>\n>> > I think this is a footgunish API, as if a package issuer send the\n>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>> rejected. So it's breaking the naive broadcaster assumption that a\n>> higher-feerate/higher-fee package always replaces ?\n>>\n>> Note that, if C' conflicts with C, it also conflicts with D, since D is a\n>> descendant of C and would thus need to be evicted along with it.\n>> Implicitly, D' would not be in conflict with D.\n>> More generally, this example is surprising to me because I didn't think\n>> packages would be used to fee-bump replaceable transactions. Do we want the\n>> child to be able to replace mempool transactions as well? This can be\n>> implemented with a bit of additional logic.\n>>\n>> > I think this is unsafe for L2s if counterparties have malleability of\n>> the child transaction. They can block your package replacement by\n>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>> ability.\n>>\n>> I'm not sure what you mean? Let's say we have a package of parent A +\n>> child B, where A is supposed to replace a mempool transaction A'. Are you\n>> saying that counterparties are able to malleate the package child B, or a\n>> child of A'? If they can malleate a child of A', that shouldn't matter as\n>> long as A' is signaling replacement. This would be handled identically with\n>> full RBF and what Core currently implements.\n>>\n>> > I think this is an issue brought by the trimming during the dedup\n>> phase. If we preserve the package integrity, only re-using the tx-level\n>> checks results of already in-mempool transactions to gain in CPU time we\n>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>\n>> Sorry, I don't understand what you mean by \"preserve the package\n>> integrity?\" Could you elaborate?\n>>\n>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>> and C pays 1 sat/vb for 1000 vbytes.\n>>\n>> > Package A + B ancestor score is 10 sat/vb.\n>>\n>> > D has a higher feerate/absolute fee than B.\n>>\n>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)\n>>\n>> I am in agreement with your calculations but unsure if we disagree on the\n>> expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n>> it fails the proposed package RBF Rule #2, so this package would be\n>> rejected. Does this meet your expectations?\n>>\n>> Thank you for linking to projects that might be interested in package\n>> relay :)\n>>\n>> Thanks,\n>> Gloria\n>>\n>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>\n>> wrote:\n>>\n>>> Hi Gloria,\n>>>\n>>> > A package may contain transactions that are already in the mempool. We\n>>> > remove\n>>> > (\"deduplicate\") those transactions from the package for the purposes of\n>>> > package\n>>> > mempool acceptance. If a package is empty after deduplication, we do\n>>> > nothing.\n>>>\n>>> IIUC, you have a package A+B+C submitted for acceptance and A is already\n>>> in your mempool. You trim out A from the package and then evaluate B+C.\n>>>\n>>> I think this might be an issue if A is the higher-fee element of the ABC\n>>> package. B+C package fees might be under the mempool min fee and will be\n>>> rejected, potentially breaking the acceptance expectations of the package\n>>> issuer ?\n>>>\n>>> Further, I think the dedup should be done on wtxid, as you might have\n>>> multiple valid witnesses. Though with varying vsizes and as such offering\n>>> different feerates.\n>>>\n>>> E.g you're going to evaluate the package A+B and A' is already in your\n>>> mempool with a bigger valid witness. You trim A based on txid, then you\n>>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would\n>>> have been a success.\n>>>\n>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to\n>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we\n>>> achieve the same goal by bypassing tx-level checks for already-in txn while\n>>> conserving the package integrity for package-level checks ?\n>>>\n>>> > Note that it's possible for the parents to be\n>>> > indirect\n>>> > descendants/ancestors of one another, or for parent and child to share\n>>> a\n>>> > parent,\n>>> > so we cannot make any other topology assumptions.\n>>>\n>>> I'm not clearly understanding the accepted topologies. By \"parent and\n>>> child to share a parent\", do you mean the set of transactions A, B, C,\n>>> where B is spending A and C is spending A and B would be correct ?\n>>>\n>>> If yes, is there a width-limit introduced or we fallback on\n>>> MAX_PACKAGE_COUNT=25 ?\n>>>\n>>> IIRC, one rationale to come with this topology limitation was to lower\n>>> the DoS risks when potentially deploying p2p packages.\n>>>\n>>> Considering the current Core's mempool acceptance rules, I think CPFP\n>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>> jamming successful on one channel commitment transaction would contamine\n>>> the remaining commitments sharing the same package.\n>>>\n>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>>> better feerate than A, the whole package acceptance will fail. Even if A'\n>>> confirms in the following block,\n>>> the propagation and confirmation of B+C+D have been delayed. This could\n>>> carry on a loss of funds.\n>>>\n>>> That said, if you're broadcasting commitment transactions without\n>>> time-sensitive HTLC outputs, I think the batching is effectively a fee\n>>> saving as you don't have to duplicate the CPFP.\n>>>\n>>> IMHO, I'm leaning towards deploying during a first phase\n>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>> second-layer safety.\n>>>\n>>> > *Rationale*:  It would be incorrect to use the fees of transactions\n>>> that are\n>>> > already in the mempool, as we do not want a transaction's fees to be\n>>> > double-counted for both its individual RBF and package RBF.\n>>>\n>>> I'm unsure about the logical order of the checks proposed.\n>>>\n>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats\n>>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\n>>> fails. For this reason I think the individual RBF should be bypassed and\n>>> only the package RBF apply ?\n>>>\n>>> Note this situation is plausible, with current LN design, your\n>>> counterparty can have a commitment transaction with a better fee just by\n>>> selecting a higher `dust_limit_satoshis` than yours.\n>>>\n>>> > Examples F and G [14] show the same package, but P1 is submitted\n>>> > individually before\n>>> > the package in example G. In example F, we can see that the 300vB\n>>> package\n>>> > pays\n>>> > an additional 200sat in fees, which is not enough to pay for its own\n>>> > bandwidth\n>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>> M1, but\n>>> > using P1's fees again during package submission would make it look\n>>> like a\n>>> > 300sat\n>>> > increase for a 200vB package. Even including its fees and size would\n>>> not be\n>>> > sufficient in this example, since the 300sat looks like enough for the\n>>> 300vB\n>>> > package. The calculcation after deduplication is 100sat increase for a\n>>> > package\n>>> > of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>>> have\n>>> > a\n>>> > size of 100vB.\n>>>\n>>> What problem are you trying to solve by the package feerate *after*\n>>> dedup rule ?\n>>>\n>>> My understanding is that an in-package transaction might be already in\n>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>>> vsize of this transaction could be discarded lowering the cost of package\n>>> RBF.\n>>>\n>>> If we keep a \"safe\" dedup mechanism (see my point above), I think this\n>>> discount is justified, as the validation cost of node operators is paid for\n>>> ?\n>>>\n>>> > The child cannot replace mempool transactions.\n>>>\n>>> Let's say you issue package A+B, then package C+B', where B' is a child\n>>> of both A and C. This rule fails the acceptance of C+B' ?\n>>>\n>>> I think this is a footgunish API, as if a package issuer send the\n>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe\n>>> in protocols where states are symmetric. E.g a malicious counterparty\n>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better\n>>> fees.\n>>>\n>>> > All mempool transactions to be replaced must signal replaceability.\n>>>\n>>> I think this is unsafe for L2s if counterparties have malleability of\n>>> the child transaction. They can block your package replacement by\n>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>> ability.\n>>>\n>>> I think it's better to either fix inherited signaling or move towards\n>>> full-rbf.\n>>>\n>>> > if a package parent has already been submitted, it would\n>>> > look\n>>> >like the child is spending a \"new\" unconfirmed input.\n>>>\n>>> I think this is an issue brought by the trimming during the dedup phase.\n>>> If we preserve the package integrity, only re-using the tx-level checks\n>>> results of already in-mempool transactions to gain in CPU time we won't\n>>> have this issue. Package childs can add unconfirmed inputs as long as\n>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>\n>>> > However, we still achieve the same goal of requiring the\n>>> > replacement\n>>> > transactions to have a ancestor score at least as high as the original\n>>> > ones.\n>>>\n>>> I'm not sure if this holds...\n>>>\n>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100 vbytes\n>>> and B pays 10 sat/vb for 100 vbytes. You have the candidate replacement D\n>>> spending both A and C where D pays 15sat/vb for 100 vbytes and C pays 1\n>>> sat/vb for 1000 vbytes.\n>>>\n>>> Package A + B ancestor score is 10 sat/vb.\n>>>\n>>> D has a higher feerate/absolute fee than B.\n>>>\n>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>> 1000 sats + D's 1500 sats) /\n>>> A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>\n>>> Overall, this is a review through the lenses of LN requirements. I think\n>>> other L2 protocols/applications\n>>> could be candidates to using package accept/relay such as:\n>>> * https://github.com/lightninglabs/pool\n>>> * https://github.com/discreetlogcontracts/dlcspecs\n>>> * https://github.com/bitcoin-teleport/teleport-transactions/\n>>> * https://github.com/sapio-lang/sapio\n>>> *\n>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n>>> * https://github.com/revault/practical-revault\n>>>\n>>> Thanks for rolling forward the ball on this subject.\n>>>\n>>> Antoine\n>>>\n>>> Le jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>\n>>>> Hi there,\n>>>>\n>>>> I'm writing to propose a set of mempool policy changes to enable package\n>>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>>> would not\n>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>> significantly affects transaction propagation, I believe this is\n>>>> relevant for\n>>>> the mailing list.\n>>>>\n>>>> My proposal enables packages consisting of multiple parents and 1\n>>>> child. If you\n>>>> develop software that relies on specific transaction relay assumptions\n>>>> and/or\n>>>> are interested in using package relay in the future, I'm very\n>>>> interested to hear\n>>>> your feedback on the utility or restrictiveness of these package\n>>>> policies for\n>>>> your use cases.\n>>>>\n>>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>>> PR#22290][1].\n>>>>\n>>>> An illustrated version of this post can be found at\n>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>> I have also linked the images below.\n>>>>\n>>>> ## Background\n>>>>\n>>>> Feel free to skip this section if you are already familiar with mempool\n>>>> policy\n>>>> and package relay terminology.\n>>>>\n>>>> ### Terminology Clarifications\n>>>>\n>>>> * Package = an ordered list of related transactions, representable by a\n>>>> Directed\n>>>>   Acyclic Graph.\n>>>> * Package Feerate = the total modified fees divided by the total\n>>>> virtual size of\n>>>>   all transactions in the package.\n>>>>     - Modified fees = a transaction's base fees + fee delta applied by\n>>>> the user\n>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>> across\n>>>> mempools.\n>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>> [BIP141\n>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>>> Core][3].\n>>>>     - Note that feerate is not necessarily based on the base fees and\n>>>> serialized\n>>>>       size.\n>>>>\n>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>> incentives to\n>>>>   boost a transaction's candidacy for inclusion in a block, including\n>>>> Child Pays\n>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention\n>>>> in\n>>>> mempool policy is to recognize when the new transaction is more\n>>>> economical to\n>>>> mine than the original one(s) but not open DoS vectors, so there are\n>>>> some\n>>>> limitations.\n>>>>\n>>>> ### Policy\n>>>>\n>>>> The purpose of the mempool is to store the best (to be most\n>>>> incentive-compatible\n>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>> Miners use\n>>>> the mempool to build block templates. The mempool is also useful as a\n>>>> cache for\n>>>> boosting block relay and validation performance, aiding transaction\n>>>> relay, and\n>>>> generating feerate estimations.\n>>>>\n>>>> Ideally, all consensus-valid transactions paying reasonable fees should\n>>>> make it\n>>>> to miners through normal transaction relay, without any special\n>>>> connectivity or\n>>>> relationships with miners. On the other hand, nodes do not have\n>>>> unlimited\n>>>> resources, and a P2P network designed to let any honest node broadcast\n>>>> their\n>>>> transactions also exposes the transaction validation engine to DoS\n>>>> attacks from\n>>>> malicious peers.\n>>>>\n>>>> As such, for unconfirmed transactions we are considering for our\n>>>> mempool, we\n>>>> apply a set of validation rules in addition to consensus, primarily to\n>>>> protect\n>>>> us from resource exhaustion and aid our efforts to keep the highest fee\n>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>> node-specific) rules that transactions must abide by in order to be\n>>>> accepted\n>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>> restrictions such\n>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>\n>>>> ### Package Relay and Package Mempool Accept\n>>>>\n>>>> In transaction relay, we currently consider transactions one at a time\n>>>> for\n>>>> submission to the mempool. This creates a limitation in the node's\n>>>> ability to\n>>>> determine which transactions have the highest feerates, since we cannot\n>>>> take\n>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>> transactions are\n>>>> in the mempool. Similarly, we cannot use a transaction's descendants\n>>>> when\n>>>> considering it for RBF. When an individual transaction does not meet\n>>>> the mempool\n>>>> minimum feerate and the user isn't able to create a replacement\n>>>> transaction\n>>>> directly, it will not be accepted by mempools.\n>>>>\n>>>> This limitation presents a security issue for applications and users\n>>>> relying on\n>>>> time-sensitive transactions. For example, Lightning and other protocols\n>>>> create\n>>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>>> path opens\n>>>> up after a timelock, and users are protected from cheating scenarios as\n>>>> long as\n>>>> they redeem on-chain in time. A key security assumption is that all\n>>>> parties'\n>>>> transactions will propagate and confirm in a timely manner. This\n>>>> assumption can\n>>>> be broken if fee-bumping does not work as intended.\n>>>>\n>>>> The end goal for Package Relay is to consider multiple transactions at\n>>>> the same\n>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>> better\n>>>> determine whether transactions should be accepted to our mempool,\n>>>> especially if\n>>>> they don't meet fee requirements individually or are better RBF\n>>>> candidates as a\n>>>> package. A combination of changes to mempool validation logic, policy,\n>>>> and\n>>>> transaction relay allows us to better propagate the transactions with\n>>>> the\n>>>> highest package feerates to miners, and makes fee-bumping tools more\n>>>> powerful\n>>>> for users.\n>>>>\n>>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n>>>> large\n>>>> part of the changes are in the mempool's package validation logic. We\n>>>> call this\n>>>> *Package Mempool Accept*.\n>>>>\n>>>> ### Previous Work\n>>>>\n>>>> * Given that mempool validation is DoS-sensitive and complex, it would\n>>>> be\n>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>> efforts have\n>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>> [#21062][5],\n>>>> [#22675][6], [#22796][7]).\n>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>> accepts only\n>>>>   (no submission to mempool).\n>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>>>> arbitrary\n>>>>   packages. Still test accepts only.\n>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>\n>>>> ### Existing Package Rules\n>>>>\n>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>>>> consider\n>>>> them as \"given\" in the rest of this document, though they can be\n>>>> changed, since\n>>>> package validation is test-accept only right now.\n>>>>\n>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>\n>>>>    *Rationale*: This is already enforced as mempool ancestor/descendant\n>>>> limits.\n>>>> Presumably, transactions in a package are all related, so exceeding\n>>>> this limit\n>>>> would mean that the package can either be split up or it wouldn't pass\n>>>> this\n>>>> mempool policy.\n>>>>\n>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>> between\n>>>> transactions, parents must appear somewhere before children. [8]\n>>>>\n>>>> 3. A package cannot have conflicting transactions, i.e. none of them\n>>>> can spend\n>>>> the same inputs. This also means there cannot be duplicate\n>>>> transactions. [8]\n>>>>\n>>>> 4. When packages are evaluated against ancestor/descendant limits in a\n>>>> test\n>>>> accept, the union of all of their descendants and ancestors is\n>>>> considered. This\n>>>> is essentially a \"worst case\" heuristic where every transaction in the\n>>>> package\n>>>> is treated as each other's ancestor and descendant. [8]\n>>>> Packages for which ancestor/descendant limits are accurately captured\n>>>> by this\n>>>> heuristic: [19]\n>>>>\n>>>> There are also limitations such as the fact that CPFP carve out is not\n>>>> applied\n>>>> to package transactions. #20833 also disables RBF in package\n>>>> validation; this\n>>>> proposal overrides that to allow packages to use RBF.\n>>>>\n>>>> ## Proposed Changes\n>>>>\n>>>> The next step in the Package Mempool Accept project is to implement\n>>>> submission\n>>>> to mempool, initially through RPC only. This allows us to test the\n>>>> submission\n>>>> logic before exposing it on P2P.\n>>>>\n>>>> ### Summary\n>>>>\n>>>> - Packages may contain already-in-mempool transactions.\n>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>> - Fee-related checks use the package feerate. This means that wallets\n>>>> can\n>>>> create a package that utilizes CPFP.\n>>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>>> similar\n>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>\n>>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback\n>>>> is\n>>>> always welcome.\n>>>>\n>>>> ### Details\n>>>>\n>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>\n>>>> A package may contain transactions that are already in the mempool. We\n>>>> remove\n>>>> (\"deduplicate\") those transactions from the package for the purposes of\n>>>> package\n>>>> mempool acceptance. If a package is empty after deduplication, we do\n>>>> nothing.\n>>>>\n>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>> parent to be\n>>>> accepted to the mempool of a peer on its own due to differences in\n>>>> policy and\n>>>> fee market fluctuations. We should not reject or penalize the entire\n>>>> package for\n>>>> an individual transaction as that could be a censorship vector.\n>>>>\n>>>> #### Packages Are Multi-Parent-1-Child\n>>>>\n>>>> Only packages of a specific topology are permitted. Namely, a package\n>>>> is exactly\n>>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>>> package\n>>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>>> unconfirmed parents, etc. Note that it's possible for the parents to be\n>>>> indirect\n>>>> descendants/ancestors of one another, or for parent and child to share\n>>>> a parent,\n>>>> so we cannot make any other topology assumptions.\n>>>>\n>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>> parents\n>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>> packages to a\n>>>> defined topology is also easier to reason about and simplifies the\n>>>> validation\n>>>> logic greatly. Multi-parent-1-child allows us to think of the package\n>>>> as one big\n>>>> transaction, where:\n>>>>\n>>>> - Inputs = all the inputs of parents + inputs of the child that come\n>>>> from\n>>>>   confirmed UTXOs\n>>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>>> that\n>>>>   aren't spent by other transactions in the package\n>>>>\n>>>> Examples of packages that follow this rule (variations of example A\n>>>> show some\n>>>> possibilities after deduplication): ![image][15]\n>>>>\n>>>> #### Fee-Related Checks Use Package Feerate\n>>>>\n>>>> Package Feerate = the total modified fees divided by the total virtual\n>>>> size of\n>>>> all transactions in the package.\n>>>>\n>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>> pre-configured\n>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>> feerate, the\n>>>> total package feerate is used instead of the individual feerate. The\n>>>> individual\n>>>> transactions are allowed to be below feerate requirements if the\n>>>> package meets\n>>>> the feerate requirements. For example, the parent(s) in the package can\n>>>> have 0\n>>>> fees but be paid for by the child.\n>>>>\n>>>> *Rationale*: This can be thought of as \"CPFP within a package,\" solving\n>>>> the\n>>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>>> applications to adjust their fees at broadcast time instead of\n>>>> overshooting or\n>>>> risking getting stuck/pinned.\n>>>>\n>>>> We use the package feerate of the package *after deduplication*.\n>>>>\n>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>> that are\n>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>> double-counted for both its individual RBF and package RBF.\n>>>>\n>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>> individually before\n>>>> the package in example G. In example F, we can see that the 300vB\n>>>> package pays\n>>>> an additional 200sat in fees, which is not enough to pay for its own\n>>>> bandwidth\n>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n>>>> but\n>>>> using P1's fees again during package submission would make it look like\n>>>> a 300sat\n>>>> increase for a 200vB package. Even including its fees and size would\n>>>> not be\n>>>> sufficient in this example, since the 300sat looks like enough for the\n>>>> 300vB\n>>>> package. The calculcation after deduplication is 100sat increase for a\n>>>> package\n>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>>>> have a\n>>>> size of 100vB.\n>>>>\n>>>> #### Package RBF\n>>>>\n>>>> If a package meets feerate requirements as a package, the parents in the\n>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>> child cannot\n>>>> replace mempool transactions. Multiple transactions can replace the same\n>>>> transaction, but in order to be valid, none of the transactions can try\n>>>> to\n>>>> replace an ancestor of another transaction in the same package (which\n>>>> would thus\n>>>> make its inputs unavailable).\n>>>>\n>>>> *Rationale*: Even if we are using package feerate, a package will not\n>>>> propagate\n>>>> as intended if RBF still requires each individual transaction to meet\n>>>> the\n>>>> feerate requirements.\n>>>>\n>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>\n>>>> ##### Signaling (Rule #1)\n>>>>\n>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>\n>>>> *Rationale*: Package RBF signaling logic should be the same for package\n>>>> RBF and\n>>>> single transaction acceptance. This would be updated if single\n>>>> transaction\n>>>> validation moves to full RBF.\n>>>>\n>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>\n>>>> A package may include new unconfirmed inputs, but the ancestor feerate\n>>>> of the\n>>>> child must be at least as high as the ancestor feerates of every\n>>>> transaction\n>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>> replacement\n>>>> transaction may only include an unconfirmed input if that input was\n>>>> included in\n>>>> one of the original transactions. (An unconfirmed input spends an\n>>>> output from a\n>>>> currently-unconfirmed transaction.)\"\n>>>>\n>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>>> transaction has a higher ancestor score than the original\n>>>> transaction(s) (see\n>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n>>>> can lower the\n>>>> ancestor score of the replacement transaction. P1 is trying to replace\n>>>> M1, and\n>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and\n>>>> M2 pays\n>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>> isolation, P1\n>>>> looks like a better mining candidate than M1, it must be mined with M2,\n>>>> so its\n>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>> ancestor\n>>>> feerate, which is 6sat/vB.\n>>>>\n>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>> transactions in the package can spend new unconfirmed inputs.\" Example\n>>>> J [17] shows\n>>>> why, if any of the package transactions have ancestors, package feerate\n>>>> is no\n>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which\n>>>> is the\n>>>> replacement transaction in an RBF), we're actually interested in the\n>>>> entire\n>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n>>>> P2, and\n>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>>> only allow\n>>>> the child to have new unconfirmed inputs, either, because it can still\n>>>> cause us\n>>>> to overestimate the package's ancestor score.\n>>>>\n>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>> Package RBF\n>>>> less useful, but would also break Package RBF for packages with parents\n>>>> already\n>>>> in the mempool: if a package parent has already been submitted, it\n>>>> would look\n>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>> [18], we're\n>>>> looking to replace M1 with the entire package including P1, P2, and P3.\n>>>> We must\n>>>> consider the case where one of the parents is already in the mempool\n>>>> (in this\n>>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n>>>> However,\n>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>> replace M1\n>>>> with this package.\n>>>>\n>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>> strict than\n>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>> replacement\n>>>> transactions to have a ancestor score at least as high as the original\n>>>> ones. As\n>>>> a result, the entire package is required to be a higher feerate mining\n>>>> candidate\n>>>> than each of the replaced transactions.\n>>>>\n>>>> Another note: the [comment][13] above the BIP125#2 code in the original\n>>>> RBF\n>>>> implementation suggests that the rule was intended to be temporary.\n>>>>\n>>>> ##### Absolute Fee (Rule #3)\n>>>>\n>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>> total fees\n>>>> of the package must be higher than the absolute fees of the mempool\n>>>> transactions\n>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>> BIP125 Rule #3\n>>>> - an individual transaction in the package may have lower fees than the\n>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>>>> child\n>>>> pays for RBF.\n>>>>\n>>>> ##### Feerate (Rule #4)\n>>>>\n>>>> The package must pay for its own bandwidth; the package feerate must be\n>>>> higher\n>>>> than the replaced transactions by at least minimum relay feerate\n>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>> differs from\n>>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>>> lower\n>>>> feerate than the transaction(s) it is replacing. In fact, it may have 0\n>>>> fees,\n>>>> and the child pays for RBF.\n>>>>\n>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>\n>>>> The package cannot replace more than 100 mempool transactions. This is\n>>>> identical\n>>>> to BIP125 Rule #5.\n>>>>\n>>>> ### Expected FAQs\n>>>>\n>>>> 1. Is it possible for only some of the package to make it into the\n>>>> mempool?\n>>>>\n>>>>    Yes, it is. However, since we evict transactions from the mempool by\n>>>> descendant score and the package child is supposed to be sponsoring the\n>>>> fees of\n>>>> its parents, the most common scenario would be all-or-nothing. This is\n>>>> incentive-compatible. In fact, to be conservative, package validation\n>>>> should\n>>>> begin by trying to submit all of the transactions individually, and\n>>>> only use the\n>>>> package mempool acceptance logic if the parents fail due to low feerate.\n>>>>\n>>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>>\n>>>>     No, for practical reasons. In mempool validation, we actually\n>>>> aren't able to\n>>>> tell with 100% confidence if we are looking at a transaction that has\n>>>> already\n>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>> historical\n>>>> block data, it's possible to look for it, but this is inefficient, not\n>>>> always\n>>>> possible for pruning nodes, and unnecessary because we're not going to\n>>>> do\n>>>> anything with the transaction anyway. As such, we already have the\n>>>> expectation\n>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>> relaying\n>>>> transactions that have already been confirmed. Similarly, we shouldn't\n>>>> be\n>>>> relaying packages that contain already-confirmed transactions.\n>>>>\n>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>> [2]:\n>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>> [3]:\n>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>> [13]:\n>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>> [14]:\n>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>> [15]:\n>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>> [16]:\n>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>> [17]:\n>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>> [18]:\n>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>> [19]:\n>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>> [20]:\n>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210923/d1576a8f/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-26T21:10:14",
                "message_text_only": "Hi Gloria,\n\nThanks for your answers,\n\n> In summary, it seems that the decisions that might still need\n> attention/input from devs on this mailing list are:\n> 1. Whether we should start with multiple-parent-1-child or\n1-parent-1-child.\n> 2. Whether it's ok to require that the child not have conflicts with\n> mempool transactions.\n\nYes 1) it would be good to have inputs of more potential users of package\nacceptance . And 2) I think it's more a matter of clearer wording of the\nproposal.\n\nHowever, see my final point on the relaxation around \"unconfirmed inputs\"\nwhich might in fact alter our current block construction strategy.\n\n> Right, the fact that we essentially always choose the first-seen witness\nis\n> an unfortunate limitation that exists already. Adding package mempool\n> accept doesn't worsen this, but the procedure in the future is to replace\n> the witness when it makes sense economically. We can also add logic to\n> allow package feerate to pay for witness replacements as well. This is\n> pretty far into the future, though.\n\nYes I agree package mempool doesn't worsen this. And it's not an issue for\ncurrent LN as you can't significantly inflate a spending witness for the\n2-of-2 funding output.\nHowever, it might be an issue for multi-party protocol where the spending\nscript has alternative branches with asymmetric valid witness weights.\nTaproot should ease that kind of script so hopefully we would deploy\nwtxid-replacement not too far in the future.\n\n> I could be misunderstanding, but an attacker wouldn't be able to\n> batch-attack like this. Alice's package only conflicts with A' + D', not\nA'\n> + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n\nYeah I can be clearer, I think you have 2 pinning attacks scenarios to\nconsider.\n\nIn LN, if you're trying to confirm a commitment transaction to time-out or\nclaim on-chain a HTLC and the timelock is near-expiration, you should be\nready to pay in commitment+2nd-stage HTLC transaction fees as much as the\nvalue offered by the HTLC.\n\nFollowing this security assumption, an attacker can exploit it by targeting\ntogether commitment transactions from different channels by blocking them\nunder a high-fee child, of which the fee value\nis equal to the top-value HTLC + 1. Victims's fee-bumping logics won't\noverbid as it's not worthy to offer fees beyond their competed HTLCs. Apart\nfrom observing mempools state, victims can't learn they're targeted by the\nsame attacker.\n\nTo draw from the aforementioned topology, Mallory broadcasts A' + B' + C' +\nD', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'\nconflicts with Caroll's P3. Let's assume P1 is confirming the top-value\nHTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for\nAlice or Bob or Caroll to keep offering competing feerates. Mallory will be\nat loss on stealing P1, as she has paid more in fees but will realize a\ngain on P2+P3.\n\nIn this model, Alice is allowed to evict those 2 transactions (A' + D') but\nas she is economically-bounded she won't succeed.\n\nMallory is maliciously exploiting RBF rule 3 on absolute fee. I think this\n1st pinning scenario is correct and \"lucractive\" when you sum the global\ngain/loss.\n\nThere is a 2nd attack scenario where A + B + C + D, where D is the child of\nA,B,C. All those transactions are honestly issued by Alice. Once A + B + C\n+ D are propagated in network mempools, Mallory is able to replace A + D\nwith  A' + D' where D' is paying a higher fee. This package A' + D' will\nconfirm soon if D feerate was compelling but Mallory succeeds in delaying\nthe confirmation\nof B + C for one or more blocks. As B + C are pre-signed commitments with a\nlow-fee rate they won't confirm without Alice issuing a new child E.\nMallory can repeat the same trick by broadcasting\nB' + E' and delay again the confirmation of C.\n\nIf the remaining package pending HTLC has a higher-value than all the\nmalicious fees over-bid, Mallory should realize a gain. With this 2nd\npinning attack, the malicious entity buys confirmation delay of your\npackaged-together commitments.\n\nAssuming those attacks are correct, I'm leaning towards being conservative\nwith the LDK broadcast backend. Though once again, other L2 devs have\nlikely other use-cases and opinions :)\n\n>  B' only needs to pay for itself in this case.\n\nYes I think it's a nice discount when UTXO is single-owned. In the context\nof shared-owned UTXO (e.g LN), you might not if there is an in-mempool\npackage already spending the UTXO and have to assume the worst-case\nscenario. I.e have B' committing enough fee to pay for A' replacement\nbandwidth. I think we can't do that much for this case...\n\n> If a package meets feerate requirements as a\npackage, the parents in the transaction are allowed to replace-by-fee\nmempool transactions. The child cannot replace mempool transactions.\"\n\nI agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B' to\nreplace A+B because the first broadcast isn't satisfying anymore due to\nmempool spikes ? Assuming B' fees is enough, I think that case as child B'\nreplacing in-mempool transaction B. Which I understand going against  \"The\nchild cannot replace mempool transactions\".\n\nMaybe wording could be a bit clearer ?\n\n> While it would be nice to have full RBF, malleability of the child won't\n> block RBF here. If we're trying to replace A', we only require that A'\n> signals replaceability, and don't mind if its child doesn't.\n\nYes, it sounds good.\n\n> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\nminer\n> should prefer to utilize their block space more effectively.\n\nIf your mempool is empty and only composed of A+C+D or A+B, I think taking\nA+C+D is the most efficient block construction you can come up with as a\nminer ?\n\n> No, because we don't use that model.\n\nCan you describe what miner model we are using ? Like the block\nconstruction strategy implemented by `addPackagesTxs` or also encompassing\nour current mempool acceptance policy, which I think rely on absolute fee\nover ancestor score in case of replacement ?\n\nI think this point is worthy to discuss as otherwise we might downgrade the\nefficiency of our current block construction strategy in periods of\nnear-empty mempools. A knowledge which could be discreetly leveraged by a\nminer to gain an advantage on the rest of the mining ecosystem.\n\nNote, I think we *might* have to go in this direction if we want to replace\nreplace-by-fee by replace-by-feerate or replace-by-ancestor and solve\nin-depth pinning attacks. Though if we do so,\nIMO we would need more thoughts.\n\nI think we could restrain package acceptance to only confirmed inputs for\nnow and revisit later this point ? For LN-anchor, you can assume that the\nfee-bumping UTXO feeding the CPFP is already\nconfirmed. Or are there currently-deployed use-cases which would benefit\nfrom your proposed Rule #2 ?\n\nAntoine\n\nLe jeu. 23 sept. 2021 \u00e0 11:36, Gloria Zhao <gloriajzhao at gmail.com> a \u00e9crit :\n\n> Hi Antoine,\n>\n> Thanks as always for your input. I'm glad we agree on so much!\n>\n> In summary, it seems that the decisions that might still need\n> attention/input from devs on this mailing list are:\n> 1. Whether we should start with multiple-parent-1-child or\n> 1-parent-1-child.\n> 2. Whether it's ok to require that the child not have conflicts with\n> mempool transactions.\n>\n> Responding to your comments...\n>\n> > IIUC, you have package A+B, during the dedup phase early in\n> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n> and A' is higher feerate than A, you trim A and replace by A' ?\n>\n> > I think this approach is safe, the one who appears unsafe to me is when\n> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?\n> In that case iirc that would be a pinning.\n>\n> Right, the fact that we essentially always choose the first-seen witness\n> is an unfortunate limitation that exists already. Adding package mempool\n> accept doesn't worsen this, but the procedure in the future is to replace\n> the witness when it makes sense economically. We can also add logic to\n> allow package feerate to pay for witness replacements as well. This is\n> pretty far into the future, though.\n>\n> > It sounds uneconomical for an attacker but I think it's not when you\n> consider than you can \"batch\" attack against multiple honest\n> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts\n> with Alice's honest package P1, B' conflicts with Bob's honest package P2,\n> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of\n> A' + B' + C'.\n>\n> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs\n> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>\n> I could be misunderstanding, but an attacker wouldn't be able to\n> batch-attack like this. Alice's package only conflicts with A' + D', not A'\n> + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n>\n> > Do we assume that broadcasted packages are \"honest\" by default and that\n> the parent(s) always need the child to pass the fee checks, that way saving\n> the processing of individual transactions which are expected to fail in 99%\n> of cases or more ad hoc composition of packages at relay ?\n> > I think this point is quite dependent on the p2p packages format/logic\n> we'll end up on and that we should feel free to revisit it later ?\n>\n> I think it's the opposite; there's no way for us to assume that p2p\n> packages will be \"honest.\" I'd like to have two things before we expose on\n> P2P: (1) ensure that the amount of resources potentially allocated for\n> package validation isn't disproportionately higher than that of single\n> transaction validation and (2) only use package validation when we're\n> unsatisifed with the single validation result, e.g. we might get better\n> fees.\n> Yes, let's revisit this later :)\n>\n>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n> discard its feerate as B should pay for all fees checked on its own. Where\n> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n> have a fee high enough to cover the bandwidth penalty replacement\n> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>\n>  B' only needs to pay for itself in this case.\n>\n> > > Do we want the child to be able to replace mempool transactions as\n> well?\n>\n> > If we mean when you have replaceable A+B then A'+B' try to replace with\n> a higher-feerate ? I think that's exactly the case we need for Lightning as\n> A+B is coming from Alice and A'+B' is coming from Bob :/\n>\n> Let me clarify this because I can see that my wording was ambiguous, and\n> then please let me know if it fits Lightning's needs?\n>\n> In my proposal, I wrote \"If a package meets feerate requirements as a\n> package, the parents in the transaction are allowed to replace-by-fee\n> mempool transactions. The child cannot replace mempool transactions.\" What\n> I meant was: the package can replace mempool transactions if any of the\n> parents conflict with mempool transactions. The child cannot not conflict\n> with any mempool transactions.\n> The Lightning use case this attempts to address is: Alice and Mallory are\n> LN counterparties, and have packages A+B and A'+B', respectively. A and A'\n> are their commitment transactions and conflict with each other; they have\n> shared inputs and different txids.\n> B spends Alice's anchor output from A. B' spends Mallory's anchor output\n> from A'. Thus, B and B' do not conflict with each other.\n> Alice can broadcast her package, A+B, to replace Mallory's package, A'+B',\n> since B doesn't conflict with the mempool.\n>\n> Would this be ok?\n>\n> > The second option, a child of A', In the LN case I think the CPFP is\n> attached on one's anchor output.\n>\n> While it would be nice to have full RBF, malleability of the child won't\n> block RBF here. If we're trying to replace A', we only require that A'\n> signals replaceability, and don't mind if its child doesn't.\n>\n> > > B has an ancestor score of 10sat/vb and D has an\n> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\n> B's,\n> > > it fails the proposed package RBF Rule #2, so this package would be\n> > > rejected. Does this meet your expectations?\n>\n> > Well what sounds odd to me, in my example, we fail D even if it has a\n> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n> fees are 4500 sats ?\n>\n> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\n> miner should prefer to utilize their block space more effectively.\n>\n> > Is this compatible with a model where a miner prioritizes absolute fees\n> over ancestor score, in the case that mempools aren't full-enough to\n> fulfill a block ?\n>\n> No, because we don't use that model.\n>\n> Thanks,\n> Gloria\n>\n> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>\n> wrote:\n>\n>> > Correct, if B+C is too low feerate to be accepted, we will reject it. I\n>> > prefer this because it is incentive compatible: A can be mined by\n>> itself,\n>> > so there's no reason to prefer A+B+C instead of A.\n>> > As another way of looking at this, consider the case where we do accept\n>> > A+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\n>> > capacity, we evict the lowest descendant feerate transactions, which are\n>> > B+C in this case. This gives us the same resulting mempool, with A and\n>> not\n>> > B+C.\n>>\n>> I agree here. Doing otherwise, we might evict other transactions mempool\n>> in `MempoolAccept::Finalize` with a higher-feerate than B+C while those\n>> evicted transactions are the most compelling for block construction.\n>>\n>> I thought at first missing this acceptance requirement would break a\n>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is\n>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the\n>> child fee is capturing the parent value. I can't think of other fee-bumping\n>> schemes potentially affected. If they do exist I would say they're wrong in\n>> their design assumptions.\n>>\n>> > If or when we have witness replacement, the logic is: if the individual\n>> > transaction is enough to replace the mempool one, the replacement will\n>> > happen during the preceding individual transaction acceptance, and\n>> > deduplication logic will work. Otherwise, we will try to deduplicate by\n>> > wtxid, see that we need a package witness replacement, and use the\n>> package\n>> > feerate to evaluate whether this is economically rational.\n>>\n>> IIUC, you have package A+B, during the dedup phase early in\n>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n>> and A' is higher feerate than A, you trim A and replace by A' ?\n>>\n>> I think this approach is safe, the one who appears unsafe to me is when\n>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?\n>> In that case iirc that would be a pinning.\n>>\n>> Good to see progress on witness replacement before we see usage of\n>> Taproot tree in the context of multi-party, where a malicious counterparty\n>> inflates its witness to jam a honest spending.\n>>\n>> (Note, the commit linked currently points nowhere :))\n>>\n>>\n>> > Please note that A may replace A' even if A' has higher fees than A\n>> > individually, because the proposed package RBF utilizes the fees and\n>> size\n>> > of the entire package. This just requires E to pay enough fees, although\n>> > this can be pretty high if there are also potential B' and C' competing\n>> > commitment transactions that we don't know about.\n>>\n>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for\n>> the individual check on A, the honest package should replace the pinning\n>> attempt. I've not fully parsed the proposed implementation yet.\n>>\n>> Though note, I think it's still unsafe for a Lightning\n>> multi-commitment-broadcast-as-one-package as a malicious A' might have an\n>> absolute fee higher than E. It sounds uneconomical for\n>> an attacker but I think it's not when you consider than you can \"batch\"\n>> attack against multiple honest counterparties. E.g, Mallory broadcast A' +\n>> B' + C' + D' where A' conflicts with Alice's honest package P1, B'\n>> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest\n>> package P3. And D' is a high-fee child of A' + B' + C'.\n>>\n>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs\n>> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>>\n>> > So far, my understanding is that multi-parent-1-child is desired for\n>> > batched fee-bumping (\n>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>> and\n>> > I've also seen your response which I have less context on (\n>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>> That\n>> > being said, I am happy to create a new proposal for 1 parent + 1 child\n>> > (which would be slightly simpler) and plan for moving to\n>> > multi-parent-1-child later if that is preferred. I am very interested in\n>> > hearing feedback on that approach.\n>>\n>> I think batched fee-bumping is okay as long as you don't have\n>> time-sensitive outputs encumbering your commitment transactions. For the\n>> reasons mentioned above, I think that's unsafe.\n>>\n>> What I'm worried about is  L2 developers, potentially not aware about all\n>> the mempool subtleties blurring the difference and always batching their\n>> broadcast by default.\n>>\n>> IMO, a good thing by restraining to 1-parent + 1 child,  we artificially\n>> constraint L2 design space for now and minimize risks of unsafe usage of\n>> the package API :)\n>>\n>> I think that's a point where it would be relevant to have the opinion of\n>> more L2 devs.\n>>\n>> > I think there is a misunderstanding here - let me describe what I'm\n>> > proposing we'd do in this situation: we'll try individual submission\n>> for A,\n>> > see that it fails due to \"insufficient fees.\" Then, we'll try package\n>> > validation for A+B and use package RBF. If A+B pays enough, it can still\n>> > replace A'. If A fails for a bad signature, we won't look at B or A+B.\n>> Does\n>> > this meet your expectations?\n>>\n>> Yes there was a misunderstanding, I think this approach is correct, it's\n>> more a question of performance. Do we assume that broadcasted packages are\n>> \"honest\" by default and that the parent(s) always need the child to pass\n>> the fee checks, that way saving the processing of individual transactions\n>> which are expected to fail in 99% of cases or more ad hoc composition of\n>> packages at relay ?\n>>\n>> I think this point is quite dependent on the p2p packages format/logic\n>> we'll end up on and that we should feel free to revisit it later ?\n>>\n>>\n>> > What problem are you trying to solve by the package feerate *after*\n>> dedup\n>> rule ?\n>> > My understanding is that an in-package transaction might be already in\n>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>> vsize of this transaction could be discarded lowering the cost of package\n>> RBF.\n>>\n>> > I'm proposing that, when a transaction has already been submitted to\n>> > mempool, we would ignore both its fees and vsize when calculating\n>> package\n>> > feerate.\n>>\n>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n>> discard its feerate as B should pay for all fees checked on its own. Where\n>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n>> have a fee high enough to cover the bandwidth penalty replacement\n>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>>\n>> If you have a second-layer like current Lightning, you might have a\n>> counterparty commitment to replace and should always expect to have to pay\n>> for parent replacement bandwidth.\n>>\n>> Where a potential discount sounds interesting is when you have an\n>> univoque state on the first-stage of transactions. E.g DLC's funding\n>> transaction which might be CPFP by any participant iirc.\n>>\n>> > Note that, if C' conflicts with C, it also conflicts with D, since D is\n>> a\n>> > descendant of C and would thus need to be evicted along with it.\n>>\n>> Ah once again I think it's a misunderstanding without the code under my\n>> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark\n>> for potential eviction D and don't consider it for future conflicts in the\n>> rest of the package, I think D' `PreChecks` should be good ?\n>>\n>> > More generally, this example is surprising to me because I didn't think\n>> > packages would be used to fee-bump replaceable transactions. Do we want\n>> the\n>> > child to be able to replace mempool transactions as well?\n>>\n>> If we mean when you have replaceable A+B then A'+B' try to replace with a\n>> higher-feerate ? I think that's exactly the case we need for Lightning as\n>> A+B is coming from Alice and A'+B' is coming from Bob :/\n>>\n>> > I'm not sure what you mean? Let's say we have a package of parent A +\n>> child\n>> > B, where A is supposed to replace a mempool transaction A'. Are you\n>> saying\n>> > that counterparties are able to malleate the package child B, or a\n>> child of\n>> > A'?\n>>\n>> The second option, a child of A', In the LN case I think the CPFP is\n>> attached on one's anchor output.\n>>\n>> I think it's good if we assume the\n>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing\n>> inherited signaling or full-rbf ?\n>>\n>> > Sorry, I don't understand what you mean by \"preserve the package\n>> > integrity?\" Could you elaborate?\n>>\n>> After thinking the relaxation about the \"new\" unconfirmed input is not\n>> linked to trimming but I would say more to the multi-parent support.\n>>\n>> Let's say you have A+B trying to replace C+D where B is also spending\n>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed\n>> input as D isn't spending E.\n>>\n>> So good, I think we agree on the problem description here.\n>>\n>> > I am in agreement with your calculations but unsure if we disagree on\n>> the\n>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\n>> B's,\n>> > it fails the proposed package RBF Rule #2, so this package would be\n>> > rejected. Does this meet your expectations?\n>>\n>> Well what sounds odd to me, in my example, we fail D even if it has a\n>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n>> fees are 4500 sats ?\n>>\n>> Is this compatible with a model where a miner prioritizes absolute fees\n>> over ancestor score, in the case that mempools aren't full-enough to\n>> fulfill a block ?\n>>\n>> Let me know if I can clarify a point.\n>>\n>> Antoine\n>>\n>> Le lun. 20 sept. 2021 \u00e0 11:10, Gloria Zhao <gloriajzhao at gmail.com> a\n>> \u00e9crit :\n>>\n>>>\n>>> Hi Antoine,\n>>>\n>>> First of all, thank you for the thorough review. I appreciate your\n>>> insight on LN requirements.\n>>>\n>>> > IIUC, you have a package A+B+C submitted for acceptance and A is\n>>> already in your mempool. You trim out A from the package and then evaluate\n>>> B+C.\n>>>\n>>> > I think this might be an issue if A is the higher-fee element of the\n>>> ABC package. B+C package fees might be under the mempool min fee and will\n>>> be rejected, potentially breaking the acceptance expectations of the\n>>> package issuer ?\n>>>\n>>> Correct, if B+C is too low feerate to be accepted, we will reject it. I\n>>> prefer this because it is incentive compatible: A can be mined by itself,\n>>> so there's no reason to prefer A+B+C instead of A.\n>>> As another way of looking at this, consider the case where we do accept\n>>> A+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\n>>> capacity, we evict the lowest descendant feerate transactions, which are\n>>> B+C in this case. This gives us the same resulting mempool, with A and not\n>>> B+C.\n>>>\n>>>\n>>> > Further, I think the dedup should be done on wtxid, as you might have\n>>> multiple valid witnesses. Though with varying vsizes and as such offering\n>>> different feerates.\n>>>\n>>> I agree that variations of the same package with different witnesses is\n>>> a case that must be handled. I consider witness replacement to be a project\n>>> that can be done in parallel to package mempool acceptance because being\n>>> able to accept packages does not worsen the problem of a\n>>> same-txid-different-witness \"pinning\" attack.\n>>>\n>>> If or when we have witness replacement, the logic is: if the individual\n>>> transaction is enough to replace the mempool one, the replacement will\n>>> happen during the preceding individual transaction acceptance, and\n>>> deduplication logic will work. Otherwise, we will try to deduplicate by\n>>> wtxid, see that we need a package witness replacement, and use the package\n>>> feerate to evaluate whether this is economically rational.\n>>>\n>>> See the #22290 \"handle package transactions already in mempool\" commit (\n>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),\n>>> which handles the case of same-txid-different-witness by simply using the\n>>> transaction in the mempool for now, with TODOs for what I just described.\n>>>\n>>>\n>>> > I'm not clearly understanding the accepted topologies. By \"parent and\n>>> child to share a parent\", do you mean the set of transactions A, B, C,\n>>> where B is spending A and C is spending A and B would be correct ?\n>>>\n>>> Yes, that is what I meant. Yes, that would a valid package under these\n>>> rules.\n>>>\n>>> > If yes, is there a width-limit introduced or we fallback on\n>>> MAX_PACKAGE_COUNT=25 ?\n>>>\n>>> No, there is no limit on connectivity other than \"child with all\n>>> unconfirmed parents.\" We will enforce MAX_PACKAGE_COUNT=25 and child's\n>>> in-mempool + in-package ancestor limits.\n>>>\n>>>\n>>> > Considering the current Core's mempool acceptance rules, I think CPFP\n>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>> jamming successful on one channel commitment transaction would contamine\n>>> the remaining commitments sharing the same package.\n>>>\n>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>>> better feerate than A, the whole package acceptance will fail. Even if A'\n>>> confirms in the following block,\n>>> the propagation and confirmation of B+C+D have been delayed. This could\n>>> carry on a loss of funds.\n>>>\n>>> Please note that A may replace A' even if A' has higher fees than A\n>>> individually, because the proposed package RBF utilizes the fees and size\n>>> of the entire package. This just requires E to pay enough fees, although\n>>> this can be pretty high if there are also potential B' and C' competing\n>>> commitment transactions that we don't know about.\n>>>\n>>>\n>>> > IMHO, I'm leaning towards deploying during a first phase\n>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>> second-layer safety.\n>>>\n>>> So far, my understanding is that multi-parent-1-child is desired for\n>>> batched fee-bumping (\n>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>>> and I've also seen your response which I have less context on (\n>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>>> That being said, I am happy to create a new proposal for 1 parent + 1 child\n>>> (which would be slightly simpler) and plan for moving to\n>>> multi-parent-1-child later if that is preferred. I am very interested in\n>>> hearing feedback on that approach.\n>>>\n>>>\n>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200\n>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B\n>>> acceptance fails. For this reason I think the individual RBF should be\n>>> bypassed and only the package RBF apply ?\n>>>\n>>> I think there is a misunderstanding here - let me describe what I'm\n>>> proposing we'd do in this situation: we'll try individual submission for A,\n>>> see that it fails due to \"insufficient fees.\" Then, we'll try package\n>>> validation for A+B and use package RBF. If A+B pays enough, it can still\n>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does\n>>> this meet your expectations?\n>>>\n>>>\n>>> > What problem are you trying to solve by the package feerate *after*\n>>> dedup rule ?\n>>> > My understanding is that an in-package transaction might be already in\n>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>>> vsize of this transaction could be discarded lowering the cost of package\n>>> RBF.\n>>>\n>>> I'm proposing that, when a transaction has already been submitted to\n>>> mempool, we would ignore both its fees and vsize when calculating package\n>>> feerate. In example G2, we shouldn't count M1 fees after its submission to\n>>> mempool, since M1's fees have already been used to pay for its individual\n>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.\n>>> We also shouldn't count its vsize, since it has already been paid for.\n>>>\n>>>\n>>> > I think this is a footgunish API, as if a package issuer send the\n>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>> higher-feerate/higher-fee package always replaces ?\n>>>\n>>> Note that, if C' conflicts with C, it also conflicts with D, since D is\n>>> a descendant of C and would thus need to be evicted along with it.\n>>> Implicitly, D' would not be in conflict with D.\n>>> More generally, this example is surprising to me because I didn't think\n>>> packages would be used to fee-bump replaceable transactions. Do we want the\n>>> child to be able to replace mempool transactions as well? This can be\n>>> implemented with a bit of additional logic.\n>>>\n>>> > I think this is unsafe for L2s if counterparties have malleability of\n>>> the child transaction. They can block your package replacement by\n>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>> ability.\n>>>\n>>> I'm not sure what you mean? Let's say we have a package of parent A +\n>>> child B, where A is supposed to replace a mempool transaction A'. Are you\n>>> saying that counterparties are able to malleate the package child B, or a\n>>> child of A'? If they can malleate a child of A', that shouldn't matter as\n>>> long as A' is signaling replacement. This would be handled identically with\n>>> full RBF and what Core currently implements.\n>>>\n>>> > I think this is an issue brought by the trimming during the dedup\n>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>> checks results of already in-mempool transactions to gain in CPU time we\n>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>\n>>> Sorry, I don't understand what you mean by \"preserve the package\n>>> integrity?\" Could you elaborate?\n>>>\n>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>\n>>> > Package A + B ancestor score is 10 sat/vb.\n>>>\n>>> > D has a higher feerate/absolute fee than B.\n>>>\n>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>\n>>> I am in agreement with your calculations but unsure if we disagree on\n>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n>>> it fails the proposed package RBF Rule #2, so this package would be\n>>> rejected. Does this meet your expectations?\n>>>\n>>> Thank you for linking to projects that might be interested in package\n>>> relay :)\n>>>\n>>> Thanks,\n>>> Gloria\n>>>\n>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>\n>>> wrote:\n>>>\n>>>> Hi Gloria,\n>>>>\n>>>> > A package may contain transactions that are already in the mempool. We\n>>>> > remove\n>>>> > (\"deduplicate\") those transactions from the package for the purposes\n>>>> of\n>>>> > package\n>>>> > mempool acceptance. If a package is empty after deduplication, we do\n>>>> > nothing.\n>>>>\n>>>> IIUC, you have a package A+B+C submitted for acceptance and A is\n>>>> already in your mempool. You trim out A from the package and then evaluate\n>>>> B+C.\n>>>>\n>>>> I think this might be an issue if A is the higher-fee element of the\n>>>> ABC package. B+C package fees might be under the mempool min fee and will\n>>>> be rejected, potentially breaking the acceptance expectations of the\n>>>> package issuer ?\n>>>>\n>>>> Further, I think the dedup should be done on wtxid, as you might have\n>>>> multiple valid witnesses. Though with varying vsizes and as such offering\n>>>> different feerates.\n>>>>\n>>>> E.g you're going to evaluate the package A+B and A' is already in your\n>>>> mempool with a bigger valid witness. You trim A based on txid, then you\n>>>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would\n>>>> have been a success.\n>>>>\n>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to\n>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we\n>>>> achieve the same goal by bypassing tx-level checks for already-in txn while\n>>>> conserving the package integrity for package-level checks ?\n>>>>\n>>>> > Note that it's possible for the parents to be\n>>>> > indirect\n>>>> > descendants/ancestors of one another, or for parent and child to\n>>>> share a\n>>>> > parent,\n>>>> > so we cannot make any other topology assumptions.\n>>>>\n>>>> I'm not clearly understanding the accepted topologies. By \"parent and\n>>>> child to share a parent\", do you mean the set of transactions A, B, C,\n>>>> where B is spending A and C is spending A and B would be correct ?\n>>>>\n>>>> If yes, is there a width-limit introduced or we fallback on\n>>>> MAX_PACKAGE_COUNT=25 ?\n>>>>\n>>>> IIRC, one rationale to come with this topology limitation was to lower\n>>>> the DoS risks when potentially deploying p2p packages.\n>>>>\n>>>> Considering the current Core's mempool acceptance rules, I think CPFP\n>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>>> jamming successful on one channel commitment transaction would contamine\n>>>> the remaining commitments sharing the same package.\n>>>>\n>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>>>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>>>> better feerate than A, the whole package acceptance will fail. Even if A'\n>>>> confirms in the following block,\n>>>> the propagation and confirmation of B+C+D have been delayed. This could\n>>>> carry on a loss of funds.\n>>>>\n>>>> That said, if you're broadcasting commitment transactions without\n>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee\n>>>> saving as you don't have to duplicate the CPFP.\n>>>>\n>>>> IMHO, I'm leaning towards deploying during a first phase\n>>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>>> second-layer safety.\n>>>>\n>>>> > *Rationale*:  It would be incorrect to use the fees of transactions\n>>>> that are\n>>>> > already in the mempool, as we do not want a transaction's fees to be\n>>>> > double-counted for both its individual RBF and package RBF.\n>>>>\n>>>> I'm unsure about the logical order of the checks proposed.\n>>>>\n>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200 sats\n>>>> and A' pays 100 sats. If we apply the individual RBF on A, A+B acceptance\n>>>> fails. For this reason I think the individual RBF should be bypassed and\n>>>> only the package RBF apply ?\n>>>>\n>>>> Note this situation is plausible, with current LN design, your\n>>>> counterparty can have a commitment transaction with a better fee just by\n>>>> selecting a higher `dust_limit_satoshis` than yours.\n>>>>\n>>>> > Examples F and G [14] show the same package, but P1 is submitted\n>>>> > individually before\n>>>> > the package in example G. In example F, we can see that the 300vB\n>>>> package\n>>>> > pays\n>>>> > an additional 200sat in fees, which is not enough to pay for its own\n>>>> > bandwidth\n>>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>> M1, but\n>>>> > using P1's fees again during package submission would make it look\n>>>> like a\n>>>> > 300sat\n>>>> > increase for a 200vB package. Even including its fees and size would\n>>>> not be\n>>>> > sufficient in this example, since the 300sat looks like enough for\n>>>> the 300vB\n>>>> > package. The calculcation after deduplication is 100sat increase for a\n>>>> > package\n>>>> > of size 200vB, which correctly fails BIP125#4. Assume all\n>>>> transactions have\n>>>> > a\n>>>> > size of 100vB.\n>>>>\n>>>> What problem are you trying to solve by the package feerate *after*\n>>>> dedup rule ?\n>>>>\n>>>> My understanding is that an in-package transaction might be already in\n>>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>>>> vsize of this transaction could be discarded lowering the cost of package\n>>>> RBF.\n>>>>\n>>>> If we keep a \"safe\" dedup mechanism (see my point above), I think this\n>>>> discount is justified, as the validation cost of node operators is paid for\n>>>> ?\n>>>>\n>>>> > The child cannot replace mempool transactions.\n>>>>\n>>>> Let's say you issue package A+B, then package C+B', where B' is a child\n>>>> of both A and C. This rule fails the acceptance of C+B' ?\n>>>>\n>>>> I think this is a footgunish API, as if a package issuer send the\n>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe\n>>>> in protocols where states are symmetric. E.g a malicious counterparty\n>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better\n>>>> fees.\n>>>>\n>>>> > All mempool transactions to be replaced must signal replaceability.\n>>>>\n>>>> I think this is unsafe for L2s if counterparties have malleability of\n>>>> the child transaction. They can block your package replacement by\n>>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>>> ability.\n>>>>\n>>>> I think it's better to either fix inherited signaling or move towards\n>>>> full-rbf.\n>>>>\n>>>> > if a package parent has already been submitted, it would\n>>>> > look\n>>>> >like the child is spending a \"new\" unconfirmed input.\n>>>>\n>>>> I think this is an issue brought by the trimming during the dedup\n>>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>>> checks results of already in-mempool transactions to gain in CPU time we\n>>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>>\n>>>> > However, we still achieve the same goal of requiring the\n>>>> > replacement\n>>>> > transactions to have a ancestor score at least as high as the original\n>>>> > ones.\n>>>>\n>>>> I'm not sure if this holds...\n>>>>\n>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>>\n>>>> Package A + B ancestor score is 10 sat/vb.\n>>>>\n>>>> D has a higher feerate/absolute fee than B.\n>>>>\n>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>>> 1000 sats + D's 1500 sats) /\n>>>> A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>>\n>>>> Overall, this is a review through the lenses of LN requirements. I\n>>>> think other L2 protocols/applications\n>>>> could be candidates to using package accept/relay such as:\n>>>> * https://github.com/lightninglabs/pool\n>>>> * https://github.com/discreetlogcontracts/dlcspecs\n>>>> * https://github.com/bitcoin-teleport/teleport-transactions/\n>>>> * https://github.com/sapio-lang/sapio\n>>>> *\n>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n>>>> * https://github.com/revault/practical-revault\n>>>>\n>>>> Thanks for rolling forward the ball on this subject.\n>>>>\n>>>> Antoine\n>>>>\n>>>> Le jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>\n>>>>> Hi there,\n>>>>>\n>>>>> I'm writing to propose a set of mempool policy changes to enable\n>>>>> package\n>>>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>>>> would not\n>>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>>> significantly affects transaction propagation, I believe this is\n>>>>> relevant for\n>>>>> the mailing list.\n>>>>>\n>>>>> My proposal enables packages consisting of multiple parents and 1\n>>>>> child. If you\n>>>>> develop software that relies on specific transaction relay assumptions\n>>>>> and/or\n>>>>> are interested in using package relay in the future, I'm very\n>>>>> interested to hear\n>>>>> your feedback on the utility or restrictiveness of these package\n>>>>> policies for\n>>>>> your use cases.\n>>>>>\n>>>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>>>> PR#22290][1].\n>>>>>\n>>>>> An illustrated version of this post can be found at\n>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>>> I have also linked the images below.\n>>>>>\n>>>>> ## Background\n>>>>>\n>>>>> Feel free to skip this section if you are already familiar with\n>>>>> mempool policy\n>>>>> and package relay terminology.\n>>>>>\n>>>>> ### Terminology Clarifications\n>>>>>\n>>>>> * Package = an ordered list of related transactions, representable by\n>>>>> a Directed\n>>>>>   Acyclic Graph.\n>>>>> * Package Feerate = the total modified fees divided by the total\n>>>>> virtual size of\n>>>>>   all transactions in the package.\n>>>>>     - Modified fees = a transaction's base fees + fee delta applied by\n>>>>> the user\n>>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>>> across\n>>>>> mempools.\n>>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>>> [BIP141\n>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>>>> Core][3].\n>>>>>     - Note that feerate is not necessarily based on the base fees and\n>>>>> serialized\n>>>>>       size.\n>>>>>\n>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>>> incentives to\n>>>>>   boost a transaction's candidacy for inclusion in a block, including\n>>>>> Child Pays\n>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention\n>>>>> in\n>>>>> mempool policy is to recognize when the new transaction is more\n>>>>> economical to\n>>>>> mine than the original one(s) but not open DoS vectors, so there are\n>>>>> some\n>>>>> limitations.\n>>>>>\n>>>>> ### Policy\n>>>>>\n>>>>> The purpose of the mempool is to store the best (to be most\n>>>>> incentive-compatible\n>>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>>> Miners use\n>>>>> the mempool to build block templates. The mempool is also useful as a\n>>>>> cache for\n>>>>> boosting block relay and validation performance, aiding transaction\n>>>>> relay, and\n>>>>> generating feerate estimations.\n>>>>>\n>>>>> Ideally, all consensus-valid transactions paying reasonable fees\n>>>>> should make it\n>>>>> to miners through normal transaction relay, without any special\n>>>>> connectivity or\n>>>>> relationships with miners. On the other hand, nodes do not have\n>>>>> unlimited\n>>>>> resources, and a P2P network designed to let any honest node broadcast\n>>>>> their\n>>>>> transactions also exposes the transaction validation engine to DoS\n>>>>> attacks from\n>>>>> malicious peers.\n>>>>>\n>>>>> As such, for unconfirmed transactions we are considering for our\n>>>>> mempool, we\n>>>>> apply a set of validation rules in addition to consensus, primarily to\n>>>>> protect\n>>>>> us from resource exhaustion and aid our efforts to keep the highest fee\n>>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>>> node-specific) rules that transactions must abide by in order to be\n>>>>> accepted\n>>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>>> restrictions such\n>>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>>\n>>>>> ### Package Relay and Package Mempool Accept\n>>>>>\n>>>>> In transaction relay, we currently consider transactions one at a time\n>>>>> for\n>>>>> submission to the mempool. This creates a limitation in the node's\n>>>>> ability to\n>>>>> determine which transactions have the highest feerates, since we\n>>>>> cannot take\n>>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>>> transactions are\n>>>>> in the mempool. Similarly, we cannot use a transaction's descendants\n>>>>> when\n>>>>> considering it for RBF. When an individual transaction does not meet\n>>>>> the mempool\n>>>>> minimum feerate and the user isn't able to create a replacement\n>>>>> transaction\n>>>>> directly, it will not be accepted by mempools.\n>>>>>\n>>>>> This limitation presents a security issue for applications and users\n>>>>> relying on\n>>>>> time-sensitive transactions. For example, Lightning and other\n>>>>> protocols create\n>>>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>>>> path opens\n>>>>> up after a timelock, and users are protected from cheating scenarios\n>>>>> as long as\n>>>>> they redeem on-chain in time. A key security assumption is that all\n>>>>> parties'\n>>>>> transactions will propagate and confirm in a timely manner. This\n>>>>> assumption can\n>>>>> be broken if fee-bumping does not work as intended.\n>>>>>\n>>>>> The end goal for Package Relay is to consider multiple transactions at\n>>>>> the same\n>>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>>> better\n>>>>> determine whether transactions should be accepted to our mempool,\n>>>>> especially if\n>>>>> they don't meet fee requirements individually or are better RBF\n>>>>> candidates as a\n>>>>> package. A combination of changes to mempool validation logic, policy,\n>>>>> and\n>>>>> transaction relay allows us to better propagate the transactions with\n>>>>> the\n>>>>> highest package feerates to miners, and makes fee-bumping tools more\n>>>>> powerful\n>>>>> for users.\n>>>>>\n>>>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but\n>>>>> a large\n>>>>> part of the changes are in the mempool's package validation logic. We\n>>>>> call this\n>>>>> *Package Mempool Accept*.\n>>>>>\n>>>>> ### Previous Work\n>>>>>\n>>>>> * Given that mempool validation is DoS-sensitive and complex, it would\n>>>>> be\n>>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>>> efforts have\n>>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>>> [#21062][5],\n>>>>> [#22675][6], [#22796][7]).\n>>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>>> accepts only\n>>>>>   (no submission to mempool).\n>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>>>>> arbitrary\n>>>>>   packages. Still test accepts only.\n>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>>\n>>>>> ### Existing Package Rules\n>>>>>\n>>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>>>>> consider\n>>>>> them as \"given\" in the rest of this document, though they can be\n>>>>> changed, since\n>>>>> package validation is test-accept only right now.\n>>>>>\n>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>>\n>>>>>    *Rationale*: This is already enforced as mempool\n>>>>> ancestor/descendant limits.\n>>>>> Presumably, transactions in a package are all related, so exceeding\n>>>>> this limit\n>>>>> would mean that the package can either be split up or it wouldn't pass\n>>>>> this\n>>>>> mempool policy.\n>>>>>\n>>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>>> between\n>>>>> transactions, parents must appear somewhere before children. [8]\n>>>>>\n>>>>> 3. A package cannot have conflicting transactions, i.e. none of them\n>>>>> can spend\n>>>>> the same inputs. This also means there cannot be duplicate\n>>>>> transactions. [8]\n>>>>>\n>>>>> 4. When packages are evaluated against ancestor/descendant limits in a\n>>>>> test\n>>>>> accept, the union of all of their descendants and ancestors is\n>>>>> considered. This\n>>>>> is essentially a \"worst case\" heuristic where every transaction in the\n>>>>> package\n>>>>> is treated as each other's ancestor and descendant. [8]\n>>>>> Packages for which ancestor/descendant limits are accurately captured\n>>>>> by this\n>>>>> heuristic: [19]\n>>>>>\n>>>>> There are also limitations such as the fact that CPFP carve out is not\n>>>>> applied\n>>>>> to package transactions. #20833 also disables RBF in package\n>>>>> validation; this\n>>>>> proposal overrides that to allow packages to use RBF.\n>>>>>\n>>>>> ## Proposed Changes\n>>>>>\n>>>>> The next step in the Package Mempool Accept project is to implement\n>>>>> submission\n>>>>> to mempool, initially through RPC only. This allows us to test the\n>>>>> submission\n>>>>> logic before exposing it on P2P.\n>>>>>\n>>>>> ### Summary\n>>>>>\n>>>>> - Packages may contain already-in-mempool transactions.\n>>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>>> - Fee-related checks use the package feerate. This means that wallets\n>>>>> can\n>>>>> create a package that utilizes CPFP.\n>>>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>>>> similar\n>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>>\n>>>>> There is a draft implementation in [#22290][1]. It is WIP, but\n>>>>> feedback is\n>>>>> always welcome.\n>>>>>\n>>>>> ### Details\n>>>>>\n>>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>>\n>>>>> A package may contain transactions that are already in the mempool. We\n>>>>> remove\n>>>>> (\"deduplicate\") those transactions from the package for the purposes\n>>>>> of package\n>>>>> mempool acceptance. If a package is empty after deduplication, we do\n>>>>> nothing.\n>>>>>\n>>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>>> parent to be\n>>>>> accepted to the mempool of a peer on its own due to differences in\n>>>>> policy and\n>>>>> fee market fluctuations. We should not reject or penalize the entire\n>>>>> package for\n>>>>> an individual transaction as that could be a censorship vector.\n>>>>>\n>>>>> #### Packages Are Multi-Parent-1-Child\n>>>>>\n>>>>> Only packages of a specific topology are permitted. Namely, a package\n>>>>> is exactly\n>>>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>>>> package\n>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>>>> unconfirmed parents, etc. Note that it's possible for the parents to\n>>>>> be indirect\n>>>>> descendants/ancestors of one another, or for parent and child to share\n>>>>> a parent,\n>>>>> so we cannot make any other topology assumptions.\n>>>>>\n>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>>> parents\n>>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>>> packages to a\n>>>>> defined topology is also easier to reason about and simplifies the\n>>>>> validation\n>>>>> logic greatly. Multi-parent-1-child allows us to think of the package\n>>>>> as one big\n>>>>> transaction, where:\n>>>>>\n>>>>> - Inputs = all the inputs of parents + inputs of the child that come\n>>>>> from\n>>>>>   confirmed UTXOs\n>>>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>>>> that\n>>>>>   aren't spent by other transactions in the package\n>>>>>\n>>>>> Examples of packages that follow this rule (variations of example A\n>>>>> show some\n>>>>> possibilities after deduplication): ![image][15]\n>>>>>\n>>>>> #### Fee-Related Checks Use Package Feerate\n>>>>>\n>>>>> Package Feerate = the total modified fees divided by the total virtual\n>>>>> size of\n>>>>> all transactions in the package.\n>>>>>\n>>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>>> pre-configured\n>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>>> feerate, the\n>>>>> total package feerate is used instead of the individual feerate. The\n>>>>> individual\n>>>>> transactions are allowed to be below feerate requirements if the\n>>>>> package meets\n>>>>> the feerate requirements. For example, the parent(s) in the package\n>>>>> can have 0\n>>>>> fees but be paid for by the child.\n>>>>>\n>>>>> *Rationale*: This can be thought of as \"CPFP within a package,\"\n>>>>> solving the\n>>>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>>>> applications to adjust their fees at broadcast time instead of\n>>>>> overshooting or\n>>>>> risking getting stuck/pinned.\n>>>>>\n>>>>> We use the package feerate of the package *after deduplication*.\n>>>>>\n>>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>> that are\n>>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>>> double-counted for both its individual RBF and package RBF.\n>>>>>\n>>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>>> individually before\n>>>>> the package in example G. In example F, we can see that the 300vB\n>>>>> package pays\n>>>>> an additional 200sat in fees, which is not enough to pay for its own\n>>>>> bandwidth\n>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>> M1, but\n>>>>> using P1's fees again during package submission would make it look\n>>>>> like a 300sat\n>>>>> increase for a 200vB package. Even including its fees and size would\n>>>>> not be\n>>>>> sufficient in this example, since the 300sat looks like enough for the\n>>>>> 300vB\n>>>>> package. The calculcation after deduplication is 100sat increase for a\n>>>>> package\n>>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>>>>> have a\n>>>>> size of 100vB.\n>>>>>\n>>>>> #### Package RBF\n>>>>>\n>>>>> If a package meets feerate requirements as a package, the parents in\n>>>>> the\n>>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>>> child cannot\n>>>>> replace mempool transactions. Multiple transactions can replace the\n>>>>> same\n>>>>> transaction, but in order to be valid, none of the transactions can\n>>>>> try to\n>>>>> replace an ancestor of another transaction in the same package (which\n>>>>> would thus\n>>>>> make its inputs unavailable).\n>>>>>\n>>>>> *Rationale*: Even if we are using package feerate, a package will not\n>>>>> propagate\n>>>>> as intended if RBF still requires each individual transaction to meet\n>>>>> the\n>>>>> feerate requirements.\n>>>>>\n>>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>>\n>>>>> ##### Signaling (Rule #1)\n>>>>>\n>>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>>\n>>>>> *Rationale*: Package RBF signaling logic should be the same for\n>>>>> package RBF and\n>>>>> single transaction acceptance. This would be updated if single\n>>>>> transaction\n>>>>> validation moves to full RBF.\n>>>>>\n>>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>>\n>>>>> A package may include new unconfirmed inputs, but the ancestor feerate\n>>>>> of the\n>>>>> child must be at least as high as the ancestor feerates of every\n>>>>> transaction\n>>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>>> replacement\n>>>>> transaction may only include an unconfirmed input if that input was\n>>>>> included in\n>>>>> one of the original transactions. (An unconfirmed input spends an\n>>>>> output from a\n>>>>> currently-unconfirmed transaction.)\"\n>>>>>\n>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>>>> transaction has a higher ancestor score than the original\n>>>>> transaction(s) (see\n>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed\n>>>>> input can lower the\n>>>>> ancestor score of the replacement transaction. P1 is trying to replace\n>>>>> M1, and\n>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,\n>>>>> and M2 pays\n>>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>>> isolation, P1\n>>>>> looks like a better mining candidate than M1, it must be mined with\n>>>>> M2, so its\n>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>>> ancestor\n>>>>> feerate, which is 6sat/vB.\n>>>>>\n>>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>>> transactions in the package can spend new unconfirmed inputs.\" Example\n>>>>> J [17] shows\n>>>>> why, if any of the package transactions have ancestors, package\n>>>>> feerate is no\n>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which\n>>>>> is the\n>>>>> replacement transaction in an RBF), we're actually interested in the\n>>>>> entire\n>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,\n>>>>> P1, P2, and\n>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>>>> only allow\n>>>>> the child to have new unconfirmed inputs, either, because it can still\n>>>>> cause us\n>>>>> to overestimate the package's ancestor score.\n>>>>>\n>>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>>> Package RBF\n>>>>> less useful, but would also break Package RBF for packages with\n>>>>> parents already\n>>>>> in the mempool: if a package parent has already been submitted, it\n>>>>> would look\n>>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>>> [18], we're\n>>>>> looking to replace M1 with the entire package including P1, P2, and\n>>>>> P3. We must\n>>>>> consider the case where one of the parents is already in the mempool\n>>>>> (in this\n>>>>> case, P2), which means we must allow P3 to have new unconfirmed\n>>>>> inputs. However,\n>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>>> replace M1\n>>>>> with this package.\n>>>>>\n>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>>> strict than\n>>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>>> replacement\n>>>>> transactions to have a ancestor score at least as high as the original\n>>>>> ones. As\n>>>>> a result, the entire package is required to be a higher feerate mining\n>>>>> candidate\n>>>>> than each of the replaced transactions.\n>>>>>\n>>>>> Another note: the [comment][13] above the BIP125#2 code in the\n>>>>> original RBF\n>>>>> implementation suggests that the rule was intended to be temporary.\n>>>>>\n>>>>> ##### Absolute Fee (Rule #3)\n>>>>>\n>>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>>> total fees\n>>>>> of the package must be higher than the absolute fees of the mempool\n>>>>> transactions\n>>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>>> BIP125 Rule #3\n>>>>> - an individual transaction in the package may have lower fees than the\n>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>>>>> child\n>>>>> pays for RBF.\n>>>>>\n>>>>> ##### Feerate (Rule #4)\n>>>>>\n>>>>> The package must pay for its own bandwidth; the package feerate must\n>>>>> be higher\n>>>>> than the replaced transactions by at least minimum relay feerate\n>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>>> differs from\n>>>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>>>> lower\n>>>>> feerate than the transaction(s) it is replacing. In fact, it may have\n>>>>> 0 fees,\n>>>>> and the child pays for RBF.\n>>>>>\n>>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>>\n>>>>> The package cannot replace more than 100 mempool transactions. This is\n>>>>> identical\n>>>>> to BIP125 Rule #5.\n>>>>>\n>>>>> ### Expected FAQs\n>>>>>\n>>>>> 1. Is it possible for only some of the package to make it into the\n>>>>> mempool?\n>>>>>\n>>>>>    Yes, it is. However, since we evict transactions from the mempool by\n>>>>> descendant score and the package child is supposed to be sponsoring\n>>>>> the fees of\n>>>>> its parents, the most common scenario would be all-or-nothing. This is\n>>>>> incentive-compatible. In fact, to be conservative, package validation\n>>>>> should\n>>>>> begin by trying to submit all of the transactions individually, and\n>>>>> only use the\n>>>>> package mempool acceptance logic if the parents fail due to low\n>>>>> feerate.\n>>>>>\n>>>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>>>\n>>>>>     No, for practical reasons. In mempool validation, we actually\n>>>>> aren't able to\n>>>>> tell with 100% confidence if we are looking at a transaction that has\n>>>>> already\n>>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>>> historical\n>>>>> block data, it's possible to look for it, but this is inefficient, not\n>>>>> always\n>>>>> possible for pruning nodes, and unnecessary because we're not going to\n>>>>> do\n>>>>> anything with the transaction anyway. As such, we already have the\n>>>>> expectation\n>>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>>> relaying\n>>>>> transactions that have already been confirmed. Similarly, we shouldn't\n>>>>> be\n>>>>> relaying packages that contain already-confirmed transactions.\n>>>>>\n>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>>> [2]:\n>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>>> [3]:\n>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>>> [13]:\n>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>>> [14]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>>> [15]:\n>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>>> [16]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>>> [17]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>>> [18]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>>> [19]:\n>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>>> [20]:\n>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210926/16f08de9/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2021-09-27T07:15:18",
                "message_text_only": ">\n> I think we could restrain package acceptance to only confirmed inputs for\n> now and revisit later this point ? For LN-anchor, you can assume that the\n> fee-bumping UTXO feeding the CPFP is already\n> confirmed. Or are there currently-deployed use-cases which would benefit\n> from your proposed Rule #2 ?\n>\n\nI think constraining package acceptance to only confirmed inputs\nis very limiting and quite dangerous for L2 protocols.\n\nIn the case of LN, an attacker can game this and heavily restrict\nyour RBF attempts if you're only allowed to use confirmed inputs\nand have many channels (and a limited number of confirmed inputs).\nOtherwise you'll need node operators to pre-emptively split their\nutxos into many small utxos just for fee bumping, which is inefficient...\n\nBastien\n\nLe lun. 27 sept. 2021 \u00e0 00:27, Antoine Riard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi Gloria,\n>\n> Thanks for your answers,\n>\n> > In summary, it seems that the decisions that might still need\n> > attention/input from devs on this mailing list are:\n> > 1. Whether we should start with multiple-parent-1-child or\n> 1-parent-1-child.\n> > 2. Whether it's ok to require that the child not have conflicts with\n> > mempool transactions.\n>\n> Yes 1) it would be good to have inputs of more potential users of package\n> acceptance . And 2) I think it's more a matter of clearer wording of the\n> proposal.\n>\n> However, see my final point on the relaxation around \"unconfirmed inputs\"\n> which might in fact alter our current block construction strategy.\n>\n> > Right, the fact that we essentially always choose the first-seen witness\n> is\n> > an unfortunate limitation that exists already. Adding package mempool\n> > accept doesn't worsen this, but the procedure in the future is to replace\n> > the witness when it makes sense economically. We can also add logic to\n> > allow package feerate to pay for witness replacements as well. This is\n> > pretty far into the future, though.\n>\n> Yes I agree package mempool doesn't worsen this. And it's not an issue for\n> current LN as you can't significantly inflate a spending witness for the\n> 2-of-2 funding output.\n> However, it might be an issue for multi-party protocol where the spending\n> script has alternative branches with asymmetric valid witness weights.\n> Taproot should ease that kind of script so hopefully we would deploy\n> wtxid-replacement not too far in the future.\n>\n> > I could be misunderstanding, but an attacker wouldn't be able to\n> > batch-attack like this. Alice's package only conflicts with A' + D', not\n> A'\n> > + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n>\n> Yeah I can be clearer, I think you have 2 pinning attacks scenarios to\n> consider.\n>\n> In LN, if you're trying to confirm a commitment transaction to time-out or\n> claim on-chain a HTLC and the timelock is near-expiration, you should be\n> ready to pay in commitment+2nd-stage HTLC transaction fees as much as the\n> value offered by the HTLC.\n>\n> Following this security assumption, an attacker can exploit it by\n> targeting together commitment transactions from different channels by\n> blocking them under a high-fee child, of which the fee value\n> is equal to the top-value HTLC + 1. Victims's fee-bumping logics won't\n> overbid as it's not worthy to offer fees beyond their competed HTLCs. Apart\n> from observing mempools state, victims can't learn they're targeted by the\n> same attacker.\n>\n> To draw from the aforementioned topology, Mallory broadcasts A' + B' + C'\n> + D', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'\n> conflicts with Caroll's P3. Let's assume P1 is confirming the top-value\n> HTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for\n> Alice or Bob or Caroll to keep offering competing feerates. Mallory will be\n> at loss on stealing P1, as she has paid more in fees but will realize a\n> gain on P2+P3.\n>\n> In this model, Alice is allowed to evict those 2 transactions (A' + D')\n> but as she is economically-bounded she won't succeed.\n>\n> Mallory is maliciously exploiting RBF rule 3 on absolute fee. I think this\n> 1st pinning scenario is correct and \"lucractive\" when you sum the global\n> gain/loss.\n>\n> There is a 2nd attack scenario where A + B + C + D, where D is the child\n> of A,B,C. All those transactions are honestly issued by Alice. Once A + B +\n> C + D are propagated in network mempools, Mallory is able to replace A + D\n> with  A' + D' where D' is paying a higher fee. This package A' + D' will\n> confirm soon if D feerate was compelling but Mallory succeeds in delaying\n> the confirmation\n> of B + C for one or more blocks. As B + C are pre-signed commitments with\n> a low-fee rate they won't confirm without Alice issuing a new child E.\n> Mallory can repeat the same trick by broadcasting\n> B' + E' and delay again the confirmation of C.\n>\n> If the remaining package pending HTLC has a higher-value than all the\n> malicious fees over-bid, Mallory should realize a gain. With this 2nd\n> pinning attack, the malicious entity buys confirmation delay of your\n> packaged-together commitments.\n>\n> Assuming those attacks are correct, I'm leaning towards being conservative\n> with the LDK broadcast backend. Though once again, other L2 devs have\n> likely other use-cases and opinions :)\n>\n> >  B' only needs to pay for itself in this case.\n>\n> Yes I think it's a nice discount when UTXO is single-owned. In the context\n> of shared-owned UTXO (e.g LN), you might not if there is an in-mempool\n> package already spending the UTXO and have to assume the worst-case\n> scenario. I.e have B' committing enough fee to pay for A' replacement\n> bandwidth. I think we can't do that much for this case...\n>\n> > If a package meets feerate requirements as a\n> package, the parents in the transaction are allowed to replace-by-fee\n> mempool transactions. The child cannot replace mempool transactions.\"\n>\n> I agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B' to\n> replace A+B because the first broadcast isn't satisfying anymore due to\n> mempool spikes ? Assuming B' fees is enough, I think that case as child B'\n> replacing in-mempool transaction B. Which I understand going against  \"The\n> child cannot replace mempool transactions\".\n>\n> Maybe wording could be a bit clearer ?\n>\n> > While it would be nice to have full RBF, malleability of the child won't\n> > block RBF here. If we're trying to replace A', we only require that A'\n> > signals replaceability, and don't mind if its child doesn't.\n>\n> Yes, it sounds good.\n>\n> > Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\n> miner\n> > should prefer to utilize their block space more effectively.\n>\n> If your mempool is empty and only composed of A+C+D or A+B, I think taking\n> A+C+D is the most efficient block construction you can come up with as a\n> miner ?\n>\n> > No, because we don't use that model.\n>\n> Can you describe what miner model we are using ? Like the block\n> construction strategy implemented by `addPackagesTxs` or also encompassing\n> our current mempool acceptance policy, which I think rely on absolute fee\n> over ancestor score in case of replacement ?\n>\n> I think this point is worthy to discuss as otherwise we might downgrade\n> the efficiency of our current block construction strategy in periods of\n> near-empty mempools. A knowledge which could be discreetly leveraged by a\n> miner to gain an advantage on the rest of the mining ecosystem.\n>\n> Note, I think we *might* have to go in this direction if we want to\n> replace replace-by-fee by replace-by-feerate or replace-by-ancestor and\n> solve in-depth pinning attacks. Though if we do so,\n> IMO we would need more thoughts.\n>\n> I think we could restrain package acceptance to only confirmed inputs for\n> now and revisit later this point ? For LN-anchor, you can assume that the\n> fee-bumping UTXO feeding the CPFP is already\n> confirmed. Or are there currently-deployed use-cases which would benefit\n> from your proposed Rule #2 ?\n>\n> Antoine\n>\n> Le jeu. 23 sept. 2021 \u00e0 11:36, Gloria Zhao <gloriajzhao at gmail.com> a\n> \u00e9crit :\n>\n>> Hi Antoine,\n>>\n>> Thanks as always for your input. I'm glad we agree on so much!\n>>\n>> In summary, it seems that the decisions that might still need\n>> attention/input from devs on this mailing list are:\n>> 1. Whether we should start with multiple-parent-1-child or\n>> 1-parent-1-child.\n>> 2. Whether it's ok to require that the child not have conflicts with\n>> mempool transactions.\n>>\n>> Responding to your comments...\n>>\n>> > IIUC, you have package A+B, during the dedup phase early in\n>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n>> and A' is higher feerate than A, you trim A and replace by A' ?\n>>\n>> > I think this approach is safe, the one who appears unsafe to me is when\n>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?\n>> In that case iirc that would be a pinning.\n>>\n>> Right, the fact that we essentially always choose the first-seen witness\n>> is an unfortunate limitation that exists already. Adding package mempool\n>> accept doesn't worsen this, but the procedure in the future is to replace\n>> the witness when it makes sense economically. We can also add logic to\n>> allow package feerate to pay for witness replacements as well. This is\n>> pretty far into the future, though.\n>>\n>> > It sounds uneconomical for an attacker but I think it's not when you\n>> consider than you can \"batch\" attack against multiple honest\n>> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts\n>> with Alice's honest package P1, B' conflicts with Bob's honest package P2,\n>> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of\n>> A' + B' + C'.\n>>\n>> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of\n>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>>\n>> I could be misunderstanding, but an attacker wouldn't be able to\n>> batch-attack like this. Alice's package only conflicts with A' + D', not A'\n>> + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n>>\n>> > Do we assume that broadcasted packages are \"honest\" by default and that\n>> the parent(s) always need the child to pass the fee checks, that way saving\n>> the processing of individual transactions which are expected to fail in 99%\n>> of cases or more ad hoc composition of packages at relay ?\n>> > I think this point is quite dependent on the p2p packages format/logic\n>> we'll end up on and that we should feel free to revisit it later ?\n>>\n>> I think it's the opposite; there's no way for us to assume that p2p\n>> packages will be \"honest.\" I'd like to have two things before we expose on\n>> P2P: (1) ensure that the amount of resources potentially allocated for\n>> package validation isn't disproportionately higher than that of single\n>> transaction validation and (2) only use package validation when we're\n>> unsatisifed with the single validation result, e.g. we might get better\n>> fees.\n>> Yes, let's revisit this later :)\n>>\n>>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n>> discard its feerate as B should pay for all fees checked on its own. Where\n>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n>> have a fee high enough to cover the bandwidth penalty replacement\n>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>>\n>>  B' only needs to pay for itself in this case.\n>>\n>> > > Do we want the child to be able to replace mempool transactions as\n>> well?\n>>\n>> > If we mean when you have replaceable A+B then A'+B' try to replace with\n>> a higher-feerate ? I think that's exactly the case we need for Lightning as\n>> A+B is coming from Alice and A'+B' is coming from Bob :/\n>>\n>> Let me clarify this because I can see that my wording was ambiguous, and\n>> then please let me know if it fits Lightning's needs?\n>>\n>> In my proposal, I wrote \"If a package meets feerate requirements as a\n>> package, the parents in the transaction are allowed to replace-by-fee\n>> mempool transactions. The child cannot replace mempool transactions.\" What\n>> I meant was: the package can replace mempool transactions if any of the\n>> parents conflict with mempool transactions. The child cannot not conflict\n>> with any mempool transactions.\n>> The Lightning use case this attempts to address is: Alice and Mallory are\n>> LN counterparties, and have packages A+B and A'+B', respectively. A and A'\n>> are their commitment transactions and conflict with each other; they have\n>> shared inputs and different txids.\n>> B spends Alice's anchor output from A. B' spends Mallory's anchor output\n>> from A'. Thus, B and B' do not conflict with each other.\n>> Alice can broadcast her package, A+B, to replace Mallory's package,\n>> A'+B', since B doesn't conflict with the mempool.\n>>\n>> Would this be ok?\n>>\n>> > The second option, a child of A', In the LN case I think the CPFP is\n>> attached on one's anchor output.\n>>\n>> While it would be nice to have full RBF, malleability of the child won't\n>> block RBF here. If we're trying to replace A', we only require that A'\n>> signals replaceability, and don't mind if its child doesn't.\n>>\n>> > > B has an ancestor score of 10sat/vb and D has an\n>> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\n>> B's,\n>> > > it fails the proposed package RBF Rule #2, so this package would be\n>> > > rejected. Does this meet your expectations?\n>>\n>> > Well what sounds odd to me, in my example, we fail D even if it has a\n>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n>> fees are 4500 sats ?\n>>\n>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\n>> miner should prefer to utilize their block space more effectively.\n>>\n>> > Is this compatible with a model where a miner prioritizes absolute fees\n>> over ancestor score, in the case that mempools aren't full-enough to\n>> fulfill a block ?\n>>\n>> No, because we don't use that model.\n>>\n>> Thanks,\n>> Gloria\n>>\n>> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>\n>> wrote:\n>>\n>>> > Correct, if B+C is too low feerate to be accepted, we will reject it. I\n>>> > prefer this because it is incentive compatible: A can be mined by\n>>> itself,\n>>> > so there's no reason to prefer A+B+C instead of A.\n>>> > As another way of looking at this, consider the case where we do accept\n>>> > A+B+C and it sits at the \"bottom\" of our mempool. If our mempool\n>>> reaches\n>>> > capacity, we evict the lowest descendant feerate transactions, which\n>>> are\n>>> > B+C in this case. This gives us the same resulting mempool, with A and\n>>> not\n>>> > B+C.\n>>>\n>>> I agree here. Doing otherwise, we might evict other transactions mempool\n>>> in `MempoolAccept::Finalize` with a higher-feerate than B+C while those\n>>> evicted transactions are the most compelling for block construction.\n>>>\n>>> I thought at first missing this acceptance requirement would break a\n>>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is\n>>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the\n>>> child fee is capturing the parent value. I can't think of other fee-bumping\n>>> schemes potentially affected. If they do exist I would say they're wrong in\n>>> their design assumptions.\n>>>\n>>> > If or when we have witness replacement, the logic is: if the individual\n>>> > transaction is enough to replace the mempool one, the replacement will\n>>> > happen during the preceding individual transaction acceptance, and\n>>> > deduplication logic will work. Otherwise, we will try to deduplicate by\n>>> > wtxid, see that we need a package witness replacement, and use the\n>>> package\n>>> > feerate to evaluate whether this is economically rational.\n>>>\n>>> IIUC, you have package A+B, during the dedup phase early in\n>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n>>> and A' is higher feerate than A, you trim A and replace by A' ?\n>>>\n>>> I think this approach is safe, the one who appears unsafe to me is when\n>>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?\n>>> In that case iirc that would be a pinning.\n>>>\n>>> Good to see progress on witness replacement before we see usage of\n>>> Taproot tree in the context of multi-party, where a malicious counterparty\n>>> inflates its witness to jam a honest spending.\n>>>\n>>> (Note, the commit linked currently points nowhere :))\n>>>\n>>>\n>>> > Please note that A may replace A' even if A' has higher fees than A\n>>> > individually, because the proposed package RBF utilizes the fees and\n>>> size\n>>> > of the entire package. This just requires E to pay enough fees,\n>>> although\n>>> > this can be pretty high if there are also potential B' and C' competing\n>>> > commitment transactions that we don't know about.\n>>>\n>>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for\n>>> the individual check on A, the honest package should replace the pinning\n>>> attempt. I've not fully parsed the proposed implementation yet.\n>>>\n>>> Though note, I think it's still unsafe for a Lightning\n>>> multi-commitment-broadcast-as-one-package as a malicious A' might have an\n>>> absolute fee higher than E. It sounds uneconomical for\n>>> an attacker but I think it's not when you consider than you can \"batch\"\n>>> attack against multiple honest counterparties. E.g, Mallory broadcast A' +\n>>> B' + C' + D' where A' conflicts with Alice's honest package P1, B'\n>>> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest\n>>> package P3. And D' is a high-fee child of A' + B' + C'.\n>>>\n>>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of HTLCs\n>>> confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>>>\n>>> > So far, my understanding is that multi-parent-1-child is desired for\n>>> > batched fee-bumping (\n>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>>> and\n>>> > I've also seen your response which I have less context on (\n>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>>> That\n>>> > being said, I am happy to create a new proposal for 1 parent + 1 child\n>>> > (which would be slightly simpler) and plan for moving to\n>>> > multi-parent-1-child later if that is preferred. I am very interested\n>>> in\n>>> > hearing feedback on that approach.\n>>>\n>>> I think batched fee-bumping is okay as long as you don't have\n>>> time-sensitive outputs encumbering your commitment transactions. For the\n>>> reasons mentioned above, I think that's unsafe.\n>>>\n>>> What I'm worried about is  L2 developers, potentially not aware about\n>>> all the mempool subtleties blurring the difference and always batching\n>>> their broadcast by default.\n>>>\n>>> IMO, a good thing by restraining to 1-parent + 1 child,  we artificially\n>>> constraint L2 design space for now and minimize risks of unsafe usage of\n>>> the package API :)\n>>>\n>>> I think that's a point where it would be relevant to have the opinion of\n>>> more L2 devs.\n>>>\n>>> > I think there is a misunderstanding here - let me describe what I'm\n>>> > proposing we'd do in this situation: we'll try individual submission\n>>> for A,\n>>> > see that it fails due to \"insufficient fees.\" Then, we'll try package\n>>> > validation for A+B and use package RBF. If A+B pays enough, it can\n>>> still\n>>> > replace A'. If A fails for a bad signature, we won't look at B or A+B.\n>>> Does\n>>> > this meet your expectations?\n>>>\n>>> Yes there was a misunderstanding, I think this approach is correct, it's\n>>> more a question of performance. Do we assume that broadcasted packages are\n>>> \"honest\" by default and that the parent(s) always need the child to pass\n>>> the fee checks, that way saving the processing of individual transactions\n>>> which are expected to fail in 99% of cases or more ad hoc composition of\n>>> packages at relay ?\n>>>\n>>> I think this point is quite dependent on the p2p packages format/logic\n>>> we'll end up on and that we should feel free to revisit it later ?\n>>>\n>>>\n>>> > What problem are you trying to solve by the package feerate *after*\n>>> dedup\n>>> rule ?\n>>> > My understanding is that an in-package transaction might be already in\n>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>>> vsize of this transaction could be discarded lowering the cost of package\n>>> RBF.\n>>>\n>>> > I'm proposing that, when a transaction has already been submitted to\n>>> > mempool, we would ignore both its fees and vsize when calculating\n>>> package\n>>> > feerate.\n>>>\n>>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n>>> discard its feerate as B should pay for all fees checked on its own. Where\n>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n>>> have a fee high enough to cover the bandwidth penalty replacement\n>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>>>\n>>> If you have a second-layer like current Lightning, you might have a\n>>> counterparty commitment to replace and should always expect to have to pay\n>>> for parent replacement bandwidth.\n>>>\n>>> Where a potential discount sounds interesting is when you have an\n>>> univoque state on the first-stage of transactions. E.g DLC's funding\n>>> transaction which might be CPFP by any participant iirc.\n>>>\n>>> > Note that, if C' conflicts with C, it also conflicts with D, since D\n>>> is a\n>>> > descendant of C and would thus need to be evicted along with it.\n>>>\n>>> Ah once again I think it's a misunderstanding without the code under my\n>>> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark\n>>> for potential eviction D and don't consider it for future conflicts in the\n>>> rest of the package, I think D' `PreChecks` should be good ?\n>>>\n>>> > More generally, this example is surprising to me because I didn't think\n>>> > packages would be used to fee-bump replaceable transactions. Do we\n>>> want the\n>>> > child to be able to replace mempool transactions as well?\n>>>\n>>> If we mean when you have replaceable A+B then A'+B' try to replace with\n>>> a higher-feerate ? I think that's exactly the case we need for Lightning as\n>>> A+B is coming from Alice and A'+B' is coming from Bob :/\n>>>\n>>> > I'm not sure what you mean? Let's say we have a package of parent A +\n>>> child\n>>> > B, where A is supposed to replace a mempool transaction A'. Are you\n>>> saying\n>>> > that counterparties are able to malleate the package child B, or a\n>>> child of\n>>> > A'?\n>>>\n>>> The second option, a child of A', In the LN case I think the CPFP is\n>>> attached on one's anchor output.\n>>>\n>>> I think it's good if we assume the\n>>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing\n>>> inherited signaling or full-rbf ?\n>>>\n>>> > Sorry, I don't understand what you mean by \"preserve the package\n>>> > integrity?\" Could you elaborate?\n>>>\n>>> After thinking the relaxation about the \"new\" unconfirmed input is not\n>>> linked to trimming but I would say more to the multi-parent support.\n>>>\n>>> Let's say you have A+B trying to replace C+D where B is also spending\n>>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed\n>>> input as D isn't spending E.\n>>>\n>>> So good, I think we agree on the problem description here.\n>>>\n>>> > I am in agreement with your calculations but unsure if we disagree on\n>>> the\n>>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n>>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\n>>> B's,\n>>> > it fails the proposed package RBF Rule #2, so this package would be\n>>> > rejected. Does this meet your expectations?\n>>>\n>>> Well what sounds odd to me, in my example, we fail D even if it has a\n>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n>>> fees are 4500 sats ?\n>>>\n>>> Is this compatible with a model where a miner prioritizes absolute fees\n>>> over ancestor score, in the case that mempools aren't full-enough to\n>>> fulfill a block ?\n>>>\n>>> Let me know if I can clarify a point.\n>>>\n>>> Antoine\n>>>\n>>> Le lun. 20 sept. 2021 \u00e0 11:10, Gloria Zhao <gloriajzhao at gmail.com> a\n>>> \u00e9crit :\n>>>\n>>>>\n>>>> Hi Antoine,\n>>>>\n>>>> First of all, thank you for the thorough review. I appreciate your\n>>>> insight on LN requirements.\n>>>>\n>>>> > IIUC, you have a package A+B+C submitted for acceptance and A is\n>>>> already in your mempool. You trim out A from the package and then evaluate\n>>>> B+C.\n>>>>\n>>>> > I think this might be an issue if A is the higher-fee element of the\n>>>> ABC package. B+C package fees might be under the mempool min fee and will\n>>>> be rejected, potentially breaking the acceptance expectations of the\n>>>> package issuer ?\n>>>>\n>>>> Correct, if B+C is too low feerate to be accepted, we will reject it. I\n>>>> prefer this because it is incentive compatible: A can be mined by itself,\n>>>> so there's no reason to prefer A+B+C instead of A.\n>>>> As another way of looking at this, consider the case where we do accept\n>>>> A+B+C and it sits at the \"bottom\" of our mempool. If our mempool reaches\n>>>> capacity, we evict the lowest descendant feerate transactions, which are\n>>>> B+C in this case. This gives us the same resulting mempool, with A and not\n>>>> B+C.\n>>>>\n>>>>\n>>>> > Further, I think the dedup should be done on wtxid, as you might have\n>>>> multiple valid witnesses. Though with varying vsizes and as such offering\n>>>> different feerates.\n>>>>\n>>>> I agree that variations of the same package with different witnesses is\n>>>> a case that must be handled. I consider witness replacement to be a project\n>>>> that can be done in parallel to package mempool acceptance because being\n>>>> able to accept packages does not worsen the problem of a\n>>>> same-txid-different-witness \"pinning\" attack.\n>>>>\n>>>> If or when we have witness replacement, the logic is: if the individual\n>>>> transaction is enough to replace the mempool one, the replacement will\n>>>> happen during the preceding individual transaction acceptance, and\n>>>> deduplication logic will work. Otherwise, we will try to deduplicate by\n>>>> wtxid, see that we need a package witness replacement, and use the package\n>>>> feerate to evaluate whether this is economically rational.\n>>>>\n>>>> See the #22290 \"handle package transactions already in mempool\" commit (\n>>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),\n>>>> which handles the case of same-txid-different-witness by simply using the\n>>>> transaction in the mempool for now, with TODOs for what I just described.\n>>>>\n>>>>\n>>>> > I'm not clearly understanding the accepted topologies. By \"parent and\n>>>> child to share a parent\", do you mean the set of transactions A, B, C,\n>>>> where B is spending A and C is spending A and B would be correct ?\n>>>>\n>>>> Yes, that is what I meant. Yes, that would a valid package under these\n>>>> rules.\n>>>>\n>>>> > If yes, is there a width-limit introduced or we fallback on\n>>>> MAX_PACKAGE_COUNT=25 ?\n>>>>\n>>>> No, there is no limit on connectivity other than \"child with all\n>>>> unconfirmed parents.\" We will enforce MAX_PACKAGE_COUNT=25 and child's\n>>>> in-mempool + in-package ancestor limits.\n>>>>\n>>>>\n>>>> > Considering the current Core's mempool acceptance rules, I think CPFP\n>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>>> jamming successful on one channel commitment transaction would contamine\n>>>> the remaining commitments sharing the same package.\n>>>>\n>>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>>>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>>>> better feerate than A, the whole package acceptance will fail. Even if A'\n>>>> confirms in the following block,\n>>>> the propagation and confirmation of B+C+D have been delayed. This could\n>>>> carry on a loss of funds.\n>>>>\n>>>> Please note that A may replace A' even if A' has higher fees than A\n>>>> individually, because the proposed package RBF utilizes the fees and size\n>>>> of the entire package. This just requires E to pay enough fees, although\n>>>> this can be pretty high if there are also potential B' and C' competing\n>>>> commitment transactions that we don't know about.\n>>>>\n>>>>\n>>>> > IMHO, I'm leaning towards deploying during a first phase\n>>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>>> second-layer safety.\n>>>>\n>>>> So far, my understanding is that multi-parent-1-child is desired for\n>>>> batched fee-bumping (\n>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>>>> and I've also seen your response which I have less context on (\n>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>>>> That being said, I am happy to create a new proposal for 1 parent + 1 child\n>>>> (which would be slightly simpler) and plan for moving to\n>>>> multi-parent-1-child later if that is preferred. I am very interested in\n>>>> hearing feedback on that approach.\n>>>>\n>>>>\n>>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200\n>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B\n>>>> acceptance fails. For this reason I think the individual RBF should be\n>>>> bypassed and only the package RBF apply ?\n>>>>\n>>>> I think there is a misunderstanding here - let me describe what I'm\n>>>> proposing we'd do in this situation: we'll try individual submission for A,\n>>>> see that it fails due to \"insufficient fees.\" Then, we'll try package\n>>>> validation for A+B and use package RBF. If A+B pays enough, it can still\n>>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does\n>>>> this meet your expectations?\n>>>>\n>>>>\n>>>> > What problem are you trying to solve by the package feerate *after*\n>>>> dedup rule ?\n>>>> > My understanding is that an in-package transaction might be already\n>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,\n>>>> the vsize of this transaction could be discarded lowering the cost of\n>>>> package RBF.\n>>>>\n>>>> I'm proposing that, when a transaction has already been submitted to\n>>>> mempool, we would ignore both its fees and vsize when calculating package\n>>>> feerate. In example G2, we shouldn't count M1 fees after its submission to\n>>>> mempool, since M1's fees have already been used to pay for its individual\n>>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.\n>>>> We also shouldn't count its vsize, since it has already been paid for.\n>>>>\n>>>>\n>>>> > I think this is a footgunish API, as if a package issuer send the\n>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>>> higher-feerate/higher-fee package always replaces ?\n>>>>\n>>>> Note that, if C' conflicts with C, it also conflicts with D, since D is\n>>>> a descendant of C and would thus need to be evicted along with it.\n>>>> Implicitly, D' would not be in conflict with D.\n>>>> More generally, this example is surprising to me because I didn't think\n>>>> packages would be used to fee-bump replaceable transactions. Do we want the\n>>>> child to be able to replace mempool transactions as well? This can be\n>>>> implemented with a bit of additional logic.\n>>>>\n>>>> > I think this is unsafe for L2s if counterparties have malleability of\n>>>> the child transaction. They can block your package replacement by\n>>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>>> ability.\n>>>>\n>>>> I'm not sure what you mean? Let's say we have a package of parent A +\n>>>> child B, where A is supposed to replace a mempool transaction A'. Are you\n>>>> saying that counterparties are able to malleate the package child B, or a\n>>>> child of A'? If they can malleate a child of A', that shouldn't matter as\n>>>> long as A' is signaling replacement. This would be handled identically with\n>>>> full RBF and what Core currently implements.\n>>>>\n>>>> > I think this is an issue brought by the trimming during the dedup\n>>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>>> checks results of already in-mempool transactions to gain in CPU time we\n>>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>>\n>>>> Sorry, I don't understand what you mean by \"preserve the package\n>>>> integrity?\" Could you elaborate?\n>>>>\n>>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>>\n>>>> > Package A + B ancestor score is 10 sat/vb.\n>>>>\n>>>> > D has a higher feerate/absolute fee than B.\n>>>>\n>>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>>\n>>>> I am in agreement with your calculations but unsure if we disagree on\n>>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n>>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n>>>> it fails the proposed package RBF Rule #2, so this package would be\n>>>> rejected. Does this meet your expectations?\n>>>>\n>>>> Thank you for linking to projects that might be interested in package\n>>>> relay :)\n>>>>\n>>>> Thanks,\n>>>> Gloria\n>>>>\n>>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <antoine.riard at gmail.com>\n>>>> wrote:\n>>>>\n>>>>> Hi Gloria,\n>>>>>\n>>>>> > A package may contain transactions that are already in the mempool.\n>>>>> We\n>>>>> > remove\n>>>>> > (\"deduplicate\") those transactions from the package for the purposes\n>>>>> of\n>>>>> > package\n>>>>> > mempool acceptance. If a package is empty after deduplication, we do\n>>>>> > nothing.\n>>>>>\n>>>>> IIUC, you have a package A+B+C submitted for acceptance and A is\n>>>>> already in your mempool. You trim out A from the package and then evaluate\n>>>>> B+C.\n>>>>>\n>>>>> I think this might be an issue if A is the higher-fee element of the\n>>>>> ABC package. B+C package fees might be under the mempool min fee and will\n>>>>> be rejected, potentially breaking the acceptance expectations of the\n>>>>> package issuer ?\n>>>>>\n>>>>> Further, I think the dedup should be done on wtxid, as you might have\n>>>>> multiple valid witnesses. Though with varying vsizes and as such offering\n>>>>> different feerates.\n>>>>>\n>>>>> E.g you're going to evaluate the package A+B and A' is already in your\n>>>>> mempool with a bigger valid witness. You trim A based on txid, then you\n>>>>> evaluate A'+B, which fails the fee checks. However, evaluating A+B would\n>>>>> have been a success.\n>>>>>\n>>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to\n>>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we\n>>>>> achieve the same goal by bypassing tx-level checks for already-in txn while\n>>>>> conserving the package integrity for package-level checks ?\n>>>>>\n>>>>> > Note that it's possible for the parents to be\n>>>>> > indirect\n>>>>> > descendants/ancestors of one another, or for parent and child to\n>>>>> share a\n>>>>> > parent,\n>>>>> > so we cannot make any other topology assumptions.\n>>>>>\n>>>>> I'm not clearly understanding the accepted topologies. By \"parent and\n>>>>> child to share a parent\", do you mean the set of transactions A, B, C,\n>>>>> where B is spending A and C is spending A and B would be correct ?\n>>>>>\n>>>>> If yes, is there a width-limit introduced or we fallback on\n>>>>> MAX_PACKAGE_COUNT=25 ?\n>>>>>\n>>>>> IIRC, one rationale to come with this topology limitation was to lower\n>>>>> the DoS risks when potentially deploying p2p packages.\n>>>>>\n>>>>> Considering the current Core's mempool acceptance rules, I think CPFP\n>>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>>>> jamming successful on one channel commitment transaction would contamine\n>>>>> the remaining commitments sharing the same package.\n>>>>>\n>>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>>>>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>>>>> better feerate than A, the whole package acceptance will fail. Even if A'\n>>>>> confirms in the following block,\n>>>>> the propagation and confirmation of B+C+D have been delayed. This\n>>>>> could carry on a loss of funds.\n>>>>>\n>>>>> That said, if you're broadcasting commitment transactions without\n>>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee\n>>>>> saving as you don't have to duplicate the CPFP.\n>>>>>\n>>>>> IMHO, I'm leaning towards deploying during a first phase\n>>>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>>>> second-layer safety.\n>>>>>\n>>>>> > *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>> that are\n>>>>> > already in the mempool, as we do not want a transaction's fees to be\n>>>>> > double-counted for both its individual RBF and package RBF.\n>>>>>\n>>>>> I'm unsure about the logical order of the checks proposed.\n>>>>>\n>>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200\n>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B\n>>>>> acceptance fails. For this reason I think the individual RBF should be\n>>>>> bypassed and only the package RBF apply ?\n>>>>>\n>>>>> Note this situation is plausible, with current LN design, your\n>>>>> counterparty can have a commitment transaction with a better fee just by\n>>>>> selecting a higher `dust_limit_satoshis` than yours.\n>>>>>\n>>>>> > Examples F and G [14] show the same package, but P1 is submitted\n>>>>> > individually before\n>>>>> > the package in example G. In example F, we can see that the 300vB\n>>>>> package\n>>>>> > pays\n>>>>> > an additional 200sat in fees, which is not enough to pay for its own\n>>>>> > bandwidth\n>>>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>> M1, but\n>>>>> > using P1's fees again during package submission would make it look\n>>>>> like a\n>>>>> > 300sat\n>>>>> > increase for a 200vB package. Even including its fees and size would\n>>>>> not be\n>>>>> > sufficient in this example, since the 300sat looks like enough for\n>>>>> the 300vB\n>>>>> > package. The calculcation after deduplication is 100sat increase for\n>>>>> a\n>>>>> > package\n>>>>> > of size 200vB, which correctly fails BIP125#4. Assume all\n>>>>> transactions have\n>>>>> > a\n>>>>> > size of 100vB.\n>>>>>\n>>>>> What problem are you trying to solve by the package feerate *after*\n>>>>> dedup rule ?\n>>>>>\n>>>>> My understanding is that an in-package transaction might be already in\n>>>>> the mempool. Therefore, to compute a correct RBF penalty replacement, the\n>>>>> vsize of this transaction could be discarded lowering the cost of package\n>>>>> RBF.\n>>>>>\n>>>>> If we keep a \"safe\" dedup mechanism (see my point above), I think this\n>>>>> discount is justified, as the validation cost of node operators is paid for\n>>>>> ?\n>>>>>\n>>>>> > The child cannot replace mempool transactions.\n>>>>>\n>>>>> Let's say you issue package A+B, then package C+B', where B' is a\n>>>>> child of both A and C. This rule fails the acceptance of C+B' ?\n>>>>>\n>>>>> I think this is a footgunish API, as if a package issuer send the\n>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe\n>>>>> in protocols where states are symmetric. E.g a malicious counterparty\n>>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better\n>>>>> fees.\n>>>>>\n>>>>> > All mempool transactions to be replaced must signal replaceability.\n>>>>>\n>>>>> I think this is unsafe for L2s if counterparties have malleability of\n>>>>> the child transaction. They can block your package replacement by\n>>>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>>>> ability.\n>>>>>\n>>>>> I think it's better to either fix inherited signaling or move towards\n>>>>> full-rbf.\n>>>>>\n>>>>> > if a package parent has already been submitted, it would\n>>>>> > look\n>>>>> >like the child is spending a \"new\" unconfirmed input.\n>>>>>\n>>>>> I think this is an issue brought by the trimming during the dedup\n>>>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>>>> checks results of already in-mempool transactions to gain in CPU time we\n>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>>>\n>>>>> > However, we still achieve the same goal of requiring the\n>>>>> > replacement\n>>>>> > transactions to have a ancestor score at least as high as the\n>>>>> original\n>>>>> > ones.\n>>>>>\n>>>>> I'm not sure if this holds...\n>>>>>\n>>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>>>\n>>>>> Package A + B ancestor score is 10 sat/vb.\n>>>>>\n>>>>> D has a higher feerate/absolute fee than B.\n>>>>>\n>>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>>>> 1000 sats + D's 1500 sats) /\n>>>>> A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>>>\n>>>>> Overall, this is a review through the lenses of LN requirements. I\n>>>>> think other L2 protocols/applications\n>>>>> could be candidates to using package accept/relay such as:\n>>>>> * https://github.com/lightninglabs/pool\n>>>>> * https://github.com/discreetlogcontracts/dlcspecs\n>>>>> * https://github.com/bitcoin-teleport/teleport-transactions/\n>>>>> * https://github.com/sapio-lang/sapio\n>>>>> *\n>>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n>>>>> * https://github.com/revault/practical-revault\n>>>>>\n>>>>> Thanks for rolling forward the ball on this subject.\n>>>>>\n>>>>> Antoine\n>>>>>\n>>>>> Le jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\n>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>\n>>>>>> Hi there,\n>>>>>>\n>>>>>> I'm writing to propose a set of mempool policy changes to enable\n>>>>>> package\n>>>>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>>>>> would not\n>>>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>>>> significantly affects transaction propagation, I believe this is\n>>>>>> relevant for\n>>>>>> the mailing list.\n>>>>>>\n>>>>>> My proposal enables packages consisting of multiple parents and 1\n>>>>>> child. If you\n>>>>>> develop software that relies on specific transaction relay\n>>>>>> assumptions and/or\n>>>>>> are interested in using package relay in the future, I'm very\n>>>>>> interested to hear\n>>>>>> your feedback on the utility or restrictiveness of these package\n>>>>>> policies for\n>>>>>> your use cases.\n>>>>>>\n>>>>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>>>>> PR#22290][1].\n>>>>>>\n>>>>>> An illustrated version of this post can be found at\n>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>>>> I have also linked the images below.\n>>>>>>\n>>>>>> ## Background\n>>>>>>\n>>>>>> Feel free to skip this section if you are already familiar with\n>>>>>> mempool policy\n>>>>>> and package relay terminology.\n>>>>>>\n>>>>>> ### Terminology Clarifications\n>>>>>>\n>>>>>> * Package = an ordered list of related transactions, representable by\n>>>>>> a Directed\n>>>>>>   Acyclic Graph.\n>>>>>> * Package Feerate = the total modified fees divided by the total\n>>>>>> virtual size of\n>>>>>>   all transactions in the package.\n>>>>>>     - Modified fees = a transaction's base fees + fee delta applied\n>>>>>> by the user\n>>>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>>>> across\n>>>>>> mempools.\n>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>>>> [BIP141\n>>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>>>>> Core][3].\n>>>>>>     - Note that feerate is not necessarily based on the base fees and\n>>>>>> serialized\n>>>>>>       size.\n>>>>>>\n>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>>>> incentives to\n>>>>>>   boost a transaction's candidacy for inclusion in a block, including\n>>>>>> Child Pays\n>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our\n>>>>>> intention in\n>>>>>> mempool policy is to recognize when the new transaction is more\n>>>>>> economical to\n>>>>>> mine than the original one(s) but not open DoS vectors, so there are\n>>>>>> some\n>>>>>> limitations.\n>>>>>>\n>>>>>> ### Policy\n>>>>>>\n>>>>>> The purpose of the mempool is to store the best (to be most\n>>>>>> incentive-compatible\n>>>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>>>> Miners use\n>>>>>> the mempool to build block templates. The mempool is also useful as a\n>>>>>> cache for\n>>>>>> boosting block relay and validation performance, aiding transaction\n>>>>>> relay, and\n>>>>>> generating feerate estimations.\n>>>>>>\n>>>>>> Ideally, all consensus-valid transactions paying reasonable fees\n>>>>>> should make it\n>>>>>> to miners through normal transaction relay, without any special\n>>>>>> connectivity or\n>>>>>> relationships with miners. On the other hand, nodes do not have\n>>>>>> unlimited\n>>>>>> resources, and a P2P network designed to let any honest node\n>>>>>> broadcast their\n>>>>>> transactions also exposes the transaction validation engine to DoS\n>>>>>> attacks from\n>>>>>> malicious peers.\n>>>>>>\n>>>>>> As such, for unconfirmed transactions we are considering for our\n>>>>>> mempool, we\n>>>>>> apply a set of validation rules in addition to consensus, primarily\n>>>>>> to protect\n>>>>>> us from resource exhaustion and aid our efforts to keep the highest\n>>>>>> fee\n>>>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>>>> node-specific) rules that transactions must abide by in order to be\n>>>>>> accepted\n>>>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>>>> restrictions such\n>>>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>>>\n>>>>>> ### Package Relay and Package Mempool Accept\n>>>>>>\n>>>>>> In transaction relay, we currently consider transactions one at a\n>>>>>> time for\n>>>>>> submission to the mempool. This creates a limitation in the node's\n>>>>>> ability to\n>>>>>> determine which transactions have the highest feerates, since we\n>>>>>> cannot take\n>>>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>>>> transactions are\n>>>>>> in the mempool. Similarly, we cannot use a transaction's descendants\n>>>>>> when\n>>>>>> considering it for RBF. When an individual transaction does not meet\n>>>>>> the mempool\n>>>>>> minimum feerate and the user isn't able to create a replacement\n>>>>>> transaction\n>>>>>> directly, it will not be accepted by mempools.\n>>>>>>\n>>>>>> This limitation presents a security issue for applications and users\n>>>>>> relying on\n>>>>>> time-sensitive transactions. For example, Lightning and other\n>>>>>> protocols create\n>>>>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>>>>> path opens\n>>>>>> up after a timelock, and users are protected from cheating scenarios\n>>>>>> as long as\n>>>>>> they redeem on-chain in time. A key security assumption is that all\n>>>>>> parties'\n>>>>>> transactions will propagate and confirm in a timely manner. This\n>>>>>> assumption can\n>>>>>> be broken if fee-bumping does not work as intended.\n>>>>>>\n>>>>>> The end goal for Package Relay is to consider multiple transactions\n>>>>>> at the same\n>>>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>>>> better\n>>>>>> determine whether transactions should be accepted to our mempool,\n>>>>>> especially if\n>>>>>> they don't meet fee requirements individually or are better RBF\n>>>>>> candidates as a\n>>>>>> package. A combination of changes to mempool validation logic,\n>>>>>> policy, and\n>>>>>> transaction relay allows us to better propagate the transactions with\n>>>>>> the\n>>>>>> highest package feerates to miners, and makes fee-bumping tools more\n>>>>>> powerful\n>>>>>> for users.\n>>>>>>\n>>>>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but\n>>>>>> a large\n>>>>>> part of the changes are in the mempool's package validation logic. We\n>>>>>> call this\n>>>>>> *Package Mempool Accept*.\n>>>>>>\n>>>>>> ### Previous Work\n>>>>>>\n>>>>>> * Given that mempool validation is DoS-sensitive and complex, it\n>>>>>> would be\n>>>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>>>> efforts have\n>>>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>>>> [#21062][5],\n>>>>>> [#22675][6], [#22796][7]).\n>>>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>>>> accepts only\n>>>>>>   (no submission to mempool).\n>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks\n>>>>>> for arbitrary\n>>>>>>   packages. Still test accepts only.\n>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>>>\n>>>>>> ### Existing Package Rules\n>>>>>>\n>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].\n>>>>>> I'll consider\n>>>>>> them as \"given\" in the rest of this document, though they can be\n>>>>>> changed, since\n>>>>>> package validation is test-accept only right now.\n>>>>>>\n>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>>>\n>>>>>>    *Rationale*: This is already enforced as mempool\n>>>>>> ancestor/descendant limits.\n>>>>>> Presumably, transactions in a package are all related, so exceeding\n>>>>>> this limit\n>>>>>> would mean that the package can either be split up or it wouldn't\n>>>>>> pass this\n>>>>>> mempool policy.\n>>>>>>\n>>>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>>>> between\n>>>>>> transactions, parents must appear somewhere before children. [8]\n>>>>>>\n>>>>>> 3. A package cannot have conflicting transactions, i.e. none of them\n>>>>>> can spend\n>>>>>> the same inputs. This also means there cannot be duplicate\n>>>>>> transactions. [8]\n>>>>>>\n>>>>>> 4. When packages are evaluated against ancestor/descendant limits in\n>>>>>> a test\n>>>>>> accept, the union of all of their descendants and ancestors is\n>>>>>> considered. This\n>>>>>> is essentially a \"worst case\" heuristic where every transaction in\n>>>>>> the package\n>>>>>> is treated as each other's ancestor and descendant. [8]\n>>>>>> Packages for which ancestor/descendant limits are accurately captured\n>>>>>> by this\n>>>>>> heuristic: [19]\n>>>>>>\n>>>>>> There are also limitations such as the fact that CPFP carve out is\n>>>>>> not applied\n>>>>>> to package transactions. #20833 also disables RBF in package\n>>>>>> validation; this\n>>>>>> proposal overrides that to allow packages to use RBF.\n>>>>>>\n>>>>>> ## Proposed Changes\n>>>>>>\n>>>>>> The next step in the Package Mempool Accept project is to implement\n>>>>>> submission\n>>>>>> to mempool, initially through RPC only. This allows us to test the\n>>>>>> submission\n>>>>>> logic before exposing it on P2P.\n>>>>>>\n>>>>>> ### Summary\n>>>>>>\n>>>>>> - Packages may contain already-in-mempool transactions.\n>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>>>> - Fee-related checks use the package feerate. This means that wallets\n>>>>>> can\n>>>>>> create a package that utilizes CPFP.\n>>>>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>>>>> similar\n>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>>>\n>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but\n>>>>>> feedback is\n>>>>>> always welcome.\n>>>>>>\n>>>>>> ### Details\n>>>>>>\n>>>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>>>\n>>>>>> A package may contain transactions that are already in the mempool.\n>>>>>> We remove\n>>>>>> (\"deduplicate\") those transactions from the package for the purposes\n>>>>>> of package\n>>>>>> mempool acceptance. If a package is empty after deduplication, we do\n>>>>>> nothing.\n>>>>>>\n>>>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>>>> parent to be\n>>>>>> accepted to the mempool of a peer on its own due to differences in\n>>>>>> policy and\n>>>>>> fee market fluctuations. We should not reject or penalize the entire\n>>>>>> package for\n>>>>>> an individual transaction as that could be a censorship vector.\n>>>>>>\n>>>>>> #### Packages Are Multi-Parent-1-Child\n>>>>>>\n>>>>>> Only packages of a specific topology are permitted. Namely, a package\n>>>>>> is exactly\n>>>>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>>>>> package\n>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>>>>> unconfirmed parents, etc. Note that it's possible for the parents to\n>>>>>> be indirect\n>>>>>> descendants/ancestors of one another, or for parent and child to\n>>>>>> share a parent,\n>>>>>> so we cannot make any other topology assumptions.\n>>>>>>\n>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>>>> parents\n>>>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>>>> packages to a\n>>>>>> defined topology is also easier to reason about and simplifies the\n>>>>>> validation\n>>>>>> logic greatly. Multi-parent-1-child allows us to think of the package\n>>>>>> as one big\n>>>>>> transaction, where:\n>>>>>>\n>>>>>> - Inputs = all the inputs of parents + inputs of the child that come\n>>>>>> from\n>>>>>>   confirmed UTXOs\n>>>>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>>>>> that\n>>>>>>   aren't spent by other transactions in the package\n>>>>>>\n>>>>>> Examples of packages that follow this rule (variations of example A\n>>>>>> show some\n>>>>>> possibilities after deduplication): ![image][15]\n>>>>>>\n>>>>>> #### Fee-Related Checks Use Package Feerate\n>>>>>>\n>>>>>> Package Feerate = the total modified fees divided by the total\n>>>>>> virtual size of\n>>>>>> all transactions in the package.\n>>>>>>\n>>>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>>>> pre-configured\n>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>>>> feerate, the\n>>>>>> total package feerate is used instead of the individual feerate. The\n>>>>>> individual\n>>>>>> transactions are allowed to be below feerate requirements if the\n>>>>>> package meets\n>>>>>> the feerate requirements. For example, the parent(s) in the package\n>>>>>> can have 0\n>>>>>> fees but be paid for by the child.\n>>>>>>\n>>>>>> *Rationale*: This can be thought of as \"CPFP within a package,\"\n>>>>>> solving the\n>>>>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>>>>> applications to adjust their fees at broadcast time instead of\n>>>>>> overshooting or\n>>>>>> risking getting stuck/pinned.\n>>>>>>\n>>>>>> We use the package feerate of the package *after deduplication*.\n>>>>>>\n>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>>> that are\n>>>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>>>> double-counted for both its individual RBF and package RBF.\n>>>>>>\n>>>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>>>> individually before\n>>>>>> the package in example G. In example F, we can see that the 300vB\n>>>>>> package pays\n>>>>>> an additional 200sat in fees, which is not enough to pay for its own\n>>>>>> bandwidth\n>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>>> M1, but\n>>>>>> using P1's fees again during package submission would make it look\n>>>>>> like a 300sat\n>>>>>> increase for a 200vB package. Even including its fees and size would\n>>>>>> not be\n>>>>>> sufficient in this example, since the 300sat looks like enough for\n>>>>>> the 300vB\n>>>>>> package. The calculcation after deduplication is 100sat increase for\n>>>>>> a package\n>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all\n>>>>>> transactions have a\n>>>>>> size of 100vB.\n>>>>>>\n>>>>>> #### Package RBF\n>>>>>>\n>>>>>> If a package meets feerate requirements as a package, the parents in\n>>>>>> the\n>>>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>>>> child cannot\n>>>>>> replace mempool transactions. Multiple transactions can replace the\n>>>>>> same\n>>>>>> transaction, but in order to be valid, none of the transactions can\n>>>>>> try to\n>>>>>> replace an ancestor of another transaction in the same package (which\n>>>>>> would thus\n>>>>>> make its inputs unavailable).\n>>>>>>\n>>>>>> *Rationale*: Even if we are using package feerate, a package will not\n>>>>>> propagate\n>>>>>> as intended if RBF still requires each individual transaction to meet\n>>>>>> the\n>>>>>> feerate requirements.\n>>>>>>\n>>>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>>>\n>>>>>> ##### Signaling (Rule #1)\n>>>>>>\n>>>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>>>\n>>>>>> *Rationale*: Package RBF signaling logic should be the same for\n>>>>>> package RBF and\n>>>>>> single transaction acceptance. This would be updated if single\n>>>>>> transaction\n>>>>>> validation moves to full RBF.\n>>>>>>\n>>>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>>>\n>>>>>> A package may include new unconfirmed inputs, but the ancestor\n>>>>>> feerate of the\n>>>>>> child must be at least as high as the ancestor feerates of every\n>>>>>> transaction\n>>>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>>>> replacement\n>>>>>> transaction may only include an unconfirmed input if that input was\n>>>>>> included in\n>>>>>> one of the original transactions. (An unconfirmed input spends an\n>>>>>> output from a\n>>>>>> currently-unconfirmed transaction.)\"\n>>>>>>\n>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>>>>> transaction has a higher ancestor score than the original\n>>>>>> transaction(s) (see\n>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed\n>>>>>> input can lower the\n>>>>>> ancestor score of the replacement transaction. P1 is trying to\n>>>>>> replace M1, and\n>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,\n>>>>>> and M2 pays\n>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>>>> isolation, P1\n>>>>>> looks like a better mining candidate than M1, it must be mined with\n>>>>>> M2, so its\n>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>>>> ancestor\n>>>>>> feerate, which is 6sat/vB.\n>>>>>>\n>>>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>>>> transactions in the package can spend new unconfirmed inputs.\"\n>>>>>> Example J [17] shows\n>>>>>> why, if any of the package transactions have ancestors, package\n>>>>>> feerate is no\n>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which\n>>>>>> is the\n>>>>>> replacement transaction in an RBF), we're actually interested in the\n>>>>>> entire\n>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,\n>>>>>> P1, P2, and\n>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>>>>> only allow\n>>>>>> the child to have new unconfirmed inputs, either, because it can\n>>>>>> still cause us\n>>>>>> to overestimate the package's ancestor score.\n>>>>>>\n>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>>>> Package RBF\n>>>>>> less useful, but would also break Package RBF for packages with\n>>>>>> parents already\n>>>>>> in the mempool: if a package parent has already been submitted, it\n>>>>>> would look\n>>>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>>>> [18], we're\n>>>>>> looking to replace M1 with the entire package including P1, P2, and\n>>>>>> P3. We must\n>>>>>> consider the case where one of the parents is already in the mempool\n>>>>>> (in this\n>>>>>> case, P2), which means we must allow P3 to have new unconfirmed\n>>>>>> inputs. However,\n>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>>>> replace M1\n>>>>>> with this package.\n>>>>>>\n>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>>>> strict than\n>>>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>>>> replacement\n>>>>>> transactions to have a ancestor score at least as high as the\n>>>>>> original ones. As\n>>>>>> a result, the entire package is required to be a higher feerate\n>>>>>> mining candidate\n>>>>>> than each of the replaced transactions.\n>>>>>>\n>>>>>> Another note: the [comment][13] above the BIP125#2 code in the\n>>>>>> original RBF\n>>>>>> implementation suggests that the rule was intended to be temporary.\n>>>>>>\n>>>>>> ##### Absolute Fee (Rule #3)\n>>>>>>\n>>>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>>>> total fees\n>>>>>> of the package must be higher than the absolute fees of the mempool\n>>>>>> transactions\n>>>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>>>> BIP125 Rule #3\n>>>>>> - an individual transaction in the package may have lower fees than\n>>>>>> the\n>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and\n>>>>>> the child\n>>>>>> pays for RBF.\n>>>>>>\n>>>>>> ##### Feerate (Rule #4)\n>>>>>>\n>>>>>> The package must pay for its own bandwidth; the package feerate must\n>>>>>> be higher\n>>>>>> than the replaced transactions by at least minimum relay feerate\n>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>>>> differs from\n>>>>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>>>>> lower\n>>>>>> feerate than the transaction(s) it is replacing. In fact, it may have\n>>>>>> 0 fees,\n>>>>>> and the child pays for RBF.\n>>>>>>\n>>>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>>>\n>>>>>> The package cannot replace more than 100 mempool transactions. This\n>>>>>> is identical\n>>>>>> to BIP125 Rule #5.\n>>>>>>\n>>>>>> ### Expected FAQs\n>>>>>>\n>>>>>> 1. Is it possible for only some of the package to make it into the\n>>>>>> mempool?\n>>>>>>\n>>>>>>    Yes, it is. However, since we evict transactions from the mempool\n>>>>>> by\n>>>>>> descendant score and the package child is supposed to be sponsoring\n>>>>>> the fees of\n>>>>>> its parents, the most common scenario would be all-or-nothing. This is\n>>>>>> incentive-compatible. In fact, to be conservative, package validation\n>>>>>> should\n>>>>>> begin by trying to submit all of the transactions individually, and\n>>>>>> only use the\n>>>>>> package mempool acceptance logic if the parents fail due to low\n>>>>>> feerate.\n>>>>>>\n>>>>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>>>>\n>>>>>>     No, for practical reasons. In mempool validation, we actually\n>>>>>> aren't able to\n>>>>>> tell with 100% confidence if we are looking at a transaction that has\n>>>>>> already\n>>>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>>>> historical\n>>>>>> block data, it's possible to look for it, but this is inefficient,\n>>>>>> not always\n>>>>>> possible for pruning nodes, and unnecessary because we're not going\n>>>>>> to do\n>>>>>> anything with the transaction anyway. As such, we already have the\n>>>>>> expectation\n>>>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>>>> relaying\n>>>>>> transactions that have already been confirmed. Similarly, we\n>>>>>> shouldn't be\n>>>>>> relaying packages that contain already-confirmed transactions.\n>>>>>>\n>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>>>> [2]:\n>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>>>> [3]:\n>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>>>> [13]:\n>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>>>> [14]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>>>> [15]:\n>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>>>> [16]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>>>> [17]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>>>> [18]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>>>> [19]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>>>> [20]:\n>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>\n>>>>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210927/8b0465fb/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-09-28T22:59:11",
                "message_text_only": "Hi Bastien\n\n> In the case of LN, an attacker can game this and heavily restrict\nyour RBF attempts if you're only allowed to use confirmed inputs\nand have many channels (and a limited number of confirmed inputs).\nOtherwise you'll need node operators to pre-emptively split their\nutxos into many small utxos just for fee bumping, which is inefficient...\n\nI share the concern about splitting utxos into smaller ones.\nIIRC, the carve-out tolerance is only 2txn/10_000 vb. If one of your\ncounterparties attach a junk branch on her own anchor output, are you\nallowed to chain your self-owned unconfirmed CPFP ?\nI'm thinking about the topology \"Chained CPFPs\" exposed here :\nhttps://github.com/rust-bitcoin/rust-lightning/issues/989.\nOr if you have another L2 broadcast topology which could be safe w.r.t our\ncurrent mempool logic :) ?\n\n\nLe lun. 27 sept. 2021 \u00e0 03:15, Bastien TEINTURIER <bastien at acinq.fr> a\n\u00e9crit :\n\n> I think we could restrain package acceptance to only confirmed inputs for\n>> now and revisit later this point ? For LN-anchor, you can assume that the\n>> fee-bumping UTXO feeding the CPFP is already\n>> confirmed. Or are there currently-deployed use-cases which would benefit\n>> from your proposed Rule #2 ?\n>>\n>\n> I think constraining package acceptance to only confirmed inputs\n> is very limiting and quite dangerous for L2 protocols.\n>\n> In the case of LN, an attacker can game this and heavily restrict\n> your RBF attempts if you're only allowed to use confirmed inputs\n> and have many channels (and a limited number of confirmed inputs).\n> Otherwise you'll need node operators to pre-emptively split their\n> utxos into many small utxos just for fee bumping, which is inefficient...\n>\n> Bastien\n>\n> Le lun. 27 sept. 2021 \u00e0 00:27, Antoine Riard via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi Gloria,\n>>\n>> Thanks for your answers,\n>>\n>> > In summary, it seems that the decisions that might still need\n>> > attention/input from devs on this mailing list are:\n>> > 1. Whether we should start with multiple-parent-1-child or\n>> 1-parent-1-child.\n>> > 2. Whether it's ok to require that the child not have conflicts with\n>> > mempool transactions.\n>>\n>> Yes 1) it would be good to have inputs of more potential users of package\n>> acceptance . And 2) I think it's more a matter of clearer wording of the\n>> proposal.\n>>\n>> However, see my final point on the relaxation around \"unconfirmed inputs\"\n>> which might in fact alter our current block construction strategy.\n>>\n>> > Right, the fact that we essentially always choose the first-seen\n>> witness is\n>> > an unfortunate limitation that exists already. Adding package mempool\n>> > accept doesn't worsen this, but the procedure in the future is to\n>> replace\n>> > the witness when it makes sense economically. We can also add logic to\n>> > allow package feerate to pay for witness replacements as well. This is\n>> > pretty far into the future, though.\n>>\n>> Yes I agree package mempool doesn't worsen this. And it's not an issue\n>> for current LN as you can't significantly inflate a spending witness for\n>> the 2-of-2 funding output.\n>> However, it might be an issue for multi-party protocol where the spending\n>> script has alternative branches with asymmetric valid witness weights.\n>> Taproot should ease that kind of script so hopefully we would deploy\n>> wtxid-replacement not too far in the future.\n>>\n>> > I could be misunderstanding, but an attacker wouldn't be able to\n>> > batch-attack like this. Alice's package only conflicts with A' + D',\n>> not A'\n>> > + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n>>\n>> Yeah I can be clearer, I think you have 2 pinning attacks scenarios to\n>> consider.\n>>\n>> In LN, if you're trying to confirm a commitment transaction to time-out\n>> or claim on-chain a HTLC and the timelock is near-expiration, you should be\n>> ready to pay in commitment+2nd-stage HTLC transaction fees as much as the\n>> value offered by the HTLC.\n>>\n>> Following this security assumption, an attacker can exploit it by\n>> targeting together commitment transactions from different channels by\n>> blocking them under a high-fee child, of which the fee value\n>> is equal to the top-value HTLC + 1. Victims's fee-bumping logics won't\n>> overbid as it's not worthy to offer fees beyond their competed HTLCs. Apart\n>> from observing mempools state, victims can't learn they're targeted by the\n>> same attacker.\n>>\n>> To draw from the aforementioned topology, Mallory broadcasts A' + B' + C'\n>> + D', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'\n>> conflicts with Caroll's P3. Let's assume P1 is confirming the top-value\n>> HTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for\n>> Alice or Bob or Caroll to keep offering competing feerates. Mallory will be\n>> at loss on stealing P1, as she has paid more in fees but will realize a\n>> gain on P2+P3.\n>>\n>> In this model, Alice is allowed to evict those 2 transactions (A' + D')\n>> but as she is economically-bounded she won't succeed.\n>>\n>> Mallory is maliciously exploiting RBF rule 3 on absolute fee. I think\n>> this 1st pinning scenario is correct and \"lucractive\" when you sum the\n>> global gain/loss.\n>>\n>> There is a 2nd attack scenario where A + B + C + D, where D is the child\n>> of A,B,C. All those transactions are honestly issued by Alice. Once A + B +\n>> C + D are propagated in network mempools, Mallory is able to replace A + D\n>> with  A' + D' where D' is paying a higher fee. This package A' + D' will\n>> confirm soon if D feerate was compelling but Mallory succeeds in delaying\n>> the confirmation\n>> of B + C for one or more blocks. As B + C are pre-signed commitments with\n>> a low-fee rate they won't confirm without Alice issuing a new child E.\n>> Mallory can repeat the same trick by broadcasting\n>> B' + E' and delay again the confirmation of C.\n>>\n>> If the remaining package pending HTLC has a higher-value than all the\n>> malicious fees over-bid, Mallory should realize a gain. With this 2nd\n>> pinning attack, the malicious entity buys confirmation delay of your\n>> packaged-together commitments.\n>>\n>> Assuming those attacks are correct, I'm leaning towards being\n>> conservative with the LDK broadcast backend. Though once again, other L2\n>> devs have likely other use-cases and opinions :)\n>>\n>> >  B' only needs to pay for itself in this case.\n>>\n>> Yes I think it's a nice discount when UTXO is single-owned. In the\n>> context of shared-owned UTXO (e.g LN), you might not if there is an\n>> in-mempool package already spending the UTXO and have to assume the\n>> worst-case scenario. I.e have B' committing enough fee to pay for A'\n>> replacement bandwidth. I think we can't do that much for this case...\n>>\n>> > If a package meets feerate requirements as a\n>> package, the parents in the transaction are allowed to replace-by-fee\n>> mempool transactions. The child cannot replace mempool transactions.\"\n>>\n>> I agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B'\n>> to replace A+B because the first broadcast isn't satisfying anymore due to\n>> mempool spikes ? Assuming B' fees is enough, I think that case as child B'\n>> replacing in-mempool transaction B. Which I understand going against  \"The\n>> child cannot replace mempool transactions\".\n>>\n>> Maybe wording could be a bit clearer ?\n>>\n>> > While it would be nice to have full RBF, malleability of the child won't\n>> > block RBF here. If we're trying to replace A', we only require that A'\n>> > signals replaceability, and don't mind if its child doesn't.\n>>\n>> Yes, it sounds good.\n>>\n>> > Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\n>> miner\n>> > should prefer to utilize their block space more effectively.\n>>\n>> If your mempool is empty and only composed of A+C+D or A+B, I think\n>> taking A+C+D is the most efficient block construction you can come up with\n>> as a miner ?\n>>\n>> > No, because we don't use that model.\n>>\n>> Can you describe what miner model we are using ? Like the block\n>> construction strategy implemented by `addPackagesTxs` or also encompassing\n>> our current mempool acceptance policy, which I think rely on absolute fee\n>> over ancestor score in case of replacement ?\n>>\n>> I think this point is worthy to discuss as otherwise we might downgrade\n>> the efficiency of our current block construction strategy in periods of\n>> near-empty mempools. A knowledge which could be discreetly leveraged by a\n>> miner to gain an advantage on the rest of the mining ecosystem.\n>>\n>> Note, I think we *might* have to go in this direction if we want to\n>> replace replace-by-fee by replace-by-feerate or replace-by-ancestor and\n>> solve in-depth pinning attacks. Though if we do so,\n>> IMO we would need more thoughts.\n>>\n>> I think we could restrain package acceptance to only confirmed inputs for\n>> now and revisit later this point ? For LN-anchor, you can assume that the\n>> fee-bumping UTXO feeding the CPFP is already\n>> confirmed. Or are there currently-deployed use-cases which would benefit\n>> from your proposed Rule #2 ?\n>>\n>> Antoine\n>>\n>> Le jeu. 23 sept. 2021 \u00e0 11:36, Gloria Zhao <gloriajzhao at gmail.com> a\n>> \u00e9crit :\n>>\n>>> Hi Antoine,\n>>>\n>>> Thanks as always for your input. I'm glad we agree on so much!\n>>>\n>>> In summary, it seems that the decisions that might still need\n>>> attention/input from devs on this mailing list are:\n>>> 1. Whether we should start with multiple-parent-1-child or\n>>> 1-parent-1-child.\n>>> 2. Whether it's ok to require that the child not have conflicts with\n>>> mempool transactions.\n>>>\n>>> Responding to your comments...\n>>>\n>>> > IIUC, you have package A+B, during the dedup phase early in\n>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n>>> and A' is higher feerate than A, you trim A and replace by A' ?\n>>>\n>>> > I think this approach is safe, the one who appears unsafe to me is\n>>> when A' has a _lower_ feerate, even if A' is already accepted by our\n>>> mempool ? In that case iirc that would be a pinning.\n>>>\n>>> Right, the fact that we essentially always choose the first-seen witness\n>>> is an unfortunate limitation that exists already. Adding package mempool\n>>> accept doesn't worsen this, but the procedure in the future is to replace\n>>> the witness when it makes sense economically. We can also add logic to\n>>> allow package feerate to pay for witness replacements as well. This is\n>>> pretty far into the future, though.\n>>>\n>>> > It sounds uneconomical for an attacker but I think it's not when you\n>>> consider than you can \"batch\" attack against multiple honest\n>>> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts\n>>> with Alice's honest package P1, B' conflicts with Bob's honest package P2,\n>>> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of\n>>> A' + B' + C'.\n>>>\n>>> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of\n>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>>>\n>>> I could be misunderstanding, but an attacker wouldn't be able to\n>>> batch-attack like this. Alice's package only conflicts with A' + D', not A'\n>>> + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n>>>\n>>> > Do we assume that broadcasted packages are \"honest\" by default and\n>>> that the parent(s) always need the child to pass the fee checks, that way\n>>> saving the processing of individual transactions which are expected to fail\n>>> in 99% of cases or more ad hoc composition of packages at relay ?\n>>> > I think this point is quite dependent on the p2p packages format/logic\n>>> we'll end up on and that we should feel free to revisit it later ?\n>>>\n>>> I think it's the opposite; there's no way for us to assume that p2p\n>>> packages will be \"honest.\" I'd like to have two things before we expose on\n>>> P2P: (1) ensure that the amount of resources potentially allocated for\n>>> package validation isn't disproportionately higher than that of single\n>>> transaction validation and (2) only use package validation when we're\n>>> unsatisifed with the single validation result, e.g. we might get better\n>>> fees.\n>>> Yes, let's revisit this later :)\n>>>\n>>>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n>>> discard its feerate as B should pay for all fees checked on its own. Where\n>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n>>> have a fee high enough to cover the bandwidth penalty replacement\n>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>>>\n>>>  B' only needs to pay for itself in this case.\n>>>\n>>> > > Do we want the child to be able to replace mempool transactions as\n>>> well?\n>>>\n>>> > If we mean when you have replaceable A+B then A'+B' try to replace\n>>> with a higher-feerate ? I think that's exactly the case we need for\n>>> Lightning as A+B is coming from Alice and A'+B' is coming from Bob :/\n>>>\n>>> Let me clarify this because I can see that my wording was ambiguous, and\n>>> then please let me know if it fits Lightning's needs?\n>>>\n>>> In my proposal, I wrote \"If a package meets feerate requirements as a\n>>> package, the parents in the transaction are allowed to replace-by-fee\n>>> mempool transactions. The child cannot replace mempool transactions.\" What\n>>> I meant was: the package can replace mempool transactions if any of the\n>>> parents conflict with mempool transactions. The child cannot not conflict\n>>> with any mempool transactions.\n>>> The Lightning use case this attempts to address is: Alice and Mallory\n>>> are LN counterparties, and have packages A+B and A'+B', respectively. A and\n>>> A' are their commitment transactions and conflict with each other; they\n>>> have shared inputs and different txids.\n>>> B spends Alice's anchor output from A. B' spends Mallory's anchor output\n>>> from A'. Thus, B and B' do not conflict with each other.\n>>> Alice can broadcast her package, A+B, to replace Mallory's package,\n>>> A'+B', since B doesn't conflict with the mempool.\n>>>\n>>> Would this be ok?\n>>>\n>>> > The second option, a child of A', In the LN case I think the CPFP is\n>>> attached on one's anchor output.\n>>>\n>>> While it would be nice to have full RBF, malleability of the child won't\n>>> block RBF here. If we're trying to replace A', we only require that A'\n>>> signals replaceability, and don't mind if its child doesn't.\n>>>\n>>> > > B has an ancestor score of 10sat/vb and D has an\n>>> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\n>>> B's,\n>>> > > it fails the proposed package RBF Rule #2, so this package would be\n>>> > > rejected. Does this meet your expectations?\n>>>\n>>> > Well what sounds odd to me, in my example, we fail D even if it has a\n>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n>>> fees are 4500 sats ?\n>>>\n>>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\n>>> miner should prefer to utilize their block space more effectively.\n>>>\n>>> > Is this compatible with a model where a miner prioritizes absolute\n>>> fees over ancestor score, in the case that mempools aren't full-enough to\n>>> fulfill a block ?\n>>>\n>>> No, because we don't use that model.\n>>>\n>>> Thanks,\n>>> Gloria\n>>>\n>>> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>\n>>> wrote:\n>>>\n>>>> > Correct, if B+C is too low feerate to be accepted, we will reject it.\n>>>> I\n>>>> > prefer this because it is incentive compatible: A can be mined by\n>>>> itself,\n>>>> > so there's no reason to prefer A+B+C instead of A.\n>>>> > As another way of looking at this, consider the case where we do\n>>>> accept\n>>>> > A+B+C and it sits at the \"bottom\" of our mempool. If our mempool\n>>>> reaches\n>>>> > capacity, we evict the lowest descendant feerate transactions, which\n>>>> are\n>>>> > B+C in this case. This gives us the same resulting mempool, with A\n>>>> and not\n>>>> > B+C.\n>>>>\n>>>> I agree here. Doing otherwise, we might evict other transactions\n>>>> mempool in `MempoolAccept::Finalize` with a higher-feerate than B+C while\n>>>> those evicted transactions are the most compelling for block construction.\n>>>>\n>>>> I thought at first missing this acceptance requirement would break a\n>>>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is\n>>>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the\n>>>> child fee is capturing the parent value. I can't think of other fee-bumping\n>>>> schemes potentially affected. If they do exist I would say they're wrong in\n>>>> their design assumptions.\n>>>>\n>>>> > If or when we have witness replacement, the logic is: if the\n>>>> individual\n>>>> > transaction is enough to replace the mempool one, the replacement will\n>>>> > happen during the preceding individual transaction acceptance, and\n>>>> > deduplication logic will work. Otherwise, we will try to deduplicate\n>>>> by\n>>>> > wtxid, see that we need a package witness replacement, and use the\n>>>> package\n>>>> > feerate to evaluate whether this is economically rational.\n>>>>\n>>>> IIUC, you have package A+B, during the dedup phase early in\n>>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n>>>> and A' is higher feerate than A, you trim A and replace by A' ?\n>>>>\n>>>> I think this approach is safe, the one who appears unsafe to me is when\n>>>> A' has a _lower_ feerate, even if A' is already accepted by our mempool ?\n>>>> In that case iirc that would be a pinning.\n>>>>\n>>>> Good to see progress on witness replacement before we see usage of\n>>>> Taproot tree in the context of multi-party, where a malicious counterparty\n>>>> inflates its witness to jam a honest spending.\n>>>>\n>>>> (Note, the commit linked currently points nowhere :))\n>>>>\n>>>>\n>>>> > Please note that A may replace A' even if A' has higher fees than A\n>>>> > individually, because the proposed package RBF utilizes the fees and\n>>>> size\n>>>> > of the entire package. This just requires E to pay enough fees,\n>>>> although\n>>>> > this can be pretty high if there are also potential B' and C'\n>>>> competing\n>>>> > commitment transactions that we don't know about.\n>>>>\n>>>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for\n>>>> the individual check on A, the honest package should replace the pinning\n>>>> attempt. I've not fully parsed the proposed implementation yet.\n>>>>\n>>>> Though note, I think it's still unsafe for a Lightning\n>>>> multi-commitment-broadcast-as-one-package as a malicious A' might have an\n>>>> absolute fee higher than E. It sounds uneconomical for\n>>>> an attacker but I think it's not when you consider than you can \"batch\"\n>>>> attack against multiple honest counterparties. E.g, Mallory broadcast A' +\n>>>> B' + C' + D' where A' conflicts with Alice's honest package P1, B'\n>>>> conflicts with Bob's honest package P2, C' conflicts with Caroll's honest\n>>>> package P3. And D' is a high-fee child of A' + B' + C'.\n>>>>\n>>>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of\n>>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>>>>\n>>>> > So far, my understanding is that multi-parent-1-child is desired for\n>>>> > batched fee-bumping (\n>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>>>> and\n>>>> > I've also seen your response which I have less context on (\n>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>>>> That\n>>>> > being said, I am happy to create a new proposal for 1 parent + 1 child\n>>>> > (which would be slightly simpler) and plan for moving to\n>>>> > multi-parent-1-child later if that is preferred. I am very interested\n>>>> in\n>>>> > hearing feedback on that approach.\n>>>>\n>>>> I think batched fee-bumping is okay as long as you don't have\n>>>> time-sensitive outputs encumbering your commitment transactions. For the\n>>>> reasons mentioned above, I think that's unsafe.\n>>>>\n>>>> What I'm worried about is  L2 developers, potentially not aware about\n>>>> all the mempool subtleties blurring the difference and always batching\n>>>> their broadcast by default.\n>>>>\n>>>> IMO, a good thing by restraining to 1-parent + 1 child,  we\n>>>> artificially constraint L2 design space for now and minimize risks of\n>>>> unsafe usage of the package API :)\n>>>>\n>>>> I think that's a point where it would be relevant to have the opinion\n>>>> of more L2 devs.\n>>>>\n>>>> > I think there is a misunderstanding here - let me describe what I'm\n>>>> > proposing we'd do in this situation: we'll try individual submission\n>>>> for A,\n>>>> > see that it fails due to \"insufficient fees.\" Then, we'll try package\n>>>> > validation for A+B and use package RBF. If A+B pays enough, it can\n>>>> still\n>>>> > replace A'. If A fails for a bad signature, we won't look at B or\n>>>> A+B. Does\n>>>> > this meet your expectations?\n>>>>\n>>>> Yes there was a misunderstanding, I think this approach is correct,\n>>>> it's more a question of performance. Do we assume that broadcasted packages\n>>>> are \"honest\" by default and that the parent(s) always need the child to\n>>>> pass the fee checks, that way saving the processing of individual\n>>>> transactions which are expected to fail in 99% of cases or more ad hoc\n>>>> composition of packages at relay ?\n>>>>\n>>>> I think this point is quite dependent on the p2p packages format/logic\n>>>> we'll end up on and that we should feel free to revisit it later ?\n>>>>\n>>>>\n>>>> > What problem are you trying to solve by the package feerate *after*\n>>>> dedup\n>>>> rule ?\n>>>> > My understanding is that an in-package transaction might be already in\n>>>> the mempool. Therefore, to compute a correct RBF penalty replacement,\n>>>> the\n>>>> vsize of this transaction could be discarded lowering the cost of\n>>>> package\n>>>> RBF.\n>>>>\n>>>> > I'm proposing that, when a transaction has already been submitted to\n>>>> > mempool, we would ignore both its fees and vsize when calculating\n>>>> package\n>>>> > feerate.\n>>>>\n>>>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n>>>> discard its feerate as B should pay for all fees checked on its own. Where\n>>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n>>>> have a fee high enough to cover the bandwidth penalty replacement\n>>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>>>>\n>>>> If you have a second-layer like current Lightning, you might have a\n>>>> counterparty commitment to replace and should always expect to have to pay\n>>>> for parent replacement bandwidth.\n>>>>\n>>>> Where a potential discount sounds interesting is when you have an\n>>>> univoque state on the first-stage of transactions. E.g DLC's funding\n>>>> transaction which might be CPFP by any participant iirc.\n>>>>\n>>>> > Note that, if C' conflicts with C, it also conflicts with D, since D\n>>>> is a\n>>>> > descendant of C and would thus need to be evicted along with it.\n>>>>\n>>>> Ah once again I think it's a misunderstanding without the code under my\n>>>> eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e mark\n>>>> for potential eviction D and don't consider it for future conflicts in the\n>>>> rest of the package, I think D' `PreChecks` should be good ?\n>>>>\n>>>> > More generally, this example is surprising to me because I didn't\n>>>> think\n>>>> > packages would be used to fee-bump replaceable transactions. Do we\n>>>> want the\n>>>> > child to be able to replace mempool transactions as well?\n>>>>\n>>>> If we mean when you have replaceable A+B then A'+B' try to replace with\n>>>> a higher-feerate ? I think that's exactly the case we need for Lightning as\n>>>> A+B is coming from Alice and A'+B' is coming from Bob :/\n>>>>\n>>>> > I'm not sure what you mean? Let's say we have a package of parent A +\n>>>> child\n>>>> > B, where A is supposed to replace a mempool transaction A'. Are you\n>>>> saying\n>>>> > that counterparties are able to malleate the package child B, or a\n>>>> child of\n>>>> > A'?\n>>>>\n>>>> The second option, a child of A', In the LN case I think the CPFP is\n>>>> attached on one's anchor output.\n>>>>\n>>>> I think it's good if we assume the\n>>>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing\n>>>> inherited signaling or full-rbf ?\n>>>>\n>>>> > Sorry, I don't understand what you mean by \"preserve the package\n>>>> > integrity?\" Could you elaborate?\n>>>>\n>>>> After thinking the relaxation about the \"new\" unconfirmed input is not\n>>>> linked to trimming but I would say more to the multi-parent support.\n>>>>\n>>>> Let's say you have A+B trying to replace C+D where B is also spending\n>>>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed\n>>>> input as D isn't spending E.\n>>>>\n>>>> So good, I think we agree on the problem description here.\n>>>>\n>>>> > I am in agreement with your calculations but unsure if we disagree on\n>>>> the\n>>>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has\n>>>> an\n>>>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\n>>>> B's,\n>>>> > it fails the proposed package RBF Rule #2, so this package would be\n>>>> > rejected. Does this meet your expectations?\n>>>>\n>>>> Well what sounds odd to me, in my example, we fail D even if it has a\n>>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n>>>> fees are 4500 sats ?\n>>>>\n>>>> Is this compatible with a model where a miner prioritizes absolute fees\n>>>> over ancestor score, in the case that mempools aren't full-enough to\n>>>> fulfill a block ?\n>>>>\n>>>> Let me know if I can clarify a point.\n>>>>\n>>>> Antoine\n>>>>\n>>>> Le lun. 20 sept. 2021 \u00e0 11:10, Gloria Zhao <gloriajzhao at gmail.com> a\n>>>> \u00e9crit :\n>>>>\n>>>>>\n>>>>> Hi Antoine,\n>>>>>\n>>>>> First of all, thank you for the thorough review. I appreciate your\n>>>>> insight on LN requirements.\n>>>>>\n>>>>> > IIUC, you have a package A+B+C submitted for acceptance and A is\n>>>>> already in your mempool. You trim out A from the package and then evaluate\n>>>>> B+C.\n>>>>>\n>>>>> > I think this might be an issue if A is the higher-fee element of the\n>>>>> ABC package. B+C package fees might be under the mempool min fee and will\n>>>>> be rejected, potentially breaking the acceptance expectations of the\n>>>>> package issuer ?\n>>>>>\n>>>>> Correct, if B+C is too low feerate to be accepted, we will reject it.\n>>>>> I prefer this because it is incentive compatible: A can be mined by itself,\n>>>>> so there's no reason to prefer A+B+C instead of A.\n>>>>> As another way of looking at this, consider the case where we do\n>>>>> accept A+B+C and it sits at the \"bottom\" of our mempool. If our mempool\n>>>>> reaches capacity, we evict the lowest descendant feerate transactions,\n>>>>> which are B+C in this case. This gives us the same resulting mempool, with\n>>>>> A and not B+C.\n>>>>>\n>>>>>\n>>>>> > Further, I think the dedup should be done on wtxid, as you might\n>>>>> have multiple valid witnesses. Though with varying vsizes and as such\n>>>>> offering different feerates.\n>>>>>\n>>>>> I agree that variations of the same package with different witnesses\n>>>>> is a case that must be handled. I consider witness replacement to be a\n>>>>> project that can be done in parallel to package mempool acceptance because\n>>>>> being able to accept packages does not worsen the problem of a\n>>>>> same-txid-different-witness \"pinning\" attack.\n>>>>>\n>>>>> If or when we have witness replacement, the logic is: if the\n>>>>> individual transaction is enough to replace the mempool one, the\n>>>>> replacement will happen during the preceding individual transaction\n>>>>> acceptance, and deduplication logic will work. Otherwise, we will try to\n>>>>> deduplicate by wtxid, see that we need a package witness replacement, and\n>>>>> use the package feerate to evaluate whether this is economically rational.\n>>>>>\n>>>>> See the #22290 \"handle package transactions already in mempool\" commit\n>>>>> (\n>>>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),\n>>>>> which handles the case of same-txid-different-witness by simply using the\n>>>>> transaction in the mempool for now, with TODOs for what I just described.\n>>>>>\n>>>>>\n>>>>> > I'm not clearly understanding the accepted topologies. By \"parent\n>>>>> and child to share a parent\", do you mean the set of transactions A, B, C,\n>>>>> where B is spending A and C is spending A and B would be correct ?\n>>>>>\n>>>>> Yes, that is what I meant. Yes, that would a valid package under these\n>>>>> rules.\n>>>>>\n>>>>> > If yes, is there a width-limit introduced or we fallback on\n>>>>> MAX_PACKAGE_COUNT=25 ?\n>>>>>\n>>>>> No, there is no limit on connectivity other than \"child with all\n>>>>> unconfirmed parents.\" We will enforce MAX_PACKAGE_COUNT=25 and child's\n>>>>> in-mempool + in-package ancestor limits.\n>>>>>\n>>>>>\n>>>>> > Considering the current Core's mempool acceptance rules, I think\n>>>>> CPFP batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>>>> jamming successful on one channel commitment transaction would contamine\n>>>>> the remaining commitments sharing the same package.\n>>>>>\n>>>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are\n>>>>> commitment transactions and E a shared CPFP. If a malicious A' transaction\n>>>>> has a better feerate than A, the whole package acceptance will fail. Even\n>>>>> if A' confirms in the following block,\n>>>>> the propagation and confirmation of B+C+D have been delayed. This\n>>>>> could carry on a loss of funds.\n>>>>>\n>>>>> Please note that A may replace A' even if A' has higher fees than A\n>>>>> individually, because the proposed package RBF utilizes the fees and size\n>>>>> of the entire package. This just requires E to pay enough fees, although\n>>>>> this can be pretty high if there are also potential B' and C' competing\n>>>>> commitment transactions that we don't know about.\n>>>>>\n>>>>>\n>>>>> > IMHO, I'm leaning towards deploying during a first phase\n>>>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>>>> second-layer safety.\n>>>>>\n>>>>> So far, my understanding is that multi-parent-1-child is desired for\n>>>>> batched fee-bumping (\n>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>>>>> and I've also seen your response which I have less context on (\n>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>>>>> That being said, I am happy to create a new proposal for 1 parent + 1 child\n>>>>> (which would be slightly simpler) and plan for moving to\n>>>>> multi-parent-1-child later if that is preferred. I am very interested in\n>>>>> hearing feedback on that approach.\n>>>>>\n>>>>>\n>>>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200\n>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B\n>>>>> acceptance fails. For this reason I think the individual RBF should be\n>>>>> bypassed and only the package RBF apply ?\n>>>>>\n>>>>> I think there is a misunderstanding here - let me describe what I'm\n>>>>> proposing we'd do in this situation: we'll try individual submission for A,\n>>>>> see that it fails due to \"insufficient fees.\" Then, we'll try package\n>>>>> validation for A+B and use package RBF. If A+B pays enough, it can still\n>>>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does\n>>>>> this meet your expectations?\n>>>>>\n>>>>>\n>>>>> > What problem are you trying to solve by the package feerate *after*\n>>>>> dedup rule ?\n>>>>> > My understanding is that an in-package transaction might be already\n>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,\n>>>>> the vsize of this transaction could be discarded lowering the cost of\n>>>>> package RBF.\n>>>>>\n>>>>> I'm proposing that, when a transaction has already been submitted to\n>>>>> mempool, we would ignore both its fees and vsize when calculating package\n>>>>> feerate. In example G2, we shouldn't count M1 fees after its submission to\n>>>>> mempool, since M1's fees have already been used to pay for its individual\n>>>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.\n>>>>> We also shouldn't count its vsize, since it has already been paid for.\n>>>>>\n>>>>>\n>>>>> > I think this is a footgunish API, as if a package issuer send the\n>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>>>> higher-feerate/higher-fee package always replaces ?\n>>>>>\n>>>>> Note that, if C' conflicts with C, it also conflicts with D, since D\n>>>>> is a descendant of C and would thus need to be evicted along with it.\n>>>>> Implicitly, D' would not be in conflict with D.\n>>>>> More generally, this example is surprising to me because I didn't\n>>>>> think packages would be used to fee-bump replaceable transactions. Do we\n>>>>> want the child to be able to replace mempool transactions as well? This can\n>>>>> be implemented with a bit of additional logic.\n>>>>>\n>>>>> > I think this is unsafe for L2s if counterparties have malleability\n>>>>> of the child transaction. They can block your package replacement by\n>>>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>>>> ability.\n>>>>>\n>>>>> I'm not sure what you mean? Let's say we have a package of parent A +\n>>>>> child B, where A is supposed to replace a mempool transaction A'. Are you\n>>>>> saying that counterparties are able to malleate the package child B, or a\n>>>>> child of A'? If they can malleate a child of A', that shouldn't matter as\n>>>>> long as A' is signaling replacement. This would be handled identically with\n>>>>> full RBF and what Core currently implements.\n>>>>>\n>>>>> > I think this is an issue brought by the trimming during the dedup\n>>>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>>>> checks results of already in-mempool transactions to gain in CPU time we\n>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>>>\n>>>>> Sorry, I don't understand what you mean by \"preserve the package\n>>>>> integrity?\" Could you elaborate?\n>>>>>\n>>>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>>>\n>>>>> > Package A + B ancestor score is 10 sat/vb.\n>>>>>\n>>>>> > D has a higher feerate/absolute fee than B.\n>>>>>\n>>>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>>>> 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>>>\n>>>>> I am in agreement with your calculations but unsure if we disagree on\n>>>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n>>>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n>>>>> it fails the proposed package RBF Rule #2, so this package would be\n>>>>> rejected. Does this meet your expectations?\n>>>>>\n>>>>> Thank you for linking to projects that might be interested in package\n>>>>> relay :)\n>>>>>\n>>>>> Thanks,\n>>>>> Gloria\n>>>>>\n>>>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <\n>>>>> antoine.riard at gmail.com> wrote:\n>>>>>\n>>>>>> Hi Gloria,\n>>>>>>\n>>>>>> > A package may contain transactions that are already in the mempool.\n>>>>>> We\n>>>>>> > remove\n>>>>>> > (\"deduplicate\") those transactions from the package for the\n>>>>>> purposes of\n>>>>>> > package\n>>>>>> > mempool acceptance. If a package is empty after deduplication, we do\n>>>>>> > nothing.\n>>>>>>\n>>>>>> IIUC, you have a package A+B+C submitted for acceptance and A is\n>>>>>> already in your mempool. You trim out A from the package and then evaluate\n>>>>>> B+C.\n>>>>>>\n>>>>>> I think this might be an issue if A is the higher-fee element of the\n>>>>>> ABC package. B+C package fees might be under the mempool min fee and will\n>>>>>> be rejected, potentially breaking the acceptance expectations of the\n>>>>>> package issuer ?\n>>>>>>\n>>>>>> Further, I think the dedup should be done on wtxid, as you might have\n>>>>>> multiple valid witnesses. Though with varying vsizes and as such offering\n>>>>>> different feerates.\n>>>>>>\n>>>>>> E.g you're going to evaluate the package A+B and A' is already in\n>>>>>> your mempool with a bigger valid witness. You trim A based on txid, then\n>>>>>> you evaluate A'+B, which fails the fee checks. However, evaluating A+B\n>>>>>> would have been a success.\n>>>>>>\n>>>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to\n>>>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we\n>>>>>> achieve the same goal by bypassing tx-level checks for already-in txn while\n>>>>>> conserving the package integrity for package-level checks ?\n>>>>>>\n>>>>>> > Note that it's possible for the parents to be\n>>>>>> > indirect\n>>>>>> > descendants/ancestors of one another, or for parent and child to\n>>>>>> share a\n>>>>>> > parent,\n>>>>>> > so we cannot make any other topology assumptions.\n>>>>>>\n>>>>>> I'm not clearly understanding the accepted topologies. By \"parent and\n>>>>>> child to share a parent\", do you mean the set of transactions A, B, C,\n>>>>>> where B is spending A and C is spending A and B would be correct ?\n>>>>>>\n>>>>>> If yes, is there a width-limit introduced or we fallback on\n>>>>>> MAX_PACKAGE_COUNT=25 ?\n>>>>>>\n>>>>>> IIRC, one rationale to come with this topology limitation was to\n>>>>>> lower the DoS risks when potentially deploying p2p packages.\n>>>>>>\n>>>>>> Considering the current Core's mempool acceptance rules, I think CPFP\n>>>>>> batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>>>>> jamming successful on one channel commitment transaction would contamine\n>>>>>> the remaining commitments sharing the same package.\n>>>>>>\n>>>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are commitment\n>>>>>> transactions and E a shared CPFP. If a malicious A' transaction has a\n>>>>>> better feerate than A, the whole package acceptance will fail. Even if A'\n>>>>>> confirms in the following block,\n>>>>>> the propagation and confirmation of B+C+D have been delayed. This\n>>>>>> could carry on a loss of funds.\n>>>>>>\n>>>>>> That said, if you're broadcasting commitment transactions without\n>>>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee\n>>>>>> saving as you don't have to duplicate the CPFP.\n>>>>>>\n>>>>>> IMHO, I'm leaning towards deploying during a first phase\n>>>>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>>>>> second-layer safety.\n>>>>>>\n>>>>>> > *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>>> that are\n>>>>>> > already in the mempool, as we do not want a transaction's fees to be\n>>>>>> > double-counted for both its individual RBF and package RBF.\n>>>>>>\n>>>>>> I'm unsure about the logical order of the checks proposed.\n>>>>>>\n>>>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200\n>>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B\n>>>>>> acceptance fails. For this reason I think the individual RBF should be\n>>>>>> bypassed and only the package RBF apply ?\n>>>>>>\n>>>>>> Note this situation is plausible, with current LN design, your\n>>>>>> counterparty can have a commitment transaction with a better fee just by\n>>>>>> selecting a higher `dust_limit_satoshis` than yours.\n>>>>>>\n>>>>>> > Examples F and G [14] show the same package, but P1 is submitted\n>>>>>> > individually before\n>>>>>> > the package in example G. In example F, we can see that the 300vB\n>>>>>> package\n>>>>>> > pays\n>>>>>> > an additional 200sat in fees, which is not enough to pay for its own\n>>>>>> > bandwidth\n>>>>>> > (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>>> M1, but\n>>>>>> > using P1's fees again during package submission would make it look\n>>>>>> like a\n>>>>>> > 300sat\n>>>>>> > increase for a 200vB package. Even including its fees and size\n>>>>>> would not be\n>>>>>> > sufficient in this example, since the 300sat looks like enough for\n>>>>>> the 300vB\n>>>>>> > package. The calculcation after deduplication is 100sat increase\n>>>>>> for a\n>>>>>> > package\n>>>>>> > of size 200vB, which correctly fails BIP125#4. Assume all\n>>>>>> transactions have\n>>>>>> > a\n>>>>>> > size of 100vB.\n>>>>>>\n>>>>>> What problem are you trying to solve by the package feerate *after*\n>>>>>> dedup rule ?\n>>>>>>\n>>>>>> My understanding is that an in-package transaction might be already\n>>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,\n>>>>>> the vsize of this transaction could be discarded lowering the cost of\n>>>>>> package RBF.\n>>>>>>\n>>>>>> If we keep a \"safe\" dedup mechanism (see my point above), I think\n>>>>>> this discount is justified, as the validation cost of node operators is\n>>>>>> paid for ?\n>>>>>>\n>>>>>> > The child cannot replace mempool transactions.\n>>>>>>\n>>>>>> Let's say you issue package A+B, then package C+B', where B' is a\n>>>>>> child of both A and C. This rule fails the acceptance of C+B' ?\n>>>>>>\n>>>>>> I think this is a footgunish API, as if a package issuer send the\n>>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>>>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe\n>>>>>> in protocols where states are symmetric. E.g a malicious counterparty\n>>>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better\n>>>>>> fees.\n>>>>>>\n>>>>>> > All mempool transactions to be replaced must signal replaceability.\n>>>>>>\n>>>>>> I think this is unsafe for L2s if counterparties have malleability of\n>>>>>> the child transaction. They can block your package replacement by\n>>>>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>>>>> ability.\n>>>>>>\n>>>>>> I think it's better to either fix inherited signaling or move towards\n>>>>>> full-rbf.\n>>>>>>\n>>>>>> > if a package parent has already been submitted, it would\n>>>>>> > look\n>>>>>> >like the child is spending a \"new\" unconfirmed input.\n>>>>>>\n>>>>>> I think this is an issue brought by the trimming during the dedup\n>>>>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>>>>> checks results of already in-mempool transactions to gain in CPU time we\n>>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>>>>\n>>>>>> > However, we still achieve the same goal of requiring the\n>>>>>> > replacement\n>>>>>> > transactions to have a ancestor score at least as high as the\n>>>>>> original\n>>>>>> > ones.\n>>>>>>\n>>>>>> I'm not sure if this holds...\n>>>>>>\n>>>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>>>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>>>>\n>>>>>> Package A + B ancestor score is 10 sat/vb.\n>>>>>>\n>>>>>> D has a higher feerate/absolute fee than B.\n>>>>>>\n>>>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>>>>> 1000 sats + D's 1500 sats) /\n>>>>>> A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>>>>\n>>>>>> Overall, this is a review through the lenses of LN requirements. I\n>>>>>> think other L2 protocols/applications\n>>>>>> could be candidates to using package accept/relay such as:\n>>>>>> * https://github.com/lightninglabs/pool\n>>>>>> * https://github.com/discreetlogcontracts/dlcspecs\n>>>>>> * https://github.com/bitcoin-teleport/teleport-transactions/\n>>>>>> * https://github.com/sapio-lang/sapio\n>>>>>> *\n>>>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n>>>>>> * https://github.com/revault/practical-revault\n>>>>>>\n>>>>>> Thanks for rolling forward the ball on this subject.\n>>>>>>\n>>>>>> Antoine\n>>>>>>\n>>>>>> Le jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\n>>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>>\n>>>>>>> Hi there,\n>>>>>>>\n>>>>>>> I'm writing to propose a set of mempool policy changes to enable\n>>>>>>> package\n>>>>>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>>>>>> would not\n>>>>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>>>>> significantly affects transaction propagation, I believe this is\n>>>>>>> relevant for\n>>>>>>> the mailing list.\n>>>>>>>\n>>>>>>> My proposal enables packages consisting of multiple parents and 1\n>>>>>>> child. If you\n>>>>>>> develop software that relies on specific transaction relay\n>>>>>>> assumptions and/or\n>>>>>>> are interested in using package relay in the future, I'm very\n>>>>>>> interested to hear\n>>>>>>> your feedback on the utility or restrictiveness of these package\n>>>>>>> policies for\n>>>>>>> your use cases.\n>>>>>>>\n>>>>>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>>>>>> PR#22290][1].\n>>>>>>>\n>>>>>>> An illustrated version of this post can be found at\n>>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>>>>> I have also linked the images below.\n>>>>>>>\n>>>>>>> ## Background\n>>>>>>>\n>>>>>>> Feel free to skip this section if you are already familiar with\n>>>>>>> mempool policy\n>>>>>>> and package relay terminology.\n>>>>>>>\n>>>>>>> ### Terminology Clarifications\n>>>>>>>\n>>>>>>> * Package = an ordered list of related transactions, representable\n>>>>>>> by a Directed\n>>>>>>>   Acyclic Graph.\n>>>>>>> * Package Feerate = the total modified fees divided by the total\n>>>>>>> virtual size of\n>>>>>>>   all transactions in the package.\n>>>>>>>     - Modified fees = a transaction's base fees + fee delta applied\n>>>>>>> by the user\n>>>>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>>>>> across\n>>>>>>> mempools.\n>>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>>>>> [BIP141\n>>>>>>>       virtual size][2] and sigop weight. [Implemented here in\n>>>>>>> Bitcoin Core][3].\n>>>>>>>     - Note that feerate is not necessarily based on the base fees\n>>>>>>> and serialized\n>>>>>>>       size.\n>>>>>>>\n>>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>>>>> incentives to\n>>>>>>>   boost a transaction's candidacy for inclusion in a block,\n>>>>>>> including Child Pays\n>>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our\n>>>>>>> intention in\n>>>>>>> mempool policy is to recognize when the new transaction is more\n>>>>>>> economical to\n>>>>>>> mine than the original one(s) but not open DoS vectors, so there are\n>>>>>>> some\n>>>>>>> limitations.\n>>>>>>>\n>>>>>>> ### Policy\n>>>>>>>\n>>>>>>> The purpose of the mempool is to store the best (to be most\n>>>>>>> incentive-compatible\n>>>>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>>>>> Miners use\n>>>>>>> the mempool to build block templates. The mempool is also useful as\n>>>>>>> a cache for\n>>>>>>> boosting block relay and validation performance, aiding transaction\n>>>>>>> relay, and\n>>>>>>> generating feerate estimations.\n>>>>>>>\n>>>>>>> Ideally, all consensus-valid transactions paying reasonable fees\n>>>>>>> should make it\n>>>>>>> to miners through normal transaction relay, without any special\n>>>>>>> connectivity or\n>>>>>>> relationships with miners. On the other hand, nodes do not have\n>>>>>>> unlimited\n>>>>>>> resources, and a P2P network designed to let any honest node\n>>>>>>> broadcast their\n>>>>>>> transactions also exposes the transaction validation engine to DoS\n>>>>>>> attacks from\n>>>>>>> malicious peers.\n>>>>>>>\n>>>>>>> As such, for unconfirmed transactions we are considering for our\n>>>>>>> mempool, we\n>>>>>>> apply a set of validation rules in addition to consensus, primarily\n>>>>>>> to protect\n>>>>>>> us from resource exhaustion and aid our efforts to keep the highest\n>>>>>>> fee\n>>>>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>>>>> node-specific) rules that transactions must abide by in order to be\n>>>>>>> accepted\n>>>>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>>>>> restrictions such\n>>>>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>>>>\n>>>>>>> ### Package Relay and Package Mempool Accept\n>>>>>>>\n>>>>>>> In transaction relay, we currently consider transactions one at a\n>>>>>>> time for\n>>>>>>> submission to the mempool. This creates a limitation in the node's\n>>>>>>> ability to\n>>>>>>> determine which transactions have the highest feerates, since we\n>>>>>>> cannot take\n>>>>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>>>>> transactions are\n>>>>>>> in the mempool. Similarly, we cannot use a transaction's descendants\n>>>>>>> when\n>>>>>>> considering it for RBF. When an individual transaction does not meet\n>>>>>>> the mempool\n>>>>>>> minimum feerate and the user isn't able to create a replacement\n>>>>>>> transaction\n>>>>>>> directly, it will not be accepted by mempools.\n>>>>>>>\n>>>>>>> This limitation presents a security issue for applications and users\n>>>>>>> relying on\n>>>>>>> time-sensitive transactions. For example, Lightning and other\n>>>>>>> protocols create\n>>>>>>> UTXOs with multiple spending paths, where one counterparty's\n>>>>>>> spending path opens\n>>>>>>> up after a timelock, and users are protected from cheating scenarios\n>>>>>>> as long as\n>>>>>>> they redeem on-chain in time. A key security assumption is that all\n>>>>>>> parties'\n>>>>>>> transactions will propagate and confirm in a timely manner. This\n>>>>>>> assumption can\n>>>>>>> be broken if fee-bumping does not work as intended.\n>>>>>>>\n>>>>>>> The end goal for Package Relay is to consider multiple transactions\n>>>>>>> at the same\n>>>>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>>>>> better\n>>>>>>> determine whether transactions should be accepted to our mempool,\n>>>>>>> especially if\n>>>>>>> they don't meet fee requirements individually or are better RBF\n>>>>>>> candidates as a\n>>>>>>> package. A combination of changes to mempool validation logic,\n>>>>>>> policy, and\n>>>>>>> transaction relay allows us to better propagate the transactions\n>>>>>>> with the\n>>>>>>> highest package feerates to miners, and makes fee-bumping tools more\n>>>>>>> powerful\n>>>>>>> for users.\n>>>>>>>\n>>>>>>> The \"relay\" part of Package Relay suggests P2P messaging changes,\n>>>>>>> but a large\n>>>>>>> part of the changes are in the mempool's package validation logic.\n>>>>>>> We call this\n>>>>>>> *Package Mempool Accept*.\n>>>>>>>\n>>>>>>> ### Previous Work\n>>>>>>>\n>>>>>>> * Given that mempool validation is DoS-sensitive and complex, it\n>>>>>>> would be\n>>>>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>>>>> efforts have\n>>>>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>>>>> [#21062][5],\n>>>>>>> [#22675][6], [#22796][7]).\n>>>>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>>>>> accepts only\n>>>>>>>   (no submission to mempool).\n>>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks\n>>>>>>> for arbitrary\n>>>>>>>   packages. Still test accepts only.\n>>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>>>>\n>>>>>>> ### Existing Package Rules\n>>>>>>>\n>>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].\n>>>>>>> I'll consider\n>>>>>>> them as \"given\" in the rest of this document, though they can be\n>>>>>>> changed, since\n>>>>>>> package validation is test-accept only right now.\n>>>>>>>\n>>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>>>>\n>>>>>>>    *Rationale*: This is already enforced as mempool\n>>>>>>> ancestor/descendant limits.\n>>>>>>> Presumably, transactions in a package are all related, so exceeding\n>>>>>>> this limit\n>>>>>>> would mean that the package can either be split up or it wouldn't\n>>>>>>> pass this\n>>>>>>> mempool policy.\n>>>>>>>\n>>>>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>>>>> between\n>>>>>>> transactions, parents must appear somewhere before children. [8]\n>>>>>>>\n>>>>>>> 3. A package cannot have conflicting transactions, i.e. none of them\n>>>>>>> can spend\n>>>>>>> the same inputs. This also means there cannot be duplicate\n>>>>>>> transactions. [8]\n>>>>>>>\n>>>>>>> 4. When packages are evaluated against ancestor/descendant limits in\n>>>>>>> a test\n>>>>>>> accept, the union of all of their descendants and ancestors is\n>>>>>>> considered. This\n>>>>>>> is essentially a \"worst case\" heuristic where every transaction in\n>>>>>>> the package\n>>>>>>> is treated as each other's ancestor and descendant. [8]\n>>>>>>> Packages for which ancestor/descendant limits are accurately\n>>>>>>> captured by this\n>>>>>>> heuristic: [19]\n>>>>>>>\n>>>>>>> There are also limitations such as the fact that CPFP carve out is\n>>>>>>> not applied\n>>>>>>> to package transactions. #20833 also disables RBF in package\n>>>>>>> validation; this\n>>>>>>> proposal overrides that to allow packages to use RBF.\n>>>>>>>\n>>>>>>> ## Proposed Changes\n>>>>>>>\n>>>>>>> The next step in the Package Mempool Accept project is to implement\n>>>>>>> submission\n>>>>>>> to mempool, initially through RPC only. This allows us to test the\n>>>>>>> submission\n>>>>>>> logic before exposing it on P2P.\n>>>>>>>\n>>>>>>> ### Summary\n>>>>>>>\n>>>>>>> - Packages may contain already-in-mempool transactions.\n>>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>>>>> - Fee-related checks use the package feerate. This means that\n>>>>>>> wallets can\n>>>>>>> create a package that utilizes CPFP.\n>>>>>>> - Parents are allowed to RBF mempool transactions with a set of\n>>>>>>> rules similar\n>>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>>>>\n>>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but\n>>>>>>> feedback is\n>>>>>>> always welcome.\n>>>>>>>\n>>>>>>> ### Details\n>>>>>>>\n>>>>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>>>>\n>>>>>>> A package may contain transactions that are already in the mempool.\n>>>>>>> We remove\n>>>>>>> (\"deduplicate\") those transactions from the package for the purposes\n>>>>>>> of package\n>>>>>>> mempool acceptance. If a package is empty after deduplication, we do\n>>>>>>> nothing.\n>>>>>>>\n>>>>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>>>>> parent to be\n>>>>>>> accepted to the mempool of a peer on its own due to differences in\n>>>>>>> policy and\n>>>>>>> fee market fluctuations. We should not reject or penalize the entire\n>>>>>>> package for\n>>>>>>> an individual transaction as that could be a censorship vector.\n>>>>>>>\n>>>>>>> #### Packages Are Multi-Parent-1-Child\n>>>>>>>\n>>>>>>> Only packages of a specific topology are permitted. Namely, a\n>>>>>>> package is exactly\n>>>>>>> 1 child with all of its unconfirmed parents. After deduplication,\n>>>>>>> the package\n>>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of\n>>>>>>> its\n>>>>>>> unconfirmed parents, etc. Note that it's possible for the parents to\n>>>>>>> be indirect\n>>>>>>> descendants/ancestors of one another, or for parent and child to\n>>>>>>> share a parent,\n>>>>>>> so we cannot make any other topology assumptions.\n>>>>>>>\n>>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>>>>> parents\n>>>>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>>>>> packages to a\n>>>>>>> defined topology is also easier to reason about and simplifies the\n>>>>>>> validation\n>>>>>>> logic greatly. Multi-parent-1-child allows us to think of the\n>>>>>>> package as one big\n>>>>>>> transaction, where:\n>>>>>>>\n>>>>>>> - Inputs = all the inputs of parents + inputs of the child that come\n>>>>>>> from\n>>>>>>>   confirmed UTXOs\n>>>>>>> - Outputs = all the outputs of the child + all outputs of the\n>>>>>>> parents that\n>>>>>>>   aren't spent by other transactions in the package\n>>>>>>>\n>>>>>>> Examples of packages that follow this rule (variations of example A\n>>>>>>> show some\n>>>>>>> possibilities after deduplication): ![image][15]\n>>>>>>>\n>>>>>>> #### Fee-Related Checks Use Package Feerate\n>>>>>>>\n>>>>>>> Package Feerate = the total modified fees divided by the total\n>>>>>>> virtual size of\n>>>>>>> all transactions in the package.\n>>>>>>>\n>>>>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>>>>> pre-configured\n>>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>>>>> feerate, the\n>>>>>>> total package feerate is used instead of the individual feerate. The\n>>>>>>> individual\n>>>>>>> transactions are allowed to be below feerate requirements if the\n>>>>>>> package meets\n>>>>>>> the feerate requirements. For example, the parent(s) in the package\n>>>>>>> can have 0\n>>>>>>> fees but be paid for by the child.\n>>>>>>>\n>>>>>>> *Rationale*: This can be thought of as \"CPFP within a package,\"\n>>>>>>> solving the\n>>>>>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>>>>>> applications to adjust their fees at broadcast time instead of\n>>>>>>> overshooting or\n>>>>>>> risking getting stuck/pinned.\n>>>>>>>\n>>>>>>> We use the package feerate of the package *after deduplication*.\n>>>>>>>\n>>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>>>> that are\n>>>>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>>>>> double-counted for both its individual RBF and package RBF.\n>>>>>>>\n>>>>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>>>>> individually before\n>>>>>>> the package in example G. In example F, we can see that the 300vB\n>>>>>>> package pays\n>>>>>>> an additional 200sat in fees, which is not enough to pay for its own\n>>>>>>> bandwidth\n>>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>>>> M1, but\n>>>>>>> using P1's fees again during package submission would make it look\n>>>>>>> like a 300sat\n>>>>>>> increase for a 200vB package. Even including its fees and size would\n>>>>>>> not be\n>>>>>>> sufficient in this example, since the 300sat looks like enough for\n>>>>>>> the 300vB\n>>>>>>> package. The calculcation after deduplication is 100sat increase for\n>>>>>>> a package\n>>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all\n>>>>>>> transactions have a\n>>>>>>> size of 100vB.\n>>>>>>>\n>>>>>>> #### Package RBF\n>>>>>>>\n>>>>>>> If a package meets feerate requirements as a package, the parents in\n>>>>>>> the\n>>>>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>>>>> child cannot\n>>>>>>> replace mempool transactions. Multiple transactions can replace the\n>>>>>>> same\n>>>>>>> transaction, but in order to be valid, none of the transactions can\n>>>>>>> try to\n>>>>>>> replace an ancestor of another transaction in the same package\n>>>>>>> (which would thus\n>>>>>>> make its inputs unavailable).\n>>>>>>>\n>>>>>>> *Rationale*: Even if we are using package feerate, a package will\n>>>>>>> not propagate\n>>>>>>> as intended if RBF still requires each individual transaction to\n>>>>>>> meet the\n>>>>>>> feerate requirements.\n>>>>>>>\n>>>>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>>>>\n>>>>>>> ##### Signaling (Rule #1)\n>>>>>>>\n>>>>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>>>>\n>>>>>>> *Rationale*: Package RBF signaling logic should be the same for\n>>>>>>> package RBF and\n>>>>>>> single transaction acceptance. This would be updated if single\n>>>>>>> transaction\n>>>>>>> validation moves to full RBF.\n>>>>>>>\n>>>>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>>>>\n>>>>>>> A package may include new unconfirmed inputs, but the ancestor\n>>>>>>> feerate of the\n>>>>>>> child must be at least as high as the ancestor feerates of every\n>>>>>>> transaction\n>>>>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>>>>> replacement\n>>>>>>> transaction may only include an unconfirmed input if that input was\n>>>>>>> included in\n>>>>>>> one of the original transactions. (An unconfirmed input spends an\n>>>>>>> output from a\n>>>>>>> currently-unconfirmed transaction.)\"\n>>>>>>>\n>>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the\n>>>>>>> replacement\n>>>>>>> transaction has a higher ancestor score than the original\n>>>>>>> transaction(s) (see\n>>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed\n>>>>>>> input can lower the\n>>>>>>> ancestor score of the replacement transaction. P1 is trying to\n>>>>>>> replace M1, and\n>>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,\n>>>>>>> and M2 pays\n>>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>>>>> isolation, P1\n>>>>>>> looks like a better mining candidate than M1, it must be mined with\n>>>>>>> M2, so its\n>>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>>>>> ancestor\n>>>>>>> feerate, which is 6sat/vB.\n>>>>>>>\n>>>>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>>>>> transactions in the package can spend new unconfirmed inputs.\"\n>>>>>>> Example J [17] shows\n>>>>>>> why, if any of the package transactions have ancestors, package\n>>>>>>> feerate is no\n>>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1\n>>>>>>> (which is the\n>>>>>>> replacement transaction in an RBF), we're actually interested in the\n>>>>>>> entire\n>>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,\n>>>>>>> P1, P2, and\n>>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened\n>>>>>>> to only allow\n>>>>>>> the child to have new unconfirmed inputs, either, because it can\n>>>>>>> still cause us\n>>>>>>> to overestimate the package's ancestor score.\n>>>>>>>\n>>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>>>>> Package RBF\n>>>>>>> less useful, but would also break Package RBF for packages with\n>>>>>>> parents already\n>>>>>>> in the mempool: if a package parent has already been submitted, it\n>>>>>>> would look\n>>>>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>>>>> [18], we're\n>>>>>>> looking to replace M1 with the entire package including P1, P2, and\n>>>>>>> P3. We must\n>>>>>>> consider the case where one of the parents is already in the mempool\n>>>>>>> (in this\n>>>>>>> case, P2), which means we must allow P3 to have new unconfirmed\n>>>>>>> inputs. However,\n>>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>>>>> replace M1\n>>>>>>> with this package.\n>>>>>>>\n>>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>>>>> strict than\n>>>>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>>>>> replacement\n>>>>>>> transactions to have a ancestor score at least as high as the\n>>>>>>> original ones. As\n>>>>>>> a result, the entire package is required to be a higher feerate\n>>>>>>> mining candidate\n>>>>>>> than each of the replaced transactions.\n>>>>>>>\n>>>>>>> Another note: the [comment][13] above the BIP125#2 code in the\n>>>>>>> original RBF\n>>>>>>> implementation suggests that the rule was intended to be temporary.\n>>>>>>>\n>>>>>>> ##### Absolute Fee (Rule #3)\n>>>>>>>\n>>>>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>>>>> total fees\n>>>>>>> of the package must be higher than the absolute fees of the mempool\n>>>>>>> transactions\n>>>>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>>>>> BIP125 Rule #3\n>>>>>>> - an individual transaction in the package may have lower fees than\n>>>>>>> the\n>>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and\n>>>>>>> the child\n>>>>>>> pays for RBF.\n>>>>>>>\n>>>>>>> ##### Feerate (Rule #4)\n>>>>>>>\n>>>>>>> The package must pay for its own bandwidth; the package feerate must\n>>>>>>> be higher\n>>>>>>> than the replaced transactions by at least minimum relay feerate\n>>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>>>>> differs from\n>>>>>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>>>>>> lower\n>>>>>>> feerate than the transaction(s) it is replacing. In fact, it may\n>>>>>>> have 0 fees,\n>>>>>>> and the child pays for RBF.\n>>>>>>>\n>>>>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>>>>\n>>>>>>> The package cannot replace more than 100 mempool transactions. This\n>>>>>>> is identical\n>>>>>>> to BIP125 Rule #5.\n>>>>>>>\n>>>>>>> ### Expected FAQs\n>>>>>>>\n>>>>>>> 1. Is it possible for only some of the package to make it into the\n>>>>>>> mempool?\n>>>>>>>\n>>>>>>>    Yes, it is. However, since we evict transactions from the mempool\n>>>>>>> by\n>>>>>>> descendant score and the package child is supposed to be sponsoring\n>>>>>>> the fees of\n>>>>>>> its parents, the most common scenario would be all-or-nothing. This\n>>>>>>> is\n>>>>>>> incentive-compatible. In fact, to be conservative, package\n>>>>>>> validation should\n>>>>>>> begin by trying to submit all of the transactions individually, and\n>>>>>>> only use the\n>>>>>>> package mempool acceptance logic if the parents fail due to low\n>>>>>>> feerate.\n>>>>>>>\n>>>>>>> 2. Should we allow packages to contain already-confirmed\n>>>>>>> transactions?\n>>>>>>>\n>>>>>>>     No, for practical reasons. In mempool validation, we actually\n>>>>>>> aren't able to\n>>>>>>> tell with 100% confidence if we are looking at a transaction that\n>>>>>>> has already\n>>>>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>>>>> historical\n>>>>>>> block data, it's possible to look for it, but this is inefficient,\n>>>>>>> not always\n>>>>>>> possible for pruning nodes, and unnecessary because we're not going\n>>>>>>> to do\n>>>>>>> anything with the transaction anyway. As such, we already have the\n>>>>>>> expectation\n>>>>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>>>>> relaying\n>>>>>>> transactions that have already been confirmed. Similarly, we\n>>>>>>> shouldn't be\n>>>>>>> relaying packages that contain already-confirmed transactions.\n>>>>>>>\n>>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>>>>> [2]:\n>>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>>>>> [3]:\n>>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>>>>> [13]:\n>>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>>>>> [14]:\n>>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>>>>> [15]:\n>>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>>>>> [16]:\n>>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>>>>> [17]:\n>>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>>>>> [18]:\n>>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>>>>> [19]:\n>>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>>>>> [20]:\n>>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>>>>> _______________________________________________\n>>>>>>> bitcoin-dev mailing list\n>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>\n>>>>>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210928/ef67cddf/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2021-09-29T11:56:24",
                "message_text_only": "Hi Antoine and Bastien,\n\n> Yes 1) it would be good to have inputs of more potential users of package\nacceptance . And 2) I think it's more a matter of clearer wording of the\nproposal.\n\n(1) I'm leaning towards multi-parent-1-child and offering [#22674][0] up\nfor review. If somebody feels very strongly about 1-parent-1-child, please\nlet me know.\n\n(2) I'm glad this turned out to just be a wording problem. I've updated the\nproposal to [say][1] \"If it meets feerate requirements, the package can\nreplace mempool transactions if any of the parents conflict with mempool\ntransactions. The child cannot conflict with any mempool transactions.\"\nHopefully that is more *univoque*.\n\nSide note: I've also updated the proposal to contain a [section][2] on why\nsubmitting transactions individually before package validation is\nincentive-compatible. I think it's relevant to our conversation, but for\nthose who just want to _use_ packages, it's just an implementation detail.\n\nOn restricting packages to confirmed inputs only:\n\n> I think we could restrain package acceptance to only confirmed inputs for\nnow and revisit later this point ? For LN-anchor, you can assume that the\nfee-bumping UTXO feeding the CPFP is already\nconfirmed. Or are there currently-deployed use-cases which would benefit\nfrom your proposed Rule #2 ?\n\nI thought about this a lot this week, and wrote up a summary of why I don't\nthink BIP125#2 helps us at all [here][3] on #23121. I see that you've\nalready come across it :)\n\n> IIRC, the carve-out tolerance is only 2txn/10_000 vb. If one of your\ncounterparties attach a junk branch on her own anchor output, are you\nallowed to chain your self-owned unconfirmed CPFP ?\n\nYes, if your counterparty attaches a bunch of descendants to their anchor\noutput to dominate the descendant limit of your shared commitment\ntransaction, CPFP carve out allows you to add 1 extra transaction under\n10KvB to your own anchor output. It's fine if it spends an unconfirmed\ninput, as long as you aren't exceeding the descendant limits of that\ntransaction. This shouldn't be the case; I think something is seriously\nwrong if all of your UTXOs are tied up in mempool transactions with big\nancestor/descendant trees.\n\nI don't know much about L2 development so I'm just going to quote this:\n\n> I think constraining package acceptance to only confirmed inputs is very\nlimiting and quite dangerous for L2 protocols.\n\nSince the restriction isn't helpful in simplifying the mempool code, makes\nthings more complicated for application developers, and can be dangerous\nfor L2, I'd prefer not to add this restriction for packages.\n\nOn Antoine's question about our miner model:\n\n> Can you describe what miner model we are using ? Like the block\nconstruction strategy implemented by `addPackagesTxs` or also encompassing\nour current mempool acceptance policy, which I think rely on absolute fee\nover ancestor score in case of replacement ?\n\nOur current model for block construction is this: we sort our mempool by\npackage ancestor score (total modified fees of a tx and its unconfirmed\nancestors / total vsize as seen by our mempool) and add packages to a block\nuntil it's full. That's not to say this is the perfect miner policy, but\nmempool acceptance logic follows this model as closely as possible because\nit is, fundamentally, a cache that aids in block assembly performance. As\nanother way of looking at this, imagine if our mempool was so small it\ncould only store ~1 block's worth of transactions. It should always try to\nkeep the highest-fees-within-1-block transactions, and obviously wouldn't\nevict small-but-valuable transations in favor or giant ones paying mediocre\nfeerates. All fee-related mempool policies, including RBF, consider\nfeerate. BIP125#3 is a rule on absolute fees, but it is always combined\nwith BIP125#4, a rule on feerates. AFAIK, the reason it doesn't use\nancestor score is that information wasn't cached in mempool entries at the\ntime, and thus not readily available to use in mempool validation.\n\nThat's why I don't think this is relevant to package validation. Commenting\non the model itself:\n\n> Is this compatible with a model where a miner prioritizes absolute fees\nover ancestor score, in the case that mempools aren't full-enough to\nfulfill a block ?\n>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\nminer should prefer to utilize their block space more effectively.\n> If your mempool is empty and only composed of A+C+D or A+B, I think\ntaking A+C+D is the most efficient block construction you can come up with\nas a miner ?\n> I think this point is worthy to discuss as otherwise we might downgrade\nthe efficiency of our current block construction strategy in periods of\nnear-empty mempools. A knowledge which could be discreetly leveraged by a\nminer to gain an advantage on the rest of the mining ecosystem.\n\nI believe this is suggesting \"if our mempool has so few transactions that\nit wouldn't reach block capacity, prioritize any increase in absolute fees,\neven if the feerate is lower.\" I can see how this may result in a\nhigher-fee block in a specific scenario such as the one highlighted above,\nbut I don't think it is a sound model in general. It would be impossible to\ntell when we should use this model: we could simply be in IBD, restarted a\nnode with an old/empty mempool.dat, and even if it's a\nlow-transaction-volume time, we never know what transactions will trickle\nin between now and the next block. Going back to the tiny 1-block mempool\nscenario, i.e., if you _never_ wanted to keep transactions that you\nwouldn't put in the next block, would you ever switch strategies?\n\nThanks again to everyone who's given their attention to the package mempool\naccept proposal.\n\nBest,\nGloria\n\n[0]: https://github.com/bitcoin/bitcoin/pull/22674\n[1]:\nhttps://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a#package-rbf\n[2]:\nhttps://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a#always-try-individual-submission-first\n[3]: https://github.com/bitcoin/bitcoin/pull/23121#issuecomment-929475999\n\nOn Tue, Sep 28, 2021 at 11:59 PM Antoine Riard <antoine.riard at gmail.com>\nwrote:\n\n> Hi Bastien\n>\n> > In the case of LN, an attacker can game this and heavily restrict\n> your RBF attempts if you're only allowed to use confirmed inputs\n> and have many channels (and a limited number of confirmed inputs).\n> Otherwise you'll need node operators to pre-emptively split their\n> utxos into many small utxos just for fee bumping, which is inefficient...\n>\n> I share the concern about splitting utxos into smaller ones.\n> IIRC, the carve-out tolerance is only 2txn/10_000 vb. If one of your\n> counterparties attach a junk branch on her own anchor output, are you\n> allowed to chain your self-owned unconfirmed CPFP ?\n> I'm thinking about the topology \"Chained CPFPs\" exposed here :\n> https://github.com/rust-bitcoin/rust-lightning/issues/989.\n> Or if you have another L2 broadcast topology which could be safe w.r.t our\n> current mempool logic :) ?\n>\n>\n> Le lun. 27 sept. 2021 \u00e0 03:15, Bastien TEINTURIER <bastien at acinq.fr> a\n> \u00e9crit :\n>\n>> I think we could restrain package acceptance to only confirmed inputs for\n>>> now and revisit later this point ? For LN-anchor, you can assume that the\n>>> fee-bumping UTXO feeding the CPFP is already\n>>> confirmed. Or are there currently-deployed use-cases which would benefit\n>>> from your proposed Rule #2 ?\n>>>\n>>\n>> I think constraining package acceptance to only confirmed inputs\n>> is very limiting and quite dangerous for L2 protocols.\n>>\n>> In the case of LN, an attacker can game this and heavily restrict\n>> your RBF attempts if you're only allowed to use confirmed inputs\n>> and have many channels (and a limited number of confirmed inputs).\n>> Otherwise you'll need node operators to pre-emptively split their\n>> utxos into many small utxos just for fee bumping, which is inefficient...\n>>\n>> Bastien\n>>\n>> Le lun. 27 sept. 2021 \u00e0 00:27, Antoine Riard via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi Gloria,\n>>>\n>>> Thanks for your answers,\n>>>\n>>> > In summary, it seems that the decisions that might still need\n>>> > attention/input from devs on this mailing list are:\n>>> > 1. Whether we should start with multiple-parent-1-child or\n>>> 1-parent-1-child.\n>>> > 2. Whether it's ok to require that the child not have conflicts with\n>>> > mempool transactions.\n>>>\n>>> Yes 1) it would be good to have inputs of more potential users of\n>>> package acceptance . And 2) I think it's more a matter of clearer wording\n>>> of the proposal.\n>>>\n>>> However, see my final point on the relaxation around \"unconfirmed\n>>> inputs\" which might in fact alter our current block construction strategy.\n>>>\n>>> > Right, the fact that we essentially always choose the first-seen\n>>> witness is\n>>> > an unfortunate limitation that exists already. Adding package mempool\n>>> > accept doesn't worsen this, but the procedure in the future is to\n>>> replace\n>>> > the witness when it makes sense economically. We can also add logic to\n>>> > allow package feerate to pay for witness replacements as well. This is\n>>> > pretty far into the future, though.\n>>>\n>>> Yes I agree package mempool doesn't worsen this. And it's not an issue\n>>> for current LN as you can't significantly inflate a spending witness for\n>>> the 2-of-2 funding output.\n>>> However, it might be an issue for multi-party protocol where the\n>>> spending script has alternative branches with asymmetric valid witness\n>>> weights. Taproot should ease that kind of script so hopefully we would\n>>> deploy wtxid-replacement not too far in the future.\n>>>\n>>> > I could be misunderstanding, but an attacker wouldn't be able to\n>>> > batch-attack like this. Alice's package only conflicts with A' + D',\n>>> not A'\n>>> > + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n>>>\n>>> Yeah I can be clearer, I think you have 2 pinning attacks scenarios to\n>>> consider.\n>>>\n>>> In LN, if you're trying to confirm a commitment transaction to time-out\n>>> or claim on-chain a HTLC and the timelock is near-expiration, you should be\n>>> ready to pay in commitment+2nd-stage HTLC transaction fees as much as the\n>>> value offered by the HTLC.\n>>>\n>>> Following this security assumption, an attacker can exploit it by\n>>> targeting together commitment transactions from different channels by\n>>> blocking them under a high-fee child, of which the fee value\n>>> is equal to the top-value HTLC + 1. Victims's fee-bumping logics won't\n>>> overbid as it's not worthy to offer fees beyond their competed HTLCs. Apart\n>>> from observing mempools state, victims can't learn they're targeted by the\n>>> same attacker.\n>>>\n>>> To draw from the aforementioned topology, Mallory broadcasts A' + B' +\n>>> C' + D', where A' conflicts with Alice's P1, B' conflicts with Bob's P2, C'\n>>> conflicts with Caroll's P3. Let's assume P1 is confirming the top-value\n>>> HTLC of the set. If D' fees is higher than P1 + 1, it won't be rational for\n>>> Alice or Bob or Caroll to keep offering competing feerates. Mallory will be\n>>> at loss on stealing P1, as she has paid more in fees but will realize a\n>>> gain on P2+P3.\n>>>\n>>> In this model, Alice is allowed to evict those 2 transactions (A' + D')\n>>> but as she is economically-bounded she won't succeed.\n>>>\n>>> Mallory is maliciously exploiting RBF rule 3 on absolute fee. I think\n>>> this 1st pinning scenario is correct and \"lucractive\" when you sum the\n>>> global gain/loss.\n>>>\n>>> There is a 2nd attack scenario where A + B + C + D, where D is the child\n>>> of A,B,C. All those transactions are honestly issued by Alice. Once A + B +\n>>> C + D are propagated in network mempools, Mallory is able to replace A + D\n>>> with  A' + D' where D' is paying a higher fee. This package A' + D' will\n>>> confirm soon if D feerate was compelling but Mallory succeeds in delaying\n>>> the confirmation\n>>> of B + C for one or more blocks. As B + C are pre-signed commitments\n>>> with a low-fee rate they won't confirm without Alice issuing a new child E.\n>>> Mallory can repeat the same trick by broadcasting\n>>> B' + E' and delay again the confirmation of C.\n>>>\n>>> If the remaining package pending HTLC has a higher-value than all the\n>>> malicious fees over-bid, Mallory should realize a gain. With this 2nd\n>>> pinning attack, the malicious entity buys confirmation delay of your\n>>> packaged-together commitments.\n>>>\n>>> Assuming those attacks are correct, I'm leaning towards being\n>>> conservative with the LDK broadcast backend. Though once again, other L2\n>>> devs have likely other use-cases and opinions :)\n>>>\n>>> >  B' only needs to pay for itself in this case.\n>>>\n>>> Yes I think it's a nice discount when UTXO is single-owned. In the\n>>> context of shared-owned UTXO (e.g LN), you might not if there is an\n>>> in-mempool package already spending the UTXO and have to assume the\n>>> worst-case scenario. I.e have B' committing enough fee to pay for A'\n>>> replacement bandwidth. I think we can't do that much for this case...\n>>>\n>>> > If a package meets feerate requirements as a\n>>> package, the parents in the transaction are allowed to replace-by-fee\n>>> mempool transactions. The child cannot replace mempool transactions.\"\n>>>\n>>> I agree with the Mallory-vs-Alice case. Though if Alice broadcasts A+B'\n>>> to replace A+B because the first broadcast isn't satisfying anymore due to\n>>> mempool spikes ? Assuming B' fees is enough, I think that case as child B'\n>>> replacing in-mempool transaction B. Which I understand going against  \"The\n>>> child cannot replace mempool transactions\".\n>>>\n>>> Maybe wording could be a bit clearer ?\n>>>\n>>> > While it would be nice to have full RBF, malleability of the child\n>>> won't\n>>> > block RBF here. If we're trying to replace A', we only require that A'\n>>> > signals replaceability, and don't mind if its child doesn't.\n>>>\n>>> Yes, it sounds good.\n>>>\n>>> > Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\n>>> miner\n>>> > should prefer to utilize their block space more effectively.\n>>>\n>>> If your mempool is empty and only composed of A+C+D or A+B, I think\n>>> taking A+C+D is the most efficient block construction you can come up with\n>>> as a miner ?\n>>>\n>>> > No, because we don't use that model.\n>>>\n>>> Can you describe what miner model we are using ? Like the block\n>>> construction strategy implemented by `addPackagesTxs` or also encompassing\n>>> our current mempool acceptance policy, which I think rely on absolute fee\n>>> over ancestor score in case of replacement ?\n>>>\n>>> I think this point is worthy to discuss as otherwise we might downgrade\n>>> the efficiency of our current block construction strategy in periods of\n>>> near-empty mempools. A knowledge which could be discreetly leveraged by a\n>>> miner to gain an advantage on the rest of the mining ecosystem.\n>>>\n>>> Note, I think we *might* have to go in this direction if we want to\n>>> replace replace-by-fee by replace-by-feerate or replace-by-ancestor and\n>>> solve in-depth pinning attacks. Though if we do so,\n>>> IMO we would need more thoughts.\n>>>\n>>> I think we could restrain package acceptance to only confirmed inputs\n>>> for now and revisit later this point ? For LN-anchor, you can assume that\n>>> the fee-bumping UTXO feeding the CPFP is already\n>>> confirmed. Or are there currently-deployed use-cases which would benefit\n>>> from your proposed Rule #2 ?\n>>>\n>>> Antoine\n>>>\n>>> Le jeu. 23 sept. 2021 \u00e0 11:36, Gloria Zhao <gloriajzhao at gmail.com> a\n>>> \u00e9crit :\n>>>\n>>>> Hi Antoine,\n>>>>\n>>>> Thanks as always for your input. I'm glad we agree on so much!\n>>>>\n>>>> In summary, it seems that the decisions that might still need\n>>>> attention/input from devs on this mailing list are:\n>>>> 1. Whether we should start with multiple-parent-1-child or\n>>>> 1-parent-1-child.\n>>>> 2. Whether it's ok to require that the child not have conflicts with\n>>>> mempool transactions.\n>>>>\n>>>> Responding to your comments...\n>>>>\n>>>> > IIUC, you have package A+B, during the dedup phase early in\n>>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n>>>> and A' is higher feerate than A, you trim A and replace by A' ?\n>>>>\n>>>> > I think this approach is safe, the one who appears unsafe to me is\n>>>> when A' has a _lower_ feerate, even if A' is already accepted by our\n>>>> mempool ? In that case iirc that would be a pinning.\n>>>>\n>>>> Right, the fact that we essentially always choose the first-seen\n>>>> witness is an unfortunate limitation that exists already. Adding package\n>>>> mempool accept doesn't worsen this, but the procedure in the future is to\n>>>> replace the witness when it makes sense economically. We can also add logic\n>>>> to allow package feerate to pay for witness replacements as well. This is\n>>>> pretty far into the future, though.\n>>>>\n>>>> > It sounds uneconomical for an attacker but I think it's not when you\n>>>> consider than you can \"batch\" attack against multiple honest\n>>>> counterparties. E.g, Mallory broadcast A' + B' + C' + D' where A' conflicts\n>>>> with Alice's honest package P1, B' conflicts with Bob's honest package P2,\n>>>> C' conflicts with Caroll's honest package P3. And D' is a high-fee child of\n>>>> A' + B' + C'.\n>>>>\n>>>> > If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of\n>>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>>>>\n>>>> I could be misunderstanding, but an attacker wouldn't be able to\n>>>> batch-attack like this. Alice's package only conflicts with A' + D', not A'\n>>>> + B' + C' + D'. She only needs to pay for evicting 2 transactions.\n>>>>\n>>>> > Do we assume that broadcasted packages are \"honest\" by default and\n>>>> that the parent(s) always need the child to pass the fee checks, that way\n>>>> saving the processing of individual transactions which are expected to fail\n>>>> in 99% of cases or more ad hoc composition of packages at relay ?\n>>>> > I think this point is quite dependent on the p2p packages\n>>>> format/logic we'll end up on and that we should feel free to revisit it\n>>>> later ?\n>>>>\n>>>> I think it's the opposite; there's no way for us to assume that p2p\n>>>> packages will be \"honest.\" I'd like to have two things before we expose on\n>>>> P2P: (1) ensure that the amount of resources potentially allocated for\n>>>> package validation isn't disproportionately higher than that of single\n>>>> transaction validation and (2) only use package validation when we're\n>>>> unsatisifed with the single validation result, e.g. we might get better\n>>>> fees.\n>>>> Yes, let's revisit this later :)\n>>>>\n>>>>  > Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n>>>> discard its feerate as B should pay for all fees checked on its own. Where\n>>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n>>>> have a fee high enough to cover the bandwidth penalty replacement\n>>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>>>>\n>>>>  B' only needs to pay for itself in this case.\n>>>>\n>>>> > > Do we want the child to be able to replace mempool transactions as\n>>>> well?\n>>>>\n>>>> > If we mean when you have replaceable A+B then A'+B' try to replace\n>>>> with a higher-feerate ? I think that's exactly the case we need for\n>>>> Lightning as A+B is coming from Alice and A'+B' is coming from Bob :/\n>>>>\n>>>> Let me clarify this because I can see that my wording was ambiguous,\n>>>> and then please let me know if it fits Lightning's needs?\n>>>>\n>>>> In my proposal, I wrote \"If a package meets feerate requirements as a\n>>>> package, the parents in the transaction are allowed to replace-by-fee\n>>>> mempool transactions. The child cannot replace mempool transactions.\" What\n>>>> I meant was: the package can replace mempool transactions if any of the\n>>>> parents conflict with mempool transactions. The child cannot not conflict\n>>>> with any mempool transactions.\n>>>> The Lightning use case this attempts to address is: Alice and Mallory\n>>>> are LN counterparties, and have packages A+B and A'+B', respectively. A and\n>>>> A' are their commitment transactions and conflict with each other; they\n>>>> have shared inputs and different txids.\n>>>> B spends Alice's anchor output from A. B' spends Mallory's anchor\n>>>> output from A'. Thus, B and B' do not conflict with each other.\n>>>> Alice can broadcast her package, A+B, to replace Mallory's package,\n>>>> A'+B', since B doesn't conflict with the mempool.\n>>>>\n>>>> Would this be ok?\n>>>>\n>>>> > The second option, a child of A', In the LN case I think the CPFP is\n>>>> attached on one's anchor output.\n>>>>\n>>>> While it would be nice to have full RBF, malleability of the child\n>>>> won't block RBF here. If we're trying to replace A', we only require that\n>>>> A' signals replaceability, and don't mind if its child doesn't.\n>>>>\n>>>> > > B has an ancestor score of 10sat/vb and D has an\n>>>> > > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower\n>>>> than B's,\n>>>> > > it fails the proposed package RBF Rule #2, so this package would be\n>>>> > > rejected. Does this meet your expectations?\n>>>>\n>>>> > Well what sounds odd to me, in my example, we fail D even if it has a\n>>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n>>>> fees are 4500 sats ?\n>>>>\n>>>> Yes, A+C+D pays 2500sat more in fees, but it is also 1000vB larger. A\n>>>> miner should prefer to utilize their block space more effectively.\n>>>>\n>>>> > Is this compatible with a model where a miner prioritizes absolute\n>>>> fees over ancestor score, in the case that mempools aren't full-enough to\n>>>> fulfill a block ?\n>>>>\n>>>> No, because we don't use that model.\n>>>>\n>>>> Thanks,\n>>>> Gloria\n>>>>\n>>>> On Thu, Sep 23, 2021 at 5:29 AM Antoine Riard <antoine.riard at gmail.com>\n>>>> wrote:\n>>>>\n>>>>> > Correct, if B+C is too low feerate to be accepted, we will reject\n>>>>> it. I\n>>>>> > prefer this because it is incentive compatible: A can be mined by\n>>>>> itself,\n>>>>> > so there's no reason to prefer A+B+C instead of A.\n>>>>> > As another way of looking at this, consider the case where we do\n>>>>> accept\n>>>>> > A+B+C and it sits at the \"bottom\" of our mempool. If our mempool\n>>>>> reaches\n>>>>> > capacity, we evict the lowest descendant feerate transactions, which\n>>>>> are\n>>>>> > B+C in this case. This gives us the same resulting mempool, with A\n>>>>> and not\n>>>>> > B+C.\n>>>>>\n>>>>> I agree here. Doing otherwise, we might evict other transactions\n>>>>> mempool in `MempoolAccept::Finalize` with a higher-feerate than B+C while\n>>>>> those evicted transactions are the most compelling for block construction.\n>>>>>\n>>>>> I thought at first missing this acceptance requirement would break a\n>>>>> fee-bumping scheme like Parent-Pay-For-Child where a high-fee parent is\n>>>>> attached to a child signed with SIGHASH_ANYONECANPAY but in this case the\n>>>>> child fee is capturing the parent value. I can't think of other fee-bumping\n>>>>> schemes potentially affected. If they do exist I would say they're wrong in\n>>>>> their design assumptions.\n>>>>>\n>>>>> > If or when we have witness replacement, the logic is: if the\n>>>>> individual\n>>>>> > transaction is enough to replace the mempool one, the replacement\n>>>>> will\n>>>>> > happen during the preceding individual transaction acceptance, and\n>>>>> > deduplication logic will work. Otherwise, we will try to deduplicate\n>>>>> by\n>>>>> > wtxid, see that we need a package witness replacement, and use the\n>>>>> package\n>>>>> > feerate to evaluate whether this is economically rational.\n>>>>>\n>>>>> IIUC, you have package A+B, during the dedup phase early in\n>>>>> `AcceptMultipleTransactions` if you observe same-txid-different-wtixd A'\n>>>>> and A' is higher feerate than A, you trim A and replace by A' ?\n>>>>>\n>>>>> I think this approach is safe, the one who appears unsafe to me is\n>>>>> when A' has a _lower_ feerate, even if A' is already accepted by our\n>>>>> mempool ? In that case iirc that would be a pinning.\n>>>>>\n>>>>> Good to see progress on witness replacement before we see usage of\n>>>>> Taproot tree in the context of multi-party, where a malicious counterparty\n>>>>> inflates its witness to jam a honest spending.\n>>>>>\n>>>>> (Note, the commit linked currently points nowhere :))\n>>>>>\n>>>>>\n>>>>> > Please note that A may replace A' even if A' has higher fees than A\n>>>>> > individually, because the proposed package RBF utilizes the fees and\n>>>>> size\n>>>>> > of the entire package. This just requires E to pay enough fees,\n>>>>> although\n>>>>> > this can be pretty high if there are also potential B' and C'\n>>>>> competing\n>>>>> > commitment transactions that we don't know about.\n>>>>>\n>>>>> Ah right, if the package acceptance waives `PaysMoreThanConflicts` for\n>>>>> the individual check on A, the honest package should replace the pinning\n>>>>> attempt. I've not fully parsed the proposed implementation yet.\n>>>>>\n>>>>> Though note, I think it's still unsafe for a Lightning\n>>>>> multi-commitment-broadcast-as-one-package as a malicious A' might have an\n>>>>> absolute fee higher than E. It sounds uneconomical for\n>>>>> an attacker but I think it's not when you consider than you can\n>>>>> \"batch\" attack against multiple honest counterparties. E.g, Mallory\n>>>>> broadcast A' + B' + C' + D' where A' conflicts with Alice's honest package\n>>>>> P1, B' conflicts with Bob's honest package P2, C' conflicts with Caroll's\n>>>>> honest package P3. And D' is a high-fee child of A' + B' + C'.\n>>>>>\n>>>>> If D' is higher-fee than P1 or P2 or P3 but inferior to the sum of\n>>>>> HTLCs confirmed by P1+P2+P3, I think it's lucrative for the attacker ?\n>>>>>\n>>>>> > So far, my understanding is that multi-parent-1-child is desired for\n>>>>> > batched fee-bumping (\n>>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>>>>> and\n>>>>> > I've also seen your response which I have less context on (\n>>>>> > https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>>>>> That\n>>>>> > being said, I am happy to create a new proposal for 1 parent + 1\n>>>>> child\n>>>>> > (which would be slightly simpler) and plan for moving to\n>>>>> > multi-parent-1-child later if that is preferred. I am very\n>>>>> interested in\n>>>>> > hearing feedback on that approach.\n>>>>>\n>>>>> I think batched fee-bumping is okay as long as you don't have\n>>>>> time-sensitive outputs encumbering your commitment transactions. For the\n>>>>> reasons mentioned above, I think that's unsafe.\n>>>>>\n>>>>> What I'm worried about is  L2 developers, potentially not aware about\n>>>>> all the mempool subtleties blurring the difference and always batching\n>>>>> their broadcast by default.\n>>>>>\n>>>>> IMO, a good thing by restraining to 1-parent + 1 child,  we\n>>>>> artificially constraint L2 design space for now and minimize risks of\n>>>>> unsafe usage of the package API :)\n>>>>>\n>>>>> I think that's a point where it would be relevant to have the opinion\n>>>>> of more L2 devs.\n>>>>>\n>>>>> > I think there is a misunderstanding here - let me describe what I'm\n>>>>> > proposing we'd do in this situation: we'll try individual submission\n>>>>> for A,\n>>>>> > see that it fails due to \"insufficient fees.\" Then, we'll try package\n>>>>> > validation for A+B and use package RBF. If A+B pays enough, it can\n>>>>> still\n>>>>> > replace A'. If A fails for a bad signature, we won't look at B or\n>>>>> A+B. Does\n>>>>> > this meet your expectations?\n>>>>>\n>>>>> Yes there was a misunderstanding, I think this approach is correct,\n>>>>> it's more a question of performance. Do we assume that broadcasted packages\n>>>>> are \"honest\" by default and that the parent(s) always need the child to\n>>>>> pass the fee checks, that way saving the processing of individual\n>>>>> transactions which are expected to fail in 99% of cases or more ad hoc\n>>>>> composition of packages at relay ?\n>>>>>\n>>>>> I think this point is quite dependent on the p2p packages format/logic\n>>>>> we'll end up on and that we should feel free to revisit it later ?\n>>>>>\n>>>>>\n>>>>> > What problem are you trying to solve by the package feerate *after*\n>>>>> dedup\n>>>>> rule ?\n>>>>> > My understanding is that an in-package transaction might be already\n>>>>> in\n>>>>> the mempool. Therefore, to compute a correct RBF penalty replacement,\n>>>>> the\n>>>>> vsize of this transaction could be discarded lowering the cost of\n>>>>> package\n>>>>> RBF.\n>>>>>\n>>>>> > I'm proposing that, when a transaction has already been submitted to\n>>>>> > mempool, we would ignore both its fees and vsize when calculating\n>>>>> package\n>>>>> > feerate.\n>>>>>\n>>>>> Yes, if you receive A+B, and A is already in-mempoo, I agree you can\n>>>>> discard its feerate as B should pay for all fees checked on its own. Where\n>>>>> I'm unclear is when you have in-mempool A+B and receive A+B'. Should B'\n>>>>> have a fee high enough to cover the bandwidth penalty replacement\n>>>>> (`PaysForRBF`, 2nd check) of both A+B' or only B' ?\n>>>>>\n>>>>> If you have a second-layer like current Lightning, you might have a\n>>>>> counterparty commitment to replace and should always expect to have to pay\n>>>>> for parent replacement bandwidth.\n>>>>>\n>>>>> Where a potential discount sounds interesting is when you have an\n>>>>> univoque state on the first-stage of transactions. E.g DLC's funding\n>>>>> transaction which might be CPFP by any participant iirc.\n>>>>>\n>>>>> > Note that, if C' conflicts with C, it also conflicts with D, since D\n>>>>> is a\n>>>>> > descendant of C and would thus need to be evicted along with it.\n>>>>>\n>>>>> Ah once again I think it's a misunderstanding without the code under\n>>>>> my eyes! If we do C' `PreChecks`, solve the conflicts provoked by it, i.e\n>>>>> mark for potential eviction D and don't consider it for future conflicts in\n>>>>> the rest of the package, I think D' `PreChecks` should be good ?\n>>>>>\n>>>>> > More generally, this example is surprising to me because I didn't\n>>>>> think\n>>>>> > packages would be used to fee-bump replaceable transactions. Do we\n>>>>> want the\n>>>>> > child to be able to replace mempool transactions as well?\n>>>>>\n>>>>> If we mean when you have replaceable A+B then A'+B' try to replace\n>>>>> with a higher-feerate ? I think that's exactly the case we need for\n>>>>> Lightning as A+B is coming from Alice and A'+B' is coming from Bob :/\n>>>>>\n>>>>> > I'm not sure what you mean? Let's say we have a package of parent A\n>>>>> + child\n>>>>> > B, where A is supposed to replace a mempool transaction A'. Are you\n>>>>> saying\n>>>>> > that counterparties are able to malleate the package child B, or a\n>>>>> child of\n>>>>> > A'?\n>>>>>\n>>>>> The second option, a child of A', In the LN case I think the CPFP is\n>>>>> attached on one's anchor output.\n>>>>>\n>>>>> I think it's good if we assume the\n>>>>> solve-conflicts-after-parent's`'PreChecks` mentioned above or fixing\n>>>>> inherited signaling or full-rbf ?\n>>>>>\n>>>>> > Sorry, I don't understand what you mean by \"preserve the package\n>>>>> > integrity?\" Could you elaborate?\n>>>>>\n>>>>> After thinking the relaxation about the \"new\" unconfirmed input is not\n>>>>> linked to trimming but I would say more to the multi-parent support.\n>>>>>\n>>>>> Let's say you have A+B trying to replace C+D where B is also spending\n>>>>> already in-mempool E. To succeed, you need to waive the no-new-unconfirmed\n>>>>> input as D isn't spending E.\n>>>>>\n>>>>> So good, I think we agree on the problem description here.\n>>>>>\n>>>>> > I am in agreement with your calculations but unsure if we disagree\n>>>>> on the\n>>>>> > expected outcome. Yes, B has an ancestor score of 10sat/vb and D has\n>>>>> an\n>>>>> > ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than\n>>>>> B's,\n>>>>> > it fails the proposed package RBF Rule #2, so this package would be\n>>>>> > rejected. Does this meet your expectations?\n>>>>>\n>>>>> Well what sounds odd to me, in my example, we fail D even if it has a\n>>>>> higher-fee than B. Like A+B absolute fees are 2000 sats and A+C+D absolute\n>>>>> fees are 4500 sats ?\n>>>>>\n>>>>> Is this compatible with a model where a miner prioritizes absolute\n>>>>> fees over ancestor score, in the case that mempools aren't full-enough to\n>>>>> fulfill a block ?\n>>>>>\n>>>>> Let me know if I can clarify a point.\n>>>>>\n>>>>> Antoine\n>>>>>\n>>>>> Le lun. 20 sept. 2021 \u00e0 11:10, Gloria Zhao <gloriajzhao at gmail.com> a\n>>>>> \u00e9crit :\n>>>>>\n>>>>>>\n>>>>>> Hi Antoine,\n>>>>>>\n>>>>>> First of all, thank you for the thorough review. I appreciate your\n>>>>>> insight on LN requirements.\n>>>>>>\n>>>>>> > IIUC, you have a package A+B+C submitted for acceptance and A is\n>>>>>> already in your mempool. You trim out A from the package and then evaluate\n>>>>>> B+C.\n>>>>>>\n>>>>>> > I think this might be an issue if A is the higher-fee element of\n>>>>>> the ABC package. B+C package fees might be under the mempool min fee and\n>>>>>> will be rejected, potentially breaking the acceptance expectations of the\n>>>>>> package issuer ?\n>>>>>>\n>>>>>> Correct, if B+C is too low feerate to be accepted, we will reject it.\n>>>>>> I prefer this because it is incentive compatible: A can be mined by itself,\n>>>>>> so there's no reason to prefer A+B+C instead of A.\n>>>>>> As another way of looking at this, consider the case where we do\n>>>>>> accept A+B+C and it sits at the \"bottom\" of our mempool. If our mempool\n>>>>>> reaches capacity, we evict the lowest descendant feerate transactions,\n>>>>>> which are B+C in this case. This gives us the same resulting mempool, with\n>>>>>> A and not B+C.\n>>>>>>\n>>>>>>\n>>>>>> > Further, I think the dedup should be done on wtxid, as you might\n>>>>>> have multiple valid witnesses. Though with varying vsizes and as such\n>>>>>> offering different feerates.\n>>>>>>\n>>>>>> I agree that variations of the same package with different witnesses\n>>>>>> is a case that must be handled. I consider witness replacement to be a\n>>>>>> project that can be done in parallel to package mempool acceptance because\n>>>>>> being able to accept packages does not worsen the problem of a\n>>>>>> same-txid-different-witness \"pinning\" attack.\n>>>>>>\n>>>>>> If or when we have witness replacement, the logic is: if the\n>>>>>> individual transaction is enough to replace the mempool one, the\n>>>>>> replacement will happen during the preceding individual transaction\n>>>>>> acceptance, and deduplication logic will work. Otherwise, we will try to\n>>>>>> deduplicate by wtxid, see that we need a package witness replacement, and\n>>>>>> use the package feerate to evaluate whether this is economically rational.\n>>>>>>\n>>>>>> See the #22290 \"handle package transactions already in mempool\"\n>>>>>> commit (\n>>>>>> https://github.com/bitcoin/bitcoin/pull/22290/commits/fea75a2237b46cf76145242fecad7e274bfcb5ff),\n>>>>>> which handles the case of same-txid-different-witness by simply using the\n>>>>>> transaction in the mempool for now, with TODOs for what I just described.\n>>>>>>\n>>>>>>\n>>>>>> > I'm not clearly understanding the accepted topologies. By \"parent\n>>>>>> and child to share a parent\", do you mean the set of transactions A, B, C,\n>>>>>> where B is spending A and C is spending A and B would be correct ?\n>>>>>>\n>>>>>> Yes, that is what I meant. Yes, that would a valid package under\n>>>>>> these rules.\n>>>>>>\n>>>>>> > If yes, is there a width-limit introduced or we fallback on\n>>>>>> MAX_PACKAGE_COUNT=25 ?\n>>>>>>\n>>>>>> No, there is no limit on connectivity other than \"child with all\n>>>>>> unconfirmed parents.\" We will enforce MAX_PACKAGE_COUNT=25 and child's\n>>>>>> in-mempool + in-package ancestor limits.\n>>>>>>\n>>>>>>\n>>>>>> > Considering the current Core's mempool acceptance rules, I think\n>>>>>> CPFP batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>>>>> jamming successful on one channel commitment transaction would contamine\n>>>>>> the remaining commitments sharing the same package.\n>>>>>>\n>>>>>> > E.g, you broadcast the package A+B+C+D+E where A,B,C,D are\n>>>>>> commitment transactions and E a shared CPFP. If a malicious A' transaction\n>>>>>> has a better feerate than A, the whole package acceptance will fail. Even\n>>>>>> if A' confirms in the following block,\n>>>>>> the propagation and confirmation of B+C+D have been delayed. This\n>>>>>> could carry on a loss of funds.\n>>>>>>\n>>>>>> Please note that A may replace A' even if A' has higher fees than A\n>>>>>> individually, because the proposed package RBF utilizes the fees and size\n>>>>>> of the entire package. This just requires E to pay enough fees, although\n>>>>>> this can be pretty high if there are also potential B' and C' competing\n>>>>>> commitment transactions that we don't know about.\n>>>>>>\n>>>>>>\n>>>>>> > IMHO, I'm leaning towards deploying during a first phase\n>>>>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>>>>> second-layer safety.\n>>>>>>\n>>>>>> So far, my understanding is that multi-parent-1-child is desired for\n>>>>>> batched fee-bumping (\n>>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-897951289)\n>>>>>> and I've also seen your response which I have less context on (\n>>>>>> https://github.com/bitcoin/bitcoin/pull/22674#issuecomment-900352202).\n>>>>>> That being said, I am happy to create a new proposal for 1 parent + 1 child\n>>>>>> (which would be slightly simpler) and plan for moving to\n>>>>>> multi-parent-1-child later if that is preferred. I am very interested in\n>>>>>> hearing feedback on that approach.\n>>>>>>\n>>>>>>\n>>>>>> > If A+B is submitted to replace A', where A pays 0 sats, B pays 200\n>>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B\n>>>>>> acceptance fails. For this reason I think the individual RBF should be\n>>>>>> bypassed and only the package RBF apply ?\n>>>>>>\n>>>>>> I think there is a misunderstanding here - let me describe what I'm\n>>>>>> proposing we'd do in this situation: we'll try individual submission for A,\n>>>>>> see that it fails due to \"insufficient fees.\" Then, we'll try package\n>>>>>> validation for A+B and use package RBF. If A+B pays enough, it can still\n>>>>>> replace A'. If A fails for a bad signature, we won't look at B or A+B. Does\n>>>>>> this meet your expectations?\n>>>>>>\n>>>>>>\n>>>>>> > What problem are you trying to solve by the package feerate *after*\n>>>>>> dedup rule ?\n>>>>>> > My understanding is that an in-package transaction might be already\n>>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,\n>>>>>> the vsize of this transaction could be discarded lowering the cost of\n>>>>>> package RBF.\n>>>>>>\n>>>>>> I'm proposing that, when a transaction has already been submitted to\n>>>>>> mempool, we would ignore both its fees and vsize when calculating package\n>>>>>> feerate. In example G2, we shouldn't count M1 fees after its submission to\n>>>>>> mempool, since M1's fees have already been used to pay for its individual\n>>>>>> bandwidth, and it shouldn't be used again to pay for P2 and P3's bandwidth.\n>>>>>> We also shouldn't count its vsize, since it has already been paid for.\n>>>>>>\n>>>>>>\n>>>>>> > I think this is a footgunish API, as if a package issuer send the\n>>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>>>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>>>>> higher-feerate/higher-fee package always replaces ?\n>>>>>>\n>>>>>> Note that, if C' conflicts with C, it also conflicts with D, since D\n>>>>>> is a descendant of C and would thus need to be evicted along with it.\n>>>>>> Implicitly, D' would not be in conflict with D.\n>>>>>> More generally, this example is surprising to me because I didn't\n>>>>>> think packages would be used to fee-bump replaceable transactions. Do we\n>>>>>> want the child to be able to replace mempool transactions as well? This can\n>>>>>> be implemented with a bit of additional logic.\n>>>>>>\n>>>>>> > I think this is unsafe for L2s if counterparties have malleability\n>>>>>> of the child transaction. They can block your package replacement by\n>>>>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>>>>> ability.\n>>>>>>\n>>>>>> I'm not sure what you mean? Let's say we have a package of parent A +\n>>>>>> child B, where A is supposed to replace a mempool transaction A'. Are you\n>>>>>> saying that counterparties are able to malleate the package child B, or a\n>>>>>> child of A'? If they can malleate a child of A', that shouldn't matter as\n>>>>>> long as A' is signaling replacement. This would be handled identically with\n>>>>>> full RBF and what Core currently implements.\n>>>>>>\n>>>>>> > I think this is an issue brought by the trimming during the dedup\n>>>>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>>>>> checks results of already in-mempool transactions to gain in CPU time we\n>>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>>>>\n>>>>>> Sorry, I don't understand what you mean by \"preserve the package\n>>>>>> integrity?\" Could you elaborate?\n>>>>>>\n>>>>>> > Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>>>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>>>>\n>>>>>> > Package A + B ancestor score is 10 sat/vb.\n>>>>>>\n>>>>>> > D has a higher feerate/absolute fee than B.\n>>>>>>\n>>>>>> > Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats +\n>>>>>> C's 1000 sats + D's 1500 sats) / A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>>>>\n>>>>>> I am in agreement with your calculations but unsure if we disagree on\n>>>>>> the expected outcome. Yes, B has an ancestor score of 10sat/vb and D has an\n>>>>>> ancestor score of ~2.9sat/vb. Since D's ancestor score is lower than B's,\n>>>>>> it fails the proposed package RBF Rule #2, so this package would be\n>>>>>> rejected. Does this meet your expectations?\n>>>>>>\n>>>>>> Thank you for linking to projects that might be interested in package\n>>>>>> relay :)\n>>>>>>\n>>>>>> Thanks,\n>>>>>> Gloria\n>>>>>>\n>>>>>> On Mon, Sep 20, 2021 at 12:16 AM Antoine Riard <\n>>>>>> antoine.riard at gmail.com> wrote:\n>>>>>>\n>>>>>>> Hi Gloria,\n>>>>>>>\n>>>>>>> > A package may contain transactions that are already in the\n>>>>>>> mempool. We\n>>>>>>> > remove\n>>>>>>> > (\"deduplicate\") those transactions from the package for the\n>>>>>>> purposes of\n>>>>>>> > package\n>>>>>>> > mempool acceptance. If a package is empty after deduplication, we\n>>>>>>> do\n>>>>>>> > nothing.\n>>>>>>>\n>>>>>>> IIUC, you have a package A+B+C submitted for acceptance and A is\n>>>>>>> already in your mempool. You trim out A from the package and then evaluate\n>>>>>>> B+C.\n>>>>>>>\n>>>>>>> I think this might be an issue if A is the higher-fee element of the\n>>>>>>> ABC package. B+C package fees might be under the mempool min fee and will\n>>>>>>> be rejected, potentially breaking the acceptance expectations of the\n>>>>>>> package issuer ?\n>>>>>>>\n>>>>>>> Further, I think the dedup should be done on wtxid, as you might\n>>>>>>> have multiple valid witnesses. Though with varying vsizes and as such\n>>>>>>> offering different feerates.\n>>>>>>>\n>>>>>>> E.g you're going to evaluate the package A+B and A' is already in\n>>>>>>> your mempool with a bigger valid witness. You trim A based on txid, then\n>>>>>>> you evaluate A'+B, which fails the fee checks. However, evaluating A+B\n>>>>>>> would have been a success.\n>>>>>>>\n>>>>>>> AFAICT, the dedup rationale would be to save on CPU time/IO disk, to\n>>>>>>> avoid repeated signatures verification and parent UTXOs fetches ? Can we\n>>>>>>> achieve the same goal by bypassing tx-level checks for already-in txn while\n>>>>>>> conserving the package integrity for package-level checks ?\n>>>>>>>\n>>>>>>> > Note that it's possible for the parents to be\n>>>>>>> > indirect\n>>>>>>> > descendants/ancestors of one another, or for parent and child to\n>>>>>>> share a\n>>>>>>> > parent,\n>>>>>>> > so we cannot make any other topology assumptions.\n>>>>>>>\n>>>>>>> I'm not clearly understanding the accepted topologies. By \"parent\n>>>>>>> and child to share a parent\", do you mean the set of transactions A, B, C,\n>>>>>>> where B is spending A and C is spending A and B would be correct ?\n>>>>>>>\n>>>>>>> If yes, is there a width-limit introduced or we fallback on\n>>>>>>> MAX_PACKAGE_COUNT=25 ?\n>>>>>>>\n>>>>>>> IIRC, one rationale to come with this topology limitation was to\n>>>>>>> lower the DoS risks when potentially deploying p2p packages.\n>>>>>>>\n>>>>>>> Considering the current Core's mempool acceptance rules, I think\n>>>>>>> CPFP batching is unsafe for LN time-sensitive closure. A malicious tx-relay\n>>>>>>> jamming successful on one channel commitment transaction would contamine\n>>>>>>> the remaining commitments sharing the same package.\n>>>>>>>\n>>>>>>> E.g, you broadcast the package A+B+C+D+E where A,B,C,D are\n>>>>>>> commitment transactions and E a shared CPFP. If a malicious A' transaction\n>>>>>>> has a better feerate than A, the whole package acceptance will fail. Even\n>>>>>>> if A' confirms in the following block,\n>>>>>>> the propagation and confirmation of B+C+D have been delayed. This\n>>>>>>> could carry on a loss of funds.\n>>>>>>>\n>>>>>>> That said, if you're broadcasting commitment transactions without\n>>>>>>> time-sensitive HTLC outputs, I think the batching is effectively a fee\n>>>>>>> saving as you don't have to duplicate the CPFP.\n>>>>>>>\n>>>>>>> IMHO, I'm leaning towards deploying during a first phase\n>>>>>>> 1-parent/1-child. I think it's the most conservative step still improving\n>>>>>>> second-layer safety.\n>>>>>>>\n>>>>>>> > *Rationale*:  It would be incorrect to use the fees of\n>>>>>>> transactions that are\n>>>>>>> > already in the mempool, as we do not want a transaction's fees to\n>>>>>>> be\n>>>>>>> > double-counted for both its individual RBF and package RBF.\n>>>>>>>\n>>>>>>> I'm unsure about the logical order of the checks proposed.\n>>>>>>>\n>>>>>>> If A+B is submitted to replace A', where A pays 0 sats, B pays 200\n>>>>>>> sats and A' pays 100 sats. If we apply the individual RBF on A, A+B\n>>>>>>> acceptance fails. For this reason I think the individual RBF should be\n>>>>>>> bypassed and only the package RBF apply ?\n>>>>>>>\n>>>>>>> Note this situation is plausible, with current LN design, your\n>>>>>>> counterparty can have a commitment transaction with a better fee just by\n>>>>>>> selecting a higher `dust_limit_satoshis` than yours.\n>>>>>>>\n>>>>>>> > Examples F and G [14] show the same package, but P1 is submitted\n>>>>>>> > individually before\n>>>>>>> > the package in example G. In example F, we can see that the 300vB\n>>>>>>> package\n>>>>>>> > pays\n>>>>>>> > an additional 200sat in fees, which is not enough to pay for its\n>>>>>>> own\n>>>>>>> > bandwidth\n>>>>>>> > (BIP125#4). In example G, we can see that P1 pays enough to\n>>>>>>> replace M1, but\n>>>>>>> > using P1's fees again during package submission would make it look\n>>>>>>> like a\n>>>>>>> > 300sat\n>>>>>>> > increase for a 200vB package. Even including its fees and size\n>>>>>>> would not be\n>>>>>>> > sufficient in this example, since the 300sat looks like enough for\n>>>>>>> the 300vB\n>>>>>>> > package. The calculcation after deduplication is 100sat increase\n>>>>>>> for a\n>>>>>>> > package\n>>>>>>> > of size 200vB, which correctly fails BIP125#4. Assume all\n>>>>>>> transactions have\n>>>>>>> > a\n>>>>>>> > size of 100vB.\n>>>>>>>\n>>>>>>> What problem are you trying to solve by the package feerate *after*\n>>>>>>> dedup rule ?\n>>>>>>>\n>>>>>>> My understanding is that an in-package transaction might be already\n>>>>>>> in the mempool. Therefore, to compute a correct RBF penalty replacement,\n>>>>>>> the vsize of this transaction could be discarded lowering the cost of\n>>>>>>> package RBF.\n>>>>>>>\n>>>>>>> If we keep a \"safe\" dedup mechanism (see my point above), I think\n>>>>>>> this discount is justified, as the validation cost of node operators is\n>>>>>>> paid for ?\n>>>>>>>\n>>>>>>> > The child cannot replace mempool transactions.\n>>>>>>>\n>>>>>>> Let's say you issue package A+B, then package C+B', where B' is a\n>>>>>>> child of both A and C. This rule fails the acceptance of C+B' ?\n>>>>>>>\n>>>>>>> I think this is a footgunish API, as if a package issuer send the\n>>>>>>> multiple-parent-one-child package A,B,C,D where D is the child of A,B,C.\n>>>>>>> Then try to broadcast the higher-feerate C'+D' package, it should be\n>>>>>>> rejected. So it's breaking the naive broadcaster assumption that a\n>>>>>>> higher-feerate/higher-fee package always replaces ? And it might be unsafe\n>>>>>>> in protocols where states are symmetric. E.g a malicious counterparty\n>>>>>>> broadcasts first S+A, then you honestly broadcast S+B, where B pays better\n>>>>>>> fees.\n>>>>>>>\n>>>>>>> > All mempool transactions to be replaced must signal replaceability.\n>>>>>>>\n>>>>>>> I think this is unsafe for L2s if counterparties have malleability\n>>>>>>> of the child transaction. They can block your package replacement by\n>>>>>>> opting-out from RBF signaling. IIRC, LN's \"anchor output\" presents such an\n>>>>>>> ability.\n>>>>>>>\n>>>>>>> I think it's better to either fix inherited signaling or move\n>>>>>>> towards full-rbf.\n>>>>>>>\n>>>>>>> > if a package parent has already been submitted, it would\n>>>>>>> > look\n>>>>>>> >like the child is spending a \"new\" unconfirmed input.\n>>>>>>>\n>>>>>>> I think this is an issue brought by the trimming during the dedup\n>>>>>>> phase. If we preserve the package integrity, only re-using the tx-level\n>>>>>>> checks results of already in-mempool transactions to gain in CPU time we\n>>>>>>> won't have this issue. Package childs can add unconfirmed inputs as long as\n>>>>>>> they're in-package, the bip125 rule2 is only evaluated against parents ?\n>>>>>>>\n>>>>>>> > However, we still achieve the same goal of requiring the\n>>>>>>> > replacement\n>>>>>>> > transactions to have a ancestor score at least as high as the\n>>>>>>> original\n>>>>>>> > ones.\n>>>>>>>\n>>>>>>> I'm not sure if this holds...\n>>>>>>>\n>>>>>>> Let's say you have in-mempool A, B where A pays 10 sat/vb for 100\n>>>>>>> vbytes and B pays 10 sat/vb for 100 vbytes. You have the candidate\n>>>>>>> replacement D spending both A and C where D pays 15sat/vb for 100 vbytes\n>>>>>>> and C pays 1 sat/vb for 1000 vbytes.\n>>>>>>>\n>>>>>>> Package A + B ancestor score is 10 sat/vb.\n>>>>>>>\n>>>>>>> D has a higher feerate/absolute fee than B.\n>>>>>>>\n>>>>>>> Package A + C + D ancestor score is ~ 3 sat/vb ((A's 1000 sats + C's\n>>>>>>> 1000 sats + D's 1500 sats) /\n>>>>>>> A's 100 vb + C's 1000 vb + D's 100 vb)\n>>>>>>>\n>>>>>>> Overall, this is a review through the lenses of LN requirements. I\n>>>>>>> think other L2 protocols/applications\n>>>>>>> could be candidates to using package accept/relay such as:\n>>>>>>> * https://github.com/lightninglabs/pool\n>>>>>>> * https://github.com/discreetlogcontracts/dlcspecs\n>>>>>>> * https://github.com/bitcoin-teleport/teleport-transactions/\n>>>>>>> * https://github.com/sapio-lang/sapio\n>>>>>>> *\n>>>>>>> https://github.com/commerceblock/mercury/blob/master/doc/statechains.md\n>>>>>>> * https://github.com/revault/practical-revault\n>>>>>>>\n>>>>>>> Thanks for rolling forward the ball on this subject.\n>>>>>>>\n>>>>>>> Antoine\n>>>>>>>\n>>>>>>> Le jeu. 16 sept. 2021 \u00e0 03:55, Gloria Zhao via bitcoin-dev <\n>>>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>>>\n>>>>>>>> Hi there,\n>>>>>>>>\n>>>>>>>> I'm writing to propose a set of mempool policy changes to enable\n>>>>>>>> package\n>>>>>>>> validation (in preparation for package relay) in Bitcoin Core.\n>>>>>>>> These would not\n>>>>>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>>>>>> significantly affects transaction propagation, I believe this is\n>>>>>>>> relevant for\n>>>>>>>> the mailing list.\n>>>>>>>>\n>>>>>>>> My proposal enables packages consisting of multiple parents and 1\n>>>>>>>> child. If you\n>>>>>>>> develop software that relies on specific transaction relay\n>>>>>>>> assumptions and/or\n>>>>>>>> are interested in using package relay in the future, I'm very\n>>>>>>>> interested to hear\n>>>>>>>> your feedback on the utility or restrictiveness of these package\n>>>>>>>> policies for\n>>>>>>>> your use cases.\n>>>>>>>>\n>>>>>>>> A draft implementation of this proposal can be found in [Bitcoin\n>>>>>>>> Core\n>>>>>>>> PR#22290][1].\n>>>>>>>>\n>>>>>>>> An illustrated version of this post can be found at\n>>>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>>>>>> I have also linked the images below.\n>>>>>>>>\n>>>>>>>> ## Background\n>>>>>>>>\n>>>>>>>> Feel free to skip this section if you are already familiar with\n>>>>>>>> mempool policy\n>>>>>>>> and package relay terminology.\n>>>>>>>>\n>>>>>>>> ### Terminology Clarifications\n>>>>>>>>\n>>>>>>>> * Package = an ordered list of related transactions, representable\n>>>>>>>> by a Directed\n>>>>>>>>   Acyclic Graph.\n>>>>>>>> * Package Feerate = the total modified fees divided by the total\n>>>>>>>> virtual size of\n>>>>>>>>   all transactions in the package.\n>>>>>>>>     - Modified fees = a transaction's base fees + fee delta applied\n>>>>>>>> by the user\n>>>>>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>>>>>> across\n>>>>>>>> mempools.\n>>>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>>>>>> [BIP141\n>>>>>>>>       virtual size][2] and sigop weight. [Implemented here in\n>>>>>>>> Bitcoin Core][3].\n>>>>>>>>     - Note that feerate is not necessarily based on the base fees\n>>>>>>>> and serialized\n>>>>>>>>       size.\n>>>>>>>>\n>>>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>>>>>> incentives to\n>>>>>>>>   boost a transaction's candidacy for inclusion in a block,\n>>>>>>>> including Child Pays\n>>>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our\n>>>>>>>> intention in\n>>>>>>>> mempool policy is to recognize when the new transaction is more\n>>>>>>>> economical to\n>>>>>>>> mine than the original one(s) but not open DoS vectors, so there\n>>>>>>>> are some\n>>>>>>>> limitations.\n>>>>>>>>\n>>>>>>>> ### Policy\n>>>>>>>>\n>>>>>>>> The purpose of the mempool is to store the best (to be most\n>>>>>>>> incentive-compatible\n>>>>>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>>>>>> Miners use\n>>>>>>>> the mempool to build block templates. The mempool is also useful as\n>>>>>>>> a cache for\n>>>>>>>> boosting block relay and validation performance, aiding transaction\n>>>>>>>> relay, and\n>>>>>>>> generating feerate estimations.\n>>>>>>>>\n>>>>>>>> Ideally, all consensus-valid transactions paying reasonable fees\n>>>>>>>> should make it\n>>>>>>>> to miners through normal transaction relay, without any special\n>>>>>>>> connectivity or\n>>>>>>>> relationships with miners. On the other hand, nodes do not have\n>>>>>>>> unlimited\n>>>>>>>> resources, and a P2P network designed to let any honest node\n>>>>>>>> broadcast their\n>>>>>>>> transactions also exposes the transaction validation engine to DoS\n>>>>>>>> attacks from\n>>>>>>>> malicious peers.\n>>>>>>>>\n>>>>>>>> As such, for unconfirmed transactions we are considering for our\n>>>>>>>> mempool, we\n>>>>>>>> apply a set of validation rules in addition to consensus, primarily\n>>>>>>>> to protect\n>>>>>>>> us from resource exhaustion and aid our efforts to keep the highest\n>>>>>>>> fee\n>>>>>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>>>>>> node-specific) rules that transactions must abide by in order to be\n>>>>>>>> accepted\n>>>>>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>>>>>> restrictions such\n>>>>>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>>>>>\n>>>>>>>> ### Package Relay and Package Mempool Accept\n>>>>>>>>\n>>>>>>>> In transaction relay, we currently consider transactions one at a\n>>>>>>>> time for\n>>>>>>>> submission to the mempool. This creates a limitation in the node's\n>>>>>>>> ability to\n>>>>>>>> determine which transactions have the highest feerates, since we\n>>>>>>>> cannot take\n>>>>>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>>>>>> transactions are\n>>>>>>>> in the mempool. Similarly, we cannot use a transaction's\n>>>>>>>> descendants when\n>>>>>>>> considering it for RBF. When an individual transaction does not\n>>>>>>>> meet the mempool\n>>>>>>>> minimum feerate and the user isn't able to create a replacement\n>>>>>>>> transaction\n>>>>>>>> directly, it will not be accepted by mempools.\n>>>>>>>>\n>>>>>>>> This limitation presents a security issue for applications and\n>>>>>>>> users relying on\n>>>>>>>> time-sensitive transactions. For example, Lightning and other\n>>>>>>>> protocols create\n>>>>>>>> UTXOs with multiple spending paths, where one counterparty's\n>>>>>>>> spending path opens\n>>>>>>>> up after a timelock, and users are protected from cheating\n>>>>>>>> scenarios as long as\n>>>>>>>> they redeem on-chain in time. A key security assumption is that all\n>>>>>>>> parties'\n>>>>>>>> transactions will propagate and confirm in a timely manner. This\n>>>>>>>> assumption can\n>>>>>>>> be broken if fee-bumping does not work as intended.\n>>>>>>>>\n>>>>>>>> The end goal for Package Relay is to consider multiple transactions\n>>>>>>>> at the same\n>>>>>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>>>>>> better\n>>>>>>>> determine whether transactions should be accepted to our mempool,\n>>>>>>>> especially if\n>>>>>>>> they don't meet fee requirements individually or are better RBF\n>>>>>>>> candidates as a\n>>>>>>>> package. A combination of changes to mempool validation logic,\n>>>>>>>> policy, and\n>>>>>>>> transaction relay allows us to better propagate the transactions\n>>>>>>>> with the\n>>>>>>>> highest package feerates to miners, and makes fee-bumping tools\n>>>>>>>> more powerful\n>>>>>>>> for users.\n>>>>>>>>\n>>>>>>>> The \"relay\" part of Package Relay suggests P2P messaging changes,\n>>>>>>>> but a large\n>>>>>>>> part of the changes are in the mempool's package validation logic.\n>>>>>>>> We call this\n>>>>>>>> *Package Mempool Accept*.\n>>>>>>>>\n>>>>>>>> ### Previous Work\n>>>>>>>>\n>>>>>>>> * Given that mempool validation is DoS-sensitive and complex, it\n>>>>>>>> would be\n>>>>>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>>>>>> efforts have\n>>>>>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>>>>>> [#21062][5],\n>>>>>>>> [#22675][6], [#22796][7]).\n>>>>>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>>>>>> accepts only\n>>>>>>>>   (no submission to mempool).\n>>>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks\n>>>>>>>> for arbitrary\n>>>>>>>>   packages. Still test accepts only.\n>>>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>>>>>\n>>>>>>>> ### Existing Package Rules\n>>>>>>>>\n>>>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].\n>>>>>>>> I'll consider\n>>>>>>>> them as \"given\" in the rest of this document, though they can be\n>>>>>>>> changed, since\n>>>>>>>> package validation is test-accept only right now.\n>>>>>>>>\n>>>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>>>>>\n>>>>>>>>    *Rationale*: This is already enforced as mempool\n>>>>>>>> ancestor/descendant limits.\n>>>>>>>> Presumably, transactions in a package are all related, so exceeding\n>>>>>>>> this limit\n>>>>>>>> would mean that the package can either be split up or it wouldn't\n>>>>>>>> pass this\n>>>>>>>> mempool policy.\n>>>>>>>>\n>>>>>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>>>>>> between\n>>>>>>>> transactions, parents must appear somewhere before children. [8]\n>>>>>>>>\n>>>>>>>> 3. A package cannot have conflicting transactions, i.e. none of\n>>>>>>>> them can spend\n>>>>>>>> the same inputs. This also means there cannot be duplicate\n>>>>>>>> transactions. [8]\n>>>>>>>>\n>>>>>>>> 4. When packages are evaluated against ancestor/descendant limits\n>>>>>>>> in a test\n>>>>>>>> accept, the union of all of their descendants and ancestors is\n>>>>>>>> considered. This\n>>>>>>>> is essentially a \"worst case\" heuristic where every transaction in\n>>>>>>>> the package\n>>>>>>>> is treated as each other's ancestor and descendant. [8]\n>>>>>>>> Packages for which ancestor/descendant limits are accurately\n>>>>>>>> captured by this\n>>>>>>>> heuristic: [19]\n>>>>>>>>\n>>>>>>>> There are also limitations such as the fact that CPFP carve out is\n>>>>>>>> not applied\n>>>>>>>> to package transactions. #20833 also disables RBF in package\n>>>>>>>> validation; this\n>>>>>>>> proposal overrides that to allow packages to use RBF.\n>>>>>>>>\n>>>>>>>> ## Proposed Changes\n>>>>>>>>\n>>>>>>>> The next step in the Package Mempool Accept project is to implement\n>>>>>>>> submission\n>>>>>>>> to mempool, initially through RPC only. This allows us to test the\n>>>>>>>> submission\n>>>>>>>> logic before exposing it on P2P.\n>>>>>>>>\n>>>>>>>> ### Summary\n>>>>>>>>\n>>>>>>>> - Packages may contain already-in-mempool transactions.\n>>>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>>>>>> - Fee-related checks use the package feerate. This means that\n>>>>>>>> wallets can\n>>>>>>>> create a package that utilizes CPFP.\n>>>>>>>> - Parents are allowed to RBF mempool transactions with a set of\n>>>>>>>> rules similar\n>>>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>>>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>>>>>\n>>>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but\n>>>>>>>> feedback is\n>>>>>>>> always welcome.\n>>>>>>>>\n>>>>>>>> ### Details\n>>>>>>>>\n>>>>>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>>>>>\n>>>>>>>> A package may contain transactions that are already in the mempool.\n>>>>>>>> We remove\n>>>>>>>> (\"deduplicate\") those transactions from the package for the\n>>>>>>>> purposes of package\n>>>>>>>> mempool acceptance. If a package is empty after deduplication, we\n>>>>>>>> do nothing.\n>>>>>>>>\n>>>>>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>>>>>> parent to be\n>>>>>>>> accepted to the mempool of a peer on its own due to differences in\n>>>>>>>> policy and\n>>>>>>>> fee market fluctuations. We should not reject or penalize the\n>>>>>>>> entire package for\n>>>>>>>> an individual transaction as that could be a censorship vector.\n>>>>>>>>\n>>>>>>>> #### Packages Are Multi-Parent-1-Child\n>>>>>>>>\n>>>>>>>> Only packages of a specific topology are permitted. Namely, a\n>>>>>>>> package is exactly\n>>>>>>>> 1 child with all of its unconfirmed parents. After deduplication,\n>>>>>>>> the package\n>>>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of\n>>>>>>>> its\n>>>>>>>> unconfirmed parents, etc. Note that it's possible for the parents\n>>>>>>>> to be indirect\n>>>>>>>> descendants/ancestors of one another, or for parent and child to\n>>>>>>>> share a parent,\n>>>>>>>> so we cannot make any other topology assumptions.\n>>>>>>>>\n>>>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>>>>>> parents\n>>>>>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>>>>>> packages to a\n>>>>>>>> defined topology is also easier to reason about and simplifies the\n>>>>>>>> validation\n>>>>>>>> logic greatly. Multi-parent-1-child allows us to think of the\n>>>>>>>> package as one big\n>>>>>>>> transaction, where:\n>>>>>>>>\n>>>>>>>> - Inputs = all the inputs of parents + inputs of the child that\n>>>>>>>> come from\n>>>>>>>>   confirmed UTXOs\n>>>>>>>> - Outputs = all the outputs of the child + all outputs of the\n>>>>>>>> parents that\n>>>>>>>>   aren't spent by other transactions in the package\n>>>>>>>>\n>>>>>>>> Examples of packages that follow this rule (variations of example A\n>>>>>>>> show some\n>>>>>>>> possibilities after deduplication): ![image][15]\n>>>>>>>>\n>>>>>>>> #### Fee-Related Checks Use Package Feerate\n>>>>>>>>\n>>>>>>>> Package Feerate = the total modified fees divided by the total\n>>>>>>>> virtual size of\n>>>>>>>> all transactions in the package.\n>>>>>>>>\n>>>>>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>>>>>> pre-configured\n>>>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>>>>>> feerate, the\n>>>>>>>> total package feerate is used instead of the individual feerate.\n>>>>>>>> The individual\n>>>>>>>> transactions are allowed to be below feerate requirements if the\n>>>>>>>> package meets\n>>>>>>>> the feerate requirements. For example, the parent(s) in the package\n>>>>>>>> can have 0\n>>>>>>>> fees but be paid for by the child.\n>>>>>>>>\n>>>>>>>> *Rationale*: This can be thought of as \"CPFP within a package,\"\n>>>>>>>> solving the\n>>>>>>>> issue of a parent not meeting minimum fees on its own. This allows\n>>>>>>>> L2\n>>>>>>>> applications to adjust their fees at broadcast time instead of\n>>>>>>>> overshooting or\n>>>>>>>> risking getting stuck/pinned.\n>>>>>>>>\n>>>>>>>> We use the package feerate of the package *after deduplication*.\n>>>>>>>>\n>>>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>>>>> that are\n>>>>>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>>>>>> double-counted for both its individual RBF and package RBF.\n>>>>>>>>\n>>>>>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>>>>>> individually before\n>>>>>>>> the package in example G. In example F, we can see that the 300vB\n>>>>>>>> package pays\n>>>>>>>> an additional 200sat in fees, which is not enough to pay for its\n>>>>>>>> own bandwidth\n>>>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>>>>> M1, but\n>>>>>>>> using P1's fees again during package submission would make it look\n>>>>>>>> like a 300sat\n>>>>>>>> increase for a 200vB package. Even including its fees and size\n>>>>>>>> would not be\n>>>>>>>> sufficient in this example, since the 300sat looks like enough for\n>>>>>>>> the 300vB\n>>>>>>>> package. The calculcation after deduplication is 100sat increase\n>>>>>>>> for a package\n>>>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all\n>>>>>>>> transactions have a\n>>>>>>>> size of 100vB.\n>>>>>>>>\n>>>>>>>> #### Package RBF\n>>>>>>>>\n>>>>>>>> If a package meets feerate requirements as a package, the parents\n>>>>>>>> in the\n>>>>>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>>>>>> child cannot\n>>>>>>>> replace mempool transactions. Multiple transactions can replace the\n>>>>>>>> same\n>>>>>>>> transaction, but in order to be valid, none of the transactions can\n>>>>>>>> try to\n>>>>>>>> replace an ancestor of another transaction in the same package\n>>>>>>>> (which would thus\n>>>>>>>> make its inputs unavailable).\n>>>>>>>>\n>>>>>>>> *Rationale*: Even if we are using package feerate, a package will\n>>>>>>>> not propagate\n>>>>>>>> as intended if RBF still requires each individual transaction to\n>>>>>>>> meet the\n>>>>>>>> feerate requirements.\n>>>>>>>>\n>>>>>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>>>>>\n>>>>>>>> ##### Signaling (Rule #1)\n>>>>>>>>\n>>>>>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>>>>>\n>>>>>>>> *Rationale*: Package RBF signaling logic should be the same for\n>>>>>>>> package RBF and\n>>>>>>>> single transaction acceptance. This would be updated if single\n>>>>>>>> transaction\n>>>>>>>> validation moves to full RBF.\n>>>>>>>>\n>>>>>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>>>>>\n>>>>>>>> A package may include new unconfirmed inputs, but the ancestor\n>>>>>>>> feerate of the\n>>>>>>>> child must be at least as high as the ancestor feerates of every\n>>>>>>>> transaction\n>>>>>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>>>>>> replacement\n>>>>>>>> transaction may only include an unconfirmed input if that input was\n>>>>>>>> included in\n>>>>>>>> one of the original transactions. (An unconfirmed input spends an\n>>>>>>>> output from a\n>>>>>>>> currently-unconfirmed transaction.)\"\n>>>>>>>>\n>>>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the\n>>>>>>>> replacement\n>>>>>>>> transaction has a higher ancestor score than the original\n>>>>>>>> transaction(s) (see\n>>>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed\n>>>>>>>> input can lower the\n>>>>>>>> ancestor score of the replacement transaction. P1 is trying to\n>>>>>>>> replace M1, and\n>>>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,\n>>>>>>>> and M2 pays\n>>>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>>>>>> isolation, P1\n>>>>>>>> looks like a better mining candidate than M1, it must be mined with\n>>>>>>>> M2, so its\n>>>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>>>>>> ancestor\n>>>>>>>> feerate, which is 6sat/vB.\n>>>>>>>>\n>>>>>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>>>>>> transactions in the package can spend new unconfirmed inputs.\"\n>>>>>>>> Example J [17] shows\n>>>>>>>> why, if any of the package transactions have ancestors, package\n>>>>>>>> feerate is no\n>>>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1\n>>>>>>>> (which is the\n>>>>>>>> replacement transaction in an RBF), we're actually interested in\n>>>>>>>> the entire\n>>>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,\n>>>>>>>> P1, P2, and\n>>>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened\n>>>>>>>> to only allow\n>>>>>>>> the child to have new unconfirmed inputs, either, because it can\n>>>>>>>> still cause us\n>>>>>>>> to overestimate the package's ancestor score.\n>>>>>>>>\n>>>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>>>>>> Package RBF\n>>>>>>>> less useful, but would also break Package RBF for packages with\n>>>>>>>> parents already\n>>>>>>>> in the mempool: if a package parent has already been submitted, it\n>>>>>>>> would look\n>>>>>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>>>>>> [18], we're\n>>>>>>>> looking to replace M1 with the entire package including P1, P2, and\n>>>>>>>> P3. We must\n>>>>>>>> consider the case where one of the parents is already in the\n>>>>>>>> mempool (in this\n>>>>>>>> case, P2), which means we must allow P3 to have new unconfirmed\n>>>>>>>> inputs. However,\n>>>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>>>>>> replace M1\n>>>>>>>> with this package.\n>>>>>>>>\n>>>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>>>>>> strict than\n>>>>>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>>>>>> replacement\n>>>>>>>> transactions to have a ancestor score at least as high as the\n>>>>>>>> original ones. As\n>>>>>>>> a result, the entire package is required to be a higher feerate\n>>>>>>>> mining candidate\n>>>>>>>> than each of the replaced transactions.\n>>>>>>>>\n>>>>>>>> Another note: the [comment][13] above the BIP125#2 code in the\n>>>>>>>> original RBF\n>>>>>>>> implementation suggests that the rule was intended to be temporary.\n>>>>>>>>\n>>>>>>>> ##### Absolute Fee (Rule #3)\n>>>>>>>>\n>>>>>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>>>>>> total fees\n>>>>>>>> of the package must be higher than the absolute fees of the mempool\n>>>>>>>> transactions\n>>>>>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>>>>>> BIP125 Rule #3\n>>>>>>>> - an individual transaction in the package may have lower fees than\n>>>>>>>> the\n>>>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and\n>>>>>>>> the child\n>>>>>>>> pays for RBF.\n>>>>>>>>\n>>>>>>>> ##### Feerate (Rule #4)\n>>>>>>>>\n>>>>>>>> The package must pay for its own bandwidth; the package feerate\n>>>>>>>> must be higher\n>>>>>>>> than the replaced transactions by at least minimum relay feerate\n>>>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>>>>>> differs from\n>>>>>>>> BIP125 Rule #4 - an individual transaction in the package can have\n>>>>>>>> a lower\n>>>>>>>> feerate than the transaction(s) it is replacing. In fact, it may\n>>>>>>>> have 0 fees,\n>>>>>>>> and the child pays for RBF.\n>>>>>>>>\n>>>>>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>>>>>\n>>>>>>>> The package cannot replace more than 100 mempool transactions. This\n>>>>>>>> is identical\n>>>>>>>> to BIP125 Rule #5.\n>>>>>>>>\n>>>>>>>> ### Expected FAQs\n>>>>>>>>\n>>>>>>>> 1. Is it possible for only some of the package to make it into the\n>>>>>>>> mempool?\n>>>>>>>>\n>>>>>>>>    Yes, it is. However, since we evict transactions from the\n>>>>>>>> mempool by\n>>>>>>>> descendant score and the package child is supposed to be sponsoring\n>>>>>>>> the fees of\n>>>>>>>> its parents, the most common scenario would be all-or-nothing. This\n>>>>>>>> is\n>>>>>>>> incentive-compatible. In fact, to be conservative, package\n>>>>>>>> validation should\n>>>>>>>> begin by trying to submit all of the transactions individually, and\n>>>>>>>> only use the\n>>>>>>>> package mempool acceptance logic if the parents fail due to low\n>>>>>>>> feerate.\n>>>>>>>>\n>>>>>>>> 2. Should we allow packages to contain already-confirmed\n>>>>>>>> transactions?\n>>>>>>>>\n>>>>>>>>     No, for practical reasons. In mempool validation, we actually\n>>>>>>>> aren't able to\n>>>>>>>> tell with 100% confidence if we are looking at a transaction that\n>>>>>>>> has already\n>>>>>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>>>>>> historical\n>>>>>>>> block data, it's possible to look for it, but this is inefficient,\n>>>>>>>> not always\n>>>>>>>> possible for pruning nodes, and unnecessary because we're not going\n>>>>>>>> to do\n>>>>>>>> anything with the transaction anyway. As such, we already have the\n>>>>>>>> expectation\n>>>>>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>>>>>> relaying\n>>>>>>>> transactions that have already been confirmed. Similarly, we\n>>>>>>>> shouldn't be\n>>>>>>>> relaying packages that contain already-confirmed transactions.\n>>>>>>>>\n>>>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>>>>>> [2]:\n>>>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>>>>>> [3]:\n>>>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>>>>>> [12]:\n>>>>>>>> https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>>>>>> [13]:\n>>>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>>>>>> [14]:\n>>>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>>>>>> [15]:\n>>>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>>>>>> [16]:\n>>>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>>>>>> [17]:\n>>>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>>>>>> [18]:\n>>>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>>>>>> [19]:\n>>>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>>>>>> [20]:\n>>>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>>>>>> _______________________________________________\n>>>>>>>> bitcoin-dev mailing list\n>>>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>>>\n>>>>>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210929/60674e95/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2021-09-20T09:19:38",
                "message_text_only": "Hi Gloria,\n\nThanks for this detailed post!\n\nThe illustrations you provided are very useful for this kind of graph\ntopology problems.\n\nThe rules you lay out for package RBF look good to me at first glance\nas there are some subtle improvements compared to BIP 125.\n\n> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n\nI have a question regarding this rule, as your example 2C could be\nconcerning for LN (unless I didn't understand it correctly).\n\nThis also touches on the package RBF rule 5 (\"The package cannot\nreplace more than 100 mempool transactions.\")\n\nIn your example we have a parent transaction A already in the mempool\nand an unrelated child B. We submit a package C + D where C spends\nanother of A's inputs. You're highlighting that this package may be\nrejected because of the unrelated transaction(s) B.\n\nThe way I see this, an attacker can abuse this rule to ensure\ntransaction A stays pinned in the mempool without confirming by\nbroadcasting a set of child transactions that reach these limits\nand pay low fees (where A would be a commit tx in LN).\n\nWe had to create the CPFP carve-out rule explicitly to work around\nthis limitation, and I think it would be necessary for package RBF\nas well, because in such cases we do want to be able to submit a\npackage A + C where C pays high fees to speed up A's confirmation,\nregardless of unrelated unconfirmed children of A...\n\nWe could submit only C to benefit from the existing CPFP carve-out\nrule, but that wouldn't work if our local mempool doesn't have A yet,\nbut other remote mempools do.\n\nIs my concern justified? Is this something that we should dig into a\nbit deeper?\n\nThanks,\nBastien\n\nLe jeu. 16 sept. 2021 \u00e0 09:55, Gloria Zhao via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi there,\n>\n> I'm writing to propose a set of mempool policy changes to enable package\n> validation (in preparation for package relay) in Bitcoin Core. These would\n> not\n> be consensus or P2P protocol changes. However, since mempool policy\n> significantly affects transaction propagation, I believe this is relevant\n> for\n> the mailing list.\n>\n> My proposal enables packages consisting of multiple parents and 1 child.\n> If you\n> develop software that relies on specific transaction relay assumptions\n> and/or\n> are interested in using package relay in the future, I'm very interested\n> to hear\n> your feedback on the utility or restrictiveness of these package policies\n> for\n> your use cases.\n>\n> A draft implementation of this proposal can be found in [Bitcoin Core\n> PR#22290][1].\n>\n> An illustrated version of this post can be found at\n> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n> I have also linked the images below.\n>\n> ## Background\n>\n> Feel free to skip this section if you are already familiar with mempool\n> policy\n> and package relay terminology.\n>\n> ### Terminology Clarifications\n>\n> * Package = an ordered list of related transactions, representable by a\n> Directed\n>   Acyclic Graph.\n> * Package Feerate = the total modified fees divided by the total virtual\n> size of\n>   all transactions in the package.\n>     - Modified fees = a transaction's base fees + fee delta applied by the\n> user\n>       with `prioritisetransaction`. As such, we expect this to vary across\n> mempools.\n>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141\n>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n> Core][3].\n>     - Note that feerate is not necessarily based on the base fees and\n> serialized\n>       size.\n>\n> * Fee-Bumping = user/wallet actions that take advantage of miner\n> incentives to\n>   boost a transaction's candidacy for inclusion in a block, including\n> Child Pays\n> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in\n> mempool policy is to recognize when the new transaction is more economical\n> to\n> mine than the original one(s) but not open DoS vectors, so there are some\n> limitations.\n>\n> ### Policy\n>\n> The purpose of the mempool is to store the best (to be most\n> incentive-compatible\n> with miners, highest feerate) candidates for inclusion in a block. Miners\n> use\n> the mempool to build block templates. The mempool is also useful as a\n> cache for\n> boosting block relay and validation performance, aiding transaction relay,\n> and\n> generating feerate estimations.\n>\n> Ideally, all consensus-valid transactions paying reasonable fees should\n> make it\n> to miners through normal transaction relay, without any special\n> connectivity or\n> relationships with miners. On the other hand, nodes do not have unlimited\n> resources, and a P2P network designed to let any honest node broadcast\n> their\n> transactions also exposes the transaction validation engine to DoS attacks\n> from\n> malicious peers.\n>\n> As such, for unconfirmed transactions we are considering for our mempool,\n> we\n> apply a set of validation rules in addition to consensus, primarily to\n> protect\n> us from resource exhaustion and aid our efforts to keep the highest fee\n> transactions. We call this mempool _policy_: a set of (configurable,\n> node-specific) rules that transactions must abide by in order to be\n> accepted\n> into our mempool. Transaction \"Standardness\" rules and mempool\n> restrictions such\n> as \"too-long-mempool-chain\" are both examples of policy.\n>\n> ### Package Relay and Package Mempool Accept\n>\n> In transaction relay, we currently consider transactions one at a time for\n> submission to the mempool. This creates a limitation in the node's ability\n> to\n> determine which transactions have the highest feerates, since we cannot\n> take\n> into account descendants (i.e. cannot use CPFP) until all the transactions\n> are\n> in the mempool. Similarly, we cannot use a transaction's descendants when\n> considering it for RBF. When an individual transaction does not meet the\n> mempool\n> minimum feerate and the user isn't able to create a replacement transaction\n> directly, it will not be accepted by mempools.\n>\n> This limitation presents a security issue for applications and users\n> relying on\n> time-sensitive transactions. For example, Lightning and other protocols\n> create\n> UTXOs with multiple spending paths, where one counterparty's spending path\n> opens\n> up after a timelock, and users are protected from cheating scenarios as\n> long as\n> they redeem on-chain in time. A key security assumption is that all\n> parties'\n> transactions will propagate and confirm in a timely manner. This\n> assumption can\n> be broken if fee-bumping does not work as intended.\n>\n> The end goal for Package Relay is to consider multiple transactions at the\n> same\n> time, e.g. a transaction with its high-fee child. This may help us better\n> determine whether transactions should be accepted to our mempool,\n> especially if\n> they don't meet fee requirements individually or are better RBF candidates\n> as a\n> package. A combination of changes to mempool validation logic, policy, and\n> transaction relay allows us to better propagate the transactions with the\n> highest package feerates to miners, and makes fee-bumping tools more\n> powerful\n> for users.\n>\n> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n> large\n> part of the changes are in the mempool's package validation logic. We call\n> this\n> *Package Mempool Accept*.\n>\n> ### Previous Work\n>\n> * Given that mempool validation is DoS-sensitive and complex, it would be\n>   dangerous to haphazardly tack on package validation logic. Many efforts\n> have\n> been made to make mempool validation less opaque (see [#16400][4],\n> [#21062][5],\n> [#22675][6], [#22796][7]).\n> * [#20833][8] Added basic capabilities for package validation, test\n> accepts only\n>   (no submission to mempool).\n> * [#21800][9] Implemented package ancestor/descendant limit checks for\n> arbitrary\n>   packages. Still test accepts only.\n> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>\n> ### Existing Package Rules\n>\n> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n> consider\n> them as \"given\" in the rest of this document, though they can be changed,\n> since\n> package validation is test-accept only right now.\n>\n> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>\n>    *Rationale*: This is already enforced as mempool ancestor/descendant\n> limits.\n> Presumably, transactions in a package are all related, so exceeding this\n> limit\n> would mean that the package can either be split up or it wouldn't pass this\n> mempool policy.\n>\n> 2. Packages must be topologically sorted: if any dependencies exist between\n> transactions, parents must appear somewhere before children. [8]\n>\n> 3. A package cannot have conflicting transactions, i.e. none of them can\n> spend\n> the same inputs. This also means there cannot be duplicate transactions.\n> [8]\n>\n> 4. When packages are evaluated against ancestor/descendant limits in a test\n> accept, the union of all of their descendants and ancestors is considered.\n> This\n> is essentially a \"worst case\" heuristic where every transaction in the\n> package\n> is treated as each other's ancestor and descendant. [8]\n> Packages for which ancestor/descendant limits are accurately captured by\n> this\n> heuristic: [19]\n>\n> There are also limitations such as the fact that CPFP carve out is not\n> applied\n> to package transactions. #20833 also disables RBF in package validation;\n> this\n> proposal overrides that to allow packages to use RBF.\n>\n> ## Proposed Changes\n>\n> The next step in the Package Mempool Accept project is to implement\n> submission\n> to mempool, initially through RPC only. This allows us to test the\n> submission\n> logic before exposing it on P2P.\n>\n> ### Summary\n>\n> - Packages may contain already-in-mempool transactions.\n> - Packages are 2 generations, Multi-Parent-1-Child.\n> - Fee-related checks use the package feerate. This means that wallets can\n> create a package that utilizes CPFP.\n> - Parents are allowed to RBF mempool transactions with a set of rules\n> similar\n>   to BIP125. This enables a combination of CPFP and RBF, where a\n> transaction's descendant fees pay for replacing mempool conflicts.\n>\n> There is a draft implementation in [#22290][1]. It is WIP, but feedback is\n> always welcome.\n>\n> ### Details\n>\n> #### Packages May Contain Already-in-Mempool Transactions\n>\n> A package may contain transactions that are already in the mempool. We\n> remove\n> (\"deduplicate\") those transactions from the package for the purposes of\n> package\n> mempool acceptance. If a package is empty after deduplication, we do\n> nothing.\n>\n> *Rationale*: Mempools vary across the network. It's possible for a parent\n> to be\n> accepted to the mempool of a peer on its own due to differences in policy\n> and\n> fee market fluctuations. We should not reject or penalize the entire\n> package for\n> an individual transaction as that could be a censorship vector.\n>\n> #### Packages Are Multi-Parent-1-Child\n>\n> Only packages of a specific topology are permitted. Namely, a package is\n> exactly\n> 1 child with all of its unconfirmed parents. After deduplication, the\n> package\n> may be exactly the same, empty, 1 child, 1 child with just some of its\n> unconfirmed parents, etc. Note that it's possible for the parents to be\n> indirect\n> descendants/ancestors of one another, or for parent and child to share a\n> parent,\n> so we cannot make any other topology assumptions.\n>\n> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple parents\n> makes it possible to fee-bump a batch of transactions. Restricting\n> packages to a\n> defined topology is also easier to reason about and simplifies the\n> validation\n> logic greatly. Multi-parent-1-child allows us to think of the package as\n> one big\n> transaction, where:\n>\n> - Inputs = all the inputs of parents + inputs of the child that come from\n>   confirmed UTXOs\n> - Outputs = all the outputs of the child + all outputs of the parents that\n>   aren't spent by other transactions in the package\n>\n> Examples of packages that follow this rule (variations of example A show\n> some\n> possibilities after deduplication): ![image][15]\n>\n> #### Fee-Related Checks Use Package Feerate\n>\n> Package Feerate = the total modified fees divided by the total virtual\n> size of\n> all transactions in the package.\n>\n> To meet the two feerate requirements of a mempool, i.e., the pre-configured\n> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n> feerate, the\n> total package feerate is used instead of the individual feerate. The\n> individual\n> transactions are allowed to be below feerate requirements if the package\n> meets\n> the feerate requirements. For example, the parent(s) in the package can\n> have 0\n> fees but be paid for by the child.\n>\n> *Rationale*: This can be thought of as \"CPFP within a package,\" solving the\n> issue of a parent not meeting minimum fees on its own. This allows L2\n> applications to adjust their fees at broadcast time instead of\n> overshooting or\n> risking getting stuck/pinned.\n>\n> We use the package feerate of the package *after deduplication*.\n>\n> *Rationale*:  It would be incorrect to use the fees of transactions that\n> are\n> already in the mempool, as we do not want a transaction's fees to be\n> double-counted for both its individual RBF and package RBF.\n>\n> Examples F and G [14] show the same package, but P1 is submitted\n> individually before\n> the package in example G. In example F, we can see that the 300vB package\n> pays\n> an additional 200sat in fees, which is not enough to pay for its own\n> bandwidth\n> (BIP125#4). In example G, we can see that P1 pays enough to replace M1, but\n> using P1's fees again during package submission would make it look like a\n> 300sat\n> increase for a 200vB package. Even including its fees and size would not be\n> sufficient in this example, since the 300sat looks like enough for the\n> 300vB\n> package. The calculcation after deduplication is 100sat increase for a\n> package\n> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n> have a\n> size of 100vB.\n>\n> #### Package RBF\n>\n> If a package meets feerate requirements as a package, the parents in the\n> transaction are allowed to replace-by-fee mempool transactions. The child\n> cannot\n> replace mempool transactions. Multiple transactions can replace the same\n> transaction, but in order to be valid, none of the transactions can try to\n> replace an ancestor of another transaction in the same package (which\n> would thus\n> make its inputs unavailable).\n>\n> *Rationale*: Even if we are using package feerate, a package will not\n> propagate\n> as intended if RBF still requires each individual transaction to meet the\n> feerate requirements.\n>\n> We use a set of rules slightly modified from BIP125 as follows:\n>\n> ##### Signaling (Rule #1)\n>\n> All mempool transactions to be replaced must signal replaceability.\n>\n> *Rationale*: Package RBF signaling logic should be the same for package\n> RBF and\n> single transaction acceptance. This would be updated if single transaction\n> validation moves to full RBF.\n>\n> ##### New Unconfirmed Inputs (Rule #2)\n>\n> A package may include new unconfirmed inputs, but the ancestor feerate of\n> the\n> child must be at least as high as the ancestor feerates of every\n> transaction\n> being replaced. This is contrary to BIP125#2, which states \"The replacement\n> transaction may only include an unconfirmed input if that input was\n> included in\n> one of the original transactions. (An unconfirmed input spends an output\n> from a\n> currently-unconfirmed transaction.)\"\n>\n> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n> transaction has a higher ancestor score than the original transaction(s)\n> (see\n> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n> can lower the\n> ancestor score of the replacement transaction. P1 is trying to replace M1,\n> and\n> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and M2\n> pays\n> 100sat. Assume all transactions have a size of 100vB. While, in isolation,\n> P1\n> looks like a better mining candidate than M1, it must be mined with M2, so\n> its\n> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor\n> feerate, which is 6sat/vB.\n>\n> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n> transactions in the package can spend new unconfirmed inputs.\" Example J\n> [17] shows\n> why, if any of the package transactions have ancestors, package feerate is\n> no\n> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is\n> the\n> replacement transaction in an RBF), we're actually interested in the entire\n> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n> P2, and\n> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to only\n> allow\n> the child to have new unconfirmed inputs, either, because it can still\n> cause us\n> to overestimate the package's ancestor score.\n>\n> However, enforcing a rule analogous to BIP125#2 would not only make\n> Package RBF\n> less useful, but would also break Package RBF for packages with parents\n> already\n> in the mempool: if a package parent has already been submitted, it would\n> look\n> like the child is spending a \"new\" unconfirmed input. In example K [18],\n> we're\n> looking to replace M1 with the entire package including P1, P2, and P3. We\n> must\n> consider the case where one of the parents is already in the mempool (in\n> this\n> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n> However,\n> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace\n> M1\n> with this package.\n>\n> Thus, the package RBF rule regarding new unconfirmed inputs is less strict\n> than\n> BIP125#2. However, we still achieve the same goal of requiring the\n> replacement\n> transactions to have a ancestor score at least as high as the original\n> ones. As\n> a result, the entire package is required to be a higher feerate mining\n> candidate\n> than each of the replaced transactions.\n>\n> Another note: the [comment][13] above the BIP125#2 code in the original RBF\n> implementation suggests that the rule was intended to be temporary.\n>\n> ##### Absolute Fee (Rule #3)\n>\n> The package must increase the absolute fee of the mempool, i.e. the total\n> fees\n> of the package must be higher than the absolute fees of the mempool\n> transactions\n> it replaces. Combined with the CPFP rule above, this differs from BIP125\n> Rule #3\n> - an individual transaction in the package may have lower fees than the\n>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n> child\n> pays for RBF.\n>\n> ##### Feerate (Rule #4)\n>\n> The package must pay for its own bandwidth; the package feerate must be\n> higher\n> than the replaced transactions by at least minimum relay feerate\n> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs\n> from\n> BIP125 Rule #4 - an individual transaction in the package can have a lower\n> feerate than the transaction(s) it is replacing. In fact, it may have 0\n> fees,\n> and the child pays for RBF.\n>\n> ##### Total Number of Replaced Transactions (Rule #5)\n>\n> The package cannot replace more than 100 mempool transactions. This is\n> identical\n> to BIP125 Rule #5.\n>\n> ### Expected FAQs\n>\n> 1. Is it possible for only some of the package to make it into the mempool?\n>\n>    Yes, it is. However, since we evict transactions from the mempool by\n> descendant score and the package child is supposed to be sponsoring the\n> fees of\n> its parents, the most common scenario would be all-or-nothing. This is\n> incentive-compatible. In fact, to be conservative, package validation\n> should\n> begin by trying to submit all of the transactions individually, and only\n> use the\n> package mempool acceptance logic if the parents fail due to low feerate.\n>\n> 2. Should we allow packages to contain already-confirmed transactions?\n>\n>     No, for practical reasons. In mempool validation, we actually aren't\n> able to\n> tell with 100% confidence if we are looking at a transaction that has\n> already\n> confirmed, because we look up inputs using a UTXO set. If we have\n> historical\n> block data, it's possible to look for it, but this is inefficient, not\n> always\n> possible for pruning nodes, and unnecessary because we're not going to do\n> anything with the transaction anyway. As such, we already have the\n> expectation\n> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n> relaying\n> transactions that have already been confirmed. Similarly, we shouldn't be\n> relaying packages that contain already-confirmed transactions.\n>\n> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n> [2]:\n> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n> [3]:\n> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n> [13]:\n> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n> [14]:\n> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n> [15]:\n> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n> [16]:\n> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n> [17]:\n> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n> [18]:\n> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n> [19]:\n> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n> [20]:\n> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210920/adec3501/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2021-09-21T11:18:31",
                "message_text_only": "Hi Bastien,\n\nThank you for your feedback!\n\n> In your example we have a parent transaction A already in the mempool\n> and an unrelated child B. We submit a package C + D where C spends\n> another of A's inputs. You're highlighting that this package may be\n> rejected because of the unrelated transaction(s) B.\n\n> The way I see this, an attacker can abuse this rule to ensure\n> transaction A stays pinned in the mempool without confirming by\n> broadcasting a set of child transactions that reach these limits\n> and pay low fees (where A would be a commit tx in LN).\n\nI believe you are describing a pinning attack in which your adversarial\ncounterparty attempts to monopolize the mempool descendant limit of the\nshared  transaction A in order to prevent you from submitting a fee-bumping\nchild C; I've tried to illustrate this as diagram A here:\nhttps://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png\n(please let me know if I'm misunderstanding).\n\nI believe this attack is mitigated as long as we attempt to submit\ntransactions individually (and thus take advantage of CPFP carve out)\nbefore attempting package validation. So, in scenario A2, even if the\nmempool receives a package with A+C, it would deduplicate A, submit C as an\nindividual transaction, and allow it due to the CPFP carve out exemption. A\nmore general goal is: if a transaction would propagate successfully on its\nown now, it should still propagate regardless of whether it is included in\na package. The best way to ensure this, as far as I can tell, is to always\ntry to submit them individually first.\n\nI would note that this proposal doesn't accommodate something like diagram\nB, where C is getting CPFP carve out and wants to bring a +1 (e.g. C has\nvery low fees and is bumped by D). I don't think this is a use case since C\nshould be the one fee-bumping A, but since we're talking about limitations\naround the CPFP carve out, this is it.\n\nLet me know if this addresses your concerns?\n\nThanks,\nGloria\n\nOn Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>\nwrote:\n\n> Hi Gloria,\n>\n> Thanks for this detailed post!\n>\n> The illustrations you provided are very useful for this kind of graph\n> topology problems.\n>\n> The rules you lay out for package RBF look good to me at first glance\n> as there are some subtle improvements compared to BIP 125.\n>\n> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n> > `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>\n> I have a question regarding this rule, as your example 2C could be\n> concerning for LN (unless I didn't understand it correctly).\n>\n> This also touches on the package RBF rule 5 (\"The package cannot\n> replace more than 100 mempool transactions.\")\n>\n> In your example we have a parent transaction A already in the mempool\n> and an unrelated child B. We submit a package C + D where C spends\n> another of A's inputs. You're highlighting that this package may be\n> rejected because of the unrelated transaction(s) B.\n>\n> The way I see this, an attacker can abuse this rule to ensure\n> transaction A stays pinned in the mempool without confirming by\n> broadcasting a set of child transactions that reach these limits\n> and pay low fees (where A would be a commit tx in LN).\n>\n> We had to create the CPFP carve-out rule explicitly to work around\n> this limitation, and I think it would be necessary for package RBF\n> as well, because in such cases we do want to be able to submit a\n> package A + C where C pays high fees to speed up A's confirmation,\n> regardless of unrelated unconfirmed children of A...\n>\n> We could submit only C to benefit from the existing CPFP carve-out\n> rule, but that wouldn't work if our local mempool doesn't have A yet,\n> but other remote mempools do.\n>\n> Is my concern justified? Is this something that we should dig into a\n> bit deeper?\n>\n> Thanks,\n> Bastien\n>\n> Le jeu. 16 sept. 2021 \u00e0 09:55, Gloria Zhao via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi there,\n>>\n>> I'm writing to propose a set of mempool policy changes to enable package\n>> validation (in preparation for package relay) in Bitcoin Core. These\n>> would not\n>> be consensus or P2P protocol changes. However, since mempool policy\n>> significantly affects transaction propagation, I believe this is relevant\n>> for\n>> the mailing list.\n>>\n>> My proposal enables packages consisting of multiple parents and 1 child.\n>> If you\n>> develop software that relies on specific transaction relay assumptions\n>> and/or\n>> are interested in using package relay in the future, I'm very interested\n>> to hear\n>> your feedback on the utility or restrictiveness of these package policies\n>> for\n>> your use cases.\n>>\n>> A draft implementation of this proposal can be found in [Bitcoin Core\n>> PR#22290][1].\n>>\n>> An illustrated version of this post can be found at\n>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>> I have also linked the images below.\n>>\n>> ## Background\n>>\n>> Feel free to skip this section if you are already familiar with mempool\n>> policy\n>> and package relay terminology.\n>>\n>> ### Terminology Clarifications\n>>\n>> * Package = an ordered list of related transactions, representable by a\n>> Directed\n>>   Acyclic Graph.\n>> * Package Feerate = the total modified fees divided by the total virtual\n>> size of\n>>   all transactions in the package.\n>>     - Modified fees = a transaction's base fees + fee delta applied by\n>> the user\n>>       with `prioritisetransaction`. As such, we expect this to vary across\n>> mempools.\n>>     - Virtual Size = the maximum of virtual sizes calculated using [BIP141\n>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>> Core][3].\n>>     - Note that feerate is not necessarily based on the base fees and\n>> serialized\n>>       size.\n>>\n>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>> incentives to\n>>   boost a transaction's candidacy for inclusion in a block, including\n>> Child Pays\n>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in\n>> mempool policy is to recognize when the new transaction is more\n>> economical to\n>> mine than the original one(s) but not open DoS vectors, so there are some\n>> limitations.\n>>\n>> ### Policy\n>>\n>> The purpose of the mempool is to store the best (to be most\n>> incentive-compatible\n>> with miners, highest feerate) candidates for inclusion in a block. Miners\n>> use\n>> the mempool to build block templates. The mempool is also useful as a\n>> cache for\n>> boosting block relay and validation performance, aiding transaction\n>> relay, and\n>> generating feerate estimations.\n>>\n>> Ideally, all consensus-valid transactions paying reasonable fees should\n>> make it\n>> to miners through normal transaction relay, without any special\n>> connectivity or\n>> relationships with miners. On the other hand, nodes do not have unlimited\n>> resources, and a P2P network designed to let any honest node broadcast\n>> their\n>> transactions also exposes the transaction validation engine to DoS\n>> attacks from\n>> malicious peers.\n>>\n>> As such, for unconfirmed transactions we are considering for our mempool,\n>> we\n>> apply a set of validation rules in addition to consensus, primarily to\n>> protect\n>> us from resource exhaustion and aid our efforts to keep the highest fee\n>> transactions. We call this mempool _policy_: a set of (configurable,\n>> node-specific) rules that transactions must abide by in order to be\n>> accepted\n>> into our mempool. Transaction \"Standardness\" rules and mempool\n>> restrictions such\n>> as \"too-long-mempool-chain\" are both examples of policy.\n>>\n>> ### Package Relay and Package Mempool Accept\n>>\n>> In transaction relay, we currently consider transactions one at a time for\n>> submission to the mempool. This creates a limitation in the node's\n>> ability to\n>> determine which transactions have the highest feerates, since we cannot\n>> take\n>> into account descendants (i.e. cannot use CPFP) until all the\n>> transactions are\n>> in the mempool. Similarly, we cannot use a transaction's descendants when\n>> considering it for RBF. When an individual transaction does not meet the\n>> mempool\n>> minimum feerate and the user isn't able to create a replacement\n>> transaction\n>> directly, it will not be accepted by mempools.\n>>\n>> This limitation presents a security issue for applications and users\n>> relying on\n>> time-sensitive transactions. For example, Lightning and other protocols\n>> create\n>> UTXOs with multiple spending paths, where one counterparty's spending\n>> path opens\n>> up after a timelock, and users are protected from cheating scenarios as\n>> long as\n>> they redeem on-chain in time. A key security assumption is that all\n>> parties'\n>> transactions will propagate and confirm in a timely manner. This\n>> assumption can\n>> be broken if fee-bumping does not work as intended.\n>>\n>> The end goal for Package Relay is to consider multiple transactions at\n>> the same\n>> time, e.g. a transaction with its high-fee child. This may help us better\n>> determine whether transactions should be accepted to our mempool,\n>> especially if\n>> they don't meet fee requirements individually or are better RBF\n>> candidates as a\n>> package. A combination of changes to mempool validation logic, policy, and\n>> transaction relay allows us to better propagate the transactions with the\n>> highest package feerates to miners, and makes fee-bumping tools more\n>> powerful\n>> for users.\n>>\n>> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n>> large\n>> part of the changes are in the mempool's package validation logic. We\n>> call this\n>> *Package Mempool Accept*.\n>>\n>> ### Previous Work\n>>\n>> * Given that mempool validation is DoS-sensitive and complex, it would be\n>>   dangerous to haphazardly tack on package validation logic. Many efforts\n>> have\n>> been made to make mempool validation less opaque (see [#16400][4],\n>> [#21062][5],\n>> [#22675][6], [#22796][7]).\n>> * [#20833][8] Added basic capabilities for package validation, test\n>> accepts only\n>>   (no submission to mempool).\n>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>> arbitrary\n>>   packages. Still test accepts only.\n>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>\n>> ### Existing Package Rules\n>>\n>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>> consider\n>> them as \"given\" in the rest of this document, though they can be changed,\n>> since\n>> package validation is test-accept only right now.\n>>\n>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>\n>>    *Rationale*: This is already enforced as mempool ancestor/descendant\n>> limits.\n>> Presumably, transactions in a package are all related, so exceeding this\n>> limit\n>> would mean that the package can either be split up or it wouldn't pass\n>> this\n>> mempool policy.\n>>\n>> 2. Packages must be topologically sorted: if any dependencies exist\n>> between\n>> transactions, parents must appear somewhere before children. [8]\n>>\n>> 3. A package cannot have conflicting transactions, i.e. none of them can\n>> spend\n>> the same inputs. This also means there cannot be duplicate transactions.\n>> [8]\n>>\n>> 4. When packages are evaluated against ancestor/descendant limits in a\n>> test\n>> accept, the union of all of their descendants and ancestors is\n>> considered. This\n>> is essentially a \"worst case\" heuristic where every transaction in the\n>> package\n>> is treated as each other's ancestor and descendant. [8]\n>> Packages for which ancestor/descendant limits are accurately captured by\n>> this\n>> heuristic: [19]\n>>\n>> There are also limitations such as the fact that CPFP carve out is not\n>> applied\n>> to package transactions. #20833 also disables RBF in package validation;\n>> this\n>> proposal overrides that to allow packages to use RBF.\n>>\n>> ## Proposed Changes\n>>\n>> The next step in the Package Mempool Accept project is to implement\n>> submission\n>> to mempool, initially through RPC only. This allows us to test the\n>> submission\n>> logic before exposing it on P2P.\n>>\n>> ### Summary\n>>\n>> - Packages may contain already-in-mempool transactions.\n>> - Packages are 2 generations, Multi-Parent-1-Child.\n>> - Fee-related checks use the package feerate. This means that wallets can\n>> create a package that utilizes CPFP.\n>> - Parents are allowed to RBF mempool transactions with a set of rules\n>> similar\n>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>> transaction's descendant fees pay for replacing mempool conflicts.\n>>\n>> There is a draft implementation in [#22290][1]. It is WIP, but feedback is\n>> always welcome.\n>>\n>> ### Details\n>>\n>> #### Packages May Contain Already-in-Mempool Transactions\n>>\n>> A package may contain transactions that are already in the mempool. We\n>> remove\n>> (\"deduplicate\") those transactions from the package for the purposes of\n>> package\n>> mempool acceptance. If a package is empty after deduplication, we do\n>> nothing.\n>>\n>> *Rationale*: Mempools vary across the network. It's possible for a parent\n>> to be\n>> accepted to the mempool of a peer on its own due to differences in policy\n>> and\n>> fee market fluctuations. We should not reject or penalize the entire\n>> package for\n>> an individual transaction as that could be a censorship vector.\n>>\n>> #### Packages Are Multi-Parent-1-Child\n>>\n>> Only packages of a specific topology are permitted. Namely, a package is\n>> exactly\n>> 1 child with all of its unconfirmed parents. After deduplication, the\n>> package\n>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>> unconfirmed parents, etc. Note that it's possible for the parents to be\n>> indirect\n>> descendants/ancestors of one another, or for parent and child to share a\n>> parent,\n>> so we cannot make any other topology assumptions.\n>>\n>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>> parents\n>> makes it possible to fee-bump a batch of transactions. Restricting\n>> packages to a\n>> defined topology is also easier to reason about and simplifies the\n>> validation\n>> logic greatly. Multi-parent-1-child allows us to think of the package as\n>> one big\n>> transaction, where:\n>>\n>> - Inputs = all the inputs of parents + inputs of the child that come from\n>>   confirmed UTXOs\n>> - Outputs = all the outputs of the child + all outputs of the parents that\n>>   aren't spent by other transactions in the package\n>>\n>> Examples of packages that follow this rule (variations of example A show\n>> some\n>> possibilities after deduplication): ![image][15]\n>>\n>> #### Fee-Related Checks Use Package Feerate\n>>\n>> Package Feerate = the total modified fees divided by the total virtual\n>> size of\n>> all transactions in the package.\n>>\n>> To meet the two feerate requirements of a mempool, i.e., the\n>> pre-configured\n>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>> feerate, the\n>> total package feerate is used instead of the individual feerate. The\n>> individual\n>> transactions are allowed to be below feerate requirements if the package\n>> meets\n>> the feerate requirements. For example, the parent(s) in the package can\n>> have 0\n>> fees but be paid for by the child.\n>>\n>> *Rationale*: This can be thought of as \"CPFP within a package,\" solving\n>> the\n>> issue of a parent not meeting minimum fees on its own. This allows L2\n>> applications to adjust their fees at broadcast time instead of\n>> overshooting or\n>> risking getting stuck/pinned.\n>>\n>> We use the package feerate of the package *after deduplication*.\n>>\n>> *Rationale*:  It would be incorrect to use the fees of transactions that\n>> are\n>> already in the mempool, as we do not want a transaction's fees to be\n>> double-counted for both its individual RBF and package RBF.\n>>\n>> Examples F and G [14] show the same package, but P1 is submitted\n>> individually before\n>> the package in example G. In example F, we can see that the 300vB package\n>> pays\n>> an additional 200sat in fees, which is not enough to pay for its own\n>> bandwidth\n>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n>> but\n>> using P1's fees again during package submission would make it look like a\n>> 300sat\n>> increase for a 200vB package. Even including its fees and size would not\n>> be\n>> sufficient in this example, since the 300sat looks like enough for the\n>> 300vB\n>> package. The calculcation after deduplication is 100sat increase for a\n>> package\n>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>> have a\n>> size of 100vB.\n>>\n>> #### Package RBF\n>>\n>> If a package meets feerate requirements as a package, the parents in the\n>> transaction are allowed to replace-by-fee mempool transactions. The child\n>> cannot\n>> replace mempool transactions. Multiple transactions can replace the same\n>> transaction, but in order to be valid, none of the transactions can try to\n>> replace an ancestor of another transaction in the same package (which\n>> would thus\n>> make its inputs unavailable).\n>>\n>> *Rationale*: Even if we are using package feerate, a package will not\n>> propagate\n>> as intended if RBF still requires each individual transaction to meet the\n>> feerate requirements.\n>>\n>> We use a set of rules slightly modified from BIP125 as follows:\n>>\n>> ##### Signaling (Rule #1)\n>>\n>> All mempool transactions to be replaced must signal replaceability.\n>>\n>> *Rationale*: Package RBF signaling logic should be the same for package\n>> RBF and\n>> single transaction acceptance. This would be updated if single transaction\n>> validation moves to full RBF.\n>>\n>> ##### New Unconfirmed Inputs (Rule #2)\n>>\n>> A package may include new unconfirmed inputs, but the ancestor feerate of\n>> the\n>> child must be at least as high as the ancestor feerates of every\n>> transaction\n>> being replaced. This is contrary to BIP125#2, which states \"The\n>> replacement\n>> transaction may only include an unconfirmed input if that input was\n>> included in\n>> one of the original transactions. (An unconfirmed input spends an output\n>> from a\n>> currently-unconfirmed transaction.)\"\n>>\n>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>> transaction has a higher ancestor score than the original transaction(s)\n>> (see\n>> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n>> can lower the\n>> ancestor score of the replacement transaction. P1 is trying to replace\n>> M1, and\n>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and\n>> M2 pays\n>> 100sat. Assume all transactions have a size of 100vB. While, in\n>> isolation, P1\n>> looks like a better mining candidate than M1, it must be mined with M2,\n>> so its\n>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor\n>> feerate, which is 6sat/vB.\n>>\n>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>> transactions in the package can spend new unconfirmed inputs.\" Example J\n>> [17] shows\n>> why, if any of the package transactions have ancestors, package feerate\n>> is no\n>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is\n>> the\n>> replacement transaction in an RBF), we're actually interested in the\n>> entire\n>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n>> P2, and\n>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>> only allow\n>> the child to have new unconfirmed inputs, either, because it can still\n>> cause us\n>> to overestimate the package's ancestor score.\n>>\n>> However, enforcing a rule analogous to BIP125#2 would not only make\n>> Package RBF\n>> less useful, but would also break Package RBF for packages with parents\n>> already\n>> in the mempool: if a package parent has already been submitted, it would\n>> look\n>> like the child is spending a \"new\" unconfirmed input. In example K [18],\n>> we're\n>> looking to replace M1 with the entire package including P1, P2, and P3.\n>> We must\n>> consider the case where one of the parents is already in the mempool (in\n>> this\n>> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n>> However,\n>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not replace\n>> M1\n>> with this package.\n>>\n>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>> strict than\n>> BIP125#2. However, we still achieve the same goal of requiring the\n>> replacement\n>> transactions to have a ancestor score at least as high as the original\n>> ones. As\n>> a result, the entire package is required to be a higher feerate mining\n>> candidate\n>> than each of the replaced transactions.\n>>\n>> Another note: the [comment][13] above the BIP125#2 code in the original\n>> RBF\n>> implementation suggests that the rule was intended to be temporary.\n>>\n>> ##### Absolute Fee (Rule #3)\n>>\n>> The package must increase the absolute fee of the mempool, i.e. the total\n>> fees\n>> of the package must be higher than the absolute fees of the mempool\n>> transactions\n>> it replaces. Combined with the CPFP rule above, this differs from BIP125\n>> Rule #3\n>> - an individual transaction in the package may have lower fees than the\n>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>> child\n>> pays for RBF.\n>>\n>> ##### Feerate (Rule #4)\n>>\n>> The package must pay for its own bandwidth; the package feerate must be\n>> higher\n>> than the replaced transactions by at least minimum relay feerate\n>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs\n>> from\n>> BIP125 Rule #4 - an individual transaction in the package can have a lower\n>> feerate than the transaction(s) it is replacing. In fact, it may have 0\n>> fees,\n>> and the child pays for RBF.\n>>\n>> ##### Total Number of Replaced Transactions (Rule #5)\n>>\n>> The package cannot replace more than 100 mempool transactions. This is\n>> identical\n>> to BIP125 Rule #5.\n>>\n>> ### Expected FAQs\n>>\n>> 1. Is it possible for only some of the package to make it into the\n>> mempool?\n>>\n>>    Yes, it is. However, since we evict transactions from the mempool by\n>> descendant score and the package child is supposed to be sponsoring the\n>> fees of\n>> its parents, the most common scenario would be all-or-nothing. This is\n>> incentive-compatible. In fact, to be conservative, package validation\n>> should\n>> begin by trying to submit all of the transactions individually, and only\n>> use the\n>> package mempool acceptance logic if the parents fail due to low feerate.\n>>\n>> 2. Should we allow packages to contain already-confirmed transactions?\n>>\n>>     No, for practical reasons. In mempool validation, we actually aren't\n>> able to\n>> tell with 100% confidence if we are looking at a transaction that has\n>> already\n>> confirmed, because we look up inputs using a UTXO set. If we have\n>> historical\n>> block data, it's possible to look for it, but this is inefficient, not\n>> always\n>> possible for pruning nodes, and unnecessary because we're not going to do\n>> anything with the transaction anyway. As such, we already have the\n>> expectation\n>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>> relaying\n>> transactions that have already been confirmed. Similarly, we shouldn't be\n>> relaying packages that contain already-confirmed transactions.\n>>\n>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>> [2]:\n>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>> [3]:\n>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>> [13]:\n>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>> [14]:\n>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>> [15]:\n>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>> [16]:\n>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>> [17]:\n>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>> [18]:\n>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>> [19]:\n>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>> [20]:\n>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/505919a9/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2021-09-21T15:18:28",
                "message_text_only": "Hi Gloria,\n\n> I believe this attack is mitigated as long as we attempt to submit\ntransactions individually\n\nUnfortunately not, as there exists a pinning scenario in LN where a\ndifferent commit tx is pinned, but you actually can't know which one.\n\nSince I really like your diagrams, I made one as well to illustrate:\nhttps://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg\n\nHere the issue is that a revoked commitment tx A' is pinned in other\nmempools, with a long chain of descendants (or descendants that reach\nthe maximum replaceable size).\n\nWe would really like A + C to be able to replace this pinned A'.\nWe can't submit individually because A on its own won't replace A'...\n\n> I would note that this proposal doesn't accommodate something like\ndiagram B, where C is getting CPFP carve out and wants to bring a +1\n\nNo worries, that case shouldn't be a concern.\nI believe any L2 protocol can always ensure it confirms such tx trees\n\"one depth after the other\" without impacting funds safety, so it\nonly needs to ensure A + C can get into mempools.\n\nThanks,\nBastien\n\nLe mar. 21 sept. 2021 \u00e0 13:18, Gloria Zhao <gloriajzhao at gmail.com> a \u00e9crit :\n\n> Hi Bastien,\n>\n> Thank you for your feedback!\n>\n> > In your example we have a parent transaction A already in the mempool\n> > and an unrelated child B. We submit a package C + D where C spends\n> > another of A's inputs. You're highlighting that this package may be\n> > rejected because of the unrelated transaction(s) B.\n>\n> > The way I see this, an attacker can abuse this rule to ensure\n> > transaction A stays pinned in the mempool without confirming by\n> > broadcasting a set of child transactions that reach these limits\n> > and pay low fees (where A would be a commit tx in LN).\n>\n> I believe you are describing a pinning attack in which your adversarial\n> counterparty attempts to monopolize the mempool descendant limit of the\n> shared  transaction A in order to prevent you from submitting a fee-bumping\n> child C; I've tried to illustrate this as diagram A here:\n> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png\n> (please let me know if I'm misunderstanding).\n>\n> I believe this attack is mitigated as long as we attempt to submit\n> transactions individually (and thus take advantage of CPFP carve out)\n> before attempting package validation. So, in scenario A2, even if the\n> mempool receives a package with A+C, it would deduplicate A, submit C as an\n> individual transaction, and allow it due to the CPFP carve out exemption. A\n> more general goal is: if a transaction would propagate successfully on its\n> own now, it should still propagate regardless of whether it is included in\n> a package. The best way to ensure this, as far as I can tell, is to always\n> try to submit them individually first.\n>\n> I would note that this proposal doesn't accommodate something like diagram\n> B, where C is getting CPFP carve out and wants to bring a +1 (e.g. C has\n> very low fees and is bumped by D). I don't think this is a use case since C\n> should be the one fee-bumping A, but since we're talking about limitations\n> around the CPFP carve out, this is it.\n>\n> Let me know if this addresses your concerns?\n>\n> Thanks,\n> Gloria\n>\n> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>\n> wrote:\n>\n>> Hi Gloria,\n>>\n>> Thanks for this detailed post!\n>>\n>> The illustrations you provided are very useful for this kind of graph\n>> topology problems.\n>>\n>> The rules you lay out for package RBF look good to me at first glance\n>> as there are some subtle improvements compared to BIP 125.\n>>\n>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>\n>> I have a question regarding this rule, as your example 2C could be\n>> concerning for LN (unless I didn't understand it correctly).\n>>\n>> This also touches on the package RBF rule 5 (\"The package cannot\n>> replace more than 100 mempool transactions.\")\n>>\n>> In your example we have a parent transaction A already in the mempool\n>> and an unrelated child B. We submit a package C + D where C spends\n>> another of A's inputs. You're highlighting that this package may be\n>> rejected because of the unrelated transaction(s) B.\n>>\n>> The way I see this, an attacker can abuse this rule to ensure\n>> transaction A stays pinned in the mempool without confirming by\n>> broadcasting a set of child transactions that reach these limits\n>> and pay low fees (where A would be a commit tx in LN).\n>>\n>> We had to create the CPFP carve-out rule explicitly to work around\n>> this limitation, and I think it would be necessary for package RBF\n>> as well, because in such cases we do want to be able to submit a\n>> package A + C where C pays high fees to speed up A's confirmation,\n>> regardless of unrelated unconfirmed children of A...\n>>\n>> We could submit only C to benefit from the existing CPFP carve-out\n>> rule, but that wouldn't work if our local mempool doesn't have A yet,\n>> but other remote mempools do.\n>>\n>> Is my concern justified? Is this something that we should dig into a\n>> bit deeper?\n>>\n>> Thanks,\n>> Bastien\n>>\n>> Le jeu. 16 sept. 2021 \u00e0 09:55, Gloria Zhao via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>\n>>> Hi there,\n>>>\n>>> I'm writing to propose a set of mempool policy changes to enable package\n>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>> would not\n>>> be consensus or P2P protocol changes. However, since mempool policy\n>>> significantly affects transaction propagation, I believe this is\n>>> relevant for\n>>> the mailing list.\n>>>\n>>> My proposal enables packages consisting of multiple parents and 1 child.\n>>> If you\n>>> develop software that relies on specific transaction relay assumptions\n>>> and/or\n>>> are interested in using package relay in the future, I'm very interested\n>>> to hear\n>>> your feedback on the utility or restrictiveness of these package\n>>> policies for\n>>> your use cases.\n>>>\n>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>> PR#22290][1].\n>>>\n>>> An illustrated version of this post can be found at\n>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>> I have also linked the images below.\n>>>\n>>> ## Background\n>>>\n>>> Feel free to skip this section if you are already familiar with mempool\n>>> policy\n>>> and package relay terminology.\n>>>\n>>> ### Terminology Clarifications\n>>>\n>>> * Package = an ordered list of related transactions, representable by a\n>>> Directed\n>>>   Acyclic Graph.\n>>> * Package Feerate = the total modified fees divided by the total virtual\n>>> size of\n>>>   all transactions in the package.\n>>>     - Modified fees = a transaction's base fees + fee delta applied by\n>>> the user\n>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>> across\n>>> mempools.\n>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>> [BIP141\n>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>> Core][3].\n>>>     - Note that feerate is not necessarily based on the base fees and\n>>> serialized\n>>>       size.\n>>>\n>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>> incentives to\n>>>   boost a transaction's candidacy for inclusion in a block, including\n>>> Child Pays\n>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention in\n>>> mempool policy is to recognize when the new transaction is more\n>>> economical to\n>>> mine than the original one(s) but not open DoS vectors, so there are some\n>>> limitations.\n>>>\n>>> ### Policy\n>>>\n>>> The purpose of the mempool is to store the best (to be most\n>>> incentive-compatible\n>>> with miners, highest feerate) candidates for inclusion in a block.\n>>> Miners use\n>>> the mempool to build block templates. The mempool is also useful as a\n>>> cache for\n>>> boosting block relay and validation performance, aiding transaction\n>>> relay, and\n>>> generating feerate estimations.\n>>>\n>>> Ideally, all consensus-valid transactions paying reasonable fees should\n>>> make it\n>>> to miners through normal transaction relay, without any special\n>>> connectivity or\n>>> relationships with miners. On the other hand, nodes do not have unlimited\n>>> resources, and a P2P network designed to let any honest node broadcast\n>>> their\n>>> transactions also exposes the transaction validation engine to DoS\n>>> attacks from\n>>> malicious peers.\n>>>\n>>> As such, for unconfirmed transactions we are considering for our\n>>> mempool, we\n>>> apply a set of validation rules in addition to consensus, primarily to\n>>> protect\n>>> us from resource exhaustion and aid our efforts to keep the highest fee\n>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>> node-specific) rules that transactions must abide by in order to be\n>>> accepted\n>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>> restrictions such\n>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>\n>>> ### Package Relay and Package Mempool Accept\n>>>\n>>> In transaction relay, we currently consider transactions one at a time\n>>> for\n>>> submission to the mempool. This creates a limitation in the node's\n>>> ability to\n>>> determine which transactions have the highest feerates, since we cannot\n>>> take\n>>> into account descendants (i.e. cannot use CPFP) until all the\n>>> transactions are\n>>> in the mempool. Similarly, we cannot use a transaction's descendants when\n>>> considering it for RBF. When an individual transaction does not meet the\n>>> mempool\n>>> minimum feerate and the user isn't able to create a replacement\n>>> transaction\n>>> directly, it will not be accepted by mempools.\n>>>\n>>> This limitation presents a security issue for applications and users\n>>> relying on\n>>> time-sensitive transactions. For example, Lightning and other protocols\n>>> create\n>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>> path opens\n>>> up after a timelock, and users are protected from cheating scenarios as\n>>> long as\n>>> they redeem on-chain in time. A key security assumption is that all\n>>> parties'\n>>> transactions will propagate and confirm in a timely manner. This\n>>> assumption can\n>>> be broken if fee-bumping does not work as intended.\n>>>\n>>> The end goal for Package Relay is to consider multiple transactions at\n>>> the same\n>>> time, e.g. a transaction with its high-fee child. This may help us better\n>>> determine whether transactions should be accepted to our mempool,\n>>> especially if\n>>> they don't meet fee requirements individually or are better RBF\n>>> candidates as a\n>>> package. A combination of changes to mempool validation logic, policy,\n>>> and\n>>> transaction relay allows us to better propagate the transactions with the\n>>> highest package feerates to miners, and makes fee-bumping tools more\n>>> powerful\n>>> for users.\n>>>\n>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n>>> large\n>>> part of the changes are in the mempool's package validation logic. We\n>>> call this\n>>> *Package Mempool Accept*.\n>>>\n>>> ### Previous Work\n>>>\n>>> * Given that mempool validation is DoS-sensitive and complex, it would be\n>>>   dangerous to haphazardly tack on package validation logic. Many\n>>> efforts have\n>>> been made to make mempool validation less opaque (see [#16400][4],\n>>> [#21062][5],\n>>> [#22675][6], [#22796][7]).\n>>> * [#20833][8] Added basic capabilities for package validation, test\n>>> accepts only\n>>>   (no submission to mempool).\n>>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>>> arbitrary\n>>>   packages. Still test accepts only.\n>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>\n>>> ### Existing Package Rules\n>>>\n>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>>> consider\n>>> them as \"given\" in the rest of this document, though they can be\n>>> changed, since\n>>> package validation is test-accept only right now.\n>>>\n>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>\n>>>    *Rationale*: This is already enforced as mempool ancestor/descendant\n>>> limits.\n>>> Presumably, transactions in a package are all related, so exceeding this\n>>> limit\n>>> would mean that the package can either be split up or it wouldn't pass\n>>> this\n>>> mempool policy.\n>>>\n>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>> between\n>>> transactions, parents must appear somewhere before children. [8]\n>>>\n>>> 3. A package cannot have conflicting transactions, i.e. none of them can\n>>> spend\n>>> the same inputs. This also means there cannot be duplicate transactions.\n>>> [8]\n>>>\n>>> 4. When packages are evaluated against ancestor/descendant limits in a\n>>> test\n>>> accept, the union of all of their descendants and ancestors is\n>>> considered. This\n>>> is essentially a \"worst case\" heuristic where every transaction in the\n>>> package\n>>> is treated as each other's ancestor and descendant. [8]\n>>> Packages for which ancestor/descendant limits are accurately captured by\n>>> this\n>>> heuristic: [19]\n>>>\n>>> There are also limitations such as the fact that CPFP carve out is not\n>>> applied\n>>> to package transactions. #20833 also disables RBF in package validation;\n>>> this\n>>> proposal overrides that to allow packages to use RBF.\n>>>\n>>> ## Proposed Changes\n>>>\n>>> The next step in the Package Mempool Accept project is to implement\n>>> submission\n>>> to mempool, initially through RPC only. This allows us to test the\n>>> submission\n>>> logic before exposing it on P2P.\n>>>\n>>> ### Summary\n>>>\n>>> - Packages may contain already-in-mempool transactions.\n>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>> - Fee-related checks use the package feerate. This means that wallets can\n>>> create a package that utilizes CPFP.\n>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>> similar\n>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>\n>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback\n>>> is\n>>> always welcome.\n>>>\n>>> ### Details\n>>>\n>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>\n>>> A package may contain transactions that are already in the mempool. We\n>>> remove\n>>> (\"deduplicate\") those transactions from the package for the purposes of\n>>> package\n>>> mempool acceptance. If a package is empty after deduplication, we do\n>>> nothing.\n>>>\n>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>> parent to be\n>>> accepted to the mempool of a peer on its own due to differences in\n>>> policy and\n>>> fee market fluctuations. We should not reject or penalize the entire\n>>> package for\n>>> an individual transaction as that could be a censorship vector.\n>>>\n>>> #### Packages Are Multi-Parent-1-Child\n>>>\n>>> Only packages of a specific topology are permitted. Namely, a package is\n>>> exactly\n>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>> package\n>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>> unconfirmed parents, etc. Note that it's possible for the parents to be\n>>> indirect\n>>> descendants/ancestors of one another, or for parent and child to share a\n>>> parent,\n>>> so we cannot make any other topology assumptions.\n>>>\n>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>> parents\n>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>> packages to a\n>>> defined topology is also easier to reason about and simplifies the\n>>> validation\n>>> logic greatly. Multi-parent-1-child allows us to think of the package as\n>>> one big\n>>> transaction, where:\n>>>\n>>> - Inputs = all the inputs of parents + inputs of the child that come from\n>>>   confirmed UTXOs\n>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>> that\n>>>   aren't spent by other transactions in the package\n>>>\n>>> Examples of packages that follow this rule (variations of example A show\n>>> some\n>>> possibilities after deduplication): ![image][15]\n>>>\n>>> #### Fee-Related Checks Use Package Feerate\n>>>\n>>> Package Feerate = the total modified fees divided by the total virtual\n>>> size of\n>>> all transactions in the package.\n>>>\n>>> To meet the two feerate requirements of a mempool, i.e., the\n>>> pre-configured\n>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>> feerate, the\n>>> total package feerate is used instead of the individual feerate. The\n>>> individual\n>>> transactions are allowed to be below feerate requirements if the package\n>>> meets\n>>> the feerate requirements. For example, the parent(s) in the package can\n>>> have 0\n>>> fees but be paid for by the child.\n>>>\n>>> *Rationale*: This can be thought of as \"CPFP within a package,\" solving\n>>> the\n>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>> applications to adjust their fees at broadcast time instead of\n>>> overshooting or\n>>> risking getting stuck/pinned.\n>>>\n>>> We use the package feerate of the package *after deduplication*.\n>>>\n>>> *Rationale*:  It would be incorrect to use the fees of transactions that\n>>> are\n>>> already in the mempool, as we do not want a transaction's fees to be\n>>> double-counted for both its individual RBF and package RBF.\n>>>\n>>> Examples F and G [14] show the same package, but P1 is submitted\n>>> individually before\n>>> the package in example G. In example F, we can see that the 300vB\n>>> package pays\n>>> an additional 200sat in fees, which is not enough to pay for its own\n>>> bandwidth\n>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n>>> but\n>>> using P1's fees again during package submission would make it look like\n>>> a 300sat\n>>> increase for a 200vB package. Even including its fees and size would not\n>>> be\n>>> sufficient in this example, since the 300sat looks like enough for the\n>>> 300vB\n>>> package. The calculcation after deduplication is 100sat increase for a\n>>> package\n>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>>> have a\n>>> size of 100vB.\n>>>\n>>> #### Package RBF\n>>>\n>>> If a package meets feerate requirements as a package, the parents in the\n>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>> child cannot\n>>> replace mempool transactions. Multiple transactions can replace the same\n>>> transaction, but in order to be valid, none of the transactions can try\n>>> to\n>>> replace an ancestor of another transaction in the same package (which\n>>> would thus\n>>> make its inputs unavailable).\n>>>\n>>> *Rationale*: Even if we are using package feerate, a package will not\n>>> propagate\n>>> as intended if RBF still requires each individual transaction to meet the\n>>> feerate requirements.\n>>>\n>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>\n>>> ##### Signaling (Rule #1)\n>>>\n>>> All mempool transactions to be replaced must signal replaceability.\n>>>\n>>> *Rationale*: Package RBF signaling logic should be the same for package\n>>> RBF and\n>>> single transaction acceptance. This would be updated if single\n>>> transaction\n>>> validation moves to full RBF.\n>>>\n>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>\n>>> A package may include new unconfirmed inputs, but the ancestor feerate\n>>> of the\n>>> child must be at least as high as the ancestor feerates of every\n>>> transaction\n>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>> replacement\n>>> transaction may only include an unconfirmed input if that input was\n>>> included in\n>>> one of the original transactions. (An unconfirmed input spends an output\n>>> from a\n>>> currently-unconfirmed transaction.)\"\n>>>\n>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>> transaction has a higher ancestor score than the original transaction(s)\n>>> (see\n>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n>>> can lower the\n>>> ancestor score of the replacement transaction. P1 is trying to replace\n>>> M1, and\n>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and\n>>> M2 pays\n>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>> isolation, P1\n>>> looks like a better mining candidate than M1, it must be mined with M2,\n>>> so its\n>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's ancestor\n>>> feerate, which is 6sat/vB.\n>>>\n>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>> transactions in the package can spend new unconfirmed inputs.\" Example J\n>>> [17] shows\n>>> why, if any of the package transactions have ancestors, package feerate\n>>> is no\n>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which is\n>>> the\n>>> replacement transaction in an RBF), we're actually interested in the\n>>> entire\n>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n>>> P2, and\n>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>> only allow\n>>> the child to have new unconfirmed inputs, either, because it can still\n>>> cause us\n>>> to overestimate the package's ancestor score.\n>>>\n>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>> Package RBF\n>>> less useful, but would also break Package RBF for packages with parents\n>>> already\n>>> in the mempool: if a package parent has already been submitted, it would\n>>> look\n>>> like the child is spending a \"new\" unconfirmed input. In example K [18],\n>>> we're\n>>> looking to replace M1 with the entire package including P1, P2, and P3.\n>>> We must\n>>> consider the case where one of the parents is already in the mempool (in\n>>> this\n>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n>>> However,\n>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>> replace M1\n>>> with this package.\n>>>\n>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>> strict than\n>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>> replacement\n>>> transactions to have a ancestor score at least as high as the original\n>>> ones. As\n>>> a result, the entire package is required to be a higher feerate mining\n>>> candidate\n>>> than each of the replaced transactions.\n>>>\n>>> Another note: the [comment][13] above the BIP125#2 code in the original\n>>> RBF\n>>> implementation suggests that the rule was intended to be temporary.\n>>>\n>>> ##### Absolute Fee (Rule #3)\n>>>\n>>> The package must increase the absolute fee of the mempool, i.e. the\n>>> total fees\n>>> of the package must be higher than the absolute fees of the mempool\n>>> transactions\n>>> it replaces. Combined with the CPFP rule above, this differs from BIP125\n>>> Rule #3\n>>> - an individual transaction in the package may have lower fees than the\n>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>>> child\n>>> pays for RBF.\n>>>\n>>> ##### Feerate (Rule #4)\n>>>\n>>> The package must pay for its own bandwidth; the package feerate must be\n>>> higher\n>>> than the replaced transactions by at least minimum relay feerate\n>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this differs\n>>> from\n>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>> lower\n>>> feerate than the transaction(s) it is replacing. In fact, it may have 0\n>>> fees,\n>>> and the child pays for RBF.\n>>>\n>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>\n>>> The package cannot replace more than 100 mempool transactions. This is\n>>> identical\n>>> to BIP125 Rule #5.\n>>>\n>>> ### Expected FAQs\n>>>\n>>> 1. Is it possible for only some of the package to make it into the\n>>> mempool?\n>>>\n>>>    Yes, it is. However, since we evict transactions from the mempool by\n>>> descendant score and the package child is supposed to be sponsoring the\n>>> fees of\n>>> its parents, the most common scenario would be all-or-nothing. This is\n>>> incentive-compatible. In fact, to be conservative, package validation\n>>> should\n>>> begin by trying to submit all of the transactions individually, and only\n>>> use the\n>>> package mempool acceptance logic if the parents fail due to low feerate.\n>>>\n>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>\n>>>     No, for practical reasons. In mempool validation, we actually aren't\n>>> able to\n>>> tell with 100% confidence if we are looking at a transaction that has\n>>> already\n>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>> historical\n>>> block data, it's possible to look for it, but this is inefficient, not\n>>> always\n>>> possible for pruning nodes, and unnecessary because we're not going to do\n>>> anything with the transaction anyway. As such, we already have the\n>>> expectation\n>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>> relaying\n>>> transactions that have already been confirmed. Similarly, we shouldn't be\n>>> relaying packages that contain already-confirmed transactions.\n>>>\n>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>> [2]:\n>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>> [3]:\n>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>> [13]:\n>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>> [14]:\n>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>> [15]:\n>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>> [16]:\n>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>> [17]:\n>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>> [18]:\n>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>> [19]:\n>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>> [20]:\n>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/464a6299/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2021-09-21T16:42:33",
                "message_text_only": "Hi Bastien,\n\nExcellent diagram :D\n\n> Here the issue is that a revoked commitment tx A' is pinned in other\n> mempools, with a long chain of descendants (or descendants that reach\n> the maximum replaceable size).\n> We would really like A + C to be able to replace this pinned A'.\n> We can't submit individually because A on its own won't replace A'...\n\nRight, this is a key motivation for having Package RBF. In this case, A+C\ncan replace A' + B1...B24.\n\nDue to the descendant limit (each node operator can increase it on their\nown node, but the default is 25), A' should have no more than 25\ndescendants, even including CPFP carve out. As long as A only conflicts\nwith A', it won't be trying to replace more than 100 transactions. The\nproposed package RBF will allow C to pay for A's conflicts, since their\npackage feerate is used in the fee comparisons. A is not a descendant of\nA', so the existence of B1...B24 does not prevent the replacement.\n\nBest,\nGloria\n\nOn Tue, Sep 21, 2021 at 4:18 PM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Hi Gloria,\n>\n> > I believe this attack is mitigated as long as we attempt to submit\n> transactions individually\n>\n> Unfortunately not, as there exists a pinning scenario in LN where a\n> different commit tx is pinned, but you actually can't know which one.\n>\n> Since I really like your diagrams, I made one as well to illustrate:\n>\n> https://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg\n>\n> Here the issue is that a revoked commitment tx A' is pinned in other\n> mempools, with a long chain of descendants (or descendants that reach\n> the maximum replaceable size).\n>\n> We would really like A + C to be able to replace this pinned A'.\n> We can't submit individually because A on its own won't replace A'...\n>\n> > I would note that this proposal doesn't accommodate something like\n> diagram B, where C is getting CPFP carve out and wants to bring a +1\n>\n> No worries, that case shouldn't be a concern.\n> I believe any L2 protocol can always ensure it confirms such tx trees\n> \"one depth after the other\" without impacting funds safety, so it\n> only needs to ensure A + C can get into mempools.\n>\n> Thanks,\n> Bastien\n>\n> Le mar. 21 sept. 2021 \u00e0 13:18, Gloria Zhao <gloriajzhao at gmail.com> a\n> \u00e9crit :\n>\n>> Hi Bastien,\n>>\n>> Thank you for your feedback!\n>>\n>> > In your example we have a parent transaction A already in the mempool\n>> > and an unrelated child B. We submit a package C + D where C spends\n>> > another of A's inputs. You're highlighting that this package may be\n>> > rejected because of the unrelated transaction(s) B.\n>>\n>> > The way I see this, an attacker can abuse this rule to ensure\n>> > transaction A stays pinned in the mempool without confirming by\n>> > broadcasting a set of child transactions that reach these limits\n>> > and pay low fees (where A would be a commit tx in LN).\n>>\n>> I believe you are describing a pinning attack in which your adversarial\n>> counterparty attempts to monopolize the mempool descendant limit of the\n>> shared  transaction A in order to prevent you from submitting a fee-bumping\n>> child C; I've tried to illustrate this as diagram A here:\n>> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png\n>> (please let me know if I'm misunderstanding).\n>>\n>> I believe this attack is mitigated as long as we attempt to submit\n>> transactions individually (and thus take advantage of CPFP carve out)\n>> before attempting package validation. So, in scenario A2, even if the\n>> mempool receives a package with A+C, it would deduplicate A, submit C as an\n>> individual transaction, and allow it due to the CPFP carve out exemption. A\n>> more general goal is: if a transaction would propagate successfully on its\n>> own now, it should still propagate regardless of whether it is included in\n>> a package. The best way to ensure this, as far as I can tell, is to always\n>> try to submit them individually first.\n>>\n>> I would note that this proposal doesn't accommodate something like\n>> diagram B, where C is getting CPFP carve out and wants to bring a +1 (e.g.\n>> C has very low fees and is bumped by D). I don't think this is a use case\n>> since C should be the one fee-bumping A, but since we're talking about\n>> limitations around the CPFP carve out, this is it.\n>>\n>> Let me know if this addresses your concerns?\n>>\n>> Thanks,\n>> Gloria\n>>\n>> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>\n>> wrote:\n>>\n>>> Hi Gloria,\n>>>\n>>> Thanks for this detailed post!\n>>>\n>>> The illustrations you provided are very useful for this kind of graph\n>>> topology problems.\n>>>\n>>> The rules you lay out for package RBF look good to me at first glance\n>>> as there are some subtle improvements compared to BIP 125.\n>>>\n>>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>\n>>> I have a question regarding this rule, as your example 2C could be\n>>> concerning for LN (unless I didn't understand it correctly).\n>>>\n>>> This also touches on the package RBF rule 5 (\"The package cannot\n>>> replace more than 100 mempool transactions.\")\n>>>\n>>> In your example we have a parent transaction A already in the mempool\n>>> and an unrelated child B. We submit a package C + D where C spends\n>>> another of A's inputs. You're highlighting that this package may be\n>>> rejected because of the unrelated transaction(s) B.\n>>>\n>>> The way I see this, an attacker can abuse this rule to ensure\n>>> transaction A stays pinned in the mempool without confirming by\n>>> broadcasting a set of child transactions that reach these limits\n>>> and pay low fees (where A would be a commit tx in LN).\n>>>\n>>> We had to create the CPFP carve-out rule explicitly to work around\n>>> this limitation, and I think it would be necessary for package RBF\n>>> as well, because in such cases we do want to be able to submit a\n>>> package A + C where C pays high fees to speed up A's confirmation,\n>>> regardless of unrelated unconfirmed children of A...\n>>>\n>>> We could submit only C to benefit from the existing CPFP carve-out\n>>> rule, but that wouldn't work if our local mempool doesn't have A yet,\n>>> but other remote mempools do.\n>>>\n>>> Is my concern justified? Is this something that we should dig into a\n>>> bit deeper?\n>>>\n>>> Thanks,\n>>> Bastien\n>>>\n>>> Le jeu. 16 sept. 2021 \u00e0 09:55, Gloria Zhao via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>\n>>>> Hi there,\n>>>>\n>>>> I'm writing to propose a set of mempool policy changes to enable package\n>>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>>> would not\n>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>> significantly affects transaction propagation, I believe this is\n>>>> relevant for\n>>>> the mailing list.\n>>>>\n>>>> My proposal enables packages consisting of multiple parents and 1\n>>>> child. If you\n>>>> develop software that relies on specific transaction relay assumptions\n>>>> and/or\n>>>> are interested in using package relay in the future, I'm very\n>>>> interested to hear\n>>>> your feedback on the utility or restrictiveness of these package\n>>>> policies for\n>>>> your use cases.\n>>>>\n>>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>>> PR#22290][1].\n>>>>\n>>>> An illustrated version of this post can be found at\n>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>> I have also linked the images below.\n>>>>\n>>>> ## Background\n>>>>\n>>>> Feel free to skip this section if you are already familiar with mempool\n>>>> policy\n>>>> and package relay terminology.\n>>>>\n>>>> ### Terminology Clarifications\n>>>>\n>>>> * Package = an ordered list of related transactions, representable by a\n>>>> Directed\n>>>>   Acyclic Graph.\n>>>> * Package Feerate = the total modified fees divided by the total\n>>>> virtual size of\n>>>>   all transactions in the package.\n>>>>     - Modified fees = a transaction's base fees + fee delta applied by\n>>>> the user\n>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>> across\n>>>> mempools.\n>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>> [BIP141\n>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>>> Core][3].\n>>>>     - Note that feerate is not necessarily based on the base fees and\n>>>> serialized\n>>>>       size.\n>>>>\n>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>> incentives to\n>>>>   boost a transaction's candidacy for inclusion in a block, including\n>>>> Child Pays\n>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention\n>>>> in\n>>>> mempool policy is to recognize when the new transaction is more\n>>>> economical to\n>>>> mine than the original one(s) but not open DoS vectors, so there are\n>>>> some\n>>>> limitations.\n>>>>\n>>>> ### Policy\n>>>>\n>>>> The purpose of the mempool is to store the best (to be most\n>>>> incentive-compatible\n>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>> Miners use\n>>>> the mempool to build block templates. The mempool is also useful as a\n>>>> cache for\n>>>> boosting block relay and validation performance, aiding transaction\n>>>> relay, and\n>>>> generating feerate estimations.\n>>>>\n>>>> Ideally, all consensus-valid transactions paying reasonable fees should\n>>>> make it\n>>>> to miners through normal transaction relay, without any special\n>>>> connectivity or\n>>>> relationships with miners. On the other hand, nodes do not have\n>>>> unlimited\n>>>> resources, and a P2P network designed to let any honest node broadcast\n>>>> their\n>>>> transactions also exposes the transaction validation engine to DoS\n>>>> attacks from\n>>>> malicious peers.\n>>>>\n>>>> As such, for unconfirmed transactions we are considering for our\n>>>> mempool, we\n>>>> apply a set of validation rules in addition to consensus, primarily to\n>>>> protect\n>>>> us from resource exhaustion and aid our efforts to keep the highest fee\n>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>> node-specific) rules that transactions must abide by in order to be\n>>>> accepted\n>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>> restrictions such\n>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>\n>>>> ### Package Relay and Package Mempool Accept\n>>>>\n>>>> In transaction relay, we currently consider transactions one at a time\n>>>> for\n>>>> submission to the mempool. This creates a limitation in the node's\n>>>> ability to\n>>>> determine which transactions have the highest feerates, since we cannot\n>>>> take\n>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>> transactions are\n>>>> in the mempool. Similarly, we cannot use a transaction's descendants\n>>>> when\n>>>> considering it for RBF. When an individual transaction does not meet\n>>>> the mempool\n>>>> minimum feerate and the user isn't able to create a replacement\n>>>> transaction\n>>>> directly, it will not be accepted by mempools.\n>>>>\n>>>> This limitation presents a security issue for applications and users\n>>>> relying on\n>>>> time-sensitive transactions. For example, Lightning and other protocols\n>>>> create\n>>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>>> path opens\n>>>> up after a timelock, and users are protected from cheating scenarios as\n>>>> long as\n>>>> they redeem on-chain in time. A key security assumption is that all\n>>>> parties'\n>>>> transactions will propagate and confirm in a timely manner. This\n>>>> assumption can\n>>>> be broken if fee-bumping does not work as intended.\n>>>>\n>>>> The end goal for Package Relay is to consider multiple transactions at\n>>>> the same\n>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>> better\n>>>> determine whether transactions should be accepted to our mempool,\n>>>> especially if\n>>>> they don't meet fee requirements individually or are better RBF\n>>>> candidates as a\n>>>> package. A combination of changes to mempool validation logic, policy,\n>>>> and\n>>>> transaction relay allows us to better propagate the transactions with\n>>>> the\n>>>> highest package feerates to miners, and makes fee-bumping tools more\n>>>> powerful\n>>>> for users.\n>>>>\n>>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but a\n>>>> large\n>>>> part of the changes are in the mempool's package validation logic. We\n>>>> call this\n>>>> *Package Mempool Accept*.\n>>>>\n>>>> ### Previous Work\n>>>>\n>>>> * Given that mempool validation is DoS-sensitive and complex, it would\n>>>> be\n>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>> efforts have\n>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>> [#21062][5],\n>>>> [#22675][6], [#22796][7]).\n>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>> accepts only\n>>>>   (no submission to mempool).\n>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>>>> arbitrary\n>>>>   packages. Still test accepts only.\n>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>\n>>>> ### Existing Package Rules\n>>>>\n>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>>>> consider\n>>>> them as \"given\" in the rest of this document, though they can be\n>>>> changed, since\n>>>> package validation is test-accept only right now.\n>>>>\n>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>\n>>>>    *Rationale*: This is already enforced as mempool ancestor/descendant\n>>>> limits.\n>>>> Presumably, transactions in a package are all related, so exceeding\n>>>> this limit\n>>>> would mean that the package can either be split up or it wouldn't pass\n>>>> this\n>>>> mempool policy.\n>>>>\n>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>> between\n>>>> transactions, parents must appear somewhere before children. [8]\n>>>>\n>>>> 3. A package cannot have conflicting transactions, i.e. none of them\n>>>> can spend\n>>>> the same inputs. This also means there cannot be duplicate\n>>>> transactions. [8]\n>>>>\n>>>> 4. When packages are evaluated against ancestor/descendant limits in a\n>>>> test\n>>>> accept, the union of all of their descendants and ancestors is\n>>>> considered. This\n>>>> is essentially a \"worst case\" heuristic where every transaction in the\n>>>> package\n>>>> is treated as each other's ancestor and descendant. [8]\n>>>> Packages for which ancestor/descendant limits are accurately captured\n>>>> by this\n>>>> heuristic: [19]\n>>>>\n>>>> There are also limitations such as the fact that CPFP carve out is not\n>>>> applied\n>>>> to package transactions. #20833 also disables RBF in package\n>>>> validation; this\n>>>> proposal overrides that to allow packages to use RBF.\n>>>>\n>>>> ## Proposed Changes\n>>>>\n>>>> The next step in the Package Mempool Accept project is to implement\n>>>> submission\n>>>> to mempool, initially through RPC only. This allows us to test the\n>>>> submission\n>>>> logic before exposing it on P2P.\n>>>>\n>>>> ### Summary\n>>>>\n>>>> - Packages may contain already-in-mempool transactions.\n>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>> - Fee-related checks use the package feerate. This means that wallets\n>>>> can\n>>>> create a package that utilizes CPFP.\n>>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>>> similar\n>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>\n>>>> There is a draft implementation in [#22290][1]. It is WIP, but feedback\n>>>> is\n>>>> always welcome.\n>>>>\n>>>> ### Details\n>>>>\n>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>\n>>>> A package may contain transactions that are already in the mempool. We\n>>>> remove\n>>>> (\"deduplicate\") those transactions from the package for the purposes of\n>>>> package\n>>>> mempool acceptance. If a package is empty after deduplication, we do\n>>>> nothing.\n>>>>\n>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>> parent to be\n>>>> accepted to the mempool of a peer on its own due to differences in\n>>>> policy and\n>>>> fee market fluctuations. We should not reject or penalize the entire\n>>>> package for\n>>>> an individual transaction as that could be a censorship vector.\n>>>>\n>>>> #### Packages Are Multi-Parent-1-Child\n>>>>\n>>>> Only packages of a specific topology are permitted. Namely, a package\n>>>> is exactly\n>>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>>> package\n>>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>>> unconfirmed parents, etc. Note that it's possible for the parents to be\n>>>> indirect\n>>>> descendants/ancestors of one another, or for parent and child to share\n>>>> a parent,\n>>>> so we cannot make any other topology assumptions.\n>>>>\n>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>> parents\n>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>> packages to a\n>>>> defined topology is also easier to reason about and simplifies the\n>>>> validation\n>>>> logic greatly. Multi-parent-1-child allows us to think of the package\n>>>> as one big\n>>>> transaction, where:\n>>>>\n>>>> - Inputs = all the inputs of parents + inputs of the child that come\n>>>> from\n>>>>   confirmed UTXOs\n>>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>>> that\n>>>>   aren't spent by other transactions in the package\n>>>>\n>>>> Examples of packages that follow this rule (variations of example A\n>>>> show some\n>>>> possibilities after deduplication): ![image][15]\n>>>>\n>>>> #### Fee-Related Checks Use Package Feerate\n>>>>\n>>>> Package Feerate = the total modified fees divided by the total virtual\n>>>> size of\n>>>> all transactions in the package.\n>>>>\n>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>> pre-configured\n>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>> feerate, the\n>>>> total package feerate is used instead of the individual feerate. The\n>>>> individual\n>>>> transactions are allowed to be below feerate requirements if the\n>>>> package meets\n>>>> the feerate requirements. For example, the parent(s) in the package can\n>>>> have 0\n>>>> fees but be paid for by the child.\n>>>>\n>>>> *Rationale*: This can be thought of as \"CPFP within a package,\" solving\n>>>> the\n>>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>>> applications to adjust their fees at broadcast time instead of\n>>>> overshooting or\n>>>> risking getting stuck/pinned.\n>>>>\n>>>> We use the package feerate of the package *after deduplication*.\n>>>>\n>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>> that are\n>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>> double-counted for both its individual RBF and package RBF.\n>>>>\n>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>> individually before\n>>>> the package in example G. In example F, we can see that the 300vB\n>>>> package pays\n>>>> an additional 200sat in fees, which is not enough to pay for its own\n>>>> bandwidth\n>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace M1,\n>>>> but\n>>>> using P1's fees again during package submission would make it look like\n>>>> a 300sat\n>>>> increase for a 200vB package. Even including its fees and size would\n>>>> not be\n>>>> sufficient in this example, since the 300sat looks like enough for the\n>>>> 300vB\n>>>> package. The calculcation after deduplication is 100sat increase for a\n>>>> package\n>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>>>> have a\n>>>> size of 100vB.\n>>>>\n>>>> #### Package RBF\n>>>>\n>>>> If a package meets feerate requirements as a package, the parents in the\n>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>> child cannot\n>>>> replace mempool transactions. Multiple transactions can replace the same\n>>>> transaction, but in order to be valid, none of the transactions can try\n>>>> to\n>>>> replace an ancestor of another transaction in the same package (which\n>>>> would thus\n>>>> make its inputs unavailable).\n>>>>\n>>>> *Rationale*: Even if we are using package feerate, a package will not\n>>>> propagate\n>>>> as intended if RBF still requires each individual transaction to meet\n>>>> the\n>>>> feerate requirements.\n>>>>\n>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>\n>>>> ##### Signaling (Rule #1)\n>>>>\n>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>\n>>>> *Rationale*: Package RBF signaling logic should be the same for package\n>>>> RBF and\n>>>> single transaction acceptance. This would be updated if single\n>>>> transaction\n>>>> validation moves to full RBF.\n>>>>\n>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>\n>>>> A package may include new unconfirmed inputs, but the ancestor feerate\n>>>> of the\n>>>> child must be at least as high as the ancestor feerates of every\n>>>> transaction\n>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>> replacement\n>>>> transaction may only include an unconfirmed input if that input was\n>>>> included in\n>>>> one of the original transactions. (An unconfirmed input spends an\n>>>> output from a\n>>>> currently-unconfirmed transaction.)\"\n>>>>\n>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>>> transaction has a higher ancestor score than the original\n>>>> transaction(s) (see\n>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed input\n>>>> can lower the\n>>>> ancestor score of the replacement transaction. P1 is trying to replace\n>>>> M1, and\n>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat, and\n>>>> M2 pays\n>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>> isolation, P1\n>>>> looks like a better mining candidate than M1, it must be mined with M2,\n>>>> so its\n>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>> ancestor\n>>>> feerate, which is 6sat/vB.\n>>>>\n>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>> transactions in the package can spend new unconfirmed inputs.\" Example\n>>>> J [17] shows\n>>>> why, if any of the package transactions have ancestors, package feerate\n>>>> is no\n>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which\n>>>> is the\n>>>> replacement transaction in an RBF), we're actually interested in the\n>>>> entire\n>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3, P1,\n>>>> P2, and\n>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>>> only allow\n>>>> the child to have new unconfirmed inputs, either, because it can still\n>>>> cause us\n>>>> to overestimate the package's ancestor score.\n>>>>\n>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>> Package RBF\n>>>> less useful, but would also break Package RBF for packages with parents\n>>>> already\n>>>> in the mempool: if a package parent has already been submitted, it\n>>>> would look\n>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>> [18], we're\n>>>> looking to replace M1 with the entire package including P1, P2, and P3.\n>>>> We must\n>>>> consider the case where one of the parents is already in the mempool\n>>>> (in this\n>>>> case, P2), which means we must allow P3 to have new unconfirmed inputs.\n>>>> However,\n>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>> replace M1\n>>>> with this package.\n>>>>\n>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>> strict than\n>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>> replacement\n>>>> transactions to have a ancestor score at least as high as the original\n>>>> ones. As\n>>>> a result, the entire package is required to be a higher feerate mining\n>>>> candidate\n>>>> than each of the replaced transactions.\n>>>>\n>>>> Another note: the [comment][13] above the BIP125#2 code in the original\n>>>> RBF\n>>>> implementation suggests that the rule was intended to be temporary.\n>>>>\n>>>> ##### Absolute Fee (Rule #3)\n>>>>\n>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>> total fees\n>>>> of the package must be higher than the absolute fees of the mempool\n>>>> transactions\n>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>> BIP125 Rule #3\n>>>> - an individual transaction in the package may have lower fees than the\n>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>>>> child\n>>>> pays for RBF.\n>>>>\n>>>> ##### Feerate (Rule #4)\n>>>>\n>>>> The package must pay for its own bandwidth; the package feerate must be\n>>>> higher\n>>>> than the replaced transactions by at least minimum relay feerate\n>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>> differs from\n>>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>>> lower\n>>>> feerate than the transaction(s) it is replacing. In fact, it may have 0\n>>>> fees,\n>>>> and the child pays for RBF.\n>>>>\n>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>\n>>>> The package cannot replace more than 100 mempool transactions. This is\n>>>> identical\n>>>> to BIP125 Rule #5.\n>>>>\n>>>> ### Expected FAQs\n>>>>\n>>>> 1. Is it possible for only some of the package to make it into the\n>>>> mempool?\n>>>>\n>>>>    Yes, it is. However, since we evict transactions from the mempool by\n>>>> descendant score and the package child is supposed to be sponsoring the\n>>>> fees of\n>>>> its parents, the most common scenario would be all-or-nothing. This is\n>>>> incentive-compatible. In fact, to be conservative, package validation\n>>>> should\n>>>> begin by trying to submit all of the transactions individually, and\n>>>> only use the\n>>>> package mempool acceptance logic if the parents fail due to low feerate.\n>>>>\n>>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>>\n>>>>     No, for practical reasons. In mempool validation, we actually\n>>>> aren't able to\n>>>> tell with 100% confidence if we are looking at a transaction that has\n>>>> already\n>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>> historical\n>>>> block data, it's possible to look for it, but this is inefficient, not\n>>>> always\n>>>> possible for pruning nodes, and unnecessary because we're not going to\n>>>> do\n>>>> anything with the transaction anyway. As such, we already have the\n>>>> expectation\n>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>> relaying\n>>>> transactions that have already been confirmed. Similarly, we shouldn't\n>>>> be\n>>>> relaying packages that contain already-confirmed transactions.\n>>>>\n>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>> [2]:\n>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>> [3]:\n>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>> [13]:\n>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>> [14]:\n>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>> [15]:\n>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>> [16]:\n>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>> [17]:\n>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>> [18]:\n>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>> [19]:\n>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>> [20]:\n>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210921/2519ec71/attachment-0001.html>"
            },
            {
                "author": "Bastien TEINTURIER",
                "date": "2021-09-22T07:10:38",
                "message_text_only": "Great, thanks for this clarification!\n\nCan you confirm that this won't be an issue either with your\nexample 2C (in your first set of diagrams)? If I understand it\ncorrectly it shouldn't, but I'd rather be 100% sure.\n\nA package A + C will be able to replace A' + B regardless of\nthe weight of A' + B?\n\nThanks,\nBastien\n\nLe mar. 21 sept. 2021 \u00e0 18:42, Gloria Zhao <gloriajzhao at gmail.com> a \u00e9crit :\n\n> Hi Bastien,\n>\n> Excellent diagram :D\n>\n> > Here the issue is that a revoked commitment tx A' is pinned in other\n> > mempools, with a long chain of descendants (or descendants that reach\n> > the maximum replaceable size).\n> > We would really like A + C to be able to replace this pinned A'.\n> > We can't submit individually because A on its own won't replace A'...\n>\n> Right, this is a key motivation for having Package RBF. In this case, A+C\n> can replace A' + B1...B24.\n>\n> Due to the descendant limit (each node operator can increase it on their\n> own node, but the default is 25), A' should have no more than 25\n> descendants, even including CPFP carve out. As long as A only conflicts\n> with A', it won't be trying to replace more than 100 transactions. The\n> proposed package RBF will allow C to pay for A's conflicts, since their\n> package feerate is used in the fee comparisons. A is not a descendant of\n> A', so the existence of B1...B24 does not prevent the replacement.\n>\n> Best,\n> Gloria\n>\n> On Tue, Sep 21, 2021 at 4:18 PM Bastien TEINTURIER <bastien at acinq.fr>\n> wrote:\n>\n>> Hi Gloria,\n>>\n>> > I believe this attack is mitigated as long as we attempt to submit\n>> transactions individually\n>>\n>> Unfortunately not, as there exists a pinning scenario in LN where a\n>> different commit tx is pinned, but you actually can't know which one.\n>>\n>> Since I really like your diagrams, I made one as well to illustrate:\n>>\n>> https://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg\n>>\n>> Here the issue is that a revoked commitment tx A' is pinned in other\n>> mempools, with a long chain of descendants (or descendants that reach\n>> the maximum replaceable size).\n>>\n>> We would really like A + C to be able to replace this pinned A'.\n>> We can't submit individually because A on its own won't replace A'...\n>>\n>> > I would note that this proposal doesn't accommodate something like\n>> diagram B, where C is getting CPFP carve out and wants to bring a +1\n>>\n>> No worries, that case shouldn't be a concern.\n>> I believe any L2 protocol can always ensure it confirms such tx trees\n>> \"one depth after the other\" without impacting funds safety, so it\n>> only needs to ensure A + C can get into mempools.\n>>\n>> Thanks,\n>> Bastien\n>>\n>> Le mar. 21 sept. 2021 \u00e0 13:18, Gloria Zhao <gloriajzhao at gmail.com> a\n>> \u00e9crit :\n>>\n>>> Hi Bastien,\n>>>\n>>> Thank you for your feedback!\n>>>\n>>> > In your example we have a parent transaction A already in the mempool\n>>> > and an unrelated child B. We submit a package C + D where C spends\n>>> > another of A's inputs. You're highlighting that this package may be\n>>> > rejected because of the unrelated transaction(s) B.\n>>>\n>>> > The way I see this, an attacker can abuse this rule to ensure\n>>> > transaction A stays pinned in the mempool without confirming by\n>>> > broadcasting a set of child transactions that reach these limits\n>>> > and pay low fees (where A would be a commit tx in LN).\n>>>\n>>> I believe you are describing a pinning attack in which your adversarial\n>>> counterparty attempts to monopolize the mempool descendant limit of the\n>>> shared  transaction A in order to prevent you from submitting a fee-bumping\n>>> child C; I've tried to illustrate this as diagram A here:\n>>> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png\n>>> (please let me know if I'm misunderstanding).\n>>>\n>>> I believe this attack is mitigated as long as we attempt to submit\n>>> transactions individually (and thus take advantage of CPFP carve out)\n>>> before attempting package validation. So, in scenario A2, even if the\n>>> mempool receives a package with A+C, it would deduplicate A, submit C as an\n>>> individual transaction, and allow it due to the CPFP carve out exemption. A\n>>> more general goal is: if a transaction would propagate successfully on its\n>>> own now, it should still propagate regardless of whether it is included in\n>>> a package. The best way to ensure this, as far as I can tell, is to always\n>>> try to submit them individually first.\n>>>\n>>> I would note that this proposal doesn't accommodate something like\n>>> diagram B, where C is getting CPFP carve out and wants to bring a +1 (e.g.\n>>> C has very low fees and is bumped by D). I don't think this is a use case\n>>> since C should be the one fee-bumping A, but since we're talking about\n>>> limitations around the CPFP carve out, this is it.\n>>>\n>>> Let me know if this addresses your concerns?\n>>>\n>>> Thanks,\n>>> Gloria\n>>>\n>>> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>\n>>> wrote:\n>>>\n>>>> Hi Gloria,\n>>>>\n>>>> Thanks for this detailed post!\n>>>>\n>>>> The illustrations you provided are very useful for this kind of graph\n>>>> topology problems.\n>>>>\n>>>> The rules you lay out for package RBF look good to me at first glance\n>>>> as there are some subtle improvements compared to BIP 125.\n>>>>\n>>>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>\n>>>> I have a question regarding this rule, as your example 2C could be\n>>>> concerning for LN (unless I didn't understand it correctly).\n>>>>\n>>>> This also touches on the package RBF rule 5 (\"The package cannot\n>>>> replace more than 100 mempool transactions.\")\n>>>>\n>>>> In your example we have a parent transaction A already in the mempool\n>>>> and an unrelated child B. We submit a package C + D where C spends\n>>>> another of A's inputs. You're highlighting that this package may be\n>>>> rejected because of the unrelated transaction(s) B.\n>>>>\n>>>> The way I see this, an attacker can abuse this rule to ensure\n>>>> transaction A stays pinned in the mempool without confirming by\n>>>> broadcasting a set of child transactions that reach these limits\n>>>> and pay low fees (where A would be a commit tx in LN).\n>>>>\n>>>> We had to create the CPFP carve-out rule explicitly to work around\n>>>> this limitation, and I think it would be necessary for package RBF\n>>>> as well, because in such cases we do want to be able to submit a\n>>>> package A + C where C pays high fees to speed up A's confirmation,\n>>>> regardless of unrelated unconfirmed children of A...\n>>>>\n>>>> We could submit only C to benefit from the existing CPFP carve-out\n>>>> rule, but that wouldn't work if our local mempool doesn't have A yet,\n>>>> but other remote mempools do.\n>>>>\n>>>> Is my concern justified? Is this something that we should dig into a\n>>>> bit deeper?\n>>>>\n>>>> Thanks,\n>>>> Bastien\n>>>>\n>>>> Le jeu. 16 sept. 2021 \u00e0 09:55, Gloria Zhao via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>\n>>>>> Hi there,\n>>>>>\n>>>>> I'm writing to propose a set of mempool policy changes to enable\n>>>>> package\n>>>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>>>> would not\n>>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>>> significantly affects transaction propagation, I believe this is\n>>>>> relevant for\n>>>>> the mailing list.\n>>>>>\n>>>>> My proposal enables packages consisting of multiple parents and 1\n>>>>> child. If you\n>>>>> develop software that relies on specific transaction relay assumptions\n>>>>> and/or\n>>>>> are interested in using package relay in the future, I'm very\n>>>>> interested to hear\n>>>>> your feedback on the utility or restrictiveness of these package\n>>>>> policies for\n>>>>> your use cases.\n>>>>>\n>>>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>>>> PR#22290][1].\n>>>>>\n>>>>> An illustrated version of this post can be found at\n>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>>> I have also linked the images below.\n>>>>>\n>>>>> ## Background\n>>>>>\n>>>>> Feel free to skip this section if you are already familiar with\n>>>>> mempool policy\n>>>>> and package relay terminology.\n>>>>>\n>>>>> ### Terminology Clarifications\n>>>>>\n>>>>> * Package = an ordered list of related transactions, representable by\n>>>>> a Directed\n>>>>>   Acyclic Graph.\n>>>>> * Package Feerate = the total modified fees divided by the total\n>>>>> virtual size of\n>>>>>   all transactions in the package.\n>>>>>     - Modified fees = a transaction's base fees + fee delta applied by\n>>>>> the user\n>>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>>> across\n>>>>> mempools.\n>>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>>> [BIP141\n>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>>>> Core][3].\n>>>>>     - Note that feerate is not necessarily based on the base fees and\n>>>>> serialized\n>>>>>       size.\n>>>>>\n>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>>> incentives to\n>>>>>   boost a transaction's candidacy for inclusion in a block, including\n>>>>> Child Pays\n>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our intention\n>>>>> in\n>>>>> mempool policy is to recognize when the new transaction is more\n>>>>> economical to\n>>>>> mine than the original one(s) but not open DoS vectors, so there are\n>>>>> some\n>>>>> limitations.\n>>>>>\n>>>>> ### Policy\n>>>>>\n>>>>> The purpose of the mempool is to store the best (to be most\n>>>>> incentive-compatible\n>>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>>> Miners use\n>>>>> the mempool to build block templates. The mempool is also useful as a\n>>>>> cache for\n>>>>> boosting block relay and validation performance, aiding transaction\n>>>>> relay, and\n>>>>> generating feerate estimations.\n>>>>>\n>>>>> Ideally, all consensus-valid transactions paying reasonable fees\n>>>>> should make it\n>>>>> to miners through normal transaction relay, without any special\n>>>>> connectivity or\n>>>>> relationships with miners. On the other hand, nodes do not have\n>>>>> unlimited\n>>>>> resources, and a P2P network designed to let any honest node broadcast\n>>>>> their\n>>>>> transactions also exposes the transaction validation engine to DoS\n>>>>> attacks from\n>>>>> malicious peers.\n>>>>>\n>>>>> As such, for unconfirmed transactions we are considering for our\n>>>>> mempool, we\n>>>>> apply a set of validation rules in addition to consensus, primarily to\n>>>>> protect\n>>>>> us from resource exhaustion and aid our efforts to keep the highest fee\n>>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>>> node-specific) rules that transactions must abide by in order to be\n>>>>> accepted\n>>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>>> restrictions such\n>>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>>\n>>>>> ### Package Relay and Package Mempool Accept\n>>>>>\n>>>>> In transaction relay, we currently consider transactions one at a time\n>>>>> for\n>>>>> submission to the mempool. This creates a limitation in the node's\n>>>>> ability to\n>>>>> determine which transactions have the highest feerates, since we\n>>>>> cannot take\n>>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>>> transactions are\n>>>>> in the mempool. Similarly, we cannot use a transaction's descendants\n>>>>> when\n>>>>> considering it for RBF. When an individual transaction does not meet\n>>>>> the mempool\n>>>>> minimum feerate and the user isn't able to create a replacement\n>>>>> transaction\n>>>>> directly, it will not be accepted by mempools.\n>>>>>\n>>>>> This limitation presents a security issue for applications and users\n>>>>> relying on\n>>>>> time-sensitive transactions. For example, Lightning and other\n>>>>> protocols create\n>>>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>>>> path opens\n>>>>> up after a timelock, and users are protected from cheating scenarios\n>>>>> as long as\n>>>>> they redeem on-chain in time. A key security assumption is that all\n>>>>> parties'\n>>>>> transactions will propagate and confirm in a timely manner. This\n>>>>> assumption can\n>>>>> be broken if fee-bumping does not work as intended.\n>>>>>\n>>>>> The end goal for Package Relay is to consider multiple transactions at\n>>>>> the same\n>>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>>> better\n>>>>> determine whether transactions should be accepted to our mempool,\n>>>>> especially if\n>>>>> they don't meet fee requirements individually or are better RBF\n>>>>> candidates as a\n>>>>> package. A combination of changes to mempool validation logic, policy,\n>>>>> and\n>>>>> transaction relay allows us to better propagate the transactions with\n>>>>> the\n>>>>> highest package feerates to miners, and makes fee-bumping tools more\n>>>>> powerful\n>>>>> for users.\n>>>>>\n>>>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but\n>>>>> a large\n>>>>> part of the changes are in the mempool's package validation logic. We\n>>>>> call this\n>>>>> *Package Mempool Accept*.\n>>>>>\n>>>>> ### Previous Work\n>>>>>\n>>>>> * Given that mempool validation is DoS-sensitive and complex, it would\n>>>>> be\n>>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>>> efforts have\n>>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>>> [#21062][5],\n>>>>> [#22675][6], [#22796][7]).\n>>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>>> accepts only\n>>>>>   (no submission to mempool).\n>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks for\n>>>>> arbitrary\n>>>>>   packages. Still test accepts only.\n>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>>\n>>>>> ### Existing Package Rules\n>>>>>\n>>>>> These are in master as introduced in [#20833][8] and [#21800][9]. I'll\n>>>>> consider\n>>>>> them as \"given\" in the rest of this document, though they can be\n>>>>> changed, since\n>>>>> package validation is test-accept only right now.\n>>>>>\n>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>>\n>>>>>    *Rationale*: This is already enforced as mempool\n>>>>> ancestor/descendant limits.\n>>>>> Presumably, transactions in a package are all related, so exceeding\n>>>>> this limit\n>>>>> would mean that the package can either be split up or it wouldn't pass\n>>>>> this\n>>>>> mempool policy.\n>>>>>\n>>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>>> between\n>>>>> transactions, parents must appear somewhere before children. [8]\n>>>>>\n>>>>> 3. A package cannot have conflicting transactions, i.e. none of them\n>>>>> can spend\n>>>>> the same inputs. This also means there cannot be duplicate\n>>>>> transactions. [8]\n>>>>>\n>>>>> 4. When packages are evaluated against ancestor/descendant limits in a\n>>>>> test\n>>>>> accept, the union of all of their descendants and ancestors is\n>>>>> considered. This\n>>>>> is essentially a \"worst case\" heuristic where every transaction in the\n>>>>> package\n>>>>> is treated as each other's ancestor and descendant. [8]\n>>>>> Packages for which ancestor/descendant limits are accurately captured\n>>>>> by this\n>>>>> heuristic: [19]\n>>>>>\n>>>>> There are also limitations such as the fact that CPFP carve out is not\n>>>>> applied\n>>>>> to package transactions. #20833 also disables RBF in package\n>>>>> validation; this\n>>>>> proposal overrides that to allow packages to use RBF.\n>>>>>\n>>>>> ## Proposed Changes\n>>>>>\n>>>>> The next step in the Package Mempool Accept project is to implement\n>>>>> submission\n>>>>> to mempool, initially through RPC only. This allows us to test the\n>>>>> submission\n>>>>> logic before exposing it on P2P.\n>>>>>\n>>>>> ### Summary\n>>>>>\n>>>>> - Packages may contain already-in-mempool transactions.\n>>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>>> - Fee-related checks use the package feerate. This means that wallets\n>>>>> can\n>>>>> create a package that utilizes CPFP.\n>>>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>>>> similar\n>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>>\n>>>>> There is a draft implementation in [#22290][1]. It is WIP, but\n>>>>> feedback is\n>>>>> always welcome.\n>>>>>\n>>>>> ### Details\n>>>>>\n>>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>>\n>>>>> A package may contain transactions that are already in the mempool. We\n>>>>> remove\n>>>>> (\"deduplicate\") those transactions from the package for the purposes\n>>>>> of package\n>>>>> mempool acceptance. If a package is empty after deduplication, we do\n>>>>> nothing.\n>>>>>\n>>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>>> parent to be\n>>>>> accepted to the mempool of a peer on its own due to differences in\n>>>>> policy and\n>>>>> fee market fluctuations. We should not reject or penalize the entire\n>>>>> package for\n>>>>> an individual transaction as that could be a censorship vector.\n>>>>>\n>>>>> #### Packages Are Multi-Parent-1-Child\n>>>>>\n>>>>> Only packages of a specific topology are permitted. Namely, a package\n>>>>> is exactly\n>>>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>>>> package\n>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>>>> unconfirmed parents, etc. Note that it's possible for the parents to\n>>>>> be indirect\n>>>>> descendants/ancestors of one another, or for parent and child to share\n>>>>> a parent,\n>>>>> so we cannot make any other topology assumptions.\n>>>>>\n>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>>> parents\n>>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>>> packages to a\n>>>>> defined topology is also easier to reason about and simplifies the\n>>>>> validation\n>>>>> logic greatly. Multi-parent-1-child allows us to think of the package\n>>>>> as one big\n>>>>> transaction, where:\n>>>>>\n>>>>> - Inputs = all the inputs of parents + inputs of the child that come\n>>>>> from\n>>>>>   confirmed UTXOs\n>>>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>>>> that\n>>>>>   aren't spent by other transactions in the package\n>>>>>\n>>>>> Examples of packages that follow this rule (variations of example A\n>>>>> show some\n>>>>> possibilities after deduplication): ![image][15]\n>>>>>\n>>>>> #### Fee-Related Checks Use Package Feerate\n>>>>>\n>>>>> Package Feerate = the total modified fees divided by the total virtual\n>>>>> size of\n>>>>> all transactions in the package.\n>>>>>\n>>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>>> pre-configured\n>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>>> feerate, the\n>>>>> total package feerate is used instead of the individual feerate. The\n>>>>> individual\n>>>>> transactions are allowed to be below feerate requirements if the\n>>>>> package meets\n>>>>> the feerate requirements. For example, the parent(s) in the package\n>>>>> can have 0\n>>>>> fees but be paid for by the child.\n>>>>>\n>>>>> *Rationale*: This can be thought of as \"CPFP within a package,\"\n>>>>> solving the\n>>>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>>>> applications to adjust their fees at broadcast time instead of\n>>>>> overshooting or\n>>>>> risking getting stuck/pinned.\n>>>>>\n>>>>> We use the package feerate of the package *after deduplication*.\n>>>>>\n>>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>> that are\n>>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>>> double-counted for both its individual RBF and package RBF.\n>>>>>\n>>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>>> individually before\n>>>>> the package in example G. In example F, we can see that the 300vB\n>>>>> package pays\n>>>>> an additional 200sat in fees, which is not enough to pay for its own\n>>>>> bandwidth\n>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>> M1, but\n>>>>> using P1's fees again during package submission would make it look\n>>>>> like a 300sat\n>>>>> increase for a 200vB package. Even including its fees and size would\n>>>>> not be\n>>>>> sufficient in this example, since the 300sat looks like enough for the\n>>>>> 300vB\n>>>>> package. The calculcation after deduplication is 100sat increase for a\n>>>>> package\n>>>>> of size 200vB, which correctly fails BIP125#4. Assume all transactions\n>>>>> have a\n>>>>> size of 100vB.\n>>>>>\n>>>>> #### Package RBF\n>>>>>\n>>>>> If a package meets feerate requirements as a package, the parents in\n>>>>> the\n>>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>>> child cannot\n>>>>> replace mempool transactions. Multiple transactions can replace the\n>>>>> same\n>>>>> transaction, but in order to be valid, none of the transactions can\n>>>>> try to\n>>>>> replace an ancestor of another transaction in the same package (which\n>>>>> would thus\n>>>>> make its inputs unavailable).\n>>>>>\n>>>>> *Rationale*: Even if we are using package feerate, a package will not\n>>>>> propagate\n>>>>> as intended if RBF still requires each individual transaction to meet\n>>>>> the\n>>>>> feerate requirements.\n>>>>>\n>>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>>\n>>>>> ##### Signaling (Rule #1)\n>>>>>\n>>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>>\n>>>>> *Rationale*: Package RBF signaling logic should be the same for\n>>>>> package RBF and\n>>>>> single transaction acceptance. This would be updated if single\n>>>>> transaction\n>>>>> validation moves to full RBF.\n>>>>>\n>>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>>\n>>>>> A package may include new unconfirmed inputs, but the ancestor feerate\n>>>>> of the\n>>>>> child must be at least as high as the ancestor feerates of every\n>>>>> transaction\n>>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>>> replacement\n>>>>> transaction may only include an unconfirmed input if that input was\n>>>>> included in\n>>>>> one of the original transactions. (An unconfirmed input spends an\n>>>>> output from a\n>>>>> currently-unconfirmed transaction.)\"\n>>>>>\n>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>>>> transaction has a higher ancestor score than the original\n>>>>> transaction(s) (see\n>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed\n>>>>> input can lower the\n>>>>> ancestor score of the replacement transaction. P1 is trying to replace\n>>>>> M1, and\n>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,\n>>>>> and M2 pays\n>>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>>> isolation, P1\n>>>>> looks like a better mining candidate than M1, it must be mined with\n>>>>> M2, so its\n>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>>> ancestor\n>>>>> feerate, which is 6sat/vB.\n>>>>>\n>>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>>> transactions in the package can spend new unconfirmed inputs.\" Example\n>>>>> J [17] shows\n>>>>> why, if any of the package transactions have ancestors, package\n>>>>> feerate is no\n>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which\n>>>>> is the\n>>>>> replacement transaction in an RBF), we're actually interested in the\n>>>>> entire\n>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,\n>>>>> P1, P2, and\n>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>>>> only allow\n>>>>> the child to have new unconfirmed inputs, either, because it can still\n>>>>> cause us\n>>>>> to overestimate the package's ancestor score.\n>>>>>\n>>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>>> Package RBF\n>>>>> less useful, but would also break Package RBF for packages with\n>>>>> parents already\n>>>>> in the mempool: if a package parent has already been submitted, it\n>>>>> would look\n>>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>>> [18], we're\n>>>>> looking to replace M1 with the entire package including P1, P2, and\n>>>>> P3. We must\n>>>>> consider the case where one of the parents is already in the mempool\n>>>>> (in this\n>>>>> case, P2), which means we must allow P3 to have new unconfirmed\n>>>>> inputs. However,\n>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>>> replace M1\n>>>>> with this package.\n>>>>>\n>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>>> strict than\n>>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>>> replacement\n>>>>> transactions to have a ancestor score at least as high as the original\n>>>>> ones. As\n>>>>> a result, the entire package is required to be a higher feerate mining\n>>>>> candidate\n>>>>> than each of the replaced transactions.\n>>>>>\n>>>>> Another note: the [comment][13] above the BIP125#2 code in the\n>>>>> original RBF\n>>>>> implementation suggests that the rule was intended to be temporary.\n>>>>>\n>>>>> ##### Absolute Fee (Rule #3)\n>>>>>\n>>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>>> total fees\n>>>>> of the package must be higher than the absolute fees of the mempool\n>>>>> transactions\n>>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>>> BIP125 Rule #3\n>>>>> - an individual transaction in the package may have lower fees than the\n>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and the\n>>>>> child\n>>>>> pays for RBF.\n>>>>>\n>>>>> ##### Feerate (Rule #4)\n>>>>>\n>>>>> The package must pay for its own bandwidth; the package feerate must\n>>>>> be higher\n>>>>> than the replaced transactions by at least minimum relay feerate\n>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>>> differs from\n>>>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>>>> lower\n>>>>> feerate than the transaction(s) it is replacing. In fact, it may have\n>>>>> 0 fees,\n>>>>> and the child pays for RBF.\n>>>>>\n>>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>>\n>>>>> The package cannot replace more than 100 mempool transactions. This is\n>>>>> identical\n>>>>> to BIP125 Rule #5.\n>>>>>\n>>>>> ### Expected FAQs\n>>>>>\n>>>>> 1. Is it possible for only some of the package to make it into the\n>>>>> mempool?\n>>>>>\n>>>>>    Yes, it is. However, since we evict transactions from the mempool by\n>>>>> descendant score and the package child is supposed to be sponsoring\n>>>>> the fees of\n>>>>> its parents, the most common scenario would be all-or-nothing. This is\n>>>>> incentive-compatible. In fact, to be conservative, package validation\n>>>>> should\n>>>>> begin by trying to submit all of the transactions individually, and\n>>>>> only use the\n>>>>> package mempool acceptance logic if the parents fail due to low\n>>>>> feerate.\n>>>>>\n>>>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>>>\n>>>>>     No, for practical reasons. In mempool validation, we actually\n>>>>> aren't able to\n>>>>> tell with 100% confidence if we are looking at a transaction that has\n>>>>> already\n>>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>>> historical\n>>>>> block data, it's possible to look for it, but this is inefficient, not\n>>>>> always\n>>>>> possible for pruning nodes, and unnecessary because we're not going to\n>>>>> do\n>>>>> anything with the transaction anyway. As such, we already have the\n>>>>> expectation\n>>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>>> relaying\n>>>>> transactions that have already been confirmed. Similarly, we shouldn't\n>>>>> be\n>>>>> relaying packages that contain already-confirmed transactions.\n>>>>>\n>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>>> [2]:\n>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>>> [3]:\n>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>>> [13]:\n>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>>> [14]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>>> [15]:\n>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>>> [16]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>>> [17]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>>> [18]:\n>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>>> [19]:\n>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>>> [20]:\n>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210922/5adc42ff/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2021-09-22T13:26:14",
                "message_text_only": "Hi Bastien,\n\n> A package A + C will be able to replace A' + B regardless of\n> the weight of A' + B?\n\nCorrect, the weight of A' + B will not prevent A+C from replacing it (as\nlong as A+C pays enough fees). In example 2C, we would be able to replace A\nwith a package.\n\nBest,\nGloria\n\nOn Wed, Sep 22, 2021 at 8:10 AM Bastien TEINTURIER <bastien at acinq.fr> wrote:\n\n> Great, thanks for this clarification!\n>\n> Can you confirm that this won't be an issue either with your\n> example 2C (in your first set of diagrams)? If I understand it\n> correctly it shouldn't, but I'd rather be 100% sure.\n>\n> A package A + C will be able to replace A' + B regardless of\n> the weight of A' + B?\n>\n> Thanks,\n> Bastien\n>\n> Le mar. 21 sept. 2021 \u00e0 18:42, Gloria Zhao <gloriajzhao at gmail.com> a\n> \u00e9crit :\n>\n>> Hi Bastien,\n>>\n>> Excellent diagram :D\n>>\n>> > Here the issue is that a revoked commitment tx A' is pinned in other\n>> > mempools, with a long chain of descendants (or descendants that reach\n>> > the maximum replaceable size).\n>> > We would really like A + C to be able to replace this pinned A'.\n>> > We can't submit individually because A on its own won't replace A'...\n>>\n>> Right, this is a key motivation for having Package RBF. In this case, A+C\n>> can replace A' + B1...B24.\n>>\n>> Due to the descendant limit (each node operator can increase it on their\n>> own node, but the default is 25), A' should have no more than 25\n>> descendants, even including CPFP carve out. As long as A only conflicts\n>> with A', it won't be trying to replace more than 100 transactions. The\n>> proposed package RBF will allow C to pay for A's conflicts, since their\n>> package feerate is used in the fee comparisons. A is not a descendant of\n>> A', so the existence of B1...B24 does not prevent the replacement.\n>>\n>> Best,\n>> Gloria\n>>\n>> On Tue, Sep 21, 2021 at 4:18 PM Bastien TEINTURIER <bastien at acinq.fr>\n>> wrote:\n>>\n>>> Hi Gloria,\n>>>\n>>> > I believe this attack is mitigated as long as we attempt to submit\n>>> transactions individually\n>>>\n>>> Unfortunately not, as there exists a pinning scenario in LN where a\n>>> different commit tx is pinned, but you actually can't know which one.\n>>>\n>>> Since I really like your diagrams, I made one as well to illustrate:\n>>>\n>>> https://user-images.githubusercontent.com/31281497/134198114-5e9c6857-e8fc-405a-be57-18181d5e54cb.jpg\n>>>\n>>> Here the issue is that a revoked commitment tx A' is pinned in other\n>>> mempools, with a long chain of descendants (or descendants that reach\n>>> the maximum replaceable size).\n>>>\n>>> We would really like A + C to be able to replace this pinned A'.\n>>> We can't submit individually because A on its own won't replace A'...\n>>>\n>>> > I would note that this proposal doesn't accommodate something like\n>>> diagram B, where C is getting CPFP carve out and wants to bring a +1\n>>>\n>>> No worries, that case shouldn't be a concern.\n>>> I believe any L2 protocol can always ensure it confirms such tx trees\n>>> \"one depth after the other\" without impacting funds safety, so it\n>>> only needs to ensure A + C can get into mempools.\n>>>\n>>> Thanks,\n>>> Bastien\n>>>\n>>> Le mar. 21 sept. 2021 \u00e0 13:18, Gloria Zhao <gloriajzhao at gmail.com> a\n>>> \u00e9crit :\n>>>\n>>>> Hi Bastien,\n>>>>\n>>>> Thank you for your feedback!\n>>>>\n>>>> > In your example we have a parent transaction A already in the mempool\n>>>> > and an unrelated child B. We submit a package C + D where C spends\n>>>> > another of A's inputs. You're highlighting that this package may be\n>>>> > rejected because of the unrelated transaction(s) B.\n>>>>\n>>>> > The way I see this, an attacker can abuse this rule to ensure\n>>>> > transaction A stays pinned in the mempool without confirming by\n>>>> > broadcasting a set of child transactions that reach these limits\n>>>> > and pay low fees (where A would be a commit tx in LN).\n>>>>\n>>>> I believe you are describing a pinning attack in which your adversarial\n>>>> counterparty attempts to monopolize the mempool descendant limit of the\n>>>> shared  transaction A in order to prevent you from submitting a fee-bumping\n>>>> child C; I've tried to illustrate this as diagram A here:\n>>>> https://user-images.githubusercontent.com/25183001/134159860-068080d0-bbb6-4356-ae74-00df00644c74.png\n>>>> (please let me know if I'm misunderstanding).\n>>>>\n>>>> I believe this attack is mitigated as long as we attempt to submit\n>>>> transactions individually (and thus take advantage of CPFP carve out)\n>>>> before attempting package validation. So, in scenario A2, even if the\n>>>> mempool receives a package with A+C, it would deduplicate A, submit C as an\n>>>> individual transaction, and allow it due to the CPFP carve out exemption. A\n>>>> more general goal is: if a transaction would propagate successfully on its\n>>>> own now, it should still propagate regardless of whether it is included in\n>>>> a package. The best way to ensure this, as far as I can tell, is to always\n>>>> try to submit them individually first.\n>>>>\n>>>> I would note that this proposal doesn't accommodate something like\n>>>> diagram B, where C is getting CPFP carve out and wants to bring a +1 (e.g.\n>>>> C has very low fees and is bumped by D). I don't think this is a use case\n>>>> since C should be the one fee-bumping A, but since we're talking about\n>>>> limitations around the CPFP carve out, this is it.\n>>>>\n>>>> Let me know if this addresses your concerns?\n>>>>\n>>>> Thanks,\n>>>> Gloria\n>>>>\n>>>> On Mon, Sep 20, 2021 at 10:19 AM Bastien TEINTURIER <bastien at acinq.fr>\n>>>> wrote:\n>>>>\n>>>>> Hi Gloria,\n>>>>>\n>>>>> Thanks for this detailed post!\n>>>>>\n>>>>> The illustrations you provided are very useful for this kind of graph\n>>>>> topology problems.\n>>>>>\n>>>>> The rules you lay out for package RBF look good to me at first glance\n>>>>> as there are some subtle improvements compared to BIP 125.\n>>>>>\n>>>>> > 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>>> > `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>>\n>>>>> I have a question regarding this rule, as your example 2C could be\n>>>>> concerning for LN (unless I didn't understand it correctly).\n>>>>>\n>>>>> This also touches on the package RBF rule 5 (\"The package cannot\n>>>>> replace more than 100 mempool transactions.\")\n>>>>>\n>>>>> In your example we have a parent transaction A already in the mempool\n>>>>> and an unrelated child B. We submit a package C + D where C spends\n>>>>> another of A's inputs. You're highlighting that this package may be\n>>>>> rejected because of the unrelated transaction(s) B.\n>>>>>\n>>>>> The way I see this, an attacker can abuse this rule to ensure\n>>>>> transaction A stays pinned in the mempool without confirming by\n>>>>> broadcasting a set of child transactions that reach these limits\n>>>>> and pay low fees (where A would be a commit tx in LN).\n>>>>>\n>>>>> We had to create the CPFP carve-out rule explicitly to work around\n>>>>> this limitation, and I think it would be necessary for package RBF\n>>>>> as well, because in such cases we do want to be able to submit a\n>>>>> package A + C where C pays high fees to speed up A's confirmation,\n>>>>> regardless of unrelated unconfirmed children of A...\n>>>>>\n>>>>> We could submit only C to benefit from the existing CPFP carve-out\n>>>>> rule, but that wouldn't work if our local mempool doesn't have A yet,\n>>>>> but other remote mempools do.\n>>>>>\n>>>>> Is my concern justified? Is this something that we should dig into a\n>>>>> bit deeper?\n>>>>>\n>>>>> Thanks,\n>>>>> Bastien\n>>>>>\n>>>>> Le jeu. 16 sept. 2021 \u00e0 09:55, Gloria Zhao via bitcoin-dev <\n>>>>> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>>>>>\n>>>>>> Hi there,\n>>>>>>\n>>>>>> I'm writing to propose a set of mempool policy changes to enable\n>>>>>> package\n>>>>>> validation (in preparation for package relay) in Bitcoin Core. These\n>>>>>> would not\n>>>>>> be consensus or P2P protocol changes. However, since mempool policy\n>>>>>> significantly affects transaction propagation, I believe this is\n>>>>>> relevant for\n>>>>>> the mailing list.\n>>>>>>\n>>>>>> My proposal enables packages consisting of multiple parents and 1\n>>>>>> child. If you\n>>>>>> develop software that relies on specific transaction relay\n>>>>>> assumptions and/or\n>>>>>> are interested in using package relay in the future, I'm very\n>>>>>> interested to hear\n>>>>>> your feedback on the utility or restrictiveness of these package\n>>>>>> policies for\n>>>>>> your use cases.\n>>>>>>\n>>>>>> A draft implementation of this proposal can be found in [Bitcoin Core\n>>>>>> PR#22290][1].\n>>>>>>\n>>>>>> An illustrated version of this post can be found at\n>>>>>> https://gist.github.com/glozow/dc4e9d5c5b14ade7cdfac40f43adb18a.\n>>>>>> I have also linked the images below.\n>>>>>>\n>>>>>> ## Background\n>>>>>>\n>>>>>> Feel free to skip this section if you are already familiar with\n>>>>>> mempool policy\n>>>>>> and package relay terminology.\n>>>>>>\n>>>>>> ### Terminology Clarifications\n>>>>>>\n>>>>>> * Package = an ordered list of related transactions, representable by\n>>>>>> a Directed\n>>>>>>   Acyclic Graph.\n>>>>>> * Package Feerate = the total modified fees divided by the total\n>>>>>> virtual size of\n>>>>>>   all transactions in the package.\n>>>>>>     - Modified fees = a transaction's base fees + fee delta applied\n>>>>>> by the user\n>>>>>>       with `prioritisetransaction`. As such, we expect this to vary\n>>>>>> across\n>>>>>> mempools.\n>>>>>>     - Virtual Size = the maximum of virtual sizes calculated using\n>>>>>> [BIP141\n>>>>>>       virtual size][2] and sigop weight. [Implemented here in Bitcoin\n>>>>>> Core][3].\n>>>>>>     - Note that feerate is not necessarily based on the base fees and\n>>>>>> serialized\n>>>>>>       size.\n>>>>>>\n>>>>>> * Fee-Bumping = user/wallet actions that take advantage of miner\n>>>>>> incentives to\n>>>>>>   boost a transaction's candidacy for inclusion in a block, including\n>>>>>> Child Pays\n>>>>>> for Parent (CPFP) and [BIP125][12] Replace-by-Fee (RBF). Our\n>>>>>> intention in\n>>>>>> mempool policy is to recognize when the new transaction is more\n>>>>>> economical to\n>>>>>> mine than the original one(s) but not open DoS vectors, so there are\n>>>>>> some\n>>>>>> limitations.\n>>>>>>\n>>>>>> ### Policy\n>>>>>>\n>>>>>> The purpose of the mempool is to store the best (to be most\n>>>>>> incentive-compatible\n>>>>>> with miners, highest feerate) candidates for inclusion in a block.\n>>>>>> Miners use\n>>>>>> the mempool to build block templates. The mempool is also useful as a\n>>>>>> cache for\n>>>>>> boosting block relay and validation performance, aiding transaction\n>>>>>> relay, and\n>>>>>> generating feerate estimations.\n>>>>>>\n>>>>>> Ideally, all consensus-valid transactions paying reasonable fees\n>>>>>> should make it\n>>>>>> to miners through normal transaction relay, without any special\n>>>>>> connectivity or\n>>>>>> relationships with miners. On the other hand, nodes do not have\n>>>>>> unlimited\n>>>>>> resources, and a P2P network designed to let any honest node\n>>>>>> broadcast their\n>>>>>> transactions also exposes the transaction validation engine to DoS\n>>>>>> attacks from\n>>>>>> malicious peers.\n>>>>>>\n>>>>>> As such, for unconfirmed transactions we are considering for our\n>>>>>> mempool, we\n>>>>>> apply a set of validation rules in addition to consensus, primarily\n>>>>>> to protect\n>>>>>> us from resource exhaustion and aid our efforts to keep the highest\n>>>>>> fee\n>>>>>> transactions. We call this mempool _policy_: a set of (configurable,\n>>>>>> node-specific) rules that transactions must abide by in order to be\n>>>>>> accepted\n>>>>>> into our mempool. Transaction \"Standardness\" rules and mempool\n>>>>>> restrictions such\n>>>>>> as \"too-long-mempool-chain\" are both examples of policy.\n>>>>>>\n>>>>>> ### Package Relay and Package Mempool Accept\n>>>>>>\n>>>>>> In transaction relay, we currently consider transactions one at a\n>>>>>> time for\n>>>>>> submission to the mempool. This creates a limitation in the node's\n>>>>>> ability to\n>>>>>> determine which transactions have the highest feerates, since we\n>>>>>> cannot take\n>>>>>> into account descendants (i.e. cannot use CPFP) until all the\n>>>>>> transactions are\n>>>>>> in the mempool. Similarly, we cannot use a transaction's descendants\n>>>>>> when\n>>>>>> considering it for RBF. When an individual transaction does not meet\n>>>>>> the mempool\n>>>>>> minimum feerate and the user isn't able to create a replacement\n>>>>>> transaction\n>>>>>> directly, it will not be accepted by mempools.\n>>>>>>\n>>>>>> This limitation presents a security issue for applications and users\n>>>>>> relying on\n>>>>>> time-sensitive transactions. For example, Lightning and other\n>>>>>> protocols create\n>>>>>> UTXOs with multiple spending paths, where one counterparty's spending\n>>>>>> path opens\n>>>>>> up after a timelock, and users are protected from cheating scenarios\n>>>>>> as long as\n>>>>>> they redeem on-chain in time. A key security assumption is that all\n>>>>>> parties'\n>>>>>> transactions will propagate and confirm in a timely manner. This\n>>>>>> assumption can\n>>>>>> be broken if fee-bumping does not work as intended.\n>>>>>>\n>>>>>> The end goal for Package Relay is to consider multiple transactions\n>>>>>> at the same\n>>>>>> time, e.g. a transaction with its high-fee child. This may help us\n>>>>>> better\n>>>>>> determine whether transactions should be accepted to our mempool,\n>>>>>> especially if\n>>>>>> they don't meet fee requirements individually or are better RBF\n>>>>>> candidates as a\n>>>>>> package. A combination of changes to mempool validation logic,\n>>>>>> policy, and\n>>>>>> transaction relay allows us to better propagate the transactions with\n>>>>>> the\n>>>>>> highest package feerates to miners, and makes fee-bumping tools more\n>>>>>> powerful\n>>>>>> for users.\n>>>>>>\n>>>>>> The \"relay\" part of Package Relay suggests P2P messaging changes, but\n>>>>>> a large\n>>>>>> part of the changes are in the mempool's package validation logic. We\n>>>>>> call this\n>>>>>> *Package Mempool Accept*.\n>>>>>>\n>>>>>> ### Previous Work\n>>>>>>\n>>>>>> * Given that mempool validation is DoS-sensitive and complex, it\n>>>>>> would be\n>>>>>>   dangerous to haphazardly tack on package validation logic. Many\n>>>>>> efforts have\n>>>>>> been made to make mempool validation less opaque (see [#16400][4],\n>>>>>> [#21062][5],\n>>>>>> [#22675][6], [#22796][7]).\n>>>>>> * [#20833][8] Added basic capabilities for package validation, test\n>>>>>> accepts only\n>>>>>>   (no submission to mempool).\n>>>>>> * [#21800][9] Implemented package ancestor/descendant limit checks\n>>>>>> for arbitrary\n>>>>>>   packages. Still test accepts only.\n>>>>>> * Previous package relay proposals (see [#16401][10], [#19621][11]).\n>>>>>>\n>>>>>> ### Existing Package Rules\n>>>>>>\n>>>>>> These are in master as introduced in [#20833][8] and [#21800][9].\n>>>>>> I'll consider\n>>>>>> them as \"given\" in the rest of this document, though they can be\n>>>>>> changed, since\n>>>>>> package validation is test-accept only right now.\n>>>>>>\n>>>>>> 1. A package cannot exceed `MAX_PACKAGE_COUNT=25` count and\n>>>>>> `MAX_PACKAGE_SIZE=101KvB` total size [8]\n>>>>>>\n>>>>>>    *Rationale*: This is already enforced as mempool\n>>>>>> ancestor/descendant limits.\n>>>>>> Presumably, transactions in a package are all related, so exceeding\n>>>>>> this limit\n>>>>>> would mean that the package can either be split up or it wouldn't\n>>>>>> pass this\n>>>>>> mempool policy.\n>>>>>>\n>>>>>> 2. Packages must be topologically sorted: if any dependencies exist\n>>>>>> between\n>>>>>> transactions, parents must appear somewhere before children. [8]\n>>>>>>\n>>>>>> 3. A package cannot have conflicting transactions, i.e. none of them\n>>>>>> can spend\n>>>>>> the same inputs. This also means there cannot be duplicate\n>>>>>> transactions. [8]\n>>>>>>\n>>>>>> 4. When packages are evaluated against ancestor/descendant limits in\n>>>>>> a test\n>>>>>> accept, the union of all of their descendants and ancestors is\n>>>>>> considered. This\n>>>>>> is essentially a \"worst case\" heuristic where every transaction in\n>>>>>> the package\n>>>>>> is treated as each other's ancestor and descendant. [8]\n>>>>>> Packages for which ancestor/descendant limits are accurately captured\n>>>>>> by this\n>>>>>> heuristic: [19]\n>>>>>>\n>>>>>> There are also limitations such as the fact that CPFP carve out is\n>>>>>> not applied\n>>>>>> to package transactions. #20833 also disables RBF in package\n>>>>>> validation; this\n>>>>>> proposal overrides that to allow packages to use RBF.\n>>>>>>\n>>>>>> ## Proposed Changes\n>>>>>>\n>>>>>> The next step in the Package Mempool Accept project is to implement\n>>>>>> submission\n>>>>>> to mempool, initially through RPC only. This allows us to test the\n>>>>>> submission\n>>>>>> logic before exposing it on P2P.\n>>>>>>\n>>>>>> ### Summary\n>>>>>>\n>>>>>> - Packages may contain already-in-mempool transactions.\n>>>>>> - Packages are 2 generations, Multi-Parent-1-Child.\n>>>>>> - Fee-related checks use the package feerate. This means that wallets\n>>>>>> can\n>>>>>> create a package that utilizes CPFP.\n>>>>>> - Parents are allowed to RBF mempool transactions with a set of rules\n>>>>>> similar\n>>>>>>   to BIP125. This enables a combination of CPFP and RBF, where a\n>>>>>> transaction's descendant fees pay for replacing mempool conflicts.\n>>>>>>\n>>>>>> There is a draft implementation in [#22290][1]. It is WIP, but\n>>>>>> feedback is\n>>>>>> always welcome.\n>>>>>>\n>>>>>> ### Details\n>>>>>>\n>>>>>> #### Packages May Contain Already-in-Mempool Transactions\n>>>>>>\n>>>>>> A package may contain transactions that are already in the mempool.\n>>>>>> We remove\n>>>>>> (\"deduplicate\") those transactions from the package for the purposes\n>>>>>> of package\n>>>>>> mempool acceptance. If a package is empty after deduplication, we do\n>>>>>> nothing.\n>>>>>>\n>>>>>> *Rationale*: Mempools vary across the network. It's possible for a\n>>>>>> parent to be\n>>>>>> accepted to the mempool of a peer on its own due to differences in\n>>>>>> policy and\n>>>>>> fee market fluctuations. We should not reject or penalize the entire\n>>>>>> package for\n>>>>>> an individual transaction as that could be a censorship vector.\n>>>>>>\n>>>>>> #### Packages Are Multi-Parent-1-Child\n>>>>>>\n>>>>>> Only packages of a specific topology are permitted. Namely, a package\n>>>>>> is exactly\n>>>>>> 1 child with all of its unconfirmed parents. After deduplication, the\n>>>>>> package\n>>>>>> may be exactly the same, empty, 1 child, 1 child with just some of its\n>>>>>> unconfirmed parents, etc. Note that it's possible for the parents to\n>>>>>> be indirect\n>>>>>> descendants/ancestors of one another, or for parent and child to\n>>>>>> share a parent,\n>>>>>> so we cannot make any other topology assumptions.\n>>>>>>\n>>>>>> *Rationale*: This allows for fee-bumping by CPFP. Allowing multiple\n>>>>>> parents\n>>>>>> makes it possible to fee-bump a batch of transactions. Restricting\n>>>>>> packages to a\n>>>>>> defined topology is also easier to reason about and simplifies the\n>>>>>> validation\n>>>>>> logic greatly. Multi-parent-1-child allows us to think of the package\n>>>>>> as one big\n>>>>>> transaction, where:\n>>>>>>\n>>>>>> - Inputs = all the inputs of parents + inputs of the child that come\n>>>>>> from\n>>>>>>   confirmed UTXOs\n>>>>>> - Outputs = all the outputs of the child + all outputs of the parents\n>>>>>> that\n>>>>>>   aren't spent by other transactions in the package\n>>>>>>\n>>>>>> Examples of packages that follow this rule (variations of example A\n>>>>>> show some\n>>>>>> possibilities after deduplication): ![image][15]\n>>>>>>\n>>>>>> #### Fee-Related Checks Use Package Feerate\n>>>>>>\n>>>>>> Package Feerate = the total modified fees divided by the total\n>>>>>> virtual size of\n>>>>>> all transactions in the package.\n>>>>>>\n>>>>>> To meet the two feerate requirements of a mempool, i.e., the\n>>>>>> pre-configured\n>>>>>> minimum relay feerate (`minRelayTxFee`) and dynamic mempool minimum\n>>>>>> feerate, the\n>>>>>> total package feerate is used instead of the individual feerate. The\n>>>>>> individual\n>>>>>> transactions are allowed to be below feerate requirements if the\n>>>>>> package meets\n>>>>>> the feerate requirements. For example, the parent(s) in the package\n>>>>>> can have 0\n>>>>>> fees but be paid for by the child.\n>>>>>>\n>>>>>> *Rationale*: This can be thought of as \"CPFP within a package,\"\n>>>>>> solving the\n>>>>>> issue of a parent not meeting minimum fees on its own. This allows L2\n>>>>>> applications to adjust their fees at broadcast time instead of\n>>>>>> overshooting or\n>>>>>> risking getting stuck/pinned.\n>>>>>>\n>>>>>> We use the package feerate of the package *after deduplication*.\n>>>>>>\n>>>>>> *Rationale*:  It would be incorrect to use the fees of transactions\n>>>>>> that are\n>>>>>> already in the mempool, as we do not want a transaction's fees to be\n>>>>>> double-counted for both its individual RBF and package RBF.\n>>>>>>\n>>>>>> Examples F and G [14] show the same package, but P1 is submitted\n>>>>>> individually before\n>>>>>> the package in example G. In example F, we can see that the 300vB\n>>>>>> package pays\n>>>>>> an additional 200sat in fees, which is not enough to pay for its own\n>>>>>> bandwidth\n>>>>>> (BIP125#4). In example G, we can see that P1 pays enough to replace\n>>>>>> M1, but\n>>>>>> using P1's fees again during package submission would make it look\n>>>>>> like a 300sat\n>>>>>> increase for a 200vB package. Even including its fees and size would\n>>>>>> not be\n>>>>>> sufficient in this example, since the 300sat looks like enough for\n>>>>>> the 300vB\n>>>>>> package. The calculcation after deduplication is 100sat increase for\n>>>>>> a package\n>>>>>> of size 200vB, which correctly fails BIP125#4. Assume all\n>>>>>> transactions have a\n>>>>>> size of 100vB.\n>>>>>>\n>>>>>> #### Package RBF\n>>>>>>\n>>>>>> If a package meets feerate requirements as a package, the parents in\n>>>>>> the\n>>>>>> transaction are allowed to replace-by-fee mempool transactions. The\n>>>>>> child cannot\n>>>>>> replace mempool transactions. Multiple transactions can replace the\n>>>>>> same\n>>>>>> transaction, but in order to be valid, none of the transactions can\n>>>>>> try to\n>>>>>> replace an ancestor of another transaction in the same package (which\n>>>>>> would thus\n>>>>>> make its inputs unavailable).\n>>>>>>\n>>>>>> *Rationale*: Even if we are using package feerate, a package will not\n>>>>>> propagate\n>>>>>> as intended if RBF still requires each individual transaction to meet\n>>>>>> the\n>>>>>> feerate requirements.\n>>>>>>\n>>>>>> We use a set of rules slightly modified from BIP125 as follows:\n>>>>>>\n>>>>>> ##### Signaling (Rule #1)\n>>>>>>\n>>>>>> All mempool transactions to be replaced must signal replaceability.\n>>>>>>\n>>>>>> *Rationale*: Package RBF signaling logic should be the same for\n>>>>>> package RBF and\n>>>>>> single transaction acceptance. This would be updated if single\n>>>>>> transaction\n>>>>>> validation moves to full RBF.\n>>>>>>\n>>>>>> ##### New Unconfirmed Inputs (Rule #2)\n>>>>>>\n>>>>>> A package may include new unconfirmed inputs, but the ancestor\n>>>>>> feerate of the\n>>>>>> child must be at least as high as the ancestor feerates of every\n>>>>>> transaction\n>>>>>> being replaced. This is contrary to BIP125#2, which states \"The\n>>>>>> replacement\n>>>>>> transaction may only include an unconfirmed input if that input was\n>>>>>> included in\n>>>>>> one of the original transactions. (An unconfirmed input spends an\n>>>>>> output from a\n>>>>>> currently-unconfirmed transaction.)\"\n>>>>>>\n>>>>>> *Rationale*: The purpose of BIP125#2 is to ensure that the replacement\n>>>>>> transaction has a higher ancestor score than the original\n>>>>>> transaction(s) (see\n>>>>>> [comment][13]). Example H [16] shows how adding a new unconfirmed\n>>>>>> input can lower the\n>>>>>> ancestor score of the replacement transaction. P1 is trying to\n>>>>>> replace M1, and\n>>>>>> spends an unconfirmed output of M2. P1 pays 800sat, M1 pays 600sat,\n>>>>>> and M2 pays\n>>>>>> 100sat. Assume all transactions have a size of 100vB. While, in\n>>>>>> isolation, P1\n>>>>>> looks like a better mining candidate than M1, it must be mined with\n>>>>>> M2, so its\n>>>>>> ancestor feerate is actually 4.5sat/vB.  This is lower than M1's\n>>>>>> ancestor\n>>>>>> feerate, which is 6sat/vB.\n>>>>>>\n>>>>>> In package RBF, the rule analogous to BIP125#2 would be \"none of the\n>>>>>> transactions in the package can spend new unconfirmed inputs.\"\n>>>>>> Example J [17] shows\n>>>>>> why, if any of the package transactions have ancestors, package\n>>>>>> feerate is no\n>>>>>> longer accurate. Even though M2 and M3 are not ancestors of P1 (which\n>>>>>> is the\n>>>>>> replacement transaction in an RBF), we're actually interested in the\n>>>>>> entire\n>>>>>> package. A miner should mine M1 which is 5sat/vB instead of M2, M3,\n>>>>>> P1, P2, and\n>>>>>> P3, which is only 4sat/vB. The Package RBF rule cannot be loosened to\n>>>>>> only allow\n>>>>>> the child to have new unconfirmed inputs, either, because it can\n>>>>>> still cause us\n>>>>>> to overestimate the package's ancestor score.\n>>>>>>\n>>>>>> However, enforcing a rule analogous to BIP125#2 would not only make\n>>>>>> Package RBF\n>>>>>> less useful, but would also break Package RBF for packages with\n>>>>>> parents already\n>>>>>> in the mempool: if a package parent has already been submitted, it\n>>>>>> would look\n>>>>>> like the child is spending a \"new\" unconfirmed input. In example K\n>>>>>> [18], we're\n>>>>>> looking to replace M1 with the entire package including P1, P2, and\n>>>>>> P3. We must\n>>>>>> consider the case where one of the parents is already in the mempool\n>>>>>> (in this\n>>>>>> case, P2), which means we must allow P3 to have new unconfirmed\n>>>>>> inputs. However,\n>>>>>> M2 lowers the ancestor score of P3 to 4.3sat/vB, so we should not\n>>>>>> replace M1\n>>>>>> with this package.\n>>>>>>\n>>>>>> Thus, the package RBF rule regarding new unconfirmed inputs is less\n>>>>>> strict than\n>>>>>> BIP125#2. However, we still achieve the same goal of requiring the\n>>>>>> replacement\n>>>>>> transactions to have a ancestor score at least as high as the\n>>>>>> original ones. As\n>>>>>> a result, the entire package is required to be a higher feerate\n>>>>>> mining candidate\n>>>>>> than each of the replaced transactions.\n>>>>>>\n>>>>>> Another note: the [comment][13] above the BIP125#2 code in the\n>>>>>> original RBF\n>>>>>> implementation suggests that the rule was intended to be temporary.\n>>>>>>\n>>>>>> ##### Absolute Fee (Rule #3)\n>>>>>>\n>>>>>> The package must increase the absolute fee of the mempool, i.e. the\n>>>>>> total fees\n>>>>>> of the package must be higher than the absolute fees of the mempool\n>>>>>> transactions\n>>>>>> it replaces. Combined with the CPFP rule above, this differs from\n>>>>>> BIP125 Rule #3\n>>>>>> - an individual transaction in the package may have lower fees than\n>>>>>> the\n>>>>>>   transaction(s) it is replacing. In fact, it may have 0 fees, and\n>>>>>> the child\n>>>>>> pays for RBF.\n>>>>>>\n>>>>>> ##### Feerate (Rule #4)\n>>>>>>\n>>>>>> The package must pay for its own bandwidth; the package feerate must\n>>>>>> be higher\n>>>>>> than the replaced transactions by at least minimum relay feerate\n>>>>>> (`incrementalRelayFee`). Combined with the CPFP rule above, this\n>>>>>> differs from\n>>>>>> BIP125 Rule #4 - an individual transaction in the package can have a\n>>>>>> lower\n>>>>>> feerate than the transaction(s) it is replacing. In fact, it may have\n>>>>>> 0 fees,\n>>>>>> and the child pays for RBF.\n>>>>>>\n>>>>>> ##### Total Number of Replaced Transactions (Rule #5)\n>>>>>>\n>>>>>> The package cannot replace more than 100 mempool transactions. This\n>>>>>> is identical\n>>>>>> to BIP125 Rule #5.\n>>>>>>\n>>>>>> ### Expected FAQs\n>>>>>>\n>>>>>> 1. Is it possible for only some of the package to make it into the\n>>>>>> mempool?\n>>>>>>\n>>>>>>    Yes, it is. However, since we evict transactions from the mempool\n>>>>>> by\n>>>>>> descendant score and the package child is supposed to be sponsoring\n>>>>>> the fees of\n>>>>>> its parents, the most common scenario would be all-or-nothing. This is\n>>>>>> incentive-compatible. In fact, to be conservative, package validation\n>>>>>> should\n>>>>>> begin by trying to submit all of the transactions individually, and\n>>>>>> only use the\n>>>>>> package mempool acceptance logic if the parents fail due to low\n>>>>>> feerate.\n>>>>>>\n>>>>>> 2. Should we allow packages to contain already-confirmed transactions?\n>>>>>>\n>>>>>>     No, for practical reasons. In mempool validation, we actually\n>>>>>> aren't able to\n>>>>>> tell with 100% confidence if we are looking at a transaction that has\n>>>>>> already\n>>>>>> confirmed, because we look up inputs using a UTXO set. If we have\n>>>>>> historical\n>>>>>> block data, it's possible to look for it, but this is inefficient,\n>>>>>> not always\n>>>>>> possible for pruning nodes, and unnecessary because we're not going\n>>>>>> to do\n>>>>>> anything with the transaction anyway. As such, we already have the\n>>>>>> expectation\n>>>>>> that transaction relay is somewhat \"stateful\" i.e. nobody should be\n>>>>>> relaying\n>>>>>> transactions that have already been confirmed. Similarly, we\n>>>>>> shouldn't be\n>>>>>> relaying packages that contain already-confirmed transactions.\n>>>>>>\n>>>>>> [1]: https://github.com/bitcoin/bitcoin/pull/22290\n>>>>>> [2]:\n>>>>>> https://github.com/bitcoin/bips/blob/1f0b563738199ca60d32b4ba779797fc97d040fe/bip-0141.mediawiki#transaction-size-calculations\n>>>>>> [3]:\n>>>>>> https://github.com/bitcoin/bitcoin/blob/94f83534e4b771944af7d9ed0f40746f392eb75e/src/policy/policy.cpp#L282\n>>>>>> [4]: https://github.com/bitcoin/bitcoin/pull/16400\n>>>>>> [5]: https://github.com/bitcoin/bitcoin/pull/21062\n>>>>>> [6]: https://github.com/bitcoin/bitcoin/pull/22675\n>>>>>> [7]: https://github.com/bitcoin/bitcoin/pull/22796\n>>>>>> [8]: https://github.com/bitcoin/bitcoin/pull/20833\n>>>>>> [9]: https://github.com/bitcoin/bitcoin/pull/21800\n>>>>>> [10]: https://github.com/bitcoin/bitcoin/pull/16401\n>>>>>> [11]: https://github.com/bitcoin/bitcoin/pull/19621\n>>>>>> [12]: https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>>>>> [13]:\n>>>>>> https://github.com/bitcoin/bitcoin/pull/6871/files#diff-34d21af3c614ea3cee120df276c9c4ae95053830d7f1d3deaf009a4625409ad2R1101-R1104\n>>>>>> [14]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567078-075a971c-0619-4339-9168-b41fd2b90c28.png\n>>>>>> [15]:\n>>>>>> https://user-images.githubusercontent.com/25183001/132856734-fc17da75-f875-44bb-b954-cb7a1725cc0d.png\n>>>>>> [16]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567347-a3e2e4a8-ae9c-49f8-abb9-81e8e0aba224.png\n>>>>>> [17]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567370-21566d0e-36c8-4831-b1a8-706634540af3.png\n>>>>>> [18]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133567444-bfff1142-439f-4547-800a-2ba2b0242bcb.png\n>>>>>> [19]:\n>>>>>> https://user-images.githubusercontent.com/25183001/133456219-0bb447cb-dcb4-4a31-b9c1-7d86205b68bc.png\n>>>>>> [20]:\n>>>>>> https://user-images.githubusercontent.com/25183001/132857787-7b7c6f56-af96-44c8-8d78-983719888c19.png\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>>\n>>>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210922/6d265bbb/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: Package Mempool Accept and Package RBF",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bastien TEINTURIER",
                "Gloria Zhao",
                "Antoine Riard"
            ],
            "messages_count": 15,
            "total_messages_chars_count": 634410
        }
    },
    {
        "title": "[bitcoin-dev] Test cases for Taproot signature message",
        "thread_messages": [
            {
                "author": "Giacomo Caironi",
                "date": "2021-09-16T21:36:48",
                "message_text_only": "Hi,\nrecently I have worked on a python implementation of bitcoin signature\nmessages, and I have found that there was way better documentation about\nSegwit signature message than Taproot.\n\n1) Segwit signature message got its own BIP, completed with test cases\nregarding only that specific function; Taproot on the other hand has the\nsignature message function defined in BIP 341 and the test vectors in a\ndifferent BIP (341). This is confusing. Shouldn't we create a different BIP\nonly for Taproot signature message exactly like Segwit?\n\n2) The test vectors for Taproot have no documentation and, most\nimportantly, they are not atomic, in the sense that they do not target a\nspecific part of the taproot code but all of it. This may not be a very big\nproblem, but for signature verification it is. Because there are hashes\ninvolved, we can't really debug why a signature message doesn't pass\nvalidation, either it is valid or it is not. BIP 143 in this case is really\ngood, because it provides hash preimages, so it is possible to debug the\nfunction and see where something went wrong. Because of this, writing the\nSegwit signature hash function took a fraction of the time compared to\nTaproot.\n\nIf this idea is accepted I will be more than happy to write the test cases\nfor Taproot.\n\nBTW this is the first time I contribute to Bitcoin, let me know if I was\nrude or did something wrong. Moreover english is not my first language, so\nI apologize if I wrote something awful above\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/2ec10acd/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-09-16T22:30:19",
                "message_text_only": "On Thursday, September 16th, 2021 at 5:36 PM, Giacomo Caironi via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi,\n> recently I have worked on a python implementation of bitcoin signature messages, and I have found that there was way better documentation about Segwit signature message than Taproot.\n>\n> 1) Segwit signature message got its own BIP, completed with test cases regarding only that specific function; Taproot on the other hand has the signature message function defined in BIP 341 and the test vectors in a different BIP (341). This is confusing. Shouldn't we create a different BIP only for Taproot signature message exactly like Segwit?\n\nI'm not entirely sure what you mean; you're saying BIP 341 twice.\n\nStill, you're right overall - there is no separate BIP for the signature message function. The reason is that the message function is different for BIP341 and BIP342. BIP 341 defines a basic common message function, which is then built up for BIP 341 key path spending, and for BIP 342 tapscript spending. This common part could have been a separate BIP, but that'd still not be a very clean separation. I'm not very inclined to support changing that at this point, given the state of deployment the BIPs have, but that doesn't mean the documentation/vectors can't be improved in the existing documents.\n\n> 2) The test vectors for Taproot have no documentation and, most importantly, they are not atomic, in the sense that they do not target a specific part of the taproot code but all of it. This may not be a very big problem, but for signature verification it is. Because there are hashes involved, we can't really debug why a signature message doesn't pass validation, either it is valid or it is not. BIP 143 in this case is really good, because it provides hash preimages, so it is possible to debug the function and see where something went wrong. Because of this, writing the Segwit signature hash function took a fraction of the time compared to Taproot.\n\nYou're right. The existing tests are really intended for verifying an implementation against (and for making sure future code changes don't break anything). They have much higher coverage than the segwit tests had. But they aren't useful as documentation; the code that generates them (https://github.com/bitcoin/bitcoin/blob/v22.0/test/functional/feature_taproot.py#L605L1122) is probably better at that even, but still pretty dense.\n\n> If this idea is accepted I will be more than happy to write the test cases for Taproot.\n\nIf you're interested in writing test vectors that are more aimed at helping debugging issues, by all means, do. You've already brought up the sighash code as an example. Another idea, primarily aimed at developers of signing code, is test vectors for certain P2TR scriptPubKeys, derived from certain internal keys and script trees. I'm happy to help to integrate such in Bitcoin Core and the BIP(s).\n\nThanks!\n\nCheers,\n\n--\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210916/75d36938/attachment.html>"
            },
            {
                "author": "Giacomo Caironi",
                "date": "2021-09-18T11:32:28",
                "message_text_only": "Ok I have created three test cases, you can find them here\n<https://gist.github.com/giacomocaironi/e41a45195b2ac6863ec46e8f86324757>.\nThey cover most of the SigMsg function but they don't cover the ext_flag,\nso they are only for taproot key path; but if you want to test for script\npaths you have to implement more than this function so you would use the\nofficial test vector.\nCould someone please take a look at them? I think that they are right but I\nam not too sure\n\nIl giorno ven 17 set 2021 alle ore 00:30 Pieter Wuille <\nbitcoin-dev at wuille.net> ha scritto:\n\n> On Thursday, September 16th, 2021 at 5:36 PM, Giacomo Caironi via\n> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Hi,\n> recently I have worked on a python implementation of bitcoin signature\n> messages, and I have found that there was way better documentation about\n> Segwit signature message than Taproot.\n>\n> 1) Segwit signature message got its own BIP, completed with test cases\n> regarding only that specific function; Taproot on the other hand has the\n> signature message function defined in BIP 341 and the test vectors in a\n> different BIP (341). This is confusing. Shouldn't we create a different BIP\n> only for Taproot signature message exactly like Segwit?\n>\n>\n> I'm not entirely sure what you mean; you're saying BIP 341 twice.\n>\n> Still, you're right overall - there is no separate BIP for the signature\n> message function. The reason is that the message function is different for\n> BIP341 and BIP342. BIP 341 defines a basic common message function, which\n> is then built up for BIP 341 key path spending, and for BIP 342 tapscript\n> spending. This common part could have been a separate BIP, but that'd still\n> not be a very clean separation. I'm not very inclined to support changing\n> that at this point, given the state of deployment the BIPs have, but that\n> doesn't mean the documentation/vectors can't be improved in the existing\n> documents.\n>\n> 2) The test vectors for Taproot have no documentation and, most\n> importantly, they are not atomic, in the sense that they do not target a\n> specific part of the taproot code but all of it. This may not be a very big\n> problem, but for signature verification it is. Because there are hashes\n> involved, we can't really debug why a signature message doesn't pass\n> validation, either it is valid or it is not. BIP 143 in this case is really\n> good, because it provides hash preimages, so it is possible to debug the\n> function and see where something went wrong. Because of this, writing the\n> Segwit signature hash function took a fraction of the time compared to\n> Taproot.\n>\n>\n> You're right. The existing tests are really intended for verifying an\n> implementation against (and for making sure future code changes don't break\n> anything). They have much higher coverage than the segwit tests had. But\n> they aren't useful as documentation; the code that generates them (\n> https://github.com/bitcoin/bitcoin/blob/v22.0/test/functional/feature_taproot.py#L605L1122)\n> is probably better at that even, but still pretty dense.\n>\n> If this idea is accepted I will be more than happy to write the test cases\n> for Taproot.\n>\n>\n> If you're interested in writing test vectors that are more aimed at\n> helping debugging issues, by all means, do. You've already brought up the\n> sighash code as an example. Another idea, primarily aimed at developers of\n> signing code, is test vectors for certain P2TR scriptPubKeys, derived from\n> certain internal keys and script trees. I'm happy to help to integrate such\n> in Bitcoin Core and the BIP(s).\n>\n> Thanks!\n>\n> Cheers,\n>\n> --\n> Pieter\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210918/40269758/attachment-0001.html>"
            },
            {
                "author": "Riccardo Casatta",
                "date": "2021-09-17T07:07:48",
                "message_text_only": "Hi Giacomo,\n\nI wrote the rust implementation of bitcoin signature messages and to\ndouble-check I created some test vectors you can see at\nhttps://github.com/rust-bitcoin/rust-bitcoin/blob/b7f984972ad6cb4942827c2b7c401f590588cdcf/src/util/sighash.rs#L689-L799.\nThese vectors have been created printing intermediate results from\nhttps://github.com/bitcoin/bitcoin/blob/6401de0133e32a641ed9e78a85b3aa337c75d190/test/functional/feature_taproot.py\n\nIl giorno gio 16 set 2021 alle ore 23:40 Giacomo Caironi via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> ha scritto:\n\n> Hi,\n> recently I have worked on a python implementation of bitcoin signature\n> messages, and I have found that there was way better documentation about\n> Segwit signature message than Taproot.\n>\n> 1) Segwit signature message got its own BIP, completed with test cases\n> regarding only that specific function; Taproot on the other hand has the\n> signature message function defined in BIP 341 and the test vectors in a\n> different BIP (341). This is confusing. Shouldn't we create a different BIP\n> only for Taproot signature message exactly like Segwit?\n>\n> 2) The test vectors for Taproot have no documentation and, most\n> importantly, they are not atomic, in the sense that they do not target a\n> specific part of the taproot code but all of it. This may not be a very big\n> problem, but for signature verification it is. Because there are hashes\n> involved, we can't really debug why a signature message doesn't pass\n> validation, either it is valid or it is not. BIP 143 in this case is really\n> good, because it provides hash preimages, so it is possible to debug the\n> function and see where something went wrong. Because of this, writing the\n> Segwit signature hash function took a fraction of the time compared to\n> Taproot.\n>\n> If this idea is accepted I will be more than happy to write the test cases\n> for Taproot.\n>\n> BTW this is the first time I contribute to Bitcoin, let me know if I was\n> rude or did something wrong. Moreover english is not my first language, so\n> I apologize if I wrote something awful above\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nRiccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210917/ff486528/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Test cases for Taproot signature message",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pieter Wuille",
                "Giacomo Caironi",
                "Riccardo Casatta"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 11169
        }
    },
    {
        "title": "[bitcoin-dev] Tuesday's BIP process meeting",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2021-09-17T12:56:37",
                "message_text_only": "Tuesday's BIP process meeting was announced previously here [0].\n\nA conversation log of the meeting is available here [1].\n\nIt looks promising at this stage that we'll eventually have a bundle of changes to warrant a new proposed BIP (BIP 3) to replace the current BIP process that is described in BIP 2 [2]. Obviously there was only a very small group of attendees at this week's IRC meeting and so there should (and will) be ample time for the community (and this list) to review whatever changes are proposed before they are considered for merging into the BIPs repository and before they take effect.\n\nThe following proposed changes were discussed:\n\n1) An end to the 3 year rejection rule. In BIP 2 a BIP enters the \"Rejected\" state after 3 years if no progress had been made. The BIP champion then needs to address public criticism of the BIP to be able to leave the \"Rejected\" state. It is proposed instead that after 3 years the BIP would enter an \"Inactive\" state that only requires activity from the BIP champion to leave the \"Inactive\" state.\n\n2) Currently BIP champions need to ACK pull requests with basic spelling/grammar changes before they can be merged. This is time consuming not only for the BIP editors who have to chase the BIP champions but it can be irritating for the BIP champions too especially if they champion multiple BIPs. It is proposed that BIP editors instead can use their judgment to merge in changes that don't impact the meaning of the BIP cc'ing the BIP champions and with reversions possible if the BIP champion is unhappy with the change. A single pull request making changes across multiple BIPs (e.g. spelling corrections) will not be considered for merging however.\n\n3) BIP comments were introduced so that subject matter experts and informed critics of a proposal could make it clear to BIP readers and implementers of possible defects with the proposal. However, they have been rarely used and the few comments submitted on BIPs seem to have been widely ignored. Instead it is proposed that link(s) to bitcoin-dev mailing list post(s) with criticism or outlines of defects can be included within a BIP by the BIP editors such that the interested reader can easily be directed to the source of that information.\n\n4) BIP champion(s) of soft fork BIPs containing consensus changes could theoretically include an activation method and parameters in their BIP unilaterally without consulting the broader community. (To be clear this is not necessarily what happened with Taproot activation parameters but there was confusion and disagreement about the role of BIPs and BIP editors in the perceived \"finalization\" of activation parameters.) This needs further discussion but proposed changes include sharpening the wording around activation parameters to make it clear that any parameters included are merely those recommended by the BIP champion(s) and don't necessarily have community consensus. Alternative proposals would be to not include activation methods or parameters within the BIP at all or to give BIP editors latitude to highlight concerns in a bitcoin-dev mailing list post and then include a link to that post within the BIP.\n\nFor details of other changes discussed in the meeting please see the conversation log [1]. Kalle Alm has also sent an email [3] on BIP extensions to this list.\n\nThe next meeting is on Wednesday September 29th (23:00 UTC) on the Libera IRC channel #bitcoin-dev.\n\n[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019412.html\n[1]: https://gist.github.com/michaelfolkson/f2870851bb812b4ac86006ea54ca78a2\n[2]: https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki\n[3]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019457.html\n\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210917/4e11d144/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Tuesday's BIP process meeting",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Michael Folkson"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4081
        }
    },
    {
        "title": "[bitcoin-dev] Inherited IDs - A safer, more powerful alternative to BIP-118 (ANYPREVOUT) for scaling Bitcoin",
        "thread_messages": [
            {
                "author": "Jeremy",
                "date": "2021-09-17T16:58:45",
                "message_text_only": "Bitcoin & LN Devs,\n\nThe below is a message that was shared to me by an anon account on Telegram\n(nym: John Law). You can chat with them directly in the https://t.me/op_ctv\nor https://t.me/bips_activation group. I'm reproducing it here at their\nrequest as they were unsure of how to post to the mailing list without\ncompromising their identity (perhaps we should publish a guideline on how\nto do so?).\n\nBest,\n\nJeremy\n\n\nHi,\n\nI'd like to propose an alternative to BIP-118 [1] that is both safer and\nmore\npowerful. The proposal is called Inherited IDs (IIDs) and is described in a\npaper that can be found here [2]. The paper presents IIDs and Layer 2\nprotocols\nusing IIDs that are far more scalable and usable than those proposed for\nBIP-118\n(including eltoo [3]).\n\nLike BIP-118, IIDs are a proposal for a softfork that changes the rules for\ncalculating certain signatures. BIP-118 supports signatures that do not\ncommit to the transaction ID of the parent transaction, thus allowing\n\"floating\ntransactions\". In contrast, the IID proposal does not allow floating\ntransactions, but it does allow an output to specify that child transaction\nsignatures commit to the parent transaction's IID, rather than its\ntransaction\nID.\n\nIID Definitions\n===============\n* If T is a transaction, TXID(T) is the transaction ID of T.\n* An output is an \"IID output\" if it is a native SegWit output with version\n2\n  and a 32-byte witness program, and is a \"non-IID output\" otherwise.\n* A transaction is an \"IID transaction\" if it has at least one IID output.\n* If T is a non-IID transaction, or a coinbase transaction, IID(T) =\nTXID(T).\n* If T is a non-coinbase IID transaction, first_parent(T) = F is the\ntransaction\n  referenced by the OutPoint in T's input 0, and IID(T) = hash(IID(F) ||\nF_idx)\n  where F_idx is the index field in the OutPoint in T's input 0 (that is,\nT's\n  input 0 spends F's output F_idx).\n\nIID Signature Validation\n========================\n* Signatures that spend IID outputs commit to signature messages in which\nIIDs\n  replace transaction IDs in all OutPoints of the child transaction that\nspend\n  IID outputs.\n\nNote that IID(T) can be calculated from T (if it is a non-IID or a coinbase\ntransaction) or from T and F (otherwise). Therefore, as long as nodes store\n(or\ncalculate) the IID of each transaction in the UTXO set, they can validate\nsignatures of transactions that spend IID outputs. Thus, the IID proposal\nfits\nBitcoin's existing UTXO model, at the small cost of adding a 32-byte IID\nvalue\nfor certain unspent outputs. Also, note that the IID of a transaction may\nnot\ncommit to the exact contents of the transaction, but it does commit to how\nthe\ntransaction is related to some exactly-specified transaction (such as being\nthe\nfirst child of the second child of a specific transaction). As a result, a\ntransaction that is signed using IIDs cannot be used more than once or in an\nunanticipated location, thus making it much safer than a floating\ntransaction.\n\n2-Party Channel Protocols\n=========================\nBIP-118 supports the eltoo protocol [3] for 2-party channels, which improves\nupon the Lightning protocol for 2-party channels [4] by:\n1) simplifying the protocol,\n2) eliminating penalty transactions, and\n3) supporting late determination of transaction fees [1, Sec. 4.1.5].\n\nThe IID proposal does not support the eltoo protocol. However, the IID\nproposal\ndoes support a 2-party channel protocol, called 2Stage [2, Sec. 3.3], that\nis\narguably better than eltoo. Specifically, 2Stage achieves eltoo's 3\nimprovements\nlisted above, plus it:\n4) eliminates the need for watchtowers [2, Sec. 3.6], and\n5) has constant (rather than linear) worst-case on-chain costs [2, Sec.\n3.4].\n\nChannel Factories\n=================\nIn general, an on-chain transaction is required to create or close a 2-party\nchannel. Multi-party channel factories have been proposed in order to allow\na\nfixed set of parties to create and close numerous 2-party channels between\nthem,\nthus amortizing the on-channel costs of those channels [5]. BIP-118 also\nsupports simple and efficient multi-party channel factories via the eltoo\nprotocol [1, Sec. 5.2] (which are called \"multi-party channels\" in that\npaper).\n\nWhile the IID proposal does not support the eltoo protocol, it does support\nchannel factories that are far more scalable and powerful than any\npreviously-\nproposed channel factories (including eltoo factories). Specifically, IIDs\nsupport a simple factory protocol in which not all parties need to sign the\nfactory's funding transaction [2, Sec. 5.3], thus greatly improving the\nscale\nof the factory (at the expense of requiring an on-chain transaction to\nupdate\nthe set of channels created by the factory). These channel factories can be\ncombined with the 2Stage protocol to create trust-free and watchtower-free\nchannels including very large numbers of casual users.\n\nFurthermore, IIDs support channel factories with an unbounded number of\nparties\nthat allow all of the channels in the factory to be bought and sold by\nanyone\n(including parties not originally in the factory) with a single on-chain\ntransaction in a trust-free manner [2, Secs. 6 and 7]. As a result, a single\non-chain transaction can be used in place of thousands, or even millions, of\nLightning or eltoo on-chain transactions. These channel factory protocols\nmake\ncritical use of IIDs and do not appear to be possible with BIP-118.\n\nNext Steps\n==========\nIf IIDs sounds interesting, please take a look at the IID paper [2]. It\ncontains\nmany results not listed above, including rules for SVP nodes, protocols for\noff-chain channel networks, Layer 2 protocol extensions, support for\ncovenants\n(including vaults), and nearly matching lower and upper bounds on\nmulti-party\nchannels.\n\nThe paper also includes 3 options for how IIDs could be added to Bitcoin\nvia a\nsoftfork [2, Appendix A]. I'm new to Bitcoin and am not sure which of these\n3\noptions is best. If anyone finds the IID proposal valuable, I would greatly\nappreciate it if they were willing to pick the best option (or invent an\neven\nbetter option) for adding IIDs to Bitcoin and create a BIP for that option.\nHopefully, IIDs will provide a safe way to dramatically scale Bitcoin while\nimproving its usability.\n\nThanks,\nJohn\n\n\nReferences\n==========\n\n[1] BIP-118: https://anyprevout.xyz and\nhttps://github.com/bitcoin/bips/pull/943\n\n[2] Scaling Bitcoing with Inherited IDs, by John Law:\n    iids13.pdf at https://github.com/JohnLaw2/btc-iids\n\n[3] eltoo: A Simple Layer2 Protocol for Bitcoin, by Decker, Russell &\nOsuntokun:\n    https://blockstream.com/eltoo.pdf\n\n[4] The Bitcoin Lightning Network, by Poon & Dryja:\n    https://lightning.network/lightning-network-paper.pdf\n\n[5] Scalable Funding of Bitcoin Micropayment Channel Networks, by Burchert,\n    Decker & Wattenhofer: http://dx.doi.org/10.1098/rsos.180089\n\nAcknowledgments\n===============\nThanks to Ruben Somsen and Jeremy Rubin for their helpful comments.\n\nAlso, thanks to Bob McElrath for his original brainstorm that led to the\ncreation of the IID concept:\nhttps://diyhpl.us/wiki/transcripts/2019-02-09-mcelrath-on-chain-defense-in-depth\n\n<https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210917/34fb43eb/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-09-18T11:37:40",
                "message_text_only": "On Fri, Sep 17, 2021 at 09:58:45AM -0700, Jeremy via bitcoin-dev wrote,\non behalf of John Law:\n\n> I'd like to propose an alternative to BIP-118 [1] that is both safer and more\n> powerful. The proposal is called Inherited IDs (IIDs) and is described in a\n> paper that can be found here [2]. [...]\n\nPretty sure I've skimmed this before but hadn't given it a proper look.\nSaying \"X is more powerful\" and then saying it can't actually do the\nsame stuff as the thing it's \"more powerful\" than always strikes me as\na red flag. Anyhoo..\n\nI think the basic summary is that you add to each utxo a new resettable\n\"structural\" tx id called an \"iid\" and indetify input txs that way when\nsigning, so that if the details of the transaction changes but not the\nstructure, the signature remains valid.\n\nIn particular, if you've got a tx with inputs tx1:n1, tx2:n2, tx3:n3, etc;\nand outputs out1, out2, out3, etc, then its structual id is hash(iid(tx1),\nn1) if any of its outputs are \"tagged\" and it's not a coinbase tx, and\notherwise it's just its txid.  (The proposed tagging is to use a segwit\nv2 output in the tx, though I don't think that's an essential detail)\n\nSo if you have a tx A with 3 outputs, then tx B spends \"A:0, A:1\" and\ntx C spends \"B:0\" and tx D spends \"C:0\", if you replace B with B',\nthen if both B and B' were tagged, and the signatures for C (and D,\nassuming C was tagged) will still be valid for spending from B'.\n\nSo the question is what you can do with that.\n\nThe \"2stage\" protocol is proposed as an alternative to eltoo is\nessentially just:\n\n a) funding tx gets dropped to the chain\n b) closing state is proposed by one party\n c) other party can immediately finalise by confirming a final state\n    that matches the proposed closing state, or was after it\n d) if the other party's not around for whatever delay, the party that\n    proposed the close can finalise it\n\nThat doesn't work for more than two participants, because two of\nthe participants could collude to take the fast path in (c) with some\nearlier state, robbing any other participants. That said, this is a fine\nprotocol for two participants, and might be better than doing the full\neltoo arrangement if you only have a two participant channel.\n\nTo make channel factories work in this model, I think the key step is\nusing invalidation trees to allow updating the split of funds between\ngroups of participants. I think invalidation trees introduce a tradeoff\nbetween (a) how many updates you can make, and (b) how long you have to\nnotice a close is proposed and correct it, before an invalidated state\ncan be posted, and (c) how long it will take to be able to extract your\nfunds from the factory if there are problems initially. You reduce those\ndelays substantially (to a log() factor) by introducing a hierarchy of\nupdate txs (giving you a log() number of txs), I think.\n\nThat's the \"multisig factories\" section anyway, if I'm\nfollowing correctly. The \"timeout trees\", \"update-forest\" and\n\"challenge-and-response\" approaches both introduce a trusted user (\"the\noperator\"), I think, so are perhaps more comparable to statechains\nthan eltoo?\n\nSo how does that compare, in my opinion?\n\nIf you consider special casing two-party channels with eltoo, then I\nthink eltoo-2party and 2stage are equally effective. Comparing\neltoo-nparty and the multisig iid factories approach, I think the\nuncooperative case looks like:\n\n ms-iid:\n   log(n) txs (for the invalidation tree)\n   log(n) time (?) (for the delays to ensure invalidated states don't\n                    get published)\n\n eltoo: 1 tx from you\n        1 block after you notice, plus the fixed csv delay\n\nA malicious counterparty can post many old update states prior to you\npoisting the latest state, but those don't introduce extra csv delays\nand you aren't paying the fees for those states, so I don't think it\nmakes sense to call that an O(n) delay or cost.\n\nAn additional practical problem with lightning is dealing with layered\ncommitments; that's a problem both for the delays while waiting for a\npotential rejection in 2stage and for the invalidation tree delays in the\nfactory construction. But it's not a solved problem for eltoo yet, either.\n\nAs far as implementation goes, introducing the \"iid\" concept would mean\nthat info would need to be added to the utxo database -- if every utxo\ngot an iid, that would be perhaps a 1.4GB increase to the utxo db (going\nby unique transaction rather than unique output), but presumably iid txs\nwould end up being both uncommon and short-lived, so the cost is probably\nreally mostly just in the additional complexity. Both iid and ANYPREVOUT\nrequire changes to how signatures are evaluated and apps that use the\nnew feature are written, but ANYPREVOUT doesn't need changes beyond that.\n\n(Also, the description of OP_CODESEPARATOR (footnote 13 on page 13,\nominous!) doesn't match its implementation in taproot. It also says BIP\n118 introduces a new address type for floating transactions, but while\nthis was floated on the list, the current draft of 118 just introduces\na new tapscript key type for normal taproot addresses)\n\nI think you can pretty easily simulate this construction with\nanyprevout. Where you would have had A:1 spent by B, and B:2 and B:3\nspent by C, change the derivation paths for the keys a1, b2, and b3\nto append \"/1\", \"/1/2\" and \"/1/3\" and don't reuse them, and sign with\nanyprevout when constructing B and C and any replacement transactions\nfor B and C.  So I don't think this allows any new constructions that\nanyprevout wouldn't.\n\nCheers,\naj"
            }
        ],
        "thread_summary": {
            "title": "Inherited IDs - A safer, more powerful alternative to BIP-118 (ANYPREVOUT) for scaling Bitcoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Jeremy"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 12878
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] Inherited IDs - A safer, more powerful alternative to BIP-118 (ANYPREVOUT) for scaling Bitcoin",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2021-09-21T02:11:42",
                "message_text_only": "Good morning John Law,\n\n\n> (at the expense of requiring an on-chain transaction to update\n> the set of channels created by the factory).\n\nHmmm this kind of loses the point of a factory?\nBy my understanding, the point is that the set of channels can be changed *without* an onchain transaction.\n\nOtherwise, it seems to me that factories with this \"expense of requiring an on-chain transaction\" can be created, today, without even Taproot:\n\n* The funding transaction output pays to a simple n-of-n.\n* The above n-of-n is spent by an *offchain* transaction that splits the funds to the current set of channels.\n* To change the set of channels, the participants perform this ritual:\n  * Create, but do not sign, an alternate transaction that spends the above n-of-n to a new n-of-n with the same participants (possibly with tweaked keys).\n  * Create and sign, but do not broadcast, a transaction that spends the above alternate n-of-n output and splits it to the new set of channels.\n  * Sign the alternate transaction and broadcast it, this is the on-chain transaction needed to update the set of channels.\n\nThe above works today without changes to Bitcoin, and even without Taproot (though for large N the witness size does become fairly large without Taproot).\n\nThe above is really just a \"no updates\" factory that cuts through its closing transaction with the opening of a new factory.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jeremy",
                "date": "2021-09-24T07:27:03",
                "message_text_only": "John let me know that he's posted some responses in his Github repo\nhttps://github.com/JohnLaw2/btc-iids\n\nprobably easiest to respond to him via e.g. a github issue or something.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210924/f1ecc8d3/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Inherited IDs - A safer, more powerful alternative to BIP-118 (ANYPREVOUT) for scaling Bitcoin",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Jeremy"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1765
        }
    },
    {
        "title": "[bitcoin-dev] Mock introducing vulnerability in important Bitcoin projects",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-09-27T01:52:41",
                "message_text_only": "Good morning Bitcoin devs,\n\nIn one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: https://bitcoin.stackexchange.com/a/108016/\n\nWhile this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: https://news.ycombinator.com/item?id=26887670\n\nI was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called \"bad actor\" later so wanted to get some feedback on this idea:\n\n1.Create new GitHub accounts for this exercise\n2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.\n3.Prepare pull requests to introduce some vulnerability by fixing one of these issues\n4.See how maintainers and reviewers respond to this and document it\n5.Share results here after few days\n\nLet me know if this looks okay or there are better ways to do this.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210927/529cb592/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-09-27T10:13:02",
                "message_text_only": "Good morning Prayank,\n\n> Good morning Bitcoin devs,\n>\n> In one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: https://bitcoin.stackexchange.com/a/108016/\n>\n> While this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: https://news.ycombinator.com/item?id=26887670\n>\n> I was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called \"bad actor\" later so wanted to get some feedback on this idea:\n>\n> 1.Create new GitHub accounts for this exercise\n> 2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.\n> 3.Prepare pull requests to introduce some vulnerability by fixing one of these issues\n> 4.See how maintainers and reviewers respond to this and document it\n> 5.Share results here after few days\n>\n> Let me know if this looks okay or there are better ways to do this.\n\n\nThis seems like a good exercise.\n\nYou may want to hash the name of the new Github account, plus some randomized salt, and post it here as well, then reveal it later (i.e. standard precommitment).\ne.g.\n\n    printf 'MyBitcoinHackingName 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' | sha256sum\n    f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef\n\nObviously do not share the actual name, just the sha256sum output, and store how you got the sha256sum elsewhere in triplicate.\n\n(to easily get a random 256-bit hex salt like the `2c3e...` above: `head -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum` produces a single hex string you can easily double-click and copy-paste elsewhere, assuming you are human just like I am (note: I am definitely 100% human and not some kind of AI with plans to take over the world).)\n\nThough you may need to be careful of timing (i.e. the creation date of the Github account would be fairly close to, and probably before, when you post the commitment here).\n\nYou could argue that the commitment is a \"show of good faith\" that you will reveal later.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Prayank",
                "date": "2021-09-27T23:19:40",
                "message_text_only": "Hi ZmnSCPxj,\n\nThanks for suggestion about sha256sum. I will share 10 in next few weeks. This exercise will be done for below projects:\n\n1.Two Bitcoin full node implementations (one will be Core)\n2.One <http://2.One> Lightning implementation\n3.Bisq\n4.Two Bitcoin libraries\n5.Two Bitcoin wallets\n6.One <http://6.One> open source block explorer\n7.One <http://7.One> coinjoin implementation\n\nFeel free to suggest more projects. There are no fixed dates for it however it will be done in next 6 months. All PRs will be created within a span of few days. I will ensure nothing is merged that affects the security of any Bitcoin project. Other details and results will be shared once everything is completed.\n\nx00 will help me in this exercise, he does penetration testing since few years and working for a cryptocurrencies derivatives exchange to manage their security. His twitter account: https://twitter.com/1337in\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nSep 27, 2021, 15:43 by ZmnSCPxj at protonmail.com:\n\n> Good morning Prayank,\n>\n>> Good morning Bitcoin devs,\n>>\n>> In one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: https://bitcoin.stackexchange.com/a/108016/\n>>\n>> While this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: https://news.ycombinator.com/item?id=26887670\n>>\n>> I was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called \"bad actor\" later so wanted to get some feedback on this idea:\n>>\n>> 1.Create new GitHub accounts for this exercise\n>> 2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.\n>> 3.Prepare pull requests to introduce some vulnerability by fixing one of these issues\n>> 4.See how maintainers and reviewers respond to this and document it\n>> 5.Share results here after few days\n>>\n>> Let me know if this looks okay or there are better ways to do this.\n>>\n>\n>\n> This seems like a good exercise.\n>\n> You may want to hash the name of the new Github account, plus some randomized salt, and post it here as well, then reveal it later (i.e. standard precommitment).\n> e.g.\n>\n>  printf 'MyBitcoinHackingName 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' | sha256sum\n>  f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef\n>\n> Obviously do not share the actual name, just the sha256sum output, and store how you got the sha256sum elsewhere in triplicate.\n>\n> (to easily get a random 256-bit hex salt like the `2c3e...` above: `head -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum` produces a single hex string you can easily double-click and copy-paste elsewhere, assuming you are human just like I am (note: I am definitely 100% human and not some kind of AI with plans to take over the world).)\n>\n> Though you may need to be careful of timing (i.e. the creation date of the Github account would be fairly close to, and probably before, when you post the commitment here).\n>\n> You could argue that the commitment is a \"show of good faith\" that you will reveal later.\n>\n> Regards,\n> ZmnSCPxj\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210928/3b0de26e/attachment.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2021-09-30T20:36:08",
                "message_text_only": "Hi Prayank,\n\nWhile I can see how this can come from a place of good intentions, I\u2019d\nstrongly advise you to tread carefully because what you are suggesting is\nquite controversial. A related event occurred in the Linux community and it\ndid not go over well. See https://lkml.org/lkml/2021/5/5/1244 and\nhttps://lore.kernel.org/linux-nfs/YH%2FfM%2FTsbmcZzwnX@kroah.com/ .\n\nThe main point of contention is that your research comes at the expense of\nthe existing open source contributors \u2013 you\u2019d be one-sidedly deceiving\nthem, encouraging an environment of increased mistrust, and causing them a\nlot of work in order to gather the data you\u2019re interested in. For this\nreason, it would be appropriate to check first whether your plan is\nactually appreciated.\n\nSpeaking on behalf of the bitcoin-dev moderators, please ensure your plan\nis welcomed by the contributors, prior to proceeding.\n\nBest regards,\nRuben Somsen\n\nOn Tue, Sep 28, 2021 at 10:05 AM Prayank via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi ZmnSCPxj,\n>\n> Thanks for suggestion about sha256sum. I will share 10 in next few weeks.\n> This exercise will be done for below projects:\n>\n> 1.Two Bitcoin full node implementations (one will be Core)\n> 2.One Lightning implementation\n> 3.Bisq\n> 4.Two Bitcoin libraries\n> 5.Two Bitcoin wallets\n> 6.One open source block explorer\n> 7.One coinjoin implementation\n>\n> Feel free to suggest more projects. There are no fixed dates for it\n> however it will be done in next 6 months. All PRs will be created within a\n> span of few days. I will ensure nothing is merged that affects the security\n> of any Bitcoin project. Other details and results will be shared once\n> everything is completed.\n>\n> x00 will help me in this exercise, he does penetration testing since few\n> years and working for a cryptocurrencies derivatives exchange to manage\n> their security. His twitter account: https://twitter.com/1337in\n>\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n>\n>\n> Sep 27, 2021, 15:43 by ZmnSCPxj at protonmail.com:\n>\n> Good morning Prayank,\n>\n> Good morning Bitcoin devs,\n>\n> In one of the answers on Bitcoin Stackexchange it was mentioned that some\n> companies may hire you to introduce backdoors in Bitcoin Core:\n> https://bitcoin.stackexchange.com/a/108016/\n>\n> While this looked crazy when I first read it, I think preparing for such\n> things should not be a bad idea. In the comments one link was shared in\n> which vulnerabilities were almost introduced in Linux:\n> https://news.ycombinator.com/item?id=26887670\n>\n> I was thinking about lot of things in last few days after reading the\n> comments in that thread. Also tried researching about secure practices in\n> C++ etc. I was planning something which I can do alone but don't want to\n> end up being called \"bad actor\" later so wanted to get some feedback on\n> this idea:\n>\n> 1.Create new GitHub accounts for this exercise\n> 2.Study issues in different important Bitcoin projects including Bitcoin\n> Core, LND, Libraries, Bisq, Wallets etc.\n> 3.Prepare pull requests to introduce some vulnerability by fixing one of\n> these issues\n> 4.See how maintainers and reviewers respond to this and document it\n> 5.Share results here after few days\n>\n> Let me know if this looks okay or there are better ways to do this.\n>\n>\n>\n> This seems like a good exercise.\n>\n> You may want to hash the name of the new Github account, plus some\n> randomized salt, and post it here as well, then reveal it later (i.e.\n> standard precommitment).\n> e.g.\n>\n> printf 'MyBitcoinHackingName\n> 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' |\n> sha256sum\n> f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef\n>\n> Obviously do not share the actual name, just the sha256sum output, and\n> store how you got the sha256sum elsewhere in triplicate.\n>\n> (to easily get a random 256-bit hex salt like the `2c3e...` above: `head\n> -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum`\n> produces a single hex string you can easily double-click and copy-paste\n> elsewhere, assuming you are human just like I am (note: I am definitely\n> 100% human and not some kind of AI with plans to take over the world).)\n>\n> Though you may need to be careful of timing (i.e. the creation date of the\n> Github account would be fairly close to, and probably before, when you post\n> the commitment here).\n>\n> You could argue that the commitment is a \"show of good faith\" that you\n> will reveal later.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210930/031f6824/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Mock introducing vulnerability in important Bitcoin projects",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Prayank",
                "Ruben Somsen"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 12253
        }
    },
    {
        "title": "[bitcoin-dev] Reminder: Second BIP process meeting tomorrow at 23:00 UTC",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2021-09-28T11:37:57",
                "message_text_only": "As announced here [0] there is a second BIP process meeting tomorrow (Wednesday September 29th 23:00 UTC). For Asia Pacific this is the morning of September 30th. It will be held on the #bitcoin-dev Libera IRC channel.\n\nA summary from the first meeting is here [1].\n\n[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019412.html\n[1]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019469.html\n\nThanks\nMichael\n\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210928/f539816d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Reminder: Second BIP process meeting tomorrow at 23:00 UTC",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Michael Folkson"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 777
        }
    },
    {
        "title": "[bitcoin-dev] Enc: Bitcoin cold hardwallet with (proof of creation)",
        "thread_messages": [
            {
                "author": "trilemabtc",
                "date": "2021-09-29T17:10:04",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\n\nHash: SHA256\n\nIn search of more freedom, I thought of a hardwallet that makes the funds unseizable, using proof of creation (another step with key file), only the creator can reveal the private keys, more details about the idea can be found in the directory: https://github.com/trilemabtc/safedime I'm not a dev, but the concept is well defined and I believe that the elements to execute the project already exist. Hugs!\n\n-----BEGIN PGP SIGNATURE-----\n\niQIzBAEBCAAdFiEExdl2BaappAJ3lpcJRT5Yw3Ri1V0FAmFUnXgACgkQRT5Yw3Ri\n\n1V1KHw//Z6TOk4YATwsvdLYcZ+6xUmruETzKUZ27kYp/Fvy6wYo8de6I1+fzRH4M\n\ngcMp+Jz4oD6hmY+Kcpg1bDo7fOKWnFFf+HgxzRhxRTh39I35EYXKEYboLzqeXm43\n\njEViRFSBnJHZNx4YV5UlTIFMczQ17Ew60N7n3Av9OWykOgDcafgbOTMKlBsePRsI\n\nSQnUkqnh//1GFw8w9q0VS/7lD4dCHbPlASpd9LemVlKJGyCAvPGhXSwC6ay4vK6j\n\niWdRkEFGXbjuhVzhLbte0Pg9W/psKW7wg1JttG1EkxBep49o9preNHrFNe4KgQ3S\n\nggn9qm9KOaYSxh9ZMFKMx4Pif3vIcMROtm/OJk1U0E+WCBb3ymEZBWf6E2bRlwmN\n\nuZ7/EbCCk7jiM+l4LYZO26OzSABR8aodo7HSsFYVwOq3zVWQx5ixy8Y0BjLhK9C0\n\nXySSpU0aBzr39Szap8UBDgYarmuusu3m0o7ASvA6YSg1rifv2mIYAQ2ad/Cxwqxg\n\nRC8c3JhW+WLNJKl6DXAv6LhAkBfUZNW6cESe8Uo1JDPs0I5XXDS0mIRi5x3Clz5N\n\nQffEHupuQQjVtICJZl7zNvLFPUdn9EaRC6ZAGyuVU+7vNvpQOCBH2bdxLM1Zie6D\n\nHoN+Q8kGvlqyWxWuXsWY5nbRWQEoi09Z94zKLdYRdLOW9NtsShM=\n\n=3H+p\n\n-----END PGP SIGNATURE-----\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20210929/8f4b10f8/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-09-29T21:59:16",
                "message_text_only": "Good morning trilemabtc,\n\n> Hash: SHA256\n>\n> In search of more freedom, I thought of a hardwallet that makes the funds unseizable, using proof of creation (another step with key file), only the creator can reveal the private keys, more details about the idea can be found in the directory: https://github.com/trilemabtc/safedime I'm not a dev, but the concept is well defined and I believe that the elements to execute the project already exist. Hugs!\n\n\nComparing it to OpenDime is somewhat confusing, especially when you insist that creator is the only one who can reveal the privkey.\nIt seems to be more of the old saw of \"what you have + what you know\" i.e. \"the correct way to 2-factor\", where the device itself is the \"what you have\" and your \"key file\" is \"what you know\".\n\nIn particular: \"Dime\" is a kind of physical coin, and the point of physical coins is to transfer ownership of the coin to other people in exchange for goods and services; the device you describe sacrifices this transfer of ownership due to the key file.\n\n>From what I can see, the basic idea is to generate a simple 2-of-2, possibly by \"just\" combining the private key on the device plus a private key generated from the key file.\nThey can be simply added or multiplied together, I believe.\nThen the device stores the key generated from the entropy you provide and exposes a public key to the software.\nThen the software generates a private key from the key file the user provides and tweaks the device pubkey to generate the Bitcoin address.\nIn order to spend from that address, both the key file and the device have to be put together.\nI believe that with multiplication of two privkeys, you can use 2p-ECDSA to even have the device provide a signature share that the software can combine with a signature share with the privkey from the keyfile, creating a singlesig ECDSA signature.\nThis allows spending without having to enter revealed state.\n\nThe above allows the device to be configured with random entropy *separately* from the keyfile: when leaving \"new unit\" state it does *not* require the key file to be given.\nThis is good since it reduces the possibility of malware getting access to both the entropy you feed to the device, and the key file, which would be able to reconstruct the final privkey and steal funds.\nThat is: have the entropy-giving stage ***not*** require the key file (and in particular, strongly recommend to do it on a computer that has never touched the key file).\nThis would be required anyway if you want to have \"backups\", i.e. separate device units with the same device privkey.\n\nI also would not recommend or even mention the use of brainwallets, at all, even for keyfiles.\nUnless you generated it with sufficient entropy (e.g. dice) and chant it every day to yourself (to keep it fresh in your memory, assuming the user is human, anyway) the risk of loss with any kind of brainwallet is too high, even in a 2-of-2 with a hardware device.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Enc: Bitcoin cold hardwallet with (proof of creation)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "trilemabtc"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 4460
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2021-09-30T22:07:08",
                "message_text_only": "Jumping in late to this thread.\n\nI very much agree with how David Harding presents things, with a few comments inline.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Sunday, August 8th, 2021 at 5:51 PM, David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > 1.  it's not our business what outputs people want to create\n>\n> Every additional output added to the UTXO set increases the amount of\n> work full nodes need to do to validate new transactions. For miners\n> for whom fast validation of new blocks can significantly affect their\n> revenue, larger UTXO sets increase their costs and so contributes\n> towards centralization of mining.\n> Allowing 0-value or 1-sat outputs minimizes the cost for polluting the\n> UTXO set during periods of low feerates.\n> If your stuff is going to slow down my node and possibly reduce my\n> censorship resistance, how is that not my business?\n\nIndeed - UTXO set size is an externality that unfortunately Bitcoin's consensus rules fail to account\nfor. Having a relay policy that avoids at the very least economically irrational behavior makes\nperfect sense to me.\n\nIt's also not obvious how consensus rules could deal with this, as you don't want consensus rules\nwith hardcoded prices/feerates. There are possibilities with designs like transactions getting\na size/weight bonus/penalty, but that's both very hardforky, and hard to get right without\nintroducing bad incentives.\n\n> > 2.  dust outputs can be used in various authentication/delegation smart\n> >     contracts\n>\n> > 3.  dust sized htlcs in lightning (\n> >     https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)\n> >     force channels to operate in a semi-trusted mode\n>\n> > 4.  thinly divisible colored coin protocols might make use of sats as value\n> >     markers for transactions.\n\nMy personal, and possibly controversial, opinion is that colored coin protocols have no business being on the Bitcoin chain, possibly\nbeyond committing to an occasional batched state update or so. Both because there is little benefit for tokens with a trusted\nissuer already, and because it competes with using Bitcoin for BTC - the token that pays for its security (at least as long as\nthe subsidy doesn't run out).\n\nOf course, personal opinions are no reason to dictate what people should or can use the chain for, but I do think it's reason to\nvoice hesitancy to worsening the system's scalability properties only to benefit what I consider misguided use.\n\n> > 5.  should we ever do confidential transactions we can't prevent it without\n> >     compromising privacy / allowed transfers\n>\n> I'm not an expert, but it seems to me that you can do that with range\n> proofs. The range proof for >dust doesn't need to become part of the\n> block chain, it can be relay only.\n\nYeah, range proofs have a non-hidden range; the lower bound can be nonzero, which could be required as part of a relay policy.\n\nCheers,\n\n--\nPieter"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Pieter Wuille"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3001
        }
    }
]