[
    {
        "title": "[bitcoin-dev] [Pre-BIP] Community Consensus Voting System",
        "thread_messages": [
            {
                "author": "t. khan",
                "date": "2017-02-02T19:39:51",
                "message_text_only": "Please comment on this work-in-progress BIP.\n\nThanks,\n\n- t.k.\n\n----------------------\nBIP: ?\nLayer: Process\nTitle: Community Consensus Voting System\nAuthor: t.khan <teekhan42 at gmail.com>\nComments-Summary: No comments yet.\nComments-URI: TBD\nStatus: Draft\nType: Standards Track\nCreated: 2017-02-02\nLicense: BSD-2\nVoting Address: 3CoFA3JiK5wxe9ze2HoDGDTmZvkE5Uuwh8  (just an example, don\u2019t\nsend to this!)\n\nAbstract\nCommunity Consensus Voting System (CCVS) will allow developers to measure\nsupport for BIPs prior to implementation.\n\nMotivation\nWe currently have no way of measuring consensus for potential changes to\nthe Bitcoin protocol. This is especially problematic for controversial\nchanges such as the max block size limit. As a result, we have many\nproposed solutions but no clear direction.\n\nAlso, due to our lack of ability to measure consensus, there is a general\nfeeling among many in the community that developers aren\u2019t listening to\ntheir concerns. This is a valid complaint, as it\u2019s not possible to listen\nto thousands of voices all shouting different things in a crowded\nroom\u2014basically the situation in the Bitcoin community today.\n\nThe CCVS will allow the general public, miners, companies using Bitcoin,\nand developers to vote for their preferred BIP in a way that\u2019s public and\nrelatively difficult (expensive) to manipulate.\n\nSpecification\nEach competing BIP will be assigned a unique bitcoin address which is added\nto each header. Anyone who wanted to vote would cast their ballot by\nsending a small amount (0.0001 btc) to their preferred BIP's address. Each\ntransaction counts as 1 vote.\n\nConfirmed Vote Multiplier:\nMining Pools, companies using Bitcoin, and Core maintainers/contributors\nare allowed one confirmed vote each. A confirmed vote is worth 10,000x a\nregular vote.\n\nFor example:\n\nSlush Pool casts a vote for their preferred BIP and then states publicly\n(on their blog) their vote and the transaction ID and emails the URL to the\nadmin of this system. In the final tally, this vote will count as 10,000\nvotes.\n\nCoinbase, Antpool, BitPay, BitFury, etc., all do the same.\n\nConfirmed votes would be added to a new section in each respective BIP as a\npublic record.\n\nVoting would run for a pre-defined period, ending when a particular block\nnumber is mined.\n\n\nRationale\nConfirmed Vote Multiplier - The purpose of this is twofold; it gives a\nlarger voice to organizations and the people who will have to do the work\nto implement whatever BIP the community prefers, and it will negate the\neffect of anyone trying to skew the results by voting repeatedly.\n\nDefinitions\nMiner: any individual or organization that has mined at least one valid\nblock in the last 2016 blocks.\n\nCompany using Bitcoin: any organization using Bitcoin for financial, asset\nor other purposes, with either under development and released solutions.\n\nDeveloper: any individual who has or had commit access, and any individual\nwho has authored a BIP\n\nUnresolved Issues\nNode voting: It would be desirable for any full node running an up-to-date\nblockchain to also be able to vote with a multiplier (e.g. 100x). But as\nthis would require code changes, it is outside the scope of this BIP.\n\nCopyright\nThis BIP is licensed under the BSD 2-clause license.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170202/b354d474/attachment.html>"
            },
            {
                "author": "David Vorick",
                "date": "2017-02-02T23:19:39",
                "message_text_only": "I like the idea of having some way for developers to show that they've\ngiven an idea legitimate consideration, as I feel some proposals are often\nconsidered much more in depth before rejection than the proposer realizes,\nhowever I don't think any sort of on-chain system really makes sense. It\ncomplicates things a lot, adds code, incentives, etc. when really all you\ncare about is some sort of indication of consideration, support, or\nrejection.\n\nI also prefer to think of Bitcoin as a system of vetos rather than a system\nof approvals. A lot of times changes will be small, highly technical, and\nhave no visible impact to your every day user. These types of changes don't\nreally need support outside the devs. Furthermore, I frankly don't give a\ncrap if we proposal has support from 85% of the participants if there is a\nlegitimate technical, social, or political reason that it is a bad idea.\n\nAnd finally, I don't think it should cost money or political power to raise\nan objection. A 13yo who has never been seen before should be able to raise\nan objection if they indeed have a legitimate objection. Involving money is\nalmost certainly going to shut down important valid opinions.\n\nAnd again, I mostly agree with the motivation. It would be good if it were\neasier to figure out who had considered a proposal and what their\nobjections or praises were. But I would like to see that without any\nsystemization around what is required to pass or fail a proposal, and with\nno barrier to entry (such as voting or sending coins or having a recognized\nname like 'Bitfury') to provide an opinion.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170202/da014829/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-02-03T00:24:09",
                "message_text_only": "Strongly disagree with buying \"votes\", or portraying open standards as a \nvoting process. Also, this depends on address reuse, so it's fundamentally \nflawed in design.\n\nSome way for people to express their support weighed by coins (without \nlosing/spending them), and possibly weighed by running a full node, might \nstill be desirable. The most straightforward way to do this is to support \nmessage signatures somehow (ideally without using the same pubkey as \nspending), and some [inherently unreliable, but perhaps useful if the \ncommunity \"colludes\" to not-cheat] way to sign with ones' full node.\n\nNote also that the BIP process already has BIP Comments for leaving textual \nopinions on the BIP unrelated to stake. See BIP 2 for details on that.\n\nLuke\n\n\nOn Thursday, February 02, 2017 7:39:51 PM t. khan via bitcoin-dev wrote:\n> Please comment on this work-in-progress BIP.\n> \n> Thanks,\n> \n> - t.k.\n> \n> ----------------------\n> BIP: ?\n> Layer: Process\n> Title: Community Consensus Voting System\n> Author: t.khan <teekhan42 at gmail.com>\n> Comments-Summary: No comments yet.\n> Comments-URI: TBD\n> Status: Draft\n> Type: Standards Track\n> Created: 2017-02-02\n> License: BSD-2\n> Voting Address: 3CoFA3JiK5wxe9ze2HoDGDTmZvkE5Uuwh8  (just an example, don\u2019t\n> send to this!)\n> \n> Abstract\n> Community Consensus Voting System (CCVS) will allow developers to measure\n> support for BIPs prior to implementation.\n> \n> Motivation\n> We currently have no way of measuring consensus for potential changes to\n> the Bitcoin protocol. This is especially problematic for controversial\n> changes such as the max block size limit. As a result, we have many\n> proposed solutions but no clear direction.\n> \n> Also, due to our lack of ability to measure consensus, there is a general\n> feeling among many in the community that developers aren\u2019t listening to\n> their concerns. This is a valid complaint, as it\u2019s not possible to listen\n> to thousands of voices all shouting different things in a crowded\n> room\u2014basically the situation in the Bitcoin community today.\n> \n> The CCVS will allow the general public, miners, companies using Bitcoin,\n> and developers to vote for their preferred BIP in a way that\u2019s public and\n> relatively difficult (expensive) to manipulate.\n> \n> Specification\n> Each competing BIP will be assigned a unique bitcoin address which is added\n> to each header. Anyone who wanted to vote would cast their ballot by\n> sending a small amount (0.0001 btc) to their preferred BIP's address. Each\n> transaction counts as 1 vote.\n> \n> Confirmed Vote Multiplier:\n> Mining Pools, companies using Bitcoin, and Core maintainers/contributors\n> are allowed one confirmed vote each. A confirmed vote is worth 10,000x a\n> regular vote.\n> \n> For example:\n> \n> Slush Pool casts a vote for their preferred BIP and then states publicly\n> (on their blog) their vote and the transaction ID and emails the URL to the\n> admin of this system. In the final tally, this vote will count as 10,000\n> votes.\n> \n> Coinbase, Antpool, BitPay, BitFury, etc., all do the same.\n> \n> Confirmed votes would be added to a new section in each respective BIP as a\n> public record.\n> \n> Voting would run for a pre-defined period, ending when a particular block\n> number is mined.\n> \n> \n> Rationale\n> Confirmed Vote Multiplier - The purpose of this is twofold; it gives a\n> larger voice to organizations and the people who will have to do the work\n> to implement whatever BIP the community prefers, and it will negate the\n> effect of anyone trying to skew the results by voting repeatedly.\n> \n> Definitions\n> Miner: any individual or organization that has mined at least one valid\n> block in the last 2016 blocks.\n> \n> Company using Bitcoin: any organization using Bitcoin for financial, asset\n> or other purposes, with either under development and released solutions.\n> \n> Developer: any individual who has or had commit access, and any individual\n> who has authored a BIP\n> \n> Unresolved Issues\n> Node voting: It would be desirable for any full node running an up-to-date\n> blockchain to also be able to vote with a multiplier (e.g. 100x). But as\n> this would require code changes, it is outside the scope of this BIP.\n> \n> Copyright\n> This BIP is licensed under the BSD 2-clause license."
            },
            {
                "author": "Dave Scotese",
                "date": "2017-02-03T01:32:34",
                "message_text_only": "There are two ideas here for \"on-chain\" voting, both of which require\nchanges to the software.  I agree with David that on-chain solutions\ncomplicate things.  Both proposals can be effected without any software\nchanges:\n\nThose who wish to use proof of stake can provide a service for making\nvanity addresses containing some indicator of the proposal to be supported\n- 1bigblock or 12mbblk or whatever - based on a supporter-provided secret\nkey, and then supporters can move their bitcoin into their own vanity\naddress and then whoever wants to can create a website to display the\nmatching addresses and explain that this is the financial power in the\nhands of supporters and how to add your \"financial power vote.\"\n\nThose who simply want to \"buy votes\" can use their funds in marketing\nefforts to promote the proposal they support.\n\nThis second method, of course, can be abused.  The first actually requires\npeople to control bitcoin in order to represent support.  Counting actual,\nreal people is still a technology in its infancy, and I don't think I want\nto see it progress much. People are not units, but individuals, and their\nvalue only becomes correlated to their net worth after they've been alive\nfor many years, and even then, some of the best people have died paupers.\nIf bitcoin-discuss got more traffic, I think this discussion would be\nbetter had on that list.\n\nnotplato\n\nOn Thu, Feb 2, 2017 at 4:24 PM, Luke Dashjr via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Strongly disagree with buying \"votes\", or portraying open standards as a\n> voting process. Also, this depends on address reuse, so it's fundamentally\n> flawed in design.\n>\n> Some way for people to express their support weighed by coins (without\n> losing/spending them), and possibly weighed by running a full node, might\n> still be desirable. The most straightforward way to do this is to support\n> message signatures somehow (ideally without using the same pubkey as\n> spending), and some [inherently unreliable, but perhaps useful if the\n> community \"colludes\" to not-cheat] way to sign with ones' full node.\n>\n> Note also that the BIP process already has BIP Comments for leaving textual\n> opinions on the BIP unrelated to stake. See BIP 2 for details on that.\n>\n> Luke\n>\n>\n> On Thursday, February 02, 2017 7:39:51 PM t. khan via bitcoin-dev wrote:\n> > Please comment on this work-in-progress BIP.\n> >\n> > Thanks,\n> >\n> > - t.k.\n> >\n> > ----------------------\n> > BIP: ?\n> > Layer: Process\n> > Title: Community Consensus Voting System\n> > Author: t.khan <teekhan42 at gmail.com>\n> > Comments-Summary: No comments yet.\n> > Comments-URI: TBD\n> > Status: Draft\n> > Type: Standards Track\n> > Created: 2017-02-02\n> > License: BSD-2\n> > Voting Address: 3CoFA3JiK5wxe9ze2HoDGDTmZvkE5Uuwh8  (just an example,\n> don\u2019t\n> > send to this!)\n> >\n> > Abstract\n> > Community Consensus Voting System (CCVS) will allow developers to measure\n> > support for BIPs prior to implementation.\n> >\n> > Motivation\n> > We currently have no way of measuring consensus for potential changes to\n> > the Bitcoin protocol. This is especially problematic for controversial\n> > changes such as the max block size limit. As a result, we have many\n> > proposed solutions but no clear direction.\n> >\n> > Also, due to our lack of ability to measure consensus, there is a general\n> > feeling among many in the community that developers aren\u2019t listening to\n> > their concerns. This is a valid complaint, as it\u2019s not possible to listen\n> > to thousands of voices all shouting different things in a crowded\n> > room\u2014basically the situation in the Bitcoin community today.\n> >\n> > The CCVS will allow the general public, miners, companies using Bitcoin,\n> > and developers to vote for their preferred BIP in a way that\u2019s public and\n> > relatively difficult (expensive) to manipulate.\n> >\n> > Specification\n> > Each competing BIP will be assigned a unique bitcoin address which is\n> added\n> > to each header. Anyone who wanted to vote would cast their ballot by\n> > sending a small amount (0.0001 btc) to their preferred BIP's address.\n> Each\n> > transaction counts as 1 vote.\n> >\n> > Confirmed Vote Multiplier:\n> > Mining Pools, companies using Bitcoin, and Core maintainers/contributors\n> > are allowed one confirmed vote each. A confirmed vote is worth 10,000x a\n> > regular vote.\n> >\n> > For example:\n> >\n> > Slush Pool casts a vote for their preferred BIP and then states publicly\n> > (on their blog) their vote and the transaction ID and emails the URL to\n> the\n> > admin of this system. In the final tally, this vote will count as 10,000\n> > votes.\n> >\n> > Coinbase, Antpool, BitPay, BitFury, etc., all do the same.\n> >\n> > Confirmed votes would be added to a new section in each respective BIP\n> as a\n> > public record.\n> >\n> > Voting would run for a pre-defined period, ending when a particular block\n> > number is mined.\n> >\n> >\n> > Rationale\n> > Confirmed Vote Multiplier - The purpose of this is twofold; it gives a\n> > larger voice to organizations and the people who will have to do the work\n> > to implement whatever BIP the community prefers, and it will negate the\n> > effect of anyone trying to skew the results by voting repeatedly.\n> >\n> > Definitions\n> > Miner: any individual or organization that has mined at least one valid\n> > block in the last 2016 blocks.\n> >\n> > Company using Bitcoin: any organization using Bitcoin for financial,\n> asset\n> > or other purposes, with either under development and released solutions.\n> >\n> > Developer: any individual who has or had commit access, and any\n> individual\n> > who has authored a BIP\n> >\n> > Unresolved Issues\n> > Node voting: It would be desirable for any full node running an\n> up-to-date\n> > blockchain to also be able to vote with a multiplier (e.g. 100x). But as\n> > this would require code changes, it is outside the scope of this BIP.\n> >\n> > Copyright\n> > This BIP is licensed under the BSD 2-clause license.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170202/b5c28eed/attachment-0001.html>"
            },
            {
                "author": "alp alp",
                "date": "2017-02-03T16:19:19",
                "message_text_only": "This proposal seems hopelessly broken.\n\nWho decides on which companies are eligible?  Is there some kind of\ncentralized database that one registers?  Who administers this?  What is to\nstop someone from creating a million fake companies to sway the voting?\nHow does a company make it's vote?  How does one verify that the person\nvoting on behalf of a company is actually the correct person?\n\n\n\nOn Thu, Feb 2, 2017 at 7:32 PM, Dave Scotese via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> There are two ideas here for \"on-chain\" voting, both of which require\n> changes to the software.  I agree with David that on-chain solutions\n> complicate things.  Both proposals can be effected without any software\n> changes:\n>\n> Those who wish to use proof of stake can provide a service for making\n> vanity addresses containing some indicator of the proposal to be supported\n> - 1bigblock or 12mbblk or whatever - based on a supporter-provided secret\n> key, and then supporters can move their bitcoin into their own vanity\n> address and then whoever wants to can create a website to display the\n> matching addresses and explain that this is the financial power in the\n> hands of supporters and how to add your \"financial power vote.\"\n>\n> Those who simply want to \"buy votes\" can use their funds in marketing\n> efforts to promote the proposal they support.\n>\n> This second method, of course, can be abused.  The first actually requires\n> people to control bitcoin in order to represent support.  Counting actual,\n> real people is still a technology in its infancy, and I don't think I want\n> to see it progress much. People are not units, but individuals, and their\n> value only becomes correlated to their net worth after they've been alive\n> for many years, and even then, some of the best people have died paupers.\n> If bitcoin-discuss got more traffic, I think this discussion would be\n> better had on that list.\n>\n> notplato\n>\n> On Thu, Feb 2, 2017 at 4:24 PM, Luke Dashjr via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Strongly disagree with buying \"votes\", or portraying open standards as a\n>> voting process. Also, this depends on address reuse, so it's fundamentally\n>> flawed in design.\n>>\n>> Some way for people to express their support weighed by coins (without\n>> losing/spending them), and possibly weighed by running a full node, might\n>> still be desirable. The most straightforward way to do this is to support\n>> message signatures somehow (ideally without using the same pubkey as\n>> spending), and some [inherently unreliable, but perhaps useful if the\n>> community \"colludes\" to not-cheat] way to sign with ones' full node.\n>>\n>> Note also that the BIP process already has BIP Comments for leaving\n>> textual\n>> opinions on the BIP unrelated to stake. See BIP 2 for details on that.\n>>\n>> Luke\n>>\n>>\n>> On Thursday, February 02, 2017 7:39:51 PM t. khan via bitcoin-dev wrote:\n>> > Please comment on this work-in-progress BIP.\n>> >\n>> > Thanks,\n>> >\n>> > - t.k.\n>> >\n>> > ----------------------\n>> > BIP: ?\n>> > Layer: Process\n>> > Title: Community Consensus Voting System\n>> > Author: t.khan <teekhan42 at gmail.com>\n>> > Comments-Summary: No comments yet.\n>> > Comments-URI: TBD\n>> > Status: Draft\n>> > Type: Standards Track\n>> > Created: 2017-02-02\n>> > License: BSD-2\n>> > Voting Address: 3CoFA3JiK5wxe9ze2HoDGDTmZvkE5Uuwh8  (just an example,\n>> don\u2019t\n>> > send to this!)\n>> >\n>> > Abstract\n>> > Community Consensus Voting System (CCVS) will allow developers to\n>> measure\n>> > support for BIPs prior to implementation.\n>> >\n>> > Motivation\n>> > We currently have no way of measuring consensus for potential changes to\n>> > the Bitcoin protocol. This is especially problematic for controversial\n>> > changes such as the max block size limit. As a result, we have many\n>> > proposed solutions but no clear direction.\n>> >\n>> > Also, due to our lack of ability to measure consensus, there is a\n>> general\n>> > feeling among many in the community that developers aren\u2019t listening to\n>> > their concerns. This is a valid complaint, as it\u2019s not possible to\n>> listen\n>> > to thousands of voices all shouting different things in a crowded\n>> > room\u2014basically the situation in the Bitcoin community today.\n>> >\n>> > The CCVS will allow the general public, miners, companies using Bitcoin,\n>> > and developers to vote for their preferred BIP in a way that\u2019s public\n>> and\n>> > relatively difficult (expensive) to manipulate.\n>> >\n>> > Specification\n>> > Each competing BIP will be assigned a unique bitcoin address which is\n>> added\n>> > to each header. Anyone who wanted to vote would cast their ballot by\n>> > sending a small amount (0.0001 btc) to their preferred BIP's address.\n>> Each\n>> > transaction counts as 1 vote.\n>> >\n>> > Confirmed Vote Multiplier:\n>> > Mining Pools, companies using Bitcoin, and Core maintainers/contributors\n>> > are allowed one confirmed vote each. A confirmed vote is worth 10,000x a\n>> > regular vote.\n>> >\n>> > For example:\n>> >\n>> > Slush Pool casts a vote for their preferred BIP and then states publicly\n>> > (on their blog) their vote and the transaction ID and emails the URL to\n>> the\n>> > admin of this system. In the final tally, this vote will count as 10,000\n>> > votes.\n>> >\n>> > Coinbase, Antpool, BitPay, BitFury, etc., all do the same.\n>> >\n>> > Confirmed votes would be added to a new section in each respective BIP\n>> as a\n>> > public record.\n>> >\n>> > Voting would run for a pre-defined period, ending when a particular\n>> block\n>> > number is mined.\n>> >\n>> >\n>> > Rationale\n>> > Confirmed Vote Multiplier - The purpose of this is twofold; it gives a\n>> > larger voice to organizations and the people who will have to do the\n>> work\n>> > to implement whatever BIP the community prefers, and it will negate the\n>> > effect of anyone trying to skew the results by voting repeatedly.\n>> >\n>> > Definitions\n>> > Miner: any individual or organization that has mined at least one valid\n>> > block in the last 2016 blocks.\n>> >\n>> > Company using Bitcoin: any organization using Bitcoin for financial,\n>> asset\n>> > or other purposes, with either under development and released solutions.\n>> >\n>> > Developer: any individual who has or had commit access, and any\n>> individual\n>> > who has authored a BIP\n>> >\n>> > Unresolved Issues\n>> > Node voting: It would be desirable for any full node running an\n>> up-to-date\n>> > blockchain to also be able to vote with a multiplier (e.g. 100x). But as\n>> > this would require code changes, it is outside the scope of this BIP.\n>> >\n>> > Copyright\n>> > This BIP is licensed under the BSD 2-clause license.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n>\n> --\n> I like to provide some work at no charge to prove my value. Do you need a\n> techie?\n> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n> <http://www.memeracing.net> (in alpha).\n> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n> which now accepts Bitcoin.\n> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n> \"He ought to find it more profitable to play by the rules\" - Satoshi\n> Nakamoto\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170203/1e9eea61/attachment.html>"
            },
            {
                "author": "t. khan",
                "date": "2017-02-03T18:20:13",
                "message_text_only": "Most of these are answered in the BIP, but for clarity:\n\n>Who decides on which companies are eligible?\nPreferably no one decides. The company would have to exist prior to the\nvote, and would need a public-facing website. In the event of contested\nvotes (meaning someone finds evidence of a fake company), the admin could\ninvestigate and post results.\n\n>Is there some kind of centralized database that one registers?\nNo.\n\n>Who administers this?\nI don't know. I'm happy to volunteer, if no one else wants to be\nresponsible for it. The only task would be adding the confirmed votes to\neach respective BIP. From there, everything's public and can be confirmed\nby everyone.\n\n>What is to stop someone from creating a million fake companies to sway the\nvoting?\nThe logistics of doing that prevent it. But let's say 10 fake companies ...\nfirst, you'd need to register ten domain names, host and customize the\nwebsite, all before the vote and in a way that no one would notice.\n\n>How does a company make it's vote?\nSomeone at the company sends a very small transaction to the BIP's vote\naddress. Someone at the company then posts what the vote was and its\ntransaction ID on the company's blog/twitter, etc., and then emails the URL\nto the administrator.\n\n>How does one verify that the person voting on behalf of a company is\nactually the correct person?\nThey post it on their company blog.\n\nOn Fri, Feb 3, 2017 at 11:19 AM, alp alp via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> This proposal seems hopelessly broken.\n>\n> Who decides on which companies are eligible?  Is there some kind of\n> centralized database that one registers?  Who administers this?  What is to\n> stop someone from creating a million fake companies to sway the voting?\n> How does a company make it's vote?  How does one verify that the person\n> voting on behalf of a company is actually the correct person?\n>\n>\n>\n> On Thu, Feb 2, 2017 at 7:32 PM, Dave Scotese via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> There are two ideas here for \"on-chain\" voting, both of which require\n>> changes to the software.  I agree with David that on-chain solutions\n>> complicate things.  Both proposals can be effected without any software\n>> changes:\n>>\n>> Those who wish to use proof of stake can provide a service for making\n>> vanity addresses containing some indicator of the proposal to be supported\n>> - 1bigblock or 12mbblk or whatever - based on a supporter-provided secret\n>> key, and then supporters can move their bitcoin into their own vanity\n>> address and then whoever wants to can create a website to display the\n>> matching addresses and explain that this is the financial power in the\n>> hands of supporters and how to add your \"financial power vote.\"\n>>\n>> Those who simply want to \"buy votes\" can use their funds in marketing\n>> efforts to promote the proposal they support.\n>>\n>> This second method, of course, can be abused.  The first actually\n>> requires people to control bitcoin in order to represent support.  Counting\n>> actual, real people is still a technology in its infancy, and I don't think\n>> I want to see it progress much. People are not units, but individuals, and\n>> their value only becomes correlated to their net worth after they've been\n>> alive for many years, and even then, some of the best people have died\n>> paupers. If bitcoin-discuss got more traffic, I think this discussion would\n>> be better had on that list.\n>>\n>> notplato\n>>\n>> On Thu, Feb 2, 2017 at 4:24 PM, Luke Dashjr via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Strongly disagree with buying \"votes\", or portraying open standards as a\n>>> voting process. Also, this depends on address reuse, so it's\n>>> fundamentally\n>>> flawed in design.\n>>>\n>>> Some way for people to express their support weighed by coins (without\n>>> losing/spending them), and possibly weighed by running a full node, might\n>>> still be desirable. The most straightforward way to do this is to support\n>>> message signatures somehow (ideally without using the same pubkey as\n>>> spending), and some [inherently unreliable, but perhaps useful if the\n>>> community \"colludes\" to not-cheat] way to sign with ones' full node.\n>>>\n>>> Note also that the BIP process already has BIP Comments for leaving\n>>> textual\n>>> opinions on the BIP unrelated to stake. See BIP 2 for details on that.\n>>>\n>>> Luke\n>>>\n>>>\n>>> On Thursday, February 02, 2017 7:39:51 PM t. khan via bitcoin-dev wrote:\n>>> > Please comment on this work-in-progress BIP.\n>>> >\n>>> > Thanks,\n>>> >\n>>> > - t.k.\n>>> >\n>>> > ----------------------\n>>> > BIP: ?\n>>> > Layer: Process\n>>> > Title: Community Consensus Voting System\n>>> > Author: t.khan <teekhan42 at gmail.com>\n>>> > Comments-Summary: No comments yet.\n>>> > Comments-URI: TBD\n>>> > Status: Draft\n>>> > Type: Standards Track\n>>> > Created: 2017-02-02\n>>> > License: BSD-2\n>>> > Voting Address: 3CoFA3JiK5wxe9ze2HoDGDTmZvkE5Uuwh8  (just an example,\n>>> don\u2019t\n>>> > send to this!)\n>>> >\n>>> > Abstract\n>>> > Community Consensus Voting System (CCVS) will allow developers to\n>>> measure\n>>> > support for BIPs prior to implementation.\n>>> >\n>>> > Motivation\n>>> > We currently have no way of measuring consensus for potential changes\n>>> to\n>>> > the Bitcoin protocol. This is especially problematic for controversial\n>>> > changes such as the max block size limit. As a result, we have many\n>>> > proposed solutions but no clear direction.\n>>> >\n>>> > Also, due to our lack of ability to measure consensus, there is a\n>>> general\n>>> > feeling among many in the community that developers aren\u2019t listening to\n>>> > their concerns. This is a valid complaint, as it\u2019s not possible to\n>>> listen\n>>> > to thousands of voices all shouting different things in a crowded\n>>> > room\u2014basically the situation in the Bitcoin community today.\n>>> >\n>>> > The CCVS will allow the general public, miners, companies using\n>>> Bitcoin,\n>>> > and developers to vote for their preferred BIP in a way that\u2019s public\n>>> and\n>>> > relatively difficult (expensive) to manipulate.\n>>> >\n>>> > Specification\n>>> > Each competing BIP will be assigned a unique bitcoin address which is\n>>> added\n>>> > to each header. Anyone who wanted to vote would cast their ballot by\n>>> > sending a small amount (0.0001 btc) to their preferred BIP's address.\n>>> Each\n>>> > transaction counts as 1 vote.\n>>> >\n>>> > Confirmed Vote Multiplier:\n>>> > Mining Pools, companies using Bitcoin, and Core\n>>> maintainers/contributors\n>>> > are allowed one confirmed vote each. A confirmed vote is worth 10,000x\n>>> a\n>>> > regular vote.\n>>> >\n>>> > For example:\n>>> >\n>>> > Slush Pool casts a vote for their preferred BIP and then states\n>>> publicly\n>>> > (on their blog) their vote and the transaction ID and emails the URL\n>>> to the\n>>> > admin of this system. In the final tally, this vote will count as\n>>> 10,000\n>>> > votes.\n>>> >\n>>> > Coinbase, Antpool, BitPay, BitFury, etc., all do the same.\n>>> >\n>>> > Confirmed votes would be added to a new section in each respective BIP\n>>> as a\n>>> > public record.\n>>> >\n>>> > Voting would run for a pre-defined period, ending when a particular\n>>> block\n>>> > number is mined.\n>>> >\n>>> >\n>>> > Rationale\n>>> > Confirmed Vote Multiplier - The purpose of this is twofold; it gives a\n>>> > larger voice to organizations and the people who will have to do the\n>>> work\n>>> > to implement whatever BIP the community prefers, and it will negate the\n>>> > effect of anyone trying to skew the results by voting repeatedly.\n>>> >\n>>> > Definitions\n>>> > Miner: any individual or organization that has mined at least one valid\n>>> > block in the last 2016 blocks.\n>>> >\n>>> > Company using Bitcoin: any organization using Bitcoin for financial,\n>>> asset\n>>> > or other purposes, with either under development and released\n>>> solutions.\n>>> >\n>>> > Developer: any individual who has or had commit access, and any\n>>> individual\n>>> > who has authored a BIP\n>>> >\n>>> > Unresolved Issues\n>>> > Node voting: It would be desirable for any full node running an\n>>> up-to-date\n>>> > blockchain to also be able to vote with a multiplier (e.g. 100x). But\n>>> as\n>>> > this would require code changes, it is outside the scope of this BIP.\n>>> >\n>>> > Copyright\n>>> > This BIP is licensed under the BSD 2-clause license.\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>>\n>>\n>> --\n>> I like to provide some work at no charge to prove my value. Do you need a\n>> techie?\n>> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n>> <http://www.memeracing.net> (in alpha).\n>> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n>> which now accepts Bitcoin.\n>> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n>> \"He ought to find it more profitable to play by the rules\" - Satoshi\n>> Nakamoto\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170203/062e0401/attachment-0001.html>"
            },
            {
                "author": "alp alp",
                "date": "2017-02-03T19:22:10",
                "message_text_only": "These are non-answers.  Someone must decide.  Someone must decide what kind\nof company counts (e.g. does a dark market seller count as a business?\nDoes some guy who sells $10/year worth of goods using Bitcoin count the\nsame as large companies like Coinbase/BitPay/Blockstream).  Someone must\ndecide which websites are checked for votes or addresses.  Someone must\ndecide if a rogue employee made a transaction on behalf of the company or\nnot.\n\nRegistering domain names is trivial and can be automated if the incentives\nwere needed for it.\n\nYou mention developers who have commit access.  This excludes the vast\nmajority of developers.  You also don't mention which repositories count.\nDo the developers of bcoin count or not?\n\nThese questions all would need to be answered before any kind of proposal\nlike this can be taken seriously.  Without these kinds of answers, this\nproposal is far from complete.\n\n\nOn Fri, Feb 3, 2017 at 12:20 PM, t. khan <teekhan42 at gmail.com> wrote:\n\n> Most of these are answered in the BIP, but for clarity:\n>\n> >Who decides on which companies are eligible?\n> Preferably no one decides. The company would have to exist prior to the\n> vote, and would need a public-facing website. In the event of contested\n> votes (meaning someone finds evidence of a fake company), the admin could\n> investigate and post results.\n>\n> >Is there some kind of centralized database that one registers?\n> No.\n>\n> >Who administers this?\n> I don't know. I'm happy to volunteer, if no one else wants to be\n> responsible for it. The only task would be adding the confirmed votes to\n> each respective BIP. From there, everything's public and can be confirmed\n> by everyone.\n>\n> >What is to stop someone from creating a million fake companies to sway\n> the voting?\n> The logistics of doing that prevent it. But let's say 10 fake companies\n> ... first, you'd need to register ten domain names, host and customize the\n> website, all before the vote and in a way that no one would notice.\n>\n> >How does a company make it's vote?\n> Someone at the company sends a very small transaction to the BIP's vote\n> address. Someone at the company then posts what the vote was and its\n> transaction ID on the company's blog/twitter, etc., and then emails the URL\n> to the administrator.\n>\n> >How does one verify that the person voting on behalf of a company is\n> actually the correct person?\n> They post it on their company blog.\n>\n> On Fri, Feb 3, 2017 at 11:19 AM, alp alp via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> This proposal seems hopelessly broken.\n>>\n>> Who decides on which companies are eligible?  Is there some kind of\n>> centralized database that one registers?  Who administers this?  What is to\n>> stop someone from creating a million fake companies to sway the voting?\n>> How does a company make it's vote?  How does one verify that the person\n>> voting on behalf of a company is actually the correct person?\n>>\n>>\n>>\n>> On Thu, Feb 2, 2017 at 7:32 PM, Dave Scotese via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> There are two ideas here for \"on-chain\" voting, both of which require\n>>> changes to the software.  I agree with David that on-chain solutions\n>>> complicate things.  Both proposals can be effected without any software\n>>> changes:\n>>>\n>>> Those who wish to use proof of stake can provide a service for making\n>>> vanity addresses containing some indicator of the proposal to be supported\n>>> - 1bigblock or 12mbblk or whatever - based on a supporter-provided secret\n>>> key, and then supporters can move their bitcoin into their own vanity\n>>> address and then whoever wants to can create a website to display the\n>>> matching addresses and explain that this is the financial power in the\n>>> hands of supporters and how to add your \"financial power vote.\"\n>>>\n>>> Those who simply want to \"buy votes\" can use their funds in marketing\n>>> efforts to promote the proposal they support.\n>>>\n>>> This second method, of course, can be abused.  The first actually\n>>> requires people to control bitcoin in order to represent support.  Counting\n>>> actual, real people is still a technology in its infancy, and I don't think\n>>> I want to see it progress much. People are not units, but individuals, and\n>>> their value only becomes correlated to their net worth after they've been\n>>> alive for many years, and even then, some of the best people have died\n>>> paupers. If bitcoin-discuss got more traffic, I think this discussion would\n>>> be better had on that list.\n>>>\n>>> notplato\n>>>\n>>> On Thu, Feb 2, 2017 at 4:24 PM, Luke Dashjr via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Strongly disagree with buying \"votes\", or portraying open standards as a\n>>>> voting process. Also, this depends on address reuse, so it's\n>>>> fundamentally\n>>>> flawed in design.\n>>>>\n>>>> Some way for people to express their support weighed by coins (without\n>>>> losing/spending them), and possibly weighed by running a full node,\n>>>> might\n>>>> still be desirable. The most straightforward way to do this is to\n>>>> support\n>>>> message signatures somehow (ideally without using the same pubkey as\n>>>> spending), and some [inherently unreliable, but perhaps useful if the\n>>>> community \"colludes\" to not-cheat] way to sign with ones' full node.\n>>>>\n>>>> Note also that the BIP process already has BIP Comments for leaving\n>>>> textual\n>>>> opinions on the BIP unrelated to stake. See BIP 2 for details on that.\n>>>>\n>>>> Luke\n>>>>\n>>>>\n>>>> On Thursday, February 02, 2017 7:39:51 PM t. khan via bitcoin-dev wrote:\n>>>> > Please comment on this work-in-progress BIP.\n>>>> >\n>>>> > Thanks,\n>>>> >\n>>>> > - t.k.\n>>>> >\n>>>> > ----------------------\n>>>> > BIP: ?\n>>>> > Layer: Process\n>>>> > Title: Community Consensus Voting System\n>>>> > Author: t.khan <teekhan42 at gmail.com>\n>>>> > Comments-Summary: No comments yet.\n>>>> > Comments-URI: TBD\n>>>> > Status: Draft\n>>>> > Type: Standards Track\n>>>> > Created: 2017-02-02\n>>>> > License: BSD-2\n>>>> > Voting Address: 3CoFA3JiK5wxe9ze2HoDGDTmZvkE5Uuwh8  (just an\n>>>> example, don\u2019t\n>>>> > send to this!)\n>>>> >\n>>>> > Abstract\n>>>> > Community Consensus Voting System (CCVS) will allow developers to\n>>>> measure\n>>>> > support for BIPs prior to implementation.\n>>>> >\n>>>> > Motivation\n>>>> > We currently have no way of measuring consensus for potential changes\n>>>> to\n>>>> > the Bitcoin protocol. This is especially problematic for controversial\n>>>> > changes such as the max block size limit. As a result, we have many\n>>>> > proposed solutions but no clear direction.\n>>>> >\n>>>> > Also, due to our lack of ability to measure consensus, there is a\n>>>> general\n>>>> > feeling among many in the community that developers aren\u2019t listening\n>>>> to\n>>>> > their concerns. This is a valid complaint, as it\u2019s not possible to\n>>>> listen\n>>>> > to thousands of voices all shouting different things in a crowded\n>>>> > room\u2014basically the situation in the Bitcoin community today.\n>>>> >\n>>>> > The CCVS will allow the general public, miners, companies using\n>>>> Bitcoin,\n>>>> > and developers to vote for their preferred BIP in a way that\u2019s public\n>>>> and\n>>>> > relatively difficult (expensive) to manipulate.\n>>>> >\n>>>> > Specification\n>>>> > Each competing BIP will be assigned a unique bitcoin address which is\n>>>> added\n>>>> > to each header. Anyone who wanted to vote would cast their ballot by\n>>>> > sending a small amount (0.0001 btc) to their preferred BIP's address.\n>>>> Each\n>>>> > transaction counts as 1 vote.\n>>>> >\n>>>> > Confirmed Vote Multiplier:\n>>>> > Mining Pools, companies using Bitcoin, and Core\n>>>> maintainers/contributors\n>>>> > are allowed one confirmed vote each. A confirmed vote is worth\n>>>> 10,000x a\n>>>> > regular vote.\n>>>> >\n>>>> > For example:\n>>>> >\n>>>> > Slush Pool casts a vote for their preferred BIP and then states\n>>>> publicly\n>>>> > (on their blog) their vote and the transaction ID and emails the URL\n>>>> to the\n>>>> > admin of this system. In the final tally, this vote will count as\n>>>> 10,000\n>>>> > votes.\n>>>> >\n>>>> > Coinbase, Antpool, BitPay, BitFury, etc., all do the same.\n>>>> >\n>>>> > Confirmed votes would be added to a new section in each respective\n>>>> BIP as a\n>>>> > public record.\n>>>> >\n>>>> > Voting would run for a pre-defined period, ending when a particular\n>>>> block\n>>>> > number is mined.\n>>>> >\n>>>> >\n>>>> > Rationale\n>>>> > Confirmed Vote Multiplier - The purpose of this is twofold; it gives a\n>>>> > larger voice to organizations and the people who will have to do the\n>>>> work\n>>>> > to implement whatever BIP the community prefers, and it will negate\n>>>> the\n>>>> > effect of anyone trying to skew the results by voting repeatedly.\n>>>> >\n>>>> > Definitions\n>>>> > Miner: any individual or organization that has mined at least one\n>>>> valid\n>>>> > block in the last 2016 blocks.\n>>>> >\n>>>> > Company using Bitcoin: any organization using Bitcoin for financial,\n>>>> asset\n>>>> > or other purposes, with either under development and released\n>>>> solutions.\n>>>> >\n>>>> > Developer: any individual who has or had commit access, and any\n>>>> individual\n>>>> > who has authored a BIP\n>>>> >\n>>>> > Unresolved Issues\n>>>> > Node voting: It would be desirable for any full node running an\n>>>> up-to-date\n>>>> > blockchain to also be able to vote with a multiplier (e.g. 100x). But\n>>>> as\n>>>> > this would require code changes, it is outside the scope of this BIP.\n>>>> >\n>>>> > Copyright\n>>>> > This BIP is licensed under the BSD 2-clause license.\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n>>>\n>>>\n>>> --\n>>> I like to provide some work at no charge to prove my value. Do you need\n>>> a techie?\n>>> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n>>> <http://www.memeracing.net> (in alpha).\n>>> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n>>> which now accepts Bitcoin.\n>>> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n>>> \"He ought to find it more profitable to play by the rules\" - Satoshi\n>>> Nakamoto\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170203/6273c7f4/attachment-0001.html>"
            },
            {
                "author": "t. khan",
                "date": "2017-02-04T21:23:19",
                "message_text_only": "On Fri, Feb 3, 2017 at 2:22 PM, alp alp <alp.bitcoin at gmail.com> wrote:\n\n> These are non-answers.  Someone must decide.  Someone must decide what\n> kind of company counts (e.g. does a dark market seller count as a\n> business?  Does some guy who sells $10/year worth of goods using Bitcoin\n> count the same as large companies like Coinbase/BitPay/Blockstream).\n> Someone must decide which websites are checked for votes or addresses.\n> Someone must decide if a rogue employee made a transaction on behalf of the\n> company or not.\n>\n\nThe less centralized decision making there is, the better. All confirmed\nvotes will be added to each BIP, so everyone can make their own decision as\nto whether or not they want to count it. Remember, this is about gauging\ncommunity support.\n\nRogue employees?: The company in question would have to deal with that.\n\n\n> Registering domain names is trivial and can be automated if the incentives\n> were needed for it.\n>\n\nMountains out of mole hills here\u2014if the domain name is registered after\nvoting is called, that would be a pretty clear indicator it's a fake\ncompany.\n\n\n> You mention developers who have commit access.  This excludes the vast\n> majority of developers.  You also don't mention which repositories count.\n> Do the developers of bcoin count or not?\n>\n\nAfter further consideration, and as the goal of this is to determine what\nthe community will support, developers won't have a confirmed vote. They\nwould get a standard vote though, just like everyone else.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170204/7406b738/attachment.html>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-02-04T00:57:52",
                "message_text_only": "Personally I think once the blocksize arguments are solved, there will\nbe no more contentious changes for this voting system to deal with.\nWhat other contentious issues have come up in the past 3 years or so\nthat wasn't blocksize/scaling related? Do other protocols like TCP/IP\nand the HTTP protocol have developers arguing every day over issues no\none can agree on?\n\nOn 2/3/17, t. khan via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Most of these are answered in the BIP, but for clarity:\n>\n>>Who decides on which companies are eligible?\n> Preferably no one decides. The company would have to exist prior to the\n> vote, and would need a public-facing website. In the event of contested\n> votes (meaning someone finds evidence of a fake company), the admin could\n> investigate and post results.\n>\n>>Is there some kind of centralized database that one registers?\n> No.\n>\n>>Who administers this?\n> I don't know. I'm happy to volunteer, if no one else wants to be\n> responsible for it. The only task would be adding the confirmed votes to\n> each respective BIP. From there, everything's public and can be confirmed\n> by everyone.\n>\n>>What is to stop someone from creating a million fake companies to sway the\n> voting?\n> The logistics of doing that prevent it. But let's say 10 fake companies ...\n> first, you'd need to register ten domain names, host and customize the\n> website, all before the vote and in a way that no one would notice.\n>\n>>How does a company make it's vote?\n> Someone at the company sends a very small transaction to the BIP's vote\n> address. Someone at the company then posts what the vote was and its\n> transaction ID on the company's blog/twitter, etc., and then emails the URL\n> to the administrator.\n>\n>>How does one verify that the person voting on behalf of a company is\n> actually the correct person?\n> They post it on their company blog.\n>\n> On Fri, Feb 3, 2017 at 11:19 AM, alp alp via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> This proposal seems hopelessly broken.\n>>\n>> Who decides on which companies are eligible?  Is there some kind of\n>> centralized database that one registers?  Who administers this?  What is\n>> to\n>> stop someone from creating a million fake companies to sway the voting?\n>> How does a company make it's vote?  How does one verify that the person\n>> voting on behalf of a company is actually the correct person?\n>>\n>>\n>>\n>> On Thu, Feb 2, 2017 at 7:32 PM, Dave Scotese via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> There are two ideas here for \"on-chain\" voting, both of which require\n>>> changes to the software.  I agree with David that on-chain solutions\n>>> complicate things.  Both proposals can be effected without any software\n>>> changes:\n>>>\n>>> Those who wish to use proof of stake can provide a service for making\n>>> vanity addresses containing some indicator of the proposal to be\n>>> supported\n>>> - 1bigblock or 12mbblk or whatever - based on a supporter-provided\n>>> secret\n>>> key, and then supporters can move their bitcoin into their own vanity\n>>> address and then whoever wants to can create a website to display the\n>>> matching addresses and explain that this is the financial power in the\n>>> hands of supporters and how to add your \"financial power vote.\"\n>>>\n>>> Those who simply want to \"buy votes\" can use their funds in marketing\n>>> efforts to promote the proposal they support.\n>>>\n>>> This second method, of course, can be abused.  The first actually\n>>> requires people to control bitcoin in order to represent support.\n>>> Counting\n>>> actual, real people is still a technology in its infancy, and I don't\n>>> think\n>>> I want to see it progress much. People are not units, but individuals,\n>>> and\n>>> their value only becomes correlated to their net worth after they've\n>>> been\n>>> alive for many years, and even then, some of the best people have died\n>>> paupers. If bitcoin-discuss got more traffic, I think this discussion\n>>> would\n>>> be better had on that list.\n>>>\n>>> notplato\n>>>\n>>> On Thu, Feb 2, 2017 at 4:24 PM, Luke Dashjr via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Strongly disagree with buying \"votes\", or portraying open standards as\n>>>> a\n>>>> voting process. Also, this depends on address reuse, so it's\n>>>> fundamentally\n>>>> flawed in design.\n>>>>\n>>>> Some way for people to express their support weighed by coins (without\n>>>> losing/spending them), and possibly weighed by running a full node,\n>>>> might\n>>>> still be desirable. The most straightforward way to do this is to\n>>>> support\n>>>> message signatures somehow (ideally without using the same pubkey as\n>>>> spending), and some [inherently unreliable, but perhaps useful if the\n>>>> community \"colludes\" to not-cheat] way to sign with ones' full node.\n>>>>\n>>>> Note also that the BIP process already has BIP Comments for leaving\n>>>> textual\n>>>> opinions on the BIP unrelated to stake. See BIP 2 for details on that.\n>>>>\n>>>> Luke\n>>>>\n>>>>\n>>>> On Thursday, February 02, 2017 7:39:51 PM t. khan via bitcoin-dev\n>>>> wrote:\n>>>> > Please comment on this work-in-progress BIP.\n>>>> >\n>>>> > Thanks,\n>>>> >\n>>>> > - t.k.\n>>>> >\n>>>> > ----------------------\n>>>> > BIP: ?\n>>>> > Layer: Process\n>>>> > Title: Community Consensus Voting System\n>>>> > Author: t.khan <teekhan42 at gmail.com>\n>>>> > Comments-Summary: No comments yet.\n>>>> > Comments-URI: TBD\n>>>> > Status: Draft\n>>>> > Type: Standards Track\n>>>> > Created: 2017-02-02\n>>>> > License: BSD-2\n>>>> > Voting Address: 3CoFA3JiK5wxe9ze2HoDGDTmZvkE5Uuwh8  (just an example,\n>>>> don\u2019t\n>>>> > send to this!)\n>>>> >\n>>>> > Abstract\n>>>> > Community Consensus Voting System (CCVS) will allow developers to\n>>>> measure\n>>>> > support for BIPs prior to implementation.\n>>>> >\n>>>> > Motivation\n>>>> > We currently have no way of measuring consensus for potential changes\n>>>> to\n>>>> > the Bitcoin protocol. This is especially problematic for\n>>>> > controversial\n>>>> > changes such as the max block size limit. As a result, we have many\n>>>> > proposed solutions but no clear direction.\n>>>> >\n>>>> > Also, due to our lack of ability to measure consensus, there is a\n>>>> general\n>>>> > feeling among many in the community that developers aren\u2019t listening\n>>>> > to\n>>>> > their concerns. This is a valid complaint, as it\u2019s not possible to\n>>>> listen\n>>>> > to thousands of voices all shouting different things in a crowded\n>>>> > room\u2014basically the situation in the Bitcoin community today.\n>>>> >\n>>>> > The CCVS will allow the general public, miners, companies using\n>>>> Bitcoin,\n>>>> > and developers to vote for their preferred BIP in a way that\u2019s public\n>>>> and\n>>>> > relatively difficult (expensive) to manipulate.\n>>>> >\n>>>> > Specification\n>>>> > Each competing BIP will be assigned a unique bitcoin address which is\n>>>> added\n>>>> > to each header. Anyone who wanted to vote would cast their ballot by\n>>>> > sending a small amount (0.0001 btc) to their preferred BIP's address.\n>>>> Each\n>>>> > transaction counts as 1 vote.\n>>>> >\n>>>> > Confirmed Vote Multiplier:\n>>>> > Mining Pools, companies using Bitcoin, and Core\n>>>> maintainers/contributors\n>>>> > are allowed one confirmed vote each. A confirmed vote is worth\n>>>> > 10,000x\n>>>> a\n>>>> > regular vote.\n>>>> >\n>>>> > For example:\n>>>> >\n>>>> > Slush Pool casts a vote for their preferred BIP and then states\n>>>> publicly\n>>>> > (on their blog) their vote and the transaction ID and emails the URL\n>>>> to the\n>>>> > admin of this system. In the final tally, this vote will count as\n>>>> 10,000\n>>>> > votes.\n>>>> >\n>>>> > Coinbase, Antpool, BitPay, BitFury, etc., all do the same.\n>>>> >\n>>>> > Confirmed votes would be added to a new section in each respective\n>>>> > BIP\n>>>> as a\n>>>> > public record.\n>>>> >\n>>>> > Voting would run for a pre-defined period, ending when a particular\n>>>> block\n>>>> > number is mined.\n>>>> >\n>>>> >\n>>>> > Rationale\n>>>> > Confirmed Vote Multiplier - The purpose of this is twofold; it gives\n>>>> > a\n>>>> > larger voice to organizations and the people who will have to do the\n>>>> work\n>>>> > to implement whatever BIP the community prefers, and it will negate\n>>>> > the\n>>>> > effect of anyone trying to skew the results by voting repeatedly.\n>>>> >\n>>>> > Definitions\n>>>> > Miner: any individual or organization that has mined at least one\n>>>> > valid\n>>>> > block in the last 2016 blocks.\n>>>> >\n>>>> > Company using Bitcoin: any organization using Bitcoin for financial,\n>>>> asset\n>>>> > or other purposes, with either under development and released\n>>>> solutions.\n>>>> >\n>>>> > Developer: any individual who has or had commit access, and any\n>>>> individual\n>>>> > who has authored a BIP\n>>>> >\n>>>> > Unresolved Issues\n>>>> > Node voting: It would be desirable for any full node running an\n>>>> up-to-date\n>>>> > blockchain to also be able to vote with a multiplier (e.g. 100x). But\n>>>> as\n>>>> > this would require code changes, it is outside the scope of this BIP.\n>>>> >\n>>>> > Copyright\n>>>> > This BIP is licensed under the BSD 2-clause license.\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n>>>\n>>>\n>>> --\n>>> I like to provide some work at no charge to prove my value. Do you need\n>>> a\n>>> techie?\n>>> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n>>> <http://www.memeracing.net> (in alpha).\n>>> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n>>> which now accepts Bitcoin.\n>>> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n>>> \"He ought to find it more profitable to play by the rules\" - Satoshi\n>>> Nakamoto\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>"
            },
            {
                "author": "Staf Verhaegen",
                "date": "2017-02-11T15:57:46",
                "message_text_only": "Chris Priest via bitcoin-dev schreef op vr 03-02-2017 om 16:57 [-0800]:\n> Personally I think once the blocksize arguments are solved, there will\n> be no more contentious changes for this voting system to deal with.\n> What other contentious issues have come up in the past 3 years or so\n> that wasn't blocksize/scaling related? Do other protocols like TCP/IP\n> and the HTTP protocol have developers arguing every day over issues no\n> one can agree on?\n\nYes, DRM for example.\nStaf.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 230 bytes\nDesc: This is a digitally signed message part\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170211/96521608/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-14T12:33:17",
                "message_text_only": "On Sat, Feb 11, 2017 at 04:57:46PM +0100, Staf Verhaegen via bitcoin-dev wrote:\n> Chris Priest via bitcoin-dev schreef op vr 03-02-2017 om 16:57 [-0800]:\n> > Personally I think once the blocksize arguments are solved, there will\n> > be no more contentious changes for this voting system to deal with.\n> > What other contentious issues have come up in the past 3 years or so\n> > that wasn't blocksize/scaling related? Do other protocols like TCP/IP\n> > and the HTTP protocol have developers arguing every day over issues no\n> > one can agree on?\n> \n> Yes, DRM for example.\n\n...and note how, like blocksize, the roots of the DRM argument at W3C aren't a\ntechnical disagreement, but rather a political disagreement.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170214/042791b2/attachment.sig>"
            },
            {
                "author": "t. khan",
                "date": "2017-02-04T22:02:00",
                "message_text_only": "On Thu, Feb 2, 2017 at 7:24 PM, Luke Dashjr <luke at dashjr.org> wrote:\n\n> Strongly disagree with buying \"votes\", or portraying open standards as a\n> voting process. Also, this depends on address reuse, so it's fundamentally\n> flawed in design.\n>\n\nThe point of this is it's available right now. It's not ideal, but it will\nwork. It doesn't require any code and we can do it today.\n\nIn case you haven't been paying attention; there's already enough support\nfor Unlimited to prevent SegWit from ever being adopted. Without\nsignificant community outreach (which is the purpose of the CCVS) and a\ncompelling solution to max block size, Core as a product is dead.\n\nAlso, you need to be pretty paranoid to believe that address reuse is an\nissue in this situation.\n\nNote also that the BIP process already has BIP Comments for leaving textual\n> opinions on the BIP unrelated to stake. See BIP 2 for details on that.\n>\n\nThis does nothing for the community in general. Plus there's no way to\nmeasure that sort of feedback.\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170204/1e55de20/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Community Consensus Voting System",
            "categories": [
                "bitcoin-dev",
                "Pre-BIP"
            ],
            "authors": [
                "David Vorick",
                "Chris Priest",
                "t. khan",
                "Peter Todd",
                "Dave Scotese",
                "Luke Dashjr",
                "alp alp",
                "Staf Verhaegen"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 59257
        }
    },
    {
        "title": "[bitcoin-dev] Transaction signalling through output address hashing",
        "thread_messages": [
            {
                "author": "John Hardy",
                "date": "2017-02-03T00:13:04",
                "message_text_only": "Currently in order to signal support for changes to Bitcoin, only miners are able to do so on the blockchain through BIP9.\n\n\nOne criticism is that the rest of the community is not able to participate in consensus, and other methods of assessing community support are fuzzy and easily manipulated through Sybil.\n\n\nI was trying to think if there was a way community support could be signaled through transactions without requiring a hard fork, and without increasing the size of transactions at all.\n\n\nMy solution is basically inspired by hashcash and vanity addresses.\n\n\nThe output address of a transaction could basically have the last 4 characters used to signal support for a particular proposal.\n\nTo generate an address with 4 consecutive case-insensitive characters should be roughly 34^4 which is just over a million attempts. On typical hardware this should take less than a second.\n\nAn example bitcoin address that wanted to support the core roadmap might be:\n\n1CLNgjuu8s51umTA76Zi8v6EdvDp8qCorE\n\n\nor to signal support for a big block proposal might be:\n\n1N62SRhBioRFrigS5eJ8kR1WYcfcYr16mB\n\n\nPopularity could be measured weighted by fee paid per voting kb.\n\n\nIssues are that this could lead to transactions been censored by particular miners for political reasons. Also miners might attempt to manipulate the results by stuffing their block with 'fake' transactions. Such attempts could be identified if a large number of voting transactions were not in the mempool.\n\n\nDespite the limitations, I believe this offers a very accessible way to immediately allow the entire economic community to signal their support within transactions. The only cost is that of a tiny hashing PoW that should tie up a CPU for a barely noticeable amount of time, and could be implemented relatively easily into wallet software.\n\n\nFor its weaknesses, surely it is better than the existing methods we use to assess support from the wider economic community?\n\n\nWhile it could just be used for signaling support and giving users a 'voice' on chain, if considered effective it could also be used to activate changes in the future.\n\n\nAny thoughts welcome.\n\n\nThanks,\n\n\nJohn Hardy\n\njohn at seebitcoin.com\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170203/1916d1f2/attachment-0001.html>"
            },
            {
                "author": "Natanael",
                "date": "2017-02-05T16:22:11",
                "message_text_only": "Den 5 feb. 2017 16:33 skrev \"John Hardy via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\nCurrently in order to signal support for changes to Bitcoin, only miners\nare able to do so on the blockchain through BIP9.\n\nOne criticism is that the rest of the community is not able to participate\nin consensus, and other methods of assessing community support are fuzzy\nand easily manipulated through Sybil.\n\nI was trying to think if there was a way community support could be\nsignaled through transactions without requiring a hard fork, and without\nincreasing the size of transactions at all.\n\nMy solution is basically inspired by hashcash and vanity addresses\n\n\nCensorship by miners isn't the only problem. Existing and normal\ntransactions will probabilistically collide with these schemes, and most\nwallets have no straightforward way of supporting it.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170205/019c529b/attachment-0001.html>"
            },
            {
                "author": "Andrew C",
                "date": "2017-02-05T16:26:30",
                "message_text_only": "Instead of using vanity addresses, the transactions could just use an\nOP_RETURN output and express the signalling there.\n\n\nHowever, such a system could be easily gamed by people who simply spam\nthe network with transactions and by miners who choose what transactions\nto include in their blocks.\n\n\nOn 2/2/2017 7:13 PM, John Hardy via bitcoin-dev wrote:\n>\n> Currently in order to signal support for changes to Bitcoin, only\n> miners are able to do so on the blockchain through BIP9.\n>\n>\n> One criticism is that the rest of the community is not able to\n> participate in consensus, and other methods of assessing community\n> support are fuzzy and easily manipulated through Sybil.\n>\n>\n> I was trying to think if there was a way community support could be\n> signaled through transactions without requiring a hard fork, and\n> without increasing the size of transactions at all.\n>\n>\n> My solution is basically inspired by hashcash and vanity addresses.\n>\n>\n> The output address of a transaction could basically have the last 4\n> characters used to signal support for a particular proposal.\n>\n> To generate an address with 4 consecutive case-insensitive characters\n> should be roughly 34^4 which is just over a million attempts. On\n> typical hardware this should take less than a second.\n>\n> An example bitcoin address that wanted to support the core roadmap\n> might be:\n>\n> 1CLNgjuu8s51umTA76Zi8v6EdvDp8q*CorE*\n>\n>\n> or to signal support for a big block proposal might be:\n>\n> 1N62SRhBioRFrigS5eJ8kR1WYcfcYr*16mB*\n>\n>\n> Popularity could be measured weighted by fee paid per voting kb.\n>\n>\n> Issues are that this could lead to transactions been censored by\n> particular miners for political reasons. Also miners might attempt to\n> manipulate the results by stuffing their block with 'fake'\n> transactions. Such attempts could be identified if a large number of\n> voting transactions were not in the mempool.\n>\n>\n> Despite the limitations, I believe this offers a very accessible way\n> to immediately allow the entire economic community to signal their\n> support within transactions. The only cost is that of a tiny hashing\n> PoW that should tie up a CPU for a barely noticeable amount of time,\n> and could be implemented relatively easily into wallet software.\n>\n>\n> For its weaknesses, surely it is better than the existing methods we\n> use to assess support from the wider economic community?\n>\n>\n> While it could just be used for signaling support and giving users a\n> 'voice' on chain, if considered effective it could also be used to\n> activate changes in the future.\n>\n>\n> Any thoughts welcome.\n>\n>\n> Thanks,\n>\n>\n> John Hardy\n>\n> john at seebitcoin.com\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170205/a28d872f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Transaction signalling through output address hashing",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Andrew C",
                "Natanael",
                "John Hardy"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 6444
        }
    },
    {
        "title": "[bitcoin-dev] Proof-of-Loss",
        "thread_messages": [
            {
                "author": "Mirelo",
                "date": "2017-02-04T12:39:45",
                "message_text_only": "An alternative consensus algorithm to both proof-of-work and proof-of-stake, proof-of-loss addresses all their deficiencies, including the lack of an organic block size limit, the risks of mining centralization, and the \"nothing at stake\" problem:\n\nhttps://proof-of-loss.money/\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170204/e577c789/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proof-of-Loss",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Mirelo"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 460
        }
    },
    {
        "title": "[bitcoin-dev] Fw: Transaction signalling through output address hashing",
        "thread_messages": [
            {
                "author": "John Hardy",
                "date": "2017-02-05T21:06:19",
                "message_text_only": "Probabilistic collisions, while present, would be statistically insignificant at 4 chars length.\n\n\nImplementation by wallets would just require a loop of their existing address generation until a match is found, trivial to implement. Wallets could provide a dropdown which shows the most commonly used signals as seen on the block chain, or a write-in.\n\nSignalling within OP_RETURN increases the tx size and cost. This address hashing method keeps the very small economic cost of voting off the chain, rather than passing it cumulatively to everyone with the insertion of additional data.\n\n\nSince I wrote this I have come across a similar idea called CryptoVoter which I think deserves more attention than it has had.\n\n________________________________\nFrom: Natanael <natanael.l at gmail.com>\nSent: Sunday, February 5, 2017 4:22 PM\nTo: Bitcoin Dev; John Hardy\nSubject: Re: [bitcoin-dev] Transaction signalling through output address hashing\n\nCensorship by miners isn't the only problem. Existing and normal transactions will probabilistically collide with these schemes, and most wallets have no straightforward way of supporting it.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170205/e1d82fb4/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fw: Transaction signalling through output address hashing",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "John Hardy"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1316
        }
    },
    {
        "title": "[bitcoin-dev] A Modified Version of Luke-jr's Block Size BIP",
        "thread_messages": [
            {
                "author": "Andrew C",
                "date": "2017-02-05T21:50:26",
                "message_text_only": "Hello all,\n\nMany people have expressed discontent with Luke-jr's proposed block size\nBIP, in particular with the decrease in size that would occur if it were\nto be activated prior to 2024.\n\nI have decided to modify the proposal to instead begin the increase\nsteps at the current 1000000 byte limit. The increases and the time spam\nof each increase will remain the same, just that the increase begins\nfrom 1000000 bytes instead of 300000 bytes.\n\nFurthermore, instead of a fixed schedule from a fixed point in time, the\nincreases will instead be calculated off of the MTP of the activation\nblock (the first block to be in the active state for this fork).\n\nWhile this proposal shares many of the same issues with the one it\nmodifies, I hope that it will be slightly less controversial and can\nallow us to move forward with scaling Bitcoin.\n\nThe full text of the proposal can be found at\nhttps://github.com/achow101/bips/blob/bip-blksize/bip-blksize.mediawiki.\nMy implementation of it is available at\nhttps://github.com/achow101/bitcoin/tree/bip-blksize\n\n\nAndrew"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-02-05T23:02:28",
                "message_text_only": "My BIP draft didn't make progress because the community opposes any block size \nincrease hardfork ever. Your version doesn't address the current block size \nissues (ie, the blocks being too large). So you've retained the only certain-\nDOA parts of my proposal, and removed the most useful part... I'm not sure the \npoint. Also, your version is now EXCLUSIVELY a hardfork, so it makes no sense \nto keep the BIP 9 deployment at all - either it gets consensus or it doesn't, \nbut miners have no part in deployment of it.\n\nOn Sunday, February 05, 2017 9:50:26 PM Andrew C via bitcoin-dev wrote:\n> Hello all,\n> \n> Many people have expressed discontent with Luke-jr's proposed block size\n> BIP, in particular with the decrease in size that would occur if it were\n> to be activated prior to 2024.\n> \n> I have decided to modify the proposal to instead begin the increase\n> steps at the current 1000000 byte limit. The increases and the time spam\n> of each increase will remain the same, just that the increase begins\n> from 1000000 bytes instead of 300000 bytes.\n> \n> Furthermore, instead of a fixed schedule from a fixed point in time, the\n> increases will instead be calculated off of the MTP of the activation\n> block (the first block to be in the active state for this fork).\n> \n> While this proposal shares many of the same issues with the one it\n> modifies, I hope that it will be slightly less controversial and can\n> allow us to move forward with scaling Bitcoin.\n> \n> The full text of the proposal can be found at\n> https://github.com/achow101/bips/blob/bip-blksize/bip-blksize.mediawiki.\n> My implementation of it is available at\n> https://github.com/achow101/bitcoin/tree/bip-blksize\n> \n> \n> Andrew\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Andrew C",
                "date": "2017-02-05T23:53:03",
                "message_text_only": "On 2/5/2017 6:02 PM, Luke Dashjr wrote:\n> My BIP draft didn't make progress because the community opposes any block size \n> increase hardfork ever.\nFrom what I have observed, it seems to be that people are more so\nopposed to a hard fork when there is a comparable soft fork available\nthan simply opposed to any block size increase hard fork ever. From the\nvarious threads discussing your proposal, it seemed that many would\nfavor it if it increased over 1 MB sooner or if it never even decreased\nin the first place.\n\n>  Your version doesn't address the current block size \n> issues (ie, the blocks being too large). \nMany users are of the opposite opinion, that the block size is too\nsmall. I understand that the decrease is to allow the blockchain size to\ngrow more slowly thereby allowing users to be more likely to run full\nnodes. Unfortunately, I think that we are way past the point of no\nreturn on that. The blockchain is already 100+ GB. Decreasing the block\nsize is not going to make that any smaller and is not going to make it\nany less painful to run a full node. Given that in order to start up a\nnew full node will still require downloading at least 100 GB of data, I\ndon't think that decreasing the block size will better facilitate full\nnode creation. Furthermore, the current trend with ISPs (at least in the\nUS) is implementing data and bandwidth caps so users are still unlikely\nto start up new full nodes regardless of any changes that we can\ncurrently do.\n\n> So you've retained the only certain-\n> DOA parts of my proposal, and removed the most useful part... I'm not sure the \n> point. Also, your version is now EXCLUSIVELY a hardfork, so it makes no sense \n> to keep the BIP 9 deployment at all - either it gets consensus or it doesn't, \n> but miners have no part in deployment of it.\nYes, I know deployment needs to be fixed. I was more proposing this for\ncomment on the modified block size schedule. I just kept the deployment\nas it was originally. However, we could use a modified version of BIP 9\nby using one of the top three bits and a longer locked-in period as a\ngrace period for all users to upgrade.\n>\n> On Sunday, February 05, 2017 9:50:26 PM Andrew C via bitcoin-dev wrote:\n>> Hello all,\n>>\n>> Many people have expressed discontent with Luke-jr's proposed block size\n>> BIP, in particular with the decrease in size that would occur if it were\n>> to be activated prior to 2024.\n>>\n>> I have decided to modify the proposal to instead begin the increase\n>> steps at the current 1000000 byte limit. The increases and the time spam\n>> of each increase will remain the same, just that the increase begins\n>> from 1000000 bytes instead of 300000 bytes.\n>>\n>> Furthermore, instead of a fixed schedule from a fixed point in time, the\n>> increases will instead be calculated off of the MTP of the activation\n>> block (the first block to be in the active state for this fork).\n>>\n>> While this proposal shares many of the same issues with the one it\n>> modifies, I hope that it will be slightly less controversial and can\n>> allow us to move forward with scaling Bitcoin.\n>>\n>> The full text of the proposal can be found at\n>> https://github.com/achow101/bips/blob/bip-blksize/bip-blksize.mediawiki.\n>> My implementation of it is available at\n>> https://github.com/achow101/bitcoin/tree/bip-blksize\n>>\n>>\n>> Andrew\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "t. khan",
                "date": "2017-02-06T18:19:43",
                "message_text_only": ">My BIP draft didn't make progress because the community opposes any block\nsize\n>increase hardfork ever.\n\nLuke, how do you know the community opposes that? Specifically, how did you\ncome to this conclusion?\n\n>Your version doesn't address the current block size\n>issues (ie, the blocks being too large).\n\nWhy do you think blocks are \"too large\"? Please cite some evidence. I've\nasked this before and you ignored it, but an answer would be helpful to the\ndiscussion.\n\n- t.k.\n\nOn Sun, Feb 5, 2017 at 6:02 PM, Luke Dashjr via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> My BIP draft didn't make progress because the community opposes any block\n> size\n> increase hardfork ever. Your version doesn't address the current block size\n> issues (ie, the blocks being too large). So you've retained the only\n> certain-\n> DOA parts of my proposal, and removed the most useful part... I'm not sure\n> the\n> point. Also, your version is now EXCLUSIVELY a hardfork, so it makes no\n> sense\n> to keep the BIP 9 deployment at all - either it gets consensus or it\n> doesn't,\n> but miners have no part in deployment of it.\n>\n> On Sunday, February 05, 2017 9:50:26 PM Andrew C via bitcoin-dev wrote:\n> > Hello all,\n> >\n> > Many people have expressed discontent with Luke-jr's proposed block size\n> > BIP, in particular with the decrease in size that would occur if it were\n> > to be activated prior to 2024.\n> >\n> > I have decided to modify the proposal to instead begin the increase\n> > steps at the current 1000000 byte limit. The increases and the time spam\n> > of each increase will remain the same, just that the increase begins\n> > from 1000000 bytes instead of 300000 bytes.\n> >\n> > Furthermore, instead of a fixed schedule from a fixed point in time, the\n> > increases will instead be calculated off of the MTP of the activation\n> > block (the first block to be in the active state for this fork).\n> >\n> > While this proposal shares many of the same issues with the one it\n> > modifies, I hope that it will be slightly less controversial and can\n> > allow us to move forward with scaling Bitcoin.\n> >\n> > The full text of the proposal can be found at\n> > https://github.com/achow101/bips/blob/bip-blksize/bip-blksize.mediawiki.\n> > My implementation of it is available at\n> > https://github.com/achow101/bitcoin/tree/bip-blksize\n> >\n> >\n> > Andrew\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170206/fcf1d14b/attachment.html>"
            },
            {
                "author": "t. khan",
                "date": "2017-02-06T20:25:13",
                "message_text_only": "On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n\n> On Monday, February 06, 2017 6:19:43 PM you wrote:\n> > >My BIP draft didn't make progress because the community opposes any\n> block\n> > >size increase hardfork ever.\n> >\n> > Luke, how do you know the community opposes that? Specifically, how did\n> you\n> > come to this conclusion?\n>\n> http://www.strawpoll.me/12228388/r\n\n\nThat poll shows 63% of votes want a larger than 1 MB block by this summer.\nHow do you go from that to \"the community opposes any block increase ever\"?\nIt shows the exact opposite of that.\n\n\n> > >Your version doesn't address the current block size\n> > >issues (ie, the blocks being too large).\n> >\n> > Why do you think blocks are \"too large\"? Please cite some evidence. I've\n> > asked this before and you ignored it, but an answer would be helpful to\n> the\n> > discussion.\n>\n> Full node count is far below the safe minimum of 85% of economic activity.\n>\n\nIs this causing a problem now? If so, what?\n\n\n> Typically reasons given for people not using full nodes themselves come\n> down\n> to the high resource requirements caused by the block size.\n\n\nThe reason people stop running nodes is because there's no incentive to\ncounteract the resource costs. Attempting to solve this by making blocks\n*smaller* is like curing a disease by killing the patient. (Incentivizing\nfull node operation would fix that problem.)\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170206/097c2546/attachment-0001.html>"
            },
            {
                "author": "alp alp",
                "date": "2017-02-08T14:44:52",
                "message_text_only": "10% say literally never.  That seems like a significant disenfranchisement\nand lack of consensus.\n\nOn Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>\n>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>> > >My BIP draft didn't make progress because the community opposes any\n>> block\n>> > >size increase hardfork ever.\n>> >\n>> > Luke, how do you know the community opposes that? Specifically, how did\n>> you\n>> > come to this conclusion?\n>>\n>> http://www.strawpoll.me/12228388/r\n>\n>\n> That poll shows 63% of votes want a larger than 1 MB block by this summer.\n> How do you go from that to \"the community opposes any block increase ever\"?\n> It shows the exact opposite of that.\n>\n>\n>> > >Your version doesn't address the current block size\n>> > >issues (ie, the blocks being too large).\n>> >\n>> > Why do you think blocks are \"too large\"? Please cite some evidence. I've\n>> > asked this before and you ignored it, but an answer would be helpful to\n>> the\n>> > discussion.\n>>\n>> Full node count is far below the safe minimum of 85% of economic activity.\n>>\n>\n> Is this causing a problem now? If so, what?\n>\n>\n>> Typically reasons given for people not using full nodes themselves come\n>> down\n>> to the high resource requirements caused by the block size.\n>\n>\n> The reason people stop running nodes is because there's no incentive to\n> counteract the resource costs. Attempting to solve this by making blocks\n> *smaller* is like curing a disease by killing the patient. (Incentivizing\n> full node operation would fix that problem.)\n>\n> - t.k.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/18d9cda5/attachment.html>"
            },
            {
                "author": "Andrew Johnson",
                "date": "2017-02-08T15:51:24",
                "message_text_only": "You're never going to reach 100% agreement, and stifling the network\nliterally forever to please a tiny minority is daft.\n\nOn Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n10% say literally never.  That seems like a significant disenfranchisement\nand lack of consensus.\n\nOn Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <bitcoin-dev at lists.\nlinuxfoundation.org> wrote:\n\n> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>\n>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>> > >My BIP draft didn't make progress because the community opposes any\n>> block\n>> > >size increase hardfork ever.\n>> >\n>> > Luke, how do you know the community opposes that? Specifically, how did\n>> you\n>> > come to this conclusion?\n>>\n>> http://www.strawpoll.me/12228388/r\n>\n>\n> That poll shows 63% of votes want a larger than 1 MB block by this summer.\n> How do you go from that to \"the community opposes any block increase ever\"?\n> It shows the exact opposite of that.\n>\n>\n>> > >Your version doesn't address the current block size\n>> > >issues (ie, the blocks being too large).\n>> >\n>> > Why do you think blocks are \"too large\"? Please cite some evidence. I've\n>> > asked this before and you ignored it, but an answer would be helpful to\n>> the\n>> > discussion.\n>>\n>> Full node count is far below the safe minimum of 85% of economic activity.\n>>\n>\n> Is this causing a problem now? If so, what?\n>\n>\n>> Typically reasons given for people not using full nodes themselves come\n>> down\n>> to the high resource requirements caused by the block size.\n>\n>\n> The reason people stop running nodes is because there's no incentive to\n> counteract the resource costs. Attempting to solve this by making blocks\n> *smaller* is like curing a disease by killing the patient. (Incentivizing\n> full node operation would fix that problem.)\n>\n> - t.k.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/fab8e0e5/attachment-0001.html>"
            },
            {
                "author": "alp alp",
                "date": "2017-02-08T15:57:21",
                "message_text_only": "10% is not a tiny minority.\n\nOn Feb 8, 2017 9:51 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com> wrote:\n\n> You're never going to reach 100% agreement, and stifling the network\n> literally forever to please a tiny minority is daft.\n>\n> On Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <bitcoin-dev at lists.\n> linuxfoundation.org> wrote:\n>\n> 10% say literally never.  That seems like a significant disenfranchisement\n> and lack of consensus.\n>\n> On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>>\n>>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>>> > >My BIP draft didn't make progress because the community opposes any\n>>> block\n>>> > >size increase hardfork ever.\n>>> >\n>>> > Luke, how do you know the community opposes that? Specifically, how\n>>> did you\n>>> > come to this conclusion?\n>>>\n>>> http://www.strawpoll.me/12228388/r\n>>\n>>\n>> That poll shows 63% of votes want a larger than 1 MB block by this\n>> summer. How do you go from that to \"the community opposes any block\n>> increase ever\"? It shows the exact opposite of that.\n>>\n>>\n>>> > >Your version doesn't address the current block size\n>>> > >issues (ie, the blocks being too large).\n>>> >\n>>> > Why do you think blocks are \"too large\"? Please cite some evidence.\n>>> I've\n>>> > asked this before and you ignored it, but an answer would be helpful\n>>> to the\n>>> > discussion.\n>>>\n>>> Full node count is far below the safe minimum of 85% of economic\n>>> activity.\n>>>\n>>\n>> Is this causing a problem now? If so, what?\n>>\n>>\n>>> Typically reasons given for people not using full nodes themselves come\n>>> down\n>>> to the high resource requirements caused by the block size.\n>>\n>>\n>> The reason people stop running nodes is because there's no incentive to\n>> counteract the resource costs. Attempting to solve this by making blocks\n>> *smaller* is like curing a disease by killing the patient. (Incentivizing\n>> full node operation would fix that problem.)\n>>\n>> - t.k.\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/b9d35e07/attachment-0001.html>"
            },
            {
                "author": "Andrew Johnson",
                "date": "2017-02-08T16:28:55",
                "message_text_only": "It is when you're talking about making a choice and 6.3x more people prefer\nsomething else. Doing nothing is a choice as well.\n\nPut another way, if 10% supported increasing the 21M coin cap and 63% were\nagainst, would you seriously consider doing it?\n\nOn Feb 8, 2017 9:57 AM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n\n> 10% is not a tiny minority.\n>\n> On Feb 8, 2017 9:51 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n> wrote:\n>\n>> You're never going to reach 100% agreement, and stifling the network\n>> literally forever to please a tiny minority is daft.\n>>\n>> On Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> 10% say literally never.  That seems like a significant\n>> disenfranchisement and lack of consensus.\n>>\n>> On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>>>\n>>>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>>>> > >My BIP draft didn't make progress because the community opposes any\n>>>> block\n>>>> > >size increase hardfork ever.\n>>>> >\n>>>> > Luke, how do you know the community opposes that? Specifically, how\n>>>> did you\n>>>> > come to this conclusion?\n>>>>\n>>>> http://www.strawpoll.me/12228388/r\n>>>\n>>>\n>>> That poll shows 63% of votes want a larger than 1 MB block by this\n>>> summer. How do you go from that to \"the community opposes any block\n>>> increase ever\"? It shows the exact opposite of that.\n>>>\n>>>\n>>>> > >Your version doesn't address the current block size\n>>>> > >issues (ie, the blocks being too large).\n>>>> >\n>>>> > Why do you think blocks are \"too large\"? Please cite some evidence.\n>>>> I've\n>>>> > asked this before and you ignored it, but an answer would be helpful\n>>>> to the\n>>>> > discussion.\n>>>>\n>>>> Full node count is far below the safe minimum of 85% of economic\n>>>> activity.\n>>>>\n>>>\n>>> Is this causing a problem now? If so, what?\n>>>\n>>>\n>>>> Typically reasons given for people not using full nodes themselves come\n>>>> down\n>>>> to the high resource requirements caused by the block size.\n>>>\n>>>\n>>> The reason people stop running nodes is because there's no incentive to\n>>> counteract the resource costs. Attempting to solve this by making blocks\n>>> *smaller* is like curing a disease by killing the patient. (Incentivizing\n>>> full node operation would fix that problem.)\n>>>\n>>> - t.k.\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/4a68bd99/attachment-0001.html>"
            },
            {
                "author": "alp alp",
                "date": "2017-02-08T18:16:07",
                "message_text_only": "Doing nothing is the rules we all agreed to.  If those rules are to be\nchanged,nearly everyone will need to consent.  The same rule applies to the\ncap, we all agreed to 21m, and if someone wants to change that, nearly\neveryone would need to agree.\n\nOn Feb 8, 2017 10:28 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\nwrote:\n\nIt is when you're talking about making a choice and 6.3x more people prefer\nsomething else. Doing nothing is a choice as well.\n\nPut another way, if 10% supported increasing the 21M coin cap and 63% were\nagainst, would you seriously consider doing it?\n\nOn Feb 8, 2017 9:57 AM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n\n> 10% is not a tiny minority.\n>\n> On Feb 8, 2017 9:51 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n> wrote:\n>\n>> You're never going to reach 100% agreement, and stifling the network\n>> literally forever to please a tiny minority is daft.\n>>\n>> On Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> 10% say literally never.  That seems like a significant\n>> disenfranchisement and lack of consensus.\n>>\n>> On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>>>\n>>>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>>>> > >My BIP draft didn't make progress because the community opposes any\n>>>> block\n>>>> > >size increase hardfork ever.\n>>>> >\n>>>> > Luke, how do you know the community opposes that? Specifically, how\n>>>> did you\n>>>> > come to this conclusion?\n>>>>\n>>>> http://www.strawpoll.me/12228388/r\n>>>\n>>>\n>>> That poll shows 63% of votes want a larger than 1 MB block by this\n>>> summer. How do you go from that to \"the community opposes any block\n>>> increase ever\"? It shows the exact opposite of that.\n>>>\n>>>\n>>>> > >Your version doesn't address the current block size\n>>>> > >issues (ie, the blocks being too large).\n>>>> >\n>>>> > Why do you think blocks are \"too large\"? Please cite some evidence.\n>>>> I've\n>>>> > asked this before and you ignored it, but an answer would be helpful\n>>>> to the\n>>>> > discussion.\n>>>>\n>>>> Full node count is far below the safe minimum of 85% of economic\n>>>> activity.\n>>>>\n>>>\n>>> Is this causing a problem now? If so, what?\n>>>\n>>>\n>>>> Typically reasons given for people not using full nodes themselves come\n>>>> down\n>>>> to the high resource requirements caused by the block size.\n>>>\n>>>\n>>> The reason people stop running nodes is because there's no incentive to\n>>> counteract the resource costs. Attempting to solve this by making blocks\n>>> *smaller* is like curing a disease by killing the patient. (Incentivizing\n>>> full node operation would fix that problem.)\n>>>\n>>> - t.k.\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/fe125147/attachment-0001.html>"
            },
            {
                "author": "t. khan",
                "date": "2017-02-08T19:53:15",
                "message_text_only": "Even ignoring the obvious flaws of that poll, Andrew is still correct: you\ncannot reach 100% consensus. It's statistically impossible in any large\ngroup.\n\nOnly the majority needs to consent, though what is considered a majority\nvaries depending on the context (95%, 75%, 51%). Nowhere does it say\n\"everyone needs to agree\".\n\nOn Wed, Feb 8, 2017 at 1:16 PM, alp alp <alp.bitcoin at gmail.com> wrote:\n\n> Doing nothing is the rules we all agreed to.  If those rules are to be\n> changed,nearly everyone will need to consent.  The same rule applies to the\n> cap, we all agreed to 21m, and if someone wants to change that, nearly\n> everyone would need to agree.\n>\n>\n> On Feb 8, 2017 10:28 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n> wrote:\n>\n> It is when you're talking about making a choice and 6.3x more people\n> prefer something else. Doing nothing is a choice as well.\n>\n> Put another way, if 10% supported increasing the 21M coin cap and 63% were\n> against, would you seriously consider doing it?\n>\n> On Feb 8, 2017 9:57 AM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n>\n>> 10% is not a tiny minority.\n>>\n>> On Feb 8, 2017 9:51 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n>> wrote:\n>>\n>>> You're never going to reach 100% agreement, and stifling the network\n>>> literally forever to please a tiny minority is daft.\n>>>\n>>> On Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> 10% say literally never.  That seems like a significant\n>>> disenfranchisement and lack of consensus.\n>>>\n>>> On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>>>>\n>>>>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>>>>> > >My BIP draft didn't make progress because the community opposes any\n>>>>> block\n>>>>> > >size increase hardfork ever.\n>>>>> >\n>>>>> > Luke, how do you know the community opposes that? Specifically, how\n>>>>> did you\n>>>>> > come to this conclusion?\n>>>>>\n>>>>> http://www.strawpoll.me/12228388/r\n>>>>\n>>>>\n>>>> That poll shows 63% of votes want a larger than 1 MB block by this\n>>>> summer. How do you go from that to \"the community opposes any block\n>>>> increase ever\"? It shows the exact opposite of that.\n>>>>\n>>>>\n>>>>> > >Your version doesn't address the current block size\n>>>>> > >issues (ie, the blocks being too large).\n>>>>> >\n>>>>> > Why do you think blocks are \"too large\"? Please cite some evidence.\n>>>>> I've\n>>>>> > asked this before and you ignored it, but an answer would be helpful\n>>>>> to the\n>>>>> > discussion.\n>>>>>\n>>>>> Full node count is far below the safe minimum of 85% of economic\n>>>>> activity.\n>>>>>\n>>>>\n>>>> Is this causing a problem now? If so, what?\n>>>>\n>>>>\n>>>>> Typically reasons given for people not using full nodes themselves\n>>>>> come down\n>>>>> to the high resource requirements caused by the block size.\n>>>>\n>>>>\n>>>> The reason people stop running nodes is because there's no incentive to\n>>>> counteract the resource costs. Attempting to solve this by making blocks\n>>>> *smaller* is like curing a disease by killing the patient. (Incentivizing\n>>>> full node operation would fix that problem.)\n>>>>\n>>>> - t.k.\n>>>>\n>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/444cd354/attachment-0001.html>"
            },
            {
                "author": "Andrew Johnson",
                "date": "2017-02-08T19:56:19",
                "message_text_only": "If a small dissenting minority can block all forward progress then bitcoin\nis no longer interesting.  What an incredibly simple attack vector...\n\nNo need to break any cryptography, find a bug to exploit, build tens of\nmillions of dollars in mining hardware, spend lots of bitcoin on fees to\nflood the network, or be clever or expend any valuable resources in any\nway, shape, or form.\n\nJust convince(or pay, if you do want to expend some resources) a few\npeople(or make up a few online personas) to staunchly refuse to accept\nanything at all and the entire system is stuck in 2013(when we first\nstarted widely discussing a blocksize increase seriously).\n\nIs that really the bitcoin that you want to be a part of?\n\nWhen the 1MB cap was implemented it was stated specifically that we could\nincrease it when we needed it.  The white paper even talks about scaling to\nhuge capacity.  Not sure where you got the idea that we all agreed to stay\nat 1MB forever, I certainly didn't.  It was never stated or implied that we\ncould change the coin cap later(please cite if I'm mistaken).\n\n\nOn Feb 8, 2017 12:16 PM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n\nDoing nothing is the rules we all agreed to.  If those rules are to be\nchanged,nearly everyone will need to consent.  The same rule applies to the\ncap, we all agreed to 21m, and if someone wants to change that, nearly\neveryone would need to agree.\n\n\nOn Feb 8, 2017 10:28 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\nwrote:\n\nIt is when you're talking about making a choice and 6.3x more people prefer\nsomething else. Doing nothing is a choice as well.\n\nPut another way, if 10% supported increasing the 21M coin cap and 63% were\nagainst, would you seriously consider doing it?\n\nOn Feb 8, 2017 9:57 AM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n\n> 10% is not a tiny minority.\n>\n> On Feb 8, 2017 9:51 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n> wrote:\n>\n>> You're never going to reach 100% agreement, and stifling the network\n>> literally forever to please a tiny minority is daft.\n>>\n>> On Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> 10% say literally never.  That seems like a significant\n>> disenfranchisement and lack of consensus.\n>>\n>> On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>>>\n>>>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>>>> > >My BIP draft didn't make progress because the community opposes any\n>>>> block\n>>>> > >size increase hardfork ever.\n>>>> >\n>>>> > Luke, how do you know the community opposes that? Specifically, how\n>>>> did you\n>>>> > come to this conclusion?\n>>>>\n>>>> http://www.strawpoll.me/12228388/r\n>>>\n>>>\n>>> That poll shows 63% of votes want a larger than 1 MB block by this\n>>> summer. How do you go from that to \"the community opposes any block\n>>> increase ever\"? It shows the exact opposite of that.\n>>>\n>>>\n>>>> > >Your version doesn't address the current block size\n>>>> > >issues (ie, the blocks being too large).\n>>>> >\n>>>> > Why do you think blocks are \"too large\"? Please cite some evidence.\n>>>> I've\n>>>> > asked this before and you ignored it, but an answer would be helpful\n>>>> to the\n>>>> > discussion.\n>>>>\n>>>> Full node count is far below the safe minimum of 85% of economic\n>>>> activity.\n>>>>\n>>>\n>>> Is this causing a problem now? If so, what?\n>>>\n>>>\n>>>> Typically reasons given for people not using full nodes themselves come\n>>>> down\n>>>> to the high resource requirements caused by the block size.\n>>>\n>>>\n>>> The reason people stop running nodes is because there's no incentive to\n>>> counteract the resource costs. Attempting to solve this by making blocks\n>>> *smaller* is like curing a disease by killing the patient. (Incentivizing\n>>> full node operation would fix that problem.)\n>>>\n>>> - t.k.\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/eb653e44/attachment-0001.html>"
            },
            {
                "author": "alp alp",
                "date": "2017-02-08T20:13:18",
                "message_text_only": ">Only the majority needs to consent, though what is considered a majority\nvaries depending on the context (95%, 75%, 51%). Nowhere does it say\n\"everyone needs to agree\".\n\nThere's a pretty huge gap between 90% and nearly 100%.  90% excluding 10%\nonly 7 times results in only 48% of the original base.\n\n>If a small dissenting minority can block all forward progress then bitcoin\nis no longer interesting.\n\nYour definition of forward may be different than other users.\n\n>Is that really the bitcoin that you want to be a part of?\n\nYes, I chose Bitcoin because it relies on a strictly held consensus\nmechanism and not one that changes on the whims of the majority.  We have\ntens of dozens of political currencies for that.\n\n>When the 1MB cap was implemented it was stated specifically that we could\nincrease it when we needed it.  The white paper even talks about scaling to\nhuge capacity.  Not sure where you got the idea that we all agreed to stay\nat 1MB forever, I certainly didn't.  It was never stated or implied that we\ncould change the coin cap later(please cite if I'm mistaken).\n\nThe community has not agreed that it is needed at this time.  Perhaps they\nwill change their mind at some point in the future.  We have also learned a\ngreat deal since the publication of the initial whitepaper, such as the\nunstable state without a backlog or subsidy.  Fortunately, participation in\nthis system is voluntary, and you are free to leave at any time.\n\nThis seems to be venturing quite off topic, and perhaps would be better\nsuited for the bitcoin-discuss list.\n\nOn Wed, Feb 8, 2017 at 1:56 PM, Andrew Johnson <andrew.johnson83 at gmail.com>\nwrote:\n\n> If a small dissenting minority can block all forward progress then bitcoin\n> is no longer interesting.  What an incredibly simple attack vector...\n>\n> No need to break any cryptography, find a bug to exploit, build tens of\n> millions of dollars in mining hardware, spend lots of bitcoin on fees to\n> flood the network, or be clever or expend any valuable resources in any\n> way, shape, or form.\n>\n> Just convince(or pay, if you do want to expend some resources) a few\n> people(or make up a few online personas) to staunchly refuse to accept\n> anything at all and the entire system is stuck in 2013(when we first\n> started widely discussing a blocksize increase seriously).\n>\n> Is that really the bitcoin that you want to be a part of?\n>\n> When the 1MB cap was implemented it was stated specifically that we could\n> increase it when we needed it.  The white paper even talks about scaling to\n> huge capacity.  Not sure where you got the idea that we all agreed to stay\n> at 1MB forever, I certainly didn't.  It was never stated or implied that we\n> could change the coin cap later(please cite if I'm mistaken).\n>\n>\n> On Feb 8, 2017 12:16 PM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n>\n> Doing nothing is the rules we all agreed to.  If those rules are to be\n> changed,nearly everyone will need to consent.  The same rule applies to the\n> cap, we all agreed to 21m, and if someone wants to change that, nearly\n> everyone would need to agree.\n>\n>\n> On Feb 8, 2017 10:28 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n> wrote:\n>\n> It is when you're talking about making a choice and 6.3x more people\n> prefer something else. Doing nothing is a choice as well.\n>\n> Put another way, if 10% supported increasing the 21M coin cap and 63% were\n> against, would you seriously consider doing it?\n>\n> On Feb 8, 2017 9:57 AM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n>\n>> 10% is not a tiny minority.\n>>\n>> On Feb 8, 2017 9:51 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n>> wrote:\n>>\n>>> You're never going to reach 100% agreement, and stifling the network\n>>> literally forever to please a tiny minority is daft.\n>>>\n>>> On Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> 10% say literally never.  That seems like a significant\n>>> disenfranchisement and lack of consensus.\n>>>\n>>> On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>>>>\n>>>>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>>>>> > >My BIP draft didn't make progress because the community opposes any\n>>>>> block\n>>>>> > >size increase hardfork ever.\n>>>>> >\n>>>>> > Luke, how do you know the community opposes that? Specifically, how\n>>>>> did you\n>>>>> > come to this conclusion?\n>>>>>\n>>>>> http://www.strawpoll.me/12228388/r\n>>>>\n>>>>\n>>>> That poll shows 63% of votes want a larger than 1 MB block by this\n>>>> summer. How do you go from that to \"the community opposes any block\n>>>> increase ever\"? It shows the exact opposite of that.\n>>>>\n>>>>\n>>>>> > >Your version doesn't address the current block size\n>>>>> > >issues (ie, the blocks being too large).\n>>>>> >\n>>>>> > Why do you think blocks are \"too large\"? Please cite some evidence.\n>>>>> I've\n>>>>> > asked this before and you ignored it, but an answer would be helpful\n>>>>> to the\n>>>>> > discussion.\n>>>>>\n>>>>> Full node count is far below the safe minimum of 85% of economic\n>>>>> activity.\n>>>>>\n>>>>\n>>>> Is this causing a problem now? If so, what?\n>>>>\n>>>>\n>>>>> Typically reasons given for people not using full nodes themselves\n>>>>> come down\n>>>>> to the high resource requirements caused by the block size.\n>>>>\n>>>>\n>>>> The reason people stop running nodes is because there's no incentive to\n>>>> counteract the resource costs. Attempting to solve this by making blocks\n>>>> *smaller* is like curing a disease by killing the patient. (Incentivizing\n>>>> full node operation would fix that problem.)\n>>>>\n>>>> - t.k.\n>>>>\n>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/672a9a83/attachment-0001.html>"
            },
            {
                "author": "Btc Drak",
                "date": "2017-02-10T10:33:52",
                "message_text_only": "Agreed, this thread is venturing somewhat out of scope for the list. Please\ncan we redirect philosophical discussion to another forum/list such as\nbitcoin-discuss, which can be found at\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-discuss\n\nRepost of the bitcoin-dev posting guidelines are:\n\n- Posts must concern development of bitcoin protocol.\n- Posts should be technical or academic in nature.\n- Generally encouraged: patches, notification of pull requests, BIP\nproposals, academic paper announcements. And discussions that follow.\n- Generally discouraged: shower thoughts, wild speculation, jokes, +1s,\nnon-technical bitcoin issues, rehashing settled topics without new data,\nmoderation concerns.\n- Detailed patch discussion generally better on a GitHub PR.\n- Meta-discussion is better on bitcoin-discuss.\n\nOn Wed, Feb 8, 2017 at 8:13 PM, alp alp via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> >Only the majority needs to consent, though what is considered a majority\n> varies depending on the context (95%, 75%, 51%). Nowhere does it say\n> \"everyone needs to agree\".\n>\n> There's a pretty huge gap between 90% and nearly 100%.  90% excluding 10%\n> only 7 times results in only 48% of the original base.\n>\n> >If a small dissenting minority can block all forward progress then\n> bitcoin is no longer interesting.\n>\n> Your definition of forward may be different than other users.\n>\n> >Is that really the bitcoin that you want to be a part of?\n>\n> Yes, I chose Bitcoin because it relies on a strictly held consensus\n> mechanism and not one that changes on the whims of the majority.  We have\n> tens of dozens of political currencies for that.\n>\n> >When the 1MB cap was implemented it was stated specifically that we\n> could increase it when we needed it.  The white paper even talks about\n> scaling to huge capacity.  Not sure where you got the idea that we all\n> agreed to stay at 1MB forever, I certainly didn't.  It was never stated or\n> implied that we could change the coin cap later(please cite if I'm\n> mistaken).\n>\n> The community has not agreed that it is needed at this time.  Perhaps they\n> will change their mind at some point in the future.  We have also learned a\n> great deal since the publication of the initial whitepaper, such as the\n> unstable state without a backlog or subsidy.  Fortunately, participation in\n> this system is voluntary, and you are free to leave at any time.\n>\n> This seems to be venturing quite off topic, and perhaps would be better\n> suited for the bitcoin-discuss list.\n>\n> On Wed, Feb 8, 2017 at 1:56 PM, Andrew Johnson <andrew.johnson83 at gmail.com\n> > wrote:\n>\n>> If a small dissenting minority can block all forward progress then\n>> bitcoin is no longer interesting.  What an incredibly simple attack\n>> vector...\n>>\n>> No need to break any cryptography, find a bug to exploit, build tens of\n>> millions of dollars in mining hardware, spend lots of bitcoin on fees to\n>> flood the network, or be clever or expend any valuable resources in any\n>> way, shape, or form.\n>>\n>> Just convince(or pay, if you do want to expend some resources) a few\n>> people(or make up a few online personas) to staunchly refuse to accept\n>> anything at all and the entire system is stuck in 2013(when we first\n>> started widely discussing a blocksize increase seriously).\n>>\n>> Is that really the bitcoin that you want to be a part of?\n>>\n>> When the 1MB cap was implemented it was stated specifically that we could\n>> increase it when we needed it.  The white paper even talks about scaling to\n>> huge capacity.  Not sure where you got the idea that we all agreed to stay\n>> at 1MB forever, I certainly didn't.  It was never stated or implied that we\n>> could change the coin cap later(please cite if I'm mistaken).\n>>\n>>\n>> On Feb 8, 2017 12:16 PM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n>>\n>> Doing nothing is the rules we all agreed to.  If those rules are to be\n>> changed,nearly everyone will need to consent.  The same rule applies to the\n>> cap, we all agreed to 21m, and if someone wants to change that, nearly\n>> everyone would need to agree.\n>>\n>>\n>> On Feb 8, 2017 10:28 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n>> wrote:\n>>\n>> It is when you're talking about making a choice and 6.3x more people\n>> prefer something else. Doing nothing is a choice as well.\n>>\n>> Put another way, if 10% supported increasing the 21M coin cap and 63%\n>> were against, would you seriously consider doing it?\n>>\n>> On Feb 8, 2017 9:57 AM, \"alp alp\" <alp.bitcoin at gmail.com> wrote:\n>>\n>>> 10% is not a tiny minority.\n>>>\n>>> On Feb 8, 2017 9:51 AM, \"Andrew Johnson\" <andrew.johnson83 at gmail.com>\n>>> wrote:\n>>>\n>>>> You're never going to reach 100% agreement, and stifling the network\n>>>> literally forever to please a tiny minority is daft.\n>>>>\n>>>> On Feb 8, 2017 8:52 AM, \"alp alp via bitcoin-dev\" <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>> 10% say literally never.  That seems like a significant\n>>>> disenfranchisement and lack of consensus.\n>>>>\n>>>> On Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>>> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>>>>>\n>>>>>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>>>>>> > >My BIP draft didn't make progress because the community opposes\n>>>>>> any block\n>>>>>> > >size increase hardfork ever.\n>>>>>> >\n>>>>>> > Luke, how do you know the community opposes that? Specifically, how\n>>>>>> did you\n>>>>>> > come to this conclusion?\n>>>>>>\n>>>>>> http://www.strawpoll.me/12228388/r\n>>>>>\n>>>>>\n>>>>> That poll shows 63% of votes want a larger than 1 MB block by this\n>>>>> summer. How do you go from that to \"the community opposes any block\n>>>>> increase ever\"? It shows the exact opposite of that.\n>>>>>\n>>>>>\n>>>>>> > >Your version doesn't address the current block size\n>>>>>> > >issues (ie, the blocks being too large).\n>>>>>> >\n>>>>>> > Why do you think blocks are \"too large\"? Please cite some evidence.\n>>>>>> I've\n>>>>>> > asked this before and you ignored it, but an answer would be\n>>>>>> helpful to the\n>>>>>> > discussion.\n>>>>>>\n>>>>>> Full node count is far below the safe minimum of 85% of economic\n>>>>>> activity.\n>>>>>>\n>>>>>\n>>>>> Is this causing a problem now? If so, what?\n>>>>>\n>>>>>\n>>>>>> Typically reasons given for people not using full nodes themselves\n>>>>>> come down\n>>>>>> to the high resource requirements caused by the block size.\n>>>>>\n>>>>>\n>>>>> The reason people stop running nodes is because there's no incentive\n>>>>> to counteract the resource costs. Attempting to solve this by making blocks\n>>>>> *smaller* is like curing a disease by killing the patient. (Incentivizing\n>>>>> full node operation would fix that problem.)\n>>>>>\n>>>>> - t.k.\n>>>>>\n>>>>>\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>>>\n>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>>\n>>>>\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170210/f18dc28a/attachment.html>"
            },
            {
                "author": "Andrew C",
                "date": "2017-02-06T21:00:18",
                "message_text_only": "I looked at the discussions about the block size and about Luke-Jr's\nproposal on Reddit and Bitcointalk. From what I observed of all of the\ndiscussions is that few users are in favor of the status quo, and even\nfewer are in favor of decreasing the block size. The majority of users\nfavored Segwit because it was a block size increase (that was a commonly\nused reason in support of it and in arguments about increasing the block\nsize).\n\nDiscussions about Luke-Jr's proposal indicated that many users disagreed\nwith the decrease in block size and the time that it took to increase\nagain to 1 MB. There was not only disagreement but explicit ridicule and\nmocking of that aspect of the proposal.\n\n\nOn 2/6/2017 3:28 PM, Thomas Kerin wrote:\n> \"Many users are of the opposite opinion, that the block size is too\n> small.\" - That is newspeak, the users can speak for themselves.\n>\n> From whom did you gather feedback from before you changed Luke-Jrs BIP?\n>\n> If people don't agree with the proposal, changing it an infinite\n> number of times light well lead to the same result.\n>\n> Have the users spoken, in their response to what Luke-Jr proposed?\n>\n> On 6 February 2017 00:53:03 CET, Andrew C via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>     On 2/5/2017 6:02 PM, Luke Dashjr wrote:\n>\n>         My BIP draft didn't make progress because the community\n>         opposes any block size increase hardfork ever. \n>\n>     From what I have observed, it seems to be that people are more so\n>     opposed to a hard fork when there is a comparable soft fork available\n>     than simply opposed to any block size increase hard fork ever. From the\n>     various threads discussing your proposal, it seemed that many would\n>     favor it if it increased over 1 MB sooner or if it never even decreased\n>     in the first place.\n>\n>         Your version doesn't address the current block size issues\n>         (ie, the blocks being too large). \n>\n>     Many users are of the opposite opinion, that the block size is too\n>     small. I understand that the decrease is to allow the blockchain size to\n>     grow more slowly thereby allowing users to be more likely to run full\n>     nodes. Unfortunately, I think that we are way past the point of no\n>     return on that. The blockchain is already 100+ GB. Decreasing the block\n>     size is not going to make that any smaller and is not going to make it\n>     any less painful to run a full node. Given that in order to start up a\n>     new full node will still require downloading at least 100 GB of data, I\n>     don't think that decreasing the block size will better facilitate full\n>     node creation. Furthermore, the current trend with ISPs (at least in the\n>     US) is implementing data and bandwidth caps so users are still unlikely\n>     to start up new full nodes regardless of any changes that we can\n>     currently do.\n>\n>         So you've retained the only certain- DOA parts of my proposal,\n>         and removed the most useful part... I'm not sure the point.\n>         Also, your version is now EXCLUSIVELY a hardfork, so it makes\n>         no sense to keep the BIP 9 deployment at all - either it gets\n>         consensus or it doesn't, but miners have no part in deployment\n>         of it. \n>\n>     Yes, I know deployment needs to be fixed. I was more proposing this for\n>     comment on the modified block size schedule. I just kept the deployment\n>     as it was originally. However, we could use a modified version of BIP 9\n>     by using one of the top three bits and a longer locked-in period as a\n>     grace period for all users to upgrade.\n>\n>         On Sunday, February 05, 2017 9:50:26 PM Andrew C via\n>         bitcoin-dev wrote:\n>\n>             Hello all, Many people have expressed discontent with\n>             Luke-jr's proposed block size BIP, in particular with the\n>             decrease in size that would occur if it were to be\n>             activated prior to 2024. I have decided to modify the\n>             proposal to instead begin the increase steps at the\n>             current 1000000 byte limit. The increases and the time\n>             spam of each increase will remain the same, just that the\n>             increase begins from 1000000 bytes instead of 300000\n>             bytes. Furthermore, instead of a fixed schedule from a\n>             fixed point in time, the increases will instead be\n>             calculated off of the MTP of the activation block (the\n>             first block to be in the active state for this fork).\n>             While this proposal shares many of the same issues with\n>             the one it modifies, I hope that it will be slightly less\n>             controversial and can allow us to move forward with\n>             scaling Bitcoin. The full text of the proposal can be\n>             found at\n>             https://github.com/achow101/bips/blob/bip-blksize/bip-blksize.mediawiki.\n>             My implementation of it is available at\n>             https://github.com/achow101/bitcoin/tree/bip-blksize Andrew\n>             ------------------------------------------------------------------------\n>             bitcoin-dev mailing list\n>             bitcoin-dev at lists.linuxfoundation.org\n>             https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n>\n>     ------------------------------------------------------------------------\n>\n>     bitcoin-dev mailing list\n>     bitcoin-dev at lists.linuxfoundation.org\n>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> -- Sent from my Android device with K-9 Mail. Please excuse my brevity. \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170206/a67c0b89/attachment-0001.html>"
            },
            {
                "author": "Ryan J Martin",
                "date": "2017-02-10T04:10:15",
                "message_text_only": "\"10% say literally never.  That seems like a significant disenfranchisement\nand lack of consensus.\"\n\nCertainly the poll results should be taken with a grain of salt and not a definitive answer or measure . \nHowever if we agree the poll has some worth (or even if not, then lets use it as hyptothetical): If we split it into two groups: those okay with a hardfork at some point > now, and those never okay with hardfork, that means there is 90% that agree a hardfork is acceptable in the future. That said, what threshold defines consensus then? 98%? 100%?       \n \nPersonally I think pursuing paths that maximize net social benefit in terms of cost surplus/burden is the best way to go since consensus is such an impossible to define, variable, case-by-case thing that doesn't always lead to the best choice.\n\n-Ryan J. MArtin\n \n\n________________________________________\nFrom: bitcoin-dev-bounces at lists.linuxfoundation.org [bitcoin-dev-bounces at lists.linuxfoundation.org] on behalf of bitcoin-dev-request at lists.linuxfoundation.org [bitcoin-dev-request at lists.linuxfoundation.org]\nSent: Thursday, February 09, 2017 7:00 AM\nTo: bitcoin-dev at lists.linuxfoundation.org\nSubject: bitcoin-dev Digest, Vol 21, Issue 10\n\nSend bitcoin-dev mailing list submissions to\n        bitcoin-dev at lists.linuxfoundation.org\n\nTo subscribe or unsubscribe via the World Wide Web, visit\n        https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\nor, via email, send a message with subject or body 'help' to\n        bitcoin-dev-request at lists.linuxfoundation.org\n\nYou can reach the person managing the list at\n        bitcoin-dev-owner at lists.linuxfoundation.org\n\nWhen replying, please edit your Subject line so it is more specific\nthan \"Re: Contents of bitcoin-dev digest...\"\n\n\nToday's Topics:\n\n   1. Re: A Modified Version of Luke-jr's Block Size BIP (alp alp)\n\n\n----------------------------------------------------------------------\n\nMessage: 1\nDate: Wed, 8 Feb 2017 08:44:52 -0600\nFrom: alp alp <alp.bitcoin at gmail.com>\nTo: \"t. khan\" <teekhan42 at gmail.com>,    Bitcoin Protocol Discussion\n        <bitcoin-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] A Modified Version of Luke-jr's Block Size\n        BIP\nMessage-ID:\n        <CAMBsKS9OS2tA4bG-JG96XNZTiPyuq322Qu=fyJcZ1BtVj3TtxQ at mail.gmail.com>\nContent-Type: text/plain; charset=\"utf-8\"\n\n10% say literally never.  That seems like a significant disenfranchisement\nand lack of consensus.\n\nOn Mon, Feb 6, 2017 at 2:25 PM, t. khan via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, Feb 6, 2017 at 2:53 PM, Luke Dashjr <luke at dashjr.org> wrote:\n>\n>> On Monday, February 06, 2017 6:19:43 PM you wrote:\n>> > >My BIP draft didn't make progress because the community opposes any\n>> block\n>> > >size increase hardfork ever.\n>> >\n>> > Luke, how do you know the community opposes that? Specifically, how did\n>> you\n>> > come to this conclusion?\n>>\n>> http://www.strawpoll.me/12228388/r\n>\n>\n> That poll shows 63% of votes want a larger than 1 MB block by this summer.\n> How do you go from that to \"the community opposes any block increase ever\"?\n> It shows the exact opposite of that.\n>\n>\n>> > >Your version doesn't address the current block size\n>> > >issues (ie, the blocks being too large).\n>> >\n>> > Why do you think blocks are \"too large\"? Please cite some evidence. I've\n>> > asked this before and you ignored it, but an answer would be helpful to\n>> the\n>> > discussion.\n>>\n>> Full node count is far below the safe minimum of 85% of economic activity.\n>>\n>\n> Is this causing a problem now? If so, what?\n>\n>\n>> Typically reasons given for people not using full nodes themselves come\n>> down\n>> to the high resource requirements caused by the block size.\n>\n>\n> The reason people stop running nodes is because there's no incentive to\n> counteract the resource costs. Attempting to solve this by making blocks\n> *smaller* is like curing a disease by killing the patient. (Incentivizing\n> full node operation would fix that problem.)\n>\n> - t.k.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170208/18d9cda5/attachment-0001.html>\n\n------------------------------\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\nEnd of bitcoin-dev Digest, Vol 21, Issue 10\n*******************************************"
            }
        ],
        "thread_summary": {
            "title": "A Modified Version of Luke-jr's Block Size BIP",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Andrew Johnson",
                "t. khan",
                "Andrew C",
                "Luke Dashjr",
                "Btc Drak",
                "Ryan J Martin",
                "alp alp"
            ],
            "messages_count": 16,
            "total_messages_chars_count": 57616
        }
    },
    {
        "title": "[bitcoin-dev] Spoonnet: another experimental hardfork",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2017-02-06T12:39:21",
                "message_text_only": "Finally got some time over the Chinese New Year holiday to code and write this up. This is not the same as my previous forcenet ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html ). It is much simpler. Trying to activate it on testnet will get you banned. Trying to activate it on mainnet before consensus is reached will make you lose money.\n\nThis proposal includes the following features:\n\n1. A fixed starting time. Not dependent on miner signalling. However, it requires at least 51% of miners to actually build the new block format in order to get activated.\n\n2. It has no mechanism to prevent a split. If 49% of miners insist on the original chain, they could keep going. Split prevention is a social problem, not a technical one.\n\n3. It is compatible with existing Stratum mining protocol. Only pool software upgrade is needed\n\n4. A new extended and flexible header is located at the witness field of the coinbase transaction\n\n5. It is backward compatible with existing light wallets\n\n6. Dedicated space for miners to put anything they want, which bitcoin users could completely ignore. Merge-mining friendly.\n\n7. Small header space for miners to include non-consensus enforced bitcoin related data, useful for fee estimation etc.\n\n8. A new transaction weight formula to encourage responsible use of UTXO\n\n9. A linear growth of actual block size until certain limit\n\n10. Sighash O(n^2) protection for legacy (non-segwit) outputs\n\n11. Optional anti-transaction replay\n\n12. A new optional coinbase tx format that allows additional inputs, including spending of immature previous coinbase outputs\n\n\n\nSpecification [Rationales]:\n\n\nActivation:\n\n* A \"hardfork signalling block\" is a block with the sign bit of header nVersion is set [Clearly invalid for old nodes; easy opt-out for light wallets]\n\n* If the median-time-past of the past 11 blocks is smaller than the HardForkTime (exact time to be determined), a hardfork signalling block is invalid.\n\n* Child of a hardfork signalling block MUST also be a hardfork signalling block\n\n* Initial hardfork signalling is optional, even if the HardForkTime has past [requires at least 51% of miners to actually build the new block format]\n\n* HardForkTime is determined by a broad consensus of the Bitcoin community. This is the only way to prevent a split.\n\n\nExtended header:\n\n* Main header refers to the original 80 bytes bitcoin block header\n\n* A hardfork signalling block MUST have a additional extended header\n\n* The extended header is placed at the witness field of the coinbase transaction [There are 2 major advantages: 1. coinbase witness is otherwise useless; 2. Significantly simply the implementation with its stack structure]\n\n* There must be exactly 3 witness items (Header1; Header2 ; Header3)\n**Header1 must be exactly 32 bytes of the original transaction hash Merkle root.\n**Header2 is the secondary header. It must be 36-80 bytes. The first 4 bytes must be little-endian encoded number of transactions (minimum 1). The next 32 bytes must be the witness Merkle root (to be defined later). The rest, if any, has no consensus meaning. However, miners MUST NOT use this space of non-bitcoin purpose [the additional space allows non-censensus enforced data to be included, easily accessible to light wallets]\n**Header3 is the miner dedicated space. It must not be larger than 252 bytes. Anything put here has no consensus meaning [space for merge mining; non-full nodes could completely ignore data in this space; 252 is the maximum size allowed for signal byte CompactSize]\n\n* The main header commitment is H(Header1|H(H(Header2)|H(Header3)))  H() = dSHA256() [The hardfork is transparent to light wallets, except one more 32-byte hash is needed to connect a transaction to the root]\n\n* To place the ext header, segwit becomes mandatory after hardfork\n\n\nA \u201cbackdoor\u201d softfork the relax the size limit of Header 2 and Header 3:\n\n* A special BIP9 softfork is defined with bit-15. If this softfork is activated, full nodes will not enforce the size limit for Header 2 and Header 3. [To allow header expansion without a hardfork. Avoid miner abuse while providing flexibility. Expansion might be needed for new commitments like fraud proof commitments]\n\n\nAnti-tx-replay:\n\n* Hardfork network version bit is 0x02000000. A tx is invalid if the highest nVersion byte is not zero, and the network version bit is not set.\n\n* Masked tx version is nVersion with the highest byte masked. If masked version is 3 or above, sighash for OP_CHECKSIG alike is calculated using BIP143, except 0x02000000 is added to the nHashType (the nHashType in signature is still a 1-byte value) [ensure a clean split of signatures; optionally fix the O(n^2) problem]\n\n* Pre-hardfork policy change: nVersion is determined by the masked tx version for policy purpose. Setting of Pre-hardfork network version bit 0x01000000 is allowed.\n\n* Details: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013473.html\n\n\nSighash limitation:\n\n* Sighash impact is estimated by \u201cLoose estimation\u201d in https://github.com/jl2012/bips/blob/065ea7429035d43ff90965f42b086fb7e1517291/bip-sighash.mediawiki\n\n* Only txs with masked version below 3 are counted. [because they are fixed by the BIP-143 like signature]\n\n* Each SigHashSize is defined as 1 tx weight (defined later).\n\n* SIGHASH_SCALE_FACTOR is 90 (see the BIP above)\n\n\nNew tx weight definition:\n\n* Weight of a transaction is the maximum of the 4 following metrics:\n\n** The total serialised size * 2 * SIGHASH_SCALE_FACTOR  (size defined by the witness tx format in BIP144)\n\n** The adjusted size = (Transaction weight by BIP141 - (number of inputs - number of non-OP_RETURN outputs) * 41) * SIGHASH_SCALE_FACTOR\n\n** nSigOps * 50 * SIGHASH_SCALE_FACTOR. All SigOps are equal (no witness scaling). For non-segwit txs, the sigops in output scriptPubKey are not counted, while the sigops in input scriptPubKey are counted.\n\n** SigHashSize defined in the last section\n\nTranslating to new metric, the current BIP141 limit is 360,000,000. This is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of adjusted size, or 80000 nSigOp.\n\nSee rationales in this post: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html\n\n\nBlock weight growing by time:\n\n* Numbers for example only. Exact number to be determined.\n\n* Block weight at HardForkTime is (5,000,000 * SIGHASH_SCALE_FACTOR)\n\n* By every 16 seconds growth of the median-time-past, the weight is increased by (1 * SIGHASH_SCALE_FACTOR)\n\n* The growth stops at (16,000,000 * SIGHASH_SCALE_FACTOR)\n\n* The growth does not dependent on the actual hardfork time. It\u2019s only based on median-time-past [using median-time-past so miners have no incentive to use a fake timestamp]\n\n* The limit for serialized size is 2.5 to 8MB in about 8 years. [again, numbers for example only]\n\n\nNew coinbase transaction format:\n\n* Existing coinbase format is allowed, except the new extended header in the coinbase witness. No OP_RETURN witness commitment is needed.\n\n* A new coinbase format is defined. The tx may have 1 or more inputs. The outpoint of the first input MUST have an n value of 0xffffffff, and use the previous block hash as the outpoint hash [This allows paying to the child of a particular block by signing the block hash]\n\n* ScriptSig of the first (coinbase) input is not executed. The size limit increased from 100 to 252 (same for old coinbase format)\n\n* Additional inputs MUST provide a valid scriptSig and/or witness for spending\n\n* Additional inputs may come from premature previous coinbase outputs [this allows previous blocks paying subsequent blocks to encourage confirmations]\n\n\nWitness merkle root:\n\n* If the coinbase is in old format, the witness merkle root is same as BIP141 by setting the witness hash of the coinbase tx as 0 (without the 32 byte witness reserved value)\n\n* If the coinbase is in new format, the witness hash of the coinbase tx is calculated by first removing the extended header\n\n* The witness merkle root is put in the extended header 2, not as an OP_RETURN output in coinbase tx.\n\n* The witness merkle root becomes mandatory. (It was optional in BIP141)\n\n\nOther consensus changes:\n\n* BIP9 will ignore the sign bit. [Setting the sign bit now is invalid so this has no real consensus impact]\n\n========\n\nAn experimental implementation of the above spec could be found at https://github.com/jl2012/bitcoin/tree/spoonnet1\n\nNot the same as my previous effort on the \u201cforcenet\u201d, the \u201cspoonnet\u201d is a full hardfork that will get you banned on the existing network.\n\nHaven\u2019t got the time to test the codes yet, not independently reviewed. But it passes all existing tests in Bitcoin Core. No one should use this in production, but I *think* it works fine on testnet like a normal bitcoind (as long as it is not activated)\n\nThings not implemented yet:\n\n1. Automated testing\n\n2. Post-hardfork support for old light wallets\n\n3. Wallet support, especially anti-tx-replay\n\n4. New p2p message to transmit secondary header (lower priority)\n\n5. Full mining and mempool support (not my priority)\n\n========\n\nPotential second stage change:\n\nRelative to the actual activation time, there could be a second stage with more drastic changes to fix one or both of the following problems:\n\n1. SHA256 shortcut like ASICBoost. All fixes to ASICBoost are not very elegant. But the question is, is it acceptable to have bitcoin-specific patent in the consensus protocol? Still, I believe the best way to solve this problem is the patent holder(s) to kindly somehow release the right to the community. \n\n2. Providing more nonce space in the 80-byte main header. However, this depends on ASICBoost being a free technology.\n\n3. Block withholding attack. There are pros and cons, but I generally agree with the analysis by Peter Todd at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012046.html . One point he didn\u2019t mention is that only small really needs pool mining, for the purpose of variance reduction. Big miners using pools are just lazy, and they work well without pool. That means only big solo miners are able to attack pools (i.e. small miners), while pools cannot do any counterattack. This obviously shows why fixing this is pro-small-miners. Also, with same hash rate, block withholding attack is more effective against a smaller pool than a big pool.\n\nAll of these changes involve a header change and require light wallets to upgrade. They also require firmware upgrade for all existing miners (change 2 doesn\u2019t). I think these shouldn\u2019t happen at least 2 years after the actual activation of the hardfork so people will have enough time to upgrade."
            },
            {
                "author": "Johnson Lau",
                "date": "2017-02-06T18:06:22",
                "message_text_only": "I fixed a flaw in my original design. It allowed a miner to create a fake transaction to cheat light wallets.\n\nAlso, with a second thought, I removed the backward compatibility with light wallets. Everyone, light or full, should opt-in to a hardfork.\n\nThe extended header is amended as follow\n\n* There must be exactly 2 witness items in coinbase witness (Header1; Header2)\n**Header1 is the miner dedicated space. Nothing here has no consensus meaning and is used for extranonce and merge mining. However, if the size is larger than 14 bytes, the first 6 bytes must be 0. For a legacy light node, this will look like a version 0 transaction with zero input and output. [No need for 14 bytes or below: even with the most broken design, a wallet can\u2019t misinterpret data with this small size as a valid tx]\n**Header2 is the secondary header. It must be 70-128 bytes. The first 4 bytes must be little-endian encoded number of transactions (minimum 1). The next 2 bytes must be 0. The next 32 bytes must be the transaction Merkle root. The next 32 bytes must be the witness Merkle root (to be defined later). The rest, if any, has no consensus meaning. However, miners MUST NOT use this space of non-bitcoin purpose [the additional space allows non-censensus enforced data to be included, easily accessible to light wallets; the structure of the first 6 bytes make it looks like an empty tx]\n* The main header commitment is H(H(Header1)|H(Header2))  H() = dSHA256() [legacy light wallets are broken and they must upgrade to join the new network]\n\nThe implementation is updated accordingly\n\n\n> On 6 Feb 2017, at 20:39, Johnson Lau via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Finally got some time over the Chinese New Year holiday to code and write this up. This is not the same as my previous forcenet ( https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html ). It is much simpler. Trying to activate it on testnet will get you banned. Trying to activate it on mainnet before consensus is reached will make you lose money.\n> \n> This proposal includes the following features:\n> \n> 1. A fixed starting time. Not dependent on miner signalling. However, it requires at least 51% of miners to actually build the new block format in order to get activated.\n> \n> 2. It has no mechanism to prevent a split. If 49% of miners insist on the original chain, they could keep going. Split prevention is a social problem, not a technical one.\n> \n> 3. It is compatible with existing Stratum mining protocol. Only pool software upgrade is needed\n> \n> 4. A new extended and flexible header is located at the witness field of the coinbase transaction\n> \n> 5. It is backward compatible with existing light wallets\n> \n> 6. Dedicated space for miners to put anything they want, which bitcoin users could completely ignore. Merge-mining friendly.\n> \n> 7. Small header space for miners to include non-consensus enforced bitcoin related data, useful for fee estimation etc.\n> \n> 8. A new transaction weight formula to encourage responsible use of UTXO\n> \n> 9. A linear growth of actual block size until certain limit\n> \n> 10. Sighash O(n^2) protection for legacy (non-segwit) outputs\n> \n> 11. Optional anti-transaction replay\n> \n> 12. A new optional coinbase tx format that allows additional inputs, including spending of immature previous coinbase outputs\n> \n> \n> \n> Specification [Rationales]:\n> \n> \n> Activation:\n> \n> * A \"hardfork signalling block\" is a block with the sign bit of header nVersion is set [Clearly invalid for old nodes; easy opt-out for light wallets]\n> \n> * If the median-time-past of the past 11 blocks is smaller than the HardForkTime (exact time to be determined), a hardfork signalling block is invalid.\n> \n> * Child of a hardfork signalling block MUST also be a hardfork signalling block\n> \n> * Initial hardfork signalling is optional, even if the HardForkTime has past [requires at least 51% of miners to actually build the new block format]\n> \n> * HardForkTime is determined by a broad consensus of the Bitcoin community. This is the only way to prevent a split.\n> \n> \n> Extended header:\n> \n> * Main header refers to the original 80 bytes bitcoin block header\n> \n> * A hardfork signalling block MUST have a additional extended header\n> \n> * The extended header is placed at the witness field of the coinbase transaction [There are 2 major advantages: 1. coinbase witness is otherwise useless; 2. Significantly simply the implementation with its stack structure]\n> \n> * There must be exactly 3 witness items (Header1; Header2 ; Header3)\n> **Header1 must be exactly 32 bytes of the original transaction hash Merkle root.\n> **Header2 is the secondary header. It must be 36-80 bytes. The first 4 bytes must be little-endian encoded number of transactions (minimum 1). The next 32 bytes must be the witness Merkle root (to be defined later). The rest, if any, has no consensus meaning. However, miners MUST NOT use this space of non-bitcoin purpose [the additional space allows non-censensus enforced data to be included, easily accessible to light wallets]\n> **Header3 is the miner dedicated space. It must not be larger than 252 bytes. Anything put here has no consensus meaning [space for merge mining; non-full nodes could completely ignore data in this space; 252 is the maximum size allowed for signal byte CompactSize]\n> \n> * The main header commitment is H(Header1|H(H(Header2)|H(Header3)))  H() = dSHA256() [The hardfork is transparent to light wallets, except one more 32-byte hash is needed to connect a transaction to the root]\n> \n> * To place the ext header, segwit becomes mandatory after hardfork\n> \n> \n> A \u201cbackdoor\u201d softfork the relax the size limit of Header 2 and Header 3:\n> \n> * A special BIP9 softfork is defined with bit-15. If this softfork is activated, full nodes will not enforce the size limit for Header 2 and Header 3. [To allow header expansion without a hardfork. Avoid miner abuse while providing flexibility. Expansion might be needed for new commitments like fraud proof commitments]\n> \n> \n> Anti-tx-replay:\n> \n> * Hardfork network version bit is 0x02000000. A tx is invalid if the highest nVersion byte is not zero, and the network version bit is not set.\n> \n> * Masked tx version is nVersion with the highest byte masked. If masked version is 3 or above, sighash for OP_CHECKSIG alike is calculated using BIP143, except 0x02000000 is added to the nHashType (the nHashType in signature is still a 1-byte value) [ensure a clean split of signatures; optionally fix the O(n^2) problem]\n> \n> * Pre-hardfork policy change: nVersion is determined by the masked tx version for policy purpose. Setting of Pre-hardfork network version bit 0x01000000 is allowed.\n> \n> * Details: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013473.html\n> \n> \n> Sighash limitation:\n> \n> * Sighash impact is estimated by \u201cLoose estimation\u201d in https://github.com/jl2012/bips/blob/065ea7429035d43ff90965f42b086fb7e1517291/bip-sighash.mediawiki\n> \n> * Only txs with masked version below 3 are counted. [because they are fixed by the BIP-143 like signature]\n> \n> * Each SigHashSize is defined as 1 tx weight (defined later).\n> \n> * SIGHASH_SCALE_FACTOR is 90 (see the BIP above)\n> \n> \n> New tx weight definition:\n> \n> * Weight of a transaction is the maximum of the 4 following metrics:\n> \n> ** The total serialised size * 2 * SIGHASH_SCALE_FACTOR  (size defined by the witness tx format in BIP144)\n> \n> ** The adjusted size = (Transaction weight by BIP141 - (number of inputs - number of non-OP_RETURN outputs) * 41) * SIGHASH_SCALE_FACTOR\n> \n> ** nSigOps * 50 * SIGHASH_SCALE_FACTOR. All SigOps are equal (no witness scaling). For non-segwit txs, the sigops in output scriptPubKey are not counted, while the sigops in input scriptPubKey are counted.\n> \n> ** SigHashSize defined in the last section\n> \n> Translating to new metric, the current BIP141 limit is 360,000,000. This is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of adjusted size, or 80000 nSigOp.\n> \n> See rationales in this post: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html\n> \n> \n> Block weight growing by time:\n> \n> * Numbers for example only. Exact number to be determined.\n> \n> * Block weight at HardForkTime is (5,000,000 * SIGHASH_SCALE_FACTOR)\n> \n> * By every 16 seconds growth of the median-time-past, the weight is increased by (1 * SIGHASH_SCALE_FACTOR)\n> \n> * The growth stops at (16,000,000 * SIGHASH_SCALE_FACTOR)\n> \n> * The growth does not dependent on the actual hardfork time. It\u2019s only based on median-time-past [using median-time-past so miners have no incentive to use a fake timestamp]\n> \n> * The limit for serialized size is 2.5 to 8MB in about 8 years. [again, numbers for example only]\n> \n> \n> New coinbase transaction format:\n> \n> * Existing coinbase format is allowed, except the new extended header in the coinbase witness. No OP_RETURN witness commitment is needed.\n> \n> * A new coinbase format is defined. The tx may have 1 or more inputs. The outpoint of the first input MUST have an n value of 0xffffffff, and use the previous block hash as the outpoint hash [This allows paying to the child of a particular block by signing the block hash]\n> \n> * ScriptSig of the first (coinbase) input is not executed. The size limit increased from 100 to 252 (same for old coinbase format)\n> \n> * Additional inputs MUST provide a valid scriptSig and/or witness for spending\n> \n> * Additional inputs may come from premature previous coinbase outputs [this allows previous blocks paying subsequent blocks to encourage confirmations]\n> \n> \n> Witness merkle root:\n> \n> * If the coinbase is in old format, the witness merkle root is same as BIP141 by setting the witness hash of the coinbase tx as 0 (without the 32 byte witness reserved value)\n> \n> * If the coinbase is in new format, the witness hash of the coinbase tx is calculated by first removing the extended header\n> \n> * The witness merkle root is put in the extended header 2, not as an OP_RETURN output in coinbase tx.\n> \n> * The witness merkle root becomes mandatory. (It was optional in BIP141)\n> \n> \n> Other consensus changes:\n> \n> * BIP9 will ignore the sign bit. [Setting the sign bit now is invalid so this has no real consensus impact]\n> \n> ========\n> \n> An experimental implementation of the above spec could be found at https://github.com/jl2012/bitcoin/tree/spoonnet1\n> \n> Not the same as my previous effort on the \u201cforcenet\u201d, the \u201cspoonnet\u201d is a full hardfork that will get you banned on the existing network.\n> \n> Haven\u2019t got the time to test the codes yet, not independently reviewed. But it passes all existing tests in Bitcoin Core. No one should use this in production, but I *think* it works fine on testnet like a normal bitcoind (as long as it is not activated)\n> \n> Things not implemented yet:\n> \n> 1. Automated testing\n> \n> 2. Post-hardfork support for old light wallets\n> \n> 3. Wallet support, especially anti-tx-replay\n> \n> 4. New p2p message to transmit secondary header (lower priority)\n> \n> 5. Full mining and mempool support (not my priority)\n> \n> ========\n> \n> Potential second stage change:\n> \n> Relative to the actual activation time, there could be a second stage with more drastic changes to fix one or both of the following problems:\n> \n> 1. SHA256 shortcut like ASICBoost. All fixes to ASICBoost are not very elegant. But the question is, is it acceptable to have bitcoin-specific patent in the consensus protocol? Still, I believe the best way to solve this problem is the patent holder(s) to kindly somehow release the right to the community. \n> \n> 2. Providing more nonce space in the 80-byte main header. However, this depends on ASICBoost being a free technology.\n> \n> 3. Block withholding attack. There are pros and cons, but I generally agree with the analysis by Peter Todd at https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012046.html . One point he didn\u2019t mention is that only small really needs pool mining, for the purpose of variance reduction. Big miners using pools are just lazy, and they work well without pool. That means only big solo miners are able to attack pools (i.e. small miners), while pools cannot do any counterattack. This obviously shows why fixing this is pro-small-miners. Also, with same hash rate, block withholding attack is more effective against a smaller pool than a big pool.\n> \n> All of these changes involve a header change and require light wallets to upgrade. They also require firmware upgrade for all existing miners (change 2 doesn\u2019t). I think these shouldn\u2019t happen at least 2 years after the actual activation of the hardfork so people will have enough time to upgrade.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Spoonnet: another experimental hardfork",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Johnson Lau"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 23604
        }
    },
    {
        "title": "[bitcoin-dev] Three hardfork-related BIPs",
        "thread_messages": [
            {
                "author": "mbtc-dev at xe0.me",
                "date": "2017-02-06T16:24:10",
                "message_text_only": "On 27.01.2017 20:03, Luke Dashjr via bitcoin-dev wrote:\n> Assume as a premise (despite your apparent disagreement below) that for\n> Bitcoin to function, a supermajority of economic activity needs to be \n> verified\n> using full nodes operated by the recipient. Evidence suggests that at \n> this\n> current time, at best 10% of economic activity is in fact using a full \n> node to\n> verify the transaction. On this basis, it seems pretty clear that \n> serious\n> action must be taken to change the status quo, and so for efforts to do \n> so\n> without dropping the block size have proven ineffective.\n> \nLets think like people in sales and marketing for a moment.\n\nThere's an implicit assumption here that ANY protocol or consensus-rule \nbased solution exists that would reverse the trend of diminishing full \nnode verified economic activity. Since there's no economic advantage to \nrunning a full node, there's no inherent motivation for implementation \n(or outright purchase) of full nodes by the very large percentage of \npeople who fall in the non-technical \"I just want it to work, and I \ndon't want my money stolen\" category. Yes, anyone on this list \nunderstands that \"don't want my money stolen\" is inherently connected to \nrunning your own node and using it for transactions, but the average \nuser does not, and even if they did, they don't have the resources (time \nand/or money) to do anything about it. Running your own full node \nincreases the protection agains double spend attacks and other protocol \nbases shenanigans, but now you've taken on another set of security \nexposures related to the physical box that is running the node. \nAnti-virus, off and on-site backups, multiple boxes/devices for \nmulti-sig, backup of key seeds.\n\nReducing (or even maintaining) the block size doesn't somehow increase \nthe number of people who are capable of running full nodes, and it \ndoesn't add any incentive for people already in that \"capable\" set to \nsuddenly take up the task of running and transacting via a full node. \nI'd argue that the size of the block-chain and the time to download it \nare the least concerning aspects to anyone faced with running their own \nnode and actually storing some of their wealth on it and using it for \ntransactions.\n\nYou're looking for a (maybe dangerous/maybe impossible) balance between \nchoking off casual (not full node) usage of bitcoin and yet trying to \nmake it more popular among the people (and organizations) who have the \ncapability and resources to run and transact on full nodes.\n\nWe should sit with this for a moment.\n\nOn one hand, Bitcoin may ultimately end up as digital currency \"only for \ngeeks and B2B transactions.\" I'd speculate we'd loose a big subset of \nthe geeks that way too, unless they happen to do a lot of transactions \nwith medium to large size businesses. (Small businesses won't be able to \nafford the expense of or the time to maintain the node.) There's some \nlevel of risk that this pushes bitcoin into oblivion. And is it really a \ndecentralized P2P currency if it's only used by medium and large \nbusinesses and a small set of technically capable individuals that \ntransact with those entities directly in BTC? And is it really a \ndecentralized currency in this scenario if its used mainly by medium and \nlarge businesses, banks, and exchanges? (I've purposely excluded small \nbusinesses because while they like the benefits of flexible payment \nsystems, more don't have the time or skill (or resources to hire the \nskill) needed to do a full node implementation.)\n\nI feel inherent cognitive dissonance between \"keep it decentralized\" and \n\"not useful to small business and individuals.\" One can make the \nargument that L2 solutions will be available for the small businesses \nand individuals but that doesn't solve the stated intent of reversing \nthe trend of transactions not originating from or being received by full \nnodes. I guess you're saying bitcoin will be stronger, more resistant to \noutside power agency and censorship if its only used by exchanges, \nbanks, large businesses, and die-hard technically inclined people.\n\n\nOn the other hand, maybe there's a scenario where an average person \nwalks into a big box electronics store in any developed country and buys \na \"personal digital bank\" appliance. I frame it this way because the \nmajority of the worlds population is never going to run a full node on \ntheir desktop or laptop. There's no viable scenario where that happens. \nLaptops and desktops are already diminishing in market share due to the \nintroduction of tablets and smartphones. General purpose OS's are also \ninherently un-secure, so  going down this route means we are immediately \nin the realm of lots of theft. Preventing theft (or loss due to errors) \nrequires additional digital key devices, or additional devices for \nmulti-sig transactions just for basic financial safety, not to mention a \nfunctioning backup plan, including off-site backups. \nRansomeware/phishing protection? Checking email and surfing the web on \nthe computer that holds your standard (non-multi-sig) wallet? \nForgetaboutit. It'll never reach critical mass. It's not a viable \nproposal. Not to mention, you can't physically carry your laptop with \nyou when you go to the shopping mall. In order for this appliance model \nto function, smartphone based implementations will need to interact with \nyour personal or family server/appliance, and you'll need to be able to \ndo multi-sig with a smartphone and another physical token you carry with \nyou. Imagine a 2 of 5 multi-sig wallet where your phone and an NFC or LE \nbluetooth device are sufficient to create a transaction on your home \nnode while shopping. Or your phone has a single sig wallet and you top \nit up from your appliance and it never has a high balance. In any case, \nI've made the argument before that the definition of \"bitcoin protocol\" \nshould, in addition to the consensus protocol, probably include a secure \nAPI protocol between wallet client and full node, and it still seems to \nbe an important missing piece. I want to be able to travel and spend BTC \nand I DON'T want to do general purpose computing like email and web \nsurfing on the same computer where I have a big chunk of life savings \nstored! I think defining this API will actually really support the use \nof user controlled full nodes for transactions! Imagine Trezor owners \nusing their own node for transactions! Bitpay is the only player I know \nof that provides enough of a software stack to set this up for yourself.\n\nI think reversing the non-full node transaction trend will have to be \nbased the appliance usage model. You buy a new 200-500Gb nvme SSD every \nyear and put it in one of the free slots. You upgrade when all slots are \nfull. This is one scenario that could put us on a trend of increasing \ntransactions originating and being received by personal full nodes, i.e. \nreversing centralization trends.\n\n\nIf there is any solution to this problem, it will need to recognize the \nfact that the supermajority of people on the planet are not technically \nsavvy nor are they inclined to take the time to learn how to protect \nthemselves with basic computer security much less how to use a full node \nfor bitcoin transactions. The solution, if it exists, will need to be \nhanded to them, and they'll need a reason to buy it. Any solution will \nalso need to recognize the fact that it will cost resources (time and \nmoney) to run a full node. Lots of people spend a huge portion of their \nincome just to get a smartphone because it's a useful communication \ndevice that does lots of other useful things. There's not nearly the \nsame level of need to spend on a full node for bitcoin security.\n\nAny solution to this problem should also recognize the fact that there's \na significant amount of work to do to have a functioning personal \nimplementation of a node and to use it for transactions. Even in my \nimagined future of polished and easy to use appliances, if you have \nenough capital in BTC that you need it and you can afford to buy it, \nyou're now only starting to deal with implementation issues. You've now \nbecome your own bank. Now you have to secure that appliance physically, \nsecure and back up the key seed material, secure the devices used to \naccess it, connect an app on your smartphone to the appliance so you can \ncreate transactions while out of your home, connect your home \ncomputer(s) to the appliance, do key exchange with the app/PC and the \nappliance or implement some sort of PKI on all devices. You've just \ntaken on the responsibility of a bank and a sysadmin! The higher the \nbalance, the more of a target you are, and the more time/money you have \nto spend mitigating risk. This is a huge centralizing force that no one \nreally seems to talk about. If you're the average person, you want to \nfind a trustworthy company or trusted friend/family to take care of that \nstuff for you. If you're a technically inclined person AND maybe there's \na way to reap some of the mining reward on a small scale, you're \nslightly more interested.\n\nAs a sysadmin for many years, I've seen first hand that most people want \ntools that just work, whether its software to make spreadsheets, \noperating systems, phones, or thermostats. My point here is that the \nnumber of people in the world who have the technical chops to run a node \nis ALWAYS going to be vastly lower than the number of people who will be \nusing bitcoin (or cryptocurrency).\n\nOf course we can make the argument that the definition of \"bitcoin\" is \nby design something to be used exclusively by institutions and geeks, \nand that this definition falls out of the necessity to ensure that it \nremains decentralized and censorship resistant. However, I'm not sure \nthat logic holds or that it doesn't introduce risk that that sort of \ndefinition drives bitcoin toward diminished relevance.\n\nAt the end of all this though experiment, I'm still convinced that if \nthe tools are built to enable flexible usage of full nodes (i.e. my \nphone, tablet or desktop app interfaces with the full node) then there's \na large potential for increased usage of full nodes.\n\nThanks,\nG"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-02-07T20:32:46",
                "message_text_only": "The semantics of a necessarily secure and private client-server protocol differ from that of a necessarily distributed and public P2P protocol. I realize you refer to the C/S as a distinct API, but this point is worthy of clarification and emphasis.\n\nThe introduction of client-server sub-protocols into the Bitcoin P2P protocol has resulted in large scale privacy loss, weakened end-user security and reduced access to the public network. Plans to mitigate these issues stand to make matters worse by restricting access to the public network through the introduction of strong identity to the P2P protocol.\n\nIt is not the case that C/S APIs against private full nodes do not exist. Electrum (stratum) and Libbitcoin (zeromq) are notable examples. The management difficulties are not small, but there are also fundamental issues that must first be addressed.\n\nIn your example you imagine pluggsble SSD space, but Satoshi derivatives have scale deficiencies unrelated to storage. If we are going to get to reliable, cheap, performant personal full nodes (which I agree is essential to Bitcoin survival) we need nodes that scale (i.e. to the available hardware). We also require a robust, reliable and performant node/server development stack, not just the impossible choice between a fragile monolith and centralizing web APIs/wallets.\n\nAll centralized interfaces to Bitcoin (wallets, web APIs, payment services) shrink the economic consensus and thereby weaken its defense of sound and fungible money. The only solution is personally-controlled full nodes, as you say. The incentives for running a full node are sufficient if the cost of doing so is low. Getting there requires a node/server architecture intended for this outcome. Then maybe appliances are feasible.\n\ne\n\n\n> On Feb 6, 2017, at 8:24 AM, netkn0t (marcus) via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> On 27.01.2017 20:03, Luke Dashjr via bitcoin-dev wrote:\n>> Assume as a premise (despite your apparent disagreement below) that for\n>> Bitcoin to function, a supermajority of economic activity needs to be verified\n>> using full nodes operated by the recipient. Evidence suggests that at this\n>> current time, at best 10% of economic activity is in fact using a full node to\n>> verify the transaction. On this basis, it seems pretty clear that serious\n>> action must be taken to change the status quo, and so for efforts to do so\n>> without dropping the block size have proven ineffective.\n> Lets think like people in sales and marketing for a moment.\n> \n> There's an implicit assumption here that ANY protocol or consensus-rule based solution exists that would reverse the trend of diminishing full node verified economic activity. Since there's no economic advantage to running a full node, there's no inherent motivation for implementation (or outright purchase) of full nodes by the very large percentage of people who fall in the non-technical \"I just want it to work, and I don't want my money stolen\" category. Yes, anyone on this list understands that \"don't want my money stolen\" is inherently connected to running your own node and using it for transactions, but the average user does not, and even if they did, they don't have the resources (time and/or money) to do anything about it. Running your own full node increases the protection agains double spend attacks and other protocol bases shenanigans, but now you've taken on another set of security exposures related to the physical box that is running the node. Anti-virus, off and on-site backups, multiple boxes/devices for multi-sig, backup of key seeds.\n> \n> Reducing (or even maintaining) the block size doesn't somehow increase the number of people who are capable of running full nodes, and it doesn't add any incentive for people already in that \"capable\" set to suddenly take up the task of running and transacting via a full node. I'd argue that the size of the block-chain and the time to download it are the least concerning aspects to anyone faced with running their own node and actually storing some of their wealth on it and using it for transactions.\n> \n> You're looking for a (maybe dangerous/maybe impossible) balance between choking off casual (not full node) usage of bitcoin and yet trying to make it more popular among the people (and organizations) who have the capability and resources to run and transact on full nodes.\n> \n> We should sit with this for a moment.\n> \n> On one hand, Bitcoin may ultimately end up as digital currency \"only for geeks and B2B transactions.\" I'd speculate we'd loose a big subset of the geeks that way too, unless they happen to do a lot of transactions with medium to large size businesses. (Small businesses won't be able to afford the expense of or the time to maintain the node.) There's some level of risk that this pushes bitcoin into oblivion. And is it really a decentralized P2P currency if it's only used by medium and large businesses and a small set of technically capable individuals that transact with those entities directly in BTC? And is it really a decentralized currency in this scenario if its used mainly by medium and large businesses, banks, and exchanges? (I've purposely excluded small businesses because while they like the benefits of flexible payment systems, more don't have the time or skill (or resources to hire the skill) needed to do a full node implementation.)\n> \n> I feel inherent cognitive dissonance between \"keep it decentralized\" and \"not useful to small business and individuals.\" One can make the argument that L2 solutions will be available for the small businesses and individuals but that doesn't solve the stated intent of reversing the trend of transactions not originating from or being received by full nodes. I guess you're saying bitcoin will be stronger, more resistant to outside power agency and censorship if its only used by exchanges, banks, large businesses, and die-hard technically inclined people.\n> \n> \n> On the other hand, maybe there's a scenario where an average person walks into a big box electronics store in any developed country and buys a \"personal digital bank\" appliance. I frame it this way because the majority of the worlds population is never going to run a full node on their desktop or laptop. There's no viable scenario where that happens. Laptops and desktops are already diminishing in market share due to the introduction of tablets and smartphones. General purpose OS's are also inherently un-secure, so  going down this route means we are immediately in the realm of lots of theft. Preventing theft (or loss due to errors) requires additional digital key devices, or additional devices for multi-sig transactions just for basic financial safety, not to mention a functioning backup plan, including off-site backups. Ransomeware/phishing protection? Checking email and surfing the web on the computer that holds your standard (non-multi-sig) wallet? Forgetaboutit. It'll never reach critical mass. It's not a viable proposal. Not to mention, you can't physically carry your laptop with you when you go to the shopping mall. In order for this appliance model to function, smartphone based implementations will need to interact with your personal or family server/appliance, and you'll need to be able to do multi-sig with a smartphone and another physical token you carry with you. Imagine a 2 of 5 multi-sig wallet where your phone and an NFC or LE bluetooth device are sufficient to create a transaction on your home node while shopping. Or your phone has a single sig wallet and you top it up from your appliance and it never has a high balance. In any case, I've made the argument before that the definition of \"bitcoin protocol\" should, in addition to the consensus protocol, probably include a secure API protocol between wallet client and full node, and it still seems to be an important missing piece. I want to be able to travel and spend BTC and I DON'T want to do general purpose computing like email and web surfing on the same computer where I have a big chunk of life savings stored! I think defining this API will actually really support the use of user controlled full nodes for transactions! Imagine Trezor owners using their own node for transactions! Bitpay is the only player I know of that provides enough of a software stack to set this up for yourself.\n> \n> I think reversing the non-full node transaction trend will have to be based the appliance usage model. You buy a new 200-500Gb nvme SSD every year and put it in one of the free slots. You upgrade when all slots are full. This is one scenario that could put us on a trend of increasing transactions originating and being received by personal full nodes, i.e. reversing centralization trends.\n> \n> \n> If there is any solution to this problem, it will need to recognize the fact that the supermajority of people on the planet are not technically savvy nor are they inclined to take the time to learn how to protect themselves with basic computer security much less how to use a full node for bitcoin transactions. The solution, if it exists, will need to be handed to them, and they'll need a reason to buy it. Any solution will also need to recognize the fact that it will cost resources (time and money) to run a full node. Lots of people spend a huge portion of their income just to get a smartphone because it's a useful communication device that does lots of other useful things. There's not nearly the same level of need to spend on a full node for bitcoin security.\n> \n> Any solution to this problem should also recognize the fact that there's a significant amount of work to do to have a functioning personal implementation of a node and to use it for transactions. Even in my imagined future of polished and easy to use appliances, if you have enough capital in BTC that you need it and you can afford to buy it, you're now only starting to deal with implementation issues. You've now become your own bank. Now you have to secure that appliance physically, secure and back up the key seed material, secure the devices used to access it, connect an app on your smartphone to the appliance so you can create transactions while out of your home, connect your home computer(s) to the appliance, do key exchange with the app/PC and the appliance or implement some sort of PKI on all devices. You've just taken on the responsibility of a bank and a sysadmin! The higher the balance, the more of a target you are, and the more time/money you have to spend mitigating risk. This is a huge centralizing force that no one really seems to talk about. If you're the average person, you want to find a trustworthy company or trusted friend/family to take care of that stuff for you. If you're a technically inclined person AND maybe there's a way to reap some of the mining reward on a small scale, you're slightly more interested.\n> \n> As a sysadmin for many years, I've seen first hand that most people want tools that just work, whether its software to make spreadsheets, operating systems, phones, or thermostats. My point here is that the number of people in the world who have the technical chops to run a node is ALWAYS going to be vastly lower than the number of people who will be using bitcoin (or cryptocurrency).\n> \n> Of course we can make the argument that the definition of \"bitcoin\" is by design something to be used exclusively by institutions and geeks, and that this definition falls out of the necessity to ensure that it remains decentralized and censorship resistant. However, I'm not sure that logic holds or that it doesn't introduce risk that that sort of definition drives bitcoin toward diminished relevance.\n> \n> At the end of all this though experiment, I'm still convinced that if the tools are built to enable flexible usage of full nodes (i.e. my phone, tablet or desktop app interfaces with the full node) then there's a large potential for increased usage of full nodes.\n> \n> Thanks,\n> G\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Staf Verhaegen",
                "date": "2017-02-11T15:26:33",
                "message_text_only": "Eric Voskuil via bitcoin-dev schreef op zo 29-01-2017 om 11:37 [-0800]:\n> > On Jan 29, 2017, at 11:15 AM, Tom Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > \n> >> On 1/28/2017 10:29 AM, Peter Todd via bitcoin-dev wrote:\n> >> a world of nodes in large datacenters is a world where it's very easy\n> >> to force the few Bitcoin nodes remaining to follow AML/KYC rules\n> > \n> > If that's true, why haven't we already seen AML/KYC required of mining\n> > pools?  That would be comparatively trivial.\n> \n> It is true, there is no question. The fact that an attack does not appear to have occurred does not mean that the vulnerability exists. It is as you say a trivial exploit, which means it will happen when the economic incentive is great enough. Analogous attacks on other points of centralization are already well underway.\n\nWhat on first sight seems trivial may be totally different when looking\ndeeper. People here seem to not realise that a lot of data centers (the\nIAAS ones) just are just grouping the computers and provide remote\naccess. The client have full control over the machines. The center thus\njust provides the hardware, the power and the internet access. They\ntypically don't inspect your internet traffic only reduce the speed if\nyou are going above certain threshold. Additionally there are countries\nlike Iceland that specifically make laws to not let the government get\npower over data and network traffic in data centers.\nDomestic ISP services typically want to prioritize traffic and thus have\nmost of the time network traffic deep packet inspection (DPI)\ncapabilities. These are thus much easier forced by government to filter\ncertain traffic. Additionally these companies often fall under\ntelecommunication laws also given government more control over them than\nin a typical data center.\n\nI host my Bitcoin node in a German datacenter and am sure it is more\ncensorship resistant that a node going through any American ISP.\n\ngreets,\nStaf.\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 230 bytes\nDesc: This is a digitally signed message part\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170211/df91ff2d/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Three hardfork-related BIPs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "mbtc-dev at xe0.me",
                "Eric Voskuil",
                "Staf Verhaegen"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 24629
        }
    },
    {
        "title": "[bitcoin-dev] Proof of Nodework (PoNW) - a method to trustlessly reward nodes for storing and verifying the blockchain",
        "thread_messages": [
            {
                "author": "John Hardy",
                "date": "2017-02-07T11:27:55",
                "message_text_only": "Proof of Nodework (PoNW) is a way to reward individual nodes for keeping a full copy of and verifying the blockchain.\n\n\nHopefully they also do useful \u2018traditional\u2019 node activities too like relay transactions and blocks, but there isn\u2019t really any way I can think of to trustlessly verify this also.\n\n\nPoNW would require a new separate area of block space, a nodeblock, purely concerned with administering the system. A nodeblock is committed to a block as with SegWit. A recent history of nodeblocks needs to be stored by nodes, however the data eventually becomes obsolete and so does not need to be retained forever.\n\n\nIn order to prevent Sybil, a node must register an Bitcoin address by submitting an addNode transaction - along with a security deposit to prevent cheating.\n\n\nThis transaction will be stored in the nodeblock. Once a node can see that its addNode transaction has been added it can begin the PoNW process. The node\u2019s registered address will be hashed with the block header of the block it wants to work on. This will determine exactly where within the blockchain to begin the PoNW.\n\n\nThe PoNW method could be as simple as creating a Merkle tree from the randomly generated point on the blockchain, though a method that is CPU/Memory heavy and less likely to be replaced by dedicated hardware like ASICs would be better. This process could not begin until the most recent block has been fully verified, and while being carried out should still enable normal relay activities to proceed as normal, since it shouldn\u2019t tie up network at all. The data processed should also be mixed with data from the latest block so that it cannot be computed in advance.\n\n\nA node can do as much PoNW for a block as it likes. Once finished it will then create a nodeWorkComplete transaction for that block with its final proof value, add how much \u2018work\u2019 it did - and create a couple of assertions about what it processed (such as there were x number of pieces of data matching a particular value during calculating). These assertions can be accurate or inaccurate.\n\n\nThe system will run in epochs. During each epoch of say 2016 blocks, there will be an extended window for PoNW transactions to be added to nodeblocks to limit minor censorship.\n\n\nThe random hash generated from a node\u2019s address and blockhash will also be used to determine nodeWorkComplete transactions from a previous block that the node must also verify, and correctly calculate whether the assertions it made were true or false. The average PoNW that a node performed in its previous x nodeblocks will be used to determine the target PoNW for the node to verify - and this will randomly be a large number of smaller PoNW transactions, or a smaller number of large PoNW. This process will be deterministic based on that block and address hash. All the data will be put together in a transaction and then signed by the node addresses private key.\n\n\nIf a nodeWorkComplete transaction contains any incorrect information in an attempt to cheat the validation process a challenge transaction can be created. This begins a refereeing process where other nodes check the challenge and vote whether it is to be upheld or not. The losing node is punished by losing their accrued PoNW for that epoch and a percentage of their security deposit.\n\n\nNodes will also be punished if they broadcast more than one signed transaction per block.\n\n\nIn order to prevent nodes from having multiple keys registered - which would enable them choose to perform PoNW on a subset of the data that they hold - the share of reward that the node gets will be multiplied based on the number of blocks within an epoch that the node performs PoNW on. The share of reward is limited based on how much security deposit has been staked. The higher the PoNW the higher the deposit needed in order to claim their full allocation of any reward.\n\n\nAt the end of an epoch, with a wait period for any delayed or censored transactions or challenges to be included and settled up, the process of calculating the reward each node is due can begin. This will then be then paid in a regular block, and means for all the data involved in PoNW, the only permanent mark it makes on the main blockchain is for a transaction that pays all addresses their share of the reward at the end of epoch. Any miner who creates a block without correctly calculating and paying the due reward will have mined an invalid block and be orphaned.\n\n\nThe question of where and how much the reward comes from is a different one. It could come from the existing miner reward, or a special new tx donation fee for nodes. If there was some way for users to \u2018donate\u2019 to the reward pool for nodes this would increase the incentive for additional nodes to participate on the network in the event of centralisation.\n\n\nThis is a relatively effective way to create a reward for all nodes participating on a network. I\u2019d be keen to field any questions or critiques.\n\nThanks,\n\n\nJohn Hardy\n\njohn at seebitcoin.com\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170207/ff6c8c31/attachment-0001.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-02-12T20:22:33",
                "message_text_only": "Hi John,\n RSK platform (a Bitcoin sidechain) is already prepared to do something\nsimilar to this, although very efficiently. We set apart 1% of the block\nreward to automatically reward full nodes.\n\nWe have two systems being evaluated: the first is based on PoUBS (Proof of\nUnique Blockchain Storage) which uses asymmetric-time operations to encode\nthe blockchain based on each user public key such that decoding is fast,\nbut encoding is slow. The second is more traditional proof of\nretrievability, but it requires some ASIC-resistance assumptions.\n\nIn both cases, a special smart contract is being called at every block that\ncreates periodic challenges. Every full node that wants to participate can\nsubmits a commitment to the Merkle hash root of a pseudo-random sequence of\nencoded blocks. Then the smart contract chooses random elements from the\ncommitted dataset, and each full node has a period to submit Merkle-proofs\nthat such random elements belong to the commitment.\n\nTo prevent blockchain bloat we designed a very cool new type of transaction\npayload: Ephemeral Payload. Ephemeral payload is a payload in a transaction\nthat gets discarded after N blocks if no smart contract does reference it.\nIf is does, it's solidified forever in the blockchain.\nThen there is a challenge phase where other full nodes can inform the smart\ncontract if they find an error in the submitted responses. Then the smart\ncontract ONLY evaluates the responses which have been questioned by users.\n\nThis way the smart contract does very little computation (only when a user\nmisbehaves) and the blockchain normally does not store any proof forever\n(only the ones created by misbehaving users).\n\nBecause RSK/Rootstock has a very short block interval (10 seconds), all\nthis happens very quickly and does not require much computation.\n\nBest regards,\n Sergio Lerner\n Chief Scientist RSK (aka Roostock)\n\n\nOn Tue, Feb 7, 2017 at 8:27 AM, John Hardy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Proof of Nodework (PoNW) is a way to reward individual nodes for keeping a\n> full copy of and verifying the blockchain.\n>\n> Hopefully they also do useful \u2018traditional\u2019 node activities too like relay\n> transactions and blocks, but there isn\u2019t really any way I can think of to\n> trustlessly verify this also.\n>\n> PoNW would require a new separate area of block space, a nodeblock, purely\n> concerned with administering the system. A nodeblock is committed to a\n> block as with SegWit. A recent history of nodeblocks needs to be stored by\n> nodes, however the data eventually becomes obsolete and so does not need to\n> be retained forever.\n>\n> In order to prevent Sybil, a node must register an Bitcoin address by\n> submitting an addNode transaction - along with a security deposit to\n> prevent cheating.\n>\n> This transaction will be stored in the nodeblock. Once a node can see that\n> its addNode transaction has been added it can begin the PoNW process. The\n> node\u2019s registered address will be hashed with the block header of the block\n> it wants to work on. This will determine exactly where within the\n> blockchain to begin the PoNW.\n>\n> The PoNW method could be as simple as creating a Merkle tree from the\n> randomly generated point on the blockchain, though a method that is\n> CPU/Memory heavy and less likely to be replaced by dedicated hardware like\n> ASICs would be better. This process could not begin until the most recent\n> block has been fully verified, and while being carried out should still\n> enable normal relay activities to proceed as normal, since it shouldn\u2019t tie\n> up network at all. The data processed should also be mixed with data from\n> the latest block so that it cannot be computed in advance.\n>\n> A node can do as much PoNW for a block as it likes. Once finished it will\n> then create a nodeWorkComplete transaction for that block with its final\n> proof value, add how much \u2018work\u2019 it did - and create a couple of assertions\n> about what it processed (such as there were x number of pieces of data\n> matching a particular value during calculating). These assertions can be\n> accurate or inaccurate.\n>\n> The system will run in epochs. During each epoch of say 2016 blocks, there\n> will be an extended window for PoNW transactions to be added to nodeblocks\n> to limit minor censorship.\n>\n> The random hash generated from a node\u2019s address and blockhash will also be\n> used to determine nodeWorkComplete transactions from a previous block that\n> the node must also verify, and correctly calculate whether the assertions\n> it made were true or false. The average PoNW that a node performed in its\n> previous x nodeblocks will be used to determine the target PoNW for the\n> node to verify - and this will randomly be a large number of smaller PoNW\n> transactions, or a smaller number of large PoNW. This process will be\n> deterministic based on that block and address hash. All the data will be\n> put together in a transaction and then signed by the node addresses private\n> key.\n>\n> If a nodeWorkComplete transaction contains any incorrect information in an\n> attempt to cheat the validation process a challenge transaction can be\n> created. This begins a refereeing process where other nodes check the\n> challenge and vote whether it is to be upheld or not. The losing node is\n> punished by losing their accrued PoNW for that epoch and a percentage of\n> their security deposit.\n>\n> Nodes will also be punished if they broadcast more than one signed\n> transaction per block.\n>\n> In order to prevent nodes from having multiple keys registered - which\n> would enable them choose to perform PoNW on a subset of the data that they\n> hold - the share of reward that the node gets will be multiplied based on\n> the number of blocks within an epoch that the node performs PoNW on. The\n> share of reward is limited based on how much security deposit has been\n> staked. The higher the PoNW the higher the deposit needed in order to claim\n> their full allocation of any reward.\n>\n> At the end of an epoch, with a wait period for any delayed or censored\n> transactions or challenges to be included and settled up, the process of\n> calculating the reward each node is due can begin. This will then be then\n> paid in a regular block, and means for all the data involved in PoNW, the\n> only permanent mark it makes on the main blockchain is for a transaction\n> that pays all addresses their share of the reward at the end of epoch. Any\n> miner who creates a block without correctly calculating and paying the due\n> reward will have mined an invalid block and be orphaned.\n>\n> The question of where and how much the reward comes from is a different\n> one. It could come from the existing miner reward, or a special new tx\n> donation fee for nodes. If there was some way for users to \u2018donate\u2019 to the\n> reward pool for nodes this would increase the incentive for additional\n> nodes to participate on the network in the event of centralisation.\n>\n> This is a relatively effective way to create a reward for all nodes\n> participating on a network. I\u2019d be keen to field any questions or critiques.\n>\n> Thanks,\n>\n>\n> John Hardy\n>\n> john at seebitcoin.com\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170212/71eb46b5/attachment.html>"
            },
            {
                "author": "John Hardy",
                "date": "2017-02-13T11:58:09",
                "message_text_only": "Hi Sergio,\n\n\nThanks for your response, interesting work, very excited for RSK.\n\n\nI like the ephemeral payload, I suppose that aspect of my proposal could be described as ephemeral blockspace.\n\n\nI'm curious about the challenge phase, what incentive do nodes to have to check other nodes' responses? Is any validation of responses mandatory, or does policing the system rely on altruism?\n\n\nI also wondered how time-based responses are enforced? What prevents a miner censoring challenge responses so they do not get included in a block 'in time' - if  inclusion within a block is the mechanism used?\n\n\nI saw your tweet on Lumino - sounds very promising. Would be keen to take a look at the paper if you're looking for any additional review at this stage.\n\n\nRegards,\n\n\nJohn Hardy\n\n\n________________________________\nFrom: Sergio Demian Lerner <sergio.d.lerner at gmail.com>\nSent: Sunday, February 12, 2017 8:22 PM\nTo: John Hardy; Bitcoin Protocol Discussion\nSubject: Re: [bitcoin-dev] Proof of Nodework (PoNW) - a method to trustlessly reward nodes for storing and verifying the blockchain\n\nHi John,\n RSK platform (a Bitcoin sidechain) is already prepared to do something similar to this, although very efficiently. We set apart 1% of the block reward to automatically reward full nodes.\n\nWe have two systems being evaluated: the first is based on PoUBS (Proof of Unique Blockchain Storage) which uses asymmetric-time operations to encode the blockchain based on each user public key such that decoding is fast, but encoding is slow. The second is more traditional proof of retrievability, but it requires some ASIC-resistance assumptions.\n\nIn both cases, a special smart contract is being called at every block that creates periodic challenges. Every full node that wants to participate can submits a commitment to the Merkle hash root of a pseudo-random sequence of encoded blocks. Then the smart contract chooses random elements from the committed dataset, and each full node has a period to submit Merkle-proofs that such random elements belong to the commitment.\n\nTo prevent blockchain bloat we designed a very cool new type of transaction payload: Ephemeral Payload. Ephemeral payload is a payload in a transaction that gets discarded after N blocks if no smart contract does reference it. If is does, it's solidified forever in the blockchain.\nThen there is a challenge phase where other full nodes can inform the smart contract if they find an error in the submitted responses. Then the smart contract ONLY evaluates the responses which have been questioned by users.\n\nThis way the smart contract does very little computation (only when a user misbehaves) and the blockchain normally does not store any proof forever (only the ones created by misbehaving users).\n\nBecause RSK/Rootstock has a very short block interval (10 seconds), all this happens very quickly and does not require much computation.\n\nBest regards,\n Sergio Lerner\n Chief Scientist RSK (aka Roostock)\n\n\nOn Tue, Feb 7, 2017 at 8:27 AM, John Hardy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org<mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n\nProof of Nodework (PoNW) is a way to reward individual nodes for keeping a full copy of and verifying the blockchain.\n\n\nHopefully they also do useful \u2018traditional\u2019 node activities too like relay transactions and blocks, but there isn\u2019t really any way I can think of to trustlessly verify this also.\n\n\nPoNW would require a new separate area of block space, a nodeblock, purely concerned with administering the system. A nodeblock is committed to a block as with SegWit. A recent history of nodeblocks needs to be stored by nodes, however the data eventually becomes obsolete and so does not need to be retained forever.\n\n\nIn order to prevent Sybil, a node must register an Bitcoin address by submitting an addNode transaction - along with a security deposit to prevent cheating.\n\n\nThis transaction will be stored in the nodeblock. Once a node can see that its addNode transaction has been added it can begin the PoNW process. The node\u2019s registered address will be hashed with the block header of the block it wants to work on. This will determine exactly where within the blockchain to begin the PoNW.\n\n\nThe PoNW method could be as simple as creating a Merkle tree from the randomly generated point on the blockchain, though a method that is CPU/Memory heavy and less likely to be replaced by dedicated hardware like ASICs would be better. This process could not begin until the most recent block has been fully verified, and while being carried out should still enable normal relay activities to proceed as normal, since it shouldn\u2019t tie up network at all. The data processed should also be mixed with data from the latest block so that it cannot be computed in advance.\n\n\nA node can do as much PoNW for a block as it likes. Once finished it will then create a nodeWorkComplete transaction for that block with its final proof value, add how much \u2018work\u2019 it did - and create a couple of assertions about what it processed (such as there were x number of pieces of data matching a particular value during calculating). These assertions can be accurate or inaccurate.\n\n\nThe system will run in epochs. During each epoch of say 2016 blocks, there will be an extended window for PoNW transactions to be added to nodeblocks to limit minor censorship.\n\n\nThe random hash generated from a node\u2019s address and blockhash will also be used to determine nodeWorkComplete transactions from a previous block that the node must also verify, and correctly calculate whether the assertions it made were true or false. The average PoNW that a node performed in its previous x nodeblocks will be used to determine the target PoNW for the node to verify - and this will randomly be a large number of smaller PoNW transactions, or a smaller number of large PoNW. This process will be deterministic based on that block and address hash. All the data will be put together in a transaction and then signed by the node addresses private key.\n\n\nIf a nodeWorkComplete transaction contains any incorrect information in an attempt to cheat the validation process a challenge transaction can be created. This begins a refereeing process where other nodes check the challenge and vote whether it is to be upheld or not. The losing node is punished by losing their accrued PoNW for that epoch and a percentage of their security deposit.\n\n\nNodes will also be punished if they broadcast more than one signed transaction per block.\n\n\nIn order to prevent nodes from having multiple keys registered - which would enable them choose to perform PoNW on a subset of the data that they hold - the share of reward that the node gets will be multiplied based on the number of blocks within an epoch that the node performs PoNW on. The share of reward is limited based on how much security deposit has been staked. The higher the PoNW the higher the deposit needed in order to claim their full allocation of any reward.\n\n\nAt the end of an epoch, with a wait period for any delayed or censored transactions or challenges to be included and settled up, the process of calculating the reward each node is due can begin. This will then be then paid in a regular block, and means for all the data involved in PoNW, the only permanent mark it makes on the main blockchain is for a transaction that pays all addresses their share of the reward at the end of epoch. Any miner who creates a block without correctly calculating and paying the due reward will have mined an invalid block and be orphaned.\n\n\nThe question of where and how much the reward comes from is a different one. It could come from the existing miner reward, or a special new tx donation fee for nodes. If there was some way for users to \u2018donate\u2019 to the reward pool for nodes this would increase the incentive for additional nodes to participate on the network in the event of centralisation.\n\n\nThis is a relatively effective way to create a reward for all nodes participating on a network. I\u2019d be keen to field any questions or critiques.\n\nThanks,\n\n\nJohn Hardy\n\njohn at seebitcoin.com<mailto:john at seebitcoin.com>\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org<mailto:bitcoin-dev at lists.linuxfoundation.org>\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/1173c865/attachment.html>"
            },
            {
                "author": "Sergio Demian Lerner",
                "date": "2017-02-13T14:48:24",
                "message_text_only": "On Mon, Feb 13, 2017 at 8:58 AM, John Hardy <john at seebitcoin.com> wrote:\n\n> Hi Sergio,\n>\n>\n> Thanks for your response, interesting work, very excited for RSK.\n>\n>\n> I like the ephemeral payload, I suppose that aspect of my proposal could\n> be described as ephemeral blockspace.\n>\n>\n> I'm curious about the challenge phase, what incentive do nodes to have to\n> check other nodes' responses?\n>\nThe reward is split between all full nodes. Therefore each full node has an\nincentive to check at least some other full nodes responses because there\nis a competition for the full node reward. At the end, each full node\nresponse will be checked by more than other node with high probability.\nAlso each full node does a small pre-deposit, that is consumed if the node\ncheats.\n\nIs any validation of responses mandatory, or does policing the system rely\n> on altruism?\n>\n>\n> As previously said,  validation is not mandatory.\n\n> I also wondered how time-based responses are enforced? What prevents a\n> miner censoring challenge responses so they do not get included in a block\n> 'in time' - if  inclusion within a block is the mechanism used?\n>\nThere is not many defenses against censorship but try to hide your identity\nuntil the end of the protocol. But if somebody knows that your transactions\nbelong to you, then there is little defense. We just wait more than a\nsingle block for the commitments, so several miners must collude in order\nto censor a transaction.\n\n>\n> I saw your tweet on Lumino - sounds very promising. Would be keen to take\n> a look at the paper if you're looking for any additional review at this\n> stage.\n>\nI'm keeping it private against all my desire because I want it to be\nreviewed before I publish it. Credibility is very easily lost.\nThe same idea (Ephemeral Data) has been used to design the Lumino Network.\n\n>\n> Regards,\n>\n>\n> John Hardy\n>\n>\n> ------------------------------\n> *From:* Sergio Demian Lerner <sergio.d.lerner at gmail.com>\n> *Sent:* Sunday, February 12, 2017 8:22 PM\n> *To:* John Hardy; Bitcoin Protocol Discussion\n> *Subject:* Re: [bitcoin-dev] Proof of Nodework (PoNW) - a method to\n> trustlessly reward nodes for storing and verifying the blockchain\n>\n> Hi John,\n>  RSK platform (a Bitcoin sidechain) is already prepared to do something\n> similar to this, although very efficiently. We set apart 1% of the block\n> reward to automatically reward full nodes.\n>\n> We have two systems being evaluated: the first is based on PoUBS (Proof of\n> Unique Blockchain Storage) which uses asymmetric-time operations to encode\n> the blockchain based on each user public key such that decoding is fast,\n> but encoding is slow. The second is more traditional proof of\n> retrievability, but it requires some ASIC-resistance assumptions.\n>\n> In both cases, a special smart contract is being called at every block\n> that creates periodic challenges. Every full node that wants to participate\n> can submits a commitment to the Merkle hash root of a pseudo-random\n> sequence of encoded blocks. Then the smart contract chooses random elements\n> from the committed dataset, and each full node has a period to submit\n> Merkle-proofs that such random elements belong to the commitment.\n>\n> To prevent blockchain bloat we designed a very cool new type of\n> transaction payload: Ephemeral Payload. Ephemeral payload is a payload in a\n> transaction that gets discarded after N blocks if no smart contract does\n> reference it. If is does, it's solidified forever in the blockchain.\n> Then there is a challenge phase where other full nodes can inform the\n> smart contract if they find an error in the submitted responses. Then the\n> smart contract ONLY evaluates the responses which have been questioned by\n> users.\n>\n> This way the smart contract does very little computation (only when a user\n> misbehaves) and the blockchain normally does not store any proof forever\n> (only the ones created by misbehaving users).\n>\n> Because RSK/Rootstock has a very short block interval (10 seconds), all\n> this happens very quickly and does not require much computation.\n>\n> Best regards,\n>  Sergio Lerner\n>  Chief Scientist RSK (aka Roostock)\n>\n>\n> On Tue, Feb 7, 2017 at 8:27 AM, John Hardy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Proof of Nodework (PoNW) is a way to reward individual nodes for keeping\n>> a full copy of and verifying the blockchain.\n>>\n>> Hopefully they also do useful \u2018traditional\u2019 node activities too like\n>> relay transactions and blocks, but there isn\u2019t really any way I can think\n>> of to trustlessly verify this also.\n>>\n>> PoNW would require a new separate area of block space, a nodeblock,\n>> purely concerned with administering the system. A nodeblock is committed to\n>> a block as with SegWit. A recent history of nodeblocks needs to be stored\n>> by nodes, however the data eventually becomes obsolete and so does not need\n>> to be retained forever.\n>>\n>> In order to prevent Sybil, a node must register an Bitcoin address by\n>> submitting an addNode transaction - along with a security deposit to\n>> prevent cheating.\n>>\n>> This transaction will be stored in the nodeblock. Once a node can see\n>> that its addNode transaction has been added it can begin the PoNW process.\n>> The node\u2019s registered address will be hashed with the block header of the\n>> block it wants to work on. This will determine exactly where within the\n>> blockchain to begin the PoNW.\n>>\n>> The PoNW method could be as simple as creating a Merkle tree from the\n>> randomly generated point on the blockchain, though a method that is\n>> CPU/Memory heavy and less likely to be replaced by dedicated hardware like\n>> ASICs would be better. This process could not begin until the most recent\n>> block has been fully verified, and while being carried out should still\n>> enable normal relay activities to proceed as normal, since it shouldn\u2019t tie\n>> up network at all. The data processed should also be mixed with data from\n>> the latest block so that it cannot be computed in advance.\n>>\n>> A node can do as much PoNW for a block as it likes. Once finished it will\n>> then create a nodeWorkComplete transaction for that block with its final\n>> proof value, add how much \u2018work\u2019 it did - and create a couple of assertions\n>> about what it processed (such as there were x number of pieces of data\n>> matching a particular value during calculating). These assertions can be\n>> accurate or inaccurate.\n>>\n>> The system will run in epochs. During each epoch of say 2016 blocks,\n>> there will be an extended window for PoNW transactions to be added to\n>> nodeblocks to limit minor censorship.\n>>\n>> The random hash generated from a node\u2019s address and blockhash will also\n>> be used to determine nodeWorkComplete transactions from a previous block\n>> that the node must also verify, and correctly calculate whether the\n>> assertions it made were true or false. The average PoNW that a node\n>> performed in its previous x nodeblocks will be used to determine the target\n>> PoNW for the node to verify - and this will randomly be a large number of\n>> smaller PoNW transactions, or a smaller number of large PoNW. This process\n>> will be deterministic based on that block and address hash. All the data\n>> will be put together in a transaction and then signed by the node addresses\n>> private key.\n>>\n>> If a nodeWorkComplete transaction contains any incorrect information in\n>> an attempt to cheat the validation process a challenge transaction can be\n>> created. This begins a refereeing process where other nodes check the\n>> challenge and vote whether it is to be upheld or not. The losing node is\n>> punished by losing their accrued PoNW for that epoch and a percentage of\n>> their security deposit.\n>>\n>> Nodes will also be punished if they broadcast more than one signed\n>> transaction per block.\n>>\n>> In order to prevent nodes from having multiple keys registered - which\n>> would enable them choose to perform PoNW on a subset of the data that they\n>> hold - the share of reward that the node gets will be multiplied based on\n>> the number of blocks within an epoch that the node performs PoNW on. The\n>> share of reward is limited based on how much security deposit has been\n>> staked. The higher the PoNW the higher the deposit needed in order to claim\n>> their full allocation of any reward.\n>>\n>> At the end of an epoch, with a wait period for any delayed or censored\n>> transactions or challenges to be included and settled up, the process of\n>> calculating the reward each node is due can begin. This will then be then\n>> paid in a regular block, and means for all the data involved in PoNW, the\n>> only permanent mark it makes on the main blockchain is for a transaction\n>> that pays all addresses their share of the reward at the end of epoch. Any\n>> miner who creates a block without correctly calculating and paying the due\n>> reward will have mined an invalid block and be orphaned.\n>>\n>> The question of where and how much the reward comes from is a different\n>> one. It could come from the existing miner reward, or a special new tx\n>> donation fee for nodes. If there was some way for users to \u2018donate\u2019 to the\n>> reward pool for nodes this would increase the incentive for additional\n>> nodes to participate on the network in the event of centralisation.\n>>\n>> This is a relatively effective way to create a reward for all nodes\n>> participating on a network. I\u2019d be keen to field any questions or critiques.\n>>\n>> Thanks,\n>>\n>>\n>> John Hardy\n>>\n>> john at seebitcoin.com\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/2d644e4a/attachment-0001.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-02-14T18:44:26",
                "message_text_only": "I started writing this\nhttps://gist.github.com/Ayms/aab6f8e08fef0792ab3448f542a826bf some time\nago, but stopped since I was under the impression that this was of very\nlittle interest for the Bitcoin community\n\nIt's not final and finished at all, but since I wrote it and don't have\nplans right now to pursue it, I placed it in a gist and publish the\nlink, probably not everything is correct and this does not cover\neverything but it can maybe give some ideas (which are for some the\ncombination of concepts from former/other projects) that could be\nreused, addressing:\n\n- incentive to run full nodes\n\n- make sure that they are indeed full nodes\n\n- make sure that they participate to the network and are efficient enough\n\n- make sure that they don't collude in pools to get the rewards and are\nindependent\n\n- set up quickly a full node (incremental torrent-like download)\n\nAs this was written this was supposed to add some modifications to the\nbitcoin protocol but I don't think that's necessarily a good idea, most\nlikely this can be handled via sidechains and/or external systems\n\n\nLe 13/02/2017 \u00e0 15:48, Sergio Demian Lerner via bitcoin-dev a \u00e9crit :\n>\n>\n> On Mon, Feb 13, 2017 at 8:58 AM, John Hardy <john at seebitcoin.com\n> <mailto:john at seebitcoin.com>> wrote:\n>\n>     Hi Sergio,\n>\n>\n>     Thanks for your response, interesting work, very excited for RSK.\n>\n>\n>     I like the ephemeral payload, I suppose that aspect of my proposal\n>     could be described as ephemeral blockspace.\n>\n>\n>     I'm curious about the challenge phase, what incentive do nodes to\n>     have to check other nodes' responses?\n>\n> The reward is split between all full nodes. Therefore each full node\n> has an incentive to check at least some other full nodes responses\n> because there is a competition for the full node reward. At the end,\n> each full node response will be checked by more than other node with\n> high probability. Also each full node does a small pre-deposit, that\n> is consumed if the node cheats.\n>\n>     Is any validation of responses mandatory, or does policing the\n>     system rely on altruism?\n>\n>\n> As previously said,  validation is not mandatory.\n>\n>     I also wondered how time-based responses are enforced? What\n>     prevents a miner censoring challenge responses so they do not get\n>     included in a block 'in time' - if  inclusion within a block is\n>     the mechanism used?\n>\n> There is not many defenses against censorship but try to hide your\n> identity until the end of the protocol. But if somebody knows that\n> your transactions belong to you, then there is little defense. We just\n> wait more than a single block for the commitments, so several miners\n> must collude in order to censor a transaction. \n>\n>\n>     I saw your tweet on Lumino - sounds very promising. Would be keen\n>     to take a look at the paper if you're looking for any additional\n>     review at this stage.\n>\n> I'm keeping it private against all my desire because I want it to be\n> reviewed before I publish it. Credibility is very easily lost. \n> The same idea (Ephemeral Data) has been used to design the Lumino Network.\n>\n>\n>     Regards,\n>\n>\n>     John Hardy\n>\n>\n>\n>     ------------------------------------------------------------------------\n>     *From:* Sergio Demian Lerner <sergio.d.lerner at gmail.com\n>     <mailto:sergio.d.lerner at gmail.com>>\n>     *Sent:* Sunday, February 12, 2017 8:22 PM\n>     *To:* John Hardy; Bitcoin Protocol Discussion\n>     *Subject:* Re: [bitcoin-dev] Proof of Nodework (PoNW) - a method\n>     to trustlessly reward nodes for storing and verifying the blockchain\n>      \n>     Hi John,\n>      RSK platform (a Bitcoin sidechain) is already prepared to do\n>     something similar to this, although very efficiently. We set apart\n>     1% of the block reward to automatically reward full nodes.\n>\n>     We have two systems being evaluated: the first is based on PoUBS\n>     (Proof of Unique Blockchain Storage) which uses asymmetric-time\n>     operations to encode the blockchain based on each user public key\n>     such that decoding is fast, but encoding is slow. The second is\n>     more traditional proof of retrievability, but it requires some\n>     ASIC-resistance assumptions. \n>\n>     In both cases, a special smart contract is being called at every\n>     block that creates periodic challenges. Every full node that wants\n>     to participate can submits a commitment to the Merkle hash root of\n>     a pseudo-random sequence of encoded blocks. Then the smart\n>     contract chooses random elements from the committed dataset, and\n>     each full node has a period to submit Merkle-proofs that such\n>     random elements belong to the commitment.\n>\n>     To prevent blockchain bloat we designed a very cool new type of\n>     transaction payload: Ephemeral Payload. Ephemeral payload is a\n>     payload in a transaction that gets discarded after N blocks if no\n>     smart contract does reference it. If is does, it's solidified\n>     forever in the blockchain.\n>     Then there is a challenge phase where other full nodes can inform\n>     the smart contract if they find an error in the submitted\n>     responses. Then the smart contract ONLY evaluates the responses\n>     which have been questioned by users.\n>\n>     This way the smart contract does very little computation (only\n>     when a user misbehaves) and the blockchain normally does not store\n>     any proof forever (only the ones created by misbehaving users).\n>\n>     Because RSK/Rootstock has a very short block interval (10\n>     seconds), all this happens very quickly and does not require much\n>     computation. \n>\n>     Best regards,\n>      Sergio Lerner\n>      Chief Scientist RSK (aka Roostock)\n>\n>\n>     On Tue, Feb 7, 2017 at 8:27 AM, John Hardy via bitcoin-dev\n>     <bitcoin-dev at lists.linuxfoundation.org\n>     <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n>\n>         Proof of Nodework (PoNW) is a way to reward individual nodes\n>         for keeping a full copy of and verifying the blockchain.\n>\n>\n>         Hopefully they also do useful \u2018traditional\u2019 node activities\n>         too like relay transactions and blocks, but there isn\u2019t really\n>         any way I can think of to trustlessly verify this also.\n>\n>\n>         PoNW would require a new separate area of block space, a\n>         nodeblock, purely concerned with administering the system. A\n>         nodeblock is committed to a block as with SegWit. A recent\n>         history of nodeblocks needs to be stored by nodes, however the\n>         data eventually becomes obsolete and so does not need to be\n>         retained forever.\n>\n>\n>         In order to prevent Sybil, a node must register an Bitcoin\n>         address by submitting an addNode transaction - along with a\n>         security deposit to prevent cheating.\n>\n>\n>         This transaction will be stored in the nodeblock. Once a node\n>         can see that its addNode transaction has been added it can\n>         begin the PoNW process. The node\u2019s registered address will be\n>         hashed with the block header of the block it wants to work on.\n>         This will determine exactly where within the blockchain to\n>         begin the PoNW.\n>\n>\n>         The PoNW method could be as simple as creating a Merkle tree\n>         from the randomly generated point on the blockchain, though a\n>         method that is CPU/Memory heavy and less likely to be replaced\n>         by dedicated hardware like ASICs would be better. This process\n>         could not begin until the most recent block has been fully\n>         verified, and while being carried out should still enable\n>         normal relay activities to proceed as normal, since it\n>         shouldn\u2019t tie up network at all. The data processed should\n>         also be mixed with data from the latest block so that it\n>         cannot be computed in advance.\n>\n>\n>         A node can do as much PoNW for a block as it likes. Once\n>         finished it will then create a nodeWorkComplete transaction\n>         for that block with its final proof value, add how much \u2018work\u2019\n>         it did - and create a couple of assertions about what it\n>         processed (such as there were x number of pieces of data\n>         matching a particular value during calculating). These\n>         assertions can be accurate or inaccurate.\n>\n>\n>         The system will run in epochs. During each epoch of say 2016\n>         blocks, there will be an extended window for PoNW transactions\n>         to be added to nodeblocks to limit minor censorship.\n>\n>\n>         The random hash generated from a node\u2019s address and blockhash\n>         will also be used to determine nodeWorkComplete transactions\n>         from a previous block that the node must also verify, and\n>         correctly calculate whether the assertions it made were true\n>         or false. The average PoNW that a node performed in its\n>         previous x nodeblocks will be used to determine the target\n>         PoNW for the node to verify - and this will randomly be a\n>         large number of smaller PoNW transactions, or a smaller number\n>         of large PoNW. This process will be deterministic based on\n>         that block and address hash. All the data will be put together\n>         in a transaction and then signed by the node addresses private\n>         key.\n>\n>\n>         If a nodeWorkComplete transaction contains any incorrect\n>         information in an attempt to cheat the validation process a\n>         challenge transaction can be created. This begins a refereeing\n>         process where other nodes check the challenge and vote whether\n>         it is to be upheld or not. The losing node is punished by\n>         losing their accrued PoNW for that epoch and a percentage of\n>         their security deposit.\n>\n>\n>         Nodes will also be punished if they broadcast more than one\n>         signed transaction per block.\n>\n>\n>         In order to prevent nodes from having multiple keys registered\n>         - which would enable them choose to perform PoNW on a subset\n>         of the data that they hold - the share of reward that the node\n>         gets will be multiplied based on the number of blocks within\n>         an epoch that the node performs PoNW on. The share of reward\n>         is limited based on how much security deposit has been staked.\n>         The higher the PoNW the higher the deposit needed in order to\n>         claim their full allocation of any reward.\n>\n>\n>         At the end of an epoch, with a wait period for any delayed or\n>         censored transactions or challenges to be included and settled\n>         up, the process of calculating the reward each node is due can\n>         begin. This will then be then paid in a regular block, and\n>         means for all the data involved in PoNW, the only permanent\n>         mark it makes on the main blockchain is for a transaction that\n>         pays all addresses their share of the reward at the end of\n>         epoch. Any miner who creates a block without correctly\n>         calculating and paying the due reward will have mined an\n>         invalid block and be orphaned.\n>\n>\n>         The question of where and how much the reward comes from is a\n>         different one. It could come from the existing miner reward,\n>         or a special new tx donation fee for nodes. If there was some\n>         way for users to \u2018donate\u2019 to the reward pool for nodes this\n>         would increase the incentive for additional nodes to\n>         participate on the network in the event of centralisation.\n>\n>\n>         This is a relatively effective way to create a reward for all\n>         nodes participating on a network. I\u2019d be keen to field any\n>         questions or critiques.\n>\n>         Thanks,\n>\n>\n>         John Hardy\n>\n>         john at seebitcoin.com <mailto:john at seebitcoin.com>\n>\n>\n>         _______________________________________________\n>         bitcoin-dev mailing list\n>         bitcoin-dev at lists.linuxfoundation.org\n>         <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>         <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\n>\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170214/5d088c81/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Proof of Nodework (PoNW) - a method to trustlessly reward nodes for storing and verifying the blockchain",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Sergio Demian Lerner",
                "Aymeric Vitte",
                "John Hardy"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 44299
        }
    },
    {
        "title": "[bitcoin-dev] Proof-of-Loss -- Errata",
        "thread_messages": [
            {
                "author": "Mirelo",
                "date": "2017-02-08T01:34:29",
                "message_text_only": "There was an error on page 5 of the paper, which made the block-chaining odds formula confusing. The error was in the text, not in the formula, and consisted of assuming the affected route as always being the rewarded one, which is false. The corrected version is already available at the same URL ([https://proof-of-loss.money](https://proof-of-loss.money/)). The new file's date is 02/07/2017, and downloading it will probably require clearing the browser's cache.\n\nI would greatly appreciate any feedback, preferably directly to my email, to avoid overloading this list with something not directly related to Bitcoin.\n\nMirelo\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170207/a5c052d8/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Proof-of-Loss -- Errata",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Mirelo"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 816
        }
    },
    {
        "title": "[bitcoin-dev] BIP151 protocol incompatibility",
        "thread_messages": [
            {
                "author": "Eric Voskuil",
                "date": "2017-02-13T05:18:41",
                "message_text_only": "The BIP151 proposal states:\n\n> This proposal is backward compatible. Non-supporting peers will ignore\nthe encinit messages.\n\nThis statement is incorrect. Sending content that existing nodes do not\nexpect is clearly an incompatibility. An implementation that ignores\ninvalid content leaves itself wide open to DOS attacks. The version\nhandshake must be complete before the protocol level can be determined.\nWhile it may be desirable for this change to precede the version\nhandshake it cannot be described as backward compatible.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170212/6275a29b/attachment.sig>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2017-02-13T08:47:38",
                "message_text_only": "On Feb 12, 2017 23:58, \"Eric Voskuil via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nThe BIP151 proposal states:\n\n> This proposal is backward compatible. Non-supporting peers will ignore\nthe encinit messages.\n\nThis statement is incorrect. Sending content that existing nodes do not\nexpect is clearly an incompatibility. An implementation that ignores\ninvalid content leaves itself wide open to DOS attacks. The version\nhandshake must be complete before the protocol level can be determined.\nWhile it may be desirable for this change to precede the version\nhandshake it cannot be described as backward compatible.\n\n\nThe worst possible effect of ignoring unknown messages is a waste of\ndownstream bandwidth. The same is already possible by being sent addr\nmessages.\n\nUsing the protocol level requires a strict linear progression of (allowed)\nnetwork protocol features, which I expect to become harder and harder to\nmaintain.\n\nUsing otherwise ignored messages for determining optional features is\nelegant, simple and opens no new attack vectors. I think it's very much\npreferable over continued increments of the protocol version.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/78920df7/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-02-13T09:36:21",
                "message_text_only": "On 02/13/2017 12:47 AM, Pieter Wuille wrote:\n> On Feb 12, 2017 23:58, \"Eric Voskuil via bitcoin-dev\"\n> <bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n>     The BIP151 proposal states:\n> \n>     > This proposal is backward compatible. Non-supporting peers will ignore\n>     the encinit messages.\n> \n>     This statement is incorrect. Sending content that existing nodes do not\n>     expect is clearly an incompatibility. An implementation that ignores\n>     invalid content leaves itself wide open to DOS attacks. The version\n>     handshake must be complete before the protocol level can be determined.\n>     While it may be desirable for this change to precede the version\n>     handshake it cannot be described as backward compatible.\n> \n> The worst possible effect of ignoring unknown messages is a waste of\n> downstream bandwidth. The same is already possible by being sent addr\n> messages.\n> \n> Using the protocol level requires a strict linear progression of\n> (allowed) network protocol features, which I expect to become harder and\n> harder to maintain.\n> \n> Using otherwise ignored messages for determining optional features is\n> elegant, simple and opens no new attack vectors. I think it's very much\n> preferable over continued increments of the protocol version.\n\nAs I said, it *may* be desirable, but it is *not* backward compatible,\nand you do not actually dispute that above.\n\nThere are other control messages that qualify as \"optional messages\" but\nthese are only sent if the peer is at a version to expect them -\nexplicit in their BIPs. All adopted BIPs to date have followed this\npattern. This is not the same and it is not helpful to imply that it is\njust following that pattern.\n\nAs for DOS, waste of bandwidth is not something to be ignored. If a peer\nis flooding a node with addr message the node can manage it because it\nunderstands the semantics of addr messages. If a node is required to\nallow any message that it cannot understand it has no recourse. It\ncannot determine whether it is under attack or if the behavior is\ncorrect and for proper continued operation must be ignored.\n\nThis approach breaks any implementation that validates traffic, which is\nclearly correct behavior given the existence of the version handshake.\nYour comments make it clear that this is a *change* in network behavior\n- essentially abandoning the version handshake. Whether is is harder to\nmaintain is irrelevant to the question of whether it is a break with\nexisting protocol.\n\nIf you intend for the network to abandon the version handshake and/or\npromote changes that break it I propose that you write up this new\nbehavior as a BIP and solicit community feedback. There are a lot of\ndevices connected to the network and it would be irresponsible to break\nsomething as fundamental as the P2P protocol handshake because you have\na feeling it's going to be hard to maintain.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/21d7d014/attachment.sig>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2017-02-13T10:07:12",
                "message_text_only": "> All adopted BIPs to date have followed this\n> pattern. This is not the same and it is not helpful to imply that it is\n> just following that pattern.\n\nLook at feefilter BIP 133\n(https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki#backward-compatibility)\nor sendheaders BIP130\n(https://github.com/bitcoin/bips/blob/master/bip-0130.mediawiki#backward-compatibility)\nIsn't it the same there?\nOnce BIP151 is implemented, it would make sense to bump the protocol\nversion, but this needs to be done once this has been\nimplemented/deployed. Or do I make a mistake somewhere?\n>\n> As for DOS, waste of bandwidth is not something to be ignored. If a peer\n> is flooding a node with addr message the node can manage it because it\n> understands the semantics of addr messages. If a node is required to\n> allow any message that it cannot understand it has no recourse. It\n> cannot determine whether it is under attack or if the behavior is\n> correct and for proper continued operation must be ignored.\nHow do you threat any other not known message types? Any peer can send\nyou any type of message anytime. Why would your implementation how you\nthreat unknown messages be different for messages specified in BIP151?\n\n\n</jonas>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/310f938e/attachment.sig>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-02-13T10:30:14",
                "message_text_only": "On 02/13/2017 02:07 AM, Jonas Schnelli via bitcoin-dev wrote:\n>> All adopted BIPs to date have followed this\n>> pattern. This is not the same and it is not helpful to imply that it is\n>> just following that pattern.\n> \n> Look at feefilter BIP 133\n> (https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki#backward-compatibility)\n> or sendheaders BIP130\n> (https://github.com/bitcoin/bips/blob/master/bip-0130.mediawiki#backward-compatibility)\n> Isn't it the same there?\n\nNo. This is what I was referring to. These messages are enabled by\nprotocol version. If they are received by a node below the version at\nwhich they are activated, they are unknown messages, implying an invalid\npeer. The above messages cannot be sent until *after* the version is\nnegotiated. BIP151 violates this rule by allowing the new control\nmessage to be sent *before* the version handshake.\n\n> Once BIP151 is implemented, it would make sense to bump the protocol\n> version, but this needs to be done once this has been\n> implemented/deployed.\n\nThere are already nodes out there breaking connections based on the BIP.\n\n> Or do I make a mistake somewhere?\n\nYes, the ordering of the messages. New messages can only be added after\nthe handshake negotiates the higher version. Otherwise the handshake is\nboth irrelevant (as Pieter is implying) and broken (for all existing\nprotocol versions).\n\n>> As for DOS, waste of bandwidth is not something to be ignored. If a peer\n>> is flooding a node with addr message the node can manage it because it\n>> understands the semantics of addr messages. If a node is required to\n>> allow any message that it cannot understand it has no recourse. It\n>> cannot determine whether it is under attack or if the behavior is\n>> correct and for proper continued operation must be ignored.\n\n> How do you threat any other not known message types?\n\nYou may be more familiar with non-validating peers. If a message type is\nnot known it is an invalid message and the peer is immediately dropped.\nWe started seeing early drops in handshakes with bcoin nodes because of\nthis issue.\n\n> Any peer can send you any type of message anytime.\n\nSure, a peer can do what it wants. It can send photos. But I'm not sure\nwhat makes you think it would be correct to maintain the connection when\nan *invalid* message is received.\n\n> Why would your implementation how you threat unknown messages be\ndifferent for messages specified in BIP151?\n\nBecause it properly validates the protocol.\n\nMore than that it supports a configurable protocol range. So by setting\nthe min protocol (below which the node won't connect) and the max\nprotocol (at which it desires to connect) we can observe the behavior of\nthe network at any protocol levels (currently between 31402 and 70013).\nThis is very helpful for a development stack as it allows one to easily\ntest against each protocol level that one wishes to support.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/7fd2b845/attachment.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-02-13T10:16:13",
                "message_text_only": "For the reasons Pieter listed, an explicit part of our version handshake and protocol negotiation is the exchange of otherwise-ignored messages to set up optional features.\n\nPeers that do not support this ignore such messages, just as if they had indicated they wouldn't support it, see, eg BIP 152's handshake. Not sure why you consider this backwards incompatible, as I would say it's pretty clearly allowing old nodes to communicate just fine.\n\nOn February 13, 2017 10:36:21 AM GMT+01:00, Eric Voskuil via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>On 02/13/2017 12:47 AM, Pieter Wuille wrote:\n>> On Feb 12, 2017 23:58, \"Eric Voskuil via bitcoin-dev\"\n>> <bitcoin-dev at lists.linuxfoundation.org wrote:\n>> \n>>     The BIP151 proposal states:\n>> \n>>     > This proposal is backward compatible. Non-supporting peers will\n>ignore\n>>     the encinit messages.\n>> \n>>     This statement is incorrect. Sending content that existing nodes\n>do not\n>>     expect is clearly an incompatibility. An implementation that\n>ignores\n>>     invalid content leaves itself wide open to DOS attacks. The\n>version\n>>     handshake must be complete before the protocol level can be\n>determined.\n>>     While it may be desirable for this change to precede the version\n>>     handshake it cannot be described as backward compatible.\n>> \n>> The worst possible effect of ignoring unknown messages is a waste of\n>> downstream bandwidth. The same is already possible by being sent addr\n>> messages.\n>> \n>> Using the protocol level requires a strict linear progression of\n>> (allowed) network protocol features, which I expect to become harder\n>and\n>> harder to maintain.\n>> \n>> Using otherwise ignored messages for determining optional features is\n>> elegant, simple and opens no new attack vectors. I think it's very\n>much\n>> preferable over continued increments of the protocol version.\n>\n>As I said, it *may* be desirable, but it is *not* backward compatible,\n>and you do not actually dispute that above.\n>\n>There are other control messages that qualify as \"optional messages\"\n>but\n>these are only sent if the peer is at a version to expect them -\n>explicit in their BIPs. All adopted BIPs to date have followed this\n>pattern. This is not the same and it is not helpful to imply that it is\n>just following that pattern.\n>\n>As for DOS, waste of bandwidth is not something to be ignored. If a\n>peer\n>is flooding a node with addr message the node can manage it because it\n>understands the semantics of addr messages. If a node is required to\n>allow any message that it cannot understand it has no recourse. It\n>cannot determine whether it is under attack or if the behavior is\n>correct and for proper continued operation must be ignored.\n>\n>This approach breaks any implementation that validates traffic, which\n>is\n>clearly correct behavior given the existence of the version handshake.\n>Your comments make it clear that this is a *change* in network behavior\n>- essentially abandoning the version handshake. Whether is is harder to\n>maintain is irrelevant to the question of whether it is a break with\n>existing protocol.\n>\n>If you intend for the network to abandon the version handshake and/or\n>promote changes that break it I propose that you write up this new\n>behavior as a BIP and solicit community feedback. There are a lot of\n>devices connected to the network and it would be irresponsible to break\n>something as fundamental as the P2P protocol handshake because you have\n>a feeling it's going to be hard to maintain.\n>\n>e"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-02-13T10:54:23",
                "message_text_only": "On 02/13/2017 02:16 AM, Matt Corallo wrote:\n> For the reasons Pieter listed, an explicit part of our version\nhandshake and protocol negotiation is the exchange of otherwise-ignored\nmessages to set up optional features.\n\nOnly if the peer is at the protocol level that allows the message:\n\ncompact blocks:\n\nhttps://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L217-L242\n\nfee filter:\n\nhttps://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L211-L216\n\nsend headers:\n\nhttps://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L204-L210\n\nfilters:\n\nhttps://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L170-L196\n\n> Peers that do not support this ignore such messages, just as if they\nhad indicated they wouldn't support it, see, eg BIP 152's handshake. Not\nsure why you consider this backwards incompatible, as I would say it's\npretty clearly allowing old nodes to communicate just fine.\n\nNo, it is not the same as BIP152. Control messages apart from BIP151 are\nnot sent until *after* the version is negotiated.\n\nI assume that BIP151 is different in this manner because it has a desire\nto negotiate encryption before any other communications, including version.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/db42ecc6/attachment.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-02-13T11:11:11",
                "message_text_only": "I believe many, if not all, of those messages are sent irrespective of version number.\n\nIn any case, I fail to see how adding any additional messages which are ignored by old peers amounts to a lack of backward compatibility.\n\nOn February 13, 2017 11:54:23 AM GMT+01:00, Eric Voskuil <eric at voskuil.org> wrote:\n>On 02/13/2017 02:16 AM, Matt Corallo wrote:\n>> For the reasons Pieter listed, an explicit part of our version\n>handshake and protocol negotiation is the exchange of otherwise-ignored\n>messages to set up optional features.\n>\n>Only if the peer is at the protocol level that allows the message:\n>\n>compact blocks:\n>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L217-L242\n>\n>fee filter:\n>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L211-L216\n>\n>send headers:\n>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L204-L210\n>\n>filters:\n>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L170-L196\n>\n>> Peers that do not support this ignore such messages, just as if they\n>had indicated they wouldn't support it, see, eg BIP 152's handshake.\n>Not\n>sure why you consider this backwards incompatible, as I would say it's\n>pretty clearly allowing old nodes to communicate just fine.\n>\n>No, it is not the same as BIP152. Control messages apart from BIP151\n>are\n>not sent until *after* the version is negotiated.\n>\n>I assume that BIP151 is different in this manner because it has a\n>desire\n>to negotiate encryption before any other communications, including\n>version.\n>\n>e"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-02-13T11:17:11",
                "message_text_only": "On 02/13/2017 03:11 AM, Matt Corallo wrote:\n> I believe many, if not all, of those messages are sent irrespective of version number.\n\nIn the interest of perfect clarity, see your code:\n\nhttps://github.com/bitcoin/bitcoin/blob/master/src/net_processing.cpp#L1372-L1403\n\nInside of the VERACK handler (i.e. after the handshake) there is a peer\nversion test before sending SENDCMPCT (and SENDHEADERS).\n\nI have no idea where the fee filter message is sent, if it is sent at\nall. But I have *never* seen any control messages arrive before the\nhandshake is complete.\n\n> In any case, I fail to see how adding any additional messages which\nare ignored by old peers amounts to a lack of backward compatibility.\n\nSee preceding messages in this thread, I think it's pretty clearly\nspelled out.\n\ne\n\n> On February 13, 2017 11:54:23 AM GMT+01:00, Eric Voskuil <eric at voskuil.org> wrote:\n>> On 02/13/2017 02:16 AM, Matt Corallo wrote:\n>>> For the reasons Pieter listed, an explicit part of our version\n>> handshake and protocol negotiation is the exchange of otherwise-ignored\n>> messages to set up optional features.\n>>\n>> Only if the peer is at the protocol level that allows the message:\n>>\n>> compact blocks:\n>>\n>> https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L217-L242\n>>\n>> fee filter:\n>>\n>> https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L211-L216\n>>\n>> send headers:\n>>\n>> https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L204-L210\n>>\n>> filters:\n>>\n>> https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L170-L196\n>>\n>>> Peers that do not support this ignore such messages, just as if they\n>> had indicated they wouldn't support it, see, eg BIP 152's handshake.\n>> Not\n>> sure why you consider this backwards incompatible, as I would say it's\n>> pretty clearly allowing old nodes to communicate just fine.\n>>\n>> No, it is not the same as BIP152. Control messages apart from BIP151\n>> are\n>> not sent until *after* the version is negotiated.\n>>\n>> I assume that BIP151 is different in this manner because it has a\n>> desire\n>> to negotiate encryption before any other communications, including\n>> version.\n>>\n>> e\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/58969eab/attachment.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-02-13T13:04:15",
                "message_text_only": "Sorry, I'm still missing it...\nSo your claim is that a) ignoring incoming messages of a type you do not recognize is bad, and thus b) we should be disconnecting/banning peers which send us messages we do not recognize (can you spell out why? Anyone is free to send your host address messages/transactions they are generating/etc/etc, we don't ban nodes for such messages, as that would be crazy - why should we ban a peer for sending us an extra 50 bytes which we ignore?), and thus c) this would be backwards incompatible with software which does not currently exist?\n\nUsually \"backwards incompatible\" refers to breaking existing software, not breaking theoretical software. Note that, last I heard, BIP 151 is still a draft, if such software actually exists we can discuss changing it, but there are real wins in sending these messages before VERSION.\n\nOn February 13, 2017 12:17:11 PM GMT+01:00, Eric Voskuil <eric at voskuil.org> wrote:\n>On 02/13/2017 03:11 AM, Matt Corallo wrote:\n>> I believe many, if not all, of those messages are sent irrespective\n>of version number.\n>\n>In the interest of perfect clarity, see your code:\n>\n>https://github.com/bitcoin/bitcoin/blob/master/src/net_processing.cpp#L1372-L1403\n>\n>Inside of the VERACK handler (i.e. after the handshake) there is a peer\n>version test before sending SENDCMPCT (and SENDHEADERS).\n>\n>I have no idea where the fee filter message is sent, if it is sent at\n>all. But I have *never* seen any control messages arrive before the\n>handshake is complete.\n>\n>> In any case, I fail to see how adding any additional messages which\n>are ignored by old peers amounts to a lack of backward compatibility.\n>\n>See preceding messages in this thread, I think it's pretty clearly\n>spelled out.\n>\n>e\n>\n>> On February 13, 2017 11:54:23 AM GMT+01:00, Eric Voskuil\n><eric at voskuil.org> wrote:\n>>> On 02/13/2017 02:16 AM, Matt Corallo wrote:\n>>>> For the reasons Pieter listed, an explicit part of our version\n>>> handshake and protocol negotiation is the exchange of\n>otherwise-ignored\n>>> messages to set up optional features.\n>>>\n>>> Only if the peer is at the protocol level that allows the message:\n>>>\n>>> compact blocks:\n>>>\n>>>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L217-L242\n>>>\n>>> fee filter:\n>>>\n>>>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L211-L216\n>>>\n>>> send headers:\n>>>\n>>>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L204-L210\n>>>\n>>> filters:\n>>>\n>>>\n>https://github.com/bitcoin/bitcoin/blob/master/src/protocol.h#L170-L196\n>>>\n>>>> Peers that do not support this ignore such messages, just as if\n>they\n>>> had indicated they wouldn't support it, see, eg BIP 152's handshake.\n>>> Not\n>>> sure why you consider this backwards incompatible, as I would say\n>it's\n>>> pretty clearly allowing old nodes to communicate just fine.\n>>>\n>>> No, it is not the same as BIP152. Control messages apart from BIP151\n>>> are\n>>> not sent until *after* the version is negotiated.\n>>>\n>>> I assume that BIP151 is different in this manner because it has a\n>>> desire\n>>> to negotiate encryption before any other communications, including\n>>> version.\n>>>\n>>> e"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2017-02-13T11:14:01",
                "message_text_only": ">> Look at feefilter BIP 133\n>> (https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki#backward-compatibility)\n>> or sendheaders BIP130\n>> (https://github.com/bitcoin/bips/blob/master/bip-0130.mediawiki#backward-compatibility)\n>> Isn't it the same there?\n> No. This is what I was referring to. These messages are enabled by\n> protocol version. If they are received by a node below the version at\n> which they are activated, they are unknown messages, implying an invalid\n> peer. The above messages cannot be sent until *after* the version is\n> negotiated. BIP151 violates this rule by allowing the new control\n> message to be sent *before* the version handshake.\nThis indeed is not ideal for compatibility checks, but increases security.\nI could not find a protocol specification that said communication must\nbe terminated when messages are transmitted before the version handshake\nhas been done. I mostly looked into Bitcoin-Cores implementation (which\nmeans also into BitcoinXT/UT, where this is allowed).\n\nAlso. BIP151 clearly says that the requesting peer needs to initiate the\nencryption (encinit).\nIn case of light clients not supporting BIP151 connecting to peers\nsupporting BIP151, there should never be transmission of new message\ntypes specified in BIP151.\n>\n>> Once BIP151 is implemented, it would make sense to bump the protocol\n>> version, but this needs to be done once this has been\n>> implemented/deployed.\n> There are already nodes out there breaking connections based on the BIP.\nIt could very likely be possible that the initial responding peer tries\nto initiate a encryption session which would mean that BIP151 was not\nimplemented correctly.\nCorrect me if I'm wrong please.\n>\n>> Or do I make a mistake somewhere?\n> Yes, the ordering of the messages. New messages can only be added after\n> the handshake negotiates the higher version. Otherwise the handshake is\n> both irrelevant (as Pieter is implying) and broken (for all existing\n> protocol versions).\nI could not find evidence of the protocol specification that would\nforbid (=terminate connection) such messages and I think allowing\nunknown-messages before the version handshake makes the protocol flexible.\n\nAre there any reasons we should drop peers if they send us unknown, but\ncorrectly formatted p2p packages (magic, checksum, etc.) before the\nversion handshake, ... but not drop them if we have received unknown\nmessages after the version handshake?\n\nI can't see that a such spec. would reduce the DOS attack vector?\n\n>\n>>> As for DOS, waste of bandwidth is not something to be ignored. If a peer\n>>> is flooding a node with addr message the node can manage it because it\n>>> understands the semantics of addr messages. If a node is required to\n>>> allow any message that it cannot understand it has no recourse. It\n>>> cannot determine whether it is under attack or if the behavior is\n>>> correct and for proper continued operation must be ignored.\n>> How do you threat any other not known message types?\n> You may be more familiar with non-validating peers. If a message type is\n> not known it is an invalid message and the peer is immediately dropped.\n> We started seeing early drops in handshakes with bcoin nodes because of\n> this issue.\nIf this had happened, it's very likely because the responding peer tried\nto initiate a encryption session which is against BIP151 specs.\n>\n>> Any peer can send you any type of message anytime.\n> Sure, a peer can do what it wants. It can send photos. But I'm not sure\n> what makes you think it would be correct to maintain the connection when\n> an *invalid* message is received.\nCheck:\nhttps://github.com/bitcoin/bitcoin/blob/a06ede9a138d0fb86b0de17c42b936d9fe6e2158/src/net_processing.cpp#L2595\nI think it was a wise implementation decision to allow unknown (not\ninvalid) messages.\nThis had allowed us to deploy stuff like compact blocks, feefilter, etc.\nwithout breaking backward compatibility.\nIMO, without a such flexibility, the deployment complexity would be\nirresponsible high without really solving the DOS problem.\n>\n>> Why would your implementation how you threat unknown messages be\n> different for messages specified in BIP151?\n>\n> Because it properly validates the protocol.\nFor feefilter or compact block or sendheaders?\nYou can't link a (unimplemented) specification (improvement process) to\na protocol version before deployment. Or can you?\nOnce it has been widely deployed, we should set a protocol minversion\nfor BIP151, right.\n\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/ec9da1fb/attachment-0001.sig>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-02-14T19:54:37",
                "message_text_only": "On 02/13/2017 03:14 AM, Jonas Schnelli wrote:\n>>> Look at feefilter BIP 133\n>>> (https://github.com/bitcoin/bips/blob/master/bip-0133.mediawiki#backward-compatibility)\n>>> or sendheaders BIP130\n>>> (https://github.com/bitcoin/bips/blob/master/bip-0130.mediawiki#backward-compatibility)\n>>> Isn't it the same there?\n>> No. This is what I was referring to. These messages are enabled by\n>> protocol version. If they are received by a node below the version at\n>> which they are activated, they are unknown messages, implying an invalid\n>> peer. The above messages cannot be sent until *after* the version is\n>> negotiated. BIP151 violates this rule by allowing the new control\n>> message to be sent *before* the version handshake.\n\n> This indeed is not ideal for compatibility checks, but increases security.\n\nThe issue I raised is that it is not backward compatible. It sounds like\nyou agree but consider it a fair trade. My suggesting was that the BIP\nbe updated to reflect the lack of compatibility.\n\n> I could not find a protocol specification that said communication must\n> be terminated when messages are transmitted before the version handshake\n> has been done.\n\nIt doesn't need to be specified, most of Bitcoin is unspecified. The\nversion handshake establishes the negotiated version. It is not possible\nto determine if a message is of the negotiated version before the\nversion is negotiated. All messages apart from this one have followed\nthat rule.\n\n> Also. BIP151 clearly says that the requesting peer needs to initiate the\n> encryption (encinit).\n\nAn incoming connection will be dropped due to invalid protocol and\npotentially banned depending on the implementation.\n\n> In case of light clients not supporting BIP151 connecting to peers\n> supporting BIP151, there should never be transmission of new message\n> types specified in BIP151.\n\nNot working with peers not supporting BIP151 is the compatibility issue.\nBut it sort of seems the intent in this case is to rely on that\nincompatibility (expecting connections to nonsupporting peers to fail as\nopposed to negotiating).\n\n>>> Once BIP151 is implemented, it would make sense to bump the protocol\n>>> version, but this needs to be done once this has been\n>>> implemented/deployed.\n>> There are already nodes out there breaking connections based on the BIP.\n\n> It could very likely be possible that the initial responding peer tries\n> to initiate a encryption session which would mean that BIP151 was not\n> implemented correctly.\n> Correct me if I'm wrong please.\n\nI did consider the possibility, but there's this:\n\n\"Encryption initialization must happen before sending any other messages\nto the responding peer (encinit message after a version message must be\nignored).\"\n\nhttps://github.com/bitcoin/bips/blob/master/bip-0151.mediawiki#specification\n\nThe BIP does not define \"responding\" and \"requesting\" peers, but:\n\n\"A peer that supports encryption must accept encryption requests from\nall peers... The responding peer accepts the encryption request by\nsending a encack message.\"\n\nThis implies the requesting peer is the peer that sends the message. You\nseem to be saying that the requesting peer is the one that initiated\nthe connection and the responding peer is the connection receiver. If\nthis is the case it should be more clearly documented. But in the\ncase I experienced the \"requester\" of an encrypted session was also\nthe \"receiver\" of the connection.\n\n>>> Or do I make a mistake somewhere?\n>> Yes, the ordering of the messages. New messages can only be added after\n>> the handshake negotiates the higher version. Otherwise the handshake is\n>> both irrelevant (as Pieter is implying) and broken (for all existing\n>> protocol versions).\n\n> I could not find evidence of the protocol specification that would\n> forbid (=terminate connection) such messages and I think allowing\n> unknown-messages before the version handshake makes the protocol flexible.\n\nFlexible is certainly one word for it. Another way to describe it is\ndirty. Allowing invalid messages in a protocol encourages protocol\nincompatibility. You end up with various implementations and eventually\nhave no way of knowing how they are impacted by changes. There could be\na range of peers inter-operating with the full network while running\ntheir own sub-protocols. Given the network is public and strong\nidentification of peers is undesirable, the invalid messages would\nreasonably just get sent to everyone. So over time, what is the\nprotocol? Due to certain \"flexibility\" it is already a hassle to\nproperly implement.\n\n> Are there any reasons we should drop peers if they send us unknown, but\n> correctly formatted p2p packages (magic, checksum, etc.) before the\n> version handshake, ... but not drop them if we have received unknown\n> messages after the version handshake?\n\nThere is no reason to treat invalid messages differently based on where\nthey occur in the communication. After the handshake the agreed version\nis known to both peers. As a result there is never a reason for an\ninvalid message to be sent. Therefore it is always proper to drop a peer\nthat sends an invalid message.\n\n> I can't see that a such spec. would reduce the DOS attack vector?\n\nThis was previously addressed (immediately below).\n\n>>>> As for DOS, waste of bandwidth is not something to be ignored. If a peer\n>>>> is flooding a node with addr message the node can manage it because it\n>>>> understands the semantics of addr messages. If a node is required to\n>>>> allow any message that it cannot understand it has no recourse. It\n>>>> cannot determine whether it is under attack or if the behavior is\n>>>> correct and for proper continued operation must be ignored.\n>>> How do you threat any other not known message types?\n>> You may be more familiar with non-validating peers. If a message type is\n>> not known it is an invalid message and the peer is immediately dropped.\n>> We started seeing early drops in handshakes with bcoin nodes because of\n>> this issue.\n\n> If this had happened, it's very likely because the responding peer tried\n> to initiate a encryption session which is against BIP151 specs.\n\nSee above.\n\n>>> Any peer can send you any type of message anytime.\n>> Sure, a peer can do what it wants. It can send photos. But I'm not sure\n>> what makes you think it would be correct to maintain the connection when\n>> an *invalid* message is received.\n\n> Check:\n> https://github.com/bitcoin/bitcoin/blob/a06ede9a138d0fb86b0de17c42b936d9fe6e2158/src/net_processing.cpp#L2595\n> I think it was a wise implementation decision to allow unknown (not\n> invalid) messages.\n> This had allowed us to deploy stuff like compact blocks, feefilter, etc.\n> without breaking backward compatibility.\n> IMO, without a such flexibility, the deployment complexity would be\n> irresponsible high without really solving the DOS problem.\n\nThis is a misinterpretation. The failure to validate did not enable\nanything except possibly some broken peers not getting dropped. None of\nthe protocol changes previously deployed require the older version peer\nto allow invalid messages. While it may appear otherwise, due to a\nparticular implementation, it is never necessary to send a message to a\npeer that the peer does not understand. The handshake gives each peer\nthe other peer's version. That obligates the newer peer to conform to\nthe older (or disconnect if the old is insufficient). That's the nature\nof backward compatibility.\n\n>>> Why would your implementation how you threat unknown messages be\n>> different for messages specified in BIP151?\n>>\n>> Because it properly validates the protocol.\n\n> For feefilter or compact block or sendheaders?\n\nYes, this is the purpose of version negotiation, which is why there are\nversion and verack messages. And this is also why, in the satoshi\nclient, two of the above messages are sent from the verack handler. The\nfeefilter message is sent dynamically but only if the peer's version\nallows it.\n\n> You can't link a (unimplemented) specification (improvement process) to\n> a protocol version before deployment. Or can you?\n\nI'm not sure I follow your question. The BIP should presumably declare a\nversion number if one is necessary.\n\n> Once it has been widely deployed, we should set a protocol minversion\n> for BIP151, right.\n\nIn general you should set a version before it's ever live on the\nnetwork. But if it precedes the protocol version negotiation the\nprotocol version number is moot.\n\nI've been asked to throttle the discussion in the interest of reducing\nlist volume. I think the issue is pretty clearly addressed at this\npoint, but feel free to follow up directly and/or via the libbitcoin\ndevelopment list (copied).\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170214/031fe7e7/attachment.sig>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2017-02-14T20:58:54",
                "message_text_only": ">> This indeed is not ideal for compatibility checks, but increases security.\n> The issue I raised is that it is not backward compatible. It sounds like\n> you agree but consider it a fair trade. My suggesting was that the BIP\n> be updated to reflect the lack of compatibility.\nIt's still backward compatible. All (?) SPV clients and full node\nimplementation would still work if BIP151 has been implemented.\nIsn't that backward compatibility?\n>> I could not find a protocol specification that said communication must\n>> be terminated when messages are transmitted before the version handshake\n>> has been done.\n> It doesn't need to be specified, most of Bitcoin is unspecified. The\n> version handshake establishes the negotiated version. It is not possible\n> to determine if a message is of the negotiated version before the\n> version is negotiated. All messages apart from this one have followed\n> that rule.\nYes. But encryption negotiation must be done before the version\nhandshake (security).\n>\n>> Also. BIP151 clearly says that the requesting peer needs to initiate the\n>> encryption (encinit).\n> An incoming connection will be dropped due to invalid protocol and\n> potentially banned depending on the implementation.\nThis is not true. If the connecting peer (assume the SPV client) will\nnot request encryption, the responding peer (the node) will not enforce\nor ask for encryption.\nThis is clearly written in the BIP.\n>> It could very likely be possible that the initial responding peer tries\n>> to initiate a encryption session which would mean that BIP151 was not\n>> implemented correctly.\n>> Correct me if I'm wrong please.\n> I did consider the possibility, but there's this:\n>\n> \"Encryption initialization must happen before sending any other messages\n> to the responding peer (encinit message after a version message must be\n> ignored).\"\n>\n> https://github.com/bitcoin/bips/blob/master/bip-0151.mediawiki#specification\n>\n> The BIP does not define \"responding\" and \"requesting\" peers, but:\n>\n> \"A peer that supports encryption must accept encryption requests from\n> all peers... The responding peer accepts the encryption request by\n> sending a encack message.\"\n>\n> This implies the requesting peer is the peer that sends the message. You\n> seem to be saying that the requesting peer is the one that initiated\n> the connection and the responding peer is the connection receiver. If\n> this is the case it should be more clearly documented. But in the\n> case I experienced the \"requester\" of an encrypted session was also\n> the \"receiver\" of the connection.\nI think the BIP makes this very clear. IMO you are trying to hide your\nstandpoint behind a wired interpretations of the BIP.\n\nFrom the BIP:\n\u00abTo request encrypted communication, the requesting peer generates an EC\nephemeral-session-keypair and sends an encinit message to the responding\npeer and waits for a encack message. The responding node must do the\nsame encinit/encack interaction for the opposite communication direction.\u00bb\n\nThis seems to be pretty clear to me. You can interpret the \"requesting\npeer\" and \"responding peer\" per message interaction. But then the whole\nBIP would make no sense.\n\nI'm happy if you can do a PR on the BIP that makes the wording better.\nThis would actually be a productive step.\n>\n>>>> Or do I make a mistake somewhere?\n>>> Yes, the ordering of the messages. New messages can only be added after\n>>> the handshake negotiates the higher version. Otherwise the handshake is\n>>> both irrelevant (as Pieter is implying) and broken (for all existing\n>>> protocol versions).\n>> I could not find evidence of the protocol specification that would\n>> forbid (=terminate connection) such messages and I think allowing\n>> unknown-messages before the version handshake makes the protocol flexible.\n> Flexible is certainly one word for it. Another way to describe it is\n> dirty. Allowing invalid messages in a protocol encourages protocol\n> incompatibility. You end up with various implementations and eventually\n> have no way of knowing how they are impacted by changes. There could be\n> a range of peers inter-operating with the full network while running\n> their own sub-protocols. Given the network is public and strong\n> identification of peers is undesirable, the invalid messages would\n> reasonably just get sent to everyone. So over time, what is the\n> protocol? Due to certain \"flexibility\" it is already a hassle to\n> properly implement.\nThen you would have to go after all BIPs deployed this way. This\nargument has nothing to do with BIP151 it questions the whole new\nprotocol features deployment.\nAgain, check this code part:\n\nhttps://github.com/bitcoin/bitcoin/blob/a06ede9a138d0fb86b0de17c42b936d9fe6e2158/src/net_processing.cpp#L2595\n\n\n>\n>> Are there any reasons we should drop peers if they send us unknown, but\n>> correctly formatted p2p packages (magic, checksum, etc.) before the\n>> version handshake, ... but not drop them if we have received unknown\n>> messages after the version handshake?\n> There is no reason to treat invalid messages differently based on where\n> they occur in the communication. After the handshake the agreed version\n> is known to both peers. As a result there is never a reason for an\n> invalid message to be sent. Therefore it is always proper to drop a peer\n> that sends an invalid message.\nThat's up to the implementation. But the current flexibility exists\nbecause we not drop.\nAgain, see above.\n>> I can't see that a such spec. would reduce the DOS attack vector?\n> This was previously addressed (immediately below).\nNo. I'd like to hear from you why the DOS attack vector would be larger\nif the encryption neg. would be after the version handshake.\n>\n>>>>> As for DOS, waste of bandwidth is not something to be ignored. If a peer\n>>>>> is flooding a node with addr message the node can manage it because it\n>>>>> understands the semantics of addr messages. If a node is required to\n>>>>> allow any message that it cannot understand it has no recourse. It\n>>>>> cannot determine whether it is under attack or if the behavior is\n>>>>> correct and for proper continued operation must be ignored.\n>>>> How do you threat any other not known message types?\n>>> You may be more familiar with non-validating peers. If a message type is\n>>> not known it is an invalid message and the peer is immediately dropped.\n>>> We started seeing early drops in handshakes with bcoin nodes because of\n>>> this issue.\n> Yes, this is the purpose of version negotiation, which is why there are\n> version and verack messages. And this is also why, in the satoshi\n> client, two of the above messages are sent from the verack handler. The\n> feefilter message is sent dynamically but only if the peer's version\n> allows it.\n\nAgain. Encryption \u2013 for the sake of security \u2013 must be the first\ninteraction.\nThis is exceptional for BIP151 and I'd like to hear the real downsides\nof doing that.\n>\n>> You can't link a (unimplemented) specification (improvement process) to\n>> a protocol version before deployment. Or can you?\n> I'm not sure I follow your question. The BIP should presumably declare a\n> version number if one is necessary.\nWhat? You want to define protocol version number in draft improvement\nspecifications?\nHow should that be possible?\nIt's like defining a new HTML version number if you propose/draft a new\nvideo streaming format.\n\n\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170214/0b660431/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "BIP151 protocol incompatibility",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jonas Schnelli",
                "Eric Voskuil",
                "Matt Corallo",
                "Pieter Wuille"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 43559
        }
    },
    {
        "title": "[bitcoin-dev] BIP - 'Block75' - New algorithm",
        "thread_messages": [
            {
                "author": "Hampus Sj\u00f6berg",
                "date": "2017-02-13T11:21:44",
                "message_text_only": "> It gives miners complete control over the limit. They can make blocks of\nany size (within the current limit), thus triggering the conditions by\nwhich your proposal would raise the limit further.\n\nThere might be a long term incentive to keep increasing the blocksize, to\nfurther centralize the network (and kick smaller miners out), but it comes\nwith the cost of losing out on transaction fees.\nMiners have always needed to plan for the short term, I see no rational\nscenario where miners would spam their blocks with their own transactions\n(or low fee transactions) to keep increasing the blocksize limit.\n\nHampus\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170213/f4a99243/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP - 'Block75' - New algorithm",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Hampus Sj\u00f6berg"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 798
        }
    },
    {
        "title": "[bitcoin-dev] BIP150/151 concerns and some comments",
        "thread_messages": [
            {
                "author": "Jonas Schnelli",
                "date": "2017-02-14T16:10:15",
                "message_text_only": "Hi\n\nRecently I read some concerns about BIP150/151 and its \u201eidentity system\u201c.\nI think we should take those concerns seriously and I wrote some\ncomments for some of the concerns I'm aware of. In my opinion, most of\nthese worries are unfounded.\n\n\nConcern 1: BIP150 introduces a identity system that could partition the\nnetwork\n\n###############################################################################\n\n- Users already filtering/authenticate peers by IP tables, \u201eaddnode\u201c\ncommand, peer banning in app-layer. Fast block relay is a good example\n(example: FIBRE).\n- BIP150 allows to switch from IP based authentication (which is\nobviously not ideal) to a secure form of authentication with pre-shared\nkeys (ECDH).\n- We can\u2019t stop peer operators to selectively manage peers and there are\nvalid reasons to do that\n\nConcern 2: But BIP150 makes it simpler and increase the risk of network\npartitioning\n\n####################################################################################\n\n- What is simpler, presharing a pubkey over a secure channel (PGP /\nSignal) and store in on both peers or calling a \u201eaddnode <ip>\u201c, or\n\u201eiptables-DROP <ip>\u201c?\n\nConcern 3: Identity is not something we want in Bitcoin\n\n#######################################################\n\n- BIP150 introduces an **optional** authentication over a EC pubkey. The\nEC pubkey can be changed. It\u2019s different per network interface. You only\nreveal it to peers that already have proven the know your identity.\n- IP addresses are also a form of identity and way more inflexible and\ndifferent to hide.\n\n\nConcern 4: But peers can fingerprint my node and ban me after BIP150 has\nbeen deployed\n\n######################################################################################\n\n- They can\u2019t fingerprint you over BIP150 messages, it does not reveal\nyour identity unless the responding peer has proven he knows your identity.\n\n\nConcern 5: BIP150/151 is not necessary, we have already Tor and STunnel,\netc.\n\n#############################################################################\n\n- Tor is an alternative, right. But do we want to depend on those\ntechnologies? Using tor for a single secure channel seems like using\na sledgehammer to crack a nut.\n\n- How many SPV users have encrypted channels to trusted nodes today? Is\nit accessible for the novice user? \n\n- Peer operators who depend on designated connections (with addnode),\nwhat security do they have today (IP address, really?)?\n\n- I think tor is great, it can be an alternative or an additional\nsecurity enhancement (encrypt twice). IMO the focus of Tor is not on\nsecuring single channels (it's rather onion routing / anonymity).\n\n \n\nConcern 6: BIP151 gives a false sense of security and has no MITM detection\n\n###########################################################################\n\n- BIP151 (pure encryption) has no MITM detection, correct.\n\n- Without BIP151 encryption, everyone can hook into the stream and read\nwhat\u2019s going on. With BIP151, an attacker needs to actively substitute\nephemeral keys in both direction. This attack is A) more complex to\nachieve and B) it\u2019s an active attack (no excuse of \u201eI just made some\nstatistics\u201c), C) it requires the attacker to accept the risk of being\ndetected.\n\n- C) is true because an optional authentication (can be BIP150 or\ndifferent) would reveal the attack.\n\n- Some attacks are worthless if you have to take the risk mentioned in C)\n\n \n\nConcern 7: But Bitcoin traffic is trustless, why the hell you want to\nencrypt it?\n\n#################################################################################\n\n- If you use one of the todays available SPV clients, you will reveal\nyour complete wallet content (\u201e~all your addresses\") to every network\nobserver between you and the node you have connected to. This means, if\nyou pay for a coffee (while being on the owners WIFI), the coffee owner\nand all the involved ISPs can correlate your wallet with your other\ninternet behavior. Same is true for your cellphone provider if you use\ncellular.\n\n- They still can, if you don\u2019t have a trusted node, by performing the\nattack that involves the risk mentioned in Concern 6.\n\n \n\nConcern 8: If you want to have a light client, you should use a\ndifferent channel to communicate with your full node then the p2p layer\n\n#######################################################################################################################################\n\n- From a design perspective, this could make sense\n\n- From an end user\u2019s perspective, this is undesirable (enabled different\nport, lack of a (RPC / ZMQ, etc.) standard, no fallback option if the\ntrusted node is down, hard to setup)\n\n- Using the p2p channel as the todays SPV do, seems very reasonable to\nme. Keep the users on the p2p layer! If we don\u2019t want the users on that\nchannel, we automatically form a different layer, the wallet-com wild-west.\n\n- Keeping the users on the p2p layer also allows future changes where\nthey can help the network in some ways.\n\n- Using the p2p layer for a trusted connection also allows to fallback\nanytime to non-trusted nodes (if your trusted node is no longer\nreachable). If your SPV peer needs to catch up a couple of hours while\nyour trusted peer was done, fine, download full blocks or change your\nbloom filters FP rate significant (or sacrifices your privacy in this case).\n\n\n\n</jonas>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170214/8559c0c1/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170214/8559c0c1/attachment-0001.sig>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-02-14T18:01:03",
                "message_text_only": "On Tuesday, 14 February 2017 17:10:15 CET Jonas Schnelli via bitcoin-dev \nwrote:\n> - If you use one of the todays available SPV clients, you will reveal\n> your complete wallet content (\u201e~all your addresses\") to every network\n> observer between you and the node you have connected to. This means, if\n> you pay for a coffee (while being on the owners WIFI), the coffee owner\n> and all the involved ISPs can correlate your wallet with your other\n> internet behavior. Same is true for your cellphone provider if you use\n> cellular.\n\nWhat about allowing trusted users connecting on a different connection. Much \nlike the RPC one.\nMake that one encrypted. Different usecase, different connection.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2017-02-14T21:01:51",
                "message_text_only": ">> - If you use one of the todays available SPV clients, you will reveal\n>> your complete wallet content (\u201e~all your addresses\") to every network\n>> observer between you and the node you have connected to. This means, if\n>> you pay for a coffee (while being on the owners WIFI), the coffee owner\n>> and all the involved ISPs can correlate your wallet with your other\n>> internet behavior. Same is true for your cellphone provider if you use\n>> cellular.\n> What about allowing trusted users connecting on a different connection. Much \n> like the RPC one.\n> Make that one encrypted. Different usecase, different connection.\n>\n- What protocol would you use? The same p2p protocol but different port\nand/or different process? Why?\n- If not the p2p protocol, how would you form a standard? Would it be\nworth doing a standard?\n- Could you fall back to the current SPV model against random untrusted\npeers if you additional channel is not available?\n- What are the downsides using current p2p network?\n- Would this also solve the security problem of creating designated\nchannels between peers (the \"addnode\" thing is based on IPs)?\n\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170214/1e198586/attachment-0001.sig>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-02-16T15:10:46",
                "message_text_only": "On Tuesday, 14 February 2017 22:01:51 CET Jonas Schnelli via bitcoin-dev \nwrote:\n> >> - If you use one of the todays available SPV clients, you will reveal\n> >> your complete wallet content (\u201e~all your addresses\") to every network\n> >> observer between you and the node you have connected to. This means, if\n> >> you pay for a coffee (while being on the owners WIFI), the coffee owner\n> >> and all the involved ISPs can correlate your wallet with your other\n> >> internet behavior. Same is true for your cellphone provider if you use\n> >> cellular.\n> > \n> > What about allowing trusted users connecting on a different connection.\n> > Much like the RPC one.\n> > Make that one encrypted. Different usecase, different connection.\n> \n> - What protocol would you use?\n\nThe RPC one. Which I think is JSON.\n\nYour usecase is essentially just calling sendRawTransaction. Don\u2019t \novercomplicate things.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            }
        ],
        "thread_summary": {
            "title": "BIP150/151 concerns and some comments",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tom Zander",
                "Jonas Schnelli"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 9021
        }
    },
    {
        "title": "[bitcoin-dev] Committed bloom filters for improved wallet performance and SPV security",
        "thread_messages": [
            {
                "author": "Chris Belcher",
                "date": "2017-02-17T00:28:59",
                "message_text_only": "I believe this proposal still suffers from one problem that bip37 did,\nalbiet by a much lesser extent. Combining the partial information from\nthe block downloads with the transaction subgraph information from the\nblockchain can in some cases still reveal which addresses belong to the\nwallet. Nonetheless this proposal still has many benefits and is well\nworth working on.\n\n==BIP37==\n\nAs a recap, probably the biggest and most problematic way that bip37 was\nbroken was by combining the partial wallet information from the bloom\nfilter with the transaction subgraph information from the blockchain\n\nSuppose a wallet synchronizes it's history, if it spent a coin from its\naddress A, it must also also add the change address B to the bloom\nfilter, which is connected to A directly on transaction graph.\n\nAs an example, consider five typical transactions that consume one input\neach and produce two outputs.\nA, B, C, D, E refer to transactions. A1, A2, etc refer to addresses\nwithin those transactions\n\n          -> C1\nA1 -> B2  -> C2\n   -> B2  -> D1\n          -> D2 -> E1\n                -> E2\n\nIf a bip37 bloom filter matches addresses A1, B2, D2, E1 then it can be\nseen that they form a \"peel chain\" [this terminology comes from\nhttps://cseweb.ucsd.edu/~smeiklejohn/files/imc13.pdf]\n\n          -> X\nA1 -> X   -> X\n   -> B2  -> X\n          -> D2 -> E1\n                -> X\n\nThe same five transactions with non-matching addresses replaced by X.\nThe peel chain is visible, it's clear that B2, D2, E1 are change\naddresses which belong to the same wallet as A1.\n\nFor a given false-positive rate fp and given length of peel chain C, the\nodds of a false positive peel chain happening by chance is fp^C which\nrapidly gets very small as the wallet makes more transactions (increases C).\n\nIf only one address was matched from the above group (for example B2)\nthen it likely to be a false positive by the fact that it doesn't make\nany transactions to another address that also matches the bloom filter.\nAnother possibility is that the address is a payment output that the\nwallet received but hasn't spent yet, but the wallet cant spend it\nwithout adding the change address to the bloom filter and thus revealing\nitself to the spy.\n\nI believe the committed bloom filter proposal is vulnerability to this\nsame kind of attack because it still leaks information about which\naddresses the wallet is interested in.\n\n==Committed Bloom Filter Maths==\n\nI'll try to analyze this now. I'll find the expectation value of the\nnumber of transaction subgraphs in those blocks that appear just by\nchance. If this expectation goes to zero, then the only transaction\nsubgraph left will be the real one that the wallet is actually\ninterested in. In that case it will be possible to spy on the wallet.\n\nAssuming outputs have the same probability of being spent in each time\ninterval (i.e. they are spent in a Poisson process) This is\napproximately true, see the graphs from\n[https://letstalkbitcoin.com/blog/post/rise-of-the-zombie-bitcoins].\nThis means we can assign\na single probability P that an output is spent in each block.\n\nAssume every transaction has one change address only and spending of\nunconfirmed change doesn't happen (its more efficient to use RBF to add\nadditional outputs anyway)\n\nNumber of transactions per block = Q (about 1800 today)\nNumber of outputs per block = Z = 2*Q (approximately)\nLength of peel chain = Number of transactions in wallet = C\nAverage time an output is unspent for = T (about 1 month, very roughly\nestimating from the above blog post)\nProbability an output being spent in any particular later block = P =\n10minutes/T\n\nAssume no false positive blocks\nSay wallet downloaded two blocks and they are ordered by block height\nThe expected number of tx subgraphs between them, E(#G)\nE(#G) = number of outputs created in block 1 that get spent in block 2\n      = Z*P\n\nSay the wallet downloaded three blocks\nExpected number of subgraphs going through them all\nE(#G) = number of outputs created in block 1 get spent in block 2, that\ncreate a change address which gets spent in block 3\n      = Z*P*P\n\nSay the wallet downloaded C blocks\nExpected number of tx subgraphs going through all the blocks by chance\nE(#G) = Z*P^C\nwhich gets small quickly as C goes up, because P < 1\n\nNow drop the assumption about no false positive blocks.\n\nLet the number of candidate blocks be D.\nThis is how many blocks the wallet scans, it's related to how far in the\npast the wallet's keys was created. At one extreme wallet was created at\ngenesis block and so D = ~450000, at other extreme created now so D = 0.\nNote that D = 0 must also imply C = 0\n\nExpected number of false positive blocks downloaded = F = fp*D\n\nIn all these situations the blocks are sorted by block height\n\nSuppose have C=2, F=1, and false one is in the middle.\nI want to find E(#G|CF), the expected number of transaction subgraphs\nthat appear just by chance, given C and F.\nE(#G|CF) = how many outputs which are created in block 1 get spent in\nblock 3\n         = Z*P\n\nSame situation, but false one at the start instead of middle.\nE(#G|CF) = how many outputs which are created in block 2 get spent in\nblock 3\n         = Z*P\n\nSame situation but false one could be anywhere, result in the sum of the\nprobability for any false block position\nE(#G|CF) = C(3, 1)*Z*P = 3*Z*P\n\nwhere C() is the number of order-independent ways of choosing 1 element\nout of a set of 3 elements, also known as the binomial coefficient\n\nNow suppose C=3 and F=1\nThe same argument leads to\nE(#G|CF) = C(4, 1)*Z*P^2 = 4*Z*P^2\n\n\nNow suppose C=3 and F=2, with fp blocks at the end\nE(#G|CF)\n= how many outputs are created in block 1, are spent in block 2 and\nchange address spent in block 3\n= Z*P^2\n\nSame situation but fp blocks can be anywhere, add up all the possible\ncombinations of them within the rest\nE(#G|CF) = C(5, 2)*Z*P^2 = 5*Z*P^2\n\nWith these same rules, its clear the general expression for any F and C\nE(#G|CF) = C(F + C, F)*Z*P^(C - 1)\n\nA more interesting value might be the time evolution of E(#G)\nLet B be the blocks in the blockchain since the wallet creation date, as you\nknow it increases at an average rate of one every ten minutes\n\nw = wallet transaction creation rate, expressed per-block\nC = w * B\nF = fp * B\n\nJ = average blocks between wallet transactions = 1440 (10 days)\nw = 1/J\n\nE(#G|B) = C((fp + w)*B, fp*B)*Z*P^(w*B - 1)\n\nThis goes to zero as B becomes big, although choosing very high values\nof fp makes it go to zero slower.\n\nThis is only approximate maths, in actuality you cannot take the number\nof false positive blocks to be fp*B, you have to sum over all blocks\nweighted by probability. And outputs might not be spent in an exact\nPoisson process so you cant just multiply by P each time. Plus if your\nfalse positive rate is very high then some of your false positive blocks\nwill actually contain your real transactions, this analysis\ndouble-counts them.\n\nUsing some reasonable values and plotting E(#G|B) against B can show how\nquickly it drops and therefore leaves only the true transaction subgraph.\n\n(note: in LibreOffice Math and Microsoft Excel the binomial coefficient\nfunction is COMBIN)\n\n==Notes==\n\n*) The expected number of transaction subgraphs that happen by chance\ngoes to zero eventually as the blockchain steps ahead. Unless the fp\nrate is very high (close to 1) and time between wallet transaction very\nlong, in which case the binomial coefficient term gets larger more\nquicker than the exponential decay P^B term gets smaller.\n*) fp rate doesn't help in most cases that much compared to the\nexponential drop-off from time ticking ahead requiring more downloading\nof blocks\n*) its good for privacy if bitcoin outputs are spent more frequently so\nP is higher, because that creates more transaction subgraphs in the\nanonymity set.\n*) its good for privacy if more outputs are made per block, although\nstill only linearly which is no match for the exponential reduction from\nthe P^B term.\n*) its good for privacy to make less of your own transactions (increase\nJ and reduce w), for low-activity users the privacy of committed bloom\nfilters can be actually pretty good, for high-activity users who use\nbitcoin's blockchain all the time it's not very good\n*) For the reasonable values I tried for a once-a-month user with fp=1%,\ntheir chance-transaction-subgraph-count drops below 1 in about eight months.\n*) Because of the exponential nature, E(#G) goes from \"billions of\nbillions\" to \"about 10\" fairly quickly.\n\n==Discussion of ways to mitigate this==\n\nOne way is to not use change outputs. This is unrealistic, doesn't match\npeople's behavior and money must be divisible.\n\nA better way to mitigate this is to not leak the information that all\nthose blocks are interesting to the same wallet. Don't download all\nblocks from the same archival node. If you download blocks from many\ndifferent nodes, it gives an incentive for surveillance startups to\ncreate lots of sybil nodes they control and can then correlate together\nblock downloads with the wallet IP address. Many such startups are\nalready doing this today to try to detect the origin IP address of\nbroadcasted transactions.\n\nAnother solution could be to download a few blocks from different nodes\nwith new tor circuits used. This would delink the wallet IP address from\nthe downloads and would help a lot. This has the issue that tor is\nslower (but still not as slow as downloading the entire blockchain)\n\nAnother way a wallet could be correlated with its block downloads is\ntiming correlations. At any one time only a certain number of peers\nwould be downloading blocks which narrows down which wallets are\ndownloading what. However even today Bitcoin Core downloads blocks in\nparallel from many nodes so there's probably quite a large anonymity set\nfor lightweight wallets using committed bloom filters. Plus timing\ncorrelation can be reduced simply by waiting longer. Wallets are not\nsync'd from backup very often so it might be okay to wait.\n\nAnother way to improve privacy could be for the wallet to choose random\ntransaction subgraphs and download all the blocks related to them as well.\n\nWallet developers might choose to allow the user to configure their own\nfp rate. This is probably not a good idea since the relationship between\nfp rate and anonymity set is non-obvious. It might be better to ask the\nuser how often they expect to make transactions.\n\n==Conclusion==\n\nI think this committed bloom filter idea is very good and much better\nthan bip37, but for good privacy for when bitcoin is used often still\nrequires certain behavior namely downloading blocks\nfrom many different peers with new tor circuits.\n\nNote that I've been dealing with counting transaction subgraphs but\nactually finding them from blocks might also be computationally\ninfeasible. Although a Bayesian approach worked very\nwell for similar transaction subgraph linking\n[https://arxiv.org/pdf/1612.06747v3.pdf]\n\nIt would also be interesting to analyze what information a spy can get\nif they are missing some blocks that the wallet downloaded.\n\nFor the long term, private and high-volume bitcoin use will be best\nserved by off-chain transactions. They will probably be a huge win just\nbecause the large and public blockchain is such a non-private\nway of doing things.\n\nOn 09/05/16 09:26, bfd--- via bitcoin-dev wrote:\n> We introduce several concepts that rework the lightweight Bitcoin\n> client model in a manner which is secure, efficient and privacy\n> compatible.\n> \n> Thea properties of BIP37 SPV [0] are unfortunately not as strong as\n> originally thought:\n> \n>     * The expected privacy of the probabilistic nature of bloom\n>       filters does not exist [1][2], any user with a BIP37 SPV wallet\n>       should be operating under no expectation of privacy.\n>       Implementation flaws make this effect significantly worse, the\n>       behavior meaning that no matter how high the false positive\n>       rate (up to simply downloading the whole blocks verbatim) the\n>       intent of the client connection is recoverable.\n> \n>     * Significant processing load is placed on nodes in the Bitcoin\n>       network by lightweight clients, a single syncing wallet causes\n>       (at the time of writing) 80GB of disk reads and a large amount\n>       of CPU time to be consumed processing this data. This carries\n>       significant denial of service risk [3], non-distinguishable\n>       clients can repeatedly request taxing blocks causing\n>       reprocessing on every request. Processed data is unique to every\n>       client, and can not be cached or made more efficient while\n>       staying within specification.\n> \n>     * Wallet clients can not have strong consistency or security\n>       expectations, BIP37 merkle paths allow for a wallet to validate\n>       that an output was spendable at some point in time but does not\n>       prove that this output is not spent today.\n> \n>     * Nodes in the network can denial of service attack all BIP37 SPV\n>       wallet clients by simply returning null filter results for\n>       requests, the wallet has no way of discerning if it has been\n>       lied to and may be made simply unaware that any payment has been\n>       made to them. Many nodes can be queried in a probabilistic manor\n>       but this increases the already heavy network load with little\n>       benefit.\n> \n> \n> \n> We propose a new concept which can work towards addressing these\n> shortcomings.\n> \n> \n> A Bloom Filter Digest is deterministically created of every block\n> encompassing the inputs and outputs of the containing transactions,\n> the filter parameters being tuned such that the filter is a small\n> portion of the size of the total block data. To determine if a block\n> has contents which may be interesting a second bloom filter of all\n> relevant key material is created. A binary comparison between the two\n> filters returns true if there is probably matching transactions, and\n> false if there is certainly no matching transactions. Any matched\n> blocks can be downloaded in full and processed for transactions which\n> may be relevant.\n> \n> The BFD can be used verbatim in replacement of BIP37, where the filter\n> can be cached between clients without needing to be recomputed. It can\n> also be used by normal pruned nodes to do re-scans locally of their\n> wallet without needing to have the block data available to scan, or\n> without reading the entire block chain from disk.\n> \n> -\n> \n> For improved probabilistic security the bloom filters can be presented\n> to lightweight clients by semi-trusted oracles. A client wallet makes\n> an assumption that they trust a set, or subset of remote parties\n> (wallet vendors, services) which all all sign the BFD for each block.\n> The BFD can be downloaded from a single remote source, and the hash of\n> the filters compared against others in the trust set. Agreement is a\n> weak suggestion that the filter has not been tampered with, assuming\n> that these parties are not conspiring to defraud the client.\n> \n> The oracles do not learn any additional information about the client\n> wallet, the client can download the block data from either nodes on\n> the network, HTTP services, NTTP, or any other out of band\n> communication method that provides the privacy desired by the client.\n> \n> -\n> \n> The security model of the oracle bloom filter can be vastly improved\n> by instead committing a hash of the BFD inside every block as a soft-\n> fork consensus rule change. After this, every node in the network would\n> build the filter and validate that the hash in the block is correct,\n> then make a conscious choice discard it for space savings or cache the\n> data to disk.\n> \n> With a commitment to the filter it becomes impossible to lie to\n> lightweight clients by omission. Lightweight clients are provided with\n> a block header, merkle path, and the BFD. Altering the BFD invalidates\n> the merkle proof, it's validity is a strong indicator that the client\n> has an unadulterated picture of the UTXO condition without needing to\n> build one itself. A strong assurance that the hash of the BFD means\n> that the filters can be downloaded out of band along with the block\n> data at the leisure of the client, allowing for significantly greater\n> privacy and taking load away from the P2P Bitcoin network.\n> \n> Committing the BFD is not a hard forking change, and does not require\n> alterations to mining software so long as the coinbase transaction\n> scriptSig is not included in the bloom filter.\n> \n> \n> [0] https://github.com/bitcoin/bips/blob/master/bip-0037.mediawiki\n> [1] https://eprint.iacr.org/2014/763.pdf\n> [2] https://jonasnick.github.io/blog/2015/02/12/privacy-in-bitcoinj/\n> [3] https://github.com/petertodd/bloom-io-attack\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            }
        ],
        "thread_summary": {
            "title": "Committed bloom filters for improved wallet performance and SPV security",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Belcher"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 16906
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Knots 0.14.0 release candidate 1 available",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2017-02-19T12:12:55",
                "message_text_only": "Release candidate 1 of a new major Bitcoin Knots release, version 0.14.0, has\nbeen made available.\n\nThis is a release candidate for a new major version release, including new\nfeatures, various bugfixes and performance improvements.\n\nPreliminary release notes for the release can be found here:\n\n    https://github.com/bitcoinknots/bitcoin/blob/v0.14.0.knots20170218.rc1/doc/release-notes.md\n\nBinaries can be downloaded from:\n\n    http://bitcoinknots.org/files/0.14.x/0.14.0.knots20170218.rc1/\n\nPlease take care to verify the PGP signature of all downloads.\n\nSource code can be found on GitHub under the signed tag\n\n    https://github.com/bitcoinknots/bitcoin/tree/v0.14.0.knots20170218.rc1\n\nRelease candidates are test versions for releases. When no critical problems\nare found, this release candidate will be tagged as 0.14.0 final, otherwise\na new rc will be made available after these are solved.\n\nPlease report bugs using the issue tracker at GitHub:\n\n    https://github.com/bitcoinknots/bitcoin/issues\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 1528 bytes\nDesc: This is a digitally signed message part.\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170219/4d5b3d03/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Knots 0.14.0 release candidate 1 available",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1307
        }
    },
    {
        "title": "[bitcoin-dev] replace-by-fee-v0.14.0rc1 available",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2017-02-19T21:30:01",
                "message_text_only": "My full-RBF patched branch of Bitcoin Core v0.14.0rc1 is now available:\nhttps://github.com/petertodd/bitcoin/tree/replace-by-fee-v0.14.0rc1\n\nAs with replace-by-fee-v0.13.2, this version uses the nRelevantServices\nmachinery to do preferential peering, so it's just a few lines of code changed\nbetween it and Bitcoin Core v0.14.0rc1.\n\nThe relevant services machinery is less agressive at connecting to full-RBF\npeers than the earlier custom code previous versions used. But it seems to work\nwell enough to keep RBF peers connected to each other, so I'm inclined to keep\nusing it as doing so makes maintaining this patched branch pretty trivial every\ntime a new upstream version is released.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170219/3336b9b3/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "replace-by-fee-v0.14.0rc1 available",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Peter Todd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1019
        }
    },
    {
        "title": "[bitcoin-dev] Proposal for utxo commitment format",
        "thread_messages": [
            {
                "author": "Bram Cohen",
                "date": "2017-02-21T22:00:23",
                "message_text_only": "Here is a Merkle set data structure, whose format may be useful for utxo\ncommitments in Bitcoin blocks. It may also be useful for any other\ndistributed computation which wants an audit trail:\n\nhttps://github.com/bramcohen/MerkleSet\n\nThis is a fairly straightforward Patricia Trie, with a simple format and a\nsimple reference implementation plus a performance optimized non-reference\nimplementation which is much more cache coherent. It will need to be ported\nto C and be properly turned before the potential performance gains can be\nrealized though.\n\nThe clever things which affect the format spec are:\n\nIt uses blake2s as the internal hash function. This is the fastest hash\nfunction to use on 512 bit inputs because blake2b uses a 1024 bit block\nsize. It might make sense to use a hypothetical variant of blake which is\noptimized for 64 bits with a 512 bit block size, but that hasn't been\nspecified. Sha256 would take advantage of hardware acceleration, but that\nisn't available everywhere.\n\nTwo bits of security are sacrificed to include metadata inline which halves\nthe CPU cost of hashing.\n\nWhen one side of a node is empty and the other contains exactly two things\nthe secure hash of the child is adopted verbatim rather than rehashing it.\nThis roughly halves the amount of hashing done, and makes it more resistant\nto malicious data, and cleans up some implementation details, at the cost\nof some extra complexity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170221/a52234ed/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal for utxo commitment format",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bram Cohen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1605
        }
    },
    {
        "title": "[bitcoin-dev] Generalized Commitments",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2017-02-23T01:26:11",
                "message_text_only": "On Tue, Feb 21, 2017 at 02:00:23PM -0800, Bram Cohen via bitcoin-dev wrote:\n> When one side of a node is empty and the other contains exactly two things\n> the secure hash of the child is adopted verbatim rather than rehashing it.\n> This roughly halves the amount of hashing done, and makes it more resistant\n> to malicious data, and cleans up some implementation details, at the cost\n> of some extra complexity.\n\nNote that this is a use-case specific concept of an idea I'm calling a\n\"generalized commitment\"\n\nA commitment scheme needs only have the property that it's not feasible to find\ntwo messages m1 and m2 that map to the same commitment; it is *not* required\nthat it be difficult to find m given the commitment. Equally, it's not required\nthat commitments always be the same size.\n\nSo a perfectly reasonable thing to do is design your scheme such that the\ncommitment to short messages is the message itself! This adds just a single bit\nof data to the minimum serialized size(1) of the commitment, and in situations\nwhere sub-digest-sized messages are common, may overall be a savings.\n\n\nAnother advantage is that the scheme becomes more user-friendly: you *want*\nprogrammers to notice when a commitment is not effectively hiding the message!\nIf you need message privacy, you should implement an explicit nonce, rather\nthan relying on the data to not be brute-forcable.\n\n\n1) The more I look at these systems, the more I'm inclined to consider\nbit-granularity serialization schemes... Heck, sub-bit granularity has\nadvantages too in some cases, e.g. by making all possible inputs to the\ndeserializer be valid.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170222/c6df5d46/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-23T02:56:35",
                "message_text_only": "On Wed, Feb 22, 2017 at 5:26 PM, Peter Todd <pete at petertodd.org> wrote:\n\n>\n> A commitment scheme needs only have the property that it's not feasible to\n> find\n> two messages m1 and m2 that map to the same commitment; it is *not*\n> required\n> that it be difficult to find m given the commitment. Equally, it's not\n> required\n> that commitments always be the same size.\n\n\n> So a perfectly reasonable thing to do is design your scheme such that the\n> commitment to short messages is the message itself! This adds just a\n> single bit\n> of data to the minimum serialized size(1) of the commitment, and in\n> situations\n> where sub-digest-sized messages are common, may overall be a savings.\n>\n\nYes I'm basically doing that but to make things be all the same size I'm\nincluding the bit inline, sacrificing one bit of security. Actually I'm\nsacrificing two bits of security, to allow for four values: terminal,\nmiddle, empty, and invalid. Invalid is used internally when a value has yet\nto be calculated lazily and in proofs to mean 'this is a middle node but\nthe children are not included'. One effect of this is that the root of a\nset containing a single value is just that value with the two high order\nbits of the first byte reset to the appropriate value.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170222/10b8cd50/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Generalized Commitments",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bram Cohen",
                "Peter Todd"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3384
        }
    },
    {
        "title": "[bitcoin-dev] TXO commitments do not need a soft-fork to be useful",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2017-02-23T01:11:47",
                "message_text_only": "Something I've recently realised is that TXO commitments do not need to be\nimplemented as a consensus protocol change to be useful. All the benefits they\nprovide to full nodes with regard to allowing for old UTXO data to be pruned -\nand thus solving the UTXO bloat problem - can be implemented even without\nhaving miners commit to the TXO commitment itself. This has a significant\ndeployment advantage too: we can try out multiple TXO commitment schemes, in\nproduction, without the need for consensus changes.\n\n\n# Reasoning\n\n1) Like any other merkelized data structure, a TXO commitment allows a data set\n- the TXO set - to be securely provided by an untrusted third party, allowing\nthe data itself to be discarded. So if you have a valid TXO commitment, you can\ndiscard the TXO data itself, and rely on untrusted entities to provide you that\ndata on demand.\n\n2) The TXO set is a super-set of the UTXO set; all data in the UTXO set is also\npresent in the TXO set. Thus a TXO commitment with spent TXO's pruned is\nequivalent to a UTXO set, doubly so if inner nodes in the commitment tree\ncommit to the sum-unspent of their children.\n\n3) Where a outpoint-indexed UTXO set has a uniform access pattern, an\ninsertion-ordered TXO set has a delibrately *non-uniform* access pattern: not\nonly are new entries to the TXO set always appended to the end - an operation\nthat requires a known, log2(n), sized set of merkle tips - but due to lost\ncoins alone we can guarantee that older entries in the TXO set will be less\nfrequently updated than newer entries.\n\n4) Thus a full node that doesn't have enough local storage to maintain the full\nUTXO set can instead keep track of a TXO commitment, and prune older UTXO's\nfrom it that are unlikely to be spent. In the event those UTXO's are spent,\ntransactions and blocks spending them can trustlessly provide the necessary\ndata to temporarily fill-in the node's local TXO set database, allowing the\nnext commitment to be calculated.\n\n5) By *not* committing the TXO commitment in the block itself, we obsolete my\nconcept of delayed TXO commitments: you don't need to have calculated the TXO\ncommitment digest to validate a block anyway!\n\n\n# Deployment Plan\n\n1) Implement a TXO commitment scheme with the ability to efficiently store the\nlast n versions of the commitment state for the purpose of reorgs (a\nreference-counted scheme naturally does this).\n\n2) Add P2P support for advertising to peers what parts of the TXO set you've\npruned.\n\n3) Add P2P support to produce, consume, and update TXO unspentness proofs as\npart of transaction and block relaying.\n\n4) Profit.\n\n\n# Bootstrapping New Nodes\n\nWith a TXO commitment scheme implemented, it's also possible to produce\nserialized UTXO snapshots for bootstrapping new nodes. Equally, it's obviously\npossible to distribute those snapshots, and have people you trust attest to the\nvalidity of those snapshots.\n\nI argue that a snapshot with an attestation from known individuals that you\ntrust is a *better* security model than having miners attest to validity: the\nlatter is trusting an unknown set of unaccountable, anonymous, miners.\n\nThis security model is not unlike the recently implemented -assumevalid\nscheme(1), in that auditing the validity of the assumed valid TXO commitments\nis something anyone can do provided they have a full node. Similarly, we could\nship Bitcoin nodes with an assumed-valid TXO commitment, and have those nodes\nfill in the UTXO data from their peers.\n\nHowever it is a weaker security model, in that a false TXO commitment can more\neasily be used to trick a node into accepting invalid transactions/blocks;\nassumed valid blocks requires proof-of-work to pull off this attack. A\ncompromise may be to use assumed valid TXO commitments, extending my partial\nUTXO set(2) suggestion of having nodes validate the chain backwards, to\neventually validate 100% of the chain.\n\n\n# References\n\n1) https://github.com/bitcoin/bitcoin/pull/9484\n2) [Bitcoin-development] SPV bitcoind? (was: Introducing BitcoinKit.framework),\n   Peter Todd, Jul 17th 2013, Bitcoin development mailing list,\n   https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2013-July/002917.html\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170222/70a0a773/attachment.sig>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2017-02-23T03:30:37",
                "message_text_only": "This kind of thing is long overdue!\n\nI think it\u2019s a great idea to attempt this without soft forking TXO\ncommitments yet so we can see what works best.\n\n\n- E\n\nOn Wed, Feb 22, 2017 at 5:11 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Something I've recently realised is that TXO commitments do not need to be\n> implemented as a consensus protocol change to be useful. All the benefits\n> they\n> provide to full nodes with regard to allowing for old UTXO data to be\n> pruned -\n> and thus solving the UTXO bloat problem - can be implemented even without\n> having miners commit to the TXO commitment itself. This has a significant\n> deployment advantage too: we can try out multiple TXO commitment schemes,\n> in\n> production, without the need for consensus changes.\n>\n>\n> # Reasoning\n>\n> 1) Like any other merkelized data structure, a TXO commitment allows a\n> data set\n> - the TXO set - to be securely provided by an untrusted third party,\n> allowing\n> the data itself to be discarded. So if you have a valid TXO commitment,\n> you can\n> discard the TXO data itself, and rely on untrusted entities to provide you\n> that\n> data on demand.\n>\n> 2) The TXO set is a super-set of the UTXO set; all data in the UTXO set is\n> also\n> present in the TXO set. Thus a TXO commitment with spent TXO's pruned is\n> equivalent to a UTXO set, doubly so if inner nodes in the commitment tree\n> commit to the sum-unspent of their children.\n>\n> 3) Where a outpoint-indexed UTXO set has a uniform access pattern, an\n> insertion-ordered TXO set has a delibrately *non-uniform* access pattern:\n> not\n> only are new entries to the TXO set always appended to the end - an\n> operation\n> that requires a known, log2(n), sized set of merkle tips - but due to lost\n> coins alone we can guarantee that older entries in the TXO set will be less\n> frequently updated than newer entries.\n>\n> 4) Thus a full node that doesn't have enough local storage to maintain the\n> full\n> UTXO set can instead keep track of a TXO commitment, and prune older UTXO's\n> from it that are unlikely to be spent. In the event those UTXO's are spent,\n> transactions and blocks spending them can trustlessly provide the necessary\n> data to temporarily fill-in the node's local TXO set database, allowing the\n> next commitment to be calculated.\n>\n> 5) By *not* committing the TXO commitment in the block itself, we obsolete\n> my\n> concept of delayed TXO commitments: you don't need to have calculated the\n> TXO\n> commitment digest to validate a block anyway!\n>\n>\n> # Deployment Plan\n>\n> 1) Implement a TXO commitment scheme with the ability to efficiently store\n> the\n> last n versions of the commitment state for the purpose of reorgs (a\n> reference-counted scheme naturally does this).\n>\n> 2) Add P2P support for advertising to peers what parts of the TXO set\n> you've\n> pruned.\n>\n> 3) Add P2P support to produce, consume, and update TXO unspentness proofs\n> as\n> part of transaction and block relaying.\n>\n> 4) Profit.\n>\n>\n> # Bootstrapping New Nodes\n>\n> With a TXO commitment scheme implemented, it's also possible to produce\n> serialized UTXO snapshots for bootstrapping new nodes. Equally, it's\n> obviously\n> possible to distribute those snapshots, and have people you trust attest\n> to the\n> validity of those snapshots.\n>\n> I argue that a snapshot with an attestation from known individuals that you\n> trust is a *better* security model than having miners attest to validity:\n> the\n> latter is trusting an unknown set of unaccountable, anonymous, miners.\n>\n> This security model is not unlike the recently implemented -assumevalid\n> scheme(1), in that auditing the validity of the assumed valid TXO\n> commitments\n> is something anyone can do provided they have a full node. Similarly, we\n> could\n> ship Bitcoin nodes with an assumed-valid TXO commitment, and have those\n> nodes\n> fill in the UTXO data from their peers.\n>\n> However it is a weaker security model, in that a false TXO commitment can\n> more\n> easily be used to trick a node into accepting invalid transactions/blocks;\n> assumed valid blocks requires proof-of-work to pull off this attack. A\n> compromise may be to use assumed valid TXO commitments, extending my\n> partial\n> UTXO set(2) suggestion of having nodes validate the chain backwards, to\n> eventually validate 100% of the chain.\n>\n>\n> # References\n>\n> 1) https://github.com/bitcoin/bitcoin/pull/9484\n> 2) [Bitcoin-development] SPV bitcoind? (was: Introducing\n> BitcoinKit.framework),\n>    Peter Todd, Jul 17th 2013, Bitcoin development mailing list,\n>    https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2013-July/002917.html\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170222/cabff956/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-23T07:23:10",
                "message_text_only": "On Wed, Feb 22, 2017 at 08:11:47PM -0500, Peter Todd via bitcoin-dev wrote:\n> 5) By *not* committing the TXO commitment in the block itself, we obsolete my\n> concept of delayed TXO commitments: you don't need to have calculated the TXO\n> commitment digest to validate a block anyway!\n\nThinking about this a bit more, by not being forced to calculate a TXO\ncommitment for every block, we may be able to do significantly better than\ndelayed TXO commitments by lazily hashing.\n\nSuppose we have the following perfect merkle tree, which we're using as a\nkey-value map. We'll represent inner nodes for which we've calculated digests\nwith \"0\"'s to represent what version of the tree they correspond too:\n\n               0\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           0\n        / \\         / \\\n       /   \\       /   \\\n      0     0     0     0\n     / \\   / \\   / \\   / \\\n    a   b c   d e   f g   h\n\nIf a value is updated, digests above it become out of date and need to be\nrecalculated:\n\n\n               1\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           1\n        / \\         / \\\n       /   \\       /   \\\n      0     0     0     1\n     / \\   / \\   / \\   / \\\n    a   b c   d e   f g   H\n\n               2\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           2\n        / \\         / \\\n       /   \\       /   \\\n      0     0     2     1\n     / \\   / \\   / \\   / \\\n    A   b c   d e   F g   H\n\n               3\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           3\n        / \\         / \\\n       /   \\       /   \\\n      0     0     2     3\n     / \\   / \\   / \\   / \\\n    a   b c   d e   F G   H\n\nSuppose however that your implementation does lazy hashing; after the 3rd\nupdate your state will be:\n\n               .\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           .\n        / \\         / \\\n       /   \\       /   \\\n      0     0     .     .\n     / \\   / \\   / \\   / \\\n    a   b c   d e   F G   H\n\nBasically all the digests on the right side is out of date and need to be\nrecalculated. Now, first of all it's obviously possible for your implementation\nto keep updating values in the tree given their keys - you've essentially\nregressed to a bog standard binary tree.\n\nBut what happens if you discard part of your dataset? Let's suppose you've\ndiscarded the left half:\n\n               .\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           .\n                    / \\\n                   /   \\\n                  .     .\n                 / \\   / \\\n                e   F G   H\n\nNote how you still have sufficient information to calculate the current merkle\ntip commitment: the left side hasn't changed yet. But what happens when someone\ngives you an update proof? Specifically, suppose they want to change b -> B.\nThat requires them to provide you with the part of the merkle tree proving that\nposition #1 is b. Now you might think that's this data:\n\n               3\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           3\n        / \\\n       /   \\\n      0     0\n     / \\\n    a   b\n\nBut the inner node digests marked \"3\" are useless to you: you haven't\ncalculated those digests yet so you can't compare them to anything. What you\ncan compare is the following:\n\n         0\n        / \\\n       /   \\\n      0     0\n     / \\\n    a   b\n\nWith that extra data your local knowledge is now:\n\n               .\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         0           .\n        / \\         / \\\n       /   \\       /   \\\n      0     0     .     .\n     / \\         / \\   / \\\n    a   b       e   F G   H\n\nAllowing you to apply the update:\n\n               .\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         .           .\n        / \\         / \\\n       /   \\       /   \\\n      .     0     .     .\n     / \\         / \\   / \\\n    a   B       e   F G   H\n\nIf you want to again prune that data, simply recalculate the digests so you\ncan verify a copy given to you by a peer in the future:\n\n               .\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         4           .\n        / \\         / \\\n       /   \\       /   \\\n      4     0     .     .\n     / \\         / \\   / \\\n    a   B       e   F G   H\n\nAnd prune, leaving you with:\n\n               .\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         4           .\n                    / \\\n                   /   \\\n                  .     .\n                 / \\   / \\\n                e   F G   H\n\n\nSo tl;dr: the reason this works is that we can substitute commitments for\npointers: our merkle tree can also be viewed as a binary tree. So a reasonable\nreal-world implementation would be to delay computation of digests for anything\nwe have in RAM, and only compute digests as in-RAM data is flushed to disk.\nEqually, on disk we can use standard time-space tradeoffs to only store a\nsubset of the digests, recalculating the rest on the fly. Given that'd we could\neffectively combine both a cryptographic data structure and a standard\npointer-based data structure in one, I suspect we can get good performance out\nof this.\n\nThe main subtlety of this approach will be how exactly to handle the proofs:\nthe level of verification possible depends on what digests a given node has\ncalculated, and we want to avoid making network splitting attacks possible by\nattackers deliberately giving nodes proofs with upper digests that are\nincorrect, something only some nodes can detect. Not sure yet exactly what's\nthe right approach there.\n\nFinally, notice how this entire approach depends on schemes like MMR's where\nthe overall structure of the tree does not change as nodes are added and\nupdated; it would be much harder to implement this idea for something like a\nmerklized red-black tree where the structure changes as the tree is rebalanced.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/9d5d9e70/attachment.sig>"
            },
            {
                "author": "praxeology_guy",
                "date": "2017-02-28T16:26:40",
                "message_text_only": "Peter Todd & Eric Lombrozo,\n\nI also think communicating the UTXO would be increadibly useful. I just made a writeup called \"Synchronization Checkpoints\" on github. \"https://github.com/bitcoin/bitcoin/issues/9885\" This idea also doesn't use commitments.\n\nBut... Commitments would be a plus, because then we having all of the miners verifying the UTXO. Below I brainstorm on how to make this happen with my \"Synchronization Checkpoints\" idea.\n\nI think if there were commitments, such would not be feasible without it being a commitment on the UTXO as it was N blocks in the past rather than the highest block's UTXO set... because just one little fork of height 1 would be a big waste of effort for the miners.\n\n- Miners would put a commitment at the current Checkpoint Block that would be a hash of the full state of the UTXO at the previous Checkpoint Block.\n- I'll point out that blocks are like \"incremental diffs\" to the UTXO state.\n\nI was thinking that say if a miner and other nodes are OK with storing multiple copies/backups of the UTXO state then to make this work with high performance:\n1. Maintain a DB table who's only purpose is to sort UTXO.txid concat UTXO.vout.index.\n2. Some Wait for no Forks blocks after a CheckPoint Block is made, begin populating a new UTXO Checkpoint File that is a serialized sorted UTXO set.\n3. Merkle tree or bittorrent style hash the UTXO Checkpoint File\n4. Party!\n\nCheers,\nPraxeology\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170228/354c0708/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "TXO commitments do not need a soft-fork to be useful",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Lombrozo",
                "Peter Todd",
                "praxeology_guy"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 17890
        }
    },
    {
        "title": "[bitcoin-dev] A Better MMR Definition",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2017-02-23T01:15:06",
                "message_text_only": "Reposting something that came up recently in a private discussion with some\nacademics:\n\nConcretely, let's define a prunable MMR with the following grammar. This\ndefinition is an improvement on whats in the python-proofmarshal by committing\nto the number of items in the tree implicitly; an obvious max-log2(n)-sized\nproof-of-tree-size can be obtained by following the right-most nodes:\n\n    Maybe(T) := UNPRUNED <T> | PRUNED <Commitment(T)>\n\n    FullNode(0) := <Value>\n    FullNode(n) := <Maybe(FullNode(n-1)> <Maybe(FullNode(n-1))>\n\n    PartialNode(0) := SOME <FullNode(0)> | NONE\n    PartialNode(n) := <Maybe(FullNode(n-1))> <Maybe(PartialNode(n-1))>\n\n    MMR := FULL <N> <FullNode(n)> | PARTIAL <N> <PartialNode(n)>\n\nBasically we define it in four parts. First we define Maybe(T) to represent\npruned and unpruned (hash only) data. Secondly we define full nodes within 2^n\nsized trees. Third we define partial nodes. And finally we define the MMR\nitself as being either a full or partial node.\n\nFirst of all, with pruning we can define a rule that if any operation (other\nthan checking commitment hashes) attempts to access pruned data, it should\nimmediately fail. In particular, no operation should be able to determine if\ndata is or isn't pruned. Equally, note how an implementation can keep track of\nwhat data was accessed during any given operation, and prune the rest, which\nmeans a proof is just the parts of the data structure accessed during one or\nmore operations.\n\nWith that, notice how proving the soundness of the proofs becomes trivial: if\nvalidation is deterministic, it is obviously impossible to construct two\ndifferent proofs that prove contradictory statements, because a proof is simply\npart of the data structure itself. Contradiction would imply that the two\nproofs are different, but that's easily rejected by simply checking the hash of\nthe data.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170222/1412af52/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-23T03:07:08",
                "message_text_only": "On Wed, Feb 22, 2017 at 5:15 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> With that, notice how proving the soundness of the proofs becomes trivial:\n> if\n> validation is deterministic, it is obviously impossible to construct two\n> different proofs that prove contradictory statements, because a proof is\n> simply\n> part of the data structure itself. Contradiction would imply that the two\n> proofs are different, but that's easily rejected by simply checking the\n> hash of\n> the data.\n>\n\nMy code works this way. Proofs are serialization of a subset of the tree,\nand to validate a proof you ask a single function whether a particular\nvalue is included in that tree subset, and it answers yes or no, so\nobviously it's impossible for a single value to both validate and not\nvalidate. The proof code was quite terrifying before I made this change\n(which I did on your suggestion), and it's much cleaner and simpler now. It\nalso in principle supports compact proofs of multiple inclusions and\nexclusions in the same serialization of a subset of the tree because the\nupper branches won't have to be repeated. I haven't written code for\ngenerating those, but the validation code will happily accept them.\n\nI'm not sure what you mean by MMRs though. Are you talking about MMRs where\neach mountain is a set of diffs to the old things and are periodically\nconsolidated? Or do later mountains refer to internals of earlier ones? Or\ndo they have 'maybe' values which mean that the earlier mountain should be\nreferred to? Are these patricia tries or something flatter and more fixed\ndepth?\n\nMy code doesn't keep track of tree size, by the way. It would be trivial to\nadd that functionality to the library, and including it in the hashing\ncreates complexity and doesn't seem to have any benefit over sending that\ndata in a side channel.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170222/4a48d1ae/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-23T07:41:37",
                "message_text_only": "On Wed, Feb 22, 2017 at 07:07:08PM -0800, Bram Cohen wrote:\n> On Wed, Feb 22, 2017 at 5:15 PM, Peter Todd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> >\n> > With that, notice how proving the soundness of the proofs becomes trivial:\n> > if\n> > validation is deterministic, it is obviously impossible to construct two\n> > different proofs that prove contradictory statements, because a proof is\n> > simply\n> > part of the data structure itself. Contradiction would imply that the two\n> > proofs are different, but that's easily rejected by simply checking the\n> > hash of\n> > the data.\n> >\n> \n> My code works this way. Proofs are serialization of a subset of the tree,\n> and to validate a proof you ask a single function whether a particular\n> value is included in that tree subset, and it answers yes or no, so\n> obviously it's impossible for a single value to both validate and not\n> validate. The proof code was quite terrifying before I made this change\n> (which I did on your suggestion), and it's much cleaner and simpler now. It\n> also in principle supports compact proofs of multiple inclusions and\n> exclusions in the same serialization of a subset of the tree because the\n> upper branches won't have to be repeated. I haven't written code for\n> generating those, but the validation code will happily accept them.\n\nThat's an improvement, but I think we can do even better if we think of missing\npruned data as analogous to virtual memory: pruned data is the same as a page\nthat has been swapped to disk, with the magical property that hashing allows us\nto swap it back in from an untrusted source.\n\nThus a proof should actually be whatever data we expect our counterparty to\nhave flushed, ranging from none at all, to 100% (modulo a root hash). An\nimplementation should then do operations as normal, using parts of the proof on\nan as-needed basis where pruned data is encountered.\n\nThus if you have a key-value map and do a get() operation, you'd expect the\nproof to *not* be what the get operates on, but rather be a *context* argument\nto the get() operation. The other way around is actually an example of doing\ncomputations on untrusted data, and bad API design!\n\n> I'm not sure what you mean by MMRs though. Are you talking about MMRs where\n> each mountain is a set of diffs to the old things and are periodically\n> consolidated? Or do later mountains refer to internals of earlier ones? Or\n> do they have 'maybe' values which mean that the earlier mountain should be\n> referred to? Are these patricia tries or something flatter and more fixed\n> depth?\n\nI'm talking about these MMR's: https://github.com/proofchains/python-proofmarshal/blob/master/proofmarshal/mmr.py\n\nNotably I'm talking about an insertion ordered list, indexed by position, that\nsupports append and update operations, but *not* insertions; this is different\nthan what you've recently published re: UTXO commitments. That's a full\nkey-value map, something MMR's are delibrately are not doing.\n\nDraw out a MMR based on the formal definition you're replying too and you'll\nsee the new structure.\n\n> My code doesn't keep track of tree size, by the way. It would be trivial to\n> add that functionality to the library, and including it in the hashing\n> creates complexity and doesn't seem to have any benefit over sending that\n> data in a side channel.\n\nLike I say above, you're solving a different problem than MMR's solve.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/91289920/attachment-0001.sig>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-02-23T17:53:58",
                "message_text_only": "On 2/22/17, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Reposting something that came up recently in a private discussion with some\n> academics:\n>\n> Concretely, let's define a prunable MMR with the following grammar. This\n> definition is an improvement on whats in the python-proofmarshal by\n> committing\n> to the number of items in the tree implicitly; an obvious max-log2(n)-sized\n> proof-of-tree-size can be obtained by following the right-most nodes:\n>\n>     Maybe(T) := UNPRUNED <T> | PRUNED <Commitment(T)>\n>\n>     FullNode(0) := <Value>\n>     FullNode(n) := <Maybe(FullNode(n-1)> <Maybe(FullNode(n-1))>\n>\n>     PartialNode(0) := SOME <FullNode(0)> | NONE\n>     PartialNode(n) := <Maybe(FullNode(n-1))> <Maybe(PartialNode(n-1))>\n>\n>     MMR := FULL <N> <FullNode(n)> | PARTIAL <N> <PartialNode(n)>\n>\n> Basically we define it in four parts. First we define Maybe(T) to represent\n> pruned and unpruned (hash only) data. Secondly we define full nodes within\n> 2^n\n> sized trees. Third we define partial nodes. And finally we define the MMR\n> itself as being either a full or partial node.\n>\n> First of all, with pruning we can define a rule that if any operation\n> (other\n> than checking commitment hashes) attempts to access pruned data, it should\n> immediately fail. In particular, no operation should be able to determine\n> if\n> data is or isn't pruned. Equally, note how an implementation can keep track\n> of\n> what data was accessed during any given operation, and prune the rest,\n> which\n> means a proof is just the parts of the data structure accessed during one\n> or\n> more operations.\n>\n> With that, notice how proving the soundness of the proofs becomes trivial:\n> if\n> validation is deterministic, it is obviously impossible to construct two\n> different proofs that prove contradictory statements, because a proof is\n> simply\n> part of the data structure itself. Contradiction would imply that the two\n> proofs are different, but that's easily rejected by simply checking the hash\n> of\n> the data.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n\n\nWhat problem does this try to solve, and what does it have to do with bitcoin?"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-23T18:19:29",
                "message_text_only": "On Thu, Feb 23, 2017 at 09:53:58AM -0800, Chris Priest wrote:\n> On 2/22/17, Peter Todd via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Reposting something that came up recently in a private discussion with some\n> > academics:\n> >\n> > Concretely, let's define a prunable MMR with the following grammar. This\n> > definition is an improvement on whats in the python-proofmarshal by\n> > committing\n> > to the number of items in the tree implicitly; an obvious max-log2(n)-sized\n> > proof-of-tree-size can be obtained by following the right-most nodes:\n> \n> What problem does this try to solve, and what does it have to do with bitcoin?\n\nSee the discussion on TXO commitments for how MMR's could be used; a better MMR\nmakes for a better TXO commitment.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/8a77c8e5/attachment.sig>"
            },
            {
                "author": "G. Andrew Stone",
                "date": "2017-02-23T18:28:18",
                "message_text_only": "Can an insertion ordered MMR allow an efficient nonexistence proof?\nOn Feb 23, 2017 1:20 PM, \"Peter Todd via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, Feb 23, 2017 at 09:53:58AM -0800, Chris Priest wrote:\n> > On 2/22/17, Peter Todd via bitcoin-dev\n> > <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > > Reposting something that came up recently in a private discussion with\n> some\n> > > academics:\n> > >\n> > > Concretely, let's define a prunable MMR with the following grammar.\n> This\n> > > definition is an improvement on whats in the python-proofmarshal by\n> > > committing\n> > > to the number of items in the tree implicitly; an obvious\n> max-log2(n)-sized\n> > > proof-of-tree-size can be obtained by following the right-most nodes:\n> >\n> > What problem does this try to solve, and what does it have to do with\n> bitcoin?\n>\n> See the discussion on TXO commitments for how MMR's could be used; a\n> better MMR\n> makes for a better TXO commitment.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/07c5af54/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-23T18:31:40",
                "message_text_only": "On Thu, Feb 23, 2017 at 01:28:18PM -0500, G. Andrew Stone wrote:\n> Can an insertion ordered MMR allow an efficient nonexistence proof?\n\nWhy do you want a non-existance proof?\n\nIt supports an efficient *spentness* proof, which is sufficient for what we\nneed in Bitcoin, and much more scalable.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/74acf29d/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-23T23:13:43",
                "message_text_only": "On Thu, Feb 23, 2017 at 9:53 AM, Chris Priest via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> What problem does this try to solve, and what does it have to do with\n> bitcoin?\n>\n\nI can't speak to MMRs (they look a bit redundant with the actual blockchain\nhistory to my eye) but circling back to utxo commitments, the benefits are\nthat it enables actual proofs of non-fraud: You can prove the validity of a\nblock based on just the previous block (and maybe some previous headers\nbecause of mining rewards) and can prove to a light node that a utxo hasn't\nbeen spent yet.\n\nA major factor in the way of getting utxo commitments in blocks is\nperformance. The txo set is of course vastly larger and more unwieldy. If\nyou make the utxo commitments trail by a small fixed number of blocks\n(between 2 and 5) their latency problems shouldn't be a big deal as long as\nthe overall performance is good enough. My thesis is that with appropriate\nformat and implementation tricks it's possible to get performance good\nenough to no longer be a gating factor to deployment.\n\nDisappointingly there hasn't been any feedback about my implementation,\njust discussion about merkle sets generally.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/5dbd72da/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-23T23:51:05",
                "message_text_only": "On Thu, Feb 23, 2017 at 03:13:43PM -0800, Bram Cohen wrote:\n> On Thu, Feb 23, 2017 at 9:53 AM, Chris Priest via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> >\n> > What problem does this try to solve, and what does it have to do with\n> > bitcoin?\n> >\n> \n> I can't speak to MMRs (they look a bit redundant with the actual blockchain\n> history to my eye) but circling back to utxo commitments, the benefits are\n\nIn what way do you see MMRs as redundant?\n\nRemember that with UTXO commitments because access patterns are uniform, you'll\nover time have a lot more \"redundancy\" in the form of lost-coins evenly spread\nout across the whole keyspace.\n\n> that it enables actual proofs of non-fraud: You can prove the validity of a\n> block based on just the previous block (and maybe some previous headers\n> because of mining rewards) and can prove to a light node that a utxo hasn't\n> been spent yet.\n>\n> A major factor in the way of getting utxo commitments in blocks is\n> performance. The txo set is of course vastly larger and more unwieldy. If\n\nThat statement is incorrect with pruning: you can maintain a commitment to the\nTXO set, without actually storing the entire TXO set, because you don't need to\nstore anything for nodes that have already been spent.\n\nConcretely, this can be done with nothing more than adding a FullySpent node\ntype to the MMR definition I published earlier, with the rule being that only a\nleft or right child of an inner node be a FullySpent node, not both; if both\nsides are spent, the inner node itself becomes FullySpent. Equally, I think you\ncan re-use the Empty node for this, but I need to think a little about the\nimplications re: partial inner nodes.\n\nRegardless, with a generalized commitment scheme, the serialization/commitment\nto an Empty node is simply '0', the encoding of an unspent txout surrounded by\nspent txouts will be similar in size to a position integer followed by the\ntxout...\n\n\nA subtlety of this construction is that you can only prove that a specific\ntxout # is unspent, but that's actually sufficient, as you can also prove what\n# a txout txid corresponds too with a previous version of the MMR.\n\n> you make the utxo commitments trail by a small fixed number of blocks\n> (between 2 and 5) their latency problems shouldn't be a big deal as long as\n> the overall performance is good enough. My thesis is that with appropriate\n> format and implementation tricks it's possible to get performance good\n> enough to no longer be a gating factor to deployment.\n> \n> Disappointingly there hasn't been any feedback about my implementation,\n> just discussion about merkle sets generally.\n\nWell, I think at this point there's still discussion over whether or not a UTXO\nset commitment is the right approach to begin with; if it's not your\nimplementation isn't relevant.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/7dfb1ea7/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-24T00:49:01",
                "message_text_only": "On Thu, Feb 23, 2017 at 3:51 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Thu, Feb 23, 2017 at 03:13:43PM -0800, Bram Cohen wrote:\n> >\n> > I can't speak to MMRs (they look a bit redundant with the actual\n> blockchain\n> > history to my eye) but circling back to utxo commitments, the benefits\n> are\n>\n> In what way do you see MMRs as redundant?\n>\n\nYou can readily prove something is in the TXO or STXO set using the actual\nblockchain, and the proofs will be nice and compact because even light\nnodes are expected to already have all the historical headers.\n\nWhat you can't do with MMRs or the blockchain is make a compact proof that\nsomething is still in the utxo set, which is the whole point of utxo\ncommitments.\n\nIt's totally reasonable for full nodes to independently update and\nrecalculate the utxo set as part of their validation process. The same\ncan't be done for a balanced version of the txo set because it's too big.\nRelying on proofs as a crutch for using the full txo set would badly\nexacerbate the already extant problem of miners doing spv mining, and\nincrease the bandwidth a full validating node had to use by a multiple.\n\nThis whole conversation is badly sidetracked. If people have comments on my\nmerkle set I'd like to engage further with them, but mmrs need to be argued\nindependently on their own merits before being used as a counterpoint to\nutxo commitments.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/c2fc2e57/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-24T01:09:43",
                "message_text_only": "On Thu, Feb 23, 2017 at 04:49:01PM -0800, Bram Cohen wrote:\n> On Thu, Feb 23, 2017 at 3:51 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> > On Thu, Feb 23, 2017 at 03:13:43PM -0800, Bram Cohen wrote:\n> > >\n> > > I can't speak to MMRs (they look a bit redundant with the actual\n> > blockchain\n> > > history to my eye) but circling back to utxo commitments, the benefits\n> > are\n> >\n> > In what way do you see MMRs as redundant?\n> >\n> \n> You can readily prove something is in the TXO or STXO set using the actual\n> blockchain, and the proofs will be nice and compact because even light\n> nodes are expected to already have all the historical headers.\n> \n> What you can't do with MMRs or the blockchain is make a compact proof that\n> something is still in the utxo set, which is the whole point of utxo\n> commitments.\n\nI think you've misunderstood what TXO commitments are. From my article:\n\n\"A merkle tree committing to the state of all transaction outputs, both spent\nand unspent, can provide a method of compactly proving the current state of an\noutput.\"\n-https://petertodd.org/2016/delayed-txo-commitments#txo-commitments:\n\nI'm proposing that we commit to not just the set of transaction outputs, but\nalso the current *state* of those outputs, with the same commitment structure.\n\nConcretely, each leaf node in the TXO commitment tree needs to commit to - at\nminimum - the outpoint (txid:n) and spent/unspent status (possibly structurally\nas mentioned elsewhere in this thread). It's probably also valuable to commit\nto the scriptPubKey, nValue, as well, though technically that's redundant as\nthe txid already commits to that (there's some implementation options here).\n\n> It's totally reasonable for full nodes to independently update and\n> recalculate the utxo set as part of their validation process. The same\n> can't be done for a balanced version of the txo set because it's too big.\n\nWhy would you commit to a balanced version of the TXO set? I'm proposing\ncommitting to an insertion-ordered list, indexed by txout #.\n\n> Relying on proofs as a crutch for using the full txo set would badly\n> exacerbate the already extant problem of miners doing spv mining, and\n> increase the bandwidth a full validating node had to use by a multiple.\n> \n> This whole conversation is badly sidetracked. If people have comments on my\n> merkle set I'd like to engage further with them, but mmrs need to be argued\n> independently on their own merits before being used as a counterpoint to\n> utxo commitments.\n\nHmm? That's exactly what I'm doing. Also, as per the above, I think you've\nmisunderstood what my TXO commitment proposal is.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/6b0f13f2/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-24T02:50:10",
                "message_text_only": "On Thu, Feb 23, 2017 at 5:09 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> I think you've misunderstood what TXO commitments are. From my article:\n>\n> \"A merkle tree committing to the state of all transaction outputs, both\n> spent\n> and unspent, can provide a method of compactly proving the current state\n> of an\n> output.\"\n> -https://petertodd.org/2016/delayed-txo-commitments#txo-commitments:\n>\n\nThe proposal on that page is of a tree which does require random access\nupdates, it just positions entries in the order they happened to be added\ninstead of sorting by their hash. Once you start updating it to indicate\nspent status all the exact same issues of TXO size and cache coherence on\nupdates show up again, but now you're using a more complex bespoke data\nstructure instead of a basic fundamental one.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/e1b6fe16/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-24T02:58:11",
                "message_text_only": "On Thu, Feb 23, 2017 at 06:50:10PM -0800, Bram Cohen wrote:\n> On Thu, Feb 23, 2017 at 5:09 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> > I think you've misunderstood what TXO commitments are. From my article:\n> >\n> > \"A merkle tree committing to the state of all transaction outputs, both\n> > spent\n> > and unspent, can provide a method of compactly proving the current state\n> > of an\n> > output.\"\n> > -https://petertodd.org/2016/delayed-txo-commitments#txo-commitments:\n> >\n> \n> The proposal on that page is of a tree which does require random access\n> updates, it just positions entries in the order they happened to be added\n> instead of sorting by their hash. Once you start updating it to indicate\n> spent status all the exact same issues of TXO size and cache coherence on\n> updates show up again, but now you're using a more complex bespoke data\n> structure instead of a basic fundamental one.\n\nSorry, but I was replying to your statement:\n\n> What you can't do with MMRs or the blockchain is make a compact proof that\n> something is still in the utxo set, which is the whole point of utxo\n> commitments.\n\nSo to be clear, do you agree or disagree with me that you *can* extract a\ncompact proof from a MMR that a given output is unspent?\n\nI just want to make sure we're on the same page here before we discuss\nperformance characteristics.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/5d17c853/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-24T03:02:36",
                "message_text_only": "On Thu, Feb 23, 2017 at 6:58 PM, Peter Todd <pete at petertodd.org> wrote:\n\n>\n> So to be clear, do you agree or disagree with me that you *can* extract a\n> compact proof from a MMR that a given output is unspent?\n>\n\nAfter wading through your logic on how updates are done, I agree that that\ncan be done, but apples to apples compact proofs can also be done in a utxo\ncommitment, and proofs of the validity of updates can be done in a utxo\ncommitment, so there isn't any performance advantage to all that extra\ncomplexity.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/a93584fd/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-24T03:15:31",
                "message_text_only": "On Thu, Feb 23, 2017 at 07:02:36PM -0800, Bram Cohen wrote:\n> On Thu, Feb 23, 2017 at 6:58 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> >\n> > So to be clear, do you agree or disagree with me that you *can* extract a\n> > compact proof from a MMR that a given output is unspent?\n> >\n> \n> After wading through your logic on how updates are done, I agree that that\n> can be done, but apples to apples compact proofs can also be done in a utxo\n> commitment, and proofs of the validity of updates can be done in a utxo\n> commitment, so there isn't any performance advantage to all that extra\n> complexity.\n\nGlad we're on the same page with regard to what's possible in TXO commitments.\n\nSecondly, am I correct in saying your UTXO commitments scheme requires random\naccess? While you describe it as a \"merkle set\", obviously to be merkelized\nit'll have to have an ordering of some kind. What do you propose that ordering\nto be?\n\nMaybe more specifically, what exact values do you propose to be in the set?\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/4f1075da/attachment-0001.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-24T03:32:43",
                "message_text_only": "On Thu, Feb 23, 2017 at 7:15 PM, Peter Todd <pete at petertodd.org> wrote:\n\n>\n> Glad we're on the same page with regard to what's possible in TXO\n> commitments.\n>\n> Secondly, am I correct in saying your UTXO commitments scheme requires\n> random\n> access? While you describe it as a \"merkle set\", obviously to be merkelized\n> it'll have to have an ordering of some kind. What do you propose that\n> ordering\n> to be?\n>\n\nThe ordering is by the bits in the hash. Technically it's a Patricia Trie.\nI'm using 'merkle tree' to refer to basically anything with a hash root.\n\n\n> Maybe more specifically, what exact values do you propose to be in the set?\n>\n>\nThat is unspecified in the implementation, it just takes a 256 bit value\nwhich is presumably a hash of something. The intention is to nail down a\nsimple format and demonstrate good performance and leave those semantics to\na higher layer. The simplest thing would be to hash together the txid and\noutput number.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/ebb642c9/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-24T04:36:13",
                "message_text_only": "On Thu, Feb 23, 2017 at 07:32:43PM -0800, Bram Cohen wrote:\n> On Thu, Feb 23, 2017 at 7:15 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> >\n> > Glad we're on the same page with regard to what's possible in TXO\n> > commitments.\n> >\n> > Secondly, am I correct in saying your UTXO commitments scheme requires\n> > random\n> > access? While you describe it as a \"merkle set\", obviously to be merkelized\n> > it'll have to have an ordering of some kind. What do you propose that\n> > ordering\n> > to be?\n> >\n> \n> The ordering is by the bits in the hash. Technically it's a Patricia Trie.\n> I'm using 'merkle tree' to refer to basically anything with a hash root.\n\nThe hash of what? The values in the set?\n\n> > Maybe more specifically, what exact values do you propose to be in the set?\n> >\n> >\n> That is unspecified in the implementation, it just takes a 256 bit value\n> which is presumably a hash of something. The intention is to nail down a\n> simple format and demonstrate good performance and leave those semantics to\n> a higher layer. The simplest thing would be to hash together the txid and\n> output number.\n\nOk, so let's assume the values in the set are the unspent outpoints.\n\nSince we're ordering by the hash of the values in the set, outpoints will be\ndistributed uniformly in the set, and thus the access pattern of data in the\nset is uniform.\n\nNow let's fast-forward 10 years. For the sake of argument, assume that for\nevery 1 UTXO in the set that corresponds to funds in someone's wallet that are\nlikely to be spent, there are 2^12 = 4096 UTXO's that have been permanently\nlost (and/or created in spam attacks) and thus will never be spent.\n\nSince lost UTXO's are *also* uniformly distributed, if I'm processing a new\nblock that spends 2^12 = 4096 UTXO's, on average for each UTXO spent, I'll\nhave to update log2(4096) = 12 more digests than I would have had those \"dead\"\nUTXO's not existed.\n\nConcretely, imagine our UTXO set had just 8 values in it, and we were updating\ntwo of them:\n\n               #\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         #           #\n        / \\         / \\\n       /   \\       /   \\\n      #     .     .     #\n     / \\   / \\   / \\   / \\\n    .   X .   . .   . X   .\n\nTo mark two coins as spent, we've had to update 5 inner nodes.\n\n\nNow let's look at what happens in an insertion-ordered TXO commitment scheme.\nFor sake of argument, let's assume the best possible case, where every UTXO\nspent in that same block was recently created. Since the UTXO's are recently\ncreated, chances are almost every single one of those \"dead\" UTXO's will have\nbeen created in the past. Thus, since this is an insertion-ordered data\nstructure, those UTXO's exist in an older part of the data structure that our\nnew block doesn't need to modify at all.\n\nConcretely, again let's imagine a TXO commitment with 8 values in it, and two\nof them being spent:\n\n               #\n              / \\\n             /   \\\n            /     \\\n           /       \\\n          /         \\\n         .           #\n        / \\         / \\\n       /   \\       /   \\\n      .     .     .     #\n     / \\   / \\   / \\   / \\\n    .   . .   . .   . X   X\n\nTo mark two coins as spent, we've only had to update 3 inner nodes; while our\ntree is higher with those lost coins, those extra inner nodes are amortised\nacross all the coins we have to update.\n\n\nThe situation gets even better when we look at the *new* UTXO's that our block\ncreates. Suppose our UTXO set has size n. To mark a single coin as spent, we\nhave to update log2(n) inner nodes. We do get to amortise this a bit at the top\nlevels in the tree, but even if we assume the amortisation is totally free,\nwe're updating at least log2(n) - log2(m) inner nodes \"under\" the amortised\nnodes at the top of the tree for *each* new node.\n\nMeanwhile with an insertion-ordered TXO commitment, each new UTXO added to the\ndata set goes in the same place - the end. So almost none of the existing data\nneeds to be touched to add the new UTXOs. Equally, the hashing required for the\nnew UTXO's can be done in an incremental fashion that's very L1/L2 cache\nfriendly.\n\n\ntl;dr: Precisely because access patterns in TXO commitments are *not* uniform,\nI think we'll find that from a L1/L2/etc cache perspective alone, TXO\ncommitments will result in better performance than UTXO commitments.\n\n\nNow it is true that Bitcoin's current design means we'll need a map of\nconfirmed outpoints to TXO insertion order indexes. But it's not particularly\nhard to add that \"metadata\" to transactions on the P2P layer in the same way\nthat segwit added witnesses to transactions without modifying how txids were\ncalculated; if you only connect to peers who provide you with TXO index\ninformation in blocks and transactions, you don't need to keep that map\nyourself.\n\nFinally, note how this makes transactions *smaller* in many circumstances: it's\njust a 8-byte max index rather than a 40 byte outpoint.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/6a611ba3/attachment.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-24T22:20:19",
                "message_text_only": "So your idea is to cluster entries by entry time because newer things are\nmore likely to leave and updating multiple things near each other is\ncheaper?\n\nThat can be done with my tool. Instead of using hashes for the values being\nstored, you use position entries. The first entry gets a value of all\nzeros, the next one a one followed by all zeros, then the next two\ncorrespond to the first two with the second bit flipped to one, then the\nnext four the first four with the third bit flipped to one, etc. It\nprobably performs a little bit better to do it two bits at a time instead\nof one so that the entries are 00, 01, 10, 11, 0001, 0010, 0011, 0101,\n0110, 0111, 1001, etc. If you were to really use this you'd probably want\nto to add some optimizations to use the fact that the terminals fit in 64\nbits instead of 256, but it mostly works unchanged, and gets whatever\nbenefits there are to this clustering plus the high performance\nimplementation tricks I've built which I keep complaining that nobody's\ngiving feedback on.\n\nI'm not sold on this being a win: The empirical access patterns are\nunknown, it requires an extra cache miss per lookup to find the entry\nnumber, it may be that everything is optimized well enough without it for\nthere to be no meaningful gains, and it's a bunch of extra complexity. What\nshould be done is that a plain vanilla UTXO set solution is optimized as\nwell as it can be first, and then the insertion ordering trick is tried as\nan optimization to see if it's an improvement. Without that baseline\nthere's no meaningful basis for comparison, and I'm quite confident that a\nnaive implementation which just allocates individual nodes will\nunderperform the thing I've come up with, even without adding optimizations\nrelated to fitting in 64 bits.\n\nOn Thu, Feb 23, 2017 at 8:36 PM, Peter Todd <pete at petertodd.org> wrote:\n\n> On Thu, Feb 23, 2017 at 07:32:43PM -0800, Bram Cohen wrote:\n> > On Thu, Feb 23, 2017 at 7:15 PM, Peter Todd <pete at petertodd.org> wrote:\n> >\n> > >\n> > > Glad we're on the same page with regard to what's possible in TXO\n> > > commitments.\n> > >\n> > > Secondly, am I correct in saying your UTXO commitments scheme requires\n> > > random\n> > > access? While you describe it as a \"merkle set\", obviously to be\n> merkelized\n> > > it'll have to have an ordering of some kind. What do you propose that\n> > > ordering\n> > > to be?\n> > >\n> >\n> > The ordering is by the bits in the hash. Technically it's a Patricia\n> Trie.\n> > I'm using 'merkle tree' to refer to basically anything with a hash root.\n>\n> The hash of what? The values in the set?\n>\n> > > Maybe more specifically, what exact values do you propose to be in the\n> set?\n> > >\n> > >\n> > That is unspecified in the implementation, it just takes a 256 bit value\n> > which is presumably a hash of something. The intention is to nail down a\n> > simple format and demonstrate good performance and leave those semantics\n> to\n> > a higher layer. The simplest thing would be to hash together the txid and\n> > output number.\n>\n> Ok, so let's assume the values in the set are the unspent outpoints.\n>\n> Since we're ordering by the hash of the values in the set, outpoints will\n> be\n> distributed uniformly in the set, and thus the access pattern of data in\n> the\n> set is uniform.\n>\n> Now let's fast-forward 10 years. For the sake of argument, assume that for\n> every 1 UTXO in the set that corresponds to funds in someone's wallet that\n> are\n> likely to be spent, there are 2^12 = 4096 UTXO's that have been permanently\n> lost (and/or created in spam attacks) and thus will never be spent.\n>\n> Since lost UTXO's are *also* uniformly distributed, if I'm processing a new\n> block that spends 2^12 = 4096 UTXO's, on average for each UTXO spent, I'll\n> have to update log2(4096) = 12 more digests than I would have had those\n> \"dead\"\n> UTXO's not existed.\n>\n> Concretely, imagine our UTXO set had just 8 values in it, and we were\n> updating\n> two of them:\n>\n>                #\n>               / \\\n>              /   \\\n>             /     \\\n>            /       \\\n>           /         \\\n>          #           #\n>         / \\         / \\\n>        /   \\       /   \\\n>       #     .     .     #\n>      / \\   / \\   / \\   / \\\n>     .   X .   . .   . X   .\n>\n> To mark two coins as spent, we've had to update 5 inner nodes.\n>\n>\n> Now let's look at what happens in an insertion-ordered TXO commitment\n> scheme.\n> For sake of argument, let's assume the best possible case, where every UTXO\n> spent in that same block was recently created. Since the UTXO's are\n> recently\n> created, chances are almost every single one of those \"dead\" UTXO's will\n> have\n> been created in the past. Thus, since this is an insertion-ordered data\n> structure, those UTXO's exist in an older part of the data structure that\n> our\n> new block doesn't need to modify at all.\n>\n> Concretely, again let's imagine a TXO commitment with 8 values in it, and\n> two\n> of them being spent:\n>\n>                #\n>               / \\\n>              /   \\\n>             /     \\\n>            /       \\\n>           /         \\\n>          .           #\n>         / \\         / \\\n>        /   \\       /   \\\n>       .     .     .     #\n>      / \\   / \\   / \\   / \\\n>     .   . .   . .   . X   X\n>\n> To mark two coins as spent, we've only had to update 3 inner nodes; while\n> our\n> tree is higher with those lost coins, those extra inner nodes are amortised\n> across all the coins we have to update.\n>\n>\n> The situation gets even better when we look at the *new* UTXO's that our\n> block\n> creates. Suppose our UTXO set has size n. To mark a single coin as spent,\n> we\n> have to update log2(n) inner nodes. We do get to amortise this a bit at\n> the top\n> levels in the tree, but even if we assume the amortisation is totally free,\n> we're updating at least log2(n) - log2(m) inner nodes \"under\" the amortised\n> nodes at the top of the tree for *each* new node.\n>\n> Meanwhile with an insertion-ordered TXO commitment, each new UTXO added to\n> the\n> data set goes in the same place - the end. So almost none of the existing\n> data\n> needs to be touched to add the new UTXOs. Equally, the hashing required\n> for the\n> new UTXO's can be done in an incremental fashion that's very L1/L2 cache\n> friendly.\n>\n>\n> tl;dr: Precisely because access patterns in TXO commitments are *not*\n> uniform,\n> I think we'll find that from a L1/L2/etc cache perspective alone, TXO\n> commitments will result in better performance than UTXO commitments.\n>\n>\n> Now it is true that Bitcoin's current design means we'll need a map of\n> confirmed outpoints to TXO insertion order indexes. But it's not\n> particularly\n> hard to add that \"metadata\" to transactions on the P2P layer in the same\n> way\n> that segwit added witnesses to transactions without modifying how txids\n> were\n> calculated; if you only connect to peers who provide you with TXO index\n> information in blocks and transactions, you don't need to keep that map\n> yourself.\n>\n> Finally, note how this makes transactions *smaller* in many circumstances:\n> it's\n> just a 8-byte max index rather than a 40 byte outpoint.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170224/63ab2731/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-25T04:12:02",
                "message_text_only": "On Fri, Feb 24, 2017 at 02:20:19PM -0800, Bram Cohen wrote:\n> So your idea is to cluster entries by entry time because newer things are\n> more likely to leave and updating multiple things near each other is\n> cheaper?\n\nYes, exactly.\n\n> That can be done with my tool. Instead of using hashes for the values being\n> stored, you use position entries. The first entry gets a value of all\n> zeros, the next one a one followed by all zeros, then the next two\n> correspond to the first two with the second bit flipped to one, then the\n> next four the first four with the third bit flipped to one, etc. It\n> probably performs a little bit better to do it two bits at a time instead\n> of one so that the entries are 00, 01, 10, 11, 0001, 0010, 0011, 0101,\n> 0110, 0111, 1001, etc. If you were to really use this you'd probably want\n> to to add some optimizations to use the fact that the terminals fit in 64\n> bits instead of 256, but it mostly works unchanged, and gets whatever\n\nSo to be clear, what you're proposing there is to use the insertion order as\nthe index - once you go that far you've almost entirely re-invented my\nproposal!\n\nIn fact, when I was working my proofchains/proofmarshal libraries I put some\nthought into whether or not I could leave out the MMR merkelized list\nimplementation and use only the key-value map I also wrote. I decided to\ninclude both as they aren't quite the same datastructure - using a list for a\nlist has advantages.\n\n> benefits there are to this clustering plus the high performance\n> implementation tricks I've built which I keep complaining that nobody's\n> giving feedback on.\n\nYour merkle-set implementation is 1500 lines of densely written Python with\nalmost no comments, and less than a 100 lines of (also uncommented) tests. By\ncomparison, my Python MMR implementation is 300 lines of very readable Python\nwith lots of comments, a 200 line explanation at the top, and 200 lines of\n(commented) tests. Yet no-one is taking the (still considerable) effort to\nunderstand and comment on my implementation. :)\n\nFact is, what you've written is really daunting to review, and given it's not\nin the final language anyway, it's unclear what basis to review it on anyway. I\nsuspect you'd get more feedback if the codebase was better commented, in a\nproduction language, and you have actual real-world benchmarks and performance\nfigures.\n\nIn particular, while at the top of merkle_set.py you have a list of advantages,\nand a bunch of TODO's, you don't explain *why* the code has any of these\nadvantages. To figure that out, I'd have to read and understand 1500 lines of\ndensely written Python. Without a human-readable pitch, not many people are\ngoing to do that, myself included.\n\n> I'm not sold on this being a win: The empirical access patterns are\n> unknown,\n\nLost coins alone guarantees that access patterns will be biased towards new\ncoins being more likely to be spent. That basis alone is sufficient to justify\nan insertion-ordered data structure. Additionally, people have done graphs of\nthe average age of UTXO's when spent, and that data clearly shows that newer\ncoins are more likely to be spent than older coins.\n\n> unknown, it requires an extra cache miss per lookup to find the entry\n> number,\n\nLike I mentioned in the email you're replying to, that extra lookup can be\neasily avoided with a change to how transactions/blocks are serialized; if all\nyour peers support TXO commitments you can even discard the lookup database\nentirely, as it's only a backwards compatibility measure.\n\n> it may be that everything is optimized well enough without it for\n> there to be no meaningful gains, and it's a bunch of extra complexity. What\n\nOptimization is itself extra complexity. If you're data structure has worse\ninherent performance, and you have to make up the different with a highly\noptimized implementation, that's likely to lead to more overall complexity than\nusing a data structure with better inherent performance.\n\nYour current merkle-set implementation definitely _is_ very complex. An\napples-to-apples comparison is with my merkelized key:value tree(1), also a\npatricia tree, which like the MMR is only about 300 lines of well-commented and\nstraight-forward code.\n\n1) https://github.com/proofchains/python-proofmarshal/blob/master/proofmarshal/merbinnertree.py\n\n> should be done is that a plain vanilla UTXO set solution is optimized as\n> well as it can be first, and then the insertion ordering trick is tried as\n> an optimization to see if it's an improvement. Without that baseline\n> there's no meaningful basis for comparison, and I'm quite confident that a\n> naive implementation which just allocates individual nodes will\n> underperform the thing I've come up with, even without adding optimizations\n> related to fitting in 64 bits.\n\nTo be clear, \"insertion ordering\" isn't a simple trick, it's a fundamental\nchange to what the data structure is. Once you do that, you're talking about my\nproposal.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170224/9ad10e61/attachment-0001.sig>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-25T06:23:20",
                "message_text_only": "On Fri, Feb 24, 2017 at 8:12 PM, Peter Todd <pete at petertodd.org> wrote:\n\n>\n> So to be clear, what you're proposing there is to use the insertion order\n> as\n> the index - once you go that far you've almost entirely re-invented my\n> proposal!\n>\n\nI'm not 'proposing' this, I'm saying it could be done simply but I'm\nskeptical of the utility. Probably the most compelling argument for it is\nthat the insertion indexed values are much smaller so they can be compacted\ndown a lot resulting in using less memory and more locality and fewer\nhashes, but your implementation doesn't take advantage of that.\n\n\n> Your merkle-set implementation is 1500 lines of densely written Python\n\n\nThe reference implementation which is included in those 1500 lines is less\nthan 300 lines and fairly straightforward. The non-reference implementation\nalways behaves semantically identically to the reference implementation, it\njust does so faster and using less memory.\n\n\n> with\n> almost no comments,\n\n\nThe comments at the top explain both the proof format and the in-memory\ndata structures very precisely. The whole codebase was reviewed by a\ncoworker of mine and comments were added explaining the subtleties which\ntripped him up.\n\n\n> and less than a 100 lines of (also uncommented) tests.\n\n\nThose tests get 98% code coverage and extensively hit not only the lines of\ncode but the semantic edge cases as well. The lines which aren't hit are\nconvenience functions and error conditions of the parsing code for when\nit's passed bad data.\n\n\n> By\n> comparison, my Python MMR implementation is 300 lines of very readable\n> Python\n> with lots of comments, a 200 line explanation at the top, and 200 lines of\n> (commented) tests. Yet no-one is taking the (still considerable) effort to\n> understand and comment on my implementation. :)\n>\n\nGiven that maaku's Merkle prefix trees were shelved because of performance\nproblems despite being written in C and operating in basically the same way\nas your code and my reference code, it's clear that non-optimized Python\nwon't be touching the bitcoin codebase any time soon.\n\n\n>\n> Fact is, what you've written is really daunting to review, and given it's\n> not\n> in the final language anyway, it's unclear what basis to review it on\n> anyway.\n\n\nIt should reviewed based on semantic correctness and performance.\nPerformance can only be accurately and convincingly determined by porting\nto C and optimizing it, which mostly involves experimenting with different\nvalues for the two passed in magic numbers.\n\n\n> I\n> suspect you'd get more feedback if the codebase was better commented, in a\n> production language, and you have actual real-world benchmarks and\n> performance\n> figures.\n>\n\nPorting to C should be straightforward. Several people have already\nexpressed interest in doing so, and it's written in intentionally C-ish\nPython, resulting in some rather odd idioms which is a bit part of why you\nthink it looks 'dense'. A lot of that weird offset math should be much more\nreadable in C because it's all structs and x.y notation can be used instead\nof adding offsets.\n\n\n> In particular, while at the top of merkle_set.py you have a list of\n> advantages,\n> and a bunch of TODO's, you don't explain *why* the code has any of these\n> advantages. To figure that out, I'd have to read and understand 1500 lines\n> of\n> densely written Python. Without a human-readable pitch, not many people are\n> going to do that, myself included.\n>\n\nIt's all about cache coherence. When doing operations it pulls in a bunch\nof things which are near each other in memory instead of jumping all over\nthe place. The improvements it gets should be much greater than the ones\ngained from insertion ordering, although the two could be accretive.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170224/f104455e/attachment.html>"
            },
            {
                "author": "G. Andrew Stone",
                "date": "2017-02-28T16:43:29",
                "message_text_only": "I can understand how Bram's transaction double sha256 hashed UTXO set\npatricia trie allows a client to quickly validate inputs because the inputs\nof a transaction are specified in the same manner.  So to verify that an\ninput is unspent the client simply traverses the patricia trie.\n\nIt also makes sense that if transaction inputs were specified by a [block\nheight, tx index, output index] triple we'd have a much more size-efficient\ntransaction format.  This format would make look up pretty simple in\nPeter's pruned time-ordered TXO merkle mountain range, although you'd have\ntranslate the triple to an index, which means we'd have to at a minimum\nkeep track of the number of TXOs in each block, and then probably do a\nlinear search starting from the location where the block's TXOs begin in\nthe MMR.  (The ultimate option I guess is to specify transaction inputs by\na single number which is essentially the index of the TXO in a (never\nactually created) insertion-ordered TXO array...)\n\nBut since transactions' prevouts are not specified by [block height, tx\nindex, output index] or by TXO index, I don't understand how an insertion\nordered TXO tree can result in efficient lookups.  Can you help me\nunderstand this?\n\n\n\nOn Sat, Feb 25, 2017 at 1:23 AM, Bram Cohen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Feb 24, 2017 at 8:12 PM, Peter Todd <pete at petertodd.org> wrote:\n>\n>>\n>> So to be clear, what you're proposing there is to use the insertion order\n>> as\n>> the index - once you go that far you've almost entirely re-invented my\n>> proposal!\n>>\n>\n> I'm not 'proposing' this, I'm saying it could be done simply but I'm\n> skeptical of the utility. Probably the most compelling argument for it is\n> that the insertion indexed values are much smaller so they can be compacted\n> down a lot resulting in using less memory and more locality and fewer\n> hashes, but your implementation doesn't take advantage of that.\n>\n>\n>> Your merkle-set implementation is 1500 lines of densely written Python\n>\n>\n> The reference implementation which is included in those 1500 lines is less\n> than 300 lines and fairly straightforward. The non-reference implementation\n> always behaves semantically identically to the reference implementation, it\n> just does so faster and using less memory.\n>\n>\n>> with\n>> almost no comments,\n>\n>\n> The comments at the top explain both the proof format and the in-memory\n> data structures very precisely. The whole codebase was reviewed by a\n> coworker of mine and comments were added explaining the subtleties which\n> tripped him up.\n>\n>\n>> and less than a 100 lines of (also uncommented) tests.\n>\n>\n> Those tests get 98% code coverage and extensively hit not only the lines\n> of code but the semantic edge cases as well. The lines which aren't hit are\n> convenience functions and error conditions of the parsing code for when\n> it's passed bad data.\n>\n>\n>> By\n>> comparison, my Python MMR implementation is 300 lines of very readable\n>> Python\n>> with lots of comments, a 200 line explanation at the top, and 200 lines of\n>> (commented) tests. Yet no-one is taking the (still considerable) effort to\n>> understand and comment on my implementation. :)\n>>\n>\n> Given that maaku's Merkle prefix trees were shelved because of performance\n> problems despite being written in C and operating in basically the same way\n> as your code and my reference code, it's clear that non-optimized Python\n> won't be touching the bitcoin codebase any time soon.\n>\n>\n>>\n>> Fact is, what you've written is really daunting to review, and given it's\n>> not\n>> in the final language anyway, it's unclear what basis to review it on\n>> anyway.\n>\n>\n> It should reviewed based on semantic correctness and performance.\n> Performance can only be accurately and convincingly determined by porting\n> to C and optimizing it, which mostly involves experimenting with different\n> values for the two passed in magic numbers.\n>\n>\n>> I\n>> suspect you'd get more feedback if the codebase was better commented, in a\n>> production language, and you have actual real-world benchmarks and\n>> performance\n>> figures.\n>>\n>\n> Porting to C should be straightforward. Several people have already\n> expressed interest in doing so, and it's written in intentionally C-ish\n> Python, resulting in some rather odd idioms which is a bit part of why you\n> think it looks 'dense'. A lot of that weird offset math should be much more\n> readable in C because it's all structs and x.y notation can be used instead\n> of adding offsets.\n>\n>\n>> In particular, while at the top of merkle_set.py you have a list of\n>> advantages,\n>> and a bunch of TODO's, you don't explain *why* the code has any of these\n>> advantages. To figure that out, I'd have to read and understand 1500\n>> lines of\n>> densely written Python. Without a human-readable pitch, not many people\n>> are\n>> going to do that, myself included.\n>>\n>\n> It's all about cache coherence. When doing operations it pulls in a bunch\n> of things which are near each other in memory instead of jumping all over\n> the place. The improvements it gets should be much greater than the ones\n> gained from insertion ordering, although the two could be accretive.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170228/0d77e5d7/attachment.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2017-02-28T23:10:16",
                "message_text_only": "On Tue, Feb 28, 2017 at 8:43 AM, G. Andrew Stone <g.andrew.stone at gmail.com>\nwrote:\n\n>\n> But since transactions' prevouts are not specified by [block height, tx\n> index, output index] or by TXO index, I don't understand how an insertion\n> ordered TXO tree can result in efficient lookups.  Can you help me\n> understand this?\n>\n\nYou have to have a lookup table going from prevouts to txo index. Lookups\non that are relatively fast because looking up things in a hashtable is a\nsingle cache miss, while looking up things in a tree is logarithmic cache\nmisses.\n\nThe purported benefit of using txout is that because recent things are\nspent much more than old things, there's a lot of clustering of updates. If\nyou update two things near each other they share the top branches of\nupdates in the tree, resulting in less hashing and cache misses. But since\neverything is log scale I suspect such benefits are small. My guess is\ntransaction ordering has much larger potential from compression because you\ncram information about lots of things into a single leaf node because they\nhave very small diffs from each other. That said, those benefits are also\nsmaller than and accretive to the simple implementation tricks I already\nimplemented which cause things near each other in the tree to be near each\nother in memory.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170228/aa032a07/attachment-0001.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2017-02-28T23:24:28",
                "message_text_only": "On Feb 28, 2017 15:10, \"Bram Cohen via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nOn Tue, Feb 28, 2017 at 8:43 AM, G. Andrew Stone <g.andrew.stone at gmail.com>\nwrote:\n\n>\n> But since transactions' prevouts are not specified by [block height, tx\n> index, output index] or by TXO index, I don't understand how an insertion\n> ordered TXO tree can result in efficient lookups.  Can you help me\n> understand this?\n>\n\nYou have to have a lookup table going from prevouts to txo index. Lookups\non that are relatively fast because looking up things in a hashtable is a\nsingle cache miss, while looking up things in a tree is logarithmic cache\nmisses.\n\n\nI'm wondering if there is some confusion here.\n\nYes, someone needs to have a lookup table from prevouts to TXO tree\npositions. But because an insertion-ordered TXO tree does not rebalance,\nthat table can be maintained by wallets or service providers for just their\nown coins, instead of by every full node and miner individually for\neveryone's coins.\n\nIn the simplest committed TXO model, full nodes simply maintain the TXO\nroot hash, and every transaction/block comes with a proof that its inputs\nare in the TXO tree, and the necessary information to update the root after\nspending the inputs and adding the outputs.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170228/f1adfec6/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A Better MMR Definition",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Priest",
                "Peter Todd",
                "Bram Cohen",
                "G. Andrew Stone",
                "Pieter Wuille"
            ],
            "messages_count": 23,
            "total_messages_chars_count": 58707
        }
    },
    {
        "title": "[bitcoin-dev] SHA1 collisions make Git vulnerable to attakcs by third-parties, not just repo maintainers",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2017-02-23T18:14:09",
                "message_text_only": "Worth noting: the impact of the SHA1 collison attack on Git is *not* limited\nonly to maintainers making maliciously colliding Git commits, but also\nthird-party's submitting pull-reqs containing commits, trees, and especially\nfiles for which collisions have been found. This is likely to be exploitable in\npractice with binary files, as reviewers aren't going to necessarily notice\ngarbage at the end of a file needed for the attack; if the attack can be\nextended to constricted character sets like unicode or ASCII, we're in trouble\nin general.\n\nConcretely, I could prepare a pair of files with the same SHA1 hash, taking\ninto account the header that Git prepends when hashing files. I'd then submit\nthat pull-req to a project with the \"clean\" version of that file. Once the\nmaintainer merges my pull-req, possibly PGP signing the git commit, I then take\nthat signature and distribute the same repo, but with the \"clean\" version\nreplaced by the malicious version of the file.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/fe8c9937/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-23T21:28:02",
                "message_text_only": "On Thu, Feb 23, 2017 at 01:14:09PM -0500, Peter Todd via bitcoin-dev wrote:\n> Worth noting: the impact of the SHA1 collison attack on Git is *not* limited\n> only to maintainers making maliciously colliding Git commits, but also\n> third-party's submitting pull-reqs containing commits, trees, and especially\n> files for which collisions have been found. This is likely to be exploitable in\n> practice with binary files, as reviewers aren't going to necessarily notice\n> garbage at the end of a file needed for the attack; if the attack can be\n> extended to constricted character sets like unicode or ASCII, we're in trouble\n> in general.\n> \n> Concretely, I could prepare a pair of files with the same SHA1 hash, taking\n> into account the header that Git prepends when hashing files. I'd then submit\n> that pull-req to a project with the \"clean\" version of that file. Once the\n> maintainer merges my pull-req, possibly PGP signing the git commit, I then take\n> that signature and distribute the same repo, but with the \"clean\" version\n> replaced by the malicious version of the file.\n\nThinking about this a bit more, the most concerning avenue of attack is likely\nto be tree objects, as I'll bet you you can construct tree objs with garbage at\nthe end that many review tools don't pick up on. :(\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170223/29fe50b5/attachment-0001.sig>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-02-23T23:57:45",
                "message_text_only": "Maybe not, unlike frozen objects (certificates, etc), trees are supposed\nto extend\n\nThen you can perform progressive hash operations on the objects, ie\ninstead of hashing the intermediate hash of the objects you do it\ncontinuously (ie instead of hashing the hash of hash file a + hash file\nb + hash file c, wait for file d and then do the same, instead hash(file\na + file b + file c), when d comes compute the hash of (file a + file b\n+ file c + file d), which implies each time to keep the intermediary\nhash state because you are not going to recompute everything from the\nbeginning)\n\nI have not worked on this since some time, so that's just thoughts, but\nmaybe it can render things much more difficult than computing two files\nuntil the same hash is found\n\nThe only living example I know implementing this is the Tor protocol,\nfact apparently unknown, this is probably why nobody cares and nobody is\nwilling to take it into account (please follow bwd/fwd [1] and see [2]),\nthis is not existing in any crypto implementations, unless you hack into\nit, and this applies to progressive encryption too\n\n[1]\nhttps://lists.w3.org/Archives/Public/public-webcrypto-comments/2013Feb/0018.html\n\n\n[2] https://github.com/whatwg/streams/issues/33#issuecomment-28554151\n\n\nLe 23/02/2017 \u00e0 22:28, Peter Todd via bitcoin-dev a \u00e9crit :\n> On Thu, Feb 23, 2017 at 01:14:09PM -0500, Peter Todd via bitcoin-dev wrote:\n>> Worth noting: the impact of the SHA1 collison attack on Git is *not* limited\n>> only to maintainers making maliciously colliding Git commits, but also\n>> third-party's submitting pull-reqs containing commits, trees, and especially\n>> files for which collisions have been found. This is likely to be exploitable in\n>> practice with binary files, as reviewers aren't going to necessarily notice\n>> garbage at the end of a file needed for the attack; if the attack can be\n>> extended to constricted character sets like unicode or ASCII, we're in trouble\n>> in general.\n>>\n>> Concretely, I could prepare a pair of files with the same SHA1 hash, taking\n>> into account the header that Git prepends when hashing files. I'd then submit\n>> that pull-req to a project with the \"clean\" version of that file. Once the\n>> maintainer merges my pull-req, possibly PGP signing the git commit, I then take\n>> that signature and distribute the same repo, but with the \"clean\" version\n>> replaced by the malicious version of the file.\n> Thinking about this a bit more, the most concerning avenue of attack is likely\n> to be tree objects, as I'll bet you you can construct tree objs with garbage at\n> the end that many review tools don't pick up on. :(\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170224/7bfa2399/attachment.html>"
            },
            {
                "author": "Tim Ruffing",
                "date": "2017-02-24T10:04:54",
                "message_text_only": "On Fri, 2017-02-24 at 00:57 +0100, Aymeric Vitte via bitcoin-dev wrote:\n> \n> I have not worked on this since some time, so that's just thoughts,\n> but maybe it can render things much more difficult\n> than\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0computing two files until the same hash is found\n> \n\nYou basically rely on the idea that specific collisions are more\ndifficult to find.\u00a0This trick or similar tricks will not help.\u00a0(And\nactually, the more files you add to the hash, the more freedom you give\nthe attacker.)\n\nEven if certain collisions are more difficult to find today (which is\ncertainly true), the general rule is that someone will prove you wrong\nin a year.\n\nEven if ignore security entirely, switching to new hash function is\nmuch simpler trying to fix the usage of a broken hash function.\n\nRelying on SHA1 is hopeless. We have to get rid of it.\n\nBest,\nTim"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-02-24T15:18:43",
                "message_text_only": "Not sure that you really read deeply what I sent, because stating that\nhashing files continuously instead of hashing the intermediate steps\njust gives more latitude to the attacker can't be true when the attacker\nhas absolutely no control over the past files\n\nI did not write this as a workaround to fix SHA1, which will be dead\nsoon or later but as maybe some general concept that could possibly help\nwhatever hash function you are using for objects that are not frozen but\nextending (ie the original email stating that trees might be some kind\nof worse candidates for collisions reminded me this), indeed it makes no\nsense to patch SHA1 or play around, but this kind of proposal could\naccompany the defunct\n\nThe drawback is that you have to keep the hash state when you close the\nlatest hash computation in order to start the next one\n\nThen the question is: knowing the hash state, is it as easy to find a\ncollision between two files that will be computed in the next round than\nfinding a collision between two files only?\n\nKnowing that you can probably modify the hash state with some\nunpredictable patterns\n\nMost likely the answer is: no, it's (astronomically?) more difficult\n\nPlease take it as a suggestion that might be explored (ps: I have the\ncode for this if needed) rather than an affirmation, still amazed as\nshown in the few links provided (among others) that each time I raise\nthis subject nobody really pays attention (what's the use case?, etc)\nand by the fact that it's apparently used by only one project in the\nworld and not supported by any library\n\n\nLe 24/02/2017 \u00e0 11:04, Tim Ruffing via bitcoin-dev a \u00e9crit :\n> On Fri, 2017-02-24 at 00:57 +0100, Aymeric Vitte via bitcoin-dev wrote:\n>> I have not worked on this since some time, so that's just thoughts,\n>> but maybe it can render things much more difficult\n>> than       computing two files until the same hash is found\n>>\n> You basically rely on the idea that specific collisions are more\n> difficult to find. This trick or similar tricks will not help. (And\n> actually, the more files you add to the hash, the more freedom you give\n> the attacker.)\n>\n> Even if certain collisions are more difficult to find today (which is\n> certainly true), the general rule is that someone will prove you wrong\n> in a year.\n>\n> Even if ignore security entirely, switching to new hash function is\n> much simpler trying to fix the usage of a broken hash function.\n>\n> Relying on SHA1 is hopeless. We have to get rid of it.\n>\n> Best,\n> Tim\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms"
            },
            {
                "author": "Tim Ruffing",
                "date": "2017-02-24T16:30:49",
                "message_text_only": "On Fri, 2017-02-24 at 16:18 +0100, Aymeric Vitte via bitcoin-dev wrote:\n> Not sure that you really read deeply what I sent, because stating\n> that\n> hashing files continuously instead of hashing the intermediate steps\n> just gives more latitude to the attacker can't be true when the\n> attacker\n> has absolutely no control over the past files\nWhat prevents the attacker to provide different past files when talking\nto parties who are still in the initial state?\n\nThen the question is: knowing the hash state, is it as easy to find a\n> collision between two files that will be computed in the next round\n> than\n> finding a collision between two files only?\nWith the original usage of the hash function, the hash state is always\nthe initial state. Now that the attacker has some control over the hash\nstate even. In other words, if the original use of the hash function\nwas vulnerable, then your scheme is vulnerable for the initial state.\n\nConcrete attack: If you can find x != y with H(x) = H(y), then you can\nalso find m, x != y, with H(m||x) = H(m||y), just by setting m = \"\". \n\nNot sure if this is the right place to discuss that issue though...\n\nBest,\nTim"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-02-24T17:29:50",
                "message_text_only": "??? apparently we are not discussing the same thing\n\nMaybe I did not provide the right links (reading them again I myself\ndon't find them so clear), see maybe again\nhttps://github.com/whatwg/streams/issues/33#issuecomment-28045860\n\na - b - c -d\n\nhash(a)\n\nhash(a+b)\n\netc\n\nBut you are not going to rehash from the beginning, then:\n\nupdate a --> keep the remaining bytes a_ (+ hash state 1) --> digest\na=hash(a)\n\nupdate a_+b from hash state 1--> keep the remaining bytes b_ (+ hash\nstate 2) --> digest a_+b=hash(a+b)\n\netc\n\nBasically that's similar to a real time progressive hash of chunks of a\nfile that you are streaming and therefore don't know what will come next\n(per opposition to hashing a file that you already have), this could\napply to trees\n\nThis is different from something like:\n\nhash(a)\n\nhash(hash(a) +hash(b))\n\netc\n\nThere is no initial state, and the attacker can't modify what was\nalready hashed, to make it more difficult you can probably modify the\nhash state N\n\n\nLe 24/02/2017 \u00e0 17:30, Tim Ruffing via bitcoin-dev a \u00e9crit :\n> On Fri, 2017-02-24 at 16:18 +0100, Aymeric Vitte via bitcoin-dev wrote:\n>> Not sure that you really read deeply what I sent, because stating\n>> that\n>> hashing files continuously instead of hashing the intermediate steps\n>> just gives more latitude to the attacker can't be true when the\n>> attacker\n>> has absolutely no control over the past files\n> What prevents the attacker to provide different past files when talking\n> to parties who are still in the initial state?\n>\n> Then the question is: knowing the hash state, is it as easy to find a\n>> collision between two files that will be computed in the next round\n>> than\n>> finding a collision between two files only?\n> With the original usage of the hash function, the hash state is always\n> the initial state. Now that the attacker has some control over the hash\n> state even. In other words, if the original use of the hash function\n> was vulnerable, then your scheme is vulnerable for the initial state.\n>\n> Concrete attack: If you can find x != y with H(x) = H(y), then you can\n> also find m, x != y, with H(m||x) = H(m||y), just by setting m = \"\". \n>\n> Not sure if this is the right place to discuss that issue though...\n>\n> Best,\n> Tim\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms"
            },
            {
                "author": "Steve Davis",
                "date": "2017-02-24T23:49:36",
                "message_text_only": "If the 20 byte SHA1 is now considered insecure (with good reason), what about RIPEMD-160 which is the foundation of Bitcoin addresses?\n\nIs that also susceptible to such an attack vector?\n\nWhat does that mean for old addresses?\n\netc\n\n/s\n\n\n> Date: Fri, 24 Feb 2017 11:04:54 +0100\n> From: Tim Ruffing <tim.ruffing at mmci.uni-saarland.de>\n> To: bitcoin-dev at lists.linuxfoundation.org\n> Subject: Re: [bitcoin-dev] SHA1 collisions make Git vulnerable to\n> \tattakcs by third-parties, not just repo maintainers\n> Message-ID: <1487930694.1528.1.camel at mmci.uni-saarland.de>\n> Content-Type: text/plain; charset=\"UTF-8\"\n> \n> On Fri, 2017-02-24 at 00:57 +0100, Aymeric Vitte via bitcoin-dev wrote:\n>> \n>> I have not worked on this since some time, so that's just thoughts,\n>> but maybe it can render things much more difficult\n>> than???????computing two files until the same hash is found\n>> \n> \n> You basically rely on the idea that specific collisions are more\n> difficult to find.?This trick or similar tricks will not help.?(And\n> actually, the more files you add to the hash, the more freedom you give\n> the attacker.)\n> \n> Even if certain collisions are more difficult to find today (which is\n> certainly true), the general rule is that someone will prove you wrong\n> in a year.\n> \n> Even if ignore security entirely, switching to new hash function is\n> much simpler trying to fix the usage of a broken hash function.\n> \n> Relying on SHA1 is hopeless. We have to get rid of it.\n> \n> Best,\n> Tim\n> \n> \n> \n> \n> \n> ------------------------------\n> \n> Message: 2\n> Date: Fri, 24 Feb 2017 16:18:43 +0100\n> From: Aymeric Vitte <vitteaymeric at gmail.com>\n> To: bitcoin-dev at lists.linuxfoundation.org\n> Subject: Re: [bitcoin-dev] SHA1 collisions make Git vulnerable to\n> \tattakcs by third-parties, not just repo maintainers\n> Message-ID: <15848c1b-2873-35e8-0588-c636126257df at gmail.com>\n> Content-Type: text/plain; charset=utf-8\n> \n> Not sure that you really read deeply what I sent, because stating that\n> hashing files continuously instead of hashing the intermediate steps\n> just gives more latitude to the attacker can't be true when the attacker\n> has absolutely no control over the past files\n> \n> I did not write this as a workaround to fix SHA1, which will be dead\n> soon or later but as maybe some general concept that could possibly help\n> whatever hash function you are using for objects that are not frozen but\n> extending (ie the original email stating that trees might be some kind\n> of worse candidates for collisions reminded me this), indeed it makes no\n> sense to patch SHA1 or play around, but this kind of proposal could\n> accompany the defunct\n> \n> The drawback is that you have to keep the hash state when you close the\n> latest hash computation in order to start the next one\n> \n> Then the question is: knowing the hash state, is it as easy to find a\n> collision between two files that will be computed in the next round than\n> finding a collision between two files only?\n> \n> Knowing that you can probably modify the hash state with some\n> unpredictable patterns\n> \n> Most likely the answer is: no, it's (astronomically?) more difficult\n> \n> Please take it as a suggestion that might be explored (ps: I have the\n> code for this if needed) rather than an affirmation, still amazed as\n> shown in the few links provided (among others) that each time I raise\n> this subject nobody really pays attention (what's the use case?, etc)\n> and by the fact that it's apparently used by only one project in the\n> world and not supported by any library\n> \n> \n> Le 24/02/2017 ? 11:04, Tim Ruffing via bitcoin-dev a ?crit :\n>> On Fri, 2017-02-24 at 00:57 +0100, Aymeric Vitte via bitcoin-dev wrote:\n>>> I have not worked on this since some time, so that's just thoughts,\n>>> but maybe it can render things much more difficult\n>>> than       computing two files until the same hash is found\n>>> \n>> You basically rely on the idea that specific collisions are more\n>> difficult to find. This trick or similar tricks will not help. (And\n>> actually, the more files you add to the hash, the more freedom you give\n>> the attacker.)\n>> \n>> Even if certain collisions are more difficult to find today (which is\n>> certainly true), the general rule is that someone will prove you wrong\n>> in a year.\n>> \n>> Even if ignore security entirely, switching to new hash function is\n>> much simpler trying to fix the usage of a broken hash function.\n>> \n>> Relying on SHA1 is hopeless. We have to get rid of it.\n>> \n>> Best,\n>> Tim\n>> \n>> \n>> \n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> -- \n> Zcash wallets made simple: https://github.com/Ayms/zcash-wallets\n> Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\n> Get the torrent dynamic blocklist: http://peersm.com/getblocklist\n> Check the 10 M passwords list: http://peersm.com/findmyass\n> Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org\n> Peersm : http://www.peersm.com\n> torrent-live: https://github.com/Ayms/torrent-live\n> node-Tor : https://www.github.com/Ayms/node-Tor\n> GitHub : https://www.github.com/Ayms\n> \n> \n> \n> ------------------------------\n> \n> Message: 3\n> Date: Fri, 24 Feb 2017 17:30:49 +0100\n> From: Tim Ruffing <tim.ruffing at mmci.uni-saarland.de>\n> To: bitcoin-dev at lists.linuxfoundation.org\n> Subject: Re: [bitcoin-dev] SHA1 collisions make Git vulnerable to\n> \tattakcs by third-parties, not just repo maintainers\n> Message-ID: <1487953849.5148.2.camel at mmci.uni-saarland.de>\n> Content-Type: text/plain; charset=\"UTF-8\"\n> \n> On Fri, 2017-02-24 at 16:18 +0100, Aymeric Vitte via bitcoin-dev wrote:\n>> Not sure that you really read deeply what I sent, because stating\n>> that\n>> hashing files continuously instead of hashing the intermediate steps\n>> just gives more latitude to the attacker can't be true when the\n>> attacker\n>> has absolutely no control over the past files\n> What prevents the attacker to provide different past files when talking\n> to parties who are still in the initial state?\n> \n> Then the question is: knowing the hash state, is it as easy to find a\n>> collision between two files that will be computed in the next round\n>> than\n>> finding a collision between two files only?\n> With the original usage of the hash function, the hash state is always\n> the initial state. Now that the attacker has some control over the hash\n> state even. In other words, if the original use of the hash function\n> was vulnerable, then your scheme is vulnerable for the initial state.\n> \n> Concrete attack: If you can find x != y with H(x) = H(y), then you can\n> also find m, x != y, with H(m||x) = H(m||y), just by setting m = \"\". \n> \n> Not sure if this is the right place to discuss that issue though...\n> \n> Best,\n> Tim\n> \n> \n> ------------------------------\n> \n> Message: 4\n> Date: Fri, 24 Feb 2017 18:29:50 +0100\n> From: Aymeric Vitte <vitteaymeric at gmail.com>\n> To: Tim Ruffing <tim.ruffing at mmci.uni-saarland.de>,\tBitcoin Protocol\n> \tDiscussion <bitcoin-dev at lists.linuxfoundation.org>\n> Subject: Re: [bitcoin-dev] SHA1 collisions make Git vulnerable to\n> \tattakcs by third-parties, not just repo maintainers\n> Message-ID: <b557a0de-2492-80a1-eff7-229503ae382d at gmail.com>\n> Content-Type: text/plain; charset=windows-1252\n> \n> ??? apparently we are not discussing the same thing\n> \n> Maybe I did not provide the right links (reading them again I myself\n> don't find them so clear), see maybe again\n> https://github.com/whatwg/streams/issues/33#issuecomment-28045860\n> \n> a - b - c -d\n> \n> hash(a)\n> \n> hash(a+b)\n> \n> etc\n> \n> But you are not going to rehash from the beginning, then:\n> \n> update a --> keep the remaining bytes a_ (+ hash state 1) --> digest\n> a=hash(a)\n> \n> update a_+b from hash state 1--> keep the remaining bytes b_ (+ hash\n> state 2) --> digest a_+b=hash(a+b)\n> \n> etc\n> \n> Basically that's similar to a real time progressive hash of chunks of a\n> file that you are streaming and therefore don't know what will come next\n> (per opposition to hashing a file that you already have), this could\n> apply to trees\n> \n> This is different from something like:\n> \n> hash(a)\n> \n> hash(hash(a) +hash(b))\n> \n> etc\n> \n> There is no initial state, and the attacker can't modify what was\n> already hashed, to make it more difficult you can probably modify the\n> hash state N\n> \n> \n> Le 24/02/2017 ? 17:30, Tim Ruffing via bitcoin-dev a ?crit :\n>> On Fri, 2017-02-24 at 16:18 +0100, Aymeric Vitte via bitcoin-dev wrote:\n>>> Not sure that you really read deeply what I sent, because stating\n>>> that\n>>> hashing files continuously instead of hashing the intermediate steps\n>>> just gives more latitude to the attacker can't be true when the\n>>> attacker\n>>> has absolutely no control over the past files\n>> What prevents the attacker to provide different past files when talking\n>> to parties who are still in the initial state?\n>> \n>> Then the question is: knowing the hash state, is it as easy to find a\n>>> collision between two files that will be computed in the next round\n>>> than\n>>> finding a collision between two files only?\n>> With the original usage of the hash function, the hash state is always\n>> the initial state. Now that the attacker has some control over the hash\n>> state even. In other words, if the original use of the hash function\n>> was vulnerable, then your scheme is vulnerable for the initial state.\n>> \n>> Concrete attack: If you can find x != y with H(x) = H(y), then you can\n>> also find m, x != y, with H(m||x) = H(m||y), just by setting m = \"\". \n>> \n>> Not sure if this is the right place to discuss that issue though...\n>> \n>> Best,\n>> Tim\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> -- \n> Zcash wallets made simple: https://github.com/Ayms/zcash-wallets\n> Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\n> Get the torrent dynamic blocklist: http://peersm.com/getblocklist\n> Check the 10 M passwords list: http://peersm.com/findmyass\n> Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org\n> Peersm : http://www.peersm.com\n> torrent-live: https://github.com/Ayms/torrent-live\n> node-Tor : https://www.github.com/Ayms/node-Tor\n> GitHub : https://www.github.com/Ayms\n> \n> \n> \n> ------------------------------\n> \n> Message: 5\n> Date: Fri, 24 Feb 2017 14:20:19 -0800\n> From: Bram Cohen <bram at bittorrent.com>\n> To: Peter Todd <pete at petertodd.org>\n> Cc: Bitcoin Protocol Discussion\n> \t<bitcoin-dev at lists.linuxfoundation.org>\n> Subject: Re: [bitcoin-dev] A Better MMR Definition\n> Message-ID:\n> \t<CA+KqGkpi4GvgU-K6vt-U5ZN4AkpjZ0rruzddoJS4-V0TcnyqUQ at mail.gmail.com>\n> Content-Type: text/plain; charset=\"utf-8\"\n> \n> So your idea is to cluster entries by entry time because newer things are\n> more likely to leave and updating multiple things near each other is\n> cheaper?\n> \n> That can be done with my tool. Instead of using hashes for the values being\n> stored, you use position entries. The first entry gets a value of all\n> zeros, the next one a one followed by all zeros, then the next two\n> correspond to the first two with the second bit flipped to one, then the\n> next four the first four with the third bit flipped to one, etc. It\n> probably performs a little bit better to do it two bits at a time instead\n> of one so that the entries are 00, 01, 10, 11, 0001, 0010, 0011, 0101,\n> 0110, 0111, 1001, etc. If you were to really use this you'd probably want\n> to to add some optimizations to use the fact that the terminals fit in 64\n> bits instead of 256, but it mostly works unchanged, and gets whatever\n> benefits there are to this clustering plus the high performance\n> implementation tricks I've built which I keep complaining that nobody's\n> giving feedback on.\n> \n> I'm not sold on this being a win: The empirical access patterns are\n> unknown, it requires an extra cache miss per lookup to find the entry\n> number, it may be that everything is optimized well enough without it for\n> there to be no meaningful gains, and it's a bunch of extra complexity. What\n> should be done is that a plain vanilla UTXO set solution is optimized as\n> well as it can be first, and then the insertion ordering trick is tried as\n> an optimization to see if it's an improvement. Without that baseline\n> there's no meaningful basis for comparison, and I'm quite confident that a\n> naive implementation which just allocates individual nodes will\n> underperform the thing I've come up with, even without adding optimizations\n> related to fitting in 64 bits.\n> \n> On Thu, Feb 23, 2017 at 8:36 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n>> On Thu, Feb 23, 2017 at 07:32:43PM -0800, Bram Cohen wrote:\n>>> On Thu, Feb 23, 2017 at 7:15 PM, Peter Todd <pete at petertodd.org> wrote:\n>>> \n>>>> \n>>>> Glad we're on the same page with regard to what's possible in TXO\n>>>> commitments.\n>>>> \n>>>> Secondly, am I correct in saying your UTXO commitments scheme requires\n>>>> random\n>>>> access? While you describe it as a \"merkle set\", obviously to be\n>> merkelized\n>>>> it'll have to have an ordering of some kind. What do you propose that\n>>>> ordering\n>>>> to be?\n>>>> \n>>> \n>>> The ordering is by the bits in the hash. Technically it's a Patricia\n>> Trie.\n>>> I'm using 'merkle tree' to refer to basically anything with a hash root.\n>> \n>> The hash of what? The values in the set?\n>> \n>>>> Maybe more specifically, what exact values do you propose to be in the\n>> set?\n>>>> \n>>>> \n>>> That is unspecified in the implementation, it just takes a 256 bit value\n>>> which is presumably a hash of something. The intention is to nail down a\n>>> simple format and demonstrate good performance and leave those semantics\n>> to\n>>> a higher layer. The simplest thing would be to hash together the txid and\n>>> output number.\n>> \n>> Ok, so let's assume the values in the set are the unspent outpoints.\n>> \n>> Since we're ordering by the hash of the values in the set, outpoints will\n>> be\n>> distributed uniformly in the set, and thus the access pattern of data in\n>> the\n>> set is uniform.\n>> \n>> Now let's fast-forward 10 years. For the sake of argument, assume that for\n>> every 1 UTXO in the set that corresponds to funds in someone's wallet that\n>> are\n>> likely to be spent, there are 2^12 = 4096 UTXO's that have been permanently\n>> lost (and/or created in spam attacks) and thus will never be spent.\n>> \n>> Since lost UTXO's are *also* uniformly distributed, if I'm processing a new\n>> block that spends 2^12 = 4096 UTXO's, on average for each UTXO spent, I'll\n>> have to update log2(4096) = 12 more digests than I would have had those\n>> \"dead\"\n>> UTXO's not existed.\n>> \n>> Concretely, imagine our UTXO set had just 8 values in it, and we were\n>> updating\n>> two of them:\n>> \n>>               #\n>>              / \\\n>>             /   \\\n>>            /     \\\n>>           /       \\\n>>          /         \\\n>>         #           #\n>>        / \\         / \\\n>>       /   \\       /   \\\n>>      #     .     .     #\n>>     / \\   / \\   / \\   / \\\n>>    .   X .   . .   . X   .\n>> \n>> To mark two coins as spent, we've had to update 5 inner nodes.\n>> \n>> \n>> Now let's look at what happens in an insertion-ordered TXO commitment\n>> scheme.\n>> For sake of argument, let's assume the best possible case, where every UTXO\n>> spent in that same block was recently created. Since the UTXO's are\n>> recently\n>> created, chances are almost every single one of those \"dead\" UTXO's will\n>> have\n>> been created in the past. Thus, since this is an insertion-ordered data\n>> structure, those UTXO's exist in an older part of the data structure that\n>> our\n>> new block doesn't need to modify at all.\n>> \n>> Concretely, again let's imagine a TXO commitment with 8 values in it, and\n>> two\n>> of them being spent:\n>> \n>>               #\n>>              / \\\n>>             /   \\\n>>            /     \\\n>>           /       \\\n>>          /         \\\n>>         .           #\n>>        / \\         / \\\n>>       /   \\       /   \\\n>>      .     .     .     #\n>>     / \\   / \\   / \\   / \\\n>>    .   . .   . .   . X   X\n>> \n>> To mark two coins as spent, we've only had to update 3 inner nodes; while\n>> our\n>> tree is higher with those lost coins, those extra inner nodes are amortised\n>> across all the coins we have to update.\n>> \n>> \n>> The situation gets even better when we look at the *new* UTXO's that our\n>> block\n>> creates. Suppose our UTXO set has size n. To mark a single coin as spent,\n>> we\n>> have to update log2(n) inner nodes. We do get to amortise this a bit at\n>> the top\n>> levels in the tree, but even if we assume the amortisation is totally free,\n>> we're updating at least log2(n) - log2(m) inner nodes \"under\" the amortised\n>> nodes at the top of the tree for *each* new node.\n>> \n>> Meanwhile with an insertion-ordered TXO commitment, each new UTXO added to\n>> the\n>> data set goes in the same place - the end. So almost none of the existing\n>> data\n>> needs to be touched to add the new UTXOs. Equally, the hashing required\n>> for the\n>> new UTXO's can be done in an incremental fashion that's very L1/L2 cache\n>> friendly.\n>> \n>> \n>> tl;dr: Precisely because access patterns in TXO commitments are *not*\n>> uniform,\n>> I think we'll find that from a L1/L2/etc cache perspective alone, TXO\n>> commitments will result in better performance than UTXO commitments.\n>> \n>> \n>> Now it is true that Bitcoin's current design means we'll need a map of\n>> confirmed outpoints to TXO insertion order indexes. But it's not\n>> particularly\n>> hard to add that \"metadata\" to transactions on the P2P layer in the same\n>> way\n>> that segwit added witnesses to transactions without modifying how txids\n>> were\n>> calculated; if you only connect to peers who provide you with TXO index\n>> information in blocks and transactions, you don't need to keep that map\n>> yourself.\n>> \n>> Finally, note how this makes transactions *smaller* in many circumstances:\n>> it's\n>> just a 8-byte max index rather than a 40 byte outpoint.\n>> \n>> --\n>> https://petertodd.org 'peter'[:-1]@petertodd.org\n>> \n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170224/63ab2731/attachment.html>\n> \n> ------------------------------\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> End of bitcoin-dev Digest, Vol 21, Issue 34\n> *******************************************"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-25T01:01:22",
                "message_text_only": "On Fri, Feb 24, 2017 at 05:49:36PM -0600, Steve Davis via bitcoin-dev wrote:\n> If the 20 byte SHA1 is now considered insecure (with good reason), what about RIPEMD-160 which is the foundation of Bitcoin addresses?\n\nSHA1 is insecure because the SHA1 algorithm is insecure, not because 160bits isn't enough.\n\nAFAIK there aren't any known weaknesses in RIPEMD160, but it also hasn't been\nas closely studied as more common hash algorithms. That said, Bitcoin uses\nRIPEMD160(SHA256(msg)), which may make creating collisions harder if an attack\nis found than if it used RIPEMD160 alone.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170224/85794f98/attachment.sig>"
            },
            {
                "author": "Steve Davis",
                "date": "2017-02-25T12:04:28",
                "message_text_only": "> On Feb 24, 2017, at 7:01 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> On Fri, Feb 24, 2017 at 05:49:36PM -0600, Steve Davis via bitcoin-dev wrote:\n>> If the 20 byte SHA1 is now considered insecure (with good reason), what about RIPEMD-160 which is the foundation of Bitcoin addresses?\n> \n> SHA1 is insecure because the SHA1 algorithm is insecure, not because 160bits isn't enough.\n> \n> AFAIK there aren't any known weaknesses in RIPEMD160,\n\n\u2026so far. I wonder how long that vacation will last?\n\n> but it also hasn't been\n> as closely studied as more common hash algorithms.\n\n...but we can be sure that it will be, since the dollar value held in existing utxos continues to increase...\n\n> That said, Bitcoin uses\n> RIPEMD160(SHA256(msg)), which may make creating collisions harder if an attack\n> is found than if it used RIPEMD160 alone.\n\nDoes that offer any greater protection? That\u2019s not so clear to me as the outputs (at least for p2pkh) only verify the public key against the final 20 byte hash. Specifically, in the first (notional) case the challenge would be to find a private key that has a public key that hashes to the final hash. In the second (realistic) case, you merely need to add the sha256 hash into the problem, which doesn\u2019t seem to me to increase the difficulty by any significant amount? \n\n\n/s"
            },
            {
                "author": "Leandro Coutinho",
                "date": "2017-02-25T14:50:30",
                "message_text_only": "Google recommeds \"migrate to safer cryptographic hashes such as SHA-256 and\nSHA-3\"\nIt does not mention RIPEMD-160\n\nhttps://security.googleblog.com/2017/02/announcing-first-sha1-collision.html?m=1\n\n\nEm 25/02/2017 10:47, \"Steve Davis via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> escreveu:\n\n\n> On Feb 24, 2017, at 7:01 PM, Peter Todd <pete at petertodd.org> wrote:\n>\n> On Fri, Feb 24, 2017 at 05:49:36PM -0600, Steve Davis via bitcoin-dev\nwrote:\n>> If the 20 byte SHA1 is now considered insecure (with good reason), what\nabout RIPEMD-160 which is the foundation of Bitcoin addresses?\n>\n> SHA1 is insecure because the SHA1 algorithm is insecure, not because\n160bits isn't enough.\n>\n> AFAIK there aren't any known weaknesses in RIPEMD160,\n\n\u2026so far. I wonder how long that vacation will last?\n\n> but it also hasn't been\n> as closely studied as more common hash algorithms.\n\n...but we can be sure that it will be, since the dollar value held in\nexisting utxos continues to increase...\n\n> That said, Bitcoin uses\n> RIPEMD160(SHA256(msg)), which may make creating collisions harder if an\nattack\n> is found than if it used RIPEMD160 alone.\n\nDoes that offer any greater protection? That\u2019s not so clear to me as the\noutputs (at least for p2pkh) only verify the public key against the final\n20 byte hash. Specifically, in the first (notional) case the challenge\nwould be to find a private key that has a public key that hashes to the\nfinal hash. In the second (realistic) case, you merely need to add the\nsha256 hash into the problem, which doesn\u2019t seem to me to increase the\ndifficulty by any significant amount?\n\n\n/s\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/30468d23/attachment.html>"
            },
            {
                "author": "Ethan Heilman",
                "date": "2017-02-25T16:10:02",
                "message_text_only": ">SHA1 is insecure because the SHA1 algorithm is insecure, not because\n160bits isn't enough.\n\nI would argue that 160-bits isn't enough for collision resistance. Assuming\nRIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle), collisions\ncan be generated in 2^80 queries (actually detecting these collisions\nrequires some time-memory additional trade-offs). The Bitcoin network at\nthe current hash rate performs roughly SHA-256 ~2^78 queries a day or 2^80\nqueries every four days. Without any break in RIPEMD-160(SHA-256(msg)) the\nUS could build an ASIC datacenter and produce RIPEMD-160 collisions for a\nfraction of its yearly cryptologic budget.\n\nThe impact of collisions in RIPEMD-160(SHA-256(msg)) according to \"On\nBitcoin Security in the Presence of Broken Crypto Primitives\"(\nhttps://eprint.iacr.org/2016/167.pdf):\n\n>Collisions are similar, though in this case both public keys are under the\nadversary\u2019s control, and again the adversary does not have access to the\nprivate keys. In both scenarios, there is a question of nonrepudiation\nexternal to the protocol itself: by presenting a second pre-image of a key\nused to sign a transaction, a user/adversary can claim that his coins were\nstolen.\n\nHow would such an event effect the price of Bitcoin when headlines are\n\"Bitcoin's Cryptography Broken\"? How much money could someone make by\nplaying the market in this way?\n\nFor both reasons of credibility and good engineering (safety\nmargins) Bitcoin should strive to always use cryptography which is beyond\nreproach.\n\n\nOn Sat, Feb 25, 2017 at 9:50 AM, Leandro Coutinho via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Google recommeds \"migrate to safer cryptographic hashes such as SHA-256\n> and SHA-3\"\n> It does not mention RIPEMD-160\n>\n> https://security.googleblog.com/2017/02/announcing-first-\n> sha1-collision.html?m=1\n>\n>\n> Em 25/02/2017 10:47, \"Steve Davis via bitcoin-dev\" <bitcoin-dev at lists.\n> linuxfoundation.org> escreveu:\n>\n>\n> > On Feb 24, 2017, at 7:01 PM, Peter Todd <pete at petertodd.org> wrote:\n> >\n> > On Fri, Feb 24, 2017 at 05:49:36PM -0600, Steve Davis via bitcoin-dev\n> wrote:\n> >> If the 20 byte SHA1 is now considered insecure (with good reason), what\n> about RIPEMD-160 which is the foundation of Bitcoin addresses?\n> >\n> > SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> 160bits isn't enough.\n> >\n> > AFAIK there aren't any known weaknesses in RIPEMD160,\n>\n> \u2026so far. I wonder how long that vacation will last?\n>\n> > but it also hasn't been\n> > as closely studied as more common hash algorithms.\n>\n> ...but we can be sure that it will be, since the dollar value held in\n> existing utxos continues to increase...\n>\n> > That said, Bitcoin uses\n> > RIPEMD160(SHA256(msg)), which may make creating collisions harder if an\n> attack\n> > is found than if it used RIPEMD160 alone.\n>\n> Does that offer any greater protection? That\u2019s not so clear to me as the\n> outputs (at least for p2pkh) only verify the public key against the final\n> 20 byte hash. Specifically, in the first (notional) case the challenge\n> would be to find a private key that has a public key that hashes to the\n> final hash. In the second (realistic) case, you merely need to add the\n> sha256 hash into the problem, which doesn\u2019t seem to me to increase the\n> difficulty by any significant amount?\n>\n>\n> /s\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/99b5f0a3/attachment.html>"
            },
            {
                "author": "Shin'ichiro Matsuo",
                "date": "2017-02-25T17:45:36",
                "message_text_only": "We should distinguish collision resistance from 2nd pre-image resistance, in general.\n\nAs previously written, we should care both hash output length and algorithm itself. The weakness of SHA-0 (preliminary version of SHA-1) was reported in 2004, then many research on the structure of SHA-1 were conducted. In the case of SHA-2, it is harder than SHA-1 to find collisions.\n\nExisting security consideration and evaluation criteria were extensively discussed in the NIST SHA-3 competition. Please see the following sites.\n\nhttps://ehash.iaik.tugraz.at/wiki/The_SHA-3_Zoo\nhttps://ehash.iaik.tugraz.at/wiki/Cryptanalysis_Categories\n\nWe need similar analysis on RIPEMD160 and impacts of attacks on (RIPEMD160(SHA2(msg)). \n\nWe can also refer the security assumption of hash chain in Asiacrypt 2004 Paper. \nhttps://home.cyber.ee/~ahtbu/timestampsec.pdf\n\nIn the discussion of SHA3 competition, we choose another hash design structure, so called \"sponge structure.\" This leads diversity of design principles of hash function and gives resilience even when one hash design structure becomes vulnerable. As Peter Todd wrote, discussion on design structure and algorithm is important. Discussions on all of algorithm, output length and security requirements are needed.\n\nAt some future moment, we should think about transition of underlying hash functions. I\u2019m working on this subject and will present an idea at IEEE S&B.\n\nShin\u2019ichiro Matsuo\n\n\n> On Feb 25, 2017, at 8:10 AM, Ethan Heilman via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> >SHA1 is insecure because the SHA1 algorithm is insecure, not because 160bits isn't enough.\n> \n> I would argue that 160-bits isn't enough for collision resistance. Assuming RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle), collisions can be generated in 2^80 queries (actually detecting these collisions requires some time-memory additional trade-offs). The Bitcoin network at the current hash rate performs roughly SHA-256 ~2^78 queries a day or 2^80 queries every four days. Without any break in RIPEMD-160(SHA-256(msg)) the US could build an ASIC datacenter and produce RIPEMD-160 collisions for a fraction of its yearly cryptologic budget.\n> \n> The impact of collisions in RIPEMD-160(SHA-256(msg)) according to \"On Bitcoin Security in the Presence of Broken Crypto Primitives\"(https://eprint.iacr.org/2016/167.pdf):\n> \n> >Collisions are similar, though in this case both public keys are under the adversary\u2019s control, and again the adversary does not have access to the private keys. In both scenarios, there is a question of nonrepudiation external to the protocol itself: by presenting a second pre-image of a key used to sign a transaction, a user/adversary can claim that his coins were stolen. \n> \n> How would such an event effect the price of Bitcoin when headlines are \"Bitcoin's Cryptography Broken\"? How much money could someone make by playing the market in this way? \n> \n> For both reasons of credibility and good engineering (safety margins) Bitcoin should strive to always use cryptography which is beyond reproach.\n> \n> \n> On Sat, Feb 25, 2017 at 9:50 AM, Leandro Coutinho via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Google recommeds \"migrate to safer cryptographic hashes such as SHA-256 and SHA-3\"\n> It does not mention RIPEMD-160\n> \n> https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html?m=1\n> \n> \n> Em 25/02/2017 10:47, \"Steve Davis via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org> escreveu:\n> \n> > On Feb 24, 2017, at 7:01 PM, Peter Todd <pete at petertodd.org> wrote:\n> >\n> > On Fri, Feb 24, 2017 at 05:49:36PM -0600, Steve Davis via bitcoin-dev wrote:\n> >> If the 20 byte SHA1 is now considered insecure (with good reason), what about RIPEMD-160 which is the foundation of Bitcoin addresses?\n> >\n> > SHA1 is insecure because the SHA1 algorithm is insecure, not because 160bits isn't enough.\n> >\n> > AFAIK there aren't any known weaknesses in RIPEMD160,\n> \n> \u2026so far. I wonder how long that vacation will last?\n> \n> > but it also hasn't been\n> > as closely studied as more common hash algorithms.\n> \n> ...but we can be sure that it will be, since the dollar value held in existing utxos continues to increase...\n> \n> > That said, Bitcoin uses\n> > RIPEMD160(SHA256(msg)), which may make creating collisions harder if an attack\n> > is found than if it used RIPEMD160 alone.\n> \n> Does that offer any greater protection? That\u2019s not so clear to me as the outputs (at least for p2pkh) only verify the public key against the final 20 byte hash. Specifically, in the first (notional) case the challenge would be to find a private key that has a public key that hashes to the final hash. In the second (realistic) case, you merely need to add the sha256 hash into the problem, which doesn\u2019t seem to me to increase the difficulty by any significant amount?\n> \n> \n> /s\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Henning Kopp",
                "date": "2017-02-27T09:15:29",
                "message_text_only": "Hi all,\n\nI did not follow the whole discussion, but wanted to throw in some\nliterature on the failure of crypto primitives in Bitcoin.\n\nThere is a paper which discusses the problems, but does not give any\nremedies: https://eprint.iacr.org/2016/167.pdf\n\nAnd there are also contingency plans on the wiki:\nhttps://en.bitcoin.it/wiki/Contingency_plans These are not very\ndetailed and my impression is that this information should be viewed\nvery critically (E.g., when ECDSA is broken, the suggested vague\nresponse is \"Switch to the stronger algorithm.\" Yeah. And \"Code for\nall of this should be prepared.\" Surely. As far as I know, there is no\nsuch code and no-one is working on it).\n\nBest,\nHenning\n\n\nOn Sat, Feb 25, 2017 at 09:45:36AM -0800, Shin'ichiro Matsuo via bitcoin-dev wrote:\n> We should distinguish collision resistance from 2nd pre-image resistance, in general.\n> \n> As previously written, we should care both hash output length and algorithm itself. The weakness of SHA-0 (preliminary version of SHA-1) was reported in 2004, then many research on the structure of SHA-1 were conducted. In the case of SHA-2, it is harder than SHA-1 to find collisions.\n> \n> Existing security consideration and evaluation criteria were extensively discussed in the NIST SHA-3 competition. Please see the following sites.\n> \n> https://ehash.iaik.tugraz.at/wiki/The_SHA-3_Zoo\n> https://ehash.iaik.tugraz.at/wiki/Cryptanalysis_Categories\n> \n> We need similar analysis on RIPEMD160 and impacts of attacks on (RIPEMD160(SHA2(msg)). \n> \n> We can also refer the security assumption of hash chain in Asiacrypt 2004 Paper. \n> https://home.cyber.ee/~ahtbu/timestampsec.pdf\n> \n> In the discussion of SHA3 competition, we choose another hash design structure, so called \"sponge structure.\" This leads diversity of design principles of hash function and gives resilience even when one hash design structure becomes vulnerable. As Peter Todd wrote, discussion on design structure and algorithm is important. Discussions on all of algorithm, output length and security requirements are needed.\n> \n> At some future moment, we should think about transition of underlying hash functions. I\u2019m working on this subject and will present an idea at IEEE S&B.\n> \n> Shin\u2019ichiro Matsuo\n> \n> \n> > On Feb 25, 2017, at 8:10 AM, Ethan Heilman via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > \n> > >SHA1 is insecure because the SHA1 algorithm is insecure, not because 160bits isn't enough.\n> > \n> > I would argue that 160-bits isn't enough for collision resistance. Assuming RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle), collisions can be generated in 2^80 queries (actually detecting these collisions requires some time-memory additional trade-offs). The Bitcoin network at the current hash rate performs roughly SHA-256 ~2^78 queries a day or 2^80 queries every four days. Without any break in RIPEMD-160(SHA-256(msg)) the US could build an ASIC datacenter and produce RIPEMD-160 collisions for a fraction of its yearly cryptologic budget.\n> > \n> > The impact of collisions in RIPEMD-160(SHA-256(msg)) according to \"On Bitcoin Security in the Presence of Broken Crypto Primitives\"(https://eprint.iacr.org/2016/167.pdf):\n> > \n> > >Collisions are similar, though in this case both public keys are under the adversary\u2019s control, and again the adversary does not have access to the private keys. In both scenarios, there is a question of nonrepudiation external to the protocol itself: by presenting a second pre-image of a key used to sign a transaction, a user/adversary can claim that his coins were stolen. \n> > \n> > How would such an event effect the price of Bitcoin when headlines are \"Bitcoin's Cryptography Broken\"? How much money could someone make by playing the market in this way? \n> > \n> > For both reasons of credibility and good engineering (safety margins) Bitcoin should strive to always use cryptography which is beyond reproach.\n> > \n> > \n> > On Sat, Feb 25, 2017 at 9:50 AM, Leandro Coutinho via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Google recommeds \"migrate to safer cryptographic hashes such as SHA-256 and SHA-3\"\n> > It does not mention RIPEMD-160\n> > \n> > https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html?m=1\n> > \n> > \n> > Em 25/02/2017 10:47, \"Steve Davis via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org> escreveu:\n> > \n> > > On Feb 24, 2017, at 7:01 PM, Peter Todd <pete at petertodd.org> wrote:\n> > >\n> > > On Fri, Feb 24, 2017 at 05:49:36PM -0600, Steve Davis via bitcoin-dev wrote:\n> > >> If the 20 byte SHA1 is now considered insecure (with good reason), what about RIPEMD-160 which is the foundation of Bitcoin addresses?\n> > >\n> > > SHA1 is insecure because the SHA1 algorithm is insecure, not because 160bits isn't enough.\n> > >\n> > > AFAIK there aren't any known weaknesses in RIPEMD160,\n> > \n> > \u2026so far. I wonder how long that vacation will last?\n> > \n> > > but it also hasn't been\n> > > as closely studied as more common hash algorithms.\n> > \n> > ...but we can be sure that it will be, since the dollar value held in existing utxos continues to increase...\n> > \n> > > That said, Bitcoin uses\n> > > RIPEMD160(SHA256(msg)), which may make creating collisions harder if an attack\n> > > is found than if it used RIPEMD160 alone.\n> > \n> > Does that offer any greater protection? That\u2019s not so clear to me as the outputs (at least for p2pkh) only verify the public key against the final 20 byte hash. Specifically, in the first (notional) case the challenge would be to find a private key that has a public key that hashes to the final hash. In the second (realistic) case, you merely need to add the sha256 hash into the problem, which doesn\u2019t seem to me to increase the difficulty by any significant amount?\n> > \n> > \n> > /s\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > \n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > \n> > \n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n\n-- \nHenning Kopp\nInstitute of Distributed Systems\nUlm University, Germany\n\nOffice: O27 - 3402\nPhone: +49 731 50-24138\nWeb: http://www.uni-ulm.de/in/vs/~kopp"
            },
            {
                "author": "Alice Wonder",
                "date": "2017-02-25T18:19:11",
                "message_text_only": "On 02/25/2017 08:10 AM, Ethan Heilman via bitcoin-dev wrote:\n>>SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> 160bits isn't enough.\n>\n> I would argue that 160-bits isn't enough for collision resistance.\n> Assuming RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random\n> oracle), collisions can be generated in 2^80 queries (actually detecting\n> these collisions requires some time-memory additional trade-offs). The\n> Bitcoin network at the current hash rate performs roughly SHA-256 ~2^78\n> queries a day or 2^80 queries every four days.\n\nYou have to not only produce a ripemd160 collision, you have to produce \na collision that is also a valid sha-256 hash - and that's much much \nmuch more difficult."
            },
            {
                "author": "Ethan Heilman",
                "date": "2017-02-25T18:36:49",
                "message_text_only": ">You have to not only produce a ripemd160 collision, you have to produce a\ncollision that is also a valid sha-256 hash - and that's much much much\nmore difficult.\n\nI agree that merely finding a collision in RIPEMD-160 will be hard to use\nin Bitcoin.\n\nHowever finding a collision in RIPEMD-160(SHA-256(msg)) via bruteforce\n(2^80 queries) is not particular more difficult than finding a collision in\nRIPEMD-160 via brute force. Furthermore if you find a collision in\nRIPEMD-160(SHA-256(msg)) you also get a valid SHA-256 hash for which you\nknow the preimage.\n\n\nOn Sat, Feb 25, 2017 at 1:19 PM, Alice Wonder via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 02/25/2017 08:10 AM, Ethan Heilman via bitcoin-dev wrote:\n>\n>> SHA1 is insecure because the SHA1 algorithm is insecure, not because\n>>>\n>> 160bits isn't enough.\n>>\n>> I would argue that 160-bits isn't enough for collision resistance.\n>> Assuming RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random\n>> oracle), collisions can be generated in 2^80 queries (actually detecting\n>> these collisions requires some time-memory additional trade-offs). The\n>> Bitcoin network at the current hash rate performs roughly SHA-256 ~2^78\n>> queries a day or 2^80 queries every four days.\n>>\n>\n> You have to not only produce a ripemd160 collision, you have to produce a\n> collision that is also a valid sha-256 hash - and that's much much much\n> more difficult.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/11c54e30/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-25T19:12:01",
                "message_text_only": "On Sat, Feb 25, 2017 at 11:10:02AM -0500, Ethan Heilman via bitcoin-dev wrote:\n> >SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> 160bits isn't enough.\n> \n> I would argue that 160-bits isn't enough for collision resistance. Assuming\n> RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle), collisions\n\nThat's something that we're well aware of; there have been a few discussions on\nthis list about how P2SH's 160-bits is insufficient in certain use-cases such\nas multisig.\n\nHowever, remember that a 160-bit *security level* is sufficient, and RIPEMD160\nhas 160-bit security against preimage attacks. Thus things like\npay-to-pubkey-hash are perfectly secure: sure you could generate two pubkeys\nthat have the same RIPEMD160(SHA256()) digest, but if someone does that it\ndoesn't cause the Bitcoin network itself any harm, and doing so is something\nyou choose to do to yourself.\n\nIn any case, segwit will provide a 256-bit pay-to-witness-script-hash(1), which\nprovides a 128-bit security level against collision attacks.\n\n1) https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki#Native_P2WSH\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/ffedd518/attachment.sig>"
            },
            {
                "author": "Watson Ladd",
                "date": "2017-02-25T20:42:56",
                "message_text_only": "On Sat, Feb 25, 2017 at 11:12 AM, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Sat, Feb 25, 2017 at 11:10:02AM -0500, Ethan Heilman via bitcoin-dev wrote:\n>> >SHA1 is insecure because the SHA1 algorithm is insecure, not because\n>> 160bits isn't enough.\n>>\n>> I would argue that 160-bits isn't enough for collision resistance. Assuming\n>> RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle), collisions\n>\n> That's something that we're well aware of; there have been a few discussions on\n> this list about how P2SH's 160-bits is insufficient in certain use-cases such\n> as multisig.\n>\n> However, remember that a 160-bit *security level* is sufficient, and RIPEMD160\n> has 160-bit security against preimage attacks. Thus things like\n> pay-to-pubkey-hash are perfectly secure: sure you could generate two pubkeys\n> that have the same RIPEMD160(SHA256()) digest, but if someone does that it\n> doesn't cause the Bitcoin network itself any harm, and doing so is something\n> you choose to do to yourself.\n\nP2SH is not secure against collision. I could write two scripts with\nthe same hash, one of which is an escrow script and the other which\npays it to me, have someone pay to the escrow script, and then get the\npayment. Some formal analysis tools would ignore the unused\ninstructions even if human analysis would not.\n\n>\n> In any case, segwit will provide a 256-bit pay-to-witness-script-hash(1), which\n> provides a 128-bit security level against collision attacks.\n>\n> 1) https://github.com/bitcoin/bips/blob/master/bip-0143.mediawiki#Native_P2WSH\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n\n-- \n\"Man is born free, but everywhere he is in chains\".\n--Rousseau."
            },
            {
                "author": "Russell O'Connor",
                "date": "2017-02-25T20:53:12",
                "message_text_only": "On Sat, Feb 25, 2017 at 2:12 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Feb 25, 2017 at 11:10:02AM -0500, Ethan Heilman via bitcoin-dev\n> wrote:\n> > >SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> > 160bits isn't enough.\n> >\n> > I would argue that 160-bits isn't enough for collision resistance.\n> Assuming\n> > RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle),\n> collisions\n>\n> That's something that we're well aware of; there have been a few\n> discussions on\n> this list about how P2SH's 160-bits is insufficient in certain use-cases\n> such\n> as multisig.\n>\n> However, remember that a 160-bit *security level* is sufficient, and\n> RIPEMD160\n> has 160-bit security against preimage attacks. Thus things like\n> pay-to-pubkey-hash are perfectly secure: sure you could generate two\n> pubkeys\n> that have the same RIPEMD160(SHA256()) digest, but if someone does that it\n> doesn't cause the Bitcoin network itself any harm, and doing so is\n> something\n> you choose to do to yourself.\n>\n\nBe aware that the issue is more problematic for more complex contracts.\nFor example, you are building a P2SH 2-of-2 multisig together with someone\nelse if you are not careful, party A can hand their key over to party B,\nwho can may try to generate a collision between their second key and\nanother 2-of-2 multisig where they control both keys. See\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012205.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/aec6b098/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-25T21:04:06",
                "message_text_only": "On Sat, Feb 25, 2017 at 03:53:12PM -0500, Russell O'Connor wrote:\n> On Sat, Feb 25, 2017 at 2:12 PM, Peter Todd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> > On Sat, Feb 25, 2017 at 11:10:02AM -0500, Ethan Heilman via bitcoin-dev\n> > wrote:\n> > > >SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> > > 160bits isn't enough.\n> > >\n> > > I would argue that 160-bits isn't enough for collision resistance.\n> > Assuming\n> > > RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle),\n> > collisions\n> >\n> > That's something that we're well aware of; there have been a few\n> > discussions on\n> > this list about how P2SH's 160-bits is insufficient in certain use-cases\n> > such\n> > as multisig.\n> >\n> > However, remember that a 160-bit *security level* is sufficient, and\n> > RIPEMD160\n> > has 160-bit security against preimage attacks. Thus things like\n> > pay-to-pubkey-hash are perfectly secure: sure you could generate two\n> > pubkeys\n> > that have the same RIPEMD160(SHA256()) digest, but if someone does that it\n> > doesn't cause the Bitcoin network itself any harm, and doing so is\n> > something\n> > you choose to do to yourself.\n> >\n> \n> Be aware that the issue is more problematic for more complex contracts.\n> For example, you are building a P2SH 2-of-2 multisig together with someone\n> else if you are not careful, party A can hand their key over to party B,\n> who can may try to generate a collision between their second key and\n> another 2-of-2 multisig where they control both keys. See\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012205.html\n\nI'm very aware of that, in fact I think I may have even been the first person\nto post on this list the commit-reveal mitigation.\n\nNote how I said earlier in the message you're replying to that \"P2SH's 160-bits\nis insufficient in certain use-cases such as multisig\"\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/4cc07c5c/attachment.sig>"
            },
            {
                "author": "Dave Scotese",
                "date": "2017-02-25T21:21:56",
                "message_text_only": "I was under the impression that RIPEMD160(SHA256(msg)) is used to turn a\nPUBLIC key (msg) into a bitcoin address, so yeah, you could identify\nANOTHER (or the same, I guess - how would you know?) public key that has\nthe same bitcoin address if RIPEMD-160 collisions are easy, but I don't see\nhow that has any effect on anyone.  Maybe I'm restating what Peter wrote.\nIf so, confirmation would be nice.\n\nOn Sat, Feb 25, 2017 at 1:04 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Feb 25, 2017 at 03:53:12PM -0500, Russell O'Connor wrote:\n> > On Sat, Feb 25, 2017 at 2:12 PM, Peter Todd via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > > On Sat, Feb 25, 2017 at 11:10:02AM -0500, Ethan Heilman via bitcoin-dev\n> > > wrote:\n> > > > >SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> > > > 160bits isn't enough.\n> > > >\n> > > > I would argue that 160-bits isn't enough for collision resistance.\n> > > Assuming\n> > > > RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle),\n> > > collisions\n> > >\n> > > That's something that we're well aware of; there have been a few\n> > > discussions on\n> > > this list about how P2SH's 160-bits is insufficient in certain\n> use-cases\n> > > such\n> > > as multisig.\n> > >\n> > > However, remember that a 160-bit *security level* is sufficient, and\n> > > RIPEMD160\n> > > has 160-bit security against preimage attacks. Thus things like\n> > > pay-to-pubkey-hash are perfectly secure: sure you could generate two\n> > > pubkeys\n> > > that have the same RIPEMD160(SHA256()) digest, but if someone does\n> that it\n> > > doesn't cause the Bitcoin network itself any harm, and doing so is\n> > > something\n> > > you choose to do to yourself.\n> > >\n> >\n> > Be aware that the issue is more problematic for more complex contracts.\n> > For example, you are building a P2SH 2-of-2 multisig together with\n> someone\n> > else if you are not careful, party A can hand their key over to party B,\n> > who can may try to generate a collision between their second key and\n> > another 2-of-2 multisig where they control both keys. See\n> > https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2016-January/012205.html\n>\n> I'm very aware of that, in fact I think I may have even been the first\n> person\n> to post on this list the commit-reveal mitigation.\n>\n> Note how I said earlier in the message you're replying to that \"P2SH's\n> 160-bits\n> is insufficient in certain use-cases such as multisig\"\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/6b3b227c/attachment-0001.html>"
            },
            {
                "author": "Steve Davis",
                "date": "2017-02-25T21:34:33",
                "message_text_only": "Yea, well. I don\u2019t think it is ethical to post instructions without an associated remediation (BIP) if you don\u2019t see the potential attack.\n\nI was rather hoping that we could have a fuller discussion of what the best practical response would be to such an issue?\n\n\n> On Feb 25, 2017, at 3:21 PM, Dave Scotese <dscotese at litmocracy.com> wrote:\n> \n> I was under the impression that RIPEMD160(SHA256(msg)) is used to turn a PUBLIC key (msg) into a bitcoin address, so yeah, you could identify ANOTHER (or the same, I guess - how would you know?) public key that has the same bitcoin address if RIPEMD-160 collisions are easy, but I don't see how that has any effect on anyone.  Maybe I'm restating what Peter wrote.  If so, confirmation would be nice.\n> \n> On Sat, Feb 25, 2017 at 1:04 PM, Peter Todd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> On Sat, Feb 25, 2017 at 03:53:12PM -0500, Russell O'Connor wrote:\n> > On Sat, Feb 25, 2017 at 2:12 PM, Peter Todd via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> >\n> > > On Sat, Feb 25, 2017 at 11:10:02AM -0500, Ethan Heilman via bitcoin-dev\n> > > wrote:\n> > > > >SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> > > > 160bits isn't enough.\n> > > >\n> > > > I would argue that 160-bits isn't enough for collision resistance.\n> > > Assuming\n> > > > RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle),\n> > > collisions\n> > >\n> > > That's something that we're well aware of; there have been a few\n> > > discussions on\n> > > this list about how P2SH's 160-bits is insufficient in certain use-cases\n> > > such\n> > > as multisig.\n> > >\n> > > However, remember that a 160-bit *security level* is sufficient, and\n> > > RIPEMD160\n> > > has 160-bit security against preimage attacks. Thus things like\n> > > pay-to-pubkey-hash are perfectly secure: sure you could generate two\n> > > pubkeys\n> > > that have the same RIPEMD160(SHA256()) digest, but if someone does that it\n> > > doesn't cause the Bitcoin network itself any harm, and doing so is\n> > > something\n> > > you choose to do to yourself.\n> > >\n> >\n> > Be aware that the issue is more problematic for more complex contracts.\n> > For example, you are building a P2SH 2-of-2 multisig together with someone\n> > else if you are not careful, party A can hand their key over to party B,\n> > who can may try to generate a collision between their second key and\n> > another 2-of-2 multisig where they control both keys. See\n> > https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012205.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-January/012205.html>\n> \n> I'm very aware of that, in fact I think I may have even been the first person\n> to post on this list the commit-reveal mitigation.\n> \n> Note how I said earlier in the message you're replying to that \"P2SH's 160-bits\n> is insufficient in certain use-cases such as multisig\"\n> \n> --\n> https://petertodd.org <https://petertodd.org/> 'peter'[:-1]@petertodd.org <http://petertodd.org/>\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\n> \n> \n> \n> \n> -- \n> I like to provide some work at no charge to prove my value. Do you need a techie?  \n> I own Litmocracy <http://www.litmocracy.com/> and Meme Racing <http://www.memeracing.net/> (in alpha). \n> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com/> which now accepts Bitcoin.\n> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n> \"He ought to find it more profitable to play by the rules\" - Satoshi Nakamoto\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/461ac05d/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-25T20:57:06",
                "message_text_only": "On Sat, Feb 25, 2017 at 12:42:56PM -0800, Watson Ladd wrote:\n> On Sat, Feb 25, 2017 at 11:12 AM, Peter Todd via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > On Sat, Feb 25, 2017 at 11:10:02AM -0500, Ethan Heilman via bitcoin-dev wrote:\n> >> >SHA1 is insecure because the SHA1 algorithm is insecure, not because\n> >> 160bits isn't enough.\n> >>\n> >> I would argue that 160-bits isn't enough for collision resistance. Assuming\n> >> RIPEMD-160(SHA-256(msg)) has no flaws (i.e. is a random oracle), collisions\n> >\n> > That's something that we're well aware of; there have been a few discussions on\n> > this list about how P2SH's 160-bits is insufficient in certain use-cases such\n> > as multisig.\n> >\n> > However, remember that a 160-bit *security level* is sufficient, and RIPEMD160\n> > has 160-bit security against preimage attacks. Thus things like\n> > pay-to-pubkey-hash are perfectly secure: sure you could generate two pubkeys\n> > that have the same RIPEMD160(SHA256()) digest, but if someone does that it\n> > doesn't cause the Bitcoin network itself any harm, and doing so is something\n> > you choose to do to yourself.\n> \n> P2SH is not secure against collision. I could write two scripts with\n> the same hash, one of which is an escrow script and the other which\n> pays it to me, have someone pay to the escrow script, and then get the\n> payment. Some formal analysis tools would ignore the unused\n> instructions even if human analysis would not.\n\nThat's what I said: \"P2SH's 160-bits is insufficient in certain use-cases such\nas multisig\"\n\nObviously any usecase where multiple people are creating a P2SH redeemScript\ncollaboratively is potentially vulnerable. Use-cases where the redeemScript was\ncreated by a single-party however are _not_ vulnerable, as that party has\ncomplete control over whether or not collisions are possible, by virtue of the\nfact that they're the ones who have to make the collision happen!\n\nSimilarly, even in the multisig case, commit-reveal techniques can mitigate the\nvulnerability, by forcing parties to commit to what pubkeys/hashlocks/etc.\nthey'll use for the script prior to pubkeys/hashlocks/etc. being revealed.\nThough a better long-term approach is to use a 256-bit digest size, as segwit\ndoes.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/5ab478f4/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-02-25T21:40:18",
                "message_text_only": "On Sat, Feb 25, 2017 at 03:34:33PM -0600, Steve Davis wrote:\n> Yea, well. I don\u2019t think it is ethical to post instructions without an associated remediation (BIP) if you don\u2019t see the potential attack.\n\nI can't agree with you at all there: we're still at the point where the\ncomputational costs of such attacks limit their real-world impact, which is\nexactly when you want the *maximum* exposure to what they are and what the\nrisks are, so that people develop mitigations.\n\nKeeping details secret tends to keep the attacks out of public view, which\nmight be a good trade-off in a situation where the attacks are immediately\npractical and the need to deploy a fix is well understood. But we're in the\nexact opposite situation.\n\n> I was rather hoping that we could have a fuller discussion of what the best practical response would be to such an issue?\n\nDeploying segwit's 256-bit digests is a response that's already fully coded and\nready to deploy, with the one exception of a new address format. That address\nformat is being actively worked on, and could be deployed relatively quickly if\nneeded.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/5ef5ac64/attachment.sig>"
            },
            {
                "author": "Steve Davis",
                "date": "2017-02-25T21:54:16",
                "message_text_only": "Hi Peter,\n\n> On Feb 25, 2017, at 3:40 PM, Peter Todd <pete at petertodd.org> wrote:\n> \n> On Sat, Feb 25, 2017 at 03:34:33PM -0600, Steve Davis wrote:\n>> Yea, well. I don\u2019t think it is ethical to post instructions without an associated remediation (BIP) if you don\u2019t see the potential attack.\n> \n> I can't agree with you at all there: we're still at the point where the\n> computational costs of such attacks limit their real-world impact, which is\n> exactly when you want the *maximum* exposure to what they are and what the\n> risks are, so that people develop mitigations.\n> \n\nI agree with the latter part of your statement but am actually much less confident about the first part\u2026 I need to run some numbers on that.\n\n> Keeping details secret tends to keep the attacks out of public view, which\n> might be a good trade-off in a situation where the attacks are immediately\n> practical and the need to deploy a fix is well understood. But we're in the\n> exact opposite situation.\n> \n>> I was rather hoping that we could have a fuller discussion of what the best practical response would be to such an issue?\n> \n> Deploying segwit's 256-bit digests is a response that's already fully coded and\n> ready to deploy, with the one exception of a new address format. That address\n> format is being actively worked on, and could be deployed relatively quickly if\n> needed.\n> \n\nI really, really don\u2019t want to get into it but segwit has many aspects that are less appealing, not least of which being the amount of time it would take to reach the critical mass. \n\nSurely there's a number of alternative approaches which could be explored, even if only to make a fair assessment of a best response?\n\n/s"
            },
            {
                "author": "Pieter Wuille",
                "date": "2017-02-25T22:14:44",
                "message_text_only": "On Feb 25, 2017 14:09, \"Steve Davis via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nHi Peter,\n\n\nI really, really don\u2019t want to get into it but segwit has many aspects that\nare less appealing, not least of which being the amount of time it would\ntake to reach the critical mass.\n\nSurely there's a number of alternative approaches which could be explored,\neven if only to make a fair assessment of a best response?\n\n\nAny alternative to move us away from RIPEMD160 would require:\n* A drafting of a softfork proposal, implementation, testing, review.\n* A new address format\n* Miners accepting the new consensus rules\n* Wallets adopting the new address format, both on the sender side and\nreceiver side (which requires new signatures).\n\nI.e., exactly the same as segwit, for which most of these are already done.\nAnd it would still only apply to wallets adopting it.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/e3856947/attachment-0001.html>"
            },
            {
                "author": "Ethan Heilman",
                "date": "2017-02-25T22:34:38",
                "message_text_only": "I strongly encourage Bitcoin to move from 80-bit collision resistance\n(RIPEMD-160) to 128-bit collision resistance (SHA-256).\n\nOn Sat, Feb 25, 2017 at 5:14 PM, Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> On Feb 25, 2017 14:09, \"Steve Davis via bitcoin-dev\" <bitcoin-dev at lists.\n> linuxfoundation.org> wrote:\n>\n> Hi Peter,\n>\n>\n> I really, really don\u2019t want to get into it but segwit has many aspects\n> that are less appealing, not least of which being the amount of time it\n> would take to reach the critical mass.\n>\n> Surely there's a number of alternative approaches which could be explored,\n> even if only to make a fair assessment of a best response?\n>\n>\n> Any alternative to move us away from RIPEMD160 would require:\n> * A drafting of a softfork proposal, implementation, testing, review.\n> * A new address format\n> * Miners accepting the new consensus rules\n> * Wallets adopting the new address format, both on the sender side and\n> receiver side (which requires new signatures).\n>\n> I.e., exactly the same as segwit, for which most of these are already\n> done. And it would still only apply to wallets adopting it.\n>\n> --\n> Pieter\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/7fecc6c7/attachment.html>"
            },
            {
                "author": "Steve Davis",
                "date": "2017-02-26T06:26:45",
                "message_text_only": "Hi Pieter,\n\n> On Feb 25, 2017, at 4:14 PM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n> \n> Any alternative to move us away from RIPEMD160 would require:\n\n> <snipped>\n\n\u201cAny alternative\u201d? What about reverting to:\n\n[<public_key>, OP_CHECKSIG]\n\nor perhaps later\n\n[<\u201ccompressed\u201d public_key>, OP_CHECKSIG]\n\nThis appears to get away from the issue without introducing a lot of other concerns?\n\n(IIRC the RIPEMD thing was justified on convenience and compactness).\n\nCould that be the alternative?\n\n/s"
            },
            {
                "author": "Leandro Coutinho",
                "date": "2017-02-25T23:09:18",
                "message_text_only": "If people split their bitcoins in multiple addresses, then maybe there\nwould be no need to worry(?), because the computational cost would be\nhigher than what the attacker would get.\n\n\n>From Google:\nhttps://security.googleblog.com/2017/02/announcing-first-sha1-collision.html\n\n*Here are some numbers that give a sense of how large scale this\ncomputation was: *\n\n   - *Nine quintillion (9,223,372,036,854,775,808) SHA1 computations in\n   total*\n   - *6,500 years of CPU computation to complete the attack first phase*\n   - *110 years of GPU computation to complete the second phase*\n\n\nhttps://bitinfocharts.com/top-100-richest-bitcoin-addresses.html\nRichest address: 124,178 BTC ($142,853,079 USD)\n\n\n\nOn Sat, Feb 25, 2017 at 6:40 PM, Peter Todd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Feb 25, 2017 at 03:34:33PM -0600, Steve Davis wrote:\n> > Yea, well. I don\u2019t think it is ethical to post instructions without an\n> associated remediation (BIP) if you don\u2019t see the potential attack.\n>\n> I can't agree with you at all there: we're still at the point where the\n> computational costs of such attacks limit their real-world impact, which is\n> exactly when you want the *maximum* exposure to what they are and what the\n> risks are, so that people develop mitigations.\n>\n> Keeping details secret tends to keep the attacks out of public view, which\n> might be a good trade-off in a situation where the attacks are immediately\n> practical and the need to deploy a fix is well understood. But we're in the\n> exact opposite situation.\n>\n> > I was rather hoping that we could have a fuller discussion of what the\n> best practical response would be to such an issue?\n>\n> Deploying segwit's 256-bit digests is a response that's already fully\n> coded and\n> ready to deploy, with the one exception of a new address format. That\n> address\n> format is being actively worked on, and could be deployed relatively\n> quickly if\n> needed.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/3c3e4b53/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2017-02-26T06:36:25",
                "message_text_only": "On Feb 25, 2017 22:26, \"Steve Davis\" <steven.charles.davis at gmail.com> wrote:\n\nHi Pieter,\n\n> On Feb 25, 2017, at 4:14 PM, Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n>\n> Any alternative to move us away from RIPEMD160 would require:\n\n> <snipped>\n\n\u201cAny alternative\u201d? What about reverting to:\n\n[<public_key>, OP_CHECKSIG]\n\n\nsnip\n\n\nCould that be the alternative?\n\n\nOk, fair enough, that is an alternative that avoids the 160-bit hash\nfunction, but not where it matters. The 80-bit collision attack only\napplies to jointly constructed addresses like multisig P2SH, not single-key\nones. As far as I know for those we only rely preimage security, and\nRIPEMD160 has 160 bit security there, which is even more than our ECDSA\nsignatures offer.\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/6f7d3907/attachment.html>"
            },
            {
                "author": "Steve Davis",
                "date": "2017-02-26T07:16:37",
                "message_text_only": "> On Feb 26, 2017, at 12:36 AM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n> \n> The 80-bit collision attack only applies to jointly constructed addresses like multisig P2SH, not single-key ones.\n\nThat\u2019s the part I\u2019m less convinced about, and why I asked the original question re SHA1 vs RIPEMD. \n\nI\u2019m checking my own numbers (and as you\u2019ll appreciate it\u2019s a powers of ten thing), but I do see a vector. Which would mean that if RIPEMD were weakened in any way, single-key transactions could suddenly become badly exposed.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170226/9aeaf1d0/attachment.html>"
            },
            {
                "author": "Steve Davis",
                "date": "2017-02-26T16:53:29",
                "message_text_only": "> On Feb 26, 2017, at 1:36 AM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n> \n> Typical hash function breaks produce collision attacks, while a preimage attack is needed to reduce single-key address security.\n\nThank you Pieter - that was really helpful. I realize now that I was thinking of a preimage attack but had mistakenly assumed that the birthday bound applied...\n\nSo the unit operation: [genkeypair; ripemd160(sha256(pubkey));check_utxoset] would need to be performed 2.9*10^42 and not (as I had first calculated) 2.4*10^18. \n\nOops. My bad."
            }
        ],
        "thread_summary": {
            "title": "SHA1 collisions make Git vulnerable to attakcs by third-parties, not just repo maintainers",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Steve Davis",
                "Aymeric Vitte",
                "Peter Todd",
                "Tim Ruffing",
                "Leandro Coutinho",
                "Russell O'Connor",
                "Dave Scotese",
                "Henning Kopp",
                "Ethan Heilman",
                "Pieter Wuille",
                "Shin'ichiro Matsuo",
                "Watson Ladd",
                "Alice Wonder"
            ],
            "messages_count": 32,
            "total_messages_chars_count": 84447
        }
    },
    {
        "title": "[bitcoin-dev] Moving towards user activated soft fork activation",
        "thread_messages": [
            {
                "author": "shaolinfry",
                "date": "2017-02-25T23:55:51",
                "message_text_only": "Some thoughts about the activation mechanism for soft forks. In the past we used IsSuperMajority and currently use BIP9 as soft fork activation methods, where a supermajority of hashrate triggers nodes to begin enforcing new rules. Hashrate based activation is convenient because it is the simplest and most straightforward process. While convenient there are a number limitations with this method.\n\nFirstly, it requires trusting the hash power will validate after activation. The BIP66 soft fork was a case where 95% of the hashrate was signaling readiness but in reality about half was not actually validating the upgraded rules and mined upon an invalid block by mistake[1].\n\nSecondly, miner signalling has a natural veto which allows a small percentage of hashrate to veto node activation of the upgrade for everyone. To date, soft forks have taken advantage of the relatively centralised mining landscape where there are relatively few mining pools building valid blocks; as we move towards more hashrate decentralization, it's likely that we will suffer more and more from \"upgrade inertia\" which will veto most upgrades.\n\nUpgrade inertia in inevitable for widely deployed software and can be seen for example, with Microsoft Windows. At the time of writing 5.72% of all Microsoft Windows installations are still running Windows XP, despite mainstream support ending in 2009 and being superseded by 4 software generations, Vista, 7, 8 and 10.\n\nThirdly, the signaling methodology is widely misinterpreted to mean the hash power is voting on a proposal and it seems difficult to correct this misunderstanding in the wider community. The hash powers' role is to select valid transactions, and to extend the blockchain with valid blocks. Fully validating economic nodes ensure that blocks are valid. Nodes therefore define validity according to the software they run, but miners decide what already valid transactions gets included in the block chain.\n\nAs such, soft forks rules are actually always enforced by the nodes, not the miners. Miners of course can opt-out by simply not including transactions that use the new soft fork feature, but they cannot produce blocks that are invalid to the soft fork. The P2SH soft fork is a good example of this, where non-upgraded miners would see P2SH as spendable without a signature and consider them valid. If such an transaction were to be included in a block, the block would be invalid and the miner would lose the block reward and fees.\n\nSo-called \"censorship\" soft forks do not require nodes to opt in, because >51% of the hash power already have the ability to orphan blocks that contain transactions they have blacklisted. Since this is not a change in validity, nodes will accept the censored block chain automatically.\n\nThe fourth problem with supermajority hash power signaling is it draws unnecessary attention to miners which can become unnecessarily political. Already misunderstood as a vote, miners may feel pressure to \"make a decision\" on behalf of the community: who is and isn't signalling becomes a huge public focus and may put pressures onto miners they are unprepared for. Some miners may not be in a position to upgrade, or may prefer not to participate in the soft fork which is their right. However, that miner may now become a lone reason that vetoes activation for everyone, where the soft fork is an opt-in feature! This situation seems to be against the voluntary nature of the Bitcoin system where participation at all levels is voluntary and kept honest by well balanced incentives.\n\nSince miners already have the protocol level right to select whatever transaction they prefer (and not mine those they don't), it would be better if a miner could chose to not participate in triggering activation of something they won't use, but, without being a veto to the process (and all the ire they may have to experience as a consequence).\n\nThe alternative discussed here is \"flag day activation\" where nodes begin enforcement at a predetermined time in the future. This method needs a longer lead time than a hash power based activation trigger, but offers a number of advantages and perhaps provides a better tradeoff.\n\nSoft forks are still entirely optional to use post activation. For example, with P2SH, many participants in the Bitcoin ecosystem still do not use P2SH. Only 11% of bitcoins[2] are stored in P2SH addresses at the time of writing. Miners are free to not mine P2SH transactions, however, the incentives are such that miners should still validate transactions so they don't accidentally include invalid transactions and cause their block to be rejected. As an additional safety measure for well designed soft forks, relay policy rules prevent non-standard and invalid transactions from being relayed and mined by default; a miner would have to purposefully mine an invalid transaction, which is against their own economic interest.\n\nSince the incentives of the Bitcoin system rely on self validation, economic nodes (miners and users) should always remain safe by ensuring their nodes either validate the current rules, or, they can place their network behind a full node that will filter out invalid transactions and blocks at the edge of their network (so called firewall or border nodes).\n\nA user activated soft fork is permissive. Miners do not have to produce new version blocks and non-upgraded miners' blocks will not be orphaned as was the case with IsSuperMajority soft forks (e.g. BIP34, BIP66, BIP65-CLTV) which made it a compulsory upgrade for miners.\n\nBIP9 \"versionbits\" soft fork activation method is also permissive in so far as non-upgraded miners are not forced to upgrade after activation because their blocks wont be orphaned. A recent case was the \"CSV\" soft fork that activated BIP68, BIP112 and BIP113. As such, the CSV soft fork allows non-upgraded miners to continue mining so long as they didn't produce invalid blocks.\n\nMiners always retain discretion on which transactions to mine. However, regardless of whether they actively include transactions using the new soft fork feature, or not, the incentive for hash power to upgrade in order to validate is strong: if they do not, they could be vulnerable to a rogue miner willing to waste 12.5BTC to create an invalid block, which may cause non-validating miners to build on an invalid chain similar to the BIP66 incident. Validation has always had a strong requirement.\n\nA user activated soft fork is win-win because it adds an option that some people want that does not detract from other peoples' enjoyment. Even if only 10% of users ever wanted a feature, so long as the benefit outweighed the technical risks, it would not be rational to deny others the ability to opt-in.\n\nMy suggestion is to have the best of both worlds. Since a user activated soft fork needs a relatively long lead time before activation, we can combine with BIP9 to give the option of a faster hash power coordinated activation or activation by flag day, whichever is the sooner. In both cases, we can leverage the warning systems in BIP9. The change is relatively simple, adding an activation-time parameter which will transition the BIP9 state to LOCKED_IN before the end of the BIP9 deployment timeout.\n\nYou can find the proposal here https://gist.github.com/shaolinfry/0f7d1fd22743bb966da0c0b1682ea2ab\n\nReferences:\n\n[1]: https://bitcoin.org/en/alert/2015-07-04-spv-mining\n[2]: http://p2sh.info/dashboard/db/p2sh-statistics?from=1472043312917&to=1488030912918\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170225/e6ed76b3/attachment-0001.html>"
            },
            {
                "author": "Jameson Lopp",
                "date": "2017-02-26T17:34:57",
                "message_text_only": "You've made many salient points, Shaolin, though I have a few questions:\n\n1) How well does this model work under adversarial conditions? Fair point\nabout signaling not being reliable, though it seems more vague in terms of\nsafety given that you can't actually know what percentage of hashrate that\nis /not/ signaling for the soft fork has taken the necessary precautions to\navoid mining an invalid block and potentially causing a hard fork. It's\nprobably safe to say that if a flag-day soft fork is activated, there will\nbe at least a few parties who will attempt to trigger a chain fork by\ncrafting transactions that are valid via non-fork rules but invalid via the\nsoft fork rules.\n\n2) If the flag day soft fork is activated with only a minority of hashrate\nsupport + safely opted-out hashrate, isn't it possible for the rest of\nminers to coordinate orphaning any soft fork compatible blocks to kill the\nsoft fork chain? This would be a major difference from a miner-activated\nsoft fork, correct? Unless perhaps many miners colluded to signal soft fork\nsupport while not actually supporting it...\n\n3) In terms of complexity for mining pool operators, how well does this\nmodel scale if there are N soft forks and the pool doesn't want to opt-in\nto any of them? Couldn't this result in those pool operators having to run\nnot just one border node, but a multitude of \"chained\" border nodes if the\nsoft forks are spread across different software implementations?\n\nIt seems to me that this type of user-driven approach would preferably be\ncoupled with assurances from major Bitcoin wallets / exchanges / payment\nprocessors that they will not honor coins from a chain fork that results\nfrom invalid spends of outputs encumbered by soft fork rules. Though on the\nother hand, I don't see such an assurance being possible given that\nexchanges have an incentive to take the first mover advantage in listing a\nnew coin.\n\n- Jameson\n\nOn Sat, Feb 25, 2017 at 6:55 PM, shaolinfry via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Some thoughts about the activation mechanism for soft forks. In the past\n> we used IsSuperMajority and currently use BIP9 as soft fork activation\n> methods, where a supermajority of hashrate triggers nodes to begin\n> enforcing new rules. Hashrate based activation is convenient because it is\n> the simplest and most straightforward process. While convenient there are a\n> number limitations with this method.\n>\n> Firstly, it requires trusting the hash power will validate after\n> activation. The BIP66 soft fork was a case where 95% of the hashrate was\n> signaling readiness but in reality about half was not actually validating\n> the upgraded rules and mined upon an invalid block by mistake[1].\n>\n> Secondly, miner signalling has a natural veto which allows a small\n> percentage of hashrate to veto node activation of the upgrade for everyone.\n> To date, soft forks have taken advantage of the relatively centralised\n> mining landscape where there are relatively few mining pools building valid\n> blocks; as we move towards more hashrate decentralization, it's likely that\n> we will suffer more and more from \"upgrade inertia\" which will veto most\n> upgrades.\n>\n> Upgrade inertia in inevitable for widely deployed software and can be seen\n> for example, with Microsoft Windows. At the time of writing 5.72% of all\n> Microsoft Windows installations are still running Windows XP, despite\n> mainstream support ending in 2009 and being superseded by 4 software\n> generations, Vista, 7, 8 and 10.\n>\n> Thirdly, the signaling methodology is widely misinterpreted to mean the\n> hash power is voting on a proposal and it seems difficult to correct this\n> misunderstanding in the wider community. The hash powers' role is to select\n> valid transactions, and to extend the blockchain with valid blocks. Fully\n> validating economic nodes ensure that blocks are valid. Nodes therefore\n> define validity according to the software they run, but miners decide what\n> already valid transactions gets included in the block chain.\n>\n> As such, soft forks rules are actually always enforced by the nodes, not\n> the miners. Miners of course can opt-out by simply not including\n> transactions that use the new soft fork feature, but they cannot produce\n> blocks that are invalid to the soft fork. The P2SH soft fork is a good\n> example of this, where non-upgraded miners would see P2SH as spendable\n> without a signature and consider them valid. If such an transaction were to\n> be included in a block, the block would be invalid and the miner would lose\n> the block reward and fees.\n>\n> So-called \"censorship\" soft forks do not require nodes to opt in, because\n> >51% of the hash power already have the ability to orphan blocks that\n> contain transactions they have blacklisted. Since this is not a change in\n> validity, nodes will accept the censored block chain automatically.\n>\n> The fourth problem with supermajority hash power signaling is it draws\n> unnecessary attention to miners which can become unnecessarily political.\n> Already misunderstood as a vote, miners may feel pressure to \"make a\n> decision\" on behalf of the community: who is and isn't signalling becomes a\n> huge public focus and may put pressures onto miners they are unprepared\n> for. Some miners may not be in a position to upgrade, or may prefer not to\n> participate in the soft fork which is their right. However, that miner may\n> now become a lone reason that vetoes activation for everyone, where the\n> soft fork is an opt-in feature! This situation seems to be against the\n> voluntary nature of the Bitcoin system where participation at all levels is\n> voluntary and kept honest by well balanced incentives.\n>\n> Since miners already have the protocol level right to select whatever\n> transaction they prefer (and not mine those they don't), it would be better\n> if a miner could chose to not participate in triggering activation of\n> something they won't use, but, without being a veto to the process (and all\n> the ire they may have to experience as a consequence).\n>\n> The alternative discussed here is \"flag day activation\" where nodes begin\n> enforcement at a predetermined time in the future. This method needs a\n> longer lead time than a hash power based activation trigger, but offers a\n> number of advantages and perhaps provides a better tradeoff.\n>\n> Soft forks are still entirely optional to use post activation. For\n> example, with P2SH, many participants in the Bitcoin ecosystem still do not\n> use P2SH. Only 11% of bitcoins[2] are stored in P2SH addresses at the time\n> of writing. Miners are free to not mine P2SH transactions, however, the\n> incentives are such that miners should still validate transactions so they\n> don't accidentally include invalid transactions and cause their block to be\n> rejected. As an additional safety measure for well designed soft forks,\n> relay policy rules prevent non-standard and invalid transactions from being\n> relayed and mined by default; a miner would have to purposefully mine an\n> invalid transaction, which is against their own economic interest.\n>\n> Since the incentives of the Bitcoin system rely on self validation,\n> economic nodes (miners and users) should always remain safe by ensuring\n> their nodes either validate the current rules, or, they can place their\n> network behind a full node that will filter out invalid transactions and\n> blocks at the edge of their network (so called firewall or border nodes).\n>\n> A user activated soft fork is permissive. Miners do not have to produce\n> new version blocks and non-upgraded miners' blocks will not be orphaned as\n> was the case with IsSuperMajority soft forks (e.g. BIP34, BIP66,\n> BIP65-CLTV) which made it a compulsory upgrade for miners.\n>\n> BIP9 \"versionbits\" soft fork activation method is also permissive in so\n> far as non-upgraded miners are not forced to upgrade after activation\n> because their blocks wont be orphaned. A recent case was the \"CSV\" soft\n> fork that activated BIP68, BIP112 and BIP113. As such, the CSV soft fork\n> allows non-upgraded miners to continue mining so long as they didn't\n> produce invalid blocks.\n>\n> Miners always retain discretion on which transactions to mine. However,\n> regardless of whether they actively include transactions using the new soft\n> fork feature, or not, the incentive for hash power to upgrade in order to\n> validate is strong: if they do not, they could be vulnerable to a rogue\n> miner willing to waste 12.5BTC to create an invalid block, which may cause\n> non-validating miners to build on an invalid chain similar to the BIP66\n> incident. Validation has always had a strong requirement.\n>\n> A user activated soft fork is win-win because it adds an option that some\n> people want that does not detract from other peoples' enjoyment. Even if\n> only 10% of users ever wanted a feature, so long as the benefit outweighed\n> the technical risks, it would not be rational to deny others the ability to\n> opt-in.\n>\n> My suggestion is to have the best of both worlds. Since a user activated\n> soft fork needs a relatively long lead time before activation, we can\n> combine with BIP9 to give the option of a faster hash power coordinated\n> activation or activation by flag day, whichever is the sooner. In both\n> cases, we can leverage the warning systems in BIP9. The change is\n> relatively simple, adding an activation-time parameter which will\n> transition the BIP9 state to LOCKED_IN before the end of the BIP9\n> deployment timeout.\n>\n> You can find the proposal here https://gist.github.com/shaolinfry/\n> 0f7d1fd22743bb966da0c0b1682ea2ab\n>\n> References:\n>\n> [1]: https://bitcoin.org/en/alert/2015-07-04-spv-mining\n> [2]: http://p2sh.info/dashboard/db/p2sh-statistics?from=\n> 1472043312917&to=1488030912918\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170226/82166427/attachment-0001.html>"
            },
            {
                "author": "shaolinfry",
                "date": "2017-02-27T16:02:52",
                "message_text_only": "Dear Jameson,\n\nThank you for your questions. Answers inline below:\n\nJameson Lopp via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n\nYou've made many salient points, Shaolin, though I have a few questions:\n\n1) How well does this model work under adversarial conditions? Fair point about signaling not being reliable, though it seems more vague in terms of safety given that you can't actually know what percentage of hashrate that is /not/ signaling for the soft fork has taken the necessary precautions to avoid mining an invalid block and potentially causing a hard fork. It's probably safe to say that if a flag-day soft fork is activated, there will be at least a few parties who will attempt to trigger a chain fork by crafting transactions that are valid via non-fork rules but invalid via the soft fork rules.\n\nIn a well designed soft fork, transactions under the old rules are non-standard by default and will not propagate or be mined. A miner would have to deliberately include the invalid transaction in a block and mine it. The invalid block would be rejected by the network costing the miner block reward and fees.\n\nIf >51% of the hashrate does not upgrade or does not take steps to protect themselves from invalid blocks, they will fork if someone produces an invalid block. Game theory suggests the incentive for those who do not wish to participate, would be to do so safely. There is no incentive to allow an attacker to cause you to split off from the network and it is trivial to prevent it.\n\nThere is a valid concern about \"spy\" mining and I cited a previous incident with BIP66 activation and we should be working towards solutions that remove the incentive to spy mine. \"Weak blocks\", where miners propagate their proposed blocks before solving the PoW may provide better incentives against spy mining, while delivering more (~no propagation delay and full validation, and thus more security).\n\n\n\n2) If the flag day soft fork is activated with only a minority of hashrate support + safely opted-out hashrate, isn't it possible for the rest of miners to coordinate orphaning any soft fork compatible blocks to kill the soft fork chain? This would be a major difference from a miner-activated soft fork, correct? Unless perhaps many miners colluded to signal soft fork support while not actually supporting it...\n\nThe basic assumption in the Bitcoin system is that miners will remain honest because it is in their economic interest to do so. Of course 51% of the hashrate can censor the minority hash by orphaning blacklisted transactions or blocks. I am fairly certain it would be considered an attack by as well as being very conspicuous. A 51% attack would likely cause a dramatic loss in confidence in the Bitcoin system and adversely affect price. It is reasonable to assume miners would not do that because mining has to remain profitable. Additionally, such a scenario would draw much ire from users who may escalate demands for a PoW change.\n\nIt is assuming good-faith and that miners would not want to deny people the ability to opt into something they wanted. All that is required of miners is to upgrade their border node. Miners should update their software anyway for security reasons.\n\n\n\n3) In terms of complexity for mining pool operators, how well does this model scale if there are N soft forks and the pool doesn't want to opt-in to any of them? Couldn't this result in those pool operators having to run not just one border node, but a multitude of \"chained\" border nodes if the soft forks are spread across different software implementations?\n\nWhile BIP9 allows for 29 parallel deployments I think it is unrealistic to expect there would be such a high number of active parallel deployments at any one time: History shows soft forks take a minimum of 6 months design, consensus building, coding and testing before deployment. With such a high bar, I do not envisage more than a couple of parallel deployments at any given time. I also do not envisage \"conflicting\" soft forks, as that would not meet consensus from the technical community on the basis of safety and sanity. In any case, the deployment strategy of each soft fork should be considered on a case by case basis.\n\n\n\nIt seems to me that this type of user-driven approach would preferably be coupled with assurances from major Bitcoin wallets / exchanges / payment processors that they will not honor coins from a chain fork that results from invalid spends of outputs encumbered by soft fork rules. Though on the other hand, I don't see such an assurance being possible given that exchanges have an incentive to take the first mover advantage in listing a new coin.\n\nSoft fork consensus proposals should be sane, uncontroversial and have a reasonably high bar in terms of technical consensus as we have seen with other soft forks to date. There is an implicit assumption in my text, that the decision to deploy a soft fork (regardless of the activation method) is based on a reasonable expectation that users will make use of the new feature. Hashrate signalling is not a vote, but a coordination trigger. Soft forks are backwards compatible and opt-in; so long as they are well written and bug free, users should at worst, be agnostic towards them because they have a choice whether to safely use the new feature or not, without preventing others' enjoyment of the feature. A controversial or unreasonable soft fork would not gain traction and I believe it would be fairly self evident.\n\nIn short, I do expect wide ecosystem collaboration as part of any deployment strategy, both hashrate or flag day based.\n\nMany thanks for taking the time to read over and consider my thoughts and proposal. I would be happy to discuss more if you have any further questions or suggestions.\n\n\n\n- Jameson\n\n\n\nOn Sat, Feb 25, 2017 at 6:55 PM, shaolinfry via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\nSome thoughts about the activation mechanism for soft forks. In the past we used IsSuperMajority and currently use BIP9 as soft fork activation methods, where a supermajority of hashrate triggers nodes to begin enforcing new rules. Hashrate based activation is convenient because it is the simplest and most straightforward process. While convenient there are a number limitations with this method.\n\nFirstly, it requires trusting the hash power will validate after activation. The BIP66 soft fork was a case where 95% of the hashrate was signaling readiness but in reality about half was not actually validating the upgraded rules and mined upon an invalid block by mistake[1].\n\nSecondly, miner signalling has a natural veto which allows a small percentage of hashrate to veto node activation of the upgrade for everyone. To date, soft forks have taken advantage of the relatively centralised mining landscape where there are relatively few mining pools building valid blocks; as we move towards more hashrate decentralization, it's likely that we will suffer more and more from \"upgrade inertia\" which will veto most upgrades.\n\nUpgrade inertia in inevitable for widely deployed software and can be seen for example, with Microsoft Windows. At the time of writing 5.72% of all Microsoft Windows installations are still running Windows XP, despite mainstream support ending in 2009 and being superseded by 4 software generations, Vista, 7, 8 and 10.\n\nThirdly, the signaling methodology is widely misinterpreted to mean the hash power is voting on a proposal and it seems difficult to correct this misunderstanding in the wider community. The hash powers' role is to select valid transactions, and to extend the blockchain with valid blocks. Fully validating economic nodes ensure that blocks are valid. Nodes therefore define validity according to the software they run, but miners decide what already valid transactions gets included in the block chain.\n\nAs such, soft forks rules are actually always enforced by the nodes, not the miners. Miners of course can opt-out by simply not including transactions that use the new soft fork feature, but they cannot produce blocks that are invalid to the soft fork. The P2SH soft fork is a good example of this, where non-upgraded miners would see P2SH as spendable without a signature and consider them valid. If such an transaction were to be included in a block, the block would be invalid and the miner would lose the block reward and fees.\n\nSo-called \"censorship\" soft forks do not require nodes to opt in, because >51% of the hash power already have the ability to orphan blocks that contain transactions they have blacklisted. Since this is not a change in validity, nodes will accept the censored block chain automatically.\n\nThe fourth problem with supermajority hash power signaling is it draws unnecessary attention to miners which can become unnecessarily political. Already misunderstood as a vote, miners may feel pressure to \"make a decision\" on behalf of the community: who is and isn't signalling becomes a huge public focus and may put pressures onto miners they are unprepared for. Some miners may not be in a position to upgrade, or may prefer not to participate in the soft fork which is their right. However, that miner may now become a lone reason that vetoes activation for everyone, where the soft fork is an opt-in feature! This situation seems to be against the voluntary nature of the Bitcoin system where participation at all levels is voluntary and kept honest by well balanced incentives.\n\nSince miners already have the protocol level right to select whatever transaction they prefer (and not mine those they don't), it would be better if a miner could chose to not participate in triggering activation of something they won't use, but, without being a veto to the process (and all the ire they may have to experience as a consequence).\n\nThe alternative discussed here is \"flag day activation\" where nodes begin enforcement at a predetermined time in the future. This method needs a longer lead time than a hash power based activation trigger, but offers a number of advantages and perhaps provides a better tradeoff.\n\nSoft forks are still entirely optional to use post activation. For example, with P2SH, many participants in the Bitcoin ecosystem still do not use P2SH. Only 11% of bitcoins[2] are stored in P2SH addresses at the time of writing. Miners are free to not mine P2SH transactions, however, the incentives are such that miners should still validate transactions so they don't accidentally include invalid transactions and cause their block to be rejected. As an additional safety measure for well designed soft forks, relay policy rules prevent non-standard and invalid transactions from being relayed and mined by default; a miner would have to purposefully mine an invalid transaction, which is against their own economic interest.\n\nSince the incentives of the Bitcoin system rely on self validation, economic nodes (miners and users) should always remain safe by ensuring their nodes either validate the current rules, or, they can place their network behind a full node that will filter out invalid transactions and blocks at the edge of their network (so called firewall or border nodes).\n\nA user activated soft fork is permissive. Miners do not have to produce new version blocks and non-upgraded miners' blocks will not be orphaned as was the case with IsSuperMajority soft forks (e.g. BIP34, BIP66, BIP65-CLTV) which made it a compulsory upgrade for miners.\n\nBIP9 \"versionbits\" soft fork activation method is also permissive in so far as non-upgraded miners are not forced to upgrade after activation because their blocks wont be orphaned. A recent case was the \"CSV\" soft fork that activated BIP68, BIP112 and BIP113. As such, the CSV soft fork allows non-upgraded miners to continue mining so long as they didn't produce invalid blocks.\n\nMiners always retain discretion on which transactions to mine. However, regardless of whether they actively include transactions using the new soft fork feature, or not, the incentive for hash power to upgrade in order to validate is strong: if they do not, they could be vulnerable to a rogue miner willing to waste 12.5BTC to create an invalid block, which may cause non-validating miners to build on an invalid chain similar to the BIP66 incident. Validation has always had a strong requirement.\n\nA user activated soft fork is win-win because it adds an option that some people want that does not detract from other peoples' enjoyment. Even if only 10% of users ever wanted a feature, so long as the benefit outweighed the technical risks, it would not be rational to deny others the ability to opt-in.\n\nMy suggestion is to have the best of both worlds. Since a user activated soft fork needs a relatively long lead time before activation, we can combine with BIP9 to give the option of a faster hash power coordinated activation or activation by flag day, whichever is the sooner. In both cases, we can leverage the warning systems in BIP9. The change is relatively simple, adding an activation-time parameter which will transition the BIP9 state to LOCKED_IN before the end of the BIP9 deployment timeout.\n\nYou can find the proposal here https://gist.github.com/shaolinfry/0f7d1fd22743bb966da0c0b1682ea2ab\n\nReferences:\n\n[1]: https://bitcoin.org/en/alert/2015-07-04-spv-mining\n[2]: http://p2sh.info/dashboard/db/p2sh-statistics?from=1472043312917&to=1488030912918\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170227/5f3d5943/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-02-27T16:50:07",
                "message_text_only": "On Feb 27, 2017, at 8:02 AM, shaolinfry via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>> 3) In terms of complexity for mining pool operators, how well does this model scale if there are N soft forks and the pool doesn't want to opt-in to any of them? Couldn't this result in those pool operators having to run not just one border node, but a multitude of \"chained\" border nodes if the soft forks are spread across different software implementations?\n> \n> While BIP9 allows for 29 parallel deployments I think it is unrealistic to expect there would be such a high number of active parallel deployments at any one time: History shows soft forks take a minimum of 6 months design, consensus building, coding and testing before deployment. With such a high bar, I do not envisage more than a couple of parallel deployments at any given time. I also do not envisage \"conflicting\" soft forks, as that would not meet consensus from the technical community on the basis of safety and sanity. In any case, the deployment strategy of each soft fork should be considered on a case by case basis.\n\nThe relationship between a codebase and chain fork implementations is similar to vendor lock-in, and is being used in a similar manner.\n\nThere is nothing preventing a single codebase from implementing all forks and exposing the option to apply any non-conflicting combination of them.\n\nWhile this has not been the norm libbitcoin now utilizes this approach. Currently the options to apply any activated Bitcoin forks are exposed via config. I personally am not working to implement non-activated forks at this point, but that's just prioritization.\n\nRecently I objected to BIP90. This hard fork is presented as a code simplification and a performance optimization. I showed in the discussion that it was neither. Nevertheless we implemented this additional code and give the user the option to apply it or not. It's application produces no performance benefit, but it ensures that the choice of forks remains in the hands of the user.\n\ne"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-02-28T21:20:29",
                "message_text_only": "Without at least a majority hashrate validating blocks, it is possible just a \nsingle invalid block could split the chain such that the majority continue \nbuilding a most-work on that invalid block.\n\nThis failure to validate a softfork is similar in some respects to a hardfork, \nbut with one critical difference: the default behaviour of old nodes will be \nto follow the chain with the most-work that was valid under the pre-softfork \nrules. This actually *inverts* the benefit of the softfork over a hardfork, \nand makes a softfork deployed in such a manner de facto behave as if it had \nbeen a hardfork, IF someone ever mines a single malicious block.\n\nFor this reason, I think a minority-hashrate softfork requires a much higher \ndegree of social support than merely the widespread agreement typical of \nsoftforks. It might perhaps require less than the full ~100% consensus \nhardforks require, but it likely comes somewhat close.\n\nOnce it gets over 50% hashrate enforcement, however, the situation improves a \nlot more: a malicious block may split obsolete miners off the valid chain, but \nit will eventually resolve on its own given enough time. Due to natural \nfluctuations in block finding, however, automatic measurement may need to look \nfor >75%.\n\nSo I would suggest that instead of a simple flag day activation, this proposal \nwould be improved by changing the flag day to merely reduce the hashrate \nrequirement from 95% to 75%.\n\n(In addition to the above concerns, if >50% of miners are hostile to the \nnetwork, we likely have other problems.)\n\nLuke"
            }
        ],
        "thread_summary": {
            "title": "Moving towards user activated soft fork activation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "shaolinfry",
                "Eric Voskuil",
                "Luke Dashjr",
                "Jameson Lopp"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 35245
        }
    },
    {
        "title": "[bitcoin-dev]  Bitcoin Knots 0.14.0 release candidate 2 available",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2017-02-28T20:59:44",
                "message_text_only": "Release candidate 2 of a new major Bitcoin Knots release, version 0.14.0, has\nbeen made available.\n\nThis is a release candidate for a new major version release, including new\nfeatures, various bugfixes and performance improvements.\n\nPreliminary release notes for the release can be found here:\n\n    https://github.com/bitcoinknots/bitcoin/blob/v0.14.0.knots20170227.rc2/doc/release-notes.md\n\nBinaries can be downloaded from:\n\n    http://bitcoinknots.org/files/0.14.x/0.14.0.knots20170227.rc2/\n\nPlease take care to verify the PGP signature of all downloads.\n\nSource code can be found on GitHub under the signed tag\n\n    https://github.com/bitcoinknots/bitcoin/tree/v0.14.0.knots20170227.rc2\n\nRelease candidates are test versions for releases. When no critical problems\nare found, this release candidate will be tagged as 0.14.0 final, otherwise\na new rc will be made available after these are solved.\n\nPlease report bugs using the issue tracker at GitHub:\n\n    https://github.com/bitcoinknots/bitcoin/issues"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Knots 0.14.0 release candidate 2 available",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1006
        }
    }
]