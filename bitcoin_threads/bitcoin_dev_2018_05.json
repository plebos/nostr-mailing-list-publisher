[
    {
        "title": "[bitcoin-dev] eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-01T05:01:52",
                "message_text_only": "Good morning Jim,\n\n> If my understanding is correct though, this construction would significantly increase the safe CLTV delta requirements because HTLCs cannot be timed out immediately on the settlement transaction. Consider a case where node B receives an HTLC from A and forwards to C. If the HTLC offered to C times out and C does not fail the HTLC off-chain, Lightning currently guarantees that the CLTV delta is sufficient that I may close the channel to C on-chain and claim the timed-out HTLC before my upstream HTLC to A times out. If the CLTV delta is too small, I may fail the upstream HTLC as soon as it times out, and then C may still claim the downstream HTLC with the preimage on-chain. With eltoo, when B closes the downstream channel on-chain, it must wait the CSV timeout on the update transaction before locking in the timed-out HTLC. This effectively means the CLTV delta has to be greater than the CSV timeout, plus some extra (whereas it is currently safe to make it significantly shorter). Is that true or am I missing something?\n\nI believe this is quite true; indeed only the LN-penalty/Poon-Dryja channels do not have this drawback, as Decker-Wattenhofer invalidation trees also have the same drawback that the CSV and CLTV add up.\n\nHowever the worst-case invalidation tree total CSV timeouts under Decker-Wattenhofer can grow quite massive; it seems the new eltoo Decker-Russell-Osuntokun CSV timeouts can be shorter.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180501/e328b5ca/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-01T11:36:32",
                "message_text_only": "Jim Posen <jim.posen at gmail.com> writes:\n> If my understanding is correct though, this construction would\n> significantly increase the safe CLTV delta requirements because HTLCs\n> cannot be timed out immediately on the settlement transaction. Consider a\n> case where node B receives an HTLC from A and forwards to C. If the HTLC\n> offered to C times out and C does not fail the HTLC off-chain, Lightning\n> currently guarantees that the CLTV delta is sufficient that I may close the\n> channel to C on-chain and claim the timed-out HTLC before my upstream HTLC\n> to A times out. If the CLTV delta is too small, I may fail the upstream\n> HTLC as soon as it times out, and then C may still claim the downstream\n> HTLC with the preimage on-chain. With eltoo, when B closes the downstream\n> channel on-chain, it must wait the CSV timeout on the update transaction\n> before locking in the timed-out HTLC. This effectively means the CLTV delta\n> has to be greater than the CSV timeout, plus some extra (whereas it is\n> currently safe to make it significantly shorter). Is that true or am I\n> missing something?\n\nThat's a good point Jim. We need to make sure that the CLTVs are far\nenough in the future for the CSV timeout to expire and to grab any\npreimage downstream and insert it upstream. Overall this results in an\noffset of all the CLTVs to (less than) the maximum CSV timeout along the\npath. This would be a fixed offset for each channel and can be announced\nusing the gossip protocol, so senders can take it into consideration\nwhen computing the routes. Notice that this is not really the CLTV\ndelta, which would accumulate along the path, but an offset on which the\nCLTV deltas build on.\n\nIn today's network we have many nodes that have a CLTV delta of 144\nblocks, which quickly results in HTLC funds unavailable for several days\ndepending on the route length, so I don't think that adding a fixed\noffset is much worse. Once we have watch-towers we can reduce both the\noffset as well as the CLTV deltas. Since eltoo makes watch-towers less\nexpensive, given the reduced storage costs, I'd argue that it's a net\npositive for the Lightning network (but then again I'm biased) :-)"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-01T15:50:27",
                "message_text_only": "Can you explain why a fixed offset along the whole circuit is enough to\nensure safely as opposed to an increased delta at each hop?\n\nOn Tue, May 1, 2018, 5:05 AM Christian Decker <decker.christian at gmail.com>\nwrote:\n\n> Jim Posen <jim.posen at gmail.com> writes:\n> > If my understanding is correct though, this construction would\n> > significantly increase the safe CLTV delta requirements because HTLCs\n> > cannot be timed out immediately on the settlement transaction. Consider a\n> > case where node B receives an HTLC from A and forwards to C. If the HTLC\n> > offered to C times out and C does not fail the HTLC off-chain, Lightning\n> > currently guarantees that the CLTV delta is sufficient that I may close\n> the\n> > channel to C on-chain and claim the timed-out HTLC before my upstream\n> HTLC\n> > to A times out. If the CLTV delta is too small, I may fail the upstream\n> > HTLC as soon as it times out, and then C may still claim the downstream\n> > HTLC with the preimage on-chain. With eltoo, when B closes the downstream\n> > channel on-chain, it must wait the CSV timeout on the update transaction\n> > before locking in the timed-out HTLC. This effectively means the CLTV\n> delta\n> > has to be greater than the CSV timeout, plus some extra (whereas it is\n> > currently safe to make it significantly shorter). Is that true or am I\n> > missing something?\n>\n> That's a good point Jim. We need to make sure that the CLTVs are far\n> enough in the future for the CSV timeout to expire and to grab any\n> preimage downstream and insert it upstream. Overall this results in an\n> offset of all the CLTVs to (less than) the maximum CSV timeout along the\n> path. This would be a fixed offset for each channel and can be announced\n> using the gossip protocol, so senders can take it into consideration\n> when computing the routes. Notice that this is not really the CLTV\n> delta, which would accumulate along the path, but an offset on which the\n> CLTV deltas build on.\n>\n> In today's network we have many nodes that have a CLTV delta of 144\n> blocks, which quickly results in HTLC funds unavailable for several days\n> depending on the route length, so I don't think that adding a fixed\n> offset is much worse. Once we have watch-towers we can reduce both the\n> offset as well as the CLTV deltas. Since eltoo makes watch-towers less\n> expensive, given the reduced storage costs, I'd argue that it's a net\n> positive for the Lightning network (but then again I'm biased) :-)\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180501/817a0ce3/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-01T16:29:09",
                "message_text_only": "Jim Posen <jim.posen at gmail.com> writes:\n> Can you explain why a fixed offset along the whole circuit is enough to\n> ensure safely as opposed to an increased delta at each hop?\n\nSure. Let's assume we have chosen a path `A->B->C->D->E`. For simplicity\nlet's assume they all have a CLTV delta of 144 blocks (lnd's default\nsetting). Furthermore let's assume that the CSV timeout for the channels\nis also 144.\n\nThis means that with the current LN-penalty mechanism you'd have the\nfollowing CLTV deltas in the HTLC:\n\n```\nA -(576)-> B -(432)-> C -(288)-> D -(144)-> E\n```\n\nMeaning that if the current time is approaching the absolute CLTV we\nneed initiate a channel closure to safely fetch the preimage on-chain,\nand be able to turn around and send it on the upstream channel.\n\nThis is minimal, but can be arbitrarily higher, if you follow the best\npractice of obfuscating the final destination by building a shadow route\nbehind the real recipient, and add it's CLTV deltas and fees to your\nroute.\n\nWith eltoo you'd need to make sure that you have the settlement\ntransaction confirmed before your desired CLTV timeout delta begins to\ncount down. So if the CLTV of the HTLC is `now + CSV timeout + CLTV\ndelta` you need to initiate a close, whereas Lightning allows you to\nwait for time `now + CLTV delta`. Effectively this results in the\nfollowing time deltas:\n\n```\nA -(576+144)-> B -(432+144)-> C -(288+144)-> D -(144+144)-> E\n```\n\nTaking the last hop for example, if we had a CLTV of 1000 with eltoo\nwe'd need to start closing at height 712, instead of 856 with\nLN-penalty. However, this increased delta does not accumulate along the\npath, it's just a fixed offset. The longer the route, the smaller the\nactual impact of this offset."
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-01T17:07:22",
                "message_text_only": "I'm still not following why this doesn't accumulate.\n\nIn the example route, let's look at it from the point of view of C. C sees\nthe following regardless of whether D or E or someone behind E is the last\nhop in the route:\n\nB -> HTLC(expire = X + delta) -> C -> HTLC(expire = X) -> D\n\nSo D is not required to reveal the preimage before time X, and in the case\nof an on-chain settle, C needs to be able to redeem the HTLC output through\nthe timeout clause before time X + delta. C can't redeem the HTLC (with\nsufficient confirmations) at least until the settlement transaction is\nconfirmed. So it seems to me that regardless of the overall route and the\nmaximum CSV on it, the delta for the C hop has to be greater than the CSV\ndelay on the update transaction. And that this must be true at every hop\nfor the same reason.\n\nOn Tue, May 1, 2018 at 9:29 AM, Christian Decker <decker.christian at gmail.com\n> wrote:\n\n> Jim Posen <jim.posen at gmail.com> writes:\n> > Can you explain why a fixed offset along the whole circuit is enough to\n> > ensure safely as opposed to an increased delta at each hop?\n>\n> Sure. Let's assume we have chosen a path `A->B->C->D->E`. For simplicity\n> let's assume they all have a CLTV delta of 144 blocks (lnd's default\n> setting). Furthermore let's assume that the CSV timeout for the channels\n> is also 144.\n>\n> This means that with the current LN-penalty mechanism you'd have the\n> following CLTV deltas in the HTLC:\n>\n> ```\n> A -(576)-> B -(432)-> C -(288)-> D -(144)-> E\n> ```\n>\n> Meaning that if the current time is approaching the absolute CLTV we\n> need initiate a channel closure to safely fetch the preimage on-chain,\n> and be able to turn around and send it on the upstream channel.\n>\n> This is minimal, but can be arbitrarily higher, if you follow the best\n> practice of obfuscating the final destination by building a shadow route\n> behind the real recipient, and add it's CLTV deltas and fees to your\n> route.\n>\n> With eltoo you'd need to make sure that you have the settlement\n> transaction confirmed before your desired CLTV timeout delta begins to\n> count down. So if the CLTV of the HTLC is `now + CSV timeout + CLTV\n> delta` you need to initiate a close, whereas Lightning allows you to\n> wait for time `now + CLTV delta`. Effectively this results in the\n> following time deltas:\n>\n> ```\n> A -(576+144)-> B -(432+144)-> C -(288+144)-> D -(144+144)-> E\n> ```\n>\n> Taking the last hop for example, if we had a CLTV of 1000 with eltoo\n> we'd need to start closing at height 712, instead of 856 with\n> LN-penalty. However, this increased delta does not accumulate along the\n> path, it's just a fixed offset. The longer the route, the smaller the\n> actual impact of this offset.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180501/ecea5831/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-01T17:31:28",
                "message_text_only": "Jim Posen <jim.posen at gmail.com> writes:\n> I'm still not following why this doesn't accumulate.\n>\n> In the example route, let's look at it from the point of view of C. C sees\n> the following regardless of whether D or E or someone behind E is the last\n> hop in the route:\n>\n> B -> HTLC(expire = X + delta) -> C -> HTLC(expire = X) -> D\n>\n> So D is not required to reveal the preimage before time X, and in the case\n> of an on-chain settle, C needs to be able to redeem the HTLC output through\n> the timeout clause before time X + delta. C can't redeem the HTLC (with\n> sufficient confirmations) at least until the settlement transaction is\n> confirmed. So it seems to me that regardless of the overall route and the\n> maximum CSV on it, the delta for the C hop has to be greater than the CSV\n> delay on the update transaction. And that this must be true at every hop\n> for the same reason.\n\nThat'd be a purely reactionary behavior, i.e., chosing the delta in such\na way that I can both settle the channel and have enough time to react\nto turn around and reveal the preimage. So with the assumptions we had\nbefore (CSV = 144 and CLTV delta = 144) you'd have an effective delta of\n288 on each hop, yes. That's basically the case in which each channel\nreacts serially.\n\nYou can trivially parallelize these closures by looking ahead and\nnoticing that each hop really just cares about its own closure deadline,\ni.e., each node just cares to close 288 blocks before the CLTV expires,\nnot that its delta w.r.t. to the downstream channel is that far in the\nfuture. So all we care about is that once we are due to give the\nupstream hop the preimage we've already closed the downstream channel\nand can now read the HTLC preimage from that channel.\n\nThe CSV timeout isn't part of the delta on each hop, but we need to\nimplement the deadline computation as:\n\n```\nCLTV - CLTV delta - CSV\n```\n\ninstead of LN-penaltiy's\n\n```\nCLTV - CLTV delta\n```"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-02T01:15:10",
                "message_text_only": "OK, I see what you are saying. You are effectively suggesting pipelining\nthe broadcasts of the update transactions. I think this introduces a\nproblem that a node in the circuit that withholds the preimage for too long\ncan force all upstream channels to be closed, at only the expense of their\none upstream channel being closed. I believe such an attack could\nsignificantly disrupt the network.\n\nLet me elaborate on the way I'm thinking about this:\n\nSo say I'm a routing node with an upstream HTLC with CLTV = X. I need to\nensure that if I learn the preimage, that I have time to broadcast and\nconfirm an HTLC-success transaction before height X. We'll call this number\nof blocks D_success. So if I know the preimage, let's say X - D_success is\nthe latest height that I can safely broadcast the HTLC-success transaction,\nassuming the settlement transaction is already final (ie. the update\ntransaction is confirmed and the CSV delay has passed). So now I also need\nto know when to close the channel with the update transaction. I'll assume\nit will take at most D_update blocks from the time I broadcast the update\ntransaction for it to be mined. So unless the downstream HTLC is already\nfailed, I should always close the upstream channel at height X - D_success\n- CSV_update - D_update.\n\nNow we'll look at the downstream HTLC with CLTV = Y. In order to minimize\nthe safe delta between the upstream and downstream CLTVs, I will want to\nbroadcast and confirm an HTLC-timeout transaction as soon after height Y as\npossible. So assuming that the downstream settlement transaction is final\nat height Y and it takes at most D_timeout blocks for the HTLC timeout\ntransaction to confirm once it is final assuming no double spends, then Y +\nD_timeout is very latest I might learn the payment preimage from the\ndownstream channel on-chain. So I should be safe as long as X - D_success >\nY + D_timeout. This assumes that the update transaction for the downstream\nchannel is already mined and the CSV has passed. However, we know from\nabove that I had to close the upstream channel at time X - D_success -\nCSV_update - D_update, which may very well be before Y. So if the\ndownstream hop waits until just before Y to publish the preimage, they can\nforce me to close my upstream channel. This applies transitively for\nfurther upstream hops, assuming a large enough CSV value.\n\nGranted, upstream hops can watch the blockchain for preimage reveals in\nother closings transaction and perhaps fulfill off-chain if there is\nsufficient time. This would not be possible with payment decorrelation\nthrough scriptless scripts or the like.\n\nDoes that logic sound right to you?\n\nOn Tue, May 1, 2018 at 10:31 AM, Christian Decker <\ndecker.christian at gmail.com> wrote:\n\n> Jim Posen <jim.posen at gmail.com> writes:\n> > I'm still not following why this doesn't accumulate.\n> >\n> > In the example route, let's look at it from the point of view of C. C\n> sees\n> > the following regardless of whether D or E or someone behind E is the\n> last\n> > hop in the route:\n> >\n> > B -> HTLC(expire = X + delta) -> C -> HTLC(expire = X) -> D\n> >\n> > So D is not required to reveal the preimage before time X, and in the\n> case\n> > of an on-chain settle, C needs to be able to redeem the HTLC output\n> through\n> > the timeout clause before time X + delta. C can't redeem the HTLC (with\n> > sufficient confirmations) at least until the settlement transaction is\n> > confirmed. So it seems to me that regardless of the overall route and the\n> > maximum CSV on it, the delta for the C hop has to be greater than the CSV\n> > delay on the update transaction. And that this must be true at every hop\n> > for the same reason.\n>\n> That'd be a purely reactionary behavior, i.e., chosing the delta in such\n> a way that I can both settle the channel and have enough time to react\n> to turn around and reveal the preimage. So with the assumptions we had\n> before (CSV = 144 and CLTV delta = 144) you'd have an effective delta of\n> 288 on each hop, yes. That's basically the case in which each channel\n> reacts serially.\n>\n> You can trivially parallelize these closures by looking ahead and\n> noticing that each hop really just cares about its own closure deadline,\n> i.e., each node just cares to close 288 blocks before the CLTV expires,\n> not that its delta w.r.t. to the downstream channel is that far in the\n> future. So all we care about is that once we are due to give the\n> upstream hop the preimage we've already closed the downstream channel\n> and can now read the HTLC preimage from that channel.\n>\n> The CSV timeout isn't part of the delta on each hop, but we need to\n> implement the deadline computation as:\n>\n> ```\n> CLTV - CLTV delta - CSV\n> ```\n>\n> instead of LN-penaltiy's\n>\n> ```\n> CLTV - CLTV delta\n> ```\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180501/91de4f67/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-07T23:26:05",
                "message_text_only": "Hi Jimpo,\n\nYou're correct that the introduction of symmetric state now re-introduces\nthe\ndependency between the CSV value of the commitment, and the HTLC timeouts.\nIt's\nworth nothing that this issue existed in an earlier version of the BOLT\nspec,\nthis was pointed out by Mats in the past: [1][2]. The dependency meant that\nif\nwe wanted to allow very long CSV time outs (like 1 month +), then this would\nhave the adverse effect of increasing the total CLTV timeout along the\nentire\nroute. As a result, we moved to the 2-stage HTLC scheme which is now\nimplemented and deployed as a part of BOLT 1.0. It may be the case that in\nthe\nmid to near future, most implementations aren't concerned about long time\nlocks\ndue to the existence of robust and reliable private outsourcers.\n\nAs a side effect of the way the symmetric state changes the strategy around\nbreach attempts, we may see more breach attempts (and therefore update\ntransactions) on the chain since attempting to cheat w/ vanilla symmetric\nstate\nis now \"costless\" (worst case we just use the latest state, best case I can\ncommit the state better for me. This is in stark contrast to\npunishment/slashing based approaches where a failed breach attempt results\nin\nthe cheating party losing all their funds.\n\nHowever, with a commitment protocol that uses symmetric state. The 2-stage\nHTLC\nscheme doesn't actually apply. Observe that with Lighting's current\nasymmetric\nstate commitment protocol, the \"clock\" starts ticking as soon as the\ncommitment\nhits the chain, and we follow the \"if an output pays to me, it must be\ndelayed\nas I may be attempting a breach\". With symmetric state this no longer\napplies,\nthe clock instead starts \"officially\" ticking after the latest update\ntransaction has hit the chain, and there are no further challenges. As a\nresult\nof this, the commitment transaction itself doesn't need to have any CSV\ndelays\nwithin the Script branches of the outputs it creates. Instead, each of those\noutputs can be immediately be spent as the challenge period has already\nelapsed,\nand from the PoV of the chain, this is now the \"correct\" commitment. Due to\nthis, the HTLC outputs would now be symmetric themselves, and look very much\nlike an HTLC output that one would use in a vanilla on-chain cross-chain\natomic\nswap.\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2015-September/000182.html\n[2]:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2015-November/000339.html\n\n\nOn Tue, May 1, 2018 at 6:15 PM Jim Posen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> OK, I see what you are saying. You are effectively suggesting pipelining\n> the broadcasts of the update transactions. I think this introduces a\n> problem that a node in the circuit that withholds the preimage for too long\n> can force all upstream channels to be closed, at only the expense of their\n> one upstream channel being closed. I believe such an attack could\n> significantly disrupt the network.\n>\n> Let me elaborate on the way I'm thinking about this:\n>\n> So say I'm a routing node with an upstream HTLC with CLTV = X. I need to\n> ensure that if I learn the preimage, that I have time to broadcast and\n> confirm an HTLC-success transaction before height X. We'll call this number\n> of blocks D_success. So if I know the preimage, let's say X - D_success is\n> the latest height that I can safely broadcast the HTLC-success transaction,\n> assuming the settlement transaction is already final (ie. the update\n> transaction is confirmed and the CSV delay has passed). So now I also need\n> to know when to close the channel with the update transaction. I'll assume\n> it will take at most D_update blocks from the time I broadcast the update\n> transaction for it to be mined. So unless the downstream HTLC is already\n> failed, I should always close the upstream channel at height X - D_success\n> - CSV_update - D_update.\n>\n> Now we'll look at the downstream HTLC with CLTV = Y. In order to minimize\n> the safe delta between the upstream and downstream CLTVs, I will want to\n> broadcast and confirm an HTLC-timeout transaction as soon after height Y as\n> possible. So assuming that the downstream settlement transaction is final\n> at height Y and it takes at most D_timeout blocks for the HTLC timeout\n> transaction to confirm once it is final assuming no double spends, then Y +\n> D_timeout is very latest I might learn the payment preimage from the\n> downstream channel on-chain. So I should be safe as long as X - D_success >\n> Y + D_timeout. This assumes that the update transaction for the downstream\n> channel is already mined and the CSV has passed. However, we know from\n> above that I had to close the upstream channel at time X - D_success -\n> CSV_update - D_update, which may very well be before Y. So if the\n> downstream hop waits until just before Y to publish the preimage, they can\n> force me to close my upstream channel. This applies transitively for\n> further upstream hops, assuming a large enough CSV value.\n>\n> Granted, upstream hops can watch the blockchain for preimage reveals in\n> other closings transaction and perhaps fulfill off-chain if there is\n> sufficient time. This would not be possible with payment decorrelation\n> through scriptless scripts or the like.\n>\n> Does that logic sound right to you?\n>\n> On Tue, May 1, 2018 at 10:31 AM, Christian Decker <\n> decker.christian at gmail.com> wrote:\n>\n>> Jim Posen <jim.posen at gmail.com> writes:\n>> > I'm still not following why this doesn't accumulate.\n>> >\n>> > In the example route, let's look at it from the point of view of C. C\n>> sees\n>> > the following regardless of whether D or E or someone behind E is the\n>> last\n>> > hop in the route:\n>> >\n>> > B -> HTLC(expire = X + delta) -> C -> HTLC(expire = X) -> D\n>> >\n>> > So D is not required to reveal the preimage before time X, and in the\n>> case\n>> > of an on-chain settle, C needs to be able to redeem the HTLC output\n>> through\n>> > the timeout clause before time X + delta. C can't redeem the HTLC (with\n>> > sufficient confirmations) at least until the settlement transaction is\n>> > confirmed. So it seems to me that regardless of the overall route and\n>> the\n>> > maximum CSV on it, the delta for the C hop has to be greater than the\n>> CSV\n>> > delay on the update transaction. And that this must be true at every hop\n>> > for the same reason.\n>>\n>> That'd be a purely reactionary behavior, i.e., chosing the delta in such\n>> a way that I can both settle the channel and have enough time to react\n>> to turn around and reveal the preimage. So with the assumptions we had\n>> before (CSV = 144 and CLTV delta = 144) you'd have an effective delta of\n>> 288 on each hop, yes. That's basically the case in which each channel\n>> reacts serially.\n>>\n>> You can trivially parallelize these closures by looking ahead and\n>> noticing that each hop really just cares about its own closure deadline,\n>> i.e., each node just cares to close 288 blocks before the CLTV expires,\n>> not that its delta w.r.t. to the downstream channel is that far in the\n>> future. So all we care about is that once we are due to give the\n>> upstream hop the preimage we've already closed the downstream channel\n>> and can now read the HTLC preimage from that channel.\n>>\n>> The CSV timeout isn't part of the delta on each hop, but we need to\n>> implement the deadline computation as:\n>>\n>> ```\n>> CLTV - CLTV delta - CSV\n>> ```\n>>\n>> instead of LN-penaltiy's\n>>\n>> ```\n>> CLTV - CLTV delta\n>> ```\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180507/9008f25b/attachment.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-10T13:57:30",
                "message_text_only": "Olaoluwa Osuntokun via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> writes:\n\n> Hi Jimpo,\n>\n> You're correct that the introduction of symmetric state now\n> re-introduces the dependency between the CSV value of the commitment,\n> and the HTLC timeouts.  It's worth nothing that this issue existed in\n> an earlier version of the BOLT spec, this was pointed out by Mats in\n> the past: [1][2]. The dependency meant that if we wanted to allow very\n> long CSV time outs (like 1 month +), then this would have the adverse\n> effect of increasing the total CLTV timeout along the entire route. As\n> a result, we moved to the 2-stage HTLC scheme which is now implemented\n> and deployed as a part of BOLT 1.0. It may be the case that in the mid\n> to near future, most implementations aren't concerned about long time\n>  locks due to the existence of robust and reliable private\n> outsourcers.\n\nIt's worth mentioning that the requirement for extremely large CLTV deltas\nwould already create incredibly long CLTV deltas between the endpoints,\nsince the endpoint delta accumulates along the path. This is true for\nLN-Penalty as well as eltoo. eltoo's requirement to settle before the\nHTLCs touch the blockchain adds a stage in which need to start on-chain\nsettlement to ensure the HTLC hits the chain before its CLTV\nexpires. We can imagine this as a separate timewindow, that does not\naccumulate across multiple hops (settlement ordering is not an issue,\nCLTV resolution is).\n\nMy hope is that indeed with the simpler watch-towers we can reduce both\nthe CLTV deltas as well as the settlement timeouts for eltoo, so that\nthey become negligible.\n\n> As a side effect of the way the symmetric state changes the strategy\n> around breach attempts, we may see more breach attempts (and therefore\n> update transactions) on the chain since attempting to cheat w/ vanilla\n> symmetric state is now \"costless\" (worst case we just use the latest\n> state, best case I can commit the state better for me. This is in\n> stark contrast to punishment/slashing based approaches where a failed\n> breach attempt results in the cheating party losing all their funds.\n\nNot exactly costless, since the breaching party will have to pay the\non-chain fees, and we may be able to reintroduce the reserve in order to\nadd an additional punishment on top of the simple update mechanism\n(selectively introducing asymmetry).\n\n> However, with a commitment protocol that uses symmetric state. The\n> 2-stage HTLC scheme doesn't actually apply. Observe that with\n> Lighting's current asymmetric state commitment protocol, the \"clock\"\n> starts ticking as soon as the commitment hits the chain, and we follow\n> the \"if an output pays to me, it must be delayed as I may be\n> attempting a breach\". With symmetric state this no longer applies, the\n> clock instead starts \"officially\" ticking after the latest update\n> transaction has hit the chain, and there are no further challenges. As\n> a result of this, the commitment transaction itself doesn't need to\n> have any CSV delays within the Script branches of the outputs it\n> creates. Instead, each of those outputs can be immediately be spent as\n> the challenge period has already elapsed, and from the PoV of the\n> chain, this is now the \"correct\" commitment. Due to this, the HTLC\n> outputs would now be symmetric themselves, and look very much like an\n> HTLC output that one would use in a vanilla on-chain cross-chain\n> atomic swap.\n\nIn addition to this it is worth pointing out that the old/replaced HTLCs\nhave no way of ever touching the blockchain, so we can throw away a\nwhole heap of data about these HTLCs, that we would have to carry around\nindefinitely if this were not the case. The same reason the HTLCs start\nticking when a settlement touches the chain in LN-penalty is also the\nreason we need to carry all that data around. eltoo can be said to\ncontain the two stage HTLC commit we added on top of LN-penalty.\n\nCheers,\nChristian"
            }
        ],
        "thread_summary": {
            "title": "eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Olaoluwa Osuntokun",
                "Jim Posen",
                "Christian Decker"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 29808
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-01T05:07:54",
                "message_text_only": "Good morning Christian,\n\nThis is very interesting indeed!\n\nI have started skimming through the paper.\n\nI am uncertain if the below text is correct?\n\n> Throughout this paper we will use the terms *input script* to refer to `witnessProgram` and `scriptPubKey`, and *output script* to refer to the `witness` or `scriptSig`.\n\nFigure 2 contains to what looks to me like a `witnessProgram`, `scriptPubKey`, or `redeemScript` but refers to it as an \"output script\":\n\n>    OP_IF\n>        10 OP_CSV\n>        2 As Bs 2 OP_CHECKMULTISIGVERIFY\n>    OP_ELSE\n>        2 Au Bu 2 OP_CHECKMULTISIGVERIFY\n>    OP_ENDIF\n>\n> Figure 2: The output script used by the on-chain update transactions.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-01T11:38:12",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> Good morning Christian,\n>\n> This is very interesting indeed!\n>\n> I have started skimming through the paper.\n>\n> I am uncertain if the below text is correct?\n>\n>> Throughout this paper we will use the terms *input script* to refer to `witnessProgram` and `scriptPubKey`, and *output script* to refer to the `witness` or `scriptSig`.\n>\n> Figure 2 contains to what looks to me like a `witnessProgram`, `scriptPubKey`, or `redeemScript` but refers to it as an \"output script\":\n>\n>>    OP_IF\n>>        10 OP_CSV\n>>        2 As Bs 2 OP_CHECKMULTISIGVERIFY\n>>    OP_ELSE\n>>        2 Au Bu 2 OP_CHECKMULTISIGVERIFY\n>>    OP_ENDIF\n>>\n>> Figure 2: The output script used by the on-chain update transactions.\n>\n> Regards,\n> ZmnSCPxj\n\nDarn last minute changes! Yes, you are right, I seem to have flipped the\ntwo definitions. I'll fix that up and push a new version."
            }
        ],
        "thread_summary": {
            "title": "eltoo: A Simplified update Mechanism for Lightning and Off-Chain Contracts",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Christian Decker"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1594
        }
    },
    {
        "title": "[bitcoin-dev] BIP sighash_noinput",
        "thread_messages": [
            {
                "author": "Russell O'Connor",
                "date": "2018-05-01T16:58:37",
                "message_text_only": "At the risk of bikeshedding, shouldn't NOINPUT also zero out the\nhashSequence so that its behaviour is consistent with ANYONECANPAY?\n\nOn Mon, Apr 30, 2018 at 12:29 PM, Christian Decker via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> I'd like to pick up the discussion from a few months ago, and propose a new\n> sighash flag, `SIGHASH_NOINPUT`, that removes the commitment to the\n> previous\n> output. This was previously mentioned on the list by Joseph Poon [1], but\n> was\n> never formally proposed, so I wrote a proposal [2].\n>\n> We have long known that `SIGHASH_NOINPUT` would be a great fit for\n> Lightning.\n> They enable simple watch-towers, i.e., outsource the need to watch the\n> blockchain for channel closures, and react appropriately if our\n> counterparty\n> misbehaves. In addition to this we just released the eltoo [3,4] paper\n> which\n> describes a simplified update mechanism that can be used in Lightning, and\n> other\n> off-chain contracts, with any number of participants.\n>\n> By not committing to the previous output being spent by the transaction,\n> we can\n> rebind an input to point to any outpoint with a matching output script and\n> value. The binding therefore is no longer explicit through a reference, but\n> through script compatibility, and the transaction ID reference in the\n> input is a\n> hint to validators. The sighash flag is meant to enable some off-chain\n> use-cases\n> and should not be used unless the tradeoffs are well-known. In particular\n> we\n> suggest using contract specific key-pairs, in order to avoid having any\n> unwanted\n> rebinding opportunities.\n>\n> The proposal is very minimalistic, and simple. However, there are a few\n> things\n> where we'd like to hear the input of the wider community with regards to\n> the\n> implementation details though. We had some discussions internally on\n> whether to\n> use a separate opcode or a sighash flag, some feeling that the sighash flag\n> could lead to some confusion with existing wallets, but given that we have\n> `SIGHASH_NONE`, and that existing wallets will not sign things with unknown\n> flags, we decided to go the sighash way. Another thing is that we still\n> commit\n> to the amount of the outpoint being spent. The rationale behind this is\n> that,\n> while rebinding to outpoints with the same value maintains the value\n> relationship between input and output, we will probably not want to bind to\n> something with a different value and suddenly pay a gigantic fee.\n>\n> The deployment part of the proposal is left vague on purpose in order not\n> to\n> collide with any other proposals. It should be possible to introduce it by\n> bumping the segwit script version and adding the new behavior.\n>\n> I hope the proposal is well received, and I'm looking forward to discussing\n> variants and tradeoffs here. I think the applications we proposed so far\n> are\n> quite interesting, and I'm sure there are many more we can enable with this\n> change.\n>\n> Cheers,\n> Christian\n>\n> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2016-February/012460.html\n> [2] https://github.com/cdecker/bips/blob/noinput/bip-xyz.mediawiki\n> [3] https://blockstream.com/2018/04/30/eltoo-next-lightning.html\n> [4] https://blockstream.com/eltoo.pdf\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180501/e8691a8c/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-01T17:32:32",
                "message_text_only": "Russell O'Connor <roconnor at blockstream.io> writes:\n> At the risk of bikeshedding, shouldn't NOINPUT also zero out the\n> hashSequence so that its behaviour is consistent with ANYONECANPAY?\n\nGood catch, must've missed that somehow. I'll amend the BIP accordingly."
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-04T09:15:41",
                "message_text_only": "Good morning Christian and list,\n\nIt seems to me, that `SIGHASH_NOINPUT` may help make some protocol integrate better with existing wallets.\n\nI remember vaguely that `SIGHASH_NOINPUT` was also mentioned before in LN discussions, when the issue of transaction malleation was considered (before SegWit, being totally uncontroversial, was massively adopted).  The sketch below, I believe, is somewhat consistent with how it could have been used in funding a channel.\n\nConsider a CoinSwap protocol.  Each side writes a transaction that pays out to an ordinary 2-of-2 multisig address.  But before each side writes and signs that transaction, it first demands a timelocked backout transaction to let them recover their own funds in case it falls through (more generally, every offchain protocol has a similar setup stage where some way to back out is signed before all parties enter into the contract).\n\nNow, without `SIGHASH_NOINPUT`, we would first require that the initial funding transaction be written (but not signed and broadcast), and then the txid to the other side.  The other side then generates the backout transaction (which requires the txid and outnum of the funding outpoint) and returns the signature for the backout transaction to the first side.\n\nBecause of this, an implementation of CoinSwap needs to have control of its own coins.  This means that coin selection, blockchain tracking, and mempool tracking (i.e. to handle RBFs, which would invalidate any further transactions if you used coins received by RBF-able transactions while unconfirmed) needs to be implemented.\n\nBut it would be much nicer if instead the CoinSwap implementation could simply say \"okay, I started our CoinSwap, now send X coins to address A\", and then the user uses their ordinary wallet software to send to that address (obviously before the CoinSwap can start, the user must first provide an address to which the backoff transaction should pay; but in fact that could simply be the same as the other address in the swap).\n\n1.  The user will not have to make a separate transfer from their wallet, then initiate a swap, then transfer from the CoinSwap implementation to their usual wallet: instead the user gets an address from their wallet, initiates the swap, then pays to the address the CoinSwap implementation said to pay and wait to receive the swapped funds to their normal wallet.\n2.  Implementing the CoinSwap program is now somewhat easier since we do not need to manage our own funds: the software only needs to manage the single particular coin that was paid to the single address being used in the swap.\n3.  The smaller number of required features for use means easier implementation and testing.  It also makes it more likely to be implemented in the first place, since the effort to make it is smaller.\n4.  The lack of a wallet means users can use a trusted wallet implementation (cold storage, hardware wallet, etc) in conjunction with the software, and only risk the amount that passes through the CoinSwap software (which is smaller, since it does not have to include any extra funds to pay for fees).\n\nWith `SIGHASH_NOINPUT`, we can indeed implement such a walletless CoinSwap (or other protocol) software.  We only need to provide the public keys that will be used in the initial 2-of-2, and the other side can create a signature with `SIGHASH_NOINPUT` flag.\n\nThe setup of the CoinSwap then goes this way.  The swapping nodes exchange public keys (two for each side in this case), they agree on who gets to move first in the swap and who generates the preimage, and then they agree on what the backout transactions look like (in particular, they agree on the address the backout transactions spend) and create signatures, with `SIGHASH_NOINPUT`.  In particular, the signatures do not commit to the txid of the transaction that they authorize spending.  The CoinSwap sofwares then turn around to their users and say \"okay, send to this address\", the users initiate the swap using their normal wallet software, the CoinSwap software monitors only the address it asked the user to use, then when it appears onchain (the CoinSwap software does not even need to track the mempool) it continues with the HTLC offers and preimage exchanges until the protocol completes.\n\nIn a world where walletless CoinSwap exists, consider this:\n\n1.  A user buys Bitcoin from an exchange.  The exchange operates a wallet which they credit when the user buys Bitcoin.\n2.  The user starts a CoinSwap, giving the destination address from their cold-storage wallet.\n3.  The CoinSwap tells the user an address to send to.  The user withdraws money from the exchange using that address as destination (1 transaction)\n4.  The user waits for the CoinSwap to finish, which causes the funds to appear in their cold-storage wallet (1 transaction).\n\nIf CoinSwap implementations all needed their own wallets, then instead:\n\n1.  A user buys Bitcoin from an exchange.\n2.  The user withdraws the funds from the exchange to a CoinSwap implementation wallet (1 transaction).\n3.  The user performs a CoinSwap which goes back to the CoinSwap implementation wallet (2 transactions).\n4.  The user sends from the CoinSwap wallet to their cold storage (1 transaction). (granted, the CoinSwap implementation could offer a feature that immediately transfers the swapped funds to some other wallet, but we still cannot get around the transfer from the exchange to the CoinSwap wallet)\n\nA drawback of course, is that `SIGHASH_NOINPUT` is an unusual flag to use; it immediately paints the user as using some special protocol.  So much for `SIGHASH_NOINPUT` CoinSwap.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-04T11:09:09",
                "message_text_only": "ZmnSCPxj <ZmnSCPxj at protonmail.com> writes:\n> It seems to me, that `SIGHASH_NOINPUT` may help make some protocol\n> integrate better with existing wallets.\n\nDepends on which end of a transaction the existing wallet is: existing\nwallets will refuse to sign a transaction with an unknown sighash flag,\nbut if the wallet is creating the output that'll later be spent using a\n`SIGHASH_NOINPUT` transaction it won't (and shouldn't) care.\n\n> I remember vaguely that `SIGHASH_NOINPUT` was also mentioned before in\n> LN discussions, when the issue of transaction malleation was\n> considered (before SegWit, being totally uncontroversial, was\n> massively adopted).  The sketch below, I believe, is somewhat\n> consistent with how it could have been used in funding a channel.\n\nI consider `SIGHASH_NOINPUT` to be a poor-man's malleability fix, since\nit comes with some baggage. Without trying to undermine my own proposal,\nbut address reuse in combination with binding through script, can lead\nto very unexpected results. You need to be very careful about where you\nallow rebinding, hence the warnings in the proposal.\n\n> Consider a CoinSwap protocol.  Each side writes a transaction that\n> pays out to an ordinary 2-of-2 multisig address.  But before each side\n> writes and signs that transaction, it first demands a timelocked\n> backout transaction to let them recover their own funds in case it\n> falls through (more generally, every offchain protocol has a similar\n> setup stage where some way to back out is signed before all parties\n> enter into the contract).\n>\n> ...\n>\n> With `SIGHASH_NOINPUT`, we can indeed implement such a walletless\n> CoinSwap (or other protocol) software.  We only need to provide the\n> public keys that will be used in the initial 2-of-2, and the other\n> side can create a signature with `SIGHASH_NOINPUT` flag.\n\nI was wondering whether we could actually skip one communication round\nw.r.t. the previously described CoinSwap protocol, but it turns out we\nneed to at least exchange public keys before actually moving any\nfunds. Would have been nice to do spontaneous CoinSwaps.\n\n> The setup of the CoinSwap then goes this way.  The swapping nodes\n> exchange public keys (two for each side in this case), they agree on\n> who gets to move first in the swap and who generates the preimage, and\n> then they agree on what the backout transactions look like (in\n> particular, they agree on the address the backout transactions spend)\n> and create signatures, with `SIGHASH_NOINPUT`.  In particular, the\n> signatures do not commit to the txid of the transaction that they\n> authorize spending.  The CoinSwap sofwares then turn around to their\n> users and say \"okay, send to this address\", the users initiate the\n> swap using their normal wallet software, the CoinSwap software\n> monitors only the address it asked the user to use, then when it\n> appears onchain (the CoinSwap software does not even need to track the\n> mempool) it continues with the HTLC offers and preimage exchanges\n> until the protocol completes.\n>\n> In a world where walletless CoinSwap exists, consider this:\n>\n> 1.  A user buys Bitcoin from an exchange.  The exchange operates a\n> wallet which they credit when the user buys Bitcoin.\n> 2.  The user starts a CoinSwap, giving the destination address from\n> their cold-storage wallet.\n> 3.  The CoinSwap tells the user an address to send to.  The user\n> withdraws money from the exchange using that address as destination (1\n> transaction)\n> 4.  The user waits for the CoinSwap to finish, which causes the funds\n> to appear in their cold-storage wallet (1 transaction).\n>\n> If CoinSwap implementations all needed their own wallets, then instead:\n>\n> 1.  A user buys Bitcoin from an exchange.\n> 2.  The user withdraws the funds from the exchange to a CoinSwap\n> implementation wallet (1 transaction).\n> 3.  The user performs a CoinSwap which goes back to the CoinSwap\n> implementation wallet (2 transactions).\n> 4.  The user sends from the CoinSwap wallet to their cold storage (1\n> transaction). (granted, the CoinSwap implementation could offer a\n> feature that immediately transfers the swapped funds to some other\n> wallet, but we still cannot get around the transfer from the exchange\n> to the CoinSwap wallet)\n>\n> A drawback of course, is that `SIGHASH_NOINPUT` is an unusual flag to\n> use; it immediately paints the user as using some special protocol.\n> So much for `SIGHASH_NOINPUT` CoinSwap.\n\nBy providing a new use-case you are contributing to the obfuscation of\nthis technique. The more normal the use of `SIGHASH_NOINPUT` becomes the\nless an observer can learn from it being used. In combination with MAST,\nTaproot or Graftroot we can further hide the details of the executed\nprotocol :-)"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-04T14:25:46",
                "message_text_only": "Good morning Christian,\n\n\n> ZmnSCPxj ZmnSCPxj at protonmail.com writes:\n> \n> > It seems to me, that `SIGHASH_NOINPUT` may help make some protocol\n> > \n> > integrate better with existing wallets.\n> \n> Depends on which end of a transaction the existing wallet is: existing\n> \n> wallets will refuse to sign a transaction with an unknown sighash flag,\n> \n> but if the wallet is creating the output that'll later be spent using a\n> \n> `SIGHASH_NOINPUT` transaction it won't (and shouldn't) care.\n>\n\nYes, the intent is that specialized utilities (like the CoinSwap I gave as an example) would be the ones signing with `SIGHASH_NOINPUT`, with the existing wallet generating the output that will be spent with a `SIGHASH_NOINPUT`.\n\nThe issue is that some trustless protocols have an offchain component, where some kind of backoff transaction is created, and the creation involves the 3 steps (1) make but do not sign&broadcast a funding tx (2) make and sign a backoff transaction that spends the funding tx (3) sign and broadcast the original funding tx. This holds for Poon-Dryja, your new eltoo Decker-Russell-Osuntokun, and CoinSwap.  Commodity user wallets and exchange wallets only support the most basic \"make tx, sign, broadcast\", and integrating with the generalized funding transaction pattern is not possible.  `SIGHASH_NOINPUT` allows us to make the backoff transaction first, then make the funding transaction via the usual \"make tx, sign, broadcast\" procedure that commodity wallets implement.\n\n> > A drawback of course, is that `SIGHASH_NOINPUT` is an unusual flag to\n> > \n> > use; it immediately paints the user as using some special protocol.\n> > \n> > So much for `SIGHASH_NOINPUT` CoinSwap.\n> \n> By providing a new use-case you are contributing to the obfuscation of\n> \n> this technique. The more normal the use of `SIGHASH_NOINPUT` becomes the\n> \n> less an observer can learn from it being used. In combination with MAST,\n> \n> Taproot or Graftroot we can further hide the details of the executed\n> \n> protocol :-)\n\nThinking about it further, it turns out that in the cooperative completion of the protocol, we do not need to sign anything using `SIGHASH_NOINPUT`, but can use the typical `SIGHASH_ALL`. Indeed all generalized funding transaction patterns can be updated to use this: only the initial backout transaction needs to be signed with `SIGHASH_NOINPUT`, all others can be signed with `SIGHASH_ALL`, including the protocol conclusion transaction.\n\n1.  In CoinSwapCS, TX-0 and TX-1 are funding transactions.  The backoff transaction is the TX-2 and TX-3 transactions.  Only TX-2 and TX-3 need be signed with `SIGHASH_NOINPUT`.  TX-4 and TX-5, which complete the protocol and hide the swap, can be signed with `SIGHASH_ALL`.\n\n2.  In Poon-Dryja, the backoff transaction is the very first commitment transaction.  Again only that transaction needs to be signed with `SIGHASH_NOINPUT`: future commitment transactions as well as the mutual close transaction can be signed with `SIGHASH_ALL`.\n\n3.  In Decker-Russell-Osuntokun, the backoff transaction is the trigger transaction and the first settlement transaction.  The trigger transaction can sign with `SIGHASH_NOINPUT`.  Then only the final settlement (i.e. mutual close) can be signed with `SIGHASH_ALL`.\n\nThus if the protocol completes cooperatively, the only onchain evidence is that a 2-of-2 multisig is spent, and signed using `SIGHASH_ALL`, and the money goes to some ordinary P2WPKH addresses.\n\nThe advantage, as I mentioned, is that these protocols can be implemented using \"walletless\" software: the special protocol software runs the protocol up to the point that they get the backoff transaction, then asks the user to pay an exact amount to an exact address.  This has a number of advantages:\n\n1.  RBF can be supported if the wallet software supports RBF.  In particular without `SIGHASH_NOINPUT` the protocol would require renegotiation of a new backoff transaction in order to support RBF (and in particular the protocol spec would need to be designed in the first place to consider that possibility!), and would become more complicated since while a new backoff transaction is being negotiated, the previous version of the funding transaction may get confirmed.  With `SIGHASH_NOINPUT` all the specialized protocol software needs to do, is to watch for a transaction paying to the given address to be confirmed deeply enough to be unlikely to be reorganized: there is no need to renegotiate a backoff transaction, because whatever transaction gets confirmed, as long as it pays to the address with a given amount, the signature for the backoff transaction remains valid for it.\n2.  Wallet software of any kind can be used in conjunction with special protocol software of any kind.  Hardware wallets do not need to implement LN: the LN software starts a channel and gives a P2WSH address that hardware wallets know how to pay to.  Ditto for exchange wallets.  Etc.  And if a future protocol arises that uses the funding transaction pattern again, then again existing wallets can integrate with those protocols via P2WSH address.\n3.  Special protocol software need not implement even basic wallet functionality: they can just focus on the specific protocol they implement.  Consider how until late last year c-lightning needed a separate RPC command to inform it that it received funds, and a few months ago we had many issues with UTXOs in our database getting out of sync with the blockchain (why we implemented `dev-rescan-outputs`).\n\nRegards.\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-07T19:40:46",
                "message_text_only": "Given the general enthusiasm, and lack of major criticism, for the\n`SIGHASH_NOINPUT` proposal, I'd like to formally ask the BBEs (benevolent\nBIP editors) to be assigned a BIP number. I have hacked together a\nsimple implementation of the hashing implementation in Bitcoin Core [1]\nthough I think it's unlikely to sail through review, and given the lack\nof ground-work on witness V1 scripts, I can't really test it now, and\nonly the second commit is part of the implementation itself.\n\nOne issue that was raised off list was that some fork coins have used\nsighash 0x40 as FORKID. This does not conflict with this proposal since\nthe proposal only applies to segwit transactions, which the fork coins\nhave explicitly disabled :-)\n\nI'm looking forward to discussing how to we can move forward to\nimplementing this proposal, and how we can combine multiple proposals\ninto the next soft-fork.\n\nCheers,\nChristian\n\n[1] https://github.com/cdecker/bitcoin/tree/noinput"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-05-07T20:51:11",
                "message_text_only": "A technical point about SIGHASH_NOINPUT: It seems like a more general and\ntechnically simpler to implement idea would be to have a boolean specifying\nwhether the inputs listed must be all of them (the way it works normally)\nor a subset of everything. It feels like a similar boolean should be made\nfor outputs as well. Or maybe a single boolean should apply to both. In any\ncase, one could always use SIGHASH_SUBSET and not specify any inputs and\nthat would have the same effect as SIGHASH_NOINPUT.\n\nOn Mon, May 7, 2018 at 12:40 PM, Christian Decker via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Given the general enthusiasm, and lack of major criticism, for the\n> `SIGHASH_NOINPUT` proposal, I'd like to formally ask the BBEs (benevolent\n> BIP editors) to be assigned a BIP number. I have hacked together a\n> simple implementation of the hashing implementation in Bitcoin Core [1]\n> though I think it's unlikely to sail through review, and given the lack\n> of ground-work on witness V1 scripts, I can't really test it now, and\n> only the second commit is part of the implementation itself.\n>\n> One issue that was raised off list was that some fork coins have used\n> sighash 0x40 as FORKID. This does not conflict with this proposal since\n> the proposal only applies to segwit transactions, which the fork coins\n> have explicitly disabled :-)\n>\n> I'm looking forward to discussing how to we can move forward to\n> implementing this proposal, and how we can combine multiple proposals\n> into the next soft-fork.\n>\n> Cheers,\n> Christian\n>\n> [1] https://github.com/cdecker/bitcoin/tree/noinput\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180507/6a2c236a/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2018-05-08T14:40:21",
                "message_text_only": "On Mon, May 07, 2018 at 09:40:46PM +0200, Christian Decker via bitcoin-dev wrote:\n> Given the general enthusiasm, and lack of major criticism, for the\n> `SIGHASH_NOINPUT` proposal, [...]\n\nSo first, I'm not sure if I'm actually criticising or playing devil's\nadvocate here, but either way I think criticism always helps produce\nthe best proposal, so....\n\nThe big concern I have with _NOINPUT is that it has a huge failure\ncase: if you use the same key for multiple inputs and sign one of them\nwith _NOINPUT, you've spent all of them. The current proposal kind-of\nlimits the potential damage by still committing to the prevout amount,\nbut it still seems a big risk for all the people that reuse addresses,\nwhich seems to be just about everyone.\n\nI wonder if it wouldn't be ... I'm not sure better is the right word,\nbut perhaps \"more realistic\" to have _NOINPUT be a flag to a signature\nfor a hypothetical \"OP_CHECK_SIG_FOR_SINGLE_USE_KEY\" opcode instead,\nso that it's fundamentally not possible to trick someone who regularly\nreuses keys to sign something for one input that accidently authorises\nspends of other inputs as well.\n\nIs there any reason why an OP_CHECKSIG_1USE (or OP_CHECKMULTISIG_1USE)\nwouldn't be equally effective for the forseeable usecases? That would\nensure that a _NOINPUT signature is only ever valid for keys deliberately\nintended to be single use, rather than potentially valid for every key.\n\nIt would be ~34 witness bytes worse than being able to spend a Schnorr\naggregate key directly, I guess; but that's not worse than the normal\ntaproot tradeoff: you spend the aggregate key directly in the normal,\ncooperative case; and reserve the more expensive/NOINPUT case for the\nunusual, uncooperative cases. I believe that works fine for eltoo: in\nthe cooperative case you just do a SIGHASH_ALL spend of the original\ntransaction, and _NOINPUT isn't needed.\n\nMaybe a different opcode maybe makes sense at a \"philosophical\" level:\nnormal signatures are signing a spend of a particular \"coin\" (in the\nUTXO sense), while _NOINPUT signatures are in some sense signing a spend\nof an entire \"wallet\" (all the coins spendable by a particular key, or\nmore accurately for the current proposal, all the coins of a particular\nvalue spendable by a particular key). Those are different intentions,\nso maybe it's reasonable to encode them in different addresses, which\nin turn could be done by having a new opcode for _NOINPUT.\n\nA new opcode has the theoretical advantage that it could be deployed\ninto the existing segwit v0 address space, rather than waiting for segwit\nv1. Not sure that's really meaningful, though.\n\nCheers,\naj"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-09T23:01:39",
                "message_text_only": "> The current proposal kind-of limits the potential damage by still\ncommitting\n> to the prevout amount, but it still seems a big risk for all the people\nthat\n> reuse addresses, which seems to be just about everyone.\n\nThe typical address re-use doesn't apply here as this is a sighash flag that\nwould only really be used for doing various contracts on Bitcoin. I don't\nsee any reason why \"regular\" wallets would update to use this sighash flag.\nWe've also seen first hand with segwit that wallet authors are slow to pull\nin the latest and greatest features available, even if they solve nuisance\nissues like malleability and can result in lower fees.\n\nIMO, sighash_none is an even bigger footgun that already exists in the\nprotocol today.\n\n-- Laolu\n\n\nOn Tue, May 8, 2018 at 7:41 AM Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, May 07, 2018 at 09:40:46PM +0200, Christian Decker via bitcoin-dev\n> wrote:\n> > Given the general enthusiasm, and lack of major criticism, for the\n> > `SIGHASH_NOINPUT` proposal, [...]\n>\n> So first, I'm not sure if I'm actually criticising or playing devil's\n> advocate here, but either way I think criticism always helps produce\n> the best proposal, so....\n>\n> The big concern I have with _NOINPUT is that it has a huge failure\n> case: if you use the same key for multiple inputs and sign one of them\n> with _NOINPUT, you've spent all of them. The current proposal kind-of\n> limits the potential damage by still committing to the prevout amount,\n> but it still seems a big risk for all the people that reuse addresses,\n> which seems to be just about everyone.\n>\n> I wonder if it wouldn't be ... I'm not sure better is the right word,\n> but perhaps \"more realistic\" to have _NOINPUT be a flag to a signature\n> for a hypothetical \"OP_CHECK_SIG_FOR_SINGLE_USE_KEY\" opcode instead,\n> so that it's fundamentally not possible to trick someone who regularly\n> reuses keys to sign something for one input that accidently authorises\n> spends of other inputs as well.\n>\n> Is there any reason why an OP_CHECKSIG_1USE (or OP_CHECKMULTISIG_1USE)\n> wouldn't be equally effective for the forseeable usecases? That would\n> ensure that a _NOINPUT signature is only ever valid for keys deliberately\n> intended to be single use, rather than potentially valid for every key.\n>\n> It would be ~34 witness bytes worse than being able to spend a Schnorr\n> aggregate key directly, I guess; but that's not worse than the normal\n> taproot tradeoff: you spend the aggregate key directly in the normal,\n> cooperative case; and reserve the more expensive/NOINPUT case for the\n> unusual, uncooperative cases. I believe that works fine for eltoo: in\n> the cooperative case you just do a SIGHASH_ALL spend of the original\n> transaction, and _NOINPUT isn't needed.\n>\n> Maybe a different opcode maybe makes sense at a \"philosophical\" level:\n> normal signatures are signing a spend of a particular \"coin\" (in the\n> UTXO sense), while _NOINPUT signatures are in some sense signing a spend\n> of an entire \"wallet\" (all the coins spendable by a particular key, or\n> more accurately for the current proposal, all the coins of a particular\n> value spendable by a particular key). Those are different intentions,\n> so maybe it's reasonable to encode them in different addresses, which\n> in turn could be done by having a new opcode for _NOINPUT.\n>\n> A new opcode has the theoretical advantage that it could be deployed\n> into the existing segwit v0 address space, rather than waiting for segwit\n> v1. Not sure that's really meaningful, though.\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180509/ba4835de/attachment-0001.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-09T23:04:58",
                "message_text_only": "Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n> On Mon, May 07, 2018 at 09:40:46PM +0200, Christian Decker via bitcoin-dev wrote:\n>> Given the general enthusiasm, and lack of major criticism, for the\n>> `SIGHASH_NOINPUT` proposal, [...]\n>\n> So first, I'm not sure if I'm actually criticising or playing devil's\n> advocate here, but either way I think criticism always helps produce\n> the best proposal, so....\n>\n> The big concern I have with _NOINPUT is that it has a huge failure\n> case: if you use the same key for multiple inputs and sign one of them\n> with _NOINPUT, you've spent all of them. The current proposal kind-of\n> limits the potential damage by still committing to the prevout amount,\n> but it still seems a big risk for all the people that reuse addresses,\n> which seems to be just about everyone.\n\nIf I can convince you to sign with SIGHASH_NONE, it's already a problem\ntoday.\n\n> I wonder if it wouldn't be ... I'm not sure better is the right word,\n> but perhaps \"more realistic\" to have _NOINPUT be a flag to a signature\n> for a hypothetical \"OP_CHECK_SIG_FOR_SINGLE_USE_KEY\" opcode instead,\n> so that it's fundamentally not possible to trick someone who regularly\n> reuses keys to sign something for one input that accidently authorises\n> spends of other inputs as well.\n\nThat was also suggested by Mark Friedenbach, but I think we'll end up\nwith more \"magic key\" a-la Schnorr/taproot/graftroot and less script in\nfuture.\n\nThat means we'd actually want a different Segwit version for\n\"NOINPUT-can-be-used\", which seems super ugly.\n\n> Maybe a different opcode maybe makes sense at a \"philosophical\" level:\n> normal signatures are signing a spend of a particular \"coin\" (in the\n> UTXO sense), while _NOINPUT signatures are in some sense signing a spend\n> of an entire \"wallet\" (all the coins spendable by a particular key, or\n> more accurately for the current proposal, all the coins of a particular\n> value spendable by a particular key). Those are different intentions,\n> so maybe it's reasonable to encode them in different addresses, which\n> in turn could be done by having a new opcode for _NOINPUT.\n\nIn a world where SIGHASH_NONE didn't exist, this might be an argument :)\n\nCheers,\nRusty."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-07T23:47:23",
                "message_text_only": "Super stoked to see that no_input has been resurrected!!! I actually\nimplemented a variant back in 2015 when Tadge first described the approach\nto\nme for both btcd [1], and bitcoind [2]. The version being proposed is\n_slightly_ differ though, as the initial version I implemented still\ncommitted\nto the script being sent, while this new version just relies on\nwitness validity instead. This approach is even more flexible as the script\nattached to the output being spent can change, without rendering the\nspending\ntransaction invalid as long as the witness still ratifies a branch in the\noutput's predicate.\n\nGiven that this would introduce a _new_ sighash flag, perhaps we should also\nattempt to bundle additional more flexible sighash flags concurrently as\nwell?\nThis would require a larger overhaul w.r.t to how sighash flags are\ninterpreted, so in this case, we may need to introduce a new CHECKSIG\noperator\n(lets call it CHECKSIG_X for now), which would consume an available noop\nopcode. As a template for more fine grained sighashing control, I'll refer\nto\njl2012's BIP-0YYY [3] (particularly the \"New nHashType definitions\"\nsection).\nThis was originally proposed in the context of his merklized script work as\nit\nmore or less opened up a new opportunity to further modify script within the\ncontext of merklized script executions.  The approach reads in the\nsighash flags as a bit vector, and allows developers to express things like:\n\"don't sign the input value, nor the sequence, but sign the output of this\ninput, and ONLY the script of this output\". This approach is _extremely_\npowerful, and one would be able to express the equivalent of no_input by\nsetting the appropriate bits in the sighash.\n\nLooking forward in hearing y'alls thoughts on this approach, thanks.\n\n[1]: https://github.com/Roasbeef/btcd/commits/SIGHASH_NOINPUT\n[2]: https://github.com/Roasbeef/bitcoin/commits/SIGHASH_NOINPUT\n[3]:\nhttps://github.com/jl2012/bips/blob/vault/bip-0YYY.mediawiki#new-nhashtype-definitions\n\n-- Laolu\n\nOn Mon, Apr 30, 2018 at 10:30 AM Christian Decker via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> I'd like to pick up the discussion from a few months ago, and propose a new\n> sighash flag, `SIGHASH_NOINPUT`, that removes the commitment to the\n> previous\n> output. This was previously mentioned on the list by Joseph Poon [1], but\n> was\n> never formally proposed, so I wrote a proposal [2].\n>\n> We have long known that `SIGHASH_NOINPUT` would be a great fit for\n> Lightning.\n> They enable simple watch-towers, i.e., outsource the need to watch the\n> blockchain for channel closures, and react appropriately if our\n> counterparty\n> misbehaves. In addition to this we just released the eltoo [3,4] paper\n> which\n> describes a simplified update mechanism that can be used in Lightning, and\n> other\n> off-chain contracts, with any number of participants.\n>\n> By not committing to the previous output being spent by the transaction,\n> we can\n> rebind an input to point to any outpoint with a matching output script and\n> value. The binding therefore is no longer explicit through a reference, but\n> through script compatibility, and the transaction ID reference in the\n> input is a\n> hint to validators. The sighash flag is meant to enable some off-chain\n> use-cases\n> and should not be used unless the tradeoffs are well-known. In particular\n> we\n> suggest using contract specific key-pairs, in order to avoid having any\n> unwanted\n> rebinding opportunities.\n>\n> The proposal is very minimalistic, and simple. However, there are a few\n> things\n> where we'd like to hear the input of the wider community with regards to\n> the\n> implementation details though. We had some discussions internally on\n> whether to\n> use a separate opcode or a sighash flag, some feeling that the sighash flag\n> could lead to some confusion with existing wallets, but given that we have\n> `SIGHASH_NONE`, and that existing wallets will not sign things with unknown\n> flags, we decided to go the sighash way. Another thing is that we still\n> commit\n> to the amount of the outpoint being spent. The rationale behind this is\n> that,\n> while rebinding to outpoints with the same value maintains the value\n> relationship between input and output, we will probably not want to bind to\n> something with a different value and suddenly pay a gigantic fee.\n>\n> The deployment part of the proposal is left vague on purpose in order not\n> to\n> collide with any other proposals. It should be possible to introduce it by\n> bumping the segwit script version and adding the new behavior.\n>\n> I hope the proposal is well received, and I'm looking forward to discussing\n> variants and tradeoffs here. I think the applications we proposed so far\n> are\n> quite interesting, and I'm sure there are many more we can enable with this\n> change.\n>\n> Cheers,\n> Christian\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-February/012460.html\n> [2] https://github.com/cdecker/bips/blob/noinput/bip-xyz.mediawiki\n> [3] https://blockstream.com/2018/04/30/eltoo-next-lightning.html\n> [4] https://blockstream.com/eltoo.pdf\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180507/76a7fcac/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-10T14:12:21",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> Super stoked to see that no_input has been resurrected!!! I actually\n> implemented a variant back in 2015 when Tadge first described the\n> approach to me for both btcd [1], and bitcoind [2]. The version being\n> proposed is _slightly_ differ though, as the initial version I\n> implemented still committed to the script being sent, while this new\n> version just relies on witness validity instead. This approach is even\n> more flexible as the script attached to the output being spent can\n> change, without rendering the spending transaction invalid as long as\n> the witness still ratifies a branch in the output's predicate.\n\nYeah, we removed the script commitment out of necessity for eltoo, but\nit seems to add a lot of flexibility that might be useful. One\nadditional use-case that came to mind is having a recovery transaction\nfor vault-like scenarios, i.e., a transaction that can short-circuit a\nthawing process of frozen funds. You'd keep that transaction in a vault,\npre-signed and bind it to whatever action you'd like to interrupt.\n\n> Given that this would introduce a _new_ sighash flag, perhaps we\n> should also attempt to bundle additional more flexible sighash flags\n> concurrently as well?  This would require a larger overhaul w.r.t to\n> how sighash flags are interpreted, so in this case, we may need to\n> introduce a new CHECKSIG operator (lets call it CHECKSIG_X for now),\n> which would consume an available noop opcode. As a template for more\n> fine grained sighashing control, I'll refer to jl2012's BIP-0YYY [3]\n> (particularly the \"New nHashType definitions\" section).  This was\n> originally proposed in the context of his merklized script work as it\n> more or less opened up a new opportunity to further modify script\n> within the context of merklized script executions.  The approach reads\n> in the sighash flags as a bit vector, and allows developers to express\n> things like: \"don't sign the input value, nor the sequence, but sign\n> the output of this input, and ONLY the script of this output\". This\n> approach is _extremely_ powerful, and one would be able to express the\n> equivalent of no_input by setting the appropriate bits in the sighash.\n\nI purposefully made the proposal as small and as well defined as\npossible, with a number of possible applications to back it, since I\nthink this might be the best way to introduce a new feature and make it\nas uncontroversial as possible. I'm not opposed to additional flags\nbeing deployed in parallel, but they'll need their own justification and\nanalysis, and shouldn't be rushed just \"because we're doing noinput\".\n\nGoing for a separate op-code is definitely an option we considered, but\njust for noinput it'd be duplicating quite a lot of existing\nfunctionality. With additional sighash flags it might become necessary,\nbut I don't think it is necessary just for noinput.\n\n> Looking forward in hearing y'alls thoughts on this approach, thanks.\n>\n> [1]: https://github.com/Roasbeef/btcd/commits/SIGHASH_NOINPUT\n> [2]: https://github.com/Roasbeef/bitcoin/commits/SIGHASH_NOINPUT\n> [3]: https://github.com/jl2012/bips/blob/vault/bip-0YYY.mediawiki#new-nhashtype-definitions\n>\n> -- Laolu"
            }
        ],
        "thread_summary": {
            "title": "BIP sighash_noinput",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Anthony Towns",
                "Russell O'Connor",
                "ZmnSCPxj",
                "Olaoluwa Osuntokun",
                "Bram Cohen",
                "Christian Decker"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 40319
        }
    },
    {
        "title": "[bitcoin-dev] Multi-signature and multi-coin HD wallet in one BIP32 derivation path (new BIP)",
        "thread_messages": [
            {
                "author": "Clark Moody",
                "date": "2018-05-04T00:09:38",
                "message_text_only": "Paul,\n\nThe current BIP-49 / 84 use the purpose field of the derivation path to specify\nthe address format.\n\n\u200bI think sticking with the one-BIP-one-format method works. Otherwise, you\nwould need to modify this proposed BIP each time a new format comes along.\nIn that case, existing wallets that claim BIP-XXXX compliance will be\nincomplete.\n\n\n-Clark\n\nOn Thu, Apr 26, 2018 at 9:05 AM, Paul Brown <paul at 345.systems> wrote:\n\n> Hi\n>\n> I realised after I sent my previous response that the encoding was wrong\n> and that my smiley face at the end of the BIP number comment got turned\n> into a ? and the tongue in cheek context was lost :-(\n>\n> Anyway, back onto subject.  I've been thinking some more on the SLIP-0032\n> adoption in this proposal and specifically the address format to use when\n> generating addresses.\n>\n> My proposal states bech32 serialized addresses (P2WPKH or P2WSH), however,\n> I wonder whether there is some merit in extending the derivation path with\n> an additional level below coin type to represent the address format, with\n> the value determined by the context of the coin type value in the\n> derivation path (0x00 for P2WPKH bech32, 0x01 for P2PKH base58 if coin type\n> is Bitcoin, 0x00 for Ethereum account format if coin type is Ether, etc).\n> A separate spec similar to SLIP-0044 could be created that defines the list\n> of address formats and the derivation path values.\n>\n> When importing root master seeds or distributing the xpub's for each\n> cosigner to each party the discovery process in the proposal would need\n> extending to try each address format in turn to determine whether there is\n> a 'hit' when checking balances.  It does mean that the import process is\n> slower however the additional flexibility of supporting multiple address\n> formats possibly outweighs this.  I'm just thinking that having a rule to\n> follow during discovery, particularly where non-Bitcoin coins are\n> concerned, is more explicit than leaving it open to the wallet implementer\n> to figure out (for altcoins, what address format to use?).\n>\n> It also means that future address formats are supported as they are simply\n> added to the new spec list for the coin type (can be done by anyone,\n> similar to the way SLIP-0044 works now) - it doesn't require a new BIP to\n> support.  For example, if address format was a derivation level in BIP44,\n> would BIP49 and BIP84 be needed?\n>\n> I'm somewhat musing out loud here, but I like the idea of being able to\n> mostly self-discover as much as possible and reducing or eliminating the\n> need for proprietary metadata attached to the wallet.\n>\n> Cheers\n> Paul\n>\n> From: clarkmoody at gmail.com <clarkmoody at gmail.com> On Behalf Of Clark Moody\n> Sent: 25 April 2018 15:36\n> To: Paul Brown <paul at 345.systems>; Bitcoin Protocol Discussion <\n> bitcoin-dev at lists.linuxfoundation.org>\n> Subject: Re: [bitcoin-dev] Multi-signature and multi-coin HD wallet in one\n> BIP32 derivation path (new BIP)\n>\n> Thanks for the proposal, Paul.\n>\n> > - What address format is expected when discovering balances and creating\n> transactions?\n>\n> Your solution does not solve your first bullet point, since the xpub\n> encoding looks no different than any other xpub (BIP 44, 45, 49, etc). At\n> the least, you should propose new version bytes to change the \"xpub\" in the\n> encoding to some other string.\n>\n> Alternatively, I would suggest that you use the xpub serialization format\n> described in SLIP-0032 (\n> https://github.com/satoshilabs/slips/blob/master/slip-0032.md). It\n> includes the derivation path within the xpub itself and uses Bech32 for\n> encoding.\n>\n> Given a normal xpub with no additional information, a wallet must scan the\n> address space for the various formats. SLIP-0032 solves this bootstrapping\n> problem and avoids the UX nightmare of users being required to know to\n> which BIP number the xpub conforms.\n>\n> Also, @luke-jr will give you a hard time to self-assigning a BIP number ;-)\n>\n> Thanks\n>\n>\n>\n>\n> -Clark\n>\n> On Wed, Apr 25, 2018 at 4:35 AM, Paul Brown via bitcoin-dev <mailto:\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Hi\n>\n> I have written a new BIP describing a BIP32 derivation path that supports\n> a single or multi-signature and multi-coin wallet from a single master\n> seed.  It combines BIP44 and BIP45 and adds in a self-describing structure\n> in the derivation path for multiple multi-sig combinations within the\n> single wallet along with an extended public key export file format for\n> public key distribution between parties.  I can particularly see this being\n> useful for multiple Lightning Network 2of2 accounts for different payment\n> channels.\n>\n> The BIP can be found here:\n> https://github.com/gluexchange/bip/blob/master/bip-0046.mediawiki\n>\n> I appreciate that this might be re-hashing old ground as BIP44 in\n> particular has been widely adopted, however, BIP44 does leave itself open\n> to a lot of interpretation from a wallet portability perspective such as:\n>\n> - What address format is expected when discovering balances and creating\n> transactions?\n> - Does the master seed represent a single-sig or multi-sig wallet?\n> - If multi-sig, how many cosigners and what are their extended public keys\n> (so that the wallet can generate the correctly formatted redeem script with\n> public keys in the right order)?\n> - If multi-sig, how do you prevent collisions on the same address index\n> (in a wallet that is occasionally connected)?\n>\n> BIP45 solves the collision that occurs when the individual parties in a\n> multi-sig group each give out a new address from a wallet, where the wallet\n> hasn\u2019t been able to sync to mark the address as \u2018used\u2019 (this could happen\n> if they gave out addresses independently at the same time).  It uses a\n> cosigner index in the derivation path so that each party has their own path\n> to their addresses.  However, BIP45 drops the multi-coin support that BIP44\n> has.\n>\n> This is a useful discussion on the problems of a collision and the merits\n> of separating cosigners in the derivation path:\n> https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg05188.html\n>\n> For the purposes of the BIP text (and the example paths used to generate\n> keys) I\u2019ve temporarily assigned it the number 46.  It looks like that is\n> available and seemed somewhat appropriate given that it builds on the good\n> work of BIP44 and BIP45.\n>\n> Paul Brown\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> mailto:bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180504/90c4be2c/attachment-0001.html>"
            },
            {
                "author": "Paul Brown",
                "date": "2018-05-04T08:23:21",
                "message_text_only": "Hi Clark,\n\nThanks for the feedback.  I was somewhat coming to the same conclusion as yourself having had a few days to think on it.\n\nI am going to support SLIP-0032 for the serialization format of extended keys as I believe this adds value in terms of additional validation when extended public keys are shared by cosigners in a multi-sig group as each key import can be verified that it is indeed from a BIP-XX wallet and the size of the multi-sig group matches.  I\u2019ll re-issue the BIP, hopefully soon :-) \n\nCheers\nPaul\n\nFrom: Clark Moody <clark at clarkmoody.com> \nSent: 04 May 2018 01:10\nTo: Paul Brown <paul at 345.systems>\nCc: Clark Moody <clark at clarkmoody.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] Multi-signature and multi-coin HD wallet in one BIP32 derivation path (new BIP)\n\nPaul,\n\nThe current BIP-49 / 84 use the purpose field of the derivation path to\u00a0specify the address format.\n\n\n\u200bI think sticking with the one-BIP-one-format method works. Otherwise, you would need to modify this proposed BIP each time a new format comes along. In that case, existing wallets that claim BIP-XXXX compliance will be incomplete.\n\n\n-Clark\n\nOn Thu, Apr 26, 2018 at 9:05 AM, Paul Brown <mailto:paul at 345.systems> wrote:\nHi\n\nI realised after I sent my previous response that the encoding was wrong and that my smiley face at the end of the BIP number comment got turned into a ? and the tongue in cheek context was lost :-(\n\nAnyway, back onto subject.\u00a0 I've been thinking some more on the SLIP-0032 adoption in this proposal and specifically the address format to use when generating addresses.\n\nMy proposal states bech32 serialized addresses (P2WPKH or P2WSH), however, I wonder whether there is some merit in extending the derivation path with an additional level below coin type to represent the address format, with the value determined by the context of the coin type value in the derivation path (0x00 for P2WPKH bech32, 0x01 for P2PKH base58 if coin type is Bitcoin, 0x00 for Ethereum account format if coin type is Ether, etc).\u00a0 A separate spec similar to SLIP-0044 could be created that defines the list of address formats and the derivation path values.\n\nWhen importing root master seeds or distributing the xpub's for each cosigner to each party the discovery process in the proposal would need extending to try each address format in turn to determine whether there is a 'hit' when checking balances.\u00a0 It does mean that the import process is slower however the additional flexibility of supporting multiple address formats possibly outweighs this.\u00a0 I'm just thinking that having a rule to follow during discovery, particularly where non-Bitcoin coins are concerned, is more explicit than leaving it open to the wallet implementer to figure out (for altcoins, what address format to use?).\n\nIt also means that future address formats are supported as they are simply added to the new spec list for the coin type (can be done by anyone, similar to the way SLIP-0044 works now) - it doesn't require a new BIP to support.\u00a0 For example, if address format was a derivation level in BIP44, would BIP49 and BIP84 be needed?\n\nI'm somewhat musing out loud here, but I like the idea of being able to mostly self-discover as much as possible and reducing or eliminating the need for proprietary metadata attached to the wallet.\n\nCheers\nPaul\n\nFrom: mailto:clarkmoody at gmail.com <mailto:clarkmoody at gmail.com> On Behalf Of Clark Moody\nSent: 25 April 2018 15:36\nTo: Paul Brown <mailto:paul at 345.systems>; Bitcoin Protocol Discussion <mailto:bitcoin-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] Multi-signature and multi-coin HD wallet in one BIP32 derivation path (new BIP)\n\nThanks for the proposal, Paul.\n\n>\u00a0- What address format is expected when discovering balances and creating transactions?\n\nYour solution does not solve your first bullet point, since the xpub encoding looks no different than any other xpub (BIP 44, 45, 49, etc). At the least, you should propose new version bytes to change the \"xpub\" in the encoding to some other string.\n\nAlternatively, I would suggest that you use the xpub serialization format described in SLIP-0032 (https://github.com/satoshilabs/slips/blob/master/slip-0032.md). It includes the derivation path within the xpub itself and uses Bech32 for encoding.\n\nGiven a normal xpub with no additional information, a wallet must scan the address space for the various formats. SLIP-0032 solves this bootstrapping problem and avoids the UX nightmare of users being required to know to which BIP number the xpub conforms.\n\nAlso, @luke-jr will give you a hard time to self-assigning a BIP number ;-)\n\nThanks\n\n\n\n\n-Clark\n\nOn Wed, Apr 25, 2018 at 4:35 AM, Paul Brown via bitcoin-dev <mailto:mailto:bitcoin-dev at lists.linuxfoundation.org> wrote:\nHi\n\u00a0\nI have written a new BIP describing a BIP32 derivation path that supports a single or multi-signature and multi-coin wallet from a single master seed.\u00a0 It combines BIP44 and BIP45 and adds in a self-describing structure in the derivation path for multiple multi-sig combinations within the single wallet along with an extended public key export file format for public key distribution between parties.\u00a0 I can particularly see this being useful for multiple Lightning Network 2of2 accounts for different payment channels.\n\u00a0\nThe BIP can be found here: https://github.com/gluexchange/bip/blob/master/bip-0046.mediawiki\n\u00a0\nI appreciate that this might be re-hashing old ground as BIP44 in particular has been widely adopted, however, BIP44 does leave itself open to a lot of interpretation from a wallet portability perspective such as:\n\u00a0\n- What address format is expected when discovering balances and creating transactions?\n- Does the master seed represent a single-sig or multi-sig wallet?\n- If multi-sig, how many cosigners and what are their extended public keys (so that the wallet can generate the correctly formatted redeem script with public keys in the right order)?\n- If multi-sig, how do you prevent collisions on the same address index (in a wallet that is occasionally connected)?\n\u00a0\nBIP45 solves the collision that occurs when the individual parties in a multi-sig group each give out a new address from a wallet, where the wallet hasn\u2019t been able to sync to mark the address as \u2018used\u2019 (this could happen if they gave out addresses independently at the same time).\u00a0 It uses a cosigner index in the derivation path so that each party has their own path to their addresses.\u00a0 However, BIP45 drops the multi-coin support that BIP44 has.\n\u00a0\nThis is a useful discussion on the problems of a collision and the merits of separating cosigners in the derivation path: https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg05188.html\n\u00a0\nFor the purposes of the BIP text (and the example paths used to generate keys) I\u2019ve temporarily assigned it the number 46.\u00a0 It looks like that is available and seemed somewhat appropriate given that it builds on the good work of BIP44 and BIP45.\n\u00a0\nPaul Brown\n\u00a0\n\u00a0\n\n_______________________________________________\nbitcoin-dev mailing list\nmailto:mailto:bitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Multi-signature and multi-coin HD wallet in one BIP32 derivation path (new BIP)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Clark Moody",
                "Paul Brown"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 14048
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev]  BIP sighash_noinput",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2018-05-14T09:23:29",
                "message_text_only": "On Thu, May 10, 2018 at 08:34:58AM +0930, Rusty Russell wrote:\n> > The big concern I have with _NOINPUT is that it has a huge failure\n> > case: if you use the same key for multiple inputs and sign one of them\n> > with _NOINPUT, you've spent all of them. The current proposal kind-of\n> > limits the potential damage by still committing to the prevout amount,\n> > but it still seems a big risk for all the people that reuse addresses,\n> > which seems to be just about everyone.\n> If I can convince you to sign with SIGHASH_NONE, it's already a problem\n> today.\n\nSo, I don't find that very compelling: \"there's already a way to lose\nyour money, so it's fine to add other ways to lose your money\". And\nagain, I think NOINPUT is worse here, because a SIGHASH_NONE signature\nonly lets others take the coin you're trying to spend, messing up when\nusing NOINPUT can cause you to lose other coins as well (with caveats).\n\n> [...]\n> In a world where SIGHASH_NONE didn't exist, this might be an argument :)\n\nI could see either dropping support for SIGHASH_NONE for segwit\nv1 addresses, or possibly limiting SIGHASH_NONE in a similar way to\nlimiting SIGHASH_NOINPUT. Has anyone dug through the blockchain to see\nif SIGHASH_NONE is actually used/useful?\n\n> That was also suggested by Mark Friedenbach, but I think we'll end up\n> with more \"magic key\" a-la Schnorr/taproot/graftroot and less script in\n> future.\n\nTaproot and graftroot aren't \"less script\" at all -- if anything they're\nthe opposite in that suddenly every address can have a script path.\nI think NOINPUT has pretty much the same tradeoffs as taproot/graftroot\nscripts: in the normal case for both you just use a SIGHASH_ALL\nsignature to spend your funds; in the abnormal case for NOINPUT, you use\na SIGHASH_NOINPUT (multi)sig for unilateral eltoo closes or watchtower\npenalties, in the abnormal case for taproot/graftroot you use a script.\n\n> That means we'd actually want a different Segwit version for\n> \"NOINPUT-can-be-used\", which seems super ugly.\n\nThat's backwards. If you introduce a new opcode, you can use the existing\nsegwit version, rather than needing segwit v1. You certainly don't need\nv1 segwit for regular coins and v2 segwit for NOINPUT coins, if that's\nwhere you were going?\n\nFor segwit v0, that would mean your addresses for a key \"X\", might be:\n\n   [pubkey]  X    \n    - not usable with NOINPUT\n   [script]  2 X Y 2 CHECKMULTISIG\n    - not usable with NOINPUT\n   [script]  2 X Y 2 CHECKMULTISIG_1USE_VERIFY\n    - usable with NOINPUT (or SIGHASH_ALL)\n\nCHECKMULTISIG_1USE_VERIFY being soft-forked in by replacing an OP_NOP,\nof course. Any output spendable via a NOINPUT signature would then have\nhad to have been deliberately created as being spendable by NOINPUT.\n\nFor a new segwit version with taproot that likewise includes an opcode,\nthat might be:\n\n   [taproot]  X\n    - not usable with NOINPUT\n   [taproot]  X or: X CHECKSIG_1USE\n    - usable with NOINPUT\n\nIf you had two UTXOs (with the same value), then if you construct\na taproot witness script for the latter address it will look like:\n\n    X [X CHECKSIG_1USE] [sig_X_NOINPUT]\n\nand that signature can't be used for addresses that were just intending\nto pay to X, because the NOINPUT sig/sighash simply isn't supported\nwithout a taproot path that includes the CHECKSIG_1USE opcode.\n\nIn essence, with the above construction there's two sorts of addresses\nyou generate from a public key X: addresses where you spend each coin\nindividually, and different addresses where you spend the wallet of\ncoins with that public key (and value) at once; and that remains the\nsame even if you use a single key for both.\n\nI think it's slightly more reasonable to worry about signing with NOINPUT\ncompared to signing with SIGHASH_NONE: you could pretty reasonably setup\nyour (light) bitcoin wallet to not be able to sign (or verify) with\nSIGHASH_NONE ever; but if you want to use lightning v2, it seems pretty\nlikely your wallet will be signing things with SIGHASH_NOINPUT. From\nthere, it's a matter of having a bug or a mistake cause you to\ncross-contaminate keys into your lightning subsystem, and not be\nsufficiently protected by other measures (eg, muSig versus checkmultisig).\n\n(For me the Debian ssh key generation bug from a decade ago is sufficient\nevidence that people you'd think are smart and competent do make really\nstupid mistakes in real life; so defense in depth here makes sense even\nthough you'd have to do really stupid things to get a benefit from it)\n\nThe other benefit of a separate opcode is support can be soft-forked in\nindependently of a new segwit version (either earlier or later).\n\nI don't think the code has to be much more complicated with a separate\nopcode; passing an extra flag to TransactionSignatureChecker::CheckSig()\nis probably close to enough. Some sort of flag remains needed anyway\nsince v0 and pre-segwit signatures won't support NOINPUT.\n\nCheers,\naj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-15T14:28:22",
                "message_text_only": "Anthony Towns <aj at erisian.com.au> writes:\n\n> On Thu, May 10, 2018 at 08:34:58AM +0930, Rusty Russell wrote:\n>> > The big concern I have with _NOINPUT is that it has a huge failure\n>> > case: if you use the same key for multiple inputs and sign one of them\n>> > with _NOINPUT, you've spent all of them. The current proposal kind-of\n>> > limits the potential damage by still committing to the prevout amount,\n>> > but it still seems a big risk for all the people that reuse addresses,\n>> > which seems to be just about everyone.\n>> If I can convince you to sign with SIGHASH_NONE, it's already a problem\n>> today.\n>\n> So, I don't find that very compelling: \"there's already a way to lose\n> your money, so it's fine to add other ways to lose your money\". And\n> again, I think NOINPUT is worse here, because a SIGHASH_NONE signature\n> only lets others take the coin you're trying to spend, messing up when\n> using NOINPUT can cause you to lose other coins as well (with caveats).\n\n`SIGHASH_NOINPUT` is a rather powerful tool, but has to be used\nresponsibly, which is why we always mention that it shouldn't be used\nlightly. Then again all sighash flags can be dangerous if not well\nunderstood. Think for example `SIGHASH_SINGLE` with it's pitfall when\nthe input has no matching output, or the already mentioned SIGHASH_NONE.\n\n>From a technical, and risk, point of view I don't think there is much\ndifference between a new opcode or a new sighash flag, with the\nactivation being the one exception. I personally believe that a segwit\nscript bump has cleaner semantics than soft-forking in a new opcode\n(which has 90% overlap with the existing checksig and checkmultisig\nopcodes).\n\n>> [...]\n>> In a world where SIGHASH_NONE didn't exist, this might be an argument :)\n>\n> I could see either dropping support for SIGHASH_NONE for segwit\n> v1 addresses, or possibly limiting SIGHASH_NONE in a similar way to\n> limiting SIGHASH_NOINPUT. Has anyone dug through the blockchain to see\n> if SIGHASH_NONE is actually used/useful?\n\nThat's a good point, I'll try looking for it once I get back to my full\nnode :-) And yes, `SIGHASH_NONE` should also come with all the warning\nsigns about not using it without a very good reason.\n\n>> That was also suggested by Mark Friedenbach, but I think we'll end up\n>> with more \"magic key\" a-la Schnorr/taproot/graftroot and less script in\n>> future.\n>\n> Taproot and graftroot aren't \"less script\" at all -- if anything they're\n> the opposite in that suddenly every address can have a script path.\n> I think NOINPUT has pretty much the same tradeoffs as taproot/graftroot\n> scripts: in the normal case for both you just use a SIGHASH_ALL\n> signature to spend your funds; in the abnormal case for NOINPUT, you use\n> a SIGHASH_NOINPUT (multi)sig for unilateral eltoo closes or watchtower\n> penalties, in the abnormal case for taproot/graftroot you use a script.\n\nThat's true for today's uses of `SIGHASH_NOINPUT` and others, but there\nmight be other uses that we don't know about in which noinput isn't just\nused for the contingency, handwavy I know. That's probably not the case\nfor graftroot/taproot, but I'm happy to be corrected on that one.\n\nStill, these opcodes and hash flags being mainly used for contingencies,\ndoesn't remove the need for these contingency options to be enforced\non-chain.\n\n>> That means we'd actually want a different Segwit version for\n>> \"NOINPUT-can-be-used\", which seems super ugly.\n>\n> That's backwards. If you introduce a new opcode, you can use the existing\n> segwit version, rather than needing segwit v1. You certainly don't need\n> v1 segwit for regular coins and v2 segwit for NOINPUT coins, if that's\n> where you were going?\n>\n> For segwit v0, that would mean your addresses for a key \"X\", might be:\n>\n>    [pubkey]  X    \n>     - not usable with NOINPUT\n>    [script]  2 X Y 2 CHECKMULTISIG\n>     - not usable with NOINPUT\n>    [script]  2 X Y 2 CHECKMULTISIG_1USE_VERIFY\n>     - usable with NOINPUT (or SIGHASH_ALL)\n>\n> CHECKMULTISIG_1USE_VERIFY being soft-forked in by replacing an OP_NOP,\n> of course. Any output spendable via a NOINPUT signature would then have\n> had to have been deliberately created as being spendable by NOINPUT.\n\nThe main reason I went for the sighash flag instead of an opcode is that\nit has clean semantics, allows for it to be bundled with a number of\nother upgrades, and doesn't use up NOP-codes, which I was lectured\nfor my normalized tx BIP (BIP140) is a rare resource that should be used\nsparingly. The `SIGHASH_NOINPUT` proposal is minimal, since it enhances\n4 existing opcodes. If we were to do that with new opcodes we'd either\nwant a multisig and a singlesig variant, potentially with a verify\nvariant each. That's a lot of opcodes.\n\nThe proposal being minimal should also help against everybody trying to\nget their favorite feature added, and hopefully streamline the\ndiscussion.\n\n> For a new segwit version with taproot that likewise includes an opcode,\n> that might be:\n>\n>    [taproot]  X\n>     - not usable with NOINPUT\n>    [taproot]  X or: X CHECKSIG_1USE\n>     - usable with NOINPUT\n>\n> If you had two UTXOs (with the same value), then if you construct\n> a taproot witness script for the latter address it will look like:\n>\n>     X [X CHECKSIG_1USE] [sig_X_NOINPUT]\n>\n> and that signature can't be used for addresses that were just intending\n> to pay to X, because the NOINPUT sig/sighash simply isn't supported\n> without a taproot path that includes the CHECKSIG_1USE opcode.\n>\n> In essence, with the above construction there's two sorts of addresses\n> you generate from a public key X: addresses where you spend each coin\n> individually, and different addresses where you spend the wallet of\n> coins with that public key (and value) at once; and that remains the\n> same even if you use a single key for both.\n>\n> I think it's slightly more reasonable to worry about signing with NOINPUT\n> compared to signing with SIGHASH_NONE: you could pretty reasonably setup\n> your (light) bitcoin wallet to not be able to sign (or verify) with\n> SIGHASH_NONE ever; but if you want to use lightning v2, it seems pretty\n> likely your wallet will be signing things with SIGHASH_NOINPUT. From\n> there, it's a matter of having a bug or a mistake cause you to\n> cross-contaminate keys into your lightning subsystem, and not be\n> sufficiently protected by other measures (eg, muSig versus checkmultisig).\n\nI think the same can be addressed by simply having the wallet use a\ndifferent derivation path for keys that it is willing to sign with\nNOINPUT. I sort of dislike having a direct dependency on taproot, i.e.,\nallowing noinput only in taproot scripts, since that isn't a done deal\neither. Without that direct dependency, having the noinput path and the\nsighash_all path be differentiated in the script leaks the details\non-chain, bloating the UTXO set, and leaking details about our contract.\n\nAlso isn't the same issue true for a separate opcode?\n\n> (For me the Debian ssh key generation bug from a decade ago is sufficient\n> evidence that people you'd think are smart and competent do make really\n> stupid mistakes in real life; so defense in depth here makes sense even\n> though you'd have to do really stupid things to get a benefit from it)\n\nTotally agree, however one could argue that increased code complexity\nis a major contributor to security issues, and I'm still convinced that\nthe hashflag is the simplest and cleanest approach to getting this\nfeature implemented.\n\nThat being said, I think the soft-forked opcode is also a good option,\nif we can get agreement on the details in a reasonable amount of time.\n\n> The other benefit of a separate opcode is support can be soft-forked in\n> independently of a new segwit version (either earlier or later).\n\nThat can both be a positive as well as a negative, since a bundle of\ncomplementing features likely is easier to get reviewed and activated.\n\n> I don't think the code has to be much more complicated with a separate\n> opcode; passing an extra flag to TransactionSignatureChecker::CheckSig()\n> is probably close to enough. Some sort of flag remains needed anyway\n> since v0 and pre-segwit signatures won't support NOINPUT.\n\nThat's moving the fanout for sighash_all vs sighash_none from the opcode\nup to the interpreter, right.\n\nCheers,\nChristian"
            }
        ],
        "thread_summary": {
            "title": "BIP sighash_noinput",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Christian Decker"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 13200
        }
    },
    {
        "title": "[bitcoin-dev] Making OP_TRUE standard?",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2018-05-08T23:57:11",
                "message_text_only": "Hi all,\n\n        The largest problem we are having today with the lightning\nprotocol is trying to predict future fees.  Eltoo solves this elegantly,\nbut meanwhile we would like to include a 546 satoshi OP_TRUE output in\ncommitment transactions so that we use minimal fees and then use CPFP\n(which can't be done at the moment due to CSV delays on outputs).\n\nUnfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\nnon-standard.  Are there any reasons not to suggest such a policy\nchange?\n\nThanks!\nRusty."
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-09T00:24:59",
                "message_text_only": "What are the downsides of just using p2wsh? This route can be rolled out\nimmediately, while policy changes are pretty \"fuzzy\" and would require a\nnear uniform rollout in order to ensure wide propagation of the commitment\ntransactions.\n\nOn Tue, May 8, 2018, 4:58 PM Rusty Russell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n>         The largest problem we are having today with the lightning\n> protocol is trying to predict future fees.  Eltoo solves this elegantly,\n> but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n> commitment transactions so that we use minimal fees and then use CPFP\n> (which can't be done at the moment due to CSV delays on outputs).\n>\n> Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n> non-standard.  Are there any reasons not to suggest such a policy\n> change?\n>\n> Thanks!\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180509/63e18795/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-09T03:02:37",
                "message_text_only": "Good morning Olauluwa,\n\nI believe P2WSH is larger due to the script hash commitment in the `scriptPubKey` as well as the actual script revelation in the `witnessScript`, whereas, a flat OP_TRUE in the `scriptPubKey` is much smaller and can be spent with an empty `scriptSig`.  It seems this is the entirety of the reason to desire an isStandard OP_TRUE.\n\nRegards,\nZmnSCPxj\n\nSent with [ProtonMail](https://protonmail.com) Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn May 9, 2018 8:24 AM, Olaoluwa Osuntokun via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> What are the downsides of just using p2wsh? This route can be rolled out immediately, while policy changes are pretty \"fuzzy\" and would require a near uniform rollout in order to ensure wide propagation of the commitment transactions.\n>\n> On Tue, May 8, 2018, 4:58 PM Rusty Russell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi all,\n>>\n>>         The largest problem we are having today with the lightning\n>> protocol is trying to predict future fees.  Eltoo solves this elegantly,\n>> but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n>> commitment transactions so that we use minimal fees and then use CPFP\n>> (which can't be done at the moment due to CSV delays on outputs).\n>>\n>> Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n>> non-standard.  Are there any reasons not to suggest such a policy\n>> change?\n>>\n>> Thanks!\n>> Rusty.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180508/15690883/attachment-0001.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-10T02:08:43",
                "message_text_only": "Olaoluwa Osuntokun <laolu32 at gmail.com> writes:\n> What are the downsides of just using p2wsh? This route can be rolled out\n> immediately, while policy changes are pretty \"fuzzy\" and would require a\n> near uniform rollout in order to ensure wide propagation of the commitment\n> transactions.\n\nI expect we will, but thougth I'd ask :)\n\nI get annoyed when people say \"We found this issue, but we worked around\nit and so never bothered you with it!\" for my projects :)\n\nCheers,\nRusty."
            },
            {
                "author": "Johnson Lau",
                "date": "2018-05-09T17:56:46",
                "message_text_only": "You should make a \u201c0 fee tx with exactly one OP_TRUE output\u201d standard, but nothing else. This makes sure CPFP will always be needed, so the OP_TRUE output won\u2019t pollute the UTXO set\n\nInstead, would you consider to use ANYONECANPAY to sign the tx, so it is possible add more inputs for fees? The total tx size is bigger than the OP_TRUE approach, but you don\u2019t need to ask for any protocol change.\n\nIn long-term, I think the right way is to have a more flexible SIGHASH system to allow people to add more inputs and outputs easily.\n\n\n\n> On 9 May 2018, at 7:57 AM, Rusty Russell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Hi all,\n> \n>        The largest problem we are having today with the lightning\n> protocol is trying to predict future fees.  Eltoo solves this elegantly,\n> but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n> commitment transactions so that we use minimal fees and then use CPFP\n> (which can't be done at the moment due to CSV delays on outputs).\n> \n> Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n> non-standard.  Are there any reasons not to suggest such a policy\n> change?\n> \n> Thanks!\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Peter Todd",
                "date": "2018-05-09T19:27:33",
                "message_text_only": "On Thu, May 10, 2018 at 01:56:46AM +0800, Johnson Lau via bitcoin-dev wrote:\n> You should make a \u201c0 fee tx with exactly one OP_TRUE output\u201d standard, but nothing else. This makes sure CPFP will always be needed, so the OP_TRUE output won\u2019t pollute the UTXO set\n> \n> Instead, would you consider to use ANYONECANPAY to sign the tx, so it is possible add more inputs for fees? The total tx size is bigger than the OP_TRUE approach, but you don\u2019t need to ask for any protocol change.\n> \n> In long-term, I think the right way is to have a more flexible SIGHASH system to allow people to add more inputs and outputs easily.\n\nI don't think that will work, as a zero-fee tx won't get relayed even with\nCPFP, due to the fact that we haven't yet implemented package-based tx\nrelaying.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180509/dbad045c/attachment.sig>"
            },
            {
                "author": "Johnson Lau",
                "date": "2018-05-09T20:19:31",
                "message_text_only": "> On 10 May 2018, at 3:27 AM, Peter Todd <pete at petertodd.org> wrote:\n> \n> On Thu, May 10, 2018 at 01:56:46AM +0800, Johnson Lau via bitcoin-dev wrote:\n>> You should make a \u201c0 fee tx with exactly one OP_TRUE output\u201d standard, but nothing else. This makes sure CPFP will always be needed, so the OP_TRUE output won\u2019t pollute the UTXO set\n>> \n>> Instead, would you consider to use ANYONECANPAY to sign the tx, so it is possible add more inputs for fees? The total tx size is bigger than the OP_TRUE approach, but you don\u2019t need to ask for any protocol change.\n>> \n>> In long-term, I think the right way is to have a more flexible SIGHASH system to allow people to add more inputs and outputs easily.\n> \n> I don't think that will work, as a zero-fee tx won't get relayed even with\n> CPFP, due to the fact that we haven't yet implemented package-based tx\n> relaying.\n> \n> -- \n> https://petertodd.org 'peter'[:-1]@petertodd.org\n\nMy only concern is UTXO pollution. There could be a \u201cCPFP anchor\u201d softfork that outputs with empty scriptPubKey and 0 value are spendable only in the same block. If not spent immediately, they become invalid and are removed from UTXO. But I still think the best solution is a more flexible SIGHASH system, which doesn\u2019t need CPFP at all."
            },
            {
                "author": "Peter Todd",
                "date": "2018-05-09T20:59:14",
                "message_text_only": "On Thu, May 10, 2018 at 04:19:31AM +0800, Johnson Lau wrote:\n> \n> \n> > On 10 May 2018, at 3:27 AM, Peter Todd <pete at petertodd.org> wrote:\n> > \n> > On Thu, May 10, 2018 at 01:56:46AM +0800, Johnson Lau via bitcoin-dev wrote:\n> >> You should make a \u201c0 fee tx with exactly one OP_TRUE output\u201d standard, but nothing else. This makes sure CPFP will always be needed, so the OP_TRUE output won\u2019t pollute the UTXO set\n> >> \n> >> Instead, would you consider to use ANYONECANPAY to sign the tx, so it is possible add more inputs for fees? The total tx size is bigger than the OP_TRUE approach, but you don\u2019t need to ask for any protocol change.\n> >> \n> >> In long-term, I think the right way is to have a more flexible SIGHASH system to allow people to add more inputs and outputs easily.\n> > \n> > I don't think that will work, as a zero-fee tx won't get relayed even with\n> > CPFP, due to the fact that we haven't yet implemented package-based tx\n> > relaying.\n> > \n> > -- \n> > https://petertodd.org 'peter'[:-1]@petertodd.org\n> \n> My only concern is UTXO pollution. There could be a \u201cCPFP anchor\u201d softfork that outputs with empty scriptPubKey and 0 value are spendable only in the same block. If not spent immediately, they become invalid and are removed from UTXO. But I still think the best solution is a more flexible SIGHASH system, which doesn\u2019t need CPFP at all.\n\nI don't see any reason why UTXO pollution would be a special concern so long as\nthose outputs are subject to the same dust rules as any other output is.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180509/17714285/attachment.sig>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-09T22:06:03",
                "message_text_only": "> Instead, would you consider to use ANYONECANPAY to sign the tx, so it is\n> possible add more inputs for fees? The total tx size is bigger than the\n> OP_TRUE approach, but you don\u2019t need to ask for any protocol change.\n\nIf one has a \"root\" commitment with other nested descendent\nmulti-transaction contracts, then changing the txid of the root commitment\nwill invalidated all the nested multi tx contracts. In our specific case, we\nhave pre-signed 2-stage HTLC transaction which rely on a stable txid. As a\nresult, we can't use the ANYONECANPAY approach atm.\n\n> In long-term, I think the right way is to have a more flexible SIGHASH\n> system to allow people to add more inputs and outputs easily.\n\nAgreed, see the recent proposal to introduce SIGHASH_NOINPUT as a new\nsighash type. IMO it presents an opportunity to introduce more flexible fine\ngrained sighash inclusion control.\n\n-- Laolu\n\n\nOn Wed, May 9, 2018 at 11:12 AM Johnson Lau via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> You should make a \u201c0 fee tx with exactly one OP_TRUE output\u201d standard, but\n> nothing else. This makes sure CPFP will always be needed, so the OP_TRUE\n> output won\u2019t pollute the UTXO set\n>\n> Instead, would you consider to use ANYONECANPAY to sign the tx, so it is\n> possible add more inputs for fees? The total tx size is bigger than the\n> OP_TRUE approach, but you don\u2019t need to ask for any protocol change.\n>\n> In long-term, I think the right way is to have a more flexible SIGHASH\n> system to allow people to add more inputs and outputs easily.\n>\n>\n>\n> > On 9 May 2018, at 7:57 AM, Rusty Russell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > Hi all,\n> >\n> >        The largest problem we are having today with the lightning\n> > protocol is trying to predict future fees.  Eltoo solves this elegantly,\n> > but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n> > commitment transactions so that we use minimal fees and then use CPFP\n> > (which can't be done at the moment due to CSV delays on outputs).\n> >\n> > Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n> > non-standard.  Are there any reasons not to suggest such a policy\n> > change?\n> >\n> > Thanks!\n> > Rusty.\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180509/e98eb0d4/attachment.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-10T02:06:41",
                "message_text_only": "Johnson Lau <jl2012 at xbt.hk> writes:\n> You should make a \u201c0 fee tx with exactly one OP_TRUE output\u201d standard, but nothing else. This makes sure CPFP will always be needed, so the OP_TRUE output won\u2019t pollute the UTXO set\n\nThat won't propagate :(\n\n> Instead, would you consider to use ANYONECANPAY to sign the tx, so it\n> is possible add more inputs for fees? The total tx size is bigger than\n> the OP_TRUE approach, but you don\u2019t need to ask for any protocol\n> change.\n\nNo, that would change the TXID, which we rely on for HTLC transactions.\n\n> In long-term, I think the right way is to have a more flexible SIGHASH system to allow people to add more inputs and outputs easily.\n\nAgreed:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-April/015862.html\n\nBut in the long term we'll have Eltoo and SIGHASH_NOINPUT which both\nallow different solutions.\n\nCheers,\nRusty."
            },
            {
                "author": "Luke Dashjr",
                "date": "2018-05-10T02:27:41",
                "message_text_only": "An OP_TRUE-only script with a low value seems like a good example of where the \nweight doesn't reflect the true cost: it uses a UTXO forever, while only \ncosting a weight of 4.\n\nI like Johnson's idea to have some template (perhaps OP_2-only, to preserve \nexpected behaviour of OP_TRUE-only) that when combined with a 0-value is \nalways valid only if spent in the same block.\n\nI wonder if it would make sense to actually tie it to a transaction version \nbit, such that when the bit is set, the transaction is serialised with +1 on \nthe output count and 00000000000000000181 is simply injected into the \ntransaction hashing... But for now, simply having a consensus rule that a bit \nMUST be set for the expected behaviour, and the bit may ONLY be set when the \nlast output is exactly 00000000000000000181, would allow us to code the \ntransaction serialisation up later. (Maybe it should be the first output \ninstead of the last... Is there any legitimate reason one would have multiple \nsuch dummy outputs?)\n\nLuke\n\n\nOn Tuesday 08 May 2018 23:57:11 Rusty Russell via bitcoin-dev wrote:\n> Hi all,\n>\n>         The largest problem we are having today with the lightning\n> protocol is trying to predict future fees.  Eltoo solves this elegantly,\n> but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n> commitment transactions so that we use minimal fees and then use CPFP\n> (which can't be done at the moment due to CSV delays on outputs).\n>\n> Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n> non-standard.  Are there any reasons not to suggest such a policy\n> change?\n>\n> Thanks!\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-10T03:07:49",
                "message_text_only": "Good morning Luke and list,\n\n\n> An OP_TRUE-only script with a low value seems like a good example of where the\n> \n> weight doesn't reflect the true cost: it uses a UTXO forever, while only\n> \n> costing a weight of 4.\n> \n> I like Johnson's idea to have some template (perhaps OP_2-only, to preserve\n> \n> expected behaviour of OP_TRUE-only) that when combined with a 0-value is\n> \n> always valid only if spent in the same block.\n\nI understand the issue.  On Lightning side, if this rule is used, we would have the two options below:\n\n1.  Commitment transactions always use the minimum feerate, but always have the above OP_TRUE output.  Then to confirm the commitment transaction we would have to always spend the OP_TRUE output in CPFP transaction that pays for actual fee at unilateral close.  This consumes more blockchain space for unilateral closes, as the second transaction is always mandatory.\n2.  We store two commitment transactions and associated paraphernalia (further transactions to claim the HTLCs).  One version has a negotiated feerate without the OP_TRUE output.  The other version has a slightly increased feerate and an OP_TRUE output as above.  At unilateral close, we see if the negotiated feerate is enough and use that version if possible, but if not we RBF it with other version and in addition also CPFP on top.  As mentioned before, we do not have transaction packages, so we need to RBF with higher feerate the commitment transaction, then submit the CPFP transaction which makes the first transaction valid to include in a block as per the rule.  This requires that the fallback always have both an RBF bump and a CPFP bump.\n\n> \n>(Maybe it should be the first output\n> \n> instead of the last... Is there any legitimate reason one would have multiple\n> \n> such dummy outputs?)\n\nIt seems there are indeed none.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-15T01:22:26",
                "message_text_only": "Good morning Luke,\n\n> (Maybe it should be the first output\n> \n> instead of the last... Is there any legitimate reason one would have multiple\n> \n> such dummy outputs?)\n\nNone, but how about use of `SIGHASH_SINGLE` flag? If a dummy output is added as the first, would it not require adjustment of the inputs of the transaction?\n\nIn context you are discussing the transaction serialization, though, so perhaps `SIGHASH_SINGLE`, is unaffected?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-17T02:44:53",
                "message_text_only": "Luke Dashjr <luke at dashjr.org> writes:\n> An OP_TRUE-only script with a low value seems like a good example of where the \n> weight doesn't reflect the true cost: it uses a UTXO forever, while only \n> costing a weight of 4.\n>\n> I like Johnson's idea to have some template (perhaps OP_2-only, to preserve \n> expected behaviour of OP_TRUE-only) that when combined with a 0-value is \n> always valid only if spent in the same block.\n>\n> I wonder if it would make sense to actually tie it to a transaction version \n> bit, such that when the bit is set, the transaction is serialised with +1 on \n> the output count and 00000000000000000181 is simply injected into the \n> transaction hashing... But for now, simply having a consensus rule that a bit \n> MUST be set for the expected behaviour, and the bit may ONLY be set when the \n> last output is exactly 00000000000000000181, would allow us to code the \n> transaction serialisation up later. (Maybe it should be the first output \n> instead of the last... Is there any legitimate reason one would have multiple \n> such dummy outputs?)\n\nYour zero-val-OP_TRUE-can't-be-spent-after-same-block SF is interesting,\nbut if we want a SF just give us SIGHASH_NOINPUT and we'll not need this\nat all (though others still might).  It's nicer than the previous\ndiscussions on after-the-fact feebumping[1] though.\n\nMeanwhile, our best mitigation against UTXO bloat is:\n1. Make the fees as low as possible[2]\n2. Put a CSV delay on the to-remote output (currently there's asymmetry)\n3. Attach more value to the OP_TRUE output, say 1000 satoshi.\n\nBut turns out we probably don't want an OP_TRUE output nor P2SH, because\nthen the spending tx would be malleable.  So P2WSH is is.\n\nThis brings us another theoretical problem: someone could spend our\nOP_TRUE with a low-fee non-RBF tx, and we'd not be able to use it to\nCPFP the tx.  It'd be hard to do, but possible.  I think the network\nbenefits from using OP_TRUE (anyone can clean, and size, vs some\nonly-known-to-me pubkey) outweighs the risk, but it'd be nice if OP_TRUE\nP2WSH spends were always considered RBF.\n\nThanks,\nRusty.\n[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-April/015864.html\n[2] Because bitcoin core use legacy measurements, this is actually 253\nsatoshi per kilosipa for us, see https://github.com/ElementsProject/lightning/commit/2e687b9b352c9092b5e8bd4a688916ac50b44af0"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-17T10:28:54",
                "message_text_only": "Good morning Rusty and list,\n\n> Your zero-val-OP_TRUE-can't-be-spent-after-same-block SF is interesting,\n> \n> but if we want a SF just give us SIGHASH_NOINPUT and we'll not need this\n> \n> at all (though others still might).\n\nWe might still want this in general in Lightning; for instance we could make every funding transaction include such an output.  If it turns out, our initial feerate estimate for the funding transaction is low, we can use the `OP_TRUE` for fee-bumping.  This is a win for Lightning since the funding transaction ID remains the same (even in Decker-Russell-Osuntokun, the trigger transaction is signed with `SIGHASH_ALL`, and refers to a fixed funding transaction ID).\n\nWithout the `OP_TRUE`-for-fee-bump, we would have to pretend to open a new different channel and RBF the old funding transaction with a new higher-feerate funding transaction, then keep track of which one gets confirmed deeply (there is a race where a miner discovers a block using the older funding transaction before our broadcast of the new funding transaction reaches it).\n\n(we could also feebump using the change output of the funding transaction, but such a change output might not exist for all funding transactions.)\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2018-05-17T17:35:53",
                "message_text_only": "ZmnSCPxj via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> writes:\n\n> Good morning Rusty and list,\n>\n>> Your zero-val-OP_TRUE-can't-be-spent-after-same-block SF is\n>> interesting,\n>> \n>> but if we want a SF just give us SIGHASH_NOINPUT and we'll not need\n>> this\n>> \n>> at all (though others still might).\n>\n> We might still want this in general in Lightning; for instance we\n> could make every funding transaction include such an output.  If it\n> turns out, our initial feerate estimate for the funding transaction is\n> low, we can use the `OP_TRUE` for fee-bumping.  This is a win for\n> Lightning since the funding transaction ID remains the same (even in\n> Decker-Russell-Osuntokun, the trigger transaction is signed with\n> `SIGHASH_ALL`, and refers to a fixed funding transaction ID).\n>\n> Without the `OP_TRUE`-for-fee-bump, we would have to pretend to open a\n> new different channel and RBF the old funding transaction with a new\n> higher-feerate funding transaction, then keep track of which one gets\n> confirmed deeply (there is a race where a miner discovers a block\n> using the older funding transaction before our broadcast of the new\n> funding transaction reaches it).\n>\n> (we could also feebump using the change output of the funding\n> transaction, but such a change output might not exist for all funding\n> transactions.)\n\nThis would only really help in the case of the funding tx not having a\nchange output, which I believe will be very rare. In the case of a\nchange output we can simply do a CPFP which includes the change output."
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-17T20:06:26",
                "message_text_only": ">\n> This brings us another theoretical problem: someone could spend our\n> OP_TRUE with a low-fee non-RBF tx, and we'd not be able to use it to\n> CPFP the tx.  It'd be hard to do, but possible.  I think the network\n> benefits from using OP_TRUE (anyone can clean, and size, vs some\n> only-known-to-me pubkey) outweighs the risk, but it'd be nice if OP_TRUE\n> P2WSH spends were always considered RBF.\n>\n\nI believe OP_CSV with a relative locktime of 0 could be used to enforce RBF\non the spending tx?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/da8223c0/attachment-0001.html>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-21T03:44:06",
                "message_text_only": "Jim Posen <jim.posen at gmail.com> writes:\n> I believe OP_CSV with a relative locktime of 0 could be used to enforce RBF\n> on the spending tx?\n\nMarco points out that if the parent is RBF, this child inherits it, so\nwe're actually good here.\n\nHowever, Matt Corallo points out that you can block RBF will a\nlarge-but-lowball tx, as BIP 125 points out:\n\n   will be replaced by a new transaction...:\n\n   3. The replacement transaction pays an absolute fee of at least the sum\n      paid by the original transactions.\n\nI understand implementing a single mempool requires these kind of\nup-front decisions on which tx is \"better\", but I wonder about the\nconsequences of dropping this heuristic?  Peter?\n\nThanks!\nRusty."
            },
            {
                "author": "Peter Todd",
                "date": "2018-05-21T03:56:58",
                "message_text_only": "On Mon, May 21, 2018 at 01:14:06PM +0930, Rusty Russell via bitcoin-dev wrote:\n> Jim Posen <jim.posen at gmail.com> writes:\n> > I believe OP_CSV with a relative locktime of 0 could be used to enforce RBF\n> > on the spending tx?\n> \n> Marco points out that if the parent is RBF, this child inherits it, so\n> we're actually good here.\n> \n> However, Matt Corallo points out that you can block RBF will a\n> large-but-lowball tx, as BIP 125 points out:\n> \n>    will be replaced by a new transaction...:\n> \n>    3. The replacement transaction pays an absolute fee of at least the sum\n>       paid by the original transactions.\n> \n> I understand implementing a single mempool requires these kind of\n> up-front decisions on which tx is \"better\", but I wonder about the\n> consequences of dropping this heuristic?  Peter?\n\nWe've discussed this before: that rule prevents bandwidth usage DoS attacks on\nthe mempool; it's not a \"heuristic\". If you drop it, an attacker can repeatedly\nbroadcast and replace a series of transactions to use up tx relay bandwidth for\nsignificantly lower cost than otherwise.\n\nThough these days with relatively high minimum fees that may not matter.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180520/838d855d/attachment.sig>"
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-30T02:47:20",
                "message_text_only": "Peter Todd <pete at petertodd.org> writes:\n> On Mon, May 21, 2018 at 01:14:06PM +0930, Rusty Russell via bitcoin-dev wrote:\n>> Jim Posen <jim.posen at gmail.com> writes:\n>> > I believe OP_CSV with a relative locktime of 0 could be used to enforce RBF\n>> > on the spending tx?\n>> \n>> Marco points out that if the parent is RBF, this child inherits it, so\n>> we're actually good here.\n>> \n>> However, Matt Corallo points out that you can block RBF will a\n>> large-but-lowball tx, as BIP 125 points out:\n>> \n>>    will be replaced by a new transaction...:\n>> \n>>    3. The replacement transaction pays an absolute fee of at least the sum\n>>       paid by the original transactions.\n>> \n>> I understand implementing a single mempool requires these kind of\n>> up-front decisions on which tx is \"better\", but I wonder about the\n>> consequences of dropping this heuristic?  Peter?\n>\n> We've discussed this before: that rule prevents bandwidth usage DoS attacks on\n> the mempool; it's not a \"heuristic\". If you drop it, an attacker can repeatedly\n> broadcast and replace a series of transactions to use up tx relay bandwidth for\n> significantly lower cost than otherwise.\n>\n> Though these days with relatively high minimum fees that may not matter.\n\nAFAICT the optimal DoS is where:\n\n1.  Attacker sends a 100,000 vbyte tx @1sat/vbyte.\n2.  Replaces it with a 108 vbyte tx @2sat/vbyte which spends one of\n    those inputs.\n3.  Replaces that spent input in the 100k tx and does it again.\n\nIt takes 3.5 seconds to propagate to 50% of network[1] (probably much worse\ngiven 100k txs), so they can only do this about 86 times per block.\n\nThat means they send 86 * (100000 + 108) = 8609288 vbytes for a cost of\n86 * 2 * 108 + 100000 / 2 = 68576 satoshi (assuming 50% chance 100k tx\ngets mined).\n\nThat's a 125x cost over just sending 1sat/vbyte txs under optimal\nconditions[2], but it doesn't really reach most low-bandwidth nodes\nanyway.\n\nGiven that this rule is against miner incentives (assuming mempool is\nfull), and makes things more complex than they need to be, I think\nthere's a strong argument for its removal.\n\nCheers,\nRusty.\n[1] http://bitcoinstats.com/network/propagation/\n[2] Bandwidth overhead for just sending a 108-vbyte tx is about 160\n    bytes, so our actual bandwidth per satoshi is closer to 60x\n    even under optimal conditions."
            },
            {
                "author": "Rusty Russell",
                "date": "2018-05-31T02:47:58",
                "message_text_only": "Rusty Russell <rusty at rustcorp.com.au> writes:\n> AFAICT the optimal DoS is where:\n>\n> 1.  Attacker sends a 100,000 vbyte tx @1sat/vbyte.\n> 2.  Replaces it with a 108 vbyte tx @2sat/vbyte which spends one of\n>     those inputs.\n> 3.  Replaces that spent input in the 100k tx and does it again.\n>\n> It takes 3.5 seconds to propagate to 50% of network[1] (probably much worse\n> given 100k txs), so they can only do this about 86 times per block.\n>\n> That means they send 86 * (100000 + 108) = 8609288 vbytes for a cost of\n> 86 * 2 * 108 + 100000 / 2 = 68576 satoshi (assuming 50% chance 100k tx\n> gets mined).\n\nThis 50% chance assumption is wrong; it's almost 0% for a low enough\nfee.  Thus the cost is only 18576, making the cost for the transactions\n463x lower than just sending 1sat/vbyte txs under optimal conditions.\nThat's a bit ouch.[1]\n\nI think a better solution is to address the DoS potential directly:\nif a replacement doesn't meet #3 or #4, but *does* increase the feerate\nby at least minrelayfee, processing should be delayed by 30-60 seconds.\n\nThat means that eventually you will RBF a larger tx, but it'll take\nmuch longer.  Should be easy to implement, too, since similar timers\nwill be needed for dandelion.\n\nCheers,\nRusty.\n[1] Christian grabbed some more detailed propagation stats for me: larger\n    txs do propagate slower, but only by a factor of 2.5 or so."
            },
            {
                "author": "Russell O'Connor",
                "date": "2018-05-21T14:20:39",
                "message_text_only": "In the thread \"Revisting BIP 125 RBF policy\" @\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015717.html\nand\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-March/015797.html\nI propose replacing rule 3 with a rule that instead demands that the\nreplacement package fee rate exceeds the package fee rate of the original\ntransactions, and that there is an absolute fee bump of the particular\ntransaction being replaced that covers the min-fee rate times the size of\nthe mempool churn's data size.\n\nPerhaps this would address your issue too Rusty.\n\nOn Sun, May 20, 2018 at 11:44 PM, Rusty Russell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Jim Posen <jim.posen at gmail.com> writes:\n> > I believe OP_CSV with a relative locktime of 0 could be used to enforce\n> RBF\n> > on the spending tx?\n>\n> Marco points out that if the parent is RBF, this child inherits it, so\n> we're actually good here.\n>\n> However, Matt Corallo points out that you can block RBF will a\n> large-but-lowball tx, as BIP 125 points out:\n>\n>    will be replaced by a new transaction...:\n>\n>    3. The replacement transaction pays an absolute fee of at least the sum\n>       paid by the original transactions.\n>\n> I understand implementing a single mempool requires these kind of\n> up-front decisions on which tx is \"better\", but I wonder about the\n> consequences of dropping this heuristic?  Peter?\n>\n> Thanks!\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180521/3df4a893/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2018-05-10T09:33:29",
                "message_text_only": "I fail to see what's the practical difference between sending to op_true\nand giving the coins are fees directly. Perhaps it is ao obvious to you\nthat you forget to mention it?\nIf you did I honestlt missed it.\n\nOn Wed, 9 May 2018, 01:58 Rusty Russell via bitcoin-dev, <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n>         The largest problem we are having today with the lightning\n> protocol is trying to predict future fees.  Eltoo solves this elegantly,\n> but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n> commitment transactions so that we use minimal fees and then use CPFP\n> (which can't be done at the moment due to CSV delays on outputs).\n>\n> Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n> non-standard.  Are there any reasons not to suggest such a policy\n> change?\n>\n> Thanks!\n> Rusty.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180510/194b929c/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2018-05-10T09:33:30",
                "message_text_only": "But in prnciple I don't oppose to making it stardard, just want to\nunderstand what's the point.\n\nOn Thu, 10 May 2018, 02:16 Jorge Tim\u00f3n, <jtimon at jtimon.cc> wrote:\n\n> I fail to see what's the practical difference between sending to op_true\n> and giving the coins are fees directly. Perhaps it is ao obvious to you\n> that you forget to mention it?\n> If you did I honestlt missed it.\n>\n> On Wed, 9 May 2018, 01:58 Rusty Russell via bitcoin-dev, <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi all,\n>>\n>>         The largest problem we are having today with the lightning\n>> protocol is trying to predict future fees.  Eltoo solves this elegantly,\n>> but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n>> commitment transactions so that we use minimal fees and then use CPFP\n>> (which can't be done at the moment due to CSV delays on outputs).\n>>\n>> Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n>> non-standard.  Are there any reasons not to suggest such a policy\n>> change?\n>>\n>> Thanks!\n>> Rusty.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180510/46aa7cc4/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2018-05-10T09:43:28",
                "message_text_only": "You'd send 0 satoshis to OP_TRUE, creating a UTXO. Then you spend that 0-value \nUTXO in another transaction with a normal fee. The idea is that to get the \nlatter fee, the miner needs to confirm the original tranaction with the \n0-value OP_TRUE.\n\n(Aside, in case it wasn't clear on my previous email, the template-script idea \nwould not make it *mandatory* to spend in the same block, but that the UTXO \nwould merely cease to be valid *after* that block. So the 0-value output does \nnot take up a UTXO db entry when left unused.)\n\nOn Thursday 10 May 2018 09:33:29 Jorge Tim\u00f3n via bitcoin-dev wrote:\n> I fail to see what's the practical difference between sending to op_true\n> and giving the coins are fees directly. Perhaps it is ao obvious to you\n> that you forget to mention it?\n> If you did I honestlt missed it.\n>\n> On Wed, 9 May 2018, 01:58 Rusty Russell via bitcoin-dev, <\n>\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Hi all,\n> >\n> >         The largest problem we are having today with the lightning\n> > protocol is trying to predict future fees.  Eltoo solves this elegantly,\n> > but meanwhile we would like to include a 546 satoshi OP_TRUE output in\n> > commitment transactions so that we use minimal fees and then use CPFP\n> > (which can't be done at the moment due to CSV delays on outputs).\n> >\n> > Unfortunately, we'd have to P2SH it at the moment as a raw 'OP_TRUE' is\n> > non-standard.  Are there any reasons not to suggest such a policy\n> > change?\n> >\n> > Thanks!\n> > Rusty.\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-11T02:44:13",
                "message_text_only": "Good morning Luke and list,\n\n\n> \n> (Aside, in case it wasn't clear on my previous email, the template-script idea\n> \n> would not make it mandatory to spend in the same block, but that the UTXO\n> \n> would merely cease to be valid after that block. So the 0-value output does\n> \n> not take up a UTXO db entry when left unused.)\n\nThank you for clearing this up.  It seems, I misunderstood.  So my earlier rumination, about having two options for Lightning, is incorrect.\n\nFor Lightning, we just need to add this 0-value OP_TRUE output always to transactions that require both side signatures (commitment, HTLC-timeout, HTLC-success), and it will always serve as a \"hook\" for  adding more fees if needed.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Making OP_TRUE standard?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rusty Russell",
                "Johnson Lau",
                "Peter Todd",
                "ZmnSCPxj",
                "Olaoluwa Osuntokun",
                "Russell O'Connor",
                "Jorge Tim\u00f3n",
                "Luke Dashjr",
                "Jim Posen",
                "Christian Decker"
            ],
            "messages_count": 26,
            "total_messages_chars_count": 36201
        }
    },
    {
        "title": "[bitcoin-dev] Why not archive the backend of Bitcoin blockchain?",
        "thread_messages": [
            {
                "author": "Segue",
                "date": "2018-05-10T00:56:00",
                "message_text_only": "On 3/17/18, someone posted on the Lightning-dev list, \"Can I try \nLightning without running a fully-fledged bitcoin block chain? (Yubin \nRuan).\"\u00a0 The inquirer was asking because he didn't have much space to \nstore the entire blockchain on his laptop.\n\nI replied:\n\n\"Developers,\n\nOn THIS note and slightly off-topic but relevant, why can't chunks of \nblockchain peel off the backend periodically and be archived, say on \nminimum of 150 computers across 7 continents?\n\nIt seems crazy to continue adding on to an increasingly long chain to \ninfinity if the old chapters (i.e. more than, say, 2 years old) could be \nstored in an evenly distributed manner across the planet. The same 150 \ncomputers would not need to store every chapter either, just the index \nwould need to be widely distributed in order to reconnect with a chapter \nif needed. Then maybe it is no longer a limitation in the future for \npeople like Yubin. \"\n\nIt was suggested by a couple of lightning developers that I post this \nidea on the bitcoin-dev list.\u00a0 So, here I post :).\n\nSegue"
            },
            {
                "author": "\u30a2\u30eb\u30e0\u3000\u30ab\u30fc\u30eb\u30e8\u30cf\u30f3",
                "date": "2018-05-10T06:48:16",
                "message_text_only": "He can use pruning to only store the last X MB of the blockchain. It\nwill store the UTXO set though, which is a couple of GBs. In total, a\npruned node with pruning set to 1000 MB ends up using 4.5 GB\ncurrently, but it varies slightly due to the # of UTXOs in existence.\n\nOn Thu, May 10, 2018 at 9:56 AM, Segue via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On 3/17/18, someone posted on the Lightning-dev list, \"Can I try Lightning\n> without running a fully-fledged bitcoin block chain? (Yubin Ruan).\"  The\n> inquirer was asking because he didn't have much space to store the entire\n> blockchain on his laptop.\n>\n> I replied:\n>\n> \"Developers,\n>\n> On THIS note and slightly off-topic but relevant, why can't chunks of\n> blockchain peel off the backend periodically and be archived, say on minimum\n> of 150 computers across 7 continents?\n>\n> It seems crazy to continue adding on to an increasingly long chain to\n> infinity if the old chapters (i.e. more than, say, 2 years old) could be\n> stored in an evenly distributed manner across the planet. The same 150\n> computers would not need to store every chapter either, just the index would\n> need to be widely distributed in order to reconnect with a chapter if\n> needed. Then maybe it is no longer a limitation in the future for people\n> like Yubin. \"\n>\n> It was suggested by a couple of lightning developers that I post this idea\n> on the bitcoin-dev list.  So, here I post :).\n>\n> Segue\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-10T07:54:08",
                "message_text_only": "Good morning karl and Segue,\n\nSpecifically for c-lightning, we are not yet rated for pruned bitcoind use, although if you installed and started running bitcoind before installing the lightningd, caught up to the chain, and then installed lightningd and set things up so that bitcoind will get killed if lightningd stops running (so that bitcoind will \"never\" leave lightningd too far behind).\n\nOfficially though pruned bitcoind is not supported for c-lightning, so loss of funds due to doing the above idea is entirely your fault.\n\nOn the topic of such a \"chapter-based\" archiving, it needs to get implemented and reviewed.  As-is I see no reason why it cannot be done, but I think the details are far more important.\n\n1.  How do we select the archive servers?\n2.  How can we ensure that no chapter has only a small number of actual owners who could easily coordinate to deny access to historical blockchain data to those they deem undesirable?\n\nRegards,\nZmnSCPxj\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n\nOn May 10, 2018 2:48 PM, \u30a2\u30eb\u30e0\u3000\u30ab\u30fc\u30eb\u30e8\u30cf\u30f3 via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> He can use pruning to only store the last X MB of the blockchain. It\n> \n> will store the UTXO set though, which is a couple of GBs. In total, a\n> \n> pruned node with pruning set to 1000 MB ends up using 4.5 GB\n> \n> currently, but it varies slightly due to the # of UTXOs in existence.\n> \n> On Thu, May 10, 2018 at 9:56 AM, Segue via bitcoin-dev\n> \n> bitcoin-dev at lists.linuxfoundation.org wrote:\n> \n> > On 3/17/18, someone posted on the Lightning-dev list, \"Can I try Lightning\n> > \n> > without running a fully-fledged bitcoin block chain? (Yubin Ruan).\" The\n> > \n> > inquirer was asking because he didn't have much space to store the entire\n> > \n> > blockchain on his laptop.\n> > \n> > I replied:\n> > \n> > \"Developers,\n> > \n> > On THIS note and slightly off-topic but relevant, why can't chunks of\n> > \n> > blockchain peel off the backend periodically and be archived, say on minimum\n> > \n> > of 150 computers across 7 continents?\n> > \n> > It seems crazy to continue adding on to an increasingly long chain to\n> > \n> > infinity if the old chapters (i.e. more than, say, 2 years old) could be\n> > \n> > stored in an evenly distributed manner across the planet. The same 150\n> > \n> > computers would not need to store every chapter either, just the index would\n> > \n> > need to be widely distributed in order to reconnect with a chapter if\n> > \n> > needed. Then maybe it is no longer a limitation in the future for people\n> > \n> > like Yubin. \"\n> > \n> > It was suggested by a couple of lightning developers that I post this idea\n> > \n> > on the bitcoin-dev list. So, here I post :).\n> > \n> > Segue\n> > \n> > bitcoin-dev mailing list\n> > \n> > bitcoin-dev at lists.linuxfoundation.org\n> > \n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n> bitcoin-dev mailing list\n> \n> bitcoin-dev at lists.linuxfoundation.org\n> \n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-10T06:50:43",
                "message_text_only": "That is a good observation that most of the historical data does not need\nto be kept around. I believe what you are suggested is already implemented,\nhowever. Bitcoin Core can operate in a pruned mode, where the bulk of the\nhistorical block data is discarded and only the current UTXO set (and a few\nrecent blocks) are kept. As you note, some nodes on the network need to run\nin archive mode to help new nodes get in sync. BIP 159 helps identify these\narchive nodes at the gossip layer.\n\nIn the case of lightning, some implementations made use of the additional\ntxindex, which is not compatible with pruned mode.\n\nOn Wed, May 9, 2018 at 5:56 PM, Segue via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 3/17/18, someone posted on the Lightning-dev list, \"Can I try Lightning\n> without running a fully-fledged bitcoin block chain? (Yubin Ruan).\"  The\n> inquirer was asking because he didn't have much space to store the entire\n> blockchain on his laptop.\n>\n> I replied:\n>\n> \"Developers,\n>\n> On THIS note and slightly off-topic but relevant, why can't chunks of\n> blockchain peel off the backend periodically and be archived, say on\n> minimum of 150 computers across 7 continents?\n>\n> It seems crazy to continue adding on to an increasingly long chain to\n> infinity if the old chapters (i.e. more than, say, 2 years old) could be\n> stored in an evenly distributed manner across the planet. The same 150\n> computers would not need to store every chapter either, just the index\n> would need to be widely distributed in order to reconnect with a chapter if\n> needed. Then maybe it is no longer a limitation in the future for people\n> like Yubin. \"\n>\n> It was suggested by a couple of lightning developers that I post this idea\n> on the bitcoin-dev list.  So, here I post :).\n>\n> Segue\n>\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180509/4a7cccc7/attachment-0001.html>"
            },
            {
                "author": "Patrick Shirkey",
                "date": "2018-05-10T10:52:41",
                "message_text_only": "> On 3/17/18, someone posted on the Lightning-dev list, \"Can I try\n> Lightning without running a fully-fledged bitcoin block chain? (Yubin\n> Ruan).\"\u00a0 The inquirer was asking because he didn't have much space to\n> store the entire blockchain on his laptop.\n>\n> I replied:\n>\n> \"Developers,\n>\n> On THIS note and slightly off-topic but relevant, why can't chunks of\n> blockchain peel off the backend periodically and be archived, say on\n> minimum of 150 computers across 7 continents?\n>\n> It seems crazy to continue adding on to an increasingly long chain to\n> infinity if the old chapters (i.e. more than, say, 2 years old) could be\n> stored in an evenly distributed manner across the planet. The same 150\n> computers would not need to store every chapter either, just the index\n> would need to be widely distributed in order to reconnect with a chapter\n> if needed. Then maybe it is no longer a limitation in the future for\n> people like Yubin. \"\n>\n> It was suggested by a couple of lightning developers that I post this\n> idea on the bitcoin-dev list.\u00a0 So, here I post :).\n>\n\nYou can already use the \"prune\" flag to get a snapshot of the blockchain\nbut it is incompatible with \"txindex\" and \"rescan\" so maybe that is and\nissue for lightning nodes?\n\n\n\n\n-- \nPatrick Shirkey\nBoost Hardware"
            }
        ],
        "thread_summary": {
            "title": "Why not archive the backend of Bitcoin blockchain?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Patrick Shirkey",
                "\u30a2\u30eb\u30e0\u3000\u30ab\u30fc\u30eb\u30e8\u30cf\u30f3",
                "ZmnSCPxj",
                "Segue",
                "Jim Posen"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 9168
        }
    },
    {
        "title": "[bitcoin-dev] Idea: More decimal places for lower minimum fee",
        "thread_messages": [
            {
                "author": "st-bind at posteo.de",
                "date": "2018-05-10T08:10:51",
                "message_text_only": "Currently, the minimum fee of 1 satoshi per byte corresponds to about \n0.09 USD per kB, which is no longer insignificant. Maybe the time has \ncome now to introduce more decimal places and make the minimum fee 1 of \nthe new smallest unit. This way, everyday payments would again be \npossible with virtually no fee without flooding the mempool with free \nspam transactions."
            },
            {
                "author": "Alex Morcos",
                "date": "2018-05-10T13:05:27",
                "message_text_only": "Fee rates in Bitcoin Core are measured in satoshis/kB.   There are a couple\nplaces where a minimum of 1000 satoshis/kB is assumed.\n\nSetting \"incrementalrelayfee\" to a smaller than default value and either\nleaving \"minrelaytxfee\" unset or also setting it smaller will be sufficient\nto allow your node to accept and relay transactions with smaller fee\nrates.  Of course without the rest of the network making these changes\nand/or the miners being willing to mine those transactions, it won't be of\nmuch benefit.\n\nFee estimation doesn't distinguish fee rates less than 1000 sats/kB.  This\nwould be more substantial to change.\n\n\n\nOn Thu, May 10, 2018 at 4:10 AM, st-bind--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Currently, the minimum fee of 1 satoshi per byte corresponds to about 0.09\n> USD per kB, which is no longer insignificant. Maybe the time has come now\n> to introduce more decimal places and make the minimum fee 1 of the new\n> smallest unit. This way, everyday payments would again be possible with\n> virtually no fee without flooding the mempool with free spam transactions.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180510/3dc06c7a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Idea: More decimal places for lower minimum fee",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "st-bind at posteo.de",
                "Alex Morcos"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1855
        }
    },
    {
        "title": "[bitcoin-dev] MAST/Schnorr related soft-forks",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2018-05-10T12:10:27",
                "message_text_only": "Hello world,\n\nAfter the core dev meetup in March I wrote up some notes of where I\nthink things stand for signing stuff post-Schnorr. It was mostly for my\nown benefit but maybe it's helpful for others too, so...\n\nThey're just notes, so may assume a fair bit of background to be able to\nunderstand the meaning of the bullet points. In particular, note that I'm\nusing \"schnorr\" just to describe the signature algorithm, and the terms\n\"key aggregation\" to describe turning an n-of-n key multisig setup into\na single key setup, and \"signature aggregation\" to describe combining\nsignatures from many inputs/transactions together: those are often all\njust called \"schnorr signatures\" in various places.\n\n\nAnyway! I think it's fair to split the ideas around up as follows:\n\n1) Schnorr CHECKSIG\n\n  Benefits:\n    - opportunity to change signature encoding from DER to save a few\n      bytes per signature, and have fixed size signatures making tx size\n      calculations easier\n\n    - enables n-of-n multisig key aggregation (a single pubkey and\n      signature gives n-of-n security; setup non-interactively via muSig,\n      or semi-interactively via proof of possession of private key;\n      interactive signature protocol)\n\n    - enables m-of-n multisig key aggregation with interactive setup and\n      interactive signature protocol, and possibly substantial storage\n      requirements for participating signers\n\n    - enables scriptless scripts and discreet log contracts via\n      key aggregation and interactive\n\n    - enables payment decorrelation for lightning\n\n    - enables batch validation of signatures, which substantially reduces\n      computational cost of signature verification, provided a single\n      \"all sigs valid\" or \"some sig(s) invalid\" output (rather than\n      \"sig number 5 is invalid\") is sufficient\n\n    - better than ecdsa due to reducing signature malleability\n      (and possibly due to having a security proof that has had more\n      review?)\n\n   Approaches:\n     - bump segwit version to replace P2WPKH\n     - replace an existing OP_NOP with OP_CHECKSCHNORRVERIFY\n     - hardfork to allowing existing addresses to be solved via Schnorr sig\n       as alternative to ECDSA\n\n2) Merkelized Abstract Syntax Trees\n\n   Two main benefits for enabling MAST:\n    - logarithmic scaling for scripts with many alternative paths\n    - only reveals (approximate) number of alternative execution branches,\n      not what they may have been\n\n   Approaches:\n    - replace an existing OP_NOP with OP_MERKLE_TREE_VERIFY, and treat an\n      item remaining on the alt stack at the end of script exeution as a\n      script and do tail-recursion into it (BIP 116, 117)\n    - bump the segwit version and introduce a \"pay-to-merkelized-script\"\n      address form (BIP 114)\n\n3) Taproot\n\n   Requirements:\n    - only feasible if Schnorr is available (required in order to make the\n      pubkey spend actually be a multisig spend)\n    - andytoshi has written up a security proof at\n      https://github.com/apoelstra/taproot\n\n   Benefits:\n    - combines pay-to-pubkey and pay-to-script in a single address,\n      improving privacy\n    - allows choice of whether to use pubkey or script at spend time,\n      allowing for more efficient spends (via pubkey) without reducing\n      flexibility (via script)\n\n   Approaches:\n    - bump segwit version and introduce a \"pay-to-taproot\" address form\n\n4) Graftroot\n\n   Requirements:\n    - only really feasible if Schnorr is implemented first, so that\n      multiple signers can be required via a single pubkey/signature\n    - people seem to want a security proof for this; not sure if that's\n      hard or straightforward\n\n   Benefits:\n    - allows delegation of authorisation to spend an output already\n      on the blockchain\n    - constant scaling for scripts with many alternative paths\n      (better than MAST's logarithmic scaling)\n    - only reveals the possibility of alternative execution branches, \n      not what they may have been or if any actually existed\n\n   Drawbacks:\n    - requires signing keys to be online when constructing scripts (cannot\n      do complicated pay to cold wallet without warming it up)\n    - requires storing signatures for scripts (if you were able to\n      reconstruct the sigs, you could just sign the tx directly and wouldn't\n      use a script)\n    - cannot prove that alternative methods of spending are not\n      possible to anyone who doesn't exclusively hold (part of) the\n      output address private key\n    - adds an extra signature check on script spends\n\n   Approaches:\n    - bump segwit version and introduce a \"pay-to-graftroot\" address form\n\n5) Interactive Signature Aggregation\n\n   Requirements:\n    - needs Schnorr\n\n   Description:\n    - allows signers to interactively collaborate when constructing a\n      transaction to produce a single signature that covers multiple\n      inputs and/or OP_CHECKSIG invocations that are resolved by Schnorr\n      signatures\n\n   Benefits:\n    - reduces computational cost of additional signatures (i think?)\n    - reduces witness storage needed for additional signatures to just the\n      sighash flag byte (or bytes, if it's expanded)\n    - transaction batching and coinjoins potentially become cheaper than\n      independent transactions, indirectly improving on-chain privacy\n\n   Drawbacks:\n    - each soft-fork introduces a checkpoint, such that signatures that\n      are not validated by versions prior to the soft-fork cannot be\n      aggregated with signatures that are validated by versions prior to\n      the soft-fork (see [0] for discussion about avoiding that drawback)\n\n   Approaches:\n    - crypto logic can be implemented either by Bellare-Neven or MuSig\n    - needs a new p2wpkh output format, so likely warrants a segwit\n      version bump\n    - may warrant allowing multiple aggregation buckets\n    - may warrant peer-to-peer changes and a new per-tx witness\n\n6) Non-interactive half-signature aggregation within transaction\n\n   Requirements:\n     - needs Schnorr\n     - needs a security proof before deployment\n\n   Benefits:\n     - can halve the size of non-aggregatable signatures in a transaction\n     - in particular implies the size overhead of a graftroot script\n       is just 32B, the same as a taproot script\n\n   Drawbacks:\n     - cannot be used with scriptless-script signatures\n\n   Approaches:\n     - ideally best combined with interactive aggregate signatures, as it\n       has similar implementation requirements\n\n7) New SIGHASH modes\n\n   These will also need a new segwit version (for p2pk/p2pkh) and probably\n   need to be considered at the same time.\n\n8) p2pk versus p2pkh\n\n   Whether to stick with a pubkeyhash for the address or just have a pubkey\n   needs to be decided for any new segwit version.\n\n9) Other new opcodes\n\n   Should additional opcodes in new segwit versions be reserved as OP_NOP or\n   as OP_RETURN_VALID, or something else?\n\n   Should any meaningful new opcodes be supported or re-enabled?\n\n10) Hard-fork automatic upgrade of p2pkh to be spendable via segwit\n\n   Making existing p2pk or p2pkh outputs spendable via Schnorr with\n   interactive signature aggregation would likely be a big win for people\n   with old UTXOs, without any decrease in security, especially if done\n   a significant time after those features were supported for new outputs.\n\n11) Should addresses be hashes or scripts?\n\n   maaku's arguments for general opcodes for MAST make me wonder a bit\n   if the \"p2pkh\" approach isn't better than the \"p2wpkh\" approach; ie\n   should we have script opcodes as the top level way to write addresses,\n   rather than picking the \"best\" form of address everyone should use,\n   and having people have to opt-out of that. probably already too late\n   to actually have that debate though.\n\nAnyway, I think what that adds up to is:\n\n - Everything other than MAST and maybe some misc new CHECKVERIFY opcodes\n   really needs to be done via new segwit versions\n\n - We can evaluate MAST in segwit v0 independently -- use the existing\n   BIPs to deploy MAST for v0; and re-evaluate entirely for v1 and later\n   segwit versions.\n\n - There is no point deploying any of this for non-segwit scripts\n\n - Having the taproot script be a MAST root probably makes sense. If so,\n   a separate OP_MERKLE_MEMBERSHIP_CHECK opcode still probably makes\n   sense at some point.\n\nSo I think that adds up to:\n\n a) soft-fork for MAST in segwit v0 anytime if there's community/economic\n    support for it?\n\n b) soft-fork for OP_CHECK_SCHNORR_SIG_VERIFY in segwit v0 anytime\n\n c) soft-fork for segwit v1 providing Schnorr p2pk(h) addresses and\n    taproot+mast addresses in not too much time\n\n d) soft-fork for segwit v2 introducing further upgrades, particularly\n    graftroot\n\n e) soft-fork for segwit v2 to support interactive signature aggregation\n\n f) soft-fork for segwit v3 including non-interactive sig aggregation\n\nThe rationale there is:\n\n  (a) and (b) are self-contained and we could do them now. My feeling is\n  better to skip them and go straight to (c)\n\n  (c) is the collection of stuff that would be a huge win, and seems\n  \"easily\" technically feasible. signature aggregation seems too\n  complicated to fit in here, and getting the other stuff done while we\n  finish thinking about sigagg seems completely worthwhile.\n\n  (d) is a followon for (c), in case signature aggregation takes a\n  *really* long while. It could conceivably be done as a different\n  variation of segwit v1, really. It might turn out that there's no\n  urgency for graftroot and it should be delayed until non-interactive\n  sig aggregation is implementable.\n\n  (e) and (f) are separated just because I worry that non-interactive\n  sig aggregation might not turn out to be possible; doing them as a\n  single upgrade would be preferrable.\n\nCheers,\naj\n\n[0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-March/015838.html"
            },
            {
                "author": "Russell O'Connor",
                "date": "2018-05-10T14:23:09",
                "message_text_only": "Thanks for writing this up Anthony.\n\nDo you think that a CHECKSIGFROMSTACK proposal should be included within\nthis discussion of signature soft-forks, or do you see it as an unrelated\nissue?\n\nCHECKSIGFROMSTACK enables some forms of (more) efficent MPC (See\nhttp://people.csail.mit.edu/ranjit/papers/scd.pdf), enables poor-man's\ncovenants, and I believe the lightning folks are interested in it as well\nfor some constant space storage scheme.\n\nOn Thu, May 10, 2018 at 8:10 AM, Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello world,\n>\n> After the core dev meetup in March I wrote up some notes of where I\n> think things stand for signing stuff post-Schnorr. It was mostly for my\n> own benefit but maybe it's helpful for others too, so...\n>\n> They're just notes, so may assume a fair bit of background to be able to\n> understand the meaning of the bullet points. In particular, note that I'm\n> using \"schnorr\" just to describe the signature algorithm, and the terms\n> \"key aggregation\" to describe turning an n-of-n key multisig setup into\n> a single key setup, and \"signature aggregation\" to describe combining\n> signatures from many inputs/transactions together: those are often all\n> just called \"schnorr signatures\" in various places.\n>\n>\n> Anyway! I think it's fair to split the ideas around up as follows:\n>\n> 1) Schnorr CHECKSIG\n>\n>   Benefits:\n>     - opportunity to change signature encoding from DER to save a few\n>       bytes per signature, and have fixed size signatures making tx size\n>       calculations easier\n>\n>     - enables n-of-n multisig key aggregation (a single pubkey and\n>       signature gives n-of-n security; setup non-interactively via muSig,\n>       or semi-interactively via proof of possession of private key;\n>       interactive signature protocol)\n>\n>     - enables m-of-n multisig key aggregation with interactive setup and\n>       interactive signature protocol, and possibly substantial storage\n>       requirements for participating signers\n>\n>     - enables scriptless scripts and discreet log contracts via\n>       key aggregation and interactive\n>\n>     - enables payment decorrelation for lightning\n>\n>     - enables batch validation of signatures, which substantially reduces\n>       computational cost of signature verification, provided a single\n>       \"all sigs valid\" or \"some sig(s) invalid\" output (rather than\n>       \"sig number 5 is invalid\") is sufficient\n>\n>     - better than ecdsa due to reducing signature malleability\n>       (and possibly due to having a security proof that has had more\n>       review?)\n>\n>    Approaches:\n>      - bump segwit version to replace P2WPKH\n>      - replace an existing OP_NOP with OP_CHECKSCHNORRVERIFY\n>      - hardfork to allowing existing addresses to be solved via Schnorr sig\n>        as alternative to ECDSA\n>\n> 2) Merkelized Abstract Syntax Trees\n>\n>    Two main benefits for enabling MAST:\n>     - logarithmic scaling for scripts with many alternative paths\n>     - only reveals (approximate) number of alternative execution branches,\n>       not what they may have been\n>\n>    Approaches:\n>     - replace an existing OP_NOP with OP_MERKLE_TREE_VERIFY, and treat an\n>       item remaining on the alt stack at the end of script exeution as a\n>       script and do tail-recursion into it (BIP 116, 117)\n>     - bump the segwit version and introduce a \"pay-to-merkelized-script\"\n>       address form (BIP 114)\n>\n> 3) Taproot\n>\n>    Requirements:\n>     - only feasible if Schnorr is available (required in order to make the\n>       pubkey spend actually be a multisig spend)\n>     - andytoshi has written up a security proof at\n>       https://github.com/apoelstra/taproot\n>\n>    Benefits:\n>     - combines pay-to-pubkey and pay-to-script in a single address,\n>       improving privacy\n>     - allows choice of whether to use pubkey or script at spend time,\n>       allowing for more efficient spends (via pubkey) without reducing\n>       flexibility (via script)\n>\n>    Approaches:\n>     - bump segwit version and introduce a \"pay-to-taproot\" address form\n>\n> 4) Graftroot\n>\n>    Requirements:\n>     - only really feasible if Schnorr is implemented first, so that\n>       multiple signers can be required via a single pubkey/signature\n>     - people seem to want a security proof for this; not sure if that's\n>       hard or straightforward\n>\n>    Benefits:\n>     - allows delegation of authorisation to spend an output already\n>       on the blockchain\n>     - constant scaling for scripts with many alternative paths\n>       (better than MAST's logarithmic scaling)\n>     - only reveals the possibility of alternative execution branches,\n>       not what they may have been or if any actually existed\n>\n>    Drawbacks:\n>     - requires signing keys to be online when constructing scripts (cannot\n>       do complicated pay to cold wallet without warming it up)\n>     - requires storing signatures for scripts (if you were able to\n>       reconstruct the sigs, you could just sign the tx directly and\n> wouldn't\n>       use a script)\n>     - cannot prove that alternative methods of spending are not\n>       possible to anyone who doesn't exclusively hold (part of) the\n>       output address private key\n>     - adds an extra signature check on script spends\n>\n>    Approaches:\n>     - bump segwit version and introduce a \"pay-to-graftroot\" address form\n>\n> 5) Interactive Signature Aggregation\n>\n>    Requirements:\n>     - needs Schnorr\n>\n>    Description:\n>     - allows signers to interactively collaborate when constructing a\n>       transaction to produce a single signature that covers multiple\n>       inputs and/or OP_CHECKSIG invocations that are resolved by Schnorr\n>       signatures\n>\n>    Benefits:\n>     - reduces computational cost of additional signatures (i think?)\n>     - reduces witness storage needed for additional signatures to just the\n>       sighash flag byte (or bytes, if it's expanded)\n>     - transaction batching and coinjoins potentially become cheaper than\n>       independent transactions, indirectly improving on-chain privacy\n>\n>    Drawbacks:\n>     - each soft-fork introduces a checkpoint, such that signatures that\n>       are not validated by versions prior to the soft-fork cannot be\n>       aggregated with signatures that are validated by versions prior to\n>       the soft-fork (see [0] for discussion about avoiding that drawback)\n>\n>    Approaches:\n>     - crypto logic can be implemented either by Bellare-Neven or MuSig\n>     - needs a new p2wpkh output format, so likely warrants a segwit\n>       version bump\n>     - may warrant allowing multiple aggregation buckets\n>     - may warrant peer-to-peer changes and a new per-tx witness\n>\n> 6) Non-interactive half-signature aggregation within transaction\n>\n>    Requirements:\n>      - needs Schnorr\n>      - needs a security proof before deployment\n>\n>    Benefits:\n>      - can halve the size of non-aggregatable signatures in a transaction\n>      - in particular implies the size overhead of a graftroot script\n>        is just 32B, the same as a taproot script\n>\n>    Drawbacks:\n>      - cannot be used with scriptless-script signatures\n>\n>    Approaches:\n>      - ideally best combined with interactive aggregate signatures, as it\n>        has similar implementation requirements\n>\n> 7) New SIGHASH modes\n>\n>    These will also need a new segwit version (for p2pk/p2pkh) and probably\n>    need to be considered at the same time.\n>\n> 8) p2pk versus p2pkh\n>\n>    Whether to stick with a pubkeyhash for the address or just have a pubkey\n>    needs to be decided for any new segwit version.\n>\n> 9) Other new opcodes\n>\n>    Should additional opcodes in new segwit versions be reserved as OP_NOP\n> or\n>    as OP_RETURN_VALID, or something else?\n>\n>    Should any meaningful new opcodes be supported or re-enabled?\n>\n> 10) Hard-fork automatic upgrade of p2pkh to be spendable via segwit\n>\n>    Making existing p2pk or p2pkh outputs spendable via Schnorr with\n>    interactive signature aggregation would likely be a big win for people\n>    with old UTXOs, without any decrease in security, especially if done\n>    a significant time after those features were supported for new outputs.\n>\n> 11) Should addresses be hashes or scripts?\n>\n>    maaku's arguments for general opcodes for MAST make me wonder a bit\n>    if the \"p2pkh\" approach isn't better than the \"p2wpkh\" approach; ie\n>    should we have script opcodes as the top level way to write addresses,\n>    rather than picking the \"best\" form of address everyone should use,\n>    and having people have to opt-out of that. probably already too late\n>    to actually have that debate though.\n>\n> Anyway, I think what that adds up to is:\n>\n>  - Everything other than MAST and maybe some misc new CHECKVERIFY opcodes\n>    really needs to be done via new segwit versions\n>\n>  - We can evaluate MAST in segwit v0 independently -- use the existing\n>    BIPs to deploy MAST for v0; and re-evaluate entirely for v1 and later\n>    segwit versions.\n>\n>  - There is no point deploying any of this for non-segwit scripts\n>\n>  - Having the taproot script be a MAST root probably makes sense. If so,\n>    a separate OP_MERKLE_MEMBERSHIP_CHECK opcode still probably makes\n>    sense at some point.\n>\n> So I think that adds up to:\n>\n>  a) soft-fork for MAST in segwit v0 anytime if there's community/economic\n>     support for it?\n>\n>  b) soft-fork for OP_CHECK_SCHNORR_SIG_VERIFY in segwit v0 anytime\n>\n>  c) soft-fork for segwit v1 providing Schnorr p2pk(h) addresses and\n>     taproot+mast addresses in not too much time\n>\n>  d) soft-fork for segwit v2 introducing further upgrades, particularly\n>     graftroot\n>\n>  e) soft-fork for segwit v2 to support interactive signature aggregation\n>\n>  f) soft-fork for segwit v3 including non-interactive sig aggregation\n>\n> The rationale there is:\n>\n>   (a) and (b) are self-contained and we could do them now. My feeling is\n>   better to skip them and go straight to (c)\n>\n>   (c) is the collection of stuff that would be a huge win, and seems\n>   \"easily\" technically feasible. signature aggregation seems too\n>   complicated to fit in here, and getting the other stuff done while we\n>   finish thinking about sigagg seems completely worthwhile.\n>\n>   (d) is a followon for (c), in case signature aggregation takes a\n>   *really* long while. It could conceivably be done as a different\n>   variation of segwit v1, really. It might turn out that there's no\n>   urgency for graftroot and it should be delayed until non-interactive\n>   sig aggregation is implementable.\n>\n>   (e) and (f) are separated just because I worry that non-interactive\n>   sig aggregation might not turn out to be possible; doing them as a\n>   single upgrade would be preferrable.\n>\n> Cheers,\n> aj\n>\n> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2018-March/015838.html\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180510/dacd13e6/attachment-0001.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-05-10T20:11:04",
                "message_text_only": "I'm not sure about the best way to approach soft-forking (I've opined on it\nbefore, and still find the details mind-numbing) but the end goal seems\nfairly clearly to be an all of the above: Have aggregatable public keys\nwhich support simple signatures, taproot with BIP 114 style taproot, and\nGraftroot. And while you're at it, nuke OP_IF from orbit and make all the\nunused opcodes be return success.\n\nThis all in principle could be done in one fell swoop with a single new\nscript type. That would be a whole lot of stuff to roll out at once, but at\nleast it wouldn't have so many painstaking intermediate soft forks to\nadminister.\n\nOn Thu, May 10, 2018 at 7:23 AM, Russell O'Connor via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Thanks for writing this up Anthony.\n>\n> Do you think that a CHECKSIGFROMSTACK proposal should be included within\n> this discussion of signature soft-forks, or do you see it as an unrelated\n> issue?\n>\n> CHECKSIGFROMSTACK enables some forms of (more) efficent MPC (See\n> http://people.csail.mit.edu/ranjit/papers/scd.pdf), enables poor-man's\n> covenants, and I believe the lightning folks are interested in it as well\n> for some constant space storage scheme.\n>\n> On Thu, May 10, 2018 at 8:10 AM, Anthony Towns via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hello world,\n>>\n>> After the core dev meetup in March I wrote up some notes of where I\n>> think things stand for signing stuff post-Schnorr. It was mostly for my\n>> own benefit but maybe it's helpful for others too, so...\n>>\n>> They're just notes, so may assume a fair bit of background to be able to\n>> understand the meaning of the bullet points. In particular, note that I'm\n>> using \"schnorr\" just to describe the signature algorithm, and the terms\n>> \"key aggregation\" to describe turning an n-of-n key multisig setup into\n>> a single key setup, and \"signature aggregation\" to describe combining\n>> signatures from many inputs/transactions together: those are often all\n>> just called \"schnorr signatures\" in various places.\n>>\n>>\n>> Anyway! I think it's fair to split the ideas around up as follows:\n>>\n>> 1) Schnorr CHECKSIG\n>>\n>>   Benefits:\n>>     - opportunity to change signature encoding from DER to save a few\n>>       bytes per signature, and have fixed size signatures making tx size\n>>       calculations easier\n>>\n>>     - enables n-of-n multisig key aggregation (a single pubkey and\n>>       signature gives n-of-n security; setup non-interactively via muSig,\n>>       or semi-interactively via proof of possession of private key;\n>>       interactive signature protocol)\n>>\n>>     - enables m-of-n multisig key aggregation with interactive setup and\n>>       interactive signature protocol, and possibly substantial storage\n>>       requirements for participating signers\n>>\n>>     - enables scriptless scripts and discreet log contracts via\n>>       key aggregation and interactive\n>>\n>>     - enables payment decorrelation for lightning\n>>\n>>     - enables batch validation of signatures, which substantially reduces\n>>       computational cost of signature verification, provided a single\n>>       \"all sigs valid\" or \"some sig(s) invalid\" output (rather than\n>>       \"sig number 5 is invalid\") is sufficient\n>>\n>>     - better than ecdsa due to reducing signature malleability\n>>       (and possibly due to having a security proof that has had more\n>>       review?)\n>>\n>>    Approaches:\n>>      - bump segwit version to replace P2WPKH\n>>      - replace an existing OP_NOP with OP_CHECKSCHNORRVERIFY\n>>      - hardfork to allowing existing addresses to be solved via Schnorr\n>> sig\n>>        as alternative to ECDSA\n>>\n>> 2) Merkelized Abstract Syntax Trees\n>>\n>>    Two main benefits for enabling MAST:\n>>     - logarithmic scaling for scripts with many alternative paths\n>>     - only reveals (approximate) number of alternative execution branches,\n>>       not what they may have been\n>>\n>>    Approaches:\n>>     - replace an existing OP_NOP with OP_MERKLE_TREE_VERIFY, and treat an\n>>       item remaining on the alt stack at the end of script exeution as a\n>>       script and do tail-recursion into it (BIP 116, 117)\n>>     - bump the segwit version and introduce a \"pay-to-merkelized-script\"\n>>       address form (BIP 114)\n>>\n>> 3) Taproot\n>>\n>>    Requirements:\n>>     - only feasible if Schnorr is available (required in order to make the\n>>       pubkey spend actually be a multisig spend)\n>>     - andytoshi has written up a security proof at\n>>       https://github.com/apoelstra/taproot\n>>\n>>    Benefits:\n>>     - combines pay-to-pubkey and pay-to-script in a single address,\n>>       improving privacy\n>>     - allows choice of whether to use pubkey or script at spend time,\n>>       allowing for more efficient spends (via pubkey) without reducing\n>>       flexibility (via script)\n>>\n>>    Approaches:\n>>     - bump segwit version and introduce a \"pay-to-taproot\" address form\n>>\n>> 4) Graftroot\n>>\n>>    Requirements:\n>>     - only really feasible if Schnorr is implemented first, so that\n>>       multiple signers can be required via a single pubkey/signature\n>>     - people seem to want a security proof for this; not sure if that's\n>>       hard or straightforward\n>>\n>>    Benefits:\n>>     - allows delegation of authorisation to spend an output already\n>>       on the blockchain\n>>     - constant scaling for scripts with many alternative paths\n>>       (better than MAST's logarithmic scaling)\n>>     - only reveals the possibility of alternative execution branches,\n>>       not what they may have been or if any actually existed\n>>\n>>    Drawbacks:\n>>     - requires signing keys to be online when constructing scripts (cannot\n>>       do complicated pay to cold wallet without warming it up)\n>>     - requires storing signatures for scripts (if you were able to\n>>       reconstruct the sigs, you could just sign the tx directly and\n>> wouldn't\n>>       use a script)\n>>     - cannot prove that alternative methods of spending are not\n>>       possible to anyone who doesn't exclusively hold (part of) the\n>>       output address private key\n>>     - adds an extra signature check on script spends\n>>\n>>    Approaches:\n>>     - bump segwit version and introduce a \"pay-to-graftroot\" address form\n>>\n>> 5) Interactive Signature Aggregation\n>>\n>>    Requirements:\n>>     - needs Schnorr\n>>\n>>    Description:\n>>     - allows signers to interactively collaborate when constructing a\n>>       transaction to produce a single signature that covers multiple\n>>       inputs and/or OP_CHECKSIG invocations that are resolved by Schnorr\n>>       signatures\n>>\n>>    Benefits:\n>>     - reduces computational cost of additional signatures (i think?)\n>>     - reduces witness storage needed for additional signatures to just the\n>>       sighash flag byte (or bytes, if it's expanded)\n>>     - transaction batching and coinjoins potentially become cheaper than\n>>       independent transactions, indirectly improving on-chain privacy\n>>\n>>    Drawbacks:\n>>     - each soft-fork introduces a checkpoint, such that signatures that\n>>       are not validated by versions prior to the soft-fork cannot be\n>>       aggregated with signatures that are validated by versions prior to\n>>       the soft-fork (see [0] for discussion about avoiding that drawback)\n>>\n>>    Approaches:\n>>     - crypto logic can be implemented either by Bellare-Neven or MuSig\n>>     - needs a new p2wpkh output format, so likely warrants a segwit\n>>       version bump\n>>     - may warrant allowing multiple aggregation buckets\n>>     - may warrant peer-to-peer changes and a new per-tx witness\n>>\n>> 6) Non-interactive half-signature aggregation within transaction\n>>\n>>    Requirements:\n>>      - needs Schnorr\n>>      - needs a security proof before deployment\n>>\n>>    Benefits:\n>>      - can halve the size of non-aggregatable signatures in a transaction\n>>      - in particular implies the size overhead of a graftroot script\n>>        is just 32B, the same as a taproot script\n>>\n>>    Drawbacks:\n>>      - cannot be used with scriptless-script signatures\n>>\n>>    Approaches:\n>>      - ideally best combined with interactive aggregate signatures, as it\n>>        has similar implementation requirements\n>>\n>> 7) New SIGHASH modes\n>>\n>>    These will also need a new segwit version (for p2pk/p2pkh) and probably\n>>    need to be considered at the same time.\n>>\n>> 8) p2pk versus p2pkh\n>>\n>>    Whether to stick with a pubkeyhash for the address or just have a\n>> pubkey\n>>    needs to be decided for any new segwit version.\n>>\n>> 9) Other new opcodes\n>>\n>>    Should additional opcodes in new segwit versions be reserved as OP_NOP\n>> or\n>>    as OP_RETURN_VALID, or something else?\n>>\n>>    Should any meaningful new opcodes be supported or re-enabled?\n>>\n>> 10) Hard-fork automatic upgrade of p2pkh to be spendable via segwit\n>>\n>>    Making existing p2pk or p2pkh outputs spendable via Schnorr with\n>>    interactive signature aggregation would likely be a big win for people\n>>    with old UTXOs, without any decrease in security, especially if done\n>>    a significant time after those features were supported for new outputs.\n>>\n>> 11) Should addresses be hashes or scripts?\n>>\n>>    maaku's arguments for general opcodes for MAST make me wonder a bit\n>>    if the \"p2pkh\" approach isn't better than the \"p2wpkh\" approach; ie\n>>    should we have script opcodes as the top level way to write addresses,\n>>    rather than picking the \"best\" form of address everyone should use,\n>>    and having people have to opt-out of that. probably already too late\n>>    to actually have that debate though.\n>>\n>> Anyway, I think what that adds up to is:\n>>\n>>  - Everything other than MAST and maybe some misc new CHECKVERIFY opcodes\n>>    really needs to be done via new segwit versions\n>>\n>>  - We can evaluate MAST in segwit v0 independently -- use the existing\n>>    BIPs to deploy MAST for v0; and re-evaluate entirely for v1 and later\n>>    segwit versions.\n>>\n>>  - There is no point deploying any of this for non-segwit scripts\n>>\n>>  - Having the taproot script be a MAST root probably makes sense. If so,\n>>    a separate OP_MERKLE_MEMBERSHIP_CHECK opcode still probably makes\n>>    sense at some point.\n>>\n>> So I think that adds up to:\n>>\n>>  a) soft-fork for MAST in segwit v0 anytime if there's community/economic\n>>     support for it?\n>>\n>>  b) soft-fork for OP_CHECK_SCHNORR_SIG_VERIFY in segwit v0 anytime\n>>\n>>  c) soft-fork for segwit v1 providing Schnorr p2pk(h) addresses and\n>>     taproot+mast addresses in not too much time\n>>\n>>  d) soft-fork for segwit v2 introducing further upgrades, particularly\n>>     graftroot\n>>\n>>  e) soft-fork for segwit v2 to support interactive signature aggregation\n>>\n>>  f) soft-fork for segwit v3 including non-interactive sig aggregation\n>>\n>> The rationale there is:\n>>\n>>   (a) and (b) are self-contained and we could do them now. My feeling is\n>>   better to skip them and go straight to (c)\n>>\n>>   (c) is the collection of stuff that would be a huge win, and seems\n>>   \"easily\" technically feasible. signature aggregation seems too\n>>   complicated to fit in here, and getting the other stuff done while we\n>>   finish thinking about sigagg seems completely worthwhile.\n>>\n>>   (d) is a followon for (c), in case signature aggregation takes a\n>>   *really* long while. It could conceivably be done as a different\n>>   variation of segwit v1, really. It might turn out that there's no\n>>   urgency for graftroot and it should be delayed until non-interactive\n>>   sig aggregation is implementable.\n>>\n>>   (e) and (f) are separated just because I worry that non-interactive\n>>   sig aggregation might not turn out to be possible; doing them as a\n>>   single upgrade would be preferrable.\n>>\n>> Cheers,\n>> aj\n>>\n>> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018\n>> -March/015838.html\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180510/4f7a41ed/attachment-0001.html>"
            },
            {
                "author": "Chris Belcher",
                "date": "2018-05-10T22:44:36",
                "message_text_only": "Thanks for the summary,\n\nIt may be worth emphasizing the fungibility aspects of all this.\n\nThat summary contains ideas to possibly have separate address types,\nopcodes and scriptSigs/witnesses for different feature, at least to\nstart with. To me this would seem bad because it may miss out on the\nfungibility gain from having everything look exactly the same.\n\nWith schnorr we may have a unique opportunity to greatly improve\nfungibility. It's not too hard to imagine a world where users of\nLightning Network, coinswap, MAST, scriptless scripts, multisig,\ntaproot, graftroot, etc and regular single-signature on-chain payments\nall appear completely indistinguishable. Tracking and data mining could\nbecome pointless when coins can teleport undetectably to a different\nplace on the blockchain via any number of off-chain protocols.\n\nOf course the downside of doing it like this is that every feature would\nprobably have to be developed, reviewed, tested and deployed together,\nrather than one at a time.\n\nOn 10/05/18 13:10, Anthony Towns via bitcoin-dev wrote:\n> Hello world,\n> \n> After the core dev meetup in March I wrote up some notes of where I\n> think things stand for signing stuff post-Schnorr. It was mostly for my\n> own benefit but maybe it's helpful for others too, so...\n> \n> They're just notes, so may assume a fair bit of background to be able to\n> understand the meaning of the bullet points. In particular, note that I'm\n> using \"schnorr\" just to describe the signature algorithm, and the terms\n> \"key aggregation\" to describe turning an n-of-n key multisig setup into\n> a single key setup, and \"signature aggregation\" to describe combining\n> signatures from many inputs/transactions together: those are often all\n> just called \"schnorr signatures\" in various places.\n> \n> \n> Anyway! I think it's fair to split the ideas around up as follows:\n> \n> 1) Schnorr CHECKSIG\n> \n>   Benefits:\n>     - opportunity to change signature encoding from DER to save a few\n>       bytes per signature, and have fixed size signatures making tx size\n>       calculations easier\n> \n>     - enables n-of-n multisig key aggregation (a single pubkey and\n>       signature gives n-of-n security; setup non-interactively via muSig,\n>       or semi-interactively via proof of possession of private key;\n>       interactive signature protocol)\n> \n>     - enables m-of-n multisig key aggregation with interactive setup and\n>       interactive signature protocol, and possibly substantial storage\n>       requirements for participating signers\n> \n>     - enables scriptless scripts and discreet log contracts via\n>       key aggregation and interactive\n> \n>     - enables payment decorrelation for lightning\n> \n>     - enables batch validation of signatures, which substantially reduces\n>       computational cost of signature verification, provided a single\n>       \"all sigs valid\" or \"some sig(s) invalid\" output (rather than\n>       \"sig number 5 is invalid\") is sufficient\n> \n>     - better than ecdsa due to reducing signature malleability\n>       (and possibly due to having a security proof that has had more\n>       review?)\n> \n>    Approaches:\n>      - bump segwit version to replace P2WPKH\n>      - replace an existing OP_NOP with OP_CHECKSCHNORRVERIFY\n>      - hardfork to allowing existing addresses to be solved via Schnorr sig\n>        as alternative to ECDSA\n> \n> 2) Merkelized Abstract Syntax Trees\n> \n>    Two main benefits for enabling MAST:\n>     - logarithmic scaling for scripts with many alternative paths\n>     - only reveals (approximate) number of alternative execution branches,\n>       not what they may have been\n> \n>    Approaches:\n>     - replace an existing OP_NOP with OP_MERKLE_TREE_VERIFY, and treat an\n>       item remaining on the alt stack at the end of script exeution as a\n>       script and do tail-recursion into it (BIP 116, 117)\n>     - bump the segwit version and introduce a \"pay-to-merkelized-script\"\n>       address form (BIP 114)\n> \n> 3) Taproot\n> \n>    Requirements:\n>     - only feasible if Schnorr is available (required in order to make the\n>       pubkey spend actually be a multisig spend)\n>     - andytoshi has written up a security proof at\n>       https://github.com/apoelstra/taproot\n> \n>    Benefits:\n>     - combines pay-to-pubkey and pay-to-script in a single address,\n>       improving privacy\n>     - allows choice of whether to use pubkey or script at spend time,\n>       allowing for more efficient spends (via pubkey) without reducing\n>       flexibility (via script)\n> \n>    Approaches:\n>     - bump segwit version and introduce a \"pay-to-taproot\" address form\n> \n> 4) Graftroot\n> \n>    Requirements:\n>     - only really feasible if Schnorr is implemented first, so that\n>       multiple signers can be required via a single pubkey/signature\n>     - people seem to want a security proof for this; not sure if that's\n>       hard or straightforward\n> \n>    Benefits:\n>     - allows delegation of authorisation to spend an output already\n>       on the blockchain\n>     - constant scaling for scripts with many alternative paths\n>       (better than MAST's logarithmic scaling)\n>     - only reveals the possibility of alternative execution branches, \n>       not what they may have been or if any actually existed\n> \n>    Drawbacks:\n>     - requires signing keys to be online when constructing scripts (cannot\n>       do complicated pay to cold wallet without warming it up)\n>     - requires storing signatures for scripts (if you were able to\n>       reconstruct the sigs, you could just sign the tx directly and wouldn't\n>       use a script)\n>     - cannot prove that alternative methods of spending are not\n>       possible to anyone who doesn't exclusively hold (part of) the\n>       output address private key\n>     - adds an extra signature check on script spends\n> \n>    Approaches:\n>     - bump segwit version and introduce a \"pay-to-graftroot\" address form\n> \n> 5) Interactive Signature Aggregation\n> \n>    Requirements:\n>     - needs Schnorr\n> \n>    Description:\n>     - allows signers to interactively collaborate when constructing a\n>       transaction to produce a single signature that covers multiple\n>       inputs and/or OP_CHECKSIG invocations that are resolved by Schnorr\n>       signatures\n> \n>    Benefits:\n>     - reduces computational cost of additional signatures (i think?)\n>     - reduces witness storage needed for additional signatures to just the\n>       sighash flag byte (or bytes, if it's expanded)\n>     - transaction batching and coinjoins potentially become cheaper than\n>       independent transactions, indirectly improving on-chain privacy\n> \n>    Drawbacks:\n>     - each soft-fork introduces a checkpoint, such that signatures that\n>       are not validated by versions prior to the soft-fork cannot be\n>       aggregated with signatures that are validated by versions prior to\n>       the soft-fork (see [0] for discussion about avoiding that drawback)\n> \n>    Approaches:\n>     - crypto logic can be implemented either by Bellare-Neven or MuSig\n>     - needs a new p2wpkh output format, so likely warrants a segwit\n>       version bump\n>     - may warrant allowing multiple aggregation buckets\n>     - may warrant peer-to-peer changes and a new per-tx witness\n> \n> 6) Non-interactive half-signature aggregation within transaction\n> \n>    Requirements:\n>      - needs Schnorr\n>      - needs a security proof before deployment\n> \n>    Benefits:\n>      - can halve the size of non-aggregatable signatures in a transaction\n>      - in particular implies the size overhead of a graftroot script\n>        is just 32B, the same as a taproot script\n> \n>    Drawbacks:\n>      - cannot be used with scriptless-script signatures\n> \n>    Approaches:\n>      - ideally best combined with interactive aggregate signatures, as it\n>        has similar implementation requirements\n> \n> 7) New SIGHASH modes\n> \n>    These will also need a new segwit version (for p2pk/p2pkh) and probably\n>    need to be considered at the same time.\n> \n> 8) p2pk versus p2pkh\n> \n>    Whether to stick with a pubkeyhash for the address or just have a pubkey\n>    needs to be decided for any new segwit version.\n> \n> 9) Other new opcodes\n> \n>    Should additional opcodes in new segwit versions be reserved as OP_NOP or\n>    as OP_RETURN_VALID, or something else?\n> \n>    Should any meaningful new opcodes be supported or re-enabled?\n> \n> 10) Hard-fork automatic upgrade of p2pkh to be spendable via segwit\n> \n>    Making existing p2pk or p2pkh outputs spendable via Schnorr with\n>    interactive signature aggregation would likely be a big win for people\n>    with old UTXOs, without any decrease in security, especially if done\n>    a significant time after those features were supported for new outputs.\n> \n> 11) Should addresses be hashes or scripts?\n> \n>    maaku's arguments for general opcodes for MAST make me wonder a bit\n>    if the \"p2pkh\" approach isn't better than the \"p2wpkh\" approach; ie\n>    should we have script opcodes as the top level way to write addresses,\n>    rather than picking the \"best\" form of address everyone should use,\n>    and having people have to opt-out of that. probably already too late\n>    to actually have that debate though.\n> \n> Anyway, I think what that adds up to is:\n> \n>  - Everything other than MAST and maybe some misc new CHECKVERIFY opcodes\n>    really needs to be done via new segwit versions\n> \n>  - We can evaluate MAST in segwit v0 independently -- use the existing\n>    BIPs to deploy MAST for v0; and re-evaluate entirely for v1 and later\n>    segwit versions.\n> \n>  - There is no point deploying any of this for non-segwit scripts\n> \n>  - Having the taproot script be a MAST root probably makes sense. If so,\n>    a separate OP_MERKLE_MEMBERSHIP_CHECK opcode still probably makes\n>    sense at some point.\n> \n> So I think that adds up to:\n> \n>  a) soft-fork for MAST in segwit v0 anytime if there's community/economic\n>     support for it?\n> \n>  b) soft-fork for OP_CHECK_SCHNORR_SIG_VERIFY in segwit v0 anytime\n> \n>  c) soft-fork for segwit v1 providing Schnorr p2pk(h) addresses and\n>     taproot+mast addresses in not too much time\n> \n>  d) soft-fork for segwit v2 introducing further upgrades, particularly\n>     graftroot\n> \n>  e) soft-fork for segwit v2 to support interactive signature aggregation\n> \n>  f) soft-fork for segwit v3 including non-interactive sig aggregation\n> \n> The rationale there is:\n> \n>   (a) and (b) are self-contained and we could do them now. My feeling is\n>   better to skip them and go straight to (c)\n> \n>   (c) is the collection of stuff that would be a huge win, and seems\n>   \"easily\" technically feasible. signature aggregation seems too\n>   complicated to fit in here, and getting the other stuff done while we\n>   finish thinking about sigagg seems completely worthwhile.\n> \n>   (d) is a followon for (c), in case signature aggregation takes a\n>   *really* long while. It could conceivably be done as a different\n>   variation of segwit v1, really. It might turn out that there's no\n>   urgency for graftroot and it should be delayed until non-interactive\n>   sig aggregation is implementable.\n> \n>   (e) and (f) are separated just because I worry that non-interactive\n>   sig aggregation might not turn out to be possible; doing them as a\n>   single upgrade would be preferrable.\n> \n> Cheers,\n> aj\n> \n> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-March/015838.html\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            }
        ],
        "thread_summary": {
            "title": "MAST/Schnorr related soft-forks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Russell O'Connor",
                "Anthony Towns",
                "Chris Belcher",
                "Bram Cohen"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 45254
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal - Dandelion: Privacy Preserving Transaction Propagation",
        "thread_messages": [
            {
                "author": "Bradley Denby",
                "date": "2018-05-10T12:59:12",
                "message_text_only": "Hi all,\n\nWe're writing with an update on the Dandelion project. As a reminder,\nDandelion\nis a practical, lightweight privacy solution that provides Bitcoin users\nformal\nanonymity guarantees. While other privacy solutions aim to protect\nindividual\nusers, Dandelion protects privacy by limiting the capability of adversaries\nto\ndeanonymize the entire network.\n\nBitcoin's transaction spreading protocol is vulnerable to deanonymization\nattacks. When a node generates a transaction without Dandelion, it transmits\nthat transaction to its peers with independent, exponential delays. This\napproach, known as diffusion in academia, allows network adversaries to link\ntransactions to IP addresses.\n\nDandelion prevents this class of attacks by sending transactions over a\nrandomly\nselected path before diffusion. Transactions travel along this path during\nthe\n\"stem phase\" and are then diffused during the \"fluff phase\" (hence the name\nDandelion). We have shown that this routing protocol provides near-optimal\nanonymity guarantees among schemes that do not introduce additional\nencryption\nmechanisms.\n\nSince the last time we contacted the list, we have:\n - Completed additional theoretical analysis and simulations\n - Built a working prototype\n   (https://github.com/mablem8/bitcoin/tree/dandelion)\n - Built a test suite for the prototype\n   (https://github.com/mablem8/bitcoin/blob/dandelion/test/\nfunctional/p2p_dandelion.py)\n - Written detailed documentation for the new implementation\n   (https://github.com/mablem8/bips/blob/master/bip-\ndandelion/dandelion-reference-documentation.pdf)\n\nAmong other things, one question we've addressed in our additional analysis\nis\nhow to route messages during the stem phase. For example, if two Dandelion\ntransactions arrive at a node from different inbound peers, to which\nDandelion\ndestination(s) should these transactions be sent? We have found that some\nchoices are much better than others.\n\nConsider the case in which each Dandelion transaction is forwarded to a\nDandelion destination selected uniformly at random. We have shown that this\napproach results in a fingerprint attack allowing network-level botnet\nadversaries to achieve total deanonymization of the P2P network after\nobserving\nless than ten transactions per node.\n\nTo avoid this issue, we suggest \"per-inbound-edge\" routing. Each inbound\npeer is\nassigned a particular Dandelion destination. Each Dandelion transaction that\narrives via this peer is forwarded to the same Dandelion destination.\nPer-inbound-edge routing breaks the described attack by blocking an\nadversary's\nability to construct useful fingerprints.\n\nThis iteration of Dandelion has been tested on our own small network, and we\nwould like to get the implementation in front of a wider audience. An\nupdated\nBIP document with further details on motivation, specification,\ncompatibility,\nand implementation is located here:\nhttps://github.com/mablem8/bips/blob/master/bip-dandelion.mediawiki\n\nWe would like to thank the Bitcoin Core developers and Gregory Maxwell in\nparticular for their insightful comments, which helped to inform this\nimplementation and some of the follow-up work we conducted. We would also\nlike\nto thank the Mimblewimble development community for coining the term\n\"stempool,\"\nwhich we happily adopted for this implementation.\n\nAll the best,\nBrad Denby <bdenby at cmu.edu>\nAndrew Miller <soc1024 at illinois.edu>\nGiulia Fanti <gfanti at andrew.cmu.edu>\nSurya Bakshi <sbakshi3 at illinois.edu>\nShaileshh Bojja Venkatakrishnan <shaileshh.bv at gmail.com>\nPramod Viswanath <pramodv at illinois.edu>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180510/218df2ca/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal - Dandelion: Privacy Preserving Transaction Propagation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bradley Denby"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3761
        }
    },
    {
        "title": "[bitcoin-dev] UHS: Full-node security without maintaining a full UTXO set",
        "thread_messages": [
            {
                "author": "Cory Fields",
                "date": "2018-05-16T16:36:35",
                "message_text_only": "Tl;dr: Rather than storing all unspent outputs, store their hashes. Untrusted\npeers can supply the full outputs when needed, with very little overhead.\nAny attempt to spoof those outputs would be apparent, as their hashes would not\nbe present in the hash set. There are many advantages to this, most apparently\nin disk and memory savings, as well as a validation speedup. The primary\ndisadvantage is a small increase in network traffic. I believe that the\nadvantages outweigh the disadvantages.\n\n--\n\nBitcoin\u2019s unspent transaction output set (usually referred to as \u201cThe UTXO\nset\u201d) has two primary roles: providing proof that previous outputs exist to be\nspent, and providing the actual previous output data for verification when new\ntransactions attempts to spend them. These roles are not usually discussed\nindependently, but as Bram Cohen's TXO Bitfield [0] idea hints, there are\ncompelling reasons to consider them this way.\n\nTo see why, consider running a node with the following changes:\n\n- For each new output, gather all extra data that will be needed for\n  verification when spending it later as an input: the amount, scriptPubKey,\n  creation height, coinbaseness, and output type (p2pkh, p2sh, p2wpkh, etc.).\n  Call this the Dereferenced Prevout data.\n- Create a hash from the concatenation of the new outpoint and the dereferenced\n  prevout data. Call this a Unspent Transaction Output Hash.\n- Rather than storing the full dereferenced prevout entries in a UTXO set as is\n  currently done, instead store their hashes to an Unspent Transaction Output\n  Hash Set, or UHS.\n- When relaying a transaction, append the dereferenced prevout for each input.\n\nNow when a transaction is received, it contains everything needed for\nverification, including the input amount, height, and coinbaseness, which would\nhave otherwise required a lookup the UTXO set.\n\nTo verify an input's unspentness, again create a hash from the concatenation of\nthe referenced outpoint and the provided dereferenced prevout, and check for\nits presence in the UHS. The hash will only be present if a hash of the exact\nsame data was previously added to (and not since removed from) the UHS. As\nsuch, we are protected from a peer attempting to lie about the dereferenced\nprevout data.\n\n### Some benefits of the UHS model\n\n- Requires no consensus changes, purely a p2p/implementation change.\n\n- UHS is substantially smaller than a full UTXO set (just over half for the\n  main chain, see below). In-memory caching can be much more effective as a\n  result.\n\n- A block\u2019s transactions can be fully verified before doing a potentially\n  expensive database lookup for the previous output data. The UHS can be\n  queried afterwards (or in parallel) to verify previous output inclusion.\n\n- Entire blocks could potentially be verified out-of-order because all input\n  data is provided; only the inclusion checks have to be in-order. Admittedly\n  this is likely too complicated to be realistic.\n\n- pay-to-pubkey outputs are less burdensome on full nodes, since they use no\n  more space on-disk than pay-to-pubkey-hash or pay-to-script-hash. Taproot and\n  Graftroot outputs may share the same benefits.\n\n- The burden of holding UTXO data is technically shifted from the verifiers to\n  the spender. In reality, full nodes will likely always have a copy as well,\n  but conceptually it's a slight improvement to the incentive model.\n\n- Block data from peers can also be used to roll backwards during a reorg. This\n  potentially enables an even more aggressive pruning mode.\n\n- UTXO storage size grows exactly linearly with UTXO count, as opposed to\n  growing linearly with UTXO data size. This may be relevant when considering\n  new larger output types which would otherwise cause the UTXO Set size to\n  increase more quickly.\n\n- The UHS is a simple set, no need for a key-value database. LevelDB could\n  potentially be dropped as a dependency in some distant future.\n\n- Potentially integrates nicely with Pieter Wuille's Rolling UTXO set hashes\n  [1]. Unspent Transaction Output Hashes would simply be mapped to points on a\n  curve before adding them to the set.\n\n- With the help of inclusion proofs and rolling hashes, libbitcoinconsensus\n  could potentially safely verify entire blocks. The size of the required\n  proofs would be largely irrelevant as they would be consumed locally.\n\n- Others?\n\n### TxIn De-duplication\n\nSetting aside the potential benefits, the obvious drawback of using a UHS is a\nsignificant network traffic increase. Fortunately, some properties of\ntransactions can be exploited to offset most of the difference.\n\nFor quick reference:\n\np2pkh scriptPubKey: DUP HASH160 [pubkey hash] EQUALVERIFY CHECKSIG\np2pkh scriptSig:    [signature] [pubkey]\n\np2sh scriptPubKey:  HASH160 [script hash] EQUAL\np2sh scriptSig:     [signature(s)] [script]\n\nNotice that if a peer is sending a scriptPubKey and a scriptSig together, as\nthey would when using a UHS, there would likely be some redundancy. Using a\np2sh output for example, the scriptPubKey contains the script hash, and the\nscriptSig contains the script itself. Therefore when sending dereferenced\nprevouts over the wire, any hash which can be computed can be omitted and only\nthe preimages sent.\n\nNon-standard output scripts must be sent in-full, though because they account\nfor only ~1% of all current UTXOs, they are rare enough to be ignored here.\n\n### Intra-block Script De-duplication\n\nWhen transactions are chained together in the same block, dereferenced prevout\ndata for these inputs would be redundant, as the full output data is already\npresent. For that reason, these dereferenced prevouts can be omitted when\nsending over the wire.\n\nThe downside would be a new reconstruction pass requirement prior to\nvalidation.\n\n### Data\n\nHere's some preliminary testing with a naive POC implementation patched into\nBitcoin Core. Note that the final sizes will depend on optimization of the\nserialization format. The format used for these tests is suboptimal for sure.\nSyncing mainnet to block 516346:\n\n                      UTXO Node      UHS Node\n  IBD Network Data:   153G           157G\n  Block disk space:   175G           157G\n  UTXO disk space :   2.8G           1.6G\n  Total disk space:   177.8G         158.6G\n\nThe on-disk block-space reduction comes from the elimination of the Undo data\nthat Bitcoin Core uses to roll back orphaned blocks. For UHS Nodes, this data\nis merged into to the block data and de-duplicated.\n\nCompared to the UXTO model, using a UHS reduces disk space by ~12%, yet only\nrequires ~5% more data over the wire.\n\nExperimentation shows intra-block de-duplication to be of little help in\npractice, as it only reduces overhead by ~0.2% on mainnet. It could become more\nuseful if, for example, CPFP usage increases substantially in the future.\n\n### Safety\n\nAssuming sha256 for the UHS's hash function, I don't believe any fundamental\nchanges to Bitcoin's security model are introduced. Because the unspent\ntransaction output hashes commit to all necessary data, including output types,\nit should not be possible to be tricked into validating using mutated or forged\ninputs.\n\n### Compatibility\n\nTransitioning from the current UTXO model would be annoying, but not\nparticularly painful. I'll briefly describe my current preferred approach, but\nit makes sense to largely ignore this until there's been some discussion about\nUHS in general.\n\nA new service-bit should be allocated to indicate that a node is willing to\nserve blocks and transactions with dereferenced prevout data appended. Once\nconnected to a node advertising this feature, nodes would use a new getdata\nflag, creating MSG_PREVDATA_BLOCK and MSG_PREVDATA_TX.\n\nBecause current full nodes already have this data readily available in the\nblock-undo files, it is trivial to append on-the-fly. For that reason, it would\nbe easy to backport patches to the current stable version of Bitcoin Core in\norder to enable serving these blocks even before they could be consumed. This\nwould avoid an awkward bootstrapping phase where there may only be a few nodes\navailable to serve to all new nodes.\n\nAdmittedly I haven't put much thought into the on-disk format, I'd rather leave\nthat to a database person. Though it does seem like a reasonable excuse to\nconsider moving away from LevelDB.\n\nWallets would begin holding full prevout data for their unspent outputs, though\nthey could probably back-into the data as-is.\n\n### Serialization\n\nI would prefer to delay this discussion until a more high-level discussion has\nbeen had, otherwise this would be a magnet for nits. The format used to gather\nthe data above can be seen in the implementation below.\n\nIt should be noted, though, that the size of a UHS is directly dependent on the\nchosen hash function. Smaller hashes could potentially be used, but I believe\nthat to be unwise.\n\n### Drawbacks\n\nThe primary drawback of this approach is the ~5% network ovhead.\n\nAdditionally, there is the possibility that a few \"bridge nodes\" may be needed\nfor some time. In a future with a network of only pruned UHS nodes, an old\nwallet with no knowledge of its dereferenced prevout data would need to\nbroadcast an old-style transaction, and have a bridge node append the extra\ndata before forwarding it along the network.\n\nI won't speculate further there, except to say that I can't imagine a\ntransition problem that wouldn't have a straightforward solution.\n\nMigration issues aside, am I missing any obvious drawbacks?\n\n### Implementation\n\nThis code [2] was a quick hack-job, just enough to gather some initial data. It\nbuilds a UHS in memory and never flushes to disk. Only a single run works,\nnasty things will happen upon restart. It should only be viewed in order to get\nan idea of what changes are needed. Only enough for IBD is implemented,\nmempool/wallet/rpc are likely all broken. It is definitely not consensus-safe.\n\n### Acknowledgement\n\nI consider the UHS concept to be an evolution of Bram Cohen's TXO bitfield\nidea. Bram also provided invaluable input when initially walking through the\nfeasibility of a UHS.\n\nPieter Wuille's work on Rolling UTXO set hashes served as a catalyst for\nrethinking how the UTXO set may be represented.\n\nAdditional thanks to those at at Financial Crypto and the CoreDev event\nafterwards who helped to flesh out the idea:\n\nTadge Dryja\nGreg Sanders\nJohn Newbery\nNeha Narula\nJeremy Rubin\nJim Posen\n...and everyone else who has chimed in.\n\n\n[0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013928.html\n[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014337.html\n[2] https://github.com/theuni/bitcoin/tree/utxo-set-hash3"
            },
            {
                "author": "Matt Corallo",
                "date": "2018-05-17T15:28:28",
                "message_text_only": "Hey Cory,\n\nI'm generally a fan of having an option to \"prove a block is valid when\nrelaying it\" instead of \"just relay it\", but I am concerned that this\nproposal is overfitting the current UTXO set. Specifically, because UTXO\nentries are (roughly) 32 bytes per output plus 32 bytes per transaction\non disk today, a material increase in batching and many-output\ntransactions may significantly reduce the UTXO-set-size gain in this\nproposal while adding complexity to block relay as well as increase the\nsize of block data relayed, which can have adverse effects on\npropagation. I'd love to see your tests re-run on simulated transaction\ndata with more batching of sends.\n\nMatt\n\nOn 05/16/18 12:36, Cory Fields via bitcoin-dev wrote:\n> Tl;dr: Rather than storing all unspent outputs, store their hashes. Untrusted\n> peers can supply the full outputs when needed, with very little overhead.\n> Any attempt to spoof those outputs would be apparent, as their hashes would not\n> be present in the hash set. There are many advantages to this, most apparently\n> in disk and memory savings, as well as a validation speedup. The primary\n> disadvantage is a small increase in network traffic. I believe that the\n> advantages outweigh the disadvantages.\n> \n> --\n> \n> Bitcoin\u2019s unspent transaction output set (usually referred to as \u201cThe UTXO\n> set\u201d) has two primary roles: providing proof that previous outputs exist to be\n> spent, and providing the actual previous output data for verification when new\n> transactions attempts to spend them. These roles are not usually discussed\n> independently, but as Bram Cohen's TXO Bitfield [0] idea hints, there are\n> compelling reasons to consider them this way.\n> \n> To see why, consider running a node with the following changes:\n> \n> - For each new output, gather all extra data that will be needed for\n>   verification when spending it later as an input: the amount, scriptPubKey,\n>   creation height, coinbaseness, and output type (p2pkh, p2sh, p2wpkh, etc.).\n>   Call this the Dereferenced Prevout data.\n> - Create a hash from the concatenation of the new outpoint and the dereferenced\n>   prevout data. Call this a Unspent Transaction Output Hash.\n> - Rather than storing the full dereferenced prevout entries in a UTXO set as is\n>   currently done, instead store their hashes to an Unspent Transaction Output\n>   Hash Set, or UHS.\n> - When relaying a transaction, append the dereferenced prevout for each input.\n> \n> Now when a transaction is received, it contains everything needed for\n> verification, including the input amount, height, and coinbaseness, which would\n> have otherwise required a lookup the UTXO set.\n> \n> To verify an input's unspentness, again create a hash from the concatenation of\n> the referenced outpoint and the provided dereferenced prevout, and check for\n> its presence in the UHS. The hash will only be present if a hash of the exact\n> same data was previously added to (and not since removed from) the UHS. As\n> such, we are protected from a peer attempting to lie about the dereferenced\n> prevout data.\n> \n> ### Some benefits of the UHS model\n> \n> - Requires no consensus changes, purely a p2p/implementation change.\n> \n> - UHS is substantially smaller than a full UTXO set (just over half for the\n>   main chain, see below). In-memory caching can be much more effective as a\n>   result.\n> \n> - A block\u2019s transactions can be fully verified before doing a potentially\n>   expensive database lookup for the previous output data. The UHS can be\n>   queried afterwards (or in parallel) to verify previous output inclusion.\n> \n> - Entire blocks could potentially be verified out-of-order because all input\n>   data is provided; only the inclusion checks have to be in-order. Admittedly\n>   this is likely too complicated to be realistic.\n> \n> - pay-to-pubkey outputs are less burdensome on full nodes, since they use no\n>   more space on-disk than pay-to-pubkey-hash or pay-to-script-hash. Taproot and\n>   Graftroot outputs may share the same benefits.\n> \n> - The burden of holding UTXO data is technically shifted from the verifiers to\n>   the spender. In reality, full nodes will likely always have a copy as well,\n>   but conceptually it's a slight improvement to the incentive model.\n> \n> - Block data from peers can also be used to roll backwards during a reorg. This\n>   potentially enables an even more aggressive pruning mode.\n> \n> - UTXO storage size grows exactly linearly with UTXO count, as opposed to\n>   growing linearly with UTXO data size. This may be relevant when considering\n>   new larger output types which would otherwise cause the UTXO Set size to\n>   increase more quickly.\n> \n> - The UHS is a simple set, no need for a key-value database. LevelDB could\n>   potentially be dropped as a dependency in some distant future.\n> \n> - Potentially integrates nicely with Pieter Wuille's Rolling UTXO set hashes\n>   [1]. Unspent Transaction Output Hashes would simply be mapped to points on a\n>   curve before adding them to the set.\n> \n> - With the help of inclusion proofs and rolling hashes, libbitcoinconsensus\n>   could potentially safely verify entire blocks. The size of the required\n>   proofs would be largely irrelevant as they would be consumed locally.\n> \n> - Others?\n> \n> ### TxIn De-duplication\n> \n> Setting aside the potential benefits, the obvious drawback of using a UHS is a\n> significant network traffic increase. Fortunately, some properties of\n> transactions can be exploited to offset most of the difference.\n> \n> For quick reference:\n> \n> p2pkh scriptPubKey: DUP HASH160 [pubkey hash] EQUALVERIFY CHECKSIG\n> p2pkh scriptSig:    [signature] [pubkey]\n> \n> p2sh scriptPubKey:  HASH160 [script hash] EQUAL\n> p2sh scriptSig:     [signature(s)] [script]\n> \n> Notice that if a peer is sending a scriptPubKey and a scriptSig together, as\n> they would when using a UHS, there would likely be some redundancy. Using a\n> p2sh output for example, the scriptPubKey contains the script hash, and the\n> scriptSig contains the script itself. Therefore when sending dereferenced\n> prevouts over the wire, any hash which can be computed can be omitted and only\n> the preimages sent.\n> \n> Non-standard output scripts must be sent in-full, though because they account\n> for only ~1% of all current UTXOs, they are rare enough to be ignored here.\n> \n> ### Intra-block Script De-duplication\n> \n> When transactions are chained together in the same block, dereferenced prevout\n> data for these inputs would be redundant, as the full output data is already\n> present. For that reason, these dereferenced prevouts can be omitted when\n> sending over the wire.\n> \n> The downside would be a new reconstruction pass requirement prior to\n> validation.\n> \n> ### Data\n> \n> Here's some preliminary testing with a naive POC implementation patched into\n> Bitcoin Core. Note that the final sizes will depend on optimization of the\n> serialization format. The format used for these tests is suboptimal for sure.\n> Syncing mainnet to block 516346:\n> \n>                       UTXO Node      UHS Node\n>   IBD Network Data:   153G           157G\n>   Block disk space:   175G           157G\n>   UTXO disk space :   2.8G           1.6G\n>   Total disk space:   177.8G         158.6G\n> \n> The on-disk block-space reduction comes from the elimination of the Undo data\n> that Bitcoin Core uses to roll back orphaned blocks. For UHS Nodes, this data\n> is merged into to the block data and de-duplicated.\n> \n> Compared to the UXTO model, using a UHS reduces disk space by ~12%, yet only\n> requires ~5% more data over the wire.\n> \n> Experimentation shows intra-block de-duplication to be of little help in\n> practice, as it only reduces overhead by ~0.2% on mainnet. It could become more\n> useful if, for example, CPFP usage increases substantially in the future.\n> \n> ### Safety\n> \n> Assuming sha256 for the UHS's hash function, I don't believe any fundamental\n> changes to Bitcoin's security model are introduced. Because the unspent\n> transaction output hashes commit to all necessary data, including output types,\n> it should not be possible to be tricked into validating using mutated or forged\n> inputs.\n> \n> ### Compatibility\n> \n> Transitioning from the current UTXO model would be annoying, but not\n> particularly painful. I'll briefly describe my current preferred approach, but\n> it makes sense to largely ignore this until there's been some discussion about\n> UHS in general.\n> \n> A new service-bit should be allocated to indicate that a node is willing to\n> serve blocks and transactions with dereferenced prevout data appended. Once\n> connected to a node advertising this feature, nodes would use a new getdata\n> flag, creating MSG_PREVDATA_BLOCK and MSG_PREVDATA_TX.\n> \n> Because current full nodes already have this data readily available in the\n> block-undo files, it is trivial to append on-the-fly. For that reason, it would\n> be easy to backport patches to the current stable version of Bitcoin Core in\n> order to enable serving these blocks even before they could be consumed. This\n> would avoid an awkward bootstrapping phase where there may only be a few nodes\n> available to serve to all new nodes.\n> \n> Admittedly I haven't put much thought into the on-disk format, I'd rather leave\n> that to a database person. Though it does seem like a reasonable excuse to\n> consider moving away from LevelDB.\n> \n> Wallets would begin holding full prevout data for their unspent outputs, though\n> they could probably back-into the data as-is.\n> \n> ### Serialization\n> \n> I would prefer to delay this discussion until a more high-level discussion has\n> been had, otherwise this would be a magnet for nits. The format used to gather\n> the data above can be seen in the implementation below.\n> \n> It should be noted, though, that the size of a UHS is directly dependent on the\n> chosen hash function. Smaller hashes could potentially be used, but I believe\n> that to be unwise.\n> \n> ### Drawbacks\n> \n> The primary drawback of this approach is the ~5% network ovhead.\n> \n> Additionally, there is the possibility that a few \"bridge nodes\" may be needed\n> for some time. In a future with a network of only pruned UHS nodes, an old\n> wallet with no knowledge of its dereferenced prevout data would need to\n> broadcast an old-style transaction, and have a bridge node append the extra\n> data before forwarding it along the network.\n> \n> I won't speculate further there, except to say that I can't imagine a\n> transition problem that wouldn't have a straightforward solution.\n> \n> Migration issues aside, am I missing any obvious drawbacks?\n> \n> ### Implementation\n> \n> This code [2] was a quick hack-job, just enough to gather some initial data. It\n> builds a UHS in memory and never flushes to disk. Only a single run works,\n> nasty things will happen upon restart. It should only be viewed in order to get\n> an idea of what changes are needed. Only enough for IBD is implemented,\n> mempool/wallet/rpc are likely all broken. It is definitely not consensus-safe.\n> \n> ### Acknowledgement\n> \n> I consider the UHS concept to be an evolution of Bram Cohen's TXO bitfield\n> idea. Bram also provided invaluable input when initially walking through the\n> feasibility of a UHS.\n> \n> Pieter Wuille's work on Rolling UTXO set hashes served as a catalyst for\n> rethinking how the UTXO set may be represented.\n> \n> Additional thanks to those at at Financial Crypto and the CoreDev event\n> afterwards who helped to flesh out the idea:\n> \n> Tadge Dryja\n> Greg Sanders\n> John Newbery\n> Neha Narula\n> Jeremy Rubin\n> Jim Posen\n> ...and everyone else who has chimed in.\n> \n> \n> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013928.html\n> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014337.html\n> [2] https://github.com/theuni/bitcoin/tree/utxo-set-hash3\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-17T16:56:39",
                "message_text_only": "On Wed, May 16, 2018 at 4:36 PM, Cory Fields via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Tl;dr: Rather than storing all unspent outputs, store their hashes.\n\nMy initial thoughts are it's not _completely_ obvious to me that a 5%\nongoing bandwidth increase is actually a win to get something like a\n40% reduction in the size of a pruned node (and less than a 1%\nreduction in an archive node) primarily because I've not seen size of\na pruned node cited as a usage limiting factor basically anywhere. I\nwould assume it is a win but wouldn't be shocked to see a careful\nanalysis that concluded it wasn't.\n\nBut perhaps more interestingly, I think the overhead is not really 5%,\nbut it's 5% measured in the context of the phenomenally inefficient tx\nmechanisms ( https://bitcointalk.org/index.php?topic=1377345.0 ).\nNapkin math on the size of a txn alone tells me it's more like a 25%\nincrease if you just consider size of tx vs size of\ntx+scriptpubkeys,amounts.  If I'm not missing something there, I think\nthat would get in into a very clear not-win range.\n\nOn the positive side is that it doesn't change the blockchain\ndatastructure, so it's something implementations could do without\nmarrying the network to it forever."
            },
            {
                "author": "Cory Fields",
                "date": "2018-05-17T17:16:46",
                "message_text_only": "Matt: That's a good point. I'll do up a chart comparing utxo size at each\nblock, as well as comparing over-the-wire size for each block. I think the\nperiod of coalescing earlier this year should be a good example of what\nyou're describing.\n\nGreg: heh, I was wondering how long it would take for someone to point out\nthat I'm cheating. I avoided using the word \"compression\", mostly to\nside-step having the discussion turning into reworking the wire\nserialization. But you're absolutely right, the de-duping benefits are\nindependent of the UHS use-case.\n\nCory\n\nOn Thu, May 17, 2018, 12:56 PM Gregory Maxwell <greg at xiph.org> wrote:\n\n> On Wed, May 16, 2018 at 4:36 PM, Cory Fields via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Tl;dr: Rather than storing all unspent outputs, store their hashes.\n>\n> My initial thoughts are it's not _completely_ obvious to me that a 5%\n> ongoing bandwidth increase is actually a win to get something like a\n> 40% reduction in the size of a pruned node (and less than a 1%\n> reduction in an archive node) primarily because I've not seen size of\n> a pruned node cited as a usage limiting factor basically anywhere. I\n> would assume it is a win but wouldn't be shocked to see a careful\n> analysis that concluded it wasn't.\n>\n> But perhaps more interestingly, I think the overhead is not really 5%,\n> but it's 5% measured in the context of the phenomenally inefficient tx\n> mechanisms ( https://bitcointalk.org/index.php?topic=1377345.0 ).\n> Napkin math on the size of a txn alone tells me it's more like a 25%\n> increase if you just consider size of tx vs size of\n> tx+scriptpubkeys,amounts.  If I'm not missing something there, I think\n> that would get in into a very clear not-win range.\n>\n> On the positive side is that it doesn't change the blockchain\n> datastructure, so it's something implementations could do without\n> marrying the network to it forever.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/84d84610/attachment.html>"
            },
            {
                "author": "Alex Mizrahi",
                "date": "2018-05-18T15:42:00",
                "message_text_only": "You should read this:\nhttps://bitcointalk.org/index.php?topic=153662.10\n\nOn Wed, May 16, 2018 at 7:36 PM, Cory Fields via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Tl;dr: Rather than storing all unspent outputs, store their hashes.\n> Untrusted\n> peers can supply the full outputs when needed, with very little overhead.\n> Any attempt to spoof those outputs would be apparent, as their hashes\n> would not\n> be present in the hash set. There are many advantages to this, most\n> apparently\n> in disk and memory savings, as well as a validation speedup. The primary\n> disadvantage is a small increase in network traffic. I believe that the\n> advantages outweigh the disadvantages.\n>\n> --\n>\n> Bitcoin\u2019s unspent transaction output set (usually referred to as \u201cThe UTXO\n> set\u201d) has two primary roles: providing proof that previous outputs exist\n> to be\n> spent, and providing the actual previous output data for verification when\n> new\n> transactions attempts to spend them. These roles are not usually discussed\n> independently, but as Bram Cohen's TXO Bitfield [0] idea hints, there are\n> compelling reasons to consider them this way.\n>\n> To see why, consider running a node with the following changes:\n>\n> - For each new output, gather all extra data that will be needed for\n>   verification when spending it later as an input: the amount,\n> scriptPubKey,\n>   creation height, coinbaseness, and output type (p2pkh, p2sh, p2wpkh,\n> etc.).\n>   Call this the Dereferenced Prevout data.\n> - Create a hash from the concatenation of the new outpoint and the\n> dereferenced\n>   prevout data. Call this a Unspent Transaction Output Hash.\n> - Rather than storing the full dereferenced prevout entries in a UTXO set\n> as is\n>   currently done, instead store their hashes to an Unspent Transaction\n> Output\n>   Hash Set, or UHS.\n> - When relaying a transaction, append the dereferenced prevout for each\n> input.\n>\n> Now when a transaction is received, it contains everything needed for\n> verification, including the input amount, height, and coinbaseness, which\n> would\n> have otherwise required a lookup the UTXO set.\n>\n> To verify an input's unspentness, again create a hash from the\n> concatenation of\n> the referenced outpoint and the provided dereferenced prevout, and check\n> for\n> its presence in the UHS. The hash will only be present if a hash of the\n> exact\n> same data was previously added to (and not since removed from) the UHS. As\n> such, we are protected from a peer attempting to lie about the dereferenced\n> prevout data.\n>\n> ### Some benefits of the UHS model\n>\n> - Requires no consensus changes, purely a p2p/implementation change.\n>\n> - UHS is substantially smaller than a full UTXO set (just over half for the\n>   main chain, see below). In-memory caching can be much more effective as a\n>   result.\n>\n> - A block\u2019s transactions can be fully verified before doing a potentially\n>   expensive database lookup for the previous output data. The UHS can be\n>   queried afterwards (or in parallel) to verify previous output inclusion.\n>\n> - Entire blocks could potentially be verified out-of-order because all\n> input\n>   data is provided; only the inclusion checks have to be in-order.\n> Admittedly\n>   this is likely too complicated to be realistic.\n>\n> - pay-to-pubkey outputs are less burdensome on full nodes, since they use\n> no\n>   more space on-disk than pay-to-pubkey-hash or pay-to-script-hash.\n> Taproot and\n>   Graftroot outputs may share the same benefits.\n>\n> - The burden of holding UTXO data is technically shifted from the\n> verifiers to\n>   the spender. In reality, full nodes will likely always have a copy as\n> well,\n>   but conceptually it's a slight improvement to the incentive model.\n>\n> - Block data from peers can also be used to roll backwards during a reorg.\n> This\n>   potentially enables an even more aggressive pruning mode.\n>\n> - UTXO storage size grows exactly linearly with UTXO count, as opposed to\n>   growing linearly with UTXO data size. This may be relevant when\n> considering\n>   new larger output types which would otherwise cause the UTXO Set size to\n>   increase more quickly.\n>\n> - The UHS is a simple set, no need for a key-value database. LevelDB could\n>   potentially be dropped as a dependency in some distant future.\n>\n> - Potentially integrates nicely with Pieter Wuille's Rolling UTXO set\n> hashes\n>   [1]. Unspent Transaction Output Hashes would simply be mapped to points\n> on a\n>   curve before adding them to the set.\n>\n> - With the help of inclusion proofs and rolling hashes, libbitcoinconsensus\n>   could potentially safely verify entire blocks. The size of the required\n>   proofs would be largely irrelevant as they would be consumed locally.\n>\n> - Others?\n>\n> ### TxIn De-duplication\n>\n> Setting aside the potential benefits, the obvious drawback of using a UHS\n> is a\n> significant network traffic increase. Fortunately, some properties of\n> transactions can be exploited to offset most of the difference.\n>\n> For quick reference:\n>\n> p2pkh scriptPubKey: DUP HASH160 [pubkey hash] EQUALVERIFY CHECKSIG\n> p2pkh scriptSig:    [signature] [pubkey]\n>\n> p2sh scriptPubKey:  HASH160 [script hash] EQUAL\n> p2sh scriptSig:     [signature(s)] [script]\n>\n> Notice that if a peer is sending a scriptPubKey and a scriptSig together,\n> as\n> they would when using a UHS, there would likely be some redundancy. Using a\n> p2sh output for example, the scriptPubKey contains the script hash, and the\n> scriptSig contains the script itself. Therefore when sending dereferenced\n> prevouts over the wire, any hash which can be computed can be omitted and\n> only\n> the preimages sent.\n>\n> Non-standard output scripts must be sent in-full, though because they\n> account\n> for only ~1% of all current UTXOs, they are rare enough to be ignored here.\n>\n> ### Intra-block Script De-duplication\n>\n> When transactions are chained together in the same block, dereferenced\n> prevout\n> data for these inputs would be redundant, as the full output data is\n> already\n> present. For that reason, these dereferenced prevouts can be omitted when\n> sending over the wire.\n>\n> The downside would be a new reconstruction pass requirement prior to\n> validation.\n>\n> ### Data\n>\n> Here's some preliminary testing with a naive POC implementation patched\n> into\n> Bitcoin Core. Note that the final sizes will depend on optimization of the\n> serialization format. The format used for these tests is suboptimal for\n> sure.\n> Syncing mainnet to block 516346:\n>\n>                       UTXO Node      UHS Node\n>   IBD Network Data:   153G           157G\n>   Block disk space:   175G           157G\n>   UTXO disk space :   2.8G           1.6G\n>   Total disk space:   177.8G         158.6G\n>\n> The on-disk block-space reduction comes from the elimination of the Undo\n> data\n> that Bitcoin Core uses to roll back orphaned blocks. For UHS Nodes, this\n> data\n> is merged into to the block data and de-duplicated.\n>\n> Compared to the UXTO model, using a UHS reduces disk space by ~12%, yet\n> only\n> requires ~5% more data over the wire.\n>\n> Experimentation shows intra-block de-duplication to be of little help in\n> practice, as it only reduces overhead by ~0.2% on mainnet. It could become\n> more\n> useful if, for example, CPFP usage increases substantially in the future.\n>\n> ### Safety\n>\n> Assuming sha256 for the UHS's hash function, I don't believe any\n> fundamental\n> changes to Bitcoin's security model are introduced. Because the unspent\n> transaction output hashes commit to all necessary data, including output\n> types,\n> it should not be possible to be tricked into validating using mutated or\n> forged\n> inputs.\n>\n> ### Compatibility\n>\n> Transitioning from the current UTXO model would be annoying, but not\n> particularly painful. I'll briefly describe my current preferred approach,\n> but\n> it makes sense to largely ignore this until there's been some discussion\n> about\n> UHS in general.\n>\n> A new service-bit should be allocated to indicate that a node is willing to\n> serve blocks and transactions with dereferenced prevout data appended. Once\n> connected to a node advertising this feature, nodes would use a new getdata\n> flag, creating MSG_PREVDATA_BLOCK and MSG_PREVDATA_TX.\n>\n> Because current full nodes already have this data readily available in the\n> block-undo files, it is trivial to append on-the-fly. For that reason, it\n> would\n> be easy to backport patches to the current stable version of Bitcoin Core\n> in\n> order to enable serving these blocks even before they could be consumed.\n> This\n> would avoid an awkward bootstrapping phase where there may only be a few\n> nodes\n> available to serve to all new nodes.\n>\n> Admittedly I haven't put much thought into the on-disk format, I'd rather\n> leave\n> that to a database person. Though it does seem like a reasonable excuse to\n> consider moving away from LevelDB.\n>\n> Wallets would begin holding full prevout data for their unspent outputs,\n> though\n> they could probably back-into the data as-is.\n>\n> ### Serialization\n>\n> I would prefer to delay this discussion until a more high-level discussion\n> has\n> been had, otherwise this would be a magnet for nits. The format used to\n> gather\n> the data above can be seen in the implementation below.\n>\n> It should be noted, though, that the size of a UHS is directly dependent\n> on the\n> chosen hash function. Smaller hashes could potentially be used, but I\n> believe\n> that to be unwise.\n>\n> ### Drawbacks\n>\n> The primary drawback of this approach is the ~5% network ovhead.\n>\n> Additionally, there is the possibility that a few \"bridge nodes\" may be\n> needed\n> for some time. In a future with a network of only pruned UHS nodes, an old\n> wallet with no knowledge of its dereferenced prevout data would need to\n> broadcast an old-style transaction, and have a bridge node append the extra\n> data before forwarding it along the network.\n>\n> I won't speculate further there, except to say that I can't imagine a\n> transition problem that wouldn't have a straightforward solution.\n>\n> Migration issues aside, am I missing any obvious drawbacks?\n>\n> ### Implementation\n>\n> This code [2] was a quick hack-job, just enough to gather some initial\n> data. It\n> builds a UHS in memory and never flushes to disk. Only a single run works,\n> nasty things will happen upon restart. It should only be viewed in order\n> to get\n> an idea of what changes are needed. Only enough for IBD is implemented,\n> mempool/wallet/rpc are likely all broken. It is definitely not\n> consensus-safe.\n>\n> ### Acknowledgement\n>\n> I consider the UHS concept to be an evolution of Bram Cohen's TXO bitfield\n> idea. Bram also provided invaluable input when initially walking through\n> the\n> feasibility of a UHS.\n>\n> Pieter Wuille's work on Rolling UTXO set hashes served as a catalyst for\n> rethinking how the UTXO set may be represented.\n>\n> Additional thanks to those at at Financial Crypto and the CoreDev event\n> afterwards who helped to flesh out the idea:\n>\n> Tadge Dryja\n> Greg Sanders\n> John Newbery\n> Neha Narula\n> Jeremy Rubin\n> Jim Posen\n> ...and everyone else who has chimed in.\n>\n>\n> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2017-March/013928.html\n> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> 2017-May/014337.html\n> [2] https://github.com/theuni/bitcoin/tree/utxo-set-hash3\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180518/81546ac9/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "UHS: Full-node security without maintaining a full UTXO set",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Alex Mizrahi",
                "Gregory Maxwell",
                "Matt Corallo",
                "Cory Fields"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 37729
        }
    },
    {
        "title": "[bitcoin-dev] Moving away from BIP37, unsetting NODE_BLOOM",
        "thread_messages": [
            {
                "author": "Caius Cosades",
                "date": "2018-05-16T21:22:47",
                "message_text_only": "As previously discussed[0][1][2] on the mailing list, github issue commentary, and IRC channels, there's substantial reason to disable BIP37 in network nodes which are getting stronger as the size of the chain increases. BIP37 has significant denial of service issues which are unsolvable in the design, it introduces undue load on the bitcoin network  by default, and doesn't provide an acceptable amount of security and reliability to \"lightweight wallets\" as originally intended. \n\nBIP37 allows \"lightweight wallets\" to connect to nodes in the network, and request that they load, deseralize, and expensively apply an arbitrary bloom filter to their block files and mempool. This should never have been the role of nodes in the network, rather it should have been opt-in, or performed by a different piece of software entirely. The inability of the nodes to cache the responses or meaningfully rate limit them makes it detrimental to serve these requests. \n\nBIP37 was intended to have stronger privacy than it does in reality[3][4], where effectively any node that can capture `filterload` and `filteradd` responses can trivially de-anonymize an entire wallet that has connected irrespective of the amount of noise they add to their filters. The connected node lying by omission is undetectable by any wallet software, where they will be lead to believe that there are no matching responses; this is counter-able by further destroying privacy and loading down the network by having multiple peers simultaneously return filter results and hoping that at least one isn't lying. \n\nNODE_BLOOM has been implemented already which allows nodes to signal in their service message that they do, or do not support filtering. I suggest that in the next major release this is defaulted to 0, and any software relying on BIP37 move to using other filtering options, or another piece of dedicated software to serve the requests. Future releases of the reference software should remove BIP37 commands entirely. \n\n\n[0]: https://www.reddit.com/r/Bitcoin/comments/3hjak7/the_hard_work_of_core_devs_not_xt_makes_bitcoin/cu9xntf/?context=3\n[1]: https://github.com/bitcoin/bitcoin/issues/6578\n[2]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010535.html\n[3]: https://jonasnick.github.io/slides/2016-zurich-meetup.pdf\n[4]: https://eprint.iacr.org/2014/763.pdf"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2018-05-17T07:47:34",
                "message_text_only": "Hi Caius\n\nThanks for brining this up.\nAs far as it looks, one of the major SPV library does not yet respect the NODE_BLOOM service flag [1]. Unsure sure about others.\n\nIt think disabling NODE_BLOOM services by default in full node implementations makes sense as soon as BIP158 [2] (compact block filters) has been implemented and therefore NODE_COMPACT_FILTERS is signalled broadly. Unsure if it would make sense to even wait for block filter commitment softfork (probably no).\n\nThen, the question is, if there are alternatives for mempool filtering (display unconfirmed transactions) or if the protocol recommendation are to disable that by default or recommend to use centralised filtering technique via wallet provider infrastructure (privacy?!).\n\nI guess everyone here agrees that there are major privacy and load-distribution issues with BIP37, and, that it should be disabled in the long run.\nBut, due to the lack of alternatives, there is the risk of breaking existing SPV client models and thus pushing users to complete centralised validation-solutions (and even towards centralised key-storage), which, may result in making privacy and security even more worse.\n\nI personally miss a long term concept how to keep non expert users (or say light clients) close to the p2p protocol in a decentralized fashion. Unclear how decentralized fee estimations and \u201eincoming transactions\u201c (which is something users want even if it's a broken concept) are handled in the future.\n\n\u2014\n/jonas\n\n[1] https://github.com/bitcoinj/bitcoinj/pull/1212\n[2] https://github.com/bitcoin/bips/blob/master/bip-0158.mediawiki\n\n> Am 16.05.2018 um 23:22 schrieb Caius Cosades via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>:\n> \n> As previously discussed[0][1][2] on the mailing list, github issue commentary, and IRC channels, there's substantial reason to disable BIP37 in network nodes which are getting stronger as the size of the chain increases. BIP37 has significant denial of service issues which are unsolvable in the design, it introduces undue load on the bitcoin network  by default, and doesn't provide an acceptable amount of security and reliability to \"lightweight wallets\" as originally intended.\n> \n> BIP37 allows \"lightweight wallets\" to connect to nodes in the network, and request that they load, deseralize, and expensively apply an arbitrary bloom filter to their block files and mempool. This should never have been the role of nodes in the network, rather it should have been opt-in, or performed by a different piece of software entirely. The inability of the nodes to cache the responses or meaningfully rate limit them makes it detrimental to serve these requests.\n> \n> BIP37 was intended to have stronger privacy than it does in reality[3][4], where effectively any node that can capture `filterload` and `filteradd` responses can trivially de-anonymize an entire wallet that has connected irrespective of the amount of noise they add to their filters. The connected node lying by omission is undetectable by any wallet software, where they will be lead to believe that there are no matching responses; this is counter-able by further destroying privacy and loading down the network by having multiple peers simultaneously return filter results and hoping that at least one isn't lying.\n> \n> NODE_BLOOM has been implemented already which allows nodes to signal in their service message that they do, or do not support filtering. I suggest that in the next major release this is defaulted to 0, and any software relying on BIP37 move to using other filtering options, or another piece of dedicated software to serve the requests. Future releases of the reference software should remove BIP37 commands entirely.\n> \n> \n> [0]: https://www.reddit.com/r/Bitcoin/comments/3hjak7/the_hard_work_of_core_devs_not_xt_makes_bitcoin/cu9xntf/?context=3\n> [1]: https://github.com/bitcoin/bitcoin/issues/6578\n> [2]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-August/010535.html\n> [3]: https://jonasnick.github.io/slides/2016-zurich-meetup.pdf\n> [4]: https://eprint.iacr.org/2014/763.pdf\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/6053a286/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Moving away from BIP37, unsetting NODE_BLOOM",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jonas Schnelli",
                "Caius Cosades"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 6946
        }
    },
    {
        "title": "[bitcoin-dev] BIP 158 Flexibility and Filter Size",
        "thread_messages": [
            {
                "author": "Matt Corallo",
                "date": "2018-05-17T15:25:12",
                "message_text_only": "BIP 158 currently includes the following in the \"basic\" filter: 1)\ntxids, 2) output scripts, 3) input prevouts.\n\nI believe (1) could be skipped entirely - there is almost no reason why\nyou'd not be able to filter for, eg, the set of output scripts in a\ntransaction you know about and (2) and (3) may want to be split out -\nmany wallets may wish to just find transactions paying to them, as\ntransactions spending from their outputs should generally be things\nthey've created.\n\nIn general, I'm concerned about the size of the filters making existing\nSPV clients less willing to adopt BIP 158 instead of the existing bloom\nfilter garbage and would like to see a further exploration of ways to\nsplit out filters to make them less bandwidth intensive. Some further\nideas we should probably play with before finalizing moving forward is\nproviding filters for certain script templates, eg being able to only\nget outputs that are segwit version X or other similar ideas.\n\nMatt"
            },
            {
                "author": "Peter Todd",
                "date": "2018-05-17T15:43:15",
                "message_text_only": "On Thu, May 17, 2018 at 11:25:12AM -0400, Matt Corallo via bitcoin-dev wrote:\n> BIP 158 currently includes the following in the \"basic\" filter: 1)\n> txids, 2) output scripts, 3) input prevouts.\n> \n> I believe (1) could be skipped entirely - there is almost no reason why\n> you'd not be able to filter for, eg, the set of output scripts in a\n> transaction you know about and (2) and (3) may want to be split out -\n> many wallets may wish to just find transactions paying to them, as\n> transactions spending from their outputs should generally be things\n> they've created.\n\nSo I think we have two cases where wallets want to find txs spending from their\noutputs:\n\n1) Waiting for a confirmation\n2) Detecting theft\n\nThe former can be turned off once there are no expected unconfirmed\ntransactions.\n\nAs for the latter, this is probably a valuable thing for wallets to do. Modulo\nreorgs, reducing the frequency that you check for stolen funds doesn't decrease\ntotal bandwidth cost - it's one filter match per block regardless - but perhaps\nthe real-world bandwidth cost can be reduced by, say, waiting for a wifi\nconnection rather than using cellular data.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/7732ad90/attachment-0001.sig>"
            },
            {
                "author": "Matt Corallo",
                "date": "2018-05-17T15:46:18",
                "message_text_only": "(1) can be accomplished by filtering for the set of outputs in the transaction you created. I agree (2) would ideally be done to avoid issues with a copied wallet (theft or not), but I am worried about the size of the filters themselves, not just the size of the blocks downloaded after a match.\n\nOn May 17, 2018 3:43:15 PM UTC, Peter Todd <pete at petertodd.org> wrote:\n>On Thu, May 17, 2018 at 11:25:12AM -0400, Matt Corallo via bitcoin-dev\n>wrote:\n>> BIP 158 currently includes the following in the \"basic\" filter: 1)\n>> txids, 2) output scripts, 3) input prevouts.\n>> \n>> I believe (1) could be skipped entirely - there is almost no reason\n>why\n>> you'd not be able to filter for, eg, the set of output scripts in a\n>> transaction you know about and (2) and (3) may want to be split out -\n>> many wallets may wish to just find transactions paying to them, as\n>> transactions spending from their outputs should generally be things\n>> they've created.\n>\n>So I think we have two cases where wallets want to find txs spending\n>from their\n>outputs:\n>\n>1) Waiting for a confirmation\n>2) Detecting theft\n>\n>The former can be turned off once there are no expected unconfirmed\n>transactions.\n>\n>As for the latter, this is probably a valuable thing for wallets to do.\n>Modulo\n>reorgs, reducing the frequency that you check for stolen funds doesn't\n>decrease\n>total bandwidth cost - it's one filter match per block regardless - but\n>perhaps\n>the real-world bandwidth cost can be reduced by, say, waiting for a\n>wifi\n>connection rather than using cellular data.\n>\n>-- \n>https://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/d64f3e3b/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-17T16:36:37",
                "message_text_only": "On Thu, May 17, 2018 at 3:25 PM, Matt Corallo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I believe (1) could be skipped entirely - there is almost no reason why\n> you'd not be able to filter for, eg, the set of output scripts in a\n> transaction you know about\n\nI think this is convincing for the txids themselves.\n\nWhat about also making input prevouts filter based on the scriptpubkey\nbeing _spent_?  Layering wise in the processing it's a bit ugly, but\nif you validated the block you have the data needed.\n\nThis would eliminate the multiple data type mixing entirely."
            },
            {
                "author": "Matt Corallo",
                "date": "2018-05-17T16:59:22",
                "message_text_only": "Yea I generally would really prefer something like that but it\nsignificantly complicates the download logic - currently clients can\neasily cross-check a filter in case they differ between providers by\ndownloading the block. If we instead went with the script being spent\nthey would have to be provided all previous transactions (potentially\ncompressed via midstate) as well, making it potentially infeasible to\nidentify the offending node while remaining a lightweight client. Maybe\nthere is some other reasonable download logic to replace it with, however.\n\nMatt\n\nOn 05/17/18 12:36, Gregory Maxwell wrote:\n> On Thu, May 17, 2018 at 3:25 PM, Matt Corallo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> I believe (1) could be skipped entirely - there is almost no reason why\n>> you'd not be able to filter for, eg, the set of output scripts in a\n>> transaction you know about\n> \n> I think this is convincing for the txids themselves.\n> \n> What about also making input prevouts filter based on the scriptpubkey\n> being _spent_?  Layering wise in the processing it's a bit ugly, but\n> if you validated the block you have the data needed.\n> \n> This would eliminate the multiple data type mixing entirely.\n>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-17T18:34:45",
                "message_text_only": "On Thu, May 17, 2018 at 4:59 PM, Matt Corallo <lf-lists at mattcorallo.com> wrote:\n> Yea I generally would really prefer something like that but it\n> significantly complicates the download logic - currently clients can\n> easily cross-check [...] Maybe\n> there is some other reasonable download logic to replace it with, however.\n\nI think lite clients cross checking is something which very likely\nwill never be implemented by anyone, and probably not stay working\n(due to under-usage) if it is implemented.  This thought is driven by\nthree things  (1) the bandwidth overhead of performing the check, (2)\nthinking about the network-interacting-state-machine complexity of it,\nand by the multitude of sanity checks that lite clients already don't\nimplement (e.g. when a lite client noticed a split tip it could ask\npeers for the respective blocks and check at least the stateless\nchecks, but none has ever done that), and...\n\n(3) that kind of checking would be moot if the filters were committed\nand validated... and the commitment would be both much simpler to\ncheck for lite clients and provide much stronger protection against\nmalicious peers.\n\nMy expectation is that eventually one of these filter-map designs\nwould become committed-- not after we already had it deployed and had\nworked out the design to the n-th generation (just as your proposed\nrevisions are doing to the initial proposal), but eventually.\n\nAlso, even without this change clients can still do that \"are multiple\npeers telling me the same thing or different things\" kind of checking,\nwhich I expect is the strongest testing we'd actually see them\nimplement absent a commitment."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-17T18:34:45",
                "message_text_only": "On Thu, May 17, 2018 at 4:59 PM, Matt Corallo <lf-lists at mattcorallo.com> wrote:\n> Yea I generally would really prefer something like that but it\n> significantly complicates the download logic - currently clients can\n> easily cross-check [...] Maybe\n> there is some other reasonable download logic to replace it with, however.\n\nI think lite clients cross checking is something which very likely\nwill never be implemented by anyone, and probably not stay working\n(due to under-usage) if it is implemented.  This thought is driven by\nthree things  (1) the bandwidth overhead of performing the check, (2)\nthinking about the network-interacting-state-machine complexity of it,\nand by the multitude of sanity checks that lite clients already don't\nimplement (e.g. when a lite client noticed a split tip it could ask\npeers for the respective blocks and check at least the stateless\nchecks, but none has ever done that), and...\n\n(3) that kind of checking would be moot if the filters were committed\nand validated... and the commitment would be both much simpler to\ncheck for lite clients and provide much stronger protection against\nmalicious peers.\n\nMy expectation is that eventually one of these filter-map designs\nwould become committed-- not after we already had it deployed and had\nworked out the design to the n-th generation (just as your proposed\nrevisions are doing to the initial proposal), but eventually.\n\nAlso, even without this change clients can still do that \"are multiple\npeers telling me the same thing or different things\" kind of checking,\nwhich I expect is the strongest testing we'd actually see them\nimplement absent a commitment."
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-17T20:19:17",
                "message_text_only": ">\n> I think lite clients cross checking is something which very likely\n> will never be implemented by anyone, and probably not stay working\n> (due to under-usage) if it is implemented.  This thought is driven by\n> three things  (1) the bandwidth overhead of performing the check, (2)\n> thinking about the network-interacting-state-machine complexity of it,\n> and by the multitude of sanity checks that lite clients already don't\n> implement (e.g. when a lite client noticed a split tip it could ask\n> peers for the respective blocks and check at least the stateless\n> checks, but none has ever done that), and...\n>\n\nIn my opinion, it's overly pessimistic to design the protocol in an\ninsecure way because some light clients historically have taken shortcuts.\nIf the protocol can provide clients the option of getting additional\nsecurity, it should.\n\nOn the general topic, Peter makes a good point that in many cases filtering\nby txid of spending transaction may be preferable to filtering by outpoint\nspend, which has the nice benefit that there are obviously fewer txs in a\nblock than txins. This wouldn't work for malleable transactions though.\n\nI'm open to the idea of splitting the basic filter into three separate\nfilters based on data type, but there are some bandwidth concerns. First,\nthe GCS encoding gets better compression with a greater number of elements,\nthough as I recall in my analysis, that starts to tail off at ~1000\nelements per filter with P=20, in which case it's not as much of a concern\ngiven current block sizes. The other is that clients need to download\nand/or store the filter header chain for each filter type, which are 32\nbytes each per block. So if a client is expected to download all three\nfilter types anyway, or even two of three, it's less efficient in these\nterms. It would be possible though to split the filters themselves, but\nstill have the basic filter header cover all three filters. This would mean\nthat full nodes could not support just a subset of the basic filters --\nthey'd have to compute all of them to compute the filter header.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/c25d9cf4/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-17T20:45:33",
                "message_text_only": "On Thu, May 17, 2018 at 8:19 PM, Jim Posen <jim.posen at gmail.com> wrote:\n> In my opinion, it's overly pessimistic to design the protocol in an insecure\n> way because some light clients historically have taken shortcuts.\n\nAny non-commited form is inherently insecure.  A nearby network\nattacker (or eclipse attacker) or whatnot can moot whatever kind of\ncomparisons you make, and non-comparison based validation doesn't seem\nlike it would be useful without mooting all the bandwidth improvements\nunless I'm missing something.\n\nIt isn't a question of 'some lite clients' -- I am aware of no\nimplementation of these kinds of measures in any cryptocurrency ever.\n\nThe same kind of comparison to the block could have been done with\nBIP37 filtering, but no one has implemented that. (similarly, the\nwhitepaper suggests doing that for all network rules when a\ndisagreement has been seen, though that isn't practical for all\nnetwork rules it could be done for many of them-- but again no\nimplementation or AFAIK any interest in implementing that)\n\n> If the\n> protocol can provide clients the option of getting additional security, it\n> should.\n\nSure, but at what cost?   And \"additional\" while nice doesn't\nnecessarily translate into a meaningful increase in delivered security\nfor any particular application.\n\nI think we might be speaking too generally here.\n\nWhat I'm suggesting would still allow a lite client to verify that\nmultiple parties are offering the same map for a given block (by\nasking them for the map hash). It would still allow a future\ncommitment so that lite client could verify that the hashpower they're\nhearing from agrees that the map they got is the correct corresponding\nmap for the block. It would still allow downloading a block and\nverifying that all the outpoints in the block were included.  So still\na lot better than BIP37.\n\nWhat it would not permit is for a lite client to download a whole\nblock and completely verify the filter (they could only tell if the\nfilter at least told them about all the outputs in the block, but if\nextra bits were set or inputs were omitted, they couldn't tell).\n\nBut in exchange the filters for a given FP rate would be probably\nabout half the current size (actual measurements would be needed\nbecause the figure depends on much scriptpubkey reuse there is, it\nprobably could be anywhere between 1/3 and 2/3rd).  In some\napplications it would likely have better anonymity properties as well,\nbecause a client that always filters for both an output and and input\nas distinct items (and then leaks matches by fetching blocks) is more\ndistinguishable.\n\nI think this trade-off is at leat worth considering because if you\nalways verify by downloading you wash out the bandwidth gains, strong\nverification will eventually need a commitment in any case.  A client\ncan still partially verify, and can still multi-party comparison\nverify.  ... and a big reduction in filter bandwidth\n\nMonitoring inputs by scriptPubkey vs input-txid also has a massive\nadvantage for parallel filtering:  You can usually known your pubkeys\nwell in advance, but if you have to change what you're watching block\n N+1 for based on the txids that paid you in N you can't filter them\nin parallel.\n\n> On the general topic, Peter makes a good point that in many cases filtering\n> by txid of spending transaction may be preferable to filtering by outpoint\n> spend, which has the nice benefit that there are obviously fewer txs in a\n> block than txins. This wouldn't work for malleable transactions though.\n\nI think Peter missed Matt's point that you can monitor for a specific\ntransaction's confirmation by monitoring for any of the outpoints that\ntransaction contains. Because the txid commits to the outpoints there\nshouldn't be any case where the txid is knowable but (an) outpoint is\nnot.  Removal of the txid and monitoring for any one of the outputs\nshould be a strict reduction in the false positive rate for a given\nfilter size (the filter will contain strictly fewer elements and the\nclient will match for the same (or usually, fewer) number).\n\nI _think_ dropping txids as matt suggests is an obvious win that costs\nnothing.  Replacing inputs with scripts as I suggested has some\ntrade-offs."
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-17T21:27:15",
                "message_text_only": ">\n> It isn't a question of 'some lite clients' -- I am aware of no\n> implementation of these kinds of measures in any cryptocurrency ever.\n>\n\nDoesn't mean there can't or shouldn't be a first. :-)\n\n\n> The same kind of comparison to the block could have been done with\n> BIP37 filtering, but no one has implemented that. (similarly, the\n> whitepaper suggests doing that for all network rules when a\n> disagreement has been seen, though that isn't practical for all\n> network rules it could be done for many of them-- but again no\n> implementation or AFAIK any interest in implementing that)\n\n\nCorrect me if I'm wrong, but I don't think it's true that the same could be\ndone for BIP 37. With BIP 37, one would have to download every partial\nblock from every peer to determine if there is a difference between them.\nWith BIP 157, you only download a 32 byte filter header from every peer\n(because filters are deterministic), and using that commitment can\ndetermine whether there's a conflict requiring further interrogation. The\ndifference in overhead makes checking for conflicts with BIP 157 practical,\nwhereas it's not as practical with BIP 37.\n\n\n> Sure, but at what cost?   And \"additional\" while nice doesn't\n> necessarily translate into a meaningful increase in delivered security\n> for any particular application.\n>\n> I think we might be speaking too generally here.\n>\n\nSure. The security model that BIP 157 now allows is that a light client with*\nat least one honest peer serving filters* can get the correct information\nabout the chain. No, this does not prevent against total eclipse attacks,\nbut I think it's a much stronger security guarantee than requiring all\npeers or even a majority of peers to be honest. In a decentralized network\nthat stores money, I think there's a big difference between those security\nmodels.\n\n\n> But in exchange the filters for a given FP rate would be probably\n> about half the current size (actual measurements would be needed\n> because the figure depends on much scriptpubkey reuse there is, it\n> probably could be anywhere between 1/3 and 2/3rd).\n>\n\nThis does not seem right. Let's assume txids are removed because they are\nnot relevant to this particular point. The difference as I understand it is\nwhether to include in the filter serialized outpoints for inputs or\nserialized prev scriptPubkeys for inputs. When hashed these are the same\nsize, and there's an equal number of them (one per input in a block). So\nthe only savings comes from deduping the prev scriptPubkeys with each other\nand with the scriptPubkeys in the block's outputs. So it comes down\nentirely to how much address reuse there is on the chain.\n\n\n> Monitoring inputs by scriptPubkey vs input-txid also has a massive\n> advantage for parallel filtering:  You can usually known your pubkeys\n> well in advance, but if you have to change what you're watching block\n>  N+1 for based on the txids that paid you in N you can't filter them\n> in parallel.\n>\n\nYes, I'll grant that this is a benefit of your suggestion.\n\n\n> I think Peter missed Matt's point that you can monitor for a specific\n> transaction's confirmation by monitoring for any of the outpoints that\n> transaction contains. Because the txid commits to the outpoints there\n> shouldn't be any case where the txid is knowable but (an) outpoint is\n> not.  Removal of the txid and monitoring for any one of the outputs\n> should be a strict reduction in the false positive rate for a given\n> filter size (the filter will contain strictly fewer elements and the\n> client will match for the same (or usually, fewer) number).\n>\n> I _think_ dropping txids as matt suggests is an obvious win that costs\n> nothing.  Replacing inputs with scripts as I suggested has some\n> trade-offs.\n>\n\nI may have interpreted this differently. So wallets need a way to know when\nthe transactions they send get confirmed (for obvious usability reasons and\nso for automatic fee-bumping). One way is to match the spent outpoints\nagainst the filter, which I think of as the standard. Another would be to\nmatch the txid of the spending transaction against the first, which only\nworks if the transaction is not malleable. Another would be to match the\nchange output script against the first, assuming the wallet does not reuse\nchange addresses and that the spending transaction does in fact have a\nchange output.\n\nNow lets say these pieces of data, txids, output scripts, and spent\noutpoints are in three separate filters that a wallet can download\nseparately or choose not to download. The spent outpoint method is the most\nreliable and has no caviats. It also allows for theft detection as Peter\nnotes, which is a very nice property indeed. If the wallet uses the txid\nmatching though, the txid filter would be smaller because there are fewer\ntxids per block than inputs. So there could be some bandwidth savings to\nthat approach. The change output watching is probably the nicest in some\nways because the client needs the output filter anyway. If the transaction\nhas no change output with a unique script, the client could watch for any\nof the other outputs on the spending tx, but may get more false positives\ndepending on the degree of address reuse.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/01b8298f/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-19T03:12:09",
                "message_text_only": "On Thu, May 17, 2018 at 2:44 PM Jim Posen via bitcoin-dev <bitcoin-\n\n> Monitoring inputs by scriptPubkey vs input-txid also has a massive\n>> advantage for parallel filtering:  You can usually known your pubkeys\n>> well in advance, but if you have to change what you're watching block\n>>  N+1 for based on the txids that paid you in N you can't filter them\n>> in parallel.\n>>\n>\n> Yes, I'll grant that this is a benefit of your suggestion.\n>\n\nYeah parallel filtering would be pretty nice. We've implemented a serial\nfiltering for btcwallet [1] for the use-case of rescanning after a seed\nphrase import. Parallel filtering would help here, but also we don't yet\ntake advantage of batch querying for the filters themselves. This would\nspeed up the scanning by quite a bit.\n\nI really like the filtering model though, it really simplifies the code,\nand we can leverage identical logic for btcd (which has RPCs to fetch the\nfilters) as well.\n\n[1]:\nhttps://github.com/Roasbeef/btcwallet/blob/master/chain/neutrino.go#L180\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180518/ead286a4/attachment.html>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2018-05-21T08:35:28",
                "message_text_only": "Hi all,\nMost light wallets will want to download the minimum amount of data required to operate, which means they would ideally download the smallest possible filters containing the subset of elements they need.\nWhat if instead of trying to decide up front which subset of elements will be most useful to include in the filters, and the size tradeoff, we let the full-node decide which subsets of elements it serves filters for?\n\nFor instance, a full node would advertise that it could serve filters for the subsets 110 (txid+script+outpoint), 100 (txid only), 011 ( script+outpoint) etc. A light client could then choose to download the minimal filter type covering its needs.\nThe obvious benefit of this would be minimal bandwidth usage for the light client, but there are also some less obvious ones. We wouldn\u2019t have to decide up front what each filter type should contain, only the possible elements a filter can contain (more can be added later without breaking existing clients). This, I think, would let the most served filter types grow organically, with full-node implementations coming with sane defaults for served filter types (maybe even all possible types as long as the number of elements is small), letting their operator add/remove types at will.\nThe main disadvantage of this as I see it, is that there\u2019s an exponential blowup in the number of possible filter types in the number of element types. However, this would let us start out small with only the elements we need, and in the worst case the node operators just choose to serve the subsets corresponding to what now is called \u201cregular\u201d + \u201cextended\u201d filters anyway, requiring no more resources.\nThis would also give us some data on what is the most widely used filter types, which could be useful in making the decision on what should be part of filters to eventually commit to in blocks.\n- Johan On Sat, May 19, 2018 at 5:12, Olaoluwa Osuntokun via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nOn Thu, May 17, 2018 at 2:44 PM Jim Posen via bitcoin-dev <bitcoin- Monitoring inputs by scriptPubkey vs input-txid also has a massive\nadvantage for parallel filtering: You can usually known your pubkeys\nwell in advance, but if you have to change what you're watching block\nN+1 for based on the txids that paid you in N you can't filter them\nin parallel.\n\nYes, I'll grant that this is a benefit of your suggestion.\nYeah parallel filtering would be pretty nice. We've implemented a serial filtering for btcwallet [1] for the use-case of rescanning after a seed phrase import. Parallel filtering would help here, but also we don't yet take advantage of batch querying for the filters themselves. This would speed up the scanning by quite a bit.\nI really like the filtering model though, it really simplifies the code, and we can leverage identical logic for btcd (which has RPCs to fetch the filters) as well.\n[1]: https://github.com/Roasbeef/btcwallet/blob/master/chain/neutrino.go#L180 [https://github.com/Roasbeef/btcwallet/blob/master/chain/neutrino.go#L180]\n_______________________________________________ bitcoin-dev mailing list bitcoin-dev at lists.linuxfoundation.org https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180521/d4626f97/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-22T01:16:52",
                "message_text_only": "> What if instead of trying to decide up front which subset of elements will\n> be most useful to include in the filters, and the size tradeoff, we let\nthe\n> full-node decide which subsets of elements it serves filters for?\n\nThis is already the case. The current \"track\" is to add new service bits\n(while we're in the uncommitted phase) to introduce new fitler types. Light\nclients can then filter out nodes before even connecting to them.\n\n-- Laolu\n\nOn Mon, May 21, 2018 at 1:35 AM Johan Tor\u00e5s Halseth <johanth at gmail.com>\nwrote:\n\n> Hi all,\n>\n> Most light wallets will want to download the minimum amount of data\n> required to operate, which means they would ideally download the smallest\n> possible filters containing the subset of elements they need.\n>\n> What if instead of trying to decide up front which subset of elements will\n> be most useful to include in the filters, and the size tradeoff, we let the\n> full-node decide which subsets of elements it serves filters for?\n>\n> For instance, a full node would advertise that it could serve filters for\n> the subsets 110 (txid+script+outpoint), 100 (txid only), 011 (script+outpoint)\n> etc. A light client could then choose to download the minimal filter type\n> covering its needs.\n>\n> The obvious benefit of this would be minimal bandwidth usage for the light\n> client, but there are also some less obvious ones. We wouldn\u2019t have to\n> decide up front what each filter type should contain, only the possible\n> elements a filter can contain (more can be added later without breaking\n> existing clients). This, I think, would let the most served filter types\n> grow organically, with full-node implementations coming with sane defaults\n> for served filter types (maybe even all possible types as long as the\n> number of elements is small), letting their operator add/remove types at\n> will.\n>\n> The main disadvantage of this as I see it, is that there\u2019s an exponential\n> blowup in the number of possible filter types in the number of element\n> types. However, this would let us start out small with only the elements we\n> need, and in the worst case the node operators just choose to serve the\n> subsets corresponding to what now is called \u201cregular\u201d + \u201cextended\u201d filters\n> anyway, requiring no more resources.\n>\n> This would also give us some data on what is the most widely used filter\n> types, which could be useful in making the decision on what should be part\n> of filters to eventually commit to in blocks.\n>\n> - Johan\n> On Sat, May 19, 2018 at 5:12, Olaoluwa Osuntokun via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> On Thu, May 17, 2018 at 2:44 PM Jim Posen via bitcoin-dev <bitcoin-\n>\n>> Monitoring inputs by scriptPubkey vs input-txid also has a massive\n>>> advantage for parallel filtering: You can usually known your pubkeys\n>>> well in advance, but if you have to change what you're watching block\n>>> N+1 for based on the txids that paid you in N you can't filter them\n>>> in parallel.\n>>>\n>>\n>> Yes, I'll grant that this is a benefit of your suggestion.\n>>\n>\n> Yeah parallel filtering would be pretty nice. We've implemented a serial\n> filtering for btcwallet [1] for the use-case of rescanning after a seed\n> phrase import. Parallel filtering would help here, but also we don't yet\n> take advantage of batch querying for the filters themselves. This would\n> speed up the scanning by quite a bit.\n>\n> I really like the filtering model though, it really simplifies the code,\n> and we can leverage identical logic for btcd (which has RPCs to fetch the\n> filters) as well.\n>\n> [1]:\n> https://github.com/Roasbeef/btcwallet/blob/master/chain/neutrino.go#L180\n>\n> _______________________________________________ bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180521/92b184b8/attachment.html>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2018-05-22T09:23:29",
                "message_text_only": "Maybe I didn't make it clear, but the distinction is that the current track\nallocates\none service bit for each \"filter type\", where it has to be agreed upon up\nfront what\nelements such a filter type contains.\n\nMy suggestion was to advertise a bitfield for each filter type the node\nserves,\nwhere the bitfield indicates what elements are part of the filters. This\nessentially\nremoves the notion of decided filter types and instead leaves the decision\nto\nfull-nodes.\n\nThis would require a \"getcftypes\" message, of course.\n\n- Johan\n\n\nOn Tue, May 22, 2018 at 3:16 AM, Olaoluwa Osuntokun <laolu32 at gmail.com>\nwrote:\n\n> > What if instead of trying to decide up front which subset of elements\n> will\n> > be most useful to include in the filters, and the size tradeoff, we let\n> the\n> > full-node decide which subsets of elements it serves filters for?\n>\n> This is already the case. The current \"track\" is to add new service bits\n> (while we're in the uncommitted phase) to introduce new fitler types. Light\n> clients can then filter out nodes before even connecting to them.\n>\n> -- Laolu\n>\n> On Mon, May 21, 2018 at 1:35 AM Johan Tor\u00e5s Halseth <johanth at gmail.com>\n> wrote:\n>\n>> Hi all,\n>>\n>> Most light wallets will want to download the minimum amount of data\n>> required to operate, which means they would ideally download the smallest\n>> possible filters containing the subset of elements they need.\n>>\n>> What if instead of trying to decide up front which subset of elements\n>> will be most useful to include in the filters, and the size tradeoff, we\n>> let the full-node decide which subsets of elements it serves filters for?\n>>\n>> For instance, a full node would advertise that it could serve filters for\n>> the subsets 110 (txid+script+outpoint), 100 (txid only), 011 (script+outpoint)\n>> etc. A light client could then choose to download the minimal filter type\n>> covering its needs.\n>>\n>> The obvious benefit of this would be minimal bandwidth usage for the\n>> light client, but there are also some less obvious ones. We wouldn\u2019t have\n>> to decide up front what each filter type should contain, only the possible\n>> elements a filter can contain (more can be added later without breaking\n>> existing clients). This, I think, would let the most served filter types\n>> grow organically, with full-node implementations coming with sane defaults\n>> for served filter types (maybe even all possible types as long as the\n>> number of elements is small), letting their operator add/remove types at\n>> will.\n>>\n>> The main disadvantage of this as I see it, is that there\u2019s an exponential\n>> blowup in the number of possible filter types in the number of element\n>> types. However, this would let us start out small with only the elements we\n>> need, and in the worst case the node operators just choose to serve the\n>> subsets corresponding to what now is called \u201cregular\u201d + \u201cextended\u201d filters\n>> anyway, requiring no more resources.\n>>\n>> This would also give us some data on what is the most widely used filter\n>> types, which could be useful in making the decision on what should be part\n>> of filters to eventually commit to in blocks.\n>>\n>> - Johan\n>> On Sat, May 19, 2018 at 5:12, Olaoluwa Osuntokun via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> On Thu, May 17, 2018 at 2:44 PM Jim Posen via bitcoin-dev <bitcoin-\n>>\n>>> Monitoring inputs by scriptPubkey vs input-txid also has a massive\n>>>> advantage for parallel filtering: You can usually known your pubkeys\n>>>> well in advance, but if you have to change what you're watching block\n>>>> N+1 for based on the txids that paid you in N you can't filter them\n>>>> in parallel.\n>>>>\n>>>\n>>> Yes, I'll grant that this is a benefit of your suggestion.\n>>>\n>>\n>> Yeah parallel filtering would be pretty nice. We've implemented a serial\n>> filtering for btcwallet [1] for the use-case of rescanning after a seed\n>> phrase import. Parallel filtering would help here, but also we don't yet\n>> take advantage of batch querying for the filters themselves. This would\n>> speed up the scanning by quite a bit.\n>>\n>> I really like the filtering model though, it really simplifies the code,\n>> and we can leverage identical logic for btcd (which has RPCs to fetch the\n>> filters) as well.\n>>\n>> [1]: https://github.com/Roasbeef/btcwallet/blob/master/chain/\n>> neutrino.go#L180\n>>\n>> _______________________________________________ bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org https://lists.linuxfoundation.\n>> org/mailman/listinfo/bitcoin-dev\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180522/cdeb1da6/attachment.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-23T00:42:29",
                "message_text_only": ">\n> My suggestion was to advertise a bitfield for each filter type the node\n> serves,\n> where the bitfield indicates what elements are part of the filters. This\n> essentially\n> removes the notion of decided filter types and instead leaves the decision\n> to\n> full-nodes.\n>\n\nI think it makes more sense to construct entirely separate filters for the\ndifferent types of elements and allow clients to download only the ones\nthey care about. If there are enough elements per filter, the compression\nratio shouldn't be much worse by splitting them up. This prevents the\nexponential blowup in the number of filters that you mention, Johan, and it\nworks nicely with service bits for advertising different filter types\nindependently.\n\nSo if we created three separate filter types, one for output scripts, one\nfor input outpoints, and one for TXIDs, each signaled with a separate\nservice bit, are people good with that? Or do you think there shouldn't be\na TXID filter at all, Matt? I didn't include the option of a prev output\nscript filter or rolling that into the block output script filter because\nit changes the security model (cannot be proven to be correct/incorrect\nsuccinctly).\n\nThen there's the question of whether to separate or combine the headers.\nI'd lean towards keeping them separate because it's simpler that way.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180522/328a953f/attachment-0001.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-23T07:38:40",
                "message_text_only": "So I checked filter sizes (as a proportion of block size) for each of the\nsub-filters. The graph is attached.\n\nAs interpretation, the first ~120,000 blocks are so small that the\nGolomb-Rice coding can't compress the filters that well, which is why the\nfilter sizes are so high proportional to the block size. Except for the\ninput filter, because the coinbase input is skipped, so many of them have 0\nelements. But after block 120,000 or so, the filter compression converges\npretty quickly to near the optimal value. The encouraging thing here is\nthat if you look at the ratio of the combined size of the separated filters\nvs the size of a filter containing all of them (currently known as the\nbasic filter), they are pretty much the same size. The mean of the ratio\nbetween them after block 150,000 is 99.4%. So basically, not much\ncompression efficiently is lost by separating the basic filter into\nsub-filters.\n\nOn Tue, May 22, 2018 at 5:42 PM, Jim Posen <jim.posen at gmail.com> wrote:\n\n> My suggestion was to advertise a bitfield for each filter type the node\n>> serves,\n>> where the bitfield indicates what elements are part of the filters. This\n>> essentially\n>> removes the notion of decided filter types and instead leaves the\n>> decision to\n>> full-nodes.\n>>\n>\n> I think it makes more sense to construct entirely separate filters for the\n> different types of elements and allow clients to download only the ones\n> they care about. If there are enough elements per filter, the compression\n> ratio shouldn't be much worse by splitting them up. This prevents the\n> exponential blowup in the number of filters that you mention, Johan, and it\n> works nicely with service bits for advertising different filter types\n> independently.\n>\n> So if we created three separate filter types, one for output scripts, one\n> for input outpoints, and one for TXIDs, each signaled with a separate\n> service bit, are people good with that? Or do you think there shouldn't be\n> a TXID filter at all, Matt? I didn't include the option of a prev output\n> script filter or rolling that into the block output script filter because\n> it changes the security model (cannot be proven to be correct/incorrect\n> succinctly).\n>\n> Then there's the question of whether to separate or combine the headers.\n> I'd lean towards keeping them separate because it's simpler that way.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/5a74bcf7/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: filter_sizes.svg\nType: image/svg+xml\nSize: 2066101 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/5a74bcf7/attachment-0001.svg>"
            },
            {
                "author": "Johan Tor\u00e5s Halseth",
                "date": "2018-05-23T08:16:41",
                "message_text_only": "Thanks, Jimpo!\n\nThis is very encouraging, I think. I sorta assumed that separating the\nelements into their own sub-filters would hurt the compression a lot more.\nCan the compression ratio/false positive rate be tweaked with the\nsub-filters in mind?\n\nWith the total size of the separated filters being no larger than the\ncombined filters, I see no benefit of combined filters? Committing to them\nall in the headers would also save space, and we could ensure nodes are\nserving all sub-filters.\n\n- Johan\n\nOn Wed, May 23, 2018 at 9:38 AM, Jim Posen <jim.posen at gmail.com> wrote:\n\n> So I checked filter sizes (as a proportion of block size) for each of the\n> sub-filters. The graph is attached.\n>\n> As interpretation, the first ~120,000 blocks are so small that the\n> Golomb-Rice coding can't compress the filters that well, which is why the\n> filter sizes are so high proportional to the block size. Except for the\n> input filter, because the coinbase input is skipped, so many of them have 0\n> elements. But after block 120,000 or so, the filter compression converges\n> pretty quickly to near the optimal value. The encouraging thing here is\n> that if you look at the ratio of the combined size of the separated filters\n> vs the size of a filter containing all of them (currently known as the\n> basic filter), they are pretty much the same size. The mean of the ratio\n> between them after block 150,000 is 99.4%. So basically, not much\n> compression efficiently is lost by separating the basic filter into\n> sub-filters.\n>\n> On Tue, May 22, 2018 at 5:42 PM, Jim Posen <jim.posen at gmail.com> wrote:\n>\n>> My suggestion was to advertise a bitfield for each filter type the node\n>>> serves,\n>>> where the bitfield indicates what elements are part of the filters. This\n>>> essentially\n>>> removes the notion of decided filter types and instead leaves the\n>>> decision to\n>>> full-nodes.\n>>>\n>>\n>> I think it makes more sense to construct entirely separate filters for\n>> the different types of elements and allow clients to download only the ones\n>> they care about. If there are enough elements per filter, the compression\n>> ratio shouldn't be much worse by splitting them up. This prevents the\n>> exponential blowup in the number of filters that you mention, Johan, and it\n>> works nicely with service bits for advertising different filter types\n>> independently.\n>>\n>> So if we created three separate filter types, one for output scripts, one\n>> for input outpoints, and one for TXIDs, each signaled with a separate\n>> service bit, are people good with that? Or do you think there shouldn't be\n>> a TXID filter at all, Matt? I didn't include the option of a prev output\n>> script filter or rolling that into the block output script filter because\n>> it changes the security model (cannot be proven to be correct/incorrect\n>> succinctly).\n>>\n>> Then there's the question of whether to separate or combine the headers.\n>> I'd lean towards keeping them separate because it's simpler that way.\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/7df6715a/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-23T17:28:29",
                "message_text_only": "Any chance you could add a graph of input-scripts  (instead of input outpoints)?\n\nOn Wed, May 23, 2018 at 7:38 AM, Jim Posen via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> So I checked filter sizes (as a proportion of block size) for each of the\n> sub-filters. The graph is attached.\n>\n> As interpretation, the first ~120,000 blocks are so small that the\n> Golomb-Rice coding can't compress the filters that well, which is why the\n> filter sizes are so high proportional to the block size. Except for the\n> input filter, because the coinbase input is skipped, so many of them have 0\n> elements. But after block 120,000 or so, the filter compression converges\n> pretty quickly to near the optimal value. The encouraging thing here is that\n> if you look at the ratio of the combined size of the separated filters vs\n> the size of a filter containing all of them (currently known as the basic\n> filter), they are pretty much the same size. The mean of the ratio between\n> them after block 150,000 is 99.4%. So basically, not much compression\n> efficiently is lost by separating the basic filter into sub-filters.\n>\n> On Tue, May 22, 2018 at 5:42 PM, Jim Posen <jim.posen at gmail.com> wrote:\n>>>\n>>> My suggestion was to advertise a bitfield for each filter type the node\n>>> serves,\n>>> where the bitfield indicates what elements are part of the filters. This\n>>> essentially\n>>> removes the notion of decided filter types and instead leaves the\n>>> decision to\n>>> full-nodes.\n>>\n>>\n>> I think it makes more sense to construct entirely separate filters for the\n>> different types of elements and allow clients to download only the ones they\n>> care about. If there are enough elements per filter, the compression ratio\n>> shouldn't be much worse by splitting them up. This prevents the exponential\n>> blowup in the number of filters that you mention, Johan, and it works nicely\n>> with service bits for advertising different filter types independently.\n>>\n>> So if we created three separate filter types, one for output scripts, one\n>> for input outpoints, and one for TXIDs, each signaled with a separate\n>> service bit, are people good with that? Or do you think there shouldn't be a\n>> TXID filter at all, Matt? I didn't include the option of a prev output\n>> script filter or rolling that into the block output script filter because it\n>> changes the security model (cannot be proven to be correct/incorrect\n>> succinctly).\n>>\n>> Then there's the question of whether to separate or combine the headers.\n>> I'd lean towards keeping them separate because it's simpler that way.\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Conner Fromknecht",
                "date": "2018-05-24T01:04:34",
                "message_text_only": "Hi all,\n\nJimpo, thanks for looking into those stats! I had always imagined that there\nwould be a more significant savings in having all filters in one bundle, as\nopposed to separate. These results are interesting, to say the least, and\ndefinitely offer us some flexibility in options for filter sharding.\n\nSo far, the bulk of this discussion has centered around bandwidth. I am\nconcerned, however, that splitting up the filters is at odds with the other\ngoal of the proposal in offering improved privacy.\n\nAllowing clients to choose individual filter sets trivially exposes the\ntype of\ndata that client is interested in. This alone might be enough to\nfingerprint the\nfunction of a peer and reduce anonymity set justifying their potential\nbehavior.\n\nFurthermore, if a match is encountered, and block requested, full nodes have\nmore targeted insight into what caused a particular match. They could infer\nthat\nthe client received funds in a particular block, e.g., if they are only\nrequesting\noutput scripts.\n\nThis is above and beyond the additional complexity of now syncing,\nvalidating,\nand managing five or six distinct header/filter-header/filter/block chains.\n\nI agree that saving on bandwidth is an important goal, but bandwidth and\nprivacy\nare always seemingly at odds. Strictly comparing the bandwidth requirements\nof\na system that heavily weighs privacy to existing ones, e.g. BIP39, that\ndon't is a\nlosing battle IMO.\n\nI'm not fundamentally opposed to splitting the filters, I certainly see the\narguments for flexibility. However, I also want to ensure we are\nconsidering the\nsecond order effects that fall out of optimizing for one metric when others\nexist.\n\nCheers,\nConner\nOn Wed, May 23, 2018 at 10:29 Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Any chance you could add a graph of input-scripts  (instead of input\n> outpoints)?\n>\n> On Wed, May 23, 2018 at 7:38 AM, Jim Posen via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > So I checked filter sizes (as a proportion of block size) for each of the\n> > sub-filters. The graph is attached.\n> >\n> > As interpretation, the first ~120,000 blocks are so small that the\n> > Golomb-Rice coding can't compress the filters that well, which is why the\n> > filter sizes are so high proportional to the block size. Except for the\n> > input filter, because the coinbase input is skipped, so many of them\n> have 0\n> > elements. But after block 120,000 or so, the filter compression converges\n> > pretty quickly to near the optimal value. The encouraging thing here is\n> that\n> > if you look at the ratio of the combined size of the separated filters vs\n> > the size of a filter containing all of them (currently known as the basic\n> > filter), they are pretty much the same size. The mean of the ratio\n> between\n> > them after block 150,000 is 99.4%. So basically, not much compression\n> > efficiently is lost by separating the basic filter into sub-filters.\n> >\n> > On Tue, May 22, 2018 at 5:42 PM, Jim Posen <jim.posen at gmail.com> wrote:\n> >>>\n> >>> My suggestion was to advertise a bitfield for each filter type the node\n> >>> serves,\n> >>> where the bitfield indicates what elements are part of the filters.\n> This\n> >>> essentially\n> >>> removes the notion of decided filter types and instead leaves the\n> >>> decision to\n> >>> full-nodes.\n> >>\n> >>\n> >> I think it makes more sense to construct entirely separate filters for\n> the\n> >> different types of elements and allow clients to download only the ones\n> they\n> >> care about. If there are enough elements per filter, the compression\n> ratio\n> >> shouldn't be much worse by splitting them up. This prevents the\n> exponential\n> >> blowup in the number of filters that you mention, Johan, and it works\n> nicely\n> >> with service bits for advertising different filter types independently.\n> >>\n> >> So if we created three separate filter types, one for output scripts,\n> one\n> >> for input outpoints, and one for TXIDs, each signaled with a separate\n> >> service bit, are people good with that? Or do you think there shouldn't\n> be a\n> >> TXID filter at all, Matt? I didn't include the option of a prev output\n> >> script filter or rolling that into the block output script filter\n> because it\n> >> changes the security model (cannot be proven to be correct/incorrect\n> >> succinctly).\n> >>\n> >> Then there's the question of whether to separate or combine the headers.\n> >> I'd lean towards keeping them separate because it's simpler that way.\n> >\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/b7412db9/attachment.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-24T03:48:00",
                "message_text_only": "Greg, I've attached a graph including the input scripts.\n\nIn the top graph, we can see how the input script filter compares to the\ninput outpoint filter. It is definitely smaller as a result of address\nreuse. The bottom graph shows the ratio over time of combining the input\nprev script and output script filters vs keeping them separate. In more\nrecent blocks, it appears that there are decreasing savings.\n\nOn Wed, May 23, 2018 at 6:04 PM Conner Fromknecht\n<conner at lightning.engineering> wrote:\n\n> Hi all,\n>\n> Jimpo, thanks for looking into those stats! I had always imagined that\n> there\n> would be a more significant savings in having all filters in one bundle, as\n> opposed to separate. These results are interesting, to say the least, and\n> definitely offer us some flexibility in options for filter sharding.\n>\n> So far, the bulk of this discussion has centered around bandwidth. I am\n> concerned, however, that splitting up the filters is at odds with the\n> other\n> goal of the proposal in offering improved privacy.\n>\n> Allowing clients to choose individual filter sets trivially exposes the\n> type of\n> data that client is interested in. This alone might be enough to\n> fingerprint the\n> function of a peer and reduce anonymity set justifying their potential\n> behavior.\n>\n> Furthermore, if a match is encountered, and block requested, full nodes\n> have\n> more targeted insight into what caused a particular match. They could\n> infer that\n> the client received funds in a particular block, e.g., if they are only\n> requesting\n> output scripts.\n>\n> This is above and beyond the additional complexity of now syncing,\n> validating,\n> and managing five or six distinct header/filter-header/filter/block chains.\n>\n> I agree that saving on bandwidth is an important goal, but bandwidth and\n> privacy\n> are always seemingly at odds. Strictly comparing the bandwidth\n> requirements of\n> a system that heavily weighs privacy to existing ones, e.g. BIP39, that\n> don't is a\n> losing battle IMO.\n>\n> I'm not fundamentally opposed to splitting the filters, I certainly see the\n> arguments for flexibility. However, I also want to ensure we are\n> considering the\n> second order effects that fall out of optimizing for one metric when\n> others exist.\n>\n> Cheers,\n> Conner\n> On Wed, May 23, 2018 at 10:29 Gregory Maxwell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Any chance you could add a graph of input-scripts  (instead of input\n>> outpoints)?\n>>\n>> On Wed, May 23, 2018 at 7:38 AM, Jim Posen via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > So I checked filter sizes (as a proportion of block size) for each of\n>> the\n>> > sub-filters. The graph is attached.\n>> >\n>> > As interpretation, the first ~120,000 blocks are so small that the\n>> > Golomb-Rice coding can't compress the filters that well, which is why\n>> the\n>> > filter sizes are so high proportional to the block size. Except for the\n>> > input filter, because the coinbase input is skipped, so many of them\n>> have 0\n>> > elements. But after block 120,000 or so, the filter compression\n>> converges\n>> > pretty quickly to near the optimal value. The encouraging thing here is\n>> that\n>> > if you look at the ratio of the combined size of the separated filters\n>> vs\n>> > the size of a filter containing all of them (currently known as the\n>> basic\n>> > filter), they are pretty much the same size. The mean of the ratio\n>> between\n>> > them after block 150,000 is 99.4%. So basically, not much compression\n>> > efficiently is lost by separating the basic filter into sub-filters.\n>> >\n>> > On Tue, May 22, 2018 at 5:42 PM, Jim Posen <jim.posen at gmail.com> wrote:\n>> >>>\n>> >>> My suggestion was to advertise a bitfield for each filter type the\n>> node\n>> >>> serves,\n>> >>> where the bitfield indicates what elements are part of the filters.\n>> This\n>> >>> essentially\n>> >>> removes the notion of decided filter types and instead leaves the\n>> >>> decision to\n>> >>> full-nodes.\n>> >>\n>> >>\n>> >> I think it makes more sense to construct entirely separate filters for\n>> the\n>> >> different types of elements and allow clients to download only the\n>> ones they\n>> >> care about. If there are enough elements per filter, the compression\n>> ratio\n>> >> shouldn't be much worse by splitting them up. This prevents the\n>> exponential\n>> >> blowup in the number of filters that you mention, Johan, and it works\n>> nicely\n>> >> with service bits for advertising different filter types independently.\n>> >>\n>> >> So if we created three separate filter types, one for output scripts,\n>> one\n>> >> for input outpoints, and one for TXIDs, each signaled with a separate\n>> >> service bit, are people good with that? Or do you think there\n>> shouldn't be a\n>> >> TXID filter at all, Matt? I didn't include the option of a prev output\n>> >> script filter or rolling that into the block output script filter\n>> because it\n>> >> changes the security model (cannot be proven to be correct/incorrect\n>> >> succinctly).\n>> >>\n>> >> Then there's the question of whether to separate or combine the\n>> headers.\n>> >> I'd lean towards keeping them separate because it's simpler that way.\n>> >\n>> >\n>> >\n>> > _______________________________________________\n>> > bitcoin-dev mailing list\n>> > bitcoin-dev at lists.linuxfoundation.org\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> >\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/fe5608eb/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: filter_sizes.svg\nType: image/svg+xml\nSize: 2833873 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/fe5608eb/attachment-0001.svg>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2018-05-28T18:18:12",
                "message_text_only": "Hi Jim,\n\nA \u201cbasic\u201d combined filter would mean up to 0.5 GB filter data per month (with 100% segwith use). Considering that 1 GB is the usual data quota for an entry level mobile phone contract, this could be a too high barrier for adoption.\n\nI repeated your calculations and produced a slightly different graph that shows the fraction of cummulative filter size to cummulative blockchain size. This is less noisy but otherwise confirms your measurement.\n\nI think that the data supports separation of filters as a combined filter does not seem to come with significant savings. (basic  size ~= txid + input points + output scripts sizes)\n \nMy calculations are repeatable with:\n\nhttps://github.com/tamasblummer/rust-bitcoin-spv/blob/blockfilterstats/src/bin/blockfilterstats.rs\n\nthat is using a Rust implementation of an SPV client built on top of other libraries we work on in the rust-bitcoin GitHub community (https://github.com/rust-bitcoin). Yes, this is a shameles plug for the project hoping to attract more developer.\n\nTamas Blummer\n\n\n\n\n> On May 24, 2018, at 05:48, Jim Posen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Greg, I've attached a graph including the input scripts.\n> \n> In the top graph, we can see how the input script filter compares to the input outpoint filter. It is definitely smaller as a result of address reuse. The bottom graph shows the ratio over time of combining the input prev script and output script filters vs keeping them separate. In more recent blocks, it appears that there are decreasing savings.\n> \n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180528/98939f0d/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: filters.png\nType: image/png\nSize: 69944 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180528/98939f0d/attachment-0001.png>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2018-05-28T18:28:16",
                "message_text_only": "Forgot to mention: The link I sent is to a branch that is patched to produce the filter stats. \nThis is the main project and the BIP158 implementation: https://github.com/rust-bitcoin/rust-bitcoin-spv/blob/master/src/blockfilter.rs\n\nTamas Blummer\n\n> On May 28, 2018, at 20:18, Tamas Blummer <tamas.blummer at gmail.com> wrote:\n> \n> Hi Jim,\n> \n> A \u201cbasic\u201d combined filter would mean up to 0.5 GB filter data per month (with 100% segwith use). Considering that 1 GB is the usual data quota for an entry level mobile phone contract, this could be a too high barrier for adoption.\n> \n> I repeated your calculations and produced a slightly different graph that shows the fraction of cummulative filter size to cummulative blockchain size. This is less noisy but otherwise confirms your measurement.\n> \n> I think that the data supports separation of filters as a combined filter does not seem to come with significant savings. (basic  size ~= txid + input points + output scripts sizes)\n>  \n> My calculations are repeatable with:\n> \n> https://github.com/tamasblummer/rust-bitcoin-spv/blob/blockfilterstats/src/bin/blockfilterstats.rs\n> \n> that is using a Rust implementation of an SPV client built on top of other libraries we work on in the rust-bitcoin GitHub community (https://github.com/rust-bitcoin). Yes, this is a shameles plug for the project hoping to attract more developer.\n> \n> Tamas Blummer\n> \n> \n> <filters.png>\n> \n>> On May 24, 2018, at 05:48, Jim Posen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>> Greg, I've attached a graph including the input scripts.\n>> \n>> In the top graph, we can see how the input script filter compares to the input outpoint filter. It is definitely smaller as a result of address reuse. The bottom graph shows the ratio over time of combining the input prev script and output script filters vs keeping them separate. In more recent blocks, it appears that there are decreasing savings.\n>> \n>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-28T19:24:09",
                "message_text_only": "Is there an application that requires watching for output scripts that\ndoesn't also require watching for input scrips (or, less efficiently,\ninput outpoints)?\n\nAny of the wallet application that I'm aware of need to see coins\nbeing spent as well as created, else they may try to spend already\nspent coins. If we're not aware of any applications that wouldnt use\nboth, there isn't much reason to separate them and if input scripts\nare used instead of input outpoints there is additional savings from\ncombining them (due to the same scripts being spent as were created in\nthe block-- due to reuse and chaining).\n\nI still am of the belief, based on Matt's argument, that there is no\nuse for txid what-so-ever (instead just watch for an outpoint).\n\n\n\n\n\nOn Mon, May 28, 2018 at 6:28 PM, Tamas Blummer <tamas.blummer at gmail.com> wrote:\n> Forgot to mention: The link I sent is to a branch that is patched to produce the filter stats.\n> This is the main project and the BIP158 implementation: https://github.com/rust-bitcoin/rust-bitcoin-spv/blob/master/src/blockfilter.rs\n>\n> Tamas Blummer\n>\n>> On May 28, 2018, at 20:18, Tamas Blummer <tamas.blummer at gmail.com> wrote:\n>>\n>> Hi Jim,\n>>\n>> A \u201cbasic\u201d combined filter would mean up to 0.5 GB filter data per month (with 100% segwith use). Considering that 1 GB is the usual data quota for an entry level mobile phone contract, this could be a too high barrier for adoption.\n>>\n>> I repeated your calculations and produced a slightly different graph that shows the fraction of cummulative filter size to cummulative blockchain size. This is less noisy but otherwise confirms your measurement.\n>>\n>> I think that the data supports separation of filters as a combined filter does not seem to come with significant savings. (basic  size ~= txid + input points + output scripts sizes)\n>>\n>> My calculations are repeatable with:\n>>\n>> https://github.com/tamasblummer/rust-bitcoin-spv/blob/blockfilterstats/src/bin/blockfilterstats.rs\n>>\n>> that is using a Rust implementation of an SPV client built on top of other libraries we work on in the rust-bitcoin GitHub community (https://github.com/rust-bitcoin). Yes, this is a shameles plug for the project hoping to attract more developer.\n>>\n>> Tamas Blummer\n>>\n>>\n>> <filters.png>\n>>\n>>> On May 24, 2018, at 05:48, Jim Posen via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> Greg, I've attached a graph including the input scripts.\n>>>\n>>> In the top graph, we can see how the input script filter compares to the input outpoint filter. It is definitely smaller as a result of address reuse. The bottom graph shows the ratio over time of combining the input prev script and output script filters vs keeping them separate. In more recent blocks, it appears that there are decreasing savings.\n>>>\n>>\n>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-29T02:42:52",
                "message_text_only": "> Is there an application that requires watching for output scripts that\n> doesn't also require watching for input scrips (or, less efficiently,\n> input outpoints)?\n>\n\nCertain wallets may be able to use only the output script filter by using\noutput scripts to watch for confirmations on sent transactions, assuming\nthat application is the only one with access to the private keys. The\nadditional benefit of the input script/outpoint filter is to watch for\nunexpected spends (coins getting stolen or spent from another wallet) or\ntransactions without a unique change or output address. I think this is a\nreasonable implementation, and it would be nice to be able to download that\nfilter without any input elements.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180528/8fdc2ff6/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-29T04:01:35",
                "message_text_only": "> The additional benefit of the input script/outpoint filter is to watch for\n> unexpected spends (coins getting stolen or spent from another wallet) or\n> transactions without a unique change or output address. I think this is a\n> reasonable implementation, and it would be nice to be able to download\nthat\n> filter without any input elements.\n\nAs someone who's implemented a complete integration of the filtering\ntechnique into an existing wallet, and a higher application I disagree.\nThere's not much gain to be had in splitting up the filters: it'll result in\nadditional round trips (to fetch these distinct filter) during normal\noperation, complicate routine seed rescanning logic, and also is detrimental\nto privacy if one is fetching blocks from the same peer as they've\ndownloaded the filters from.\n\nHowever, I'm now convinced that the savings had by including the prev output\nscript (addr re-use and outputs spent in the same block as they're created)\noutweigh the additional booking keeping required in an implementation (when\nextracting the precise tx that matched) compared to using regular outpoint\nas we do currently. Combined with the recently proposed re-parametrization\nof the gcs parameters[1], the filter size should shrink by quite a bit!\n\nI'm very happy with the review the BIPs has been receiving as of late. It\nwould've been nice to have this 1+ year ago when the draft was initially\nproposed, but better late that never!\n\nBased on this thread, [1], and discussions on various IRC channels, I plan\nto make the following modifications to the BIP:\n\n  1. use P=2^19 and M=784931 as gcs parameters, and also bind these to the\n     filter instance, so future filter types may use distinct parameters\n  2. use the prev output script rather than the prev input script in the\n     regular filter\n  3. remove the txid from the regular filter(as with some extra book-keeping\n     the output script is enough)\n  4. do away with the extended filter all together, as our original use case\n     for it has been nerfed as the filter size grew too large when doing\n     recursive parsing. instead we watch for the outpoint being spent and\n     extract the pre-image from it if it matches now\n\nThe resulting changes should slash the size of the filters, yet still ensure\nthat they're useful enough for our target use case.\n\n[1]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016029.html\n\n-- Laolu\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180528/8326d73b/attachment-0001.html>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2018-05-31T14:27:50",
                "message_text_only": "I processed the historic blockchain to create a single filter populated with spent input scripts and output scripts. The Golomb parameter was P=2^20\n\nThe resulting chart shows a volatile history of same-block address re-use with a notable drops in relative filter size during the early history and in the time window where SatoshiDICE was popular, since then trending higher.\nThe history of only the last half year suggests a current filter size of between 2.0% - 2.5% of block sizes.\n\nSince most outputs are spent within a short time period, but apparently not that often in same blocks, I think it was worth considering filter series that match over a windows of 2^n blocks (n=(0\u202610)). Applications could then bracket the \nrange of interest and then narrow down requesting finer filters or blocks.\n\nThen I created 1600 random (P2SH) scripts and totaled the false positive block download data size if observing 100, 200, 400, 800, 1600 of them. \nThe result suggests that even for 1600 the false positive overhead is less than 0.1% of blockchain data size. \n\nI agree with Greg that we should optimize the parameters for a small observed set as those will be running on mobile devices.\nAs of Pieter\u2019s findings the simulation parameters were optimal for ca. 1000 observed scripts which is maybe to many for a \u201csmall\u201d application.\nOn the other hand we do not know the needs of future popular mobile applications.  \n \nWith parameters of the simulation the current minimal data burden on a mobile wallet would be ca. 0.1 GB / Month.\n\nSimulations with other parameters could be executed using this patch branch: https://github.com/tamasblummer/rust-bitcoin-spv/tree/blockfilterstats A run takes a few hours on a fast machine with release build and local bitcoind.\nThe calculation can not be reduced to the recent history as the process builds in-memory utxo from genesis.\n\nThe result of executing the binary is a CSV file containing:\nblocknumber, blocksize, utxo size, filter size, false positive data size for 100, false positive data size for 100, \u2026 false positive data size for 100\ne.g:\n524994,1112181,57166825,21556,0,0,0,0,1112181\n\nTamas Blummer\n\n\n> On May 29, 2018, at 06:01, Olaoluwa Osuntokun via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> > The additional benefit of the input script/outpoint filter is to watch for\n> > unexpected spends (coins getting stolen or spent from another wallet) or\n> > transactions without a unique change or output address. I think this is a\n> > reasonable implementation, and it would be nice to be able to download that\n> > filter without any input elements. \n> \n> As someone who's implemented a complete integration of the filtering\n> technique into an existing wallet, and a higher application I disagree.\n> There's not much gain to be had in splitting up the filters: it'll result in\n> additional round trips (to fetch these distinct filter) during normal\n> operation, complicate routine seed rescanning logic, and also is detrimental\n> to privacy if one is fetching blocks from the same peer as they've\n> downloaded the filters from.\n> \n> However, I'm now convinced that the savings had by including the prev output\n> script (addr re-use and outputs spent in the same block as they're created)\n> outweigh the additional booking keeping required in an implementation (when\n> extracting the precise tx that matched) compared to using regular outpoint\n> as we do currently. Combined with the recently proposed re-parametrization\n> of the gcs parameters[1], the filter size should shrink by quite a bit!\n> \n> I'm very happy with the review the BIPs has been receiving as of late. It\n> would've been nice to have this 1+ year ago when the draft was initially\n> proposed, but better late that never!\n> \n> Based on this thread, [1], and discussions on various IRC channels, I plan\n> to make the following modifications to the BIP:\n> \n>   1. use P=2^19 and M=784931 as gcs parameters, and also bind these to the\n>      filter instance, so future filter types may use distinct parameters\n>   2. use the prev output script rather than the prev input script in the\n>      regular filter\n>   3. remove the txid from the regular filter(as with some extra book-keeping\n>      the output script is enough) \n>   4. do away with the extended filter all together, as our original use case\n>      for it has been nerfed as the filter size grew too large when doing\n>      recursive parsing. instead we watch for the outpoint being spent and\n>      extract the pre-image from it if it matches now\n> \n> The resulting changes should slash the size of the filters, yet still ensure\n> that they're useful enough for our target use case.\n> \n> [1]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-May/016029.html\n> \n> -- Laolu\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180531/34f94f8e/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: filtersize.png\nType: image/png\nSize: 58848 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180531/34f94f8e/attachment-0003.png>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: lasthalfyear.png\nType: image/png\nSize: 50431 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180531/34f94f8e/attachment-0004.png>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: falsepositive.png\nType: image/png\nSize: 82663 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180531/34f94f8e/attachment-0005.png>"
            },
            {
                "author": "Riccardo Casatta",
                "date": "2018-05-18T08:46:29",
                "message_text_only": "Another parameter which heavily affects filter size is the false positive\nrate which is empirically set\n<https://github.com/bitcoin/bips/blob/master/bip-0158.mediawiki#construction>\nto 2^-20\nThe BIP recall some go code\n<https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go>\nfor how the parameter has been selected which I can hardly understand and\nrun, it's totally my fault but if possible I would really like more details\non the process, like charts and explanations (for example, which is the\nnumber of elements to search for which the filter has been optimized for?)\n\nInstinctively I feel 2^-20 is super low and choosing a lot higher alpha\nwill shrink the total filter size by gigabytes at the cost of having to\nwastefully download just some megabytes of blocks.\n\n\n2018-05-17 18:36 GMT+02:00 Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> On Thu, May 17, 2018 at 3:25 PM, Matt Corallo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > I believe (1) could be skipped entirely - there is almost no reason why\n> > you'd not be able to filter for, eg, the set of output scripts in a\n> > transaction you know about\n>\n> I think this is convincing for the txids themselves.\n>\n> What about also making input prevouts filter based on the scriptpubkey\n> being _spent_?  Layering wise in the processing it's a bit ugly, but\n> if you validated the block you have the data needed.\n>\n> This would eliminate the multiple data type mixing entirely.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n\n-- \nRiccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180518/fe07e012/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-19T03:08:29",
                "message_text_only": "Riccardo wrote:\n> The BIP recall some go code for how the parameter has been selected which\n> I can hardly understand and run\n\nThe code you're linking to is for generating test vectors (to allow\nimplementations to check the correctness of their gcs filters. The name of\nthe file is 'gentestvectors.go'. It produces CSV files which contain test\nvectors of various testnet blocks and at various false positive rates.\n\n> it's totally my fault but if possible I would really like more details on\n> the process, like charts and explanations\n\nWhen we published the BIP draft last year (wow, time flies!), we put up code\n(as well as an interactive website) showing the process we used to arrive at\nthe current false positive rate. The aim was to minimize the bandwidth\nrequired to download each filter plus the expected bandwidth from\ndownloading \"large-ish\" full segwit blocks. The code simulated a few wallet\ntypes (in terms of number of addrs, etc) focusing on a \"mid-sized\" wallet.\nOne could also model the selection as a Bernoulli process where we attempt\nto compute the probability that after k queries (let's say you have k\naddresses) we have k \"successes\". A success would mean the queries item\nwasn't found in the filter, while a failure is a filter match (false\npositive or not). A failure in the process requires fetching the entire\nblock.\n\n-- Laolu\n\nOn Fri, May 18, 2018 at 5:35 AM Riccardo Casatta via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Another parameter which heavily affects filter size is the false positive\n> rate which is empirically set\n> <https://github.com/bitcoin/bips/blob/master/bip-0158.mediawiki#construction>\n> to 2^-20\n> The BIP recall some go code\n> <https://github.com/Roasbeef/bips/blob/83b83c78e189be898573e0bfe936dd0c9b99ecb9/gcs_light_client/gentestvectors.go>\n> for how the parameter has been selected which I can hardly understand and\n> run, it's totally my fault but if possible I would really like more details\n> on the process, like charts and explanations (for example, which is the\n> number of elements to search for which the filter has been optimized for?)\n>\n> Instinctively I feel 2^-20 is super low and choosing a lot higher alpha\n> will shrink the total filter size by gigabytes at the cost of having to\n> wastefully download just some megabytes of blocks.\n>\n>\n> 2018-05-17 18:36 GMT+02:00 Gregory Maxwell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org>:\n>\n>> On Thu, May 17, 2018 at 3:25 PM, Matt Corallo via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > I believe (1) could be skipped entirely - there is almost no reason why\n>> > you'd not be able to filter for, eg, the set of output scripts in a\n>> > transaction you know about\n>>\n>> I think this is convincing for the txids themselves.\n>>\n>> What about also making input prevouts filter based on the scriptpubkey\n>> being _spent_?  Layering wise in the processing it's a bit ugly, but\n>> if you validated the block you have the data needed.\n>>\n>> This would eliminate the multiple data type mixing entirely.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n>\n> --\n> Riccardo Casatta - @RCasatta <https://twitter.com/RCasatta>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180518/b40519c4/attachment.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-19T02:57:12",
                "message_text_only": "Greg wrote:\n> What about also making input prevouts filter based on the scriptpubkey\nbeing\n> _spent_?  Layering wise in the processing it's a bit ugly, but if you\n> validated the block you have the data needed.\n\nAFAICT, this would mean that in order for a new node to catch up the filter\nindex (index all historical blocks), they'd either need to: build up a\nutxo-set in memory during indexing, or would require a txindex in order to\nlook up the prev out's script. The first option increases the memory load\nduring indexing, and the second requires nodes to have a transaction index\n(and would also add considerable I/O load). When proceeding from tip, this\ndoesn't add any additional load assuming that your synchronously index the\nblock as you validate it, otherwise the utxo set will already have been\nupdated (the spent scripts removed).\n\nI have a script running to compare the filter sizes assuming the regular\nfilter switches to include the prev out's script rather than the prev\noutpoint itself. The script hasn't yet finished (due to the increased I/O\nload to look up the scripts when indexing), but I'll report back once it's\nfinished.\n\n-- Laolu\n\n\nOn Thu, May 17, 2018 at 9:37 AM Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Thu, May 17, 2018 at 3:25 PM, Matt Corallo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > I believe (1) could be skipped entirely - there is almost no reason why\n> > you'd not be able to filter for, eg, the set of output scripts in a\n> > transaction you know about\n>\n> I think this is convincing for the txids themselves.\n>\n> What about also making input prevouts filter based on the scriptpubkey\n> being _spent_?  Layering wise in the processing it's a bit ugly, but\n> if you validated the block you have the data needed.\n>\n> This would eliminate the multiple data type mixing entirely.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180518/228dc08e/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-05-19T03:06:10",
                "message_text_only": "On Fri, May 18, 2018, 19:57 Olaoluwa Osuntokun via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Greg wrote:\n> > What about also making input prevouts filter based on the scriptpubkey\n> being\n> > _spent_?  Layering wise in the processing it's a bit ugly, but if you\n> > validated the block you have the data needed.\n>\n> AFAICT, this would mean that in order for a new node to catch up the filter\n> index (index all historical blocks), they'd either need to: build up a\n> utxo-set in memory during indexing, or would require a txindex in order to\n> look up the prev out's script. The first option increases the memory load\n> during indexing, and the second requires nodes to have a transaction index\n> (and would also add considerable I/O load). When proceeding from tip, this\n> doesn't add any additional load assuming that your synchronously index the\n> block as you validate it, otherwise the utxo set will already have been\n> updated (the spent scripts removed).\n>\n\nI was wondering about that too, but it turns out that isn't necessary. At\nleast in Bitcoin Core, all the data needed for such a filter is in the\nblock + undo files (the latter contain the scriptPubKeys of the outputs\nbeing spent).\n\nI have a script running to compare the filter sizes assuming the regular\n> filter switches to include the prev out's script rather than the prev\n> outpoint itself. The script hasn't yet finished (due to the increased I/O\n> load to look up the scripts when indexing), but I'll report back once it's\n> finished.\n>\n\nThat's very helpful, thank you.\n\nCheers,\n\n-- \nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180518/3077040c/attachment-0001.html>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-22T01:15:22",
                "message_text_only": "Hi Y'all,\n\nThe script finished a few days ago with the following results:\n\nreg-filter-prev-script total size:  161236078  bytes\nreg-filter-prev-script avg:         16123.6078 bytes\nreg-filter-prev-script median:      16584      bytes\nreg-filter-prev-script max:         59480      bytes\n\nCompared to the original median size of the same block range, but with the\ncurrent filter (has both txid, prev outpoint, output scripts), we see a\nroughly 34% reduction in filter size (current median is 22258 bytes).\nCompared to the suggested modified filter (no txid, prev outpoint, output\nscripts), we see a 15% reduction in size (median of that was 19198 bytes).\nThis shows that script re-use is still pretty prevalent in the chain as of\nrecent.\n\nOne thing that occurred to me, is that on the application level, switching\nto the input prev output script can make things a bit awkward. Observe that\nwhen looking for matches in the filter, upon a match, one would need access\nto an additional (outpoint -> script) map in order to locate _which_\nparticular transaction matched w/o access to an up-to-date UTOX set. In\ncontrast, as is atm, one can locate the matching transaction with no\nadditional information (as we're matching on the outpoint).\n\nAt this point, if we feel filter sizes need to drop further, then we may\nneed to consider raising the false positive rate.\n\nDoes anyone have any estimates or direct measures w.r.t how much bandwidth\ncurrent BIP 37 light clients consume? It would be nice to have a direct\ncomparison. We'd need to consider the size of their base bloom filter, the\naccumulated bandwidth as a result of repeated filterload commands (to adjust\nthe fp rate), and also the overhead of receiving the merkle branch and\ntransactions in distinct messages (both due to matches and false positives).\n\nFinally, I'd be open to removing the current \"extended\" filter from the BIP\nas is all together for now. If a compelling use case for being able to\nfilter the sigScript/witness arises, then we can examine re-adding it with a\ndistinct service bit. After all it would be harder to phase out the filter\nonce wider deployment was already reached. Similarly, if the 16% savings\nachieved by removing the txid is attractive, then we can create an\nadditional\nfilter just for the txids to allow those applications which need the\ninformation to seek out that extra filter.\n\n-- Laolu\n\n\nOn Fri, May 18, 2018 at 8:06 PM Pieter Wuille <pieter.wuille at gmail.com>\nwrote:\n\n> On Fri, May 18, 2018, 19:57 Olaoluwa Osuntokun via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Greg wrote:\n>> > What about also making input prevouts filter based on the scriptpubkey\n>> being\n>> > _spent_?  Layering wise in the processing it's a bit ugly, but if you\n>> > validated the block you have the data needed.\n>>\n>> AFAICT, this would mean that in order for a new node to catch up the\n>> filter\n>> index (index all historical blocks), they'd either need to: build up a\n>> utxo-set in memory during indexing, or would require a txindex in order to\n>> look up the prev out's script. The first option increases the memory load\n>> during indexing, and the second requires nodes to have a transaction index\n>> (and would also add considerable I/O load). When proceeding from tip, this\n>> doesn't add any additional load assuming that your synchronously index the\n>> block as you validate it, otherwise the utxo set will already have been\n>> updated (the spent scripts removed).\n>>\n>\n> I was wondering about that too, but it turns out that isn't necessary. At\n> least in Bitcoin Core, all the data needed for such a filter is in the\n> block + undo files (the latter contain the scriptPubKeys of the outputs\n> being spent).\n>\n> I have a script running to compare the filter sizes assuming the regular\n>> filter switches to include the prev out's script rather than the prev\n>> outpoint itself. The script hasn't yet finished (due to the increased I/O\n>> load to look up the scripts when indexing), but I'll report back once it's\n>> finished.\n>>\n>\n> That's very helpful, thank you.\n>\n> Cheers,\n>\n> --\n> Pieter\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180521/feb53119/attachment.html>"
            },
            {
                "author": "Karl-Johan Alm",
                "date": "2018-05-18T06:28:39",
                "message_text_only": "On Fri, May 18, 2018 at 12:25 AM, Matt Corallo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> In general, I'm concerned about the size of the filters making existing\n> SPV clients less willing to adopt BIP 158 instead of the existing bloom\n> filter garbage and would like to see a further exploration of ways to\n> split out filters to make them less bandwidth intensive. Some further\n> ideas we should probably play with before finalizing moving forward is\n> providing filters for certain script templates, eg being able to only\n> get outputs that are segwit version X or other similar ideas.\n\nThere is also the idea of multi-block filters. The idea is that light\nclients would download a pair of filters for blocks X..X+255 and\nX+256..X+511, check if they have any matches and then grab pairs for\nany that matched, e.g. X..X+127 & X+128..X+255 if left matched, and\niterate down until it ran out of hits-in-a-row or it got down to\nsingle-block level.\n\nThis has an added benefit where you can accept a slightly higher false\npositive rate for bigger ranges, because the probability of a specific\nentry having a false positive in each filter is (empirically speaking)\nindependent. I.e. with a FP probability of 1% in the 256 range block\nand a FP probability of 0.1% in the 128 range block would mean the\nprobability is actually 0.001%.\n\nWrote about this here: https://bc-2.jp/bfd-profile.pdf (but the filter\ntype is different in my experiments)"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2018-05-19T02:51:02",
                "message_text_only": "Matt wrote:\n> I believe (1) could be skipped entirely - there is almost no reason why\n> you'd not be able to filter for, eg, the set of output scripts in a\n> transaction you know about\n\nDepending on the use-case, the txid is more precise than searching for the\noutput script as it doesn't need to deal with duplicated output scripts. To\nmy knowledge, lnd is the only major project that currently utilizes BIP\n157+158. At this point, we use the txid in the regular filter for\nconfirmations (channel confirmed, sweep tx confirmed, cltv confirmed, etc).\nSwitching to use output scripts instead wouldn't be _too_ invasive w.r.t\nchanges required in the codebase, only the need to deal with output script\nduplication could be annoying.\n\n> (2) and (3) may want to be split out - many wallets may wish to just find\n> transactions paying to them, as transactions spending from their outputs\n> should generally be things they've created.\n\nFWIW, in the \"rescan after importing by seed phrase\" both are needed in\norder to ensure the wallet ends up with the proper output set after the\nscan. In lnd we actively use both (2) to detect deposits to the internal\nwallet, and (3) to be notified when our channel outputs are spent on-chain\n(and also generally when any of our special scripts are spent).\n\n> In general, I'm concerned about the size of the filters making existing\nSPV\n> clients less willing to adopt BIP 158 instead of the existing bloom filter\n> garbage and would like to see a further exploration of ways to split out\n> filters to make them less bandwidth intensive.\n\nAgreed that the current filter size may prevent adoption amongst wallets.\nHowever, the other factor that will likely prevent adoption amongst current\nBIP-37 mobile wallets is the lack of support for notifying _unconfirmed_\ntransactions. When we drafted up the protocol last year and asked around,\nthis was one of the major points of contention amongst existing mobile\nwallets that utilize BIP 37.\n\nOn the other hand, the two \"popular\" BIP 37 wallets I'm aware of\n(Breadwallet, and Andreas Schildbach's Bitcoin Wallet) have lagged massively\nbehind the existing set of wallet related protocol upgrades. For example,\nneither of them have released versions of their applications that take\nadvantage of segwit in any manner. Breadwallet has more or less \"pivoted\"\n(they did an ICO and have a token) and instead is prioritizing things like\nadding random ICO tokens over catching up with the latest protocol updates.\nBased on this behavior, even if the filter sizes were even _more_ bandwidth\nefficient that BIP 37, I don't think they'd adopt the protocol.\n\n> Some further ideas we should probably play with before finalizing moving\n> forward is providing filters for certain script templates, eg being able\nto\n> only get outputs that are segwit version X or other similar ideas.\n\nWhy should this block active deployment of BIP 157+158 as is now? As\ndefined, the protocol already allows future updates to add additional filter\ntypes. Before the filters are committed, each filter type requires a new\nfilter header. We could move to a single filter header that commits to the\nhashes of _all_ filters, but that would mean that a node couldn't serve the\nheaders unless they had all currently defined features, defeating the\noptionality offered.\n\nAdditionally, more filters entails more disk utilization for nodes serving\nthese filters. Nodes have the option to instead create the filters at \"query\ntime\", but then this counters the benefit of simply slinging the filters\nfrom disk (or a memory map or w/e). IMO, it's a desirable feature that\nserving light clients no longer requires active CPU+I/O and instead just\npassive I/O (nodes could even write the filters to disk in protocol msg\nformat).\n\nTo get a feel for the current filter sizes, a txid-only filter size, and a\nregular filter w/o txid's, I ran some stats on the last 10k blocks:\n\nregular size:    217107653  bytes\nregular avg:     21710.7653 bytes\nregular median:  22332      bytes\nregular max:     61901      bytes\n\ntxid-only size:    34518463  bytes\ntxid-only avg:     3451.8463 bytes\ntxid-only median:  3258      bytes\ntxid-only max:     10193     bytes\n\nreg-no-txid size:    182663961  bytes\nreg-no-txid avg:     18266.3961 bytes\nreg-no-txid median:  19198      bytes\nreg-no-txid max:     60172      bytes\n\nSo the median regular filter size over the past 10k blocks is 20KB. If we\nextract the txid from the regular filter and add a txid-only filter, the\nmedian size of that is 3.2KB. Finally, the median size of a modified regular\nfilter (no txid) is 19KB.\n\n-- Laolu\n\n\nOn Thu, May 17, 2018 at 8:33 AM Matt Corallo via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> BIP 158 currently includes the following in the \"basic\" filter: 1)\n> txids, 2) output scripts, 3) input prevouts.\n>\n> I believe (1) could be skipped entirely - there is almost no reason why\n> you'd not be able to filter for, eg, the set of output scripts in a\n> transaction you know about and (2) and (3) may want to be split out -\n> many wallets may wish to just find transactions paying to them, as\n> transactions spending from their outputs should generally be things\n> they've created.\n>\n> In general, I'm concerned about the size of the filters making existing\n> SPV clients less willing to adopt BIP 158 instead of the existing bloom\n> filter garbage and would like to see a further exploration of ways to\n> split out filters to make them less bandwidth intensive. Some further\n> ideas we should probably play with before finalizing moving forward is\n> providing filters for certain script templates, eg being able to only\n> get outputs that are segwit version X or other similar ideas.\n>\n> Matt\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180518/58337833/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-29T03:24:45",
                "message_text_only": "On Tue, May 29, 2018 at 2:42 AM, Jim Posen <jim.posen at gmail.com> wrote:\n> Certain wallets may be able to use only the output script filter by using\n> output scripts to watch for confirmations on sent transactions, assuming\n> that application is the only one with access to the private keys. The\n> additional benefit of the input script/outpoint filter is to watch for\n> unexpected spends (coins getting stolen or spent from another wallet) or\n> transactions without a unique change or output address. I think this is a\n> reasonable implementation, and it would be nice to be able to download that\n> filter without any input elements.\n\nIn this configuration there is little need to access historical blocks\nthough, since you're assuming that you'll never recover from a backup.\nNo?"
            }
        ],
        "thread_summary": {
            "title": "BIP 158 Flexibility and Filter Size",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Johan Tor\u00e5s Halseth",
                "Tamas Blummer",
                "Conner Fromknecht",
                "Peter Todd",
                "Riccardo Casatta",
                "Olaoluwa Osuntokun",
                "Gregory Maxwell",
                "Matt Corallo",
                "Pieter Wuille",
                "Jim Posen",
                "Karl-Johan Alm"
            ],
            "messages_count": 34,
            "total_messages_chars_count": 94592
        }
    },
    {
        "title": "[bitcoin-dev] stratum protocol extension - mining.configure, formal version rolling and other extensions",
        "thread_messages": [
            {
                "author": "Jan \u010capek",
                "date": "2018-05-17T16:49:43",
                "message_text_only": "Hello,\n\nwe (at braiins systems/slushpool) would like to kindly re-open the\ntopic of stratum protocol extension that has been in fact implemented by\nquite a few pools already (including us). This extension\ncurrently allows configuring the stratum session for version rolling\nand enables a generic mechanism for requesting protocol\nextensions from the miners. More details are in the specification\nbelow. \n\nI am aware of LukeJr's comments on why we haven't used\nhttps://en.bitcoin.it/wiki/Stratum_mining_protocol#mining.capabilities_.28DRAFT.29\n\nWe are aware that there has been some academic work done on\nconcentrating full stratum protocol specs in one BIP as referred here\ne.g.\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015728.html\n\nBelow is a copy of a full extension draft that has been published at\nthis URL:\nhttps://github.com/slushpool/stratumprotocol/blob/master/stratum-extensions.mediawiki\n\n\nWe should probably join the effort and unify all the documents in one\nsingle BIP if that would be seen as the most efficient way of having\nthe specs.\n\n\nKind regards,\n\nJan\n\n<pre>\n  BIP: ????\n  Layer: Applications\n  Title: Stratum protocol extensions\n  Author: Pavel Moravec <pavel.moravec at braiins.cz>\n\t  Jan Capek <jan.capek at braiins.cz>\n  Comments-Summary: No comments yet.\n  Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-????\n  Status: Draft\n  Type: Informational\n  Created: 2018-03-10\n  License: BSD-3-Clause\n           CC0-1.0\n</pre>\n\n==Abstract==\n\nThis BIP provides a generic mechanism for specifying stratum protocol\nextensions. At the same time, one of the important extensions that is\nspecified by this BIP is configuration of bits for \"version rolling\"\nin nVersion field of bitcoin block header.\n\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\",\n\"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this\ndocument are to be interpreted as described in RFC 2119.\n\n==Motivation==\n\nThe initial motivation for specifying some general support for stratum\nprotocol extensions was a need to allow miners to do so called\n\"version rolling\", changing value in the first field of the Bitcoin\nblock header.\n\nVersion rolling is backwards incompatible change to the stratum protocol\nbecause the miner couldn't communicate different block version value to\nthe server in the original version of the stratum protocol. Similarly,\na server couldn't communicate safe bits for rolling to a miner. So\nboth miners and pools need to implement some protocol extension to\nsupport version rolling.\n\nTypically, if a miner sends an unknown message to a server, the server\ncloses the connection (not all implementations do that but some\ndo). So it is not very safe to try to send unknown messages to\nservers.\n\nWe can use this opportunity to make one backwards incompatible\nchange to the protocol to support multiple extensions in the\nfuture. In a way that a miner can advertise its capabilities and at\nthe same time it can request some needed features from the server.\n\nIt is preferable that the same mechanism for feature negotiation can\nbe used for not yet known features. It SHOULD be easy to implement in\nthe mining software too.\n\nWe introduce one new message to the stratum protocol\n('''\"mining.configure\"''') which handles the initial\nconfiguration/negotiation of features in a generic way. So that adding\nfeatures in the future can be done without a necessity to add new\nmessages to stratum protocol.\n\nEach extension has its unique string name, so called '''extension\ncode'''.\n\n\n==Specification==\nCurrently, the following extensions are defined:\n\n* '''\"version-rolling\"'''\n* '''\"minimum-difficulty\"'''\n* '''\"subscribe-extranonce\"'''\n\n\n===Additional data types===\n\nThe following names are used as type aliases, making the message\ndescription easier.\n\n* '''TMask''' - case independent hexadecimal string of length 8,\n  encoding an unsigned 32-bit integer (~<code>[0-9a-fA-F]{8}</code>)\n\n* '''TExtensionCode''' - non-empty string with a value equal to the\n  name of some protocol extension.\n\n* '''TExtensionResult''' - <code>true</code> / <code>false</code> /\n  ''String''. ** <code>true</code> = The requested feature is supported\n  and its configuration understood and applied. ** <code>false</code> =\n  The feature is not supported or unknown. ** ''String'' = Error\n  message containing information about what went wrong.\n\n\n===Request \"mining.configure\"===\n\nThis message (JSON RPC Request) SHOULD be the '''first message''' sent\nby the miner after the connection with the server is established. The\nclient uses the message to advertise its features and to request/allow\nsome protocol extensions.\n\nThe reason for it being the first is that we want the implementation and\npossible interactions to be as easy and simple as possible. An extension\ncan define explicitly what does a repeated configuration of that\nextension mean.\n\nEach extension code provides a namespace for its extension parameters\nand extension return values. By convention, the names are formed from\nextension codes by adding \".\" and a parameter name. The same applies\nfor the return values, which are transferred in a result map\ntoo. E.g. \"version-rolling.mask\" is the name of the parameter \"mask\" of\nextension \"version-rolling\".\n\n'''Parameters''':\n\n* '''extensions''' (REQUIRED, List of ''TExtensionCode'')\n::- Each string in the list MUST be a valid extension code. The meaning\n  of each code is described independently as part of the extension\n  definition. A miner SHOULD advertise all its available features.\n\n* '''extension-parameters''' (REQUIRED, ''Map of (String -> Any)'')\n::- Parameters of the requested/allowed extensions from the first\n  parameter.\n\n\n'''Return value''':\n\n* ''Map of (String -> Any)''\n::- Each code from the '''extensions''' list MUST have a defined return\n  value (''TExtensionCode'' -> ''TExtensionResult''). This way the\n  miner knows if the extension is activated or not. E.g.\n  <code>{\"version-rolling\":false}</code> for unsupported version\n  rolling. ::- Some extensions need additional information to be\n  delivered to the miner. The return value map is used for this purpose.\n\n\nExample request (new-lines added):\n\n<pre>\n {\"method\": \"mining.configure\",\n  \"id\": 1,\n  \"params\": [[\"minimum-difficulty\", \"version-rolling\"],\n\t     {\"minimum-difficulty.value\": 2048,\n\t      \"version-rolling.mask\": \"1fffe000\",\n\"version-rolling.min-bit-count\": 2}]} </pre>\n\n(The miner requests extensions <code>\"version-rolling\"</code> and\n<code>\"minimum-difficulty\"</code>. It sets the parameters according to\nthe extensions' definitions.)\n\nExample result (new-lines added):\n\n<pre>\n {\"error\": null,\n  \"id\": 1,\n  \"result\": {\"version-rolling\": true,\n\t     \"version-rolling.mask\": \"18000000\",\n\t     \"minimum-difficulty\": true}}\n</pre>\n\n=Defined extensions=\n\n==Extension \"version-rolling\"==\n\nThis extension allows the miner to change the value of some bits in the\nversion field in the block header. Currently there are no standard bits\nused for version rolling so they need to be negotiated between a\nminer and a server.\n\nA miner sends the server a mask describing bits which the miner is\ncapable of changing. 1 = changeable bit, 0 = not changeable\n(<code>miner_mask</code>) and a minimum number of bits that it needs\nfor efficient version rolling.\n\nA server typically allows you to change only some of the version bits\n(<code>server_mask</code>) and the rest of the version bits are\nfixed. E.g. because the block needs to be valid or some signaling is\nin place.\n\nThe server responds to the configuration message by sending a mask\nwith common bits intersection of the miner's mask and its a mask\n(<code>response = server_mask & miner_mask</code>)\n\nExample request (a miner capable of changing any 2 bits from a 16-bit\nmask):\n\n {\"method\": \"mining.configure\", \"id\": 1, \"params\":\n [[\"version-rolling\"], {\"version-rolling.mask\": \"1fffe000\",\n \"version-rolling.min-bit-count\": 2}]}\n\n\nExample result (success):\n\n {\"error\": null, \"id\": 1, \"result\": {\"version-rolling\": true,\n \"version-rolling.mask\": \"18000000\"}}\n\n\nExample result (unknown extension):\n\n {\"error\": null, \"id\": 1, \"result\": {\"version-rolling\": false}}\n\n\n'''Extension parameters''':\n\n* '''\"version-rolling.mask\"''' (OPTIONAL, ''TMask'', default value\n  <code>\"ffffffff\"</code>) ::- Bits set to 1 can be changed by the\n  miner. This value is expected to be stable for the whole mining\n  session. A miner doesn't have to send the mask, in this case a\n  default full mask is used.\n\n'''Extension return values''':\n\n* '''\"version-rolling\"''' (REQUIRED, ''TExtensionResult'')\n::- When responded with <code>true</code>, the server will accept new\n  parameter of '''\"mining.submit\"''', see later.\n\n* '''\"version-rolling.mask\"''' (REQUIRED, ''TMask'')\n::- Bits set to 1 are allowed to be changed by the miner. If a miner\n  changes bits with mask value 0, the server will reject the\n  submit. ::- The server SHOULD return the largest mask possible (as\n  many bits set to 1 as possible). This can be useful in a mining proxy\n  setup when a proxy needs to negotiate the best mask for its future\n  clients. There is a [Draft\n  BIP](https://github.com/bitcoin/bips/pull/661/files) describing\n  available nVersion bits. The server SHOULD pick a mask that\n  preferably covers all bits specified in the BIP.\n\n* '''\"version-rolling.min-bit-count\"''' (REQUIRED, ''TMask'')\n::- The miner also provides a minimum number of bits that it needs for\n  efficient version rolling in hardware. Note that this parameter\n  provides important diagnostic information to the pool server. If the\n  requested bit count exceeds the limit of the pool server, the miner\n  always has the chance to operate in a degraded mode without using\n  full hashing power. The pool server SHOULD NOT terminate miner\n  connection if this rare mismatch case occurs.\n\n===Notification '''\"mining.set_version_mask\"'''===\n\nServer notifies the miner about a new mask valid for the\nconnection. This message can be sent at any time after the successful\nsetup of the version rolling extension by the \"mining.configure\"\nmessage. The new mask is valid '''immediately''', so that the server\ndoesn't wait for the next job.\n\n\n'''Parameters''':\n\n* ''mask'' (REQUIRED, ''TMask''): The meaning is the same as the\n  '''\"version-rolling.mask\"''' return parameter.\n\nExample:\n\n {\"params\":[\"00003000\"], \"id\":null, \"method\": \"mining.set_version_mask\"}\n\n\n===Changes in request '''\"mining.submit\"'''===\n\nImmediately after successful activation of the version-rolling extension\n(result to '''\"mining.configure\"''' sent by server), the server MUST\naccept an additional parameter of the message '''\"mining.submit\"'''.\nThe client MUST send one additional parameter, '''version_bits''' (6th\nparameter, after ''worker_name'', ''job_id'', ''extranonce2'',\n''ntime'' and ''nonce'').\n\n\n'''Additional parameters''':\n\n* ''version_bits'' (REQUIRED, ''TMask'') - Version bits set by miner.\n::- Miner can set only bits corresponding to the set bits in the last\n  received mask from the server either as response to\n  \"mining.configure\" or \"mining.set_version_mask\" notification\n  (<code>last_mask</code>). This must hold: version_bits & ~last_mask\n  ==  0 ::- The server computes ''nVersion'' for the submit as follows:\n  nVersion = (job_version & ~last_mask) | (version_bits & last_mask)\n  where <code>job_version</code> is the block version sent to miner as\n  part of job with id <code>job_id</code>.\n\n==Extension \"minimum-difficulty\"==\n\nThis extension allows miner to request a minimum difficulty for the\nconnected machine. It solves a problem in the original stratum\nprotocol where there is no way how to communicate hard limit of the\nconnected device.\n\n'''Extension parameters''':\n* '''\"minimum-difficulty.value\"''' (REQUIRED, ''Integer/Float'', >= 0)\n::- The minimum difficulty value acceptable for the miner/connection.\nThe value can be 0 for essentially disabling the feature.\n\n'''Extension return values''':\n* '''\"minimum-difficulty\"''' (REQUIRED, ''TExtensionResult'')\n::- Whether the minimum difficulty was accepted or not.\n::- This extension can be configured multiple times by calling\n\"mining.configure\" with \"minimum-difficulty\" code again.\n\n\n==Extension \"subscribe-extranonce\"==\n\nParameter-less extension. Miner advertises its capability of receiving\nmessage '''\"mining.set_extranonce\"''' message (useful for hash rate\nrouting scenarios).\n\n==Extension \"info\"==\n\nMiner provides additional text-based information.\n\n'''Extension parameters''':\n* '''\"info.connection-url\"''' (OPTIONAL, ''String'')\n::- Exact URL used by the mining software to connect to the stratum\nserver.\n\n* '''\"info.hw-version\"''' (OPTIONAL, ''String'')\n::- Manufacturer specific hardware revision string.\n\n* '''\"info.sw-version\"''' (OPTIONAL, ''String'')\n::- Manufacturer specific software version\n\n* '''\"info.hw-id\"''' (OPTIONAL, ''String'')\n::- Unique  identifier of the mining device\n\n==Copyright==\n\nThis document is dual licensed as BSD 3-clause, and Creative Commons\nCC0 1.0 Universal.\n\n\n-- \nCEO Braiins Systems | Slushpool.com\nemail: jan.capek at braiins.cz\nhttp://braiins.cz\nhttp://slushpool.com\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: not available\nType: application/pgp-signature\nSize: 819 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180517/8f71b2a6/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "stratum protocol extension - mining.configure, formal version rolling and other extensions",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jan \u010capek"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 13379
        }
    },
    {
        "title": "[bitcoin-dev] [bitcoin-discuss] Checkpoints in the Blockchain.",
        "thread_messages": [
            {
                "author": "Damian Williamson",
                "date": "2018-05-20T05:12:23",
                "message_text_only": "I do understand your point, however, 'something like stuxnet' cannot be used to create valid data without re-doing all the PoW. Provided some valid copies of the blockchain continue to exist, the network can re-synchronise.\n\n\nUnrelated, it would seem useful to have some kind of deep blockchain corruption recovery mechanism if it does not exist; where blocks are altered at a depth exceeding the re-scan on init, efficient recovery is possible on detection. Presumably, it would be possible for some stuxnet like thing to modify blocks by modifying the table data making blocks invalid but without causing a table corruption. I would also suppose that if the node is delving deep into the blockchain for transaction data, that is would also validate the block at least that it has a valid hash (apart from Merkle tree validation for the individual transaction?) and that the hash of its immediate ancestor is also valid.\n\n\nRegards,\n\nDamian Williamson\n\n\n________________________________\nFrom: bitcoin-discuss-bounces at lists.linuxfoundation.org <bitcoin-discuss-bounces at lists.linuxfoundation.org> on behalf of Dave Scotese via bitcoin-discuss <bitcoin-discuss at lists.linuxfoundation.org>\nSent: Sunday, 20 May 2018 11:58 AM\nTo: Scott Roberts\nCc: Bitcoin Discuss\nSubject: Re: [bitcoin-discuss] Checkpoints in the Blockchain.\n\nI wouldn't limit my rendering to words, but that is a decent starting point.  The richer the rendering, the harder it will be to forget, but it needn't all be developed at once. My goal here is to inspire the creation of art that is, at the same time, highly useful and based on randomness.\n\nAnyway, I asked what \"premise that this is needed\" you meant and I still don't know the answer.\n\n\"The archive is a shared memory\" - yes, a shared computer memory, and growing larger (ie more cumbersome) every day. If something like stuxnet is used to change a lot of the copies of it at some point, it seems likely that people will notice a change, but which history is correct won't be so obvious because for the humans whose memories are not so easily overwritten, computer data is remarkably non-memorable in it's normal form (0-9,a-f, whatever).  If we ever want to abandon the historical transaction data, having a shared memory of the state of a recent UTXO Set will help to obviate the need for it.  Yes, of course the blockchain is the perfect solution, as long as there is exactly one and everyone can see that it's the same one that everyone else sees.  Any other number of archives presents a great difficulty.\n\nIn that scenario, there's no other way to prove that the starting point is valid.  Bitcoin has included a hardcoded-checkpoint in the code which served the same purpose, but this ignores the possibility that two versions of the code could be created, one with a fake checkpoint that is useful to a powerful attacker.  If the checkpoint were rendered into something memorable at the first opportunity, there would be little question about which one is correct when the difference is discovered.\n\nOn Sat, May 19, 2018 at 5:22 PM, Scott Roberts <wordsgalore at gmail.com<mailto:wordsgalore at gmail.com>> wrote:\nI just don't see the point of needing to know it any different from the hex value. Or maybe I should say I can't imagine it being useful because I can't imagine what you're after is possible. There might be a theoretical proof that what you're after is impossible. Hard to forget is almost the opposite of many options and what we're trying to do is decide between many options. I'll assume English because  it's the only starting point I have that's even in the ballpark of being possible. You might need to constrain the word selection and the structure in which you put it. I can only imagine that you are talking about putting the words into some sort of story. The only kind of story that would be hard to forget  is one that  fits into an overall structure that we are familiar with but those types of structures are few  compared to the possibilities that we're trying to encode. \"Hard to deny\" is a type of \"hard to forget\". Besides trying to connect it to external reality or history that we can all agree on there is also an internal consistency that could be used like a checksum such as the structure I mentioned. The only thing that seems to fit the bill is the blockchain itself. It's on everyone's computer so it's impossible to forget and it has internal consistency. Is the only shared memory we have that can't be subject to a Sybil attack or other hijacking of our memory of the true history. Our translation of the hash into words and a story could potentially be subject to hijacking if it's not done perfectly correct. It just seems best to me to use the hash itself. They archived existence of the prior parts of the blockchain are what make that particular hash hard to forget. Supposedly it can't be forged to reference  a fake history. The archive is a shared memory that fits the encoding rules.\n\nOn Sat, May 19, 2018, 4:30 PM Dave Scotese <dscotese at litmocracy.com<mailto:dscotese at litmocracy.com>> wrote:\nDid you mean the premise that we have \"the need to retain the full blockchain in order to validate the UTXO Set\"?\n\nI hadn't thought of just making it easier to remember, as your suggestion does (12-13 words), and that's a great idea.  I have passphrases of that kind, but I noticed a kind of mandela effect<https://www.snopes.com/news/2016/07/24/the-mandela-effect/> just with myself.  For example, I one of the words I chose was like \"olfactory\" but after a few months, what I remembered was like \"ontology\". The solution I came up with is to couple the data with far more items than we normally do.  Every ten minutes, we get a new set of 256 bits that can be used to create something potential very difficult to forget, and that's what I'm after.\n\nAn algorithm could be used to do this with the Bip39 word list.  If we categorized the words according to parts of speech, the bits could be used to determine which word goes next in a kind of ad-lib, but this creates a phrase that is only memorable in the language for which the algorithm is developed.  As a thought experiment, I'll try adjective noun verb(as past tense) noun preposition adjective adjective noun: Stuff would come out like \"Able abstract abused accident across actual adult affair.\"  not memorable at all, but sense can be made of it and sometimes the sense will be remarkable.  As it is, I will have forgotten it in an hour or two.\n\nOn Thu, May 17, 2018 at 4:29 PM, Scott Roberts <wordsgalore at gmail.com<mailto:wordsgalore at gmail.com>> wrote:\nI disagree with your premise that this is needed, I like the question. Humans are experts at language and I don't think we have another repository at hand that we can categorize for memory that is better than words. Using words is a common way of doing what you're thinking about. If the checkpoint could be a hash that meets a difficulty target the possibilities are 2^182 at the current hashrate instead of 2^256. So we need only 12 or 13 common words with their various possible endings (30,000).  I would find it easier to insert  a letter and 3 numbers after every 2 words so there would be only 8 words used. There are probably other tricks people have figured out but there can't be any kind of advanced encoding because it wouldn't benefit more than one word. There might be a way to convert the words into something that almost sounds like English sentences but it would probably come out a cost of at least doubling the number of words.\n\nOn Thu, May 17, 2018, 12:13 PM Dave Scotese via bitcoin-discuss <bitcoin-discuss at lists.linuxfoundation.org<mailto:bitcoin-discuss at lists.linuxfoundation.org>> wrote:\nI got the idea that a SHA256 hash could be rendered into something difficult to forget.  The rendering would involve using each of the 256 bits to specify something important about the rendering - important in an instinctive human-memory way.\n\nLet's assume such a rendering is possible, and that at any time, any person can execute the rendering against the SHA256 hash of a consistent representation of the UTXO Set.  Sometimes, someone will execute the rendering and discover that it is remarkable in some way (making it even more memorable), and therefore will publish it.\n\nThe published, memorable rendering now becomes a kind of protection against any possible re-writing of the blockchain from any point prior to that UTXO Set.  When everyone involved in Bitcoin recognizes this protection, it relieves us of the need to retain the full blockchain in order to validate the UTXO Set at that point, because enough people will recognize it, and it can be validated without reference to any kind of prior computer record.\n\nThis does leave open the possibility that an attacker could create a more favorable UTXO Set that happens to have a rendering similar enough to fool people, or one that has exactly the same SHA256-hash, but that possibility is remote enough to ignore (just as we all ignore the possibility that whatever creates the master seed for our HD wallet will create a unique master seed).\n\nI've been working on how such a rendering could happen.  It could describe music, characters, colors, plot points, memorable elements of characters, etc.\n\nDave Scotese\n\n_______________________________________________\nbitcoin-discuss mailing list\nbitcoin-discuss at lists.linuxfoundation.org<mailto:bitcoin-discuss at lists.linuxfoundation.org>\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-discuss\n\n\n\n--\nI like to provide some work at no charge to prove my value. Do you need a techie?\nI own Litmocracy<http://www.litmocracy.com> and Meme Racing<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist<http://www.voluntaryist.com> which now accepts Bitcoin.\nI also code for The Dollar Vigilante<http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi Nakamoto\n\n\n\n--\nI like to provide some work at no charge to prove my value. Do you need a techie?\nI own Litmocracy<http://www.litmocracy.com> and Meme Racing<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist<http://www.voluntaryist.com> which now accepts Bitcoin.\nI also code for The Dollar Vigilante<http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi Nakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180520/8557544e/attachment-0001.html>"
            },
            {
                "author": "Dave Scotese",
                "date": "2018-05-21T20:03:37",
                "message_text_only": "Our wetware memory is faulty at details, but a rendering that provides\nfeatures at which it isn't faulty makes it a decent backup in situations\nwhere technology has been used to hide important differences from us. Some\nof us may recall being in a situation where something seems off, and we\nstart to investigate, and then we discover that something was off.  April\nFools jokes are good examples, as well as satire and news reports from The\nOnion.\n\nThe point of storing the entire blockchain history is to prevent any of\nthat history being changed in a way that illicitly alters the UTXO Set.\nWhenever a memorable enough rendering of the UTXO Set is produced (which\nhas happened exactly once - when Bitcoin started, it was empty, and after\nthat, it just looks like a bunch of random computer data), the risk of\naltering the history before it goes up, even if you have the computing\npower to make subsequent block headers follow all the rules (and even to\nsuccessfully execute a 51% attack!).  In this announcement\n<http://lists.mindrot.org/pipermail/openssh-unix-dev/2008-July/026693.html>,\nthe first item under \"new features\" has this, which follows the same\nprinciple as my idea:\n\nIntroduce experimental SSH Fingerprint ASCII Visualisation to ssh(1) and\n> ssh-keygen(1). Visual fingerprinnt display is controlled by a new\n> ssh_config(5) option \"VisualHostKey\". The intent is to render SSH host keys\n> in a visual form that is amenable to easy recall and rejection of changed\n> host keys. This technique inspired by the graphical hash visualisation\n> schemes known as \"random art[*]\", and by Dan Kaminsky's musings at 23C3 in\n> Berlin.\n>\n\n\n\nOn Sat, May 19, 2018 at 10:12 PM, Damian Williamson <willtech at live.com.au>\nwrote:\n\n> I do understand your point, however, 'something like stuxnet' cannot be\n> used to create valid data without re-doing all the PoW. Provided some valid\n> copies of the blockchain continue to exist, the network can re-synchronise.\n>\n>\n> Unrelated, it would seem useful to have some kind of deep blockchain\n> corruption recovery mechanism if it does not exist; where blocks are\n> altered at a depth exceeding the re-scan on init, efficient recovery is\n> possible *on detection*. Presumably, it would be possible for some\n> stuxnet like thing to modify blocks by modifying the table data making\n> blocks invalid but without causing a table corruption. I would also suppose\n> that if the node is delving deep into the blockchain for transaction data,\n> that is would also validate the block at least that it has a valid hash\n> (apart from Merkle tree validation for the individual transaction?) and\n> that the hash of its immediate ancestor is also valid.\n>\n>\n> Regards,\n>\n> Damian Williamson\n>\n>\n> ------------------------------\n> *From:* bitcoin-discuss-bounces at lists.linuxfoundation.org <\n> bitcoin-discuss-bounces at lists.linuxfoundation.org> on behalf of Dave\n> Scotese via bitcoin-discuss <bitcoin-discuss at lists.linuxfoundation.org>\n> *Sent:* Sunday, 20 May 2018 11:58 AM\n> *To:* Scott Roberts\n> *Cc:* Bitcoin Discuss\n> *Subject:* Re: [bitcoin-discuss] Checkpoints in the Blockchain.\n>\n> I wouldn't limit my rendering to words, but that is a decent starting\n> point.  The richer the rendering, the harder it will be to forget, but it\n> needn't all be developed at once. My goal here is to inspire the creation\n> of art that is, at the same time, highly useful and based on randomness.\n>\n> Anyway, I asked what \"premise that this is needed\" you meant and I still\n> don't know the answer.\n>\n> \"The archive is a shared memory\" - yes, a shared *computer* memory, and\n> growing larger (ie more cumbersome) every day. If something like stuxnet is\n> used to change a lot of the copies of it at some point, it seems likely\n> that people will notice a change, but which history is correct won't be so\n> obvious because for the *humans* whose memories are not so easily\n> overwritten, computer data is remarkably non-memorable in it's normal form\n> (0-9,a-f, whatever).  If we ever want to abandon the historical transaction\n> data, having a shared memory of the state of a recent UTXO Set will help to\n> obviate the need for it.  Yes, of course the blockchain is the perfect\n> solution, as long as there is *exactly one* and everyone can see that\n> it's the same one that everyone else sees.  Any other number of archives\n> presents a great difficulty.\n>\n> In that scenario, there's no other way to prove that the starting point is\n> valid.  Bitcoin has included a hardcoded-checkpoint in the code which\n> served the same purpose, but this ignores the possibility that two versions\n> of the code could be created, one with a fake checkpoint that is useful to\n> a powerful attacker.  If the checkpoint were rendered into something\n> memorable at the first opportunity, there would be little question about\n> which one is correct when the difference is discovered.\n>\n> On Sat, May 19, 2018 at 5:22 PM, Scott Roberts <wordsgalore at gmail.com>\n> wrote:\n>\n> I just don't see the point of needing to know it any different from the\n> hex value. Or maybe I should say I can't imagine it being useful because I\n> can't imagine what you're after is possible. There might be a theoretical\n> proof that what you're after is impossible. Hard to forget is almost the\n> opposite of many options and what we're trying to do is decide between many\n> options. I'll assume English because  it's the only starting point I have\n> that's even in the ballpark of being possible. You might need to constrain\n> the word selection and the structure in which you put it. I can only\n> imagine that you are talking about putting the words into some sort of\n> story. The only kind of story that would be hard to forget  is one that\n> fits into an overall structure that we are familiar with but those types of\n> structures are few  compared to the possibilities that we're trying to\n> encode. \"Hard to deny\" is a type of \"hard to forget\". Besides trying to\n> connect it to external reality or history that we can all agree on there is\n> also an internal consistency that could be used like a checksum such as the\n> structure I mentioned. The only thing that seems to fit the bill is the\n> blockchain itself. It's on everyone's computer so it's impossible to forget\n> and it has internal consistency. Is the only shared memory we have that\n> can't be subject to a Sybil attack or other hijacking of our memory of the\n> true history. Our translation of the hash into words and a story could\n> potentially be subject to hijacking if it's not done perfectly correct. It\n> just seems best to me to use the hash itself. They archived existence of\n> the prior parts of the blockchain are what make that particular hash hard\n> to forget. Supposedly it can't be forged to reference  a fake history. The\n> archive is a shared memory that fits the encoding rules.\n>\n> On Sat, May 19, 2018, 4:30 PM Dave Scotese <dscotese at litmocracy.com>\n> wrote:\n>\n> Did you mean the premise that we have \"the need to retain the full\n> blockchain in order to validate the UTXO Set\"?\n>\n> I hadn't thought of just making it *easier* to remember, as your\n> suggestion does (12-13 words), and that's a great idea.  I have passphrases\n> of that kind, but I noticed a kind of mandela effect\n> <https://www.snopes.com/news/2016/07/24/the-mandela-effect/> just with\n> myself.  For example, I one of the words I chose was like \"olfactory\" but\n> after a few months, what I remembered was like \"ontology\". The solution I\n> came up with is to couple the data with far more items than we normally\n> do.  Every ten minutes, we get a new set of 256 bits that can be used to\n> create something potential *very difficult* to forget, and that's what\n> I'm after.\n>\n> An algorithm could be used to do this with the Bip39 word list.  If we\n> categorized the words according to parts of speech, the bits could be used\n> to determine which word goes next in a kind of ad-lib, but this creates a\n> phrase that is only memorable in the language for which the algorithm is\n> developed.  As a thought experiment, I'll try adjective noun verb(as past\n> tense) noun preposition adjective adjective noun: Stuff would come out like\n> \"Able abstract abused accident across actual adult affair.\"  not memorable\n> at all, but sense can be made of it and sometimes the sense will be\n> remarkable.  As it is, I will have forgotten it in an hour or two.\n>\n> On Thu, May 17, 2018 at 4:29 PM, Scott Roberts <wordsgalore at gmail.com>\n> wrote:\n>\n> I disagree with your premise that this is needed, I like the question.\n> Humans are experts at language and I don't think we have another repository\n> at hand that we can categorize for memory that is better than words. Using\n> words is a common way of doing what you're thinking about. If the\n> checkpoint could be a hash that meets a difficulty target the possibilities\n> are 2^182 at the current hashrate instead of 2^256. So we need only 12 or\n> 13 common words with their various possible endings (30,000).  I would find\n> it easier to insert  a letter and 3 numbers after every 2 words so there\n> would be only 8 words used. There are probably other tricks people have\n> figured out but there can't be any kind of advanced encoding because it\n> wouldn't benefit more than one word. There might be a way to convert the\n> words into something that almost sounds like English sentences but it would\n> probably come out a cost of at least doubling the number of words.\n>\n> On Thu, May 17, 2018, 12:13 PM Dave Scotese via bitcoin-discuss <\n> bitcoin-discuss at lists.linuxfoundation.org> wrote:\n>\n> I got the idea that a SHA256 hash could be rendered into something\n> difficult to forget.  The rendering would involve using each of the 256\n> bits to specify something important about the rendering - important in an\n> instinctive human-memory way.\n>\n> Let's assume such a rendering is possible, and that at any time, any\n> person can execute the rendering against the SHA256 hash of a consistent\n> representation of the UTXO Set.  Sometimes, someone will execute the\n> rendering and discover that it is remarkable in some way (making it even\n> more memorable), and therefore will publish it.\n>\n> The published, memorable rendering now becomes a kind of protection\n> against any possible re-writing of the blockchain from any point prior to\n> that UTXO Set.  When everyone involved in Bitcoin recognizes this\n> protection, it relieves us of the need to retain the full blockchain in\n> order to validate the UTXO Set at that point, because enough people will\n> recognize it, and it can be validated without reference to any kind of\n> prior computer record.\n>\n> This does leave open the possibility that an attacker could create a more\n> favorable UTXO Set that happens to have a rendering similar enough to fool\n> people, or one that has exactly the same SHA256-hash, but that possibility\n> is remote enough to ignore (just as we all ignore the possibility that\n> whatever creates the master seed for our HD wallet will create a unique\n> master seed).\n>\n> I've been working on how such a rendering could happen.  It could describe\n> music, characters, colors, plot points, memorable elements of characters,\n> etc.\n>\n> Dave Scotese\n>\n> _______________________________________________\n> bitcoin-discuss mailing list\n> bitcoin-discuss at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-discuss\n>\n>\n>\n>\n> --\n> I like to provide some work at no charge to prove my value. Do you need a\n> techie?\n> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n> <http://www.memeracing.net> (in alpha).\n> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n> which now accepts Bitcoin.\n> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n> \"He ought to find it more profitable to play by the rules\" - Satoshi\n> Nakamoto\n>\n>\n>\n>\n> --\n> I like to provide some work at no charge to prove my value. Do you need a\n> techie?\n> I own Litmocracy <http://www.litmocracy.com> and Meme Racing\n> <http://www.memeracing.net> (in alpha).\n> I'm the webmaster for The Voluntaryist <http://www.voluntaryist.com>\n> which now accepts Bitcoin.\n> I also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n> \"He ought to find it more profitable to play by the rules\" - Satoshi\n> Nakamoto\n>\n\n\n\n-- \nI like to provide some work at no charge to prove my value. Do you need a\ntechie?\nI own Litmocracy <http://www.litmocracy.com> and Meme Racing\n<http://www.memeracing.net> (in alpha).\nI'm the webmaster for The Voluntaryist <http://www.voluntaryist.com> which\nnow accepts Bitcoin.\nI also code for The Dollar Vigilante <http://dollarvigilante.com/>.\n\"He ought to find it more profitable to play by the rules\" - Satoshi\nNakamoto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180521/649dd8e4/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Checkpoints in the Blockchain.",
            "categories": [
                "bitcoin-dev",
                "bitcoin-discuss"
            ],
            "authors": [
                "Damian Williamson",
                "Dave Scotese"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 23534
        }
    },
    {
        "title": "[bitcoin-dev] Lightning Channel Incentive Scheme-Seeking Feedback",
        "thread_messages": [
            {
                "author": "Chenxi Cai",
                "date": "2018-05-22T10:35:06",
                "message_text_only": "Hi All,\n\n\nI have been working on a proposal to incentivize lightning channels. Please read and give some feedback.\n\n\nThanks,\n\nChenxi\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180522/1fc2d349/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: Hey Chain.pdf\nType: application/pdf\nSize: 169019 bytes\nDesc: Hey Chain.pdf\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180522/1fc2d349/attachment-0001.pdf>"
            }
        ],
        "thread_summary": {
            "title": "Lightning Channel Incentive Scheme-Seeking Feedback",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chenxi Cai"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 591
        }
    },
    {
        "title": "[bitcoin-dev] Should Graftroot be optional?",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2018-05-22T18:17:42",
                "message_text_only": "Hello all,\n\nGiven the recent discussions about Taproot [1] and Graftroot [2], I\nwas wondering if a practical deployment needs a way to explicitly\nenable or disable the Graftroot spending path. I have no strong\nreasons why this would be necessary, but I'd like to hear other\npeople's thoughts.\n\nAs a refresher, the idea is that a script type could exists which\nlooks like a pubkey Q, but can be spent either:\n* By signing the spending transaction directly using Q (key spending)\n* By proving Q was derived as Q = P + H(P,S)*G, with a script S and\nits inputs (Taproot script spending).\n* By signing a script S using Q, and providing S's inputs (Graftroot\nscript spending).\n\nOverall, these constructions let us create combined\npay-to-pubkey-or-script outputs that are indistinguishable, and don't\neven reveal a script spending path existed in the first place when the\nkey spending path is used. The two approaches ((T)aproot and\n(G)raftroot) for the script spending path have different trade-offs:\n* T outputs can be derived noninteractively from key and scripts; G\noutputs need an interaction phase where the key owner(s) sign off on\nthe potential script spending paths.\n* T lets you prove what all the different spending paths are.\n* T without any other technology only needs to reveal an additional\npoint when spending a single script; G needs half-aggregated\nsignatures [3] to achieve the same, which complicates design (see\n[4]).\n* G is more compact when dealing with many spending paths (O(1) in the\nnumber of spending paths), while T spends need to be combined with\nMerkle branches to deal with large number of spends (and even then is\nstill O(log n)).\n* G spending paths can be added after the output is created; T\nrequires them be fixed at output creation time.\n\nMy question is whether it is safe to always permit both types of\nscript spending paths, or an explicit commitment to whether Graftroot\nis permitted is necessary. In theory, it seems that this shouldn't be\nneeded: the key owners are always capable of spending the funds\nanyway, so them choosing to delegate to others shouldn't enable\nanything that isn't\npossible by the key owners already.\n\nThere are a few concerns, however:\n\n* Accidentally (participating in) signing a script may have more broad\nconsequences. Without Graftroot, that effect is limited to a single\ntransaction with specific inputs and outputs, and only as long as all\nthose inputs are unspent. A similar but weaker concern exists for\nSIGHASH_NOINPUT.\n\n* In a multisignature setting (where the top level key is an aggregate\nof multiple participants), the above translates to the ability for a\n(threshold satsisfying) subset of participants being able to (possibly\npermanently) remove others from the set of signers (rather than for a\nsingle output).\n\n* In a situation where private keys are stored in an HSM, without\nGraftroot an attacker needs access to the device and convince it to\nsign for every output he wants to steal (assuming the HSM prevents\nleaking private keys). With Graftroot, the HSM may be tricked into\nsigning a script that does not include itself. Arguably, in a\nGraftroot setting such an HSM would need a degree of protection\nsimilar to not leaking private keys applied to not signing scripts,\nbut this may be less obvious.\n\nOverall, none of these are convincing points, but they do make me\nuncomfortable about the effect the Graftroot spending path may have on\nsome use cases. Given that Taproot/Graftroot's primary advantage is\nincreasing fungibility by making all outputs look identical, it seems\ngood to discuss potential reasons such outputs couldn't or wouldn't be\nadopted in certain applications.\n\n  [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-January/015614.html\n  [2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-February/015700.html\n  [3] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-May/014308.html\n  [4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-March/015838.html\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2018-05-23T06:15:03",
                "message_text_only": "Good morning Pieter and list,\n\nIt seems to me, naively, that it would be better to make Graftroot optional, and to somehow combine Taproot and Graftroot.\n\nSo I propose that the Taproot equation be modified to the below:\n\n    Q = P + H(P, g, S) * G\n\nWhere `g` is the \"Graftroot flag\", i.e. 0 if disable Graftroot, and 1 if enable Graftroot.\n\nA Graftroot spend would need to reveal P and the Taproot script S, then sign the Graftroot script using P (rather than Q).\n\nIf an output wants to use Graftroot but not Taproot, then it uses Q = P + H(P, 1, {OP_FALSE}) * G, meaning the Taproot script can never succeed.  Then Graftroot scripts need to be signed using P.\n\nA simple wallet software (or hardware) that only cares about spending using keys `Q = q * G` it controls does not have to worry about accidentally signing a Graftroot script, since Q is not used to sign Graftroot scripts and it would be \"impossible\" to derive a P + H(P, 1, S) * G from Q (using the same argument that it is \"impossible\" to derive a P + H(P, S) * G from Q in Taproot).\n\nIn a multisignature setting, it would not be possible (I think) to generate a single private key p1 + H(P1 + P2, 1, {<p1*G> OP_CHECKSIG}) that can be used to kick out P2, since that would be signature cancellation attack by another path.\n\nThis increases the cost of Graftroot by one point P and a Taproot script (which could be just `OP_FALSE` if Taproot is not desired).  In addition, if both Taproot and Graftroot are used, then using any Graftroot branch will reveal the existence of a Taproot script.  Similarly, using the Taproot branch reveals whether or not we also had some (hidden) Graftroot branch.\n\n--\n\nNow the above has the massive privacy loss where using Taproot reveals whether or not you intended to use Graftroot too, and using Graftroot reveals whether or not you intended to use Taproot.\n\nSo now let us consider the equation below instead:\n\n    Q = P + H(P, H(sign(P, g)), H(S)) * G\n\nA Taproot spend reveals P, H(sign(P,g)), and S, and the witness that makes S succeed.\n\nA Graftroot spend reveals P, sign(P, 1), H(S), and sign(P, Sg), and the witness that makes Sg succeed.\n\nIf we want to use Graftroot but not Taproot, then we can agree on the script S = `push(random 256-bit) OP_FALSE`, which can never be made to succeed.  On spending using Taproot, we reveal H(S) but not the S.  Nobody can now distinguish between this and a Graftroot+Taproot spent using Graftroot.  We only need to store H(S), not the original S (but we do need to verify that the original S follows the correct template above).\n\nIf we want to use Taproot but not Graftroot, then we can agree to do a `sign(P, 0)`, which definitely cannot be used to perform a Graftroot spend.  The act of signing requires a random nonce to be generated, hence making the signature itself random.  On spending using Graftroot, we reveal H(sign(P, 0)) but not the signature itself.  Nobody can now distinguish between this and a Graftroot+Taproot spent using Taproot.  We only need to store H(sign(P, 0)), not the original signature (but we do need to verify(P, sign(P, 0))).  Some other way of obfuscating the flag can be done, such as H(g, random), with the parties to the contract agreeing on the random obfuscation (but I am unsure of the safety of that).\n\nIn effect, instead of the Taproot contract S, we use as contract a one-level Merkle tree, with one branch being an enable/disable of Graftroot and the other branch being an ordinary Script.\n\nNote that even if we are fooled into signing a message sign(P, 1), as long as we made sure that the output paid to a Q = P + H(P, H(sign(p, 0)), H(S)) * G in the first place, it cannot be used after-the-fact to make a non-Graftroot output a Graftroot output.\n\nSimple wallets that use Q = q * G need not worry whether signing arbitrary messages with that key might suddenly make their outputs spendable as Graftroot.\n\nThis increases Taproot spends by a hash.\n\nThis increases Graftroot spends by a point, a signature, and a hash.\n\n--\n\nI am not a mathematician and the above could be complete bunk.\n\n--\n\nThe above also does not actually answer the question.\n\nMany users of Bitcoin have been depending on the ability to sign arbitrary messages using a public key that is also used to protect funds.  The use is common enough that people are asking for it for SegWit addresses: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-March/015818.html\n\nNow it might be possible that a valid Script can be shown as an ordinary ASCII text file containing some benign-looking message.  For example a message starting with \"L\" is OP_PUSHDATA1 (76), the next character encodes a length.  So \"LA\" means 65 bytes of data, and the succeeding 65 bytes can be an arbitrary message (e.g. \"LARGE AMOUNTS OF WEALTH ARE SAFE TO STORE IN BITCOIN, DONCHA KNOW?\\n\").  Someone might challenge some fund owner to prove their control of some UTXO by signing such a message.  Unbeknownst to the signer, the message is actually also a valid Script (`OP_PUSHDATA1(65 random bytes)`) that lets the challenger trivially acquire access to the funds via Graftroot.\n\nThus I think this is a valid concern and we should indeed make Graftroot be optional, and also ensure that the simple-signing case will not be a vulnerability for ordinary wallets, while keeping the property that use of Taproot and Graftroot is invisible if the onchain spend does not involve Taproot/Graftroot.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2018-05-23T13:50:13",
                "message_text_only": "On Tue, May 22, 2018 at 11:17:42AM -0700, Pieter Wuille via bitcoin-dev wrote:\n> \n> Given the recent discussions about Taproot [1] and Graftroot [2], I\n> was wondering if a practical deployment needs a way to explicitly\n> enable or disable the Graftroot spending path. I have no strong\n> reasons why this would be necessary, but I'd like to hear other\n> people's thoughts.\n>\n\nGraftroot also break blind signature schemes. Consider a protocol such as [1]\nwhere some party has a bunch of UTXOs all controlled (in part) by the same\nkey X. This party produces blind signatures on receipt of new funds, and can\nonly verify the number of signatures he produces, not anything about what he\nis signing.\n\nBTW, the same concern holds for SIGHASH_NOINPUT, which I'd also like to be\ndisable-able. Maybe we should extend one of ZmnSCPxj's suggestions to include\na free \"flags\" byte or two in the witness?\n\n(I also had the same concern about signature aggregation. It seems like it's\npretty hard to preserve the \"one signature = at most one input\" invariant of\nBitcoin, but I think it's important that it is preserved, at least for\noutputs that need it.)\n\nOr maybe, since it appears it will require a space hit to support optional\ngraftroot anyway, we should simply not include it in a proposal for Taproot,\nsince there would be no opportunity cost (in blockchain efficiency) to doing\nit later.\n\n[1] https://github.com/apoelstra/scriptless-scripts/pull/1 \n\n-- \nAndrew Poelstra\nMathematics Department, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\n\"A goose alone, I suppose, can know the loneliness of geese\n who can never find their peace,\n whether north or south or west or east\"\n       --Joanna Newsom\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/fe3efcc5/attachment.sig>"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2018-05-23T17:52:39",
                "message_text_only": "On Wed, May 23, 2018 at 01:50:13PM +0000, Andrew Poelstra via bitcoin-dev wrote:\n> \n> Graftroot also break blind signature schemes. Consider a protocol such as [1]\n> where some party has a bunch of UTXOs all controlled (in part) by the same\n> key X. This party produces blind signatures on receipt of new funds, and can\n> only verify the number of signatures he produces, not anything about what he\n> is signing.\n> \n> BTW, the same concern holds for SIGHASH_NOINPUT, which I'd also like to be\n> disable-able. Maybe we should extend one of ZmnSCPxj's suggestions to include\n> a free \"flags\" byte or two in the witness?\n> \n> (I also had the same concern about signature aggregation. It seems like it's\n> pretty hard to preserve the \"one signature = at most one input\" invariant of\n> Bitcoin, but I think it's important that it is preserved, at least for\n> outputs that need it.)\n> \n> Or maybe, since it appears it will require a space hit to support optional\n> graftroot anyway, we should simply not include it in a proposal for Taproot,\n> since there would be no opportunity cost (in blockchain efficiency) to doing\n> it later.\n> \n> [1] https://github.com/apoelstra/scriptless-scripts/pull/1 \n>\n\nOn further thought, I rescind this concern (ditto for SIGHASH_NOINPUT) (but\nnot for aggregate sigs, they still interact badly with blind signatures).\n\nAs long as graftroot (and NOINPUT) sigs commit to the public key, it is\npossible for a server to have unique keys for every output, even while\nretaining the same private key, and thereby ensure that \"one sig can spend\nonly one output\" holds. To do this, suppose the server has a BIP32 xpubkey\n(xG, cc). A blind signer using the private key x can be made to sign not\nonly for xG, but also for any publicly-derived child keys of (xG, cc).\n\nHere is a simple scheme that does this:\n\n  1. Signer provides a nonce R = kG\n\n  2. Challenger computes bip32 tweak h, chooses blinders alpha and beta,\n     and computes:\n         R' = R + alpha*G + beta*P\n         e  = H(P + hG || R' || tx)\n         e' = e + beta\n     and sends e' to the signer.\n\n  3. Signer replies with s = k + xe' (= k + beta*x + (x + h)e - he)\n\n  4. Challenger unblinds this as s' = s + alpha + he\n\n(This blind signature scheme is vulnerable to Wagner's attack, though see\nSchnorr 2004 [1] for mitigations that are perfectly compatible with this\nmodified BIP32ish scheme.)\n\nI'm unsure whether key-prefixing is _actually_ necessary for this, but it\nmakes the security argument much clearer since the messagehash contains\nsome data which can be made unique per-utxo and is committed in the chain.\n\n\nAndrew\n\n\n[1] http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=8ECEF929559865FD68D1D873555D18FE?doi=10.1.1.68.9836&rep=rep1&type=pdf\n\n\n-- \nAndrew Poelstra\nMathematics Department, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\n\"A goose alone, I suppose, can know the loneliness of geese\n who can never find their peace,\n whether north or south or west or east\"\n       --Joanna Newsom\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/5debf87f/attachment-0001.sig>"
            },
            {
                "author": "Johnson Lau",
                "date": "2018-05-25T09:46:29",
                "message_text_only": "While you have rescind your concern, I\u2019d like to point out that it\u2019s strictly a problem of SIGHASH_NOINPUT, not graftroot (or script delegation in general).\n\nFor example, we could modify graftroot. Instead of signing the (script), we require it to sign (outpoint | script). That means a graftroot signature would never be valid for more than one UTXO.\n\n> On 24 May 2018, at 1:52 AM, Andrew Poelstra via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> On Wed, May 23, 2018 at 01:50:13PM +0000, Andrew Poelstra via bitcoin-dev wrote:\n>> \n>> Graftroot also break blind signature schemes. Consider a protocol such as [1]\n>> where some party has a bunch of UTXOs all controlled (in part) by the same\n>> key X. This party produces blind signatures on receipt of new funds, and can\n>> only verify the number of signatures he produces, not anything about what he\n>> is signing.\n>> \n>> BTW, the same concern holds for SIGHASH_NOINPUT, which I'd also like to be\n>> disable-able. Maybe we should extend one of ZmnSCPxj's suggestions to include\n>> a free \"flags\" byte or two in the witness?\n>> \n>> (I also had the same concern about signature aggregation. It seems like it's\n>> pretty hard to preserve the \"one signature = at most one input\" invariant of\n>> Bitcoin, but I think it's important that it is preserved, at least for\n>> outputs that need it.)\n>> \n>> Or maybe, since it appears it will require a space hit to support optional\n>> graftroot anyway, we should simply not include it in a proposal for Taproot,\n>> since there would be no opportunity cost (in blockchain efficiency) to doing\n>> it later.\n>> \n>> [1] https://github.com/apoelstra/scriptless-scripts/pull/1 \n>> \n> \n> On further thought, I rescind this concern (ditto for SIGHASH_NOINPUT) (but\n> not for aggregate sigs, they still interact badly with blind signatures).\n> \n> As long as graftroot (and NOINPUT) sigs commit to the public key, it is\n> possible for a server to have unique keys for every output, even while\n> retaining the same private key, and thereby ensure that \"one sig can spend\n> only one output\" holds. To do this, suppose the server has a BIP32 xpubkey\n> (xG, cc). A blind signer using the private key x can be made to sign not\n> only for xG, but also for any publicly-derived child keys of (xG, cc).\n> \n> Here is a simple scheme that does this:\n> \n>  1. Signer provides a nonce R = kG\n> \n>  2. Challenger computes bip32 tweak h, chooses blinders alpha and beta,\n>     and computes:\n>         R' = R + alpha*G + beta*P\n>         e  = H(P + hG || R' || tx)\n>         e' = e + beta\n>     and sends e' to the signer.\n> \n>  3. Signer replies with s = k + xe' (= k + beta*x + (x + h)e - he)\n> \n>  4. Challenger unblinds this as s' = s + alpha + he\n> \n> (This blind signature scheme is vulnerable to Wagner's attack, though see\n> Schnorr 2004 [1] for mitigations that are perfectly compatible with this\n> modified BIP32ish scheme.)\n> \n> I'm unsure whether key-prefixing is _actually_ necessary for this, but it\n> makes the security argument much clearer since the messagehash contains\n> some data which can be made unique per-utxo and is committed in the chain.\n> \n> \n> Andrew\n> \n> \n> [1] http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=8ECEF929559865FD68D1D873555D18FE?doi=10.1.1.68.9836&rep=rep1&type=pdf\n> \n> \n> -- \n> Andrew Poelstra\n> Mathematics Department, Blockstream\n> Email: apoelstra at wpsoftware.net\n> Web:   https://www.wpsoftware.net/andrew\n> \n> \"A goose alone, I suppose, can know the loneliness of geese\n> who can never find their peace,\n> whether north or south or west or east\"\n>       --Joanna Newsom\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Natanael",
                "date": "2018-05-23T22:06:31",
                "message_text_only": "Den tis 22 maj 2018 20:18Pieter Wuille via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> skrev:\n\n> Hello all,\n>\n> Given the recent discussions about Taproot [1] and Graftroot [2], I\n> was wondering if a practical deployment needs a way to explicitly\n> enable or disable the Graftroot spending path. I have no strong\n> reasons why this would be necessary, but I'd like to hear other\n> people's thoughts.\n>\n\nI'm definitely in favor of the suggestion of requiring a flag to allow the\nusage of these in a transaction, so that you get to choose in advance if\nthe script will be static or \"editable\".\n\nConsider for example a P2SH address for some fund, where you create a\ntransaction in advance. Even if the parties involved in signing the\ntransaction would agree (collude), the original intent of this particular\nP2SH address may be to hold the fund accountable by enforcing some given\nrules by script. To be able to circumvent the rules could break the purpose\nof the fund.\n\nThe name of the scheme escapes me, but this could include a variety of\nproof-requiring committed transactions, like say a transaction that will\npay out if you can provide a proof satisfying some conditions such as\nembedding the solution to a given problem. This fund would only be supposed\nto pay out of the published conditions are met (which may include an expiry\ndate).\n\nTo then use taproot / graftroot to withdraw the funds despite this\npossibility not showing in the published script could be problematic.\n\nI'm simultaneously in favor of being able to have scripts where the usage\nof taproot / graftroot isn't visible in advance, but it must simultaneously\nbe possible to prove a transaction ISN'T using it.\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180524/328e93da/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-23T23:45:09",
                "message_text_only": "On Wed, May 23, 2018 at 10:06 PM, Natanael via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Consider for example a P2SH address for some fund, where you create a\n> transaction in advance. Even if the parties involved in signing the\n> transaction would agree (collude), the original intent of this particular\n> P2SH address may be to hold the fund accountable by enforcing some given\n> rules by script. To be able to circumvent the rules could break the purpose\n> of the fund.\n\nI am having a bit of difficulty understanding your example.\n\nIf graftroot were possible it would mean that the funds were paid to a\npublic key.  That holder(s) of the corresponding private key could\nsign without constraint, and so the accoutability you're expecting\nwouldn't exist there regardless of graftroot.\n\nI think maybe your example is only making the case that it should be\npossible to send funds constrained by a script without a public key\never existing at all.  If so, I agree-- but that wasn't the question\nhere as I understood it."
            },
            {
                "author": "Natanael",
                "date": "2018-05-24T09:32:23",
                "message_text_only": "Den tor 24 maj 2018 01:45Gregory Maxwell <greg at xiph.org> skrev:\n\n> I am having a bit of difficulty understanding your example.\n>\n> If graftroot were possible it would mean that the funds were paid to a\n> public key.  That holder(s) of the corresponding private key could\n> sign without constraint, and so the accoutability you're expecting\n> wouldn't exist there regardless of graftroot.\n>\n> I think maybe your example is only making the case that it should be\n> possible to send funds constrained by a script without a public key\n> ever existing at all.  If so, I agree-- but that wasn't the question\n> here as I understood it.\n>\n\nI have to admit I not an expert on this field, so some of my concerns might\nnot be relevant. However, I think Wuille understood my points and his reply\nanswered my concerns quite well. I'm only asking for the optional ability\nto prove you're not using these constructions (because some uses requires\ncommitting to an immutable script), and that already seems to exist. So for\nthe future implementations I only ask that this ability is preserved.\n\nI think such a proof don't need to be public (making such a proof in\nprivate is probably often better), although optionally it might be. A\nprivate contract wouldn't publish these details, while a public commitment\nwould do so.\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180524/6e1d11a6/attachment.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2018-05-24T01:58:11",
                "message_text_only": "On Tue, May 22, 2018 at 11:17 AM, Pieter Wuille <pieter.wuille at gmail.com> wrote:\n> Hello all,\n>\n> Given the recent discussions about Taproot [1] and Graftroot [2], I\n> was wondering if a practical deployment needs a way to explicitly\n> enable or disable the Graftroot spending path. I have no strong\n> reasons why this would be necessary, but I'd like to hear other\n> people's thoughts.\n\nThanks everyone who commented so far, but let me clarify the context\nof this question first a bit more to avoid getting into the weeds too\nmuch.\n\nIf it turns out to be necessary to explicitly commit to permitting\nGraftroot spending, there are a number of approaches:\n* Use a different witness version (or other marker directly in the\nscriptPubKey) to enable Graftroot.\n* Signal the permission to spend through Graftroot inside the Taproot\nscript as suggested by ZmnSCPxj.\n* Make \"Spend through Graftroot\" a special script (possibly indirectly\nwith a Merkle tree in Taproot).\n* Implement Graftroot as an opcode/feature inside the scripting\nlanguage (which may be more generically useful as a delegation\nmechanism).\n* Postpone Graftroot.\n\nAll of these are worse in either efficiency or privacy than always\npermitting Graftroot spends directly. Because of that, I think we\nshould first focus on reasons why a lack of commitment to enabling\nGraftroot may result in it being incompatible with certain use cases,\nor other reasons why it could interfere with applications adopting\nsuch outputs.\n\n@Natanael: all of these concerns only apply to a new hypothetical\nTaproot/Graftroot output type, which combines pay-to-pubkey and\npay-to-script in a single scriptPubKey that just contains a public\nkey. It doesn't apply to existing P2SH like constructions.\n\nAlso, the concern of making Graftroot optional does not apply to\nTaproot, as the Taproot spending path's script is committed to (using\nscriptPubKey = P + H(P,script)*G), allowing the script to be\nexplicitly chosen to be a non-spendable script, which the author could\nprove is the case (without revealing it to the entire world).\n\nIt is also always possible to create a \"script only\" Taproot output\n(without key that can unconditionally spend), by picking a pubkey that\nis provably unspendable (hashing onto a curve point, in particular),\nor through pubkey recovery.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-24T02:08:07",
                "message_text_only": "On Thu, May 24, 2018 at 1:58 AM, Pieter Wuille via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Thanks everyone who commented so far, but let me clarify the context\n> of this question first a bit more to avoid getting into the weeds too\n> much.\n\nMy understanding of the question is this:\n\nAre there any useful applications which would be impeded if a signing\nparty who could authorize an arbitrary transaction spending a coin had\nthe option to instead sign a delegation to a new script?\n\nThe reason this question is interesting to ask is because the obvious\nanswer is \"no\":  since the signer(s) could have signed an arbitrary\ntransaction instead, being able to delegate is strictly less powerful.\nMoreover, absent graftroot they could always \"delegate\" non-atomically\nby spending the coin with the output being the delegated script that\nthey would have graftrooted instead.\n\nSometimes obvious answers have non-obvious counter examples, e.g.\nAndrews points related to blindsigning are worth keeping in mind."
            },
            {
                "author": "Natanael",
                "date": "2018-05-24T09:44:16",
                "message_text_only": "Den tor 24 maj 2018 04:08Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> skrev:\n\n>\n> My understanding of the question is this:\n>\n> Are there any useful applications which would be impeded if a signing\n> party who could authorize an arbitrary transaction spending a coin had\n> the option to instead sign a delegation to a new script?\n>\n> The reason this question is interesting to ask is because the obvious\n> answer is \"no\":  since the signer(s) could have signed an arbitrary\n> transaction instead, being able to delegate is strictly less powerful.\n> Moreover, absent graftroot they could always \"delegate\" non-atomically\n> by spending the coin with the output being the delegated script that\n> they would have graftrooted instead.\n>\n> Sometimes obvious answers have non-obvious counter examples, e.g.\n> Andrews points related to blindsigning are worth keeping in mind.\n>\n\nAs stated above by Wuille this seems to not be a concern for typical P2SH\nuses, but my argument here is simply that in many cases, not all\nstakeholders in a transaction will hold one of the private keys required to\nsign. And such stakeholders would want a guarantee that the original script\nis followed as promised.\n\nI agree that such flags typically wouldn't have a meaningful effect for\nfunds from non-P2SH addresses, since the entire transaction / script could\nbe replaced by the very same keyholders.\n\nI'm not concerned by the ability to move funds to an address with the new\nrules that you'd otherwise graftroot in, only that you can provide a\ntransparent guarantee that you ALSO follow the original script as promised.\nWhat happens *after* you have followed the original script is unrelated,\nIMHO.\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180524/1eccb847/attachment-0001.html>"
            },
            {
                "author": "Andrew Poelstra",
                "date": "2018-05-24T12:39:55",
                "message_text_only": "On Thu, May 24, 2018 at 11:44:16AM +0200, Natanael via bitcoin-dev wrote:\n> \n> As stated above by Wuille this seems to not be a concern for typical P2SH\n> uses, but my argument here is simply that in many cases, not all\n> stakeholders in a transaction will hold one of the private keys required to\n> sign. And such stakeholders would want a guarantee that the original script\n> is followed as promised.\n>\n\nIn this case, even mandatory graftroot would not allow the signing stakeholders\nto take the coins. The reason is that if there are _any_ non-signing script\nconditions that must be followed, then to use Taproot the top-level public key\nneeds to be unusable, e.g. by being a NUMS point. In that case the public key\nwould also be unusable for Graftroot.\n\nAnother way to see this is -- in any context where Graftroot seems dangerous,\nthere needs to be a reason why the ability to just create transactions is not\ndangerous. In your example it seems that the signing parties can just take\nthe coins with or without Graftroot, so the problem is not in Graftroot but\nin the way that the example is set up.\n \n> I'm not concerned by the ability to move funds to an address with the new\n> rules that you'd otherwise graftroot in, only that you can provide a\n> transparent guarantee that you ALSO follow the original script as promised.\n> What happens *after* you have followed the original script is unrelated,\n> IMHO.\n>\n\nTo do this in Taproot you need to disable the top-level key, which will also\ndisable Graftroot. \n\n-- \nAndrew Poelstra\nMathematics Department, Blockstream\nEmail: apoelstra at wpsoftware.net\nWeb:   https://www.wpsoftware.net/andrew\n\n\"A goose alone, I suppose, can know the loneliness of geese\n who can never find their peace,\n whether north or south or west or east\"\n       --Joanna Newsom\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180524/eb8ee543/attachment.sig>"
            },
            {
                "author": "Johnson Lau",
                "date": "2018-05-25T10:14:48",
                "message_text_only": "> On 24 May 2018, at 10:08 AM, Gregory Maxwell via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> On Thu, May 24, 2018 at 1:58 AM, Pieter Wuille via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Thanks everyone who commented so far, but let me clarify the context\n>> of this question first a bit more to avoid getting into the weeds too\n>> much.\n> \n> since the signer(s) could have signed an arbitrary\n> transaction instead, being able to delegate is strictly less powerful.\n> \n\n\nActually, we could introduce graftroot-like delegation to all existing and new P2PK and P2PKH UTXOs with a softfork:\n\n1. Define SIGHASH_GRAFTROOT = 0x40. New rules apply if (nHashType & SIGHASH_GRAFTROOT)\n\n2. The third stack item MUST be at least 64 bytes, with 32-byte R and 32-byte S, followed by a script of arbitrary size. It MUST be a valid signature for the script with the original public key.\n\n3. The remaining stack items MUST solve the script\n\nConceptually this could be extended to arbitrary output types, not just P2PK and P2PKH. But it might be too complicated to describe here.\n\n(We can\u2019t do this in P2WPKH and P2WSH due to the implicit CLEANSTACK rules. But nothing could stop us to do it by introducing another witness structure (which is invisible to current nodes) and store the extra graftroot signatures and scripts)\n\nA graftroot design like this is a strict subset of existing signature checking rules. If this is dangerous, the existing signature checking rules must be dangerous. This also doesn\u2019t have any problem with blind signature, since the first signature will always sign the outpoint, with or without ANYONECANPAY. (As I pointed out in my reply to Andrew, his concern applies only to SIGHASH_NOINPUT, not graftroot)\n\n\n======\n\nWith the example above, I believe there is no reason to make graftroot optional, at the expense of block space and/or reduced privacy. Any potential problem (e.g. interaction with blind signature) could be fixed by a proper rules design."
            }
        ],
        "thread_summary": {
            "title": "Should Graftroot be optional?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Natanael",
                "Johnson Lau",
                "ZmnSCPxj",
                "Andrew Poelstra",
                "Gregory Maxwell",
                "Pieter Wuille"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 32340
        }
    },
    {
        "title": "[bitcoin-dev] Peer rotation",
        "thread_messages": [
            {
                "author": "Gleb Naumenko",
                "date": "2018-05-23T21:03:58",
                "message_text_only": "Hi all,\nI'm bringing this up again because since the last time (2014) new papers on\nnetwork attacks have been published, and in general I think this is\nsomething that has to be done in one or another form.\n\n### Motivation\nIt has been shown that revealing the topology of the network may increase\nthe risk of network-related attacks including partitioning/eclipse (and\nconsequentially double-spending attacks and attacks on mining) and\ndeanonymization of transactions.\n\nThe current join/leave algorithm makes the network fairly static, which\nmakes it possible to reconstruct the topology by observing events in the\nnetwork (for example, see Dandelion threat model [1] or Exploiting\nTransaction Accumulation and Double Spends for Topology Inference in\nBitcoin [2]).\nRotation of the peers is an obvious solution, but there are several\nquestions to answer.\n[The idea has also been discussed here: [3] and in the mailing list: [4],\nbut ended up not well-researched.]\n\n### Issues with rotation\nIn P2P network, rotation of peers may cause an additional threat, because\nit is safer to stick to the existing connections, due to the fact that\nhaving connections to more different peers increases the chances of\nconnecting to an attacker. Considering the fact that an attacker can\ninfluence your future behavior including what connections you make, this\nmay worsen the situation.\n\nOne important detail to keep in mind here is that a node may act\nlegitimately, but just to wait when all of the connections are under the\ncontrol of an attacker. So a good idea here is to avoid disconnecting the\nmost reliable peers.\n\n### Reliable peers\nThere are several metrics that might be used to consider peers to be\nreliable:\nWhich fraction of recent blocks have a particular node relayed to us?\n\u2026 of recent transactions ... ?\nFor how long the connection has been maintained?\n\n### Implementation details\nRotation of the outgoing connections only seems to be sufficient yet not\nvery hard to implement and analyze. In addition, it will cause rotation of\nthe incoming connections of nodes in the network due to the fact that each\nof the outgoing connections is also an incoming connection on the second\nside; and due to the scoring mechanism for replacing existing incoming\nconnections when getting a new one.\n\nCurrent 8 peers for outgoing connections is an arbitrary number, however,\nthere is a reason behind keeping a number of outgoing connections low.\nAnyway, considering the threat highlighted before it is a good idea to\nrotate only a fraction of peers.\n\nThus, there are 3 values to discuss (N, M, T):\nN \u2014 Number of persistent peers which are considered to be trustworthy based\non the metrics as per Section 3\nM \u2014 Number of peers to be rotated every T seconds\n\nThe trade-off here is how to add enough entropy while not ending up being\nconnected to dishonest peers only. It is tunable by modifying {N, M}.\n\nLower bound for T is a value that won\u2019t significantly delay transaction\npropagation because of establishing handshakes (and it will not result in\nconnecting to dishonest peers only), while the upper bound is a value at\nwhich it would be still infeasible to execute an attack.\n\nFiguring out an optimal set {N, M, T} may be done analytically or by\nsimulation.\nI'd be happy to discuss the way of figuring it out.\n\n### Protocol extensions\nIt may also be useful to keep track of the previous connections (which were\nevicted due to the rotation) and get back to those after a while under\ncertain conditions.\n\nFor example, to decrease a chance of connecting to dishonest peers, a peer\nmay alternate connecting to the brand new peer with connecting to the old\nand fairly reliable peer.\n\n### Transactions de-anonymized\nRotation of the peers itself may increase the chance that particular\nBitcoin address or set of transactions would be linked to a node.\nIn this case either Dandelion [1] or sending own transactions to a static\nset of peers (say first 8 peers) may help.\n\n[1] https://github.com/mablem8/bips/blob/master/bip-dandelion.mediawiki\n[2] https://fc18.ifca.ai/bitcoin/papers/bitcoin18-final10.pdf\n[3] https://github.com/bitcoin/bitcoin/pull/4723\n[4]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2014-August/006502.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/3364ae51/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Peer rotation",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Gleb Naumenko"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4397
        }
    },
    {
        "title": "[bitcoin-dev] TXO bitfield size graphs",
        "thread_messages": [
            {
                "author": "Jim Posen",
                "date": "2018-05-23T23:48:33",
                "message_text_only": "I decided to look into the metrics around compression ratios of TXO\nbitfields, as proposed by Bram Cohen [1]. I'm specifically interested in\nthe feasibility of committing to them with block headers. In combination\nwith block commitments to TXOs themselves, this would enable UTXO\ninclusion/exclusion proofs for light clients.\n\nFirst, looking just at proofs of inclusion in the UTXO set, each block\nneeds what Bram calls a \"proof of position.\" Concretely, one such\nconstruction is a Merkle root over all of the block's newly created coins,\nincluding their output data (scriptPubKey + amount), the outpoint (txid +\nindex), and an absolute index of the output in the entire blockchain. A\nMerkle branch in this tree constitutes a proof of position. Alternatively,\nthe \"position\", rather than being an absolute index in the chain, could be\na block hash plus an output index within the block.\n\nLet's say we use the absolute index in the chain as position. A TXO\nspentness bitfield can be constructed for the entire chain, which is added\nto when new coins are created and modified when they are spent. In order to\ncompactly prove spentness in this bitfield to a client, one could chunk up\nthe bitfield and construct a Merkle Mountain Range [2] over the chunks.\nInstead of building an MMR over outputs themselves, as proposed by Peter\nTodd [3], an MMR constructed over bitfield chunks grows far slower, by a\nlarge constant factor. Slower growth means faster updates.\n\nSo there's the question of how much these bitfields can be compressed. We\nexpect some decent level because patterns of spending coins are very\nnon-random.\n\nThe top graph in the attached figure shows the compression ratios possible\non a TXO bitfield split into 4 KiB chunks, using gzip (level=9) and lz4.\nData was collected at block height 523,303. You can see that the\ncompression ratio is much lower for older chunks and is worse for more\nrecent blocks. Over the entire history, gzip achieves 34.4%, lz4 54.8%, and\nbz2 37.6%. I'm kind of surprised that the ratios are not lower with\noff-the-shelf algorithms. And that gzip performs better than bz2 (it seems\nto be a factor of the chunk size?).\n\nAlternatively, we can look at bitfields stored separately by block, which\nis more compatible with constructions where an output's position is its\nblock hash plus relative index. The per-block bitfield sizes are shown in\nthe bottom graph. The compression ratios overall are 50% for gzip, 70% for\nlz4, and 61.5% for bz2.\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013928.html\n[2]\nhttps://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md\n[3] https://petertodd.org/2016/delayed-txo-commitments\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/8e1272fd/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: bitfield_sizes.svg\nType: image/svg+xml\nSize: 1788184 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/8e1272fd/attachment-0001.svg>"
            },
            {
                "author": "Bram Cohen",
                "date": "2018-05-24T02:43:44",
                "message_text_only": "You compressed something which is truly natively a bitfield using regular\ncompression algorithms? That is expected to get horrible results. Much\nbetter would be something which handles it natively, say doing run length\nencoding on the number of repeated bits and compressing that using elias\nomega encoding. That is suboptimal in a few ways but has the advantage of\nworking well both on things which are mostly zeros or mostly ones, and only\nperforms badly on truly random bits.\n\nIt isn't super clear how relevant this information is. The TXO bitfield is\nfairly small to begin with, and to compress the data in real time would\nrequire a special data structure which gets worse compression than straight\ncompressing the whole thing and has slower lookups than an uncompressed\nversion. Writing such a thing sounds like an interesting project though.\n\nOn Wed, May 23, 2018 at 4:48 PM, Jim Posen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I decided to look into the metrics around compression ratios of TXO\n> bitfields, as proposed by Bram Cohen [1]. I'm specifically interested in\n> the feasibility of committing to them with block headers. In combination\n> with block commitments to TXOs themselves, this would enable UTXO\n> inclusion/exclusion proofs for light clients.\n>\n> First, looking just at proofs of inclusion in the UTXO set, each block\n> needs what Bram calls a \"proof of position.\" Concretely, one such\n> construction is a Merkle root over all of the block's newly created coins,\n> including their output data (scriptPubKey + amount), the outpoint (txid +\n> index), and an absolute index of the output in the entire blockchain. A\n> Merkle branch in this tree constitutes a proof of position. Alternatively,\n> the \"position\", rather than being an absolute index in the chain, could be\n> a block hash plus an output index within the block.\n>\n> Let's say we use the absolute index in the chain as position. A TXO\n> spentness bitfield can be constructed for the entire chain, which is added\n> to when new coins are created and modified when they are spent. In order to\n> compactly prove spentness in this bitfield to a client, one could chunk up\n> the bitfield and construct a Merkle Mountain Range [2] over the chunks.\n> Instead of building an MMR over outputs themselves, as proposed by Peter\n> Todd [3], an MMR constructed over bitfield chunks grows far slower, by a\n> large constant factor. Slower growth means faster updates.\n>\n> So there's the question of how much these bitfields can be compressed. We\n> expect some decent level because patterns of spending coins are very\n> non-random.\n>\n> The top graph in the attached figure shows the compression ratios possible\n> on a TXO bitfield split into 4 KiB chunks, using gzip (level=9) and lz4.\n> Data was collected at block height 523,303. You can see that the\n> compression ratio is much lower for older chunks and is worse for more\n> recent blocks. Over the entire history, gzip achieves 34.4%, lz4 54.8%,\n> and bz2 37.6%. I'm kind of surprised that the ratios are not lower with\n> off-the-shelf algorithms. And that gzip performs better than bz2 (it seems\n> to be a factor of the chunk size?).\n>\n> Alternatively, we can look at bitfields stored separately by block, which\n> is more compatible with constructions where an output's position is its\n> block hash plus relative index. The per-block bitfield sizes are shown in\n> the bottom graph. The compression ratios overall are 50% for gzip, 70% for\n> lz4, and 61.5% for bz2.\n>\n> [1] https://lists.linuxfoundation.org/pipermail/\n> bitcoin-dev/2017-March/013928.html\n> [2] https://github.com/opentimestamps/opentimestamps-\n> server/blob/master/doc/merkle-mountain-range.md\n> [3] https://petertodd.org/2016/delayed-txo-commitments\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/4ae3fd4f/attachment-0001.html>"
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-24T04:02:17",
                "message_text_only": "Yes, certainly an RLE-style compression would work better in this instance,\nbut I wanted to see how well standard compression algorithms would work\nwithout doing something custom. If there are other standard compression\nschemes better suited to this, please let me know.\n\nAs far as relevance, I'll clarify that the intention is to compress the\nbitfields when sending proofs of spentness/unspentness to light clients,\nwhere bandwidth is a concern. As you note, the bitfields are small enough\nthat it's probably not necessary to store the compressed versions on full\nnodes. Though lz4 is fast enough that it may be worthwhile to compress\nbefore saving to disk.\n\nOn Wed, May 23, 2018 at 7:43 PM Bram Cohen <bram at chia.net> wrote:\n\n> You compressed something which is truly natively a bitfield using regular\n> compression algorithms? That is expected to get horrible results. Much\n> better would be something which handles it natively, say doing run length\n> encoding on the number of repeated bits and compressing that using elias\n> omega encoding. That is suboptimal in a few ways but has the advantage of\n> working well both on things which are mostly zeros or mostly ones, and only\n> performs badly on truly random bits.\n>\n> It isn't super clear how relevant this information is. The TXO bitfield is\n> fairly small to begin with, and to compress the data in real time would\n> require a special data structure which gets worse compression than straight\n> compressing the whole thing and has slower lookups than an uncompressed\n> version. Writing such a thing sounds like an interesting project though.\n>\n> On Wed, May 23, 2018 at 4:48 PM, Jim Posen via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I decided to look into the metrics around compression ratios of TXO\n>> bitfields, as proposed by Bram Cohen [1]. I'm specifically interested in\n>> the feasibility of committing to them with block headers. In combination\n>> with block commitments to TXOs themselves, this would enable UTXO\n>> inclusion/exclusion proofs for light clients.\n>>\n>> First, looking just at proofs of inclusion in the UTXO set, each block\n>> needs what Bram calls a \"proof of position.\" Concretely, one such\n>> construction is a Merkle root over all of the block's newly created coins,\n>> including their output data (scriptPubKey + amount), the outpoint (txid +\n>> index), and an absolute index of the output in the entire blockchain. A\n>> Merkle branch in this tree constitutes a proof of position. Alternatively,\n>> the \"position\", rather than being an absolute index in the chain, could be\n>> a block hash plus an output index within the block.\n>>\n>> Let's say we use the absolute index in the chain as position. A TXO\n>> spentness bitfield can be constructed for the entire chain, which is added\n>> to when new coins are created and modified when they are spent. In order to\n>> compactly prove spentness in this bitfield to a client, one could chunk up\n>> the bitfield and construct a Merkle Mountain Range [2] over the chunks.\n>> Instead of building an MMR over outputs themselves, as proposed by Peter\n>> Todd [3], an MMR constructed over bitfield chunks grows far slower, by a\n>> large constant factor. Slower growth means faster updates.\n>>\n>> So there's the question of how much these bitfields can be compressed. We\n>> expect some decent level because patterns of spending coins are very\n>> non-random.\n>>\n>> The top graph in the attached figure shows the compression ratios\n>> possible on a TXO bitfield split into 4 KiB chunks, using gzip (level=9)\n>> and lz4. Data was collected at block height 523,303. You can see that the\n>> compression ratio is much lower for older chunks and is worse for more\n>> recent blocks. Over the entire history, gzip achieves 34.4%, lz4 54.8%,\n>> and bz2 37.6%. I'm kind of surprised that the ratios are not lower with\n>> off-the-shelf algorithms. And that gzip performs better than bz2 (it seems\n>> to be a factor of the chunk size?).\n>>\n>> Alternatively, we can look at bitfields stored separately by block, which\n>> is more compatible with constructions where an output's position is its\n>> block hash plus relative index. The per-block bitfield sizes are shown in\n>> the bottom graph. The compression ratios overall are 50% for gzip, 70% for\n>> lz4, and 61.5% for bz2.\n>>\n>> [1]\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-March/013928.html\n>> [2]\n>> https://github.com/opentimestamps/opentimestamps-server/blob/master/doc/merkle-mountain-range.md\n>> [3] https://petertodd.org/2016/delayed-txo-commitments\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180523/12ebae07/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "TXO bitfield size graphs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Bram Cohen",
                "Jim Posen"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 12283
        }
    },
    {
        "title": "[bitcoin-dev] Minimizing the redundancy in Golomb Coded Sets",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2018-05-25T17:54:17",
                "message_text_only": "Hi all,\n\nI spent some time working out the optimal parameter selection for the\nGolomb Coded Sets that are proposed in BIP158:\nhttps://gist.github.com/sipa/576d5f09c3b86c3b1b75598d799fc845\n\nTL;DR: if we really want an FP rate of exactly 1 in 2^20, the Rice\nparameter should be 19, not 20. If we don't, we should pick an FP rate\nof 1 in a 1.4971*2^B. So for example M=784931 B=19 or M=1569861 B=20.\n\nCheers,\n\n-- \nPieter"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-25T18:42:41",
                "message_text_only": "On Fri, May 25, 2018 at 5:54 PM, Pieter Wuille via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Hi all,\n>\n> I spent some time working out the optimal parameter selection for the\n> Golomb Coded Sets that are proposed in BIP158:\n> https://gist.github.com/sipa/576d5f09c3b86c3b1b75598d799fc845\n>\n> TL;DR: if we really want an FP rate of exactly 1 in 2^20, the Rice\n> parameter should be 19, not 20. If we don't, we should pick an FP rate\n> of 1 in a 1.4971*2^B. So for example M=784931 B=19 or M=1569861 B=20.\n\n\nI did a rough analysis using Pieter's approximations on what\nparameters minimizes the total communications for a lite wallet\nscanning the chain and fetching a witnessless block whenever they get\na filter hit. For a wallet with 1000 keys and blocks of 1MB if the\nnumber of entries in the is at least 5096 then M=784931 results in a\nlower total data rate rate (FP blocks + filters) than M=1569861.\nM=392465 (the optimal value for the rice parameter 18) is\ncommunications is better if at least 10192 entries are set, and\nM=196233 (optimal FP for rice 17) is better if at least 20384 entries\nare set.\n\nThe prior filter set proposal is setting roughly 13300 entries per\nfull block,  and I guestimate that the in+out scripts only ones are\nsetting about 7500 entries (if that actual number was in any of the\nrecent posts I missed it, I'm guessing based on jimpo's sizes graph).\n\nThe breakpoints are obviously different if the client is monitoring\nfor, say, 10,000 keys instead of 1000 but I think it generally makes\nmore sense to optimize for lower key counts since bigger users are\nmore likely to tolerate the additional bandwidth usage.\n\nSo I think that assuming that all-scripts inputs and outputs (but no\ntxids) are used and that my guess of 7500 bits set for that\nconfiguration is roughly right, then M=1569861 and rice parameter 19\nshould be used.\n\nThe actual optimal FP rate for total data transferred won't be one\nthat gets the optimal rice coding efficiency, but since different\nclients will be monitoring for different numbers of keys, it probably\nmakes sense to pick a parameter with optimal compression rather than\noptimal-data-transfer-for-a-specific-key-count-- at least then we're\nspending the least amount of filter bits per false positive rate,\nwhatever that rate is... if we can't be optimal at least we can be\nefficient. :)"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-25T21:13:55",
                "message_text_only": "On Fri, May 25, 2018 at 6:42 PM, Gregory Maxwell <greg at xiph.org> wrote:\n> configuration is roughly right, then M=1569861 and rice parameter 19\n> should be used.\n\nThat should have been M=784931 B=19  ... paste error."
            },
            {
                "author": "Jim Posen",
                "date": "2018-05-29T22:38:01",
                "message_text_only": "This is a really cool finding, thanks Pieter!\n\nI did some more analysis on selecting a good P value to reduce total data\ndownloaded considering both filters themselves and blocks in the case of\nfalse positive matches, using data from mainnet. The quantity it minimizes\nis:\n\nfilter_size(N, B) + block_size * false_positive_probability(C, N, B)\n\nN is the number of filter elements per block\nB is the Golomb-Rice coding parameter\nC is the number of filter elements watched by the client\n\nThe main result is that:\n\nFor C = 10, B = 13 is optimal\nFor C = 100, B = 16 is optimal\nFor C = 1,000, B = 20 is optimal\nFor C = 10,000, B = 23 is optimal\n\nSo any value of B in the range 16 to 20 seems reasonable, with M = 1.4971 *\n2^B for optimal compression, as Pieter derived. The selection of the\nparameter depends on the target number of elements that a client may watch.\n\nI attached some of the results, and would be happy to share the CSV and raw\nnotebook if people are interested.\n\n\nOn Fri, May 25, 2018 at 2:14 PM Gregory Maxwell via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, May 25, 2018 at 6:42 PM, Gregory Maxwell <greg at xiph.org> wrote:\n> > configuration is roughly right, then M=1569861 and rice parameter 19\n> > should be used.\n>\n> That should have been M=784931 B=19  ... paste error.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180529/1a5d37e0/attachment-0002.html>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180529/1a5d37e0/attachment-0003.html>"
            },
            {
                "author": "Lucas Ontivero",
                "date": "2018-05-30T03:10:04",
                "message_text_only": "Hi Jim,\n\nYes please, could you share CSV? We are developing a Wallet that uses\nGolomb-Rice filters it would help a lot for determine the best P value\ndepending on the estimated number of elements the client needs to watch.\n\n2018-05-29 19:38 GMT-03:00 Jim Posen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> This is a really cool finding, thanks Pieter!\n>\n> I did some more analysis on selecting a good P value to reduce total data\n> downloaded considering both filters themselves and blocks in the case of\n> false positive matches, using data from mainnet. The quantity it minimizes\n> is:\n>\n> filter_size(N, B) + block_size * false_positive_probability(C, N, B)\n>\n> N is the number of filter elements per block\n> B is the Golomb-Rice coding parameter\n> C is the number of filter elements watched by the client\n>\n> The main result is that:\n>\n> For C = 10, B = 13 is optimal\n> For C = 100, B = 16 is optimal\n> For C = 1,000, B = 20 is optimal\n> For C = 10,000, B = 23 is optimal\n>\n> So any value of B in the range 16 to 20 seems reasonable, with M = 1.4971\n> * 2^B for optimal compression, as Pieter derived. The selection of the\n> parameter depends on the target number of elements that a client may watch.\n>\n> I attached some of the results, and would be happy to share the CSV and\n> raw notebook if people are interested.\n>\n>\n> On Fri, May 25, 2018 at 2:14 PM Gregory Maxwell via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Fri, May 25, 2018 at 6:42 PM, Gregory Maxwell <greg at xiph.org> wrote:\n>> > configuration is roughly right, then M=1569861 and rice parameter 19\n>> > should be used.\n>>\n>> That should have been M=784931 B=19  ... paste error.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180530/e1679c4d/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Minimizing the redundancy in Golomb Coded Sets",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pieter Wuille",
                "Jim Posen",
                "Gregory Maxwell",
                "Lucas Ontivero"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 7143
        }
    },
    {
        "title": "[bitcoin-dev] New serialization/encoding format for key material",
        "thread_messages": [
            {
                "author": "Jonas Schnelli",
                "date": "2018-05-29T09:13:37",
                "message_text_only": "Hi\n\nExtended public and private keys are defined in BIP32 [1].\n\nEncoded extended private keys should not be confused with a wallet \u201eseed\u201c\n(proposals like BIP39) while they can also partially serve the purpose to\n\u201eseed\u201c a wallet (there may be an overlap in the use-case).\n\nRecovering a wallet by its extended private master key (xpriv; may or may not\nbe at depth 0) is a complex task with risks of failing to recover all available\nfunds.\n\nIt may be reasonable to consider that recovering a wallet purely based on the\nexistence of an extended private master key is a forensic funds recovery\nprocess and should probably be the last resort in case of a backup-recovery\nsituation. A simple example here is, that it was/is possible to have used an\nxpriv (referring to extended private master key) in production that is/was used\nto derive BIP45 based P2SH multisig addresses (1of1, used by Bitpays BWS for\nwhile), later used for bare BIP45ish multisig 1of1 as well as for P2PKH after\nBIP44 & vanilla BIP32 P2WPKH (m/0\u2019/k\u2019).\nI\u2019m not aware of any wallet that would recover 100% of those funds, leading to\nthe risk that forwarding the unspents and destroying the extended master key\nmay result in coins forever lost.\n\nThe case above may be an edge case, but I\u2019m generally under the assumption that\nrecovering funds based on the sole existence of an xpriv (or seed) without further\nmetadata is a fragile concept.\n\nSecond, the missing birthday-metadata tend to lead to non-optimal blockchain\nscans (eventually increased p2p traffic). Recovering funds can take hours.\n\nAdditionally, the BIP44 gap limit seems to be a weak construct. The current gap\nlimit in BIP44 is set to 20 [2] which basically means, handing out more then 20\nincoming payment requests (addresses) results in taking the risks that funds\nmay be destroyed (or at least not detected) during a recovery.\nThe Gap limit value may also depend on the use case, but the current proposals\ndo not allow to set an arbitrary value. High load merchants very likely need a\ndifferent gap limit value then individuals create a transaction once a year.\n\nDuring creation time of an xpriv/xpub, it is impossible to know if the created\nxpriv will be used for an unforeseen derivation scheme. Future proposals may\nwant to limit an extended key to a single derivation scheme.\n\n\nThis is an early draft in order to allow discussion that may lead to a possible\nproposal.\nThis proposals could also make BIP 178 obsolete since it can be replace the\nWIF[3] standard.\n\n\nThanks for feedback\n/jonas\n\n\n------------------------------------\n\n\nTitel\n######\nBech32 encoded key material including metadata\n\nAbstract\n########\nAn error tolerant encoding format for key material up to 520bits with a minimal\namount of metadata.\n\nMotivation\n##########\n(See above; intro text)\n\n\nSpecification\n#############\n\n## Serialization format\n\n1 bit version bit\n15 bits (bit 1 to 16) key-birthday (0-32767)\n(12 bit gap limit)\n3 or 5 bits script type\n256 or 512 or 520 bits key material\n= Total 275, 545, 553 bits\n\nThe initial version bit allows extending the serialization-format in future.\nThe encoding format must hint the total length and thus allow to calculate the\nlength of the key material.\n\nThe total length for 256 or 512 bit key material is optimised for Bech32 (power\nof 5).\n\n### Key material\nIf the key material length is 520 bits, it must contain an extended public key\nIf the key material length is 512 bits, it must contain an extended private key\nKey material length other then 256, 512, 520 bits and invalid.\n\nIf 520 bits are present, first 256 bits are the BIP32 chain code, to second 264\nbits (33 bytes) define the public key (according to BIP32)\n\nIf 512 bits are present, first 256 bits are the BIP32 chain code, to second 256\nbits define the private key\n\nIf 256 bits are present, those bits represent a pure private key (or seed)\n\n### Key birthday\nA 15 bit timestamp expressed in days since genesis (valid up to ~2098). The\nbirthday must be set to the first possible derivation of the according extended\nkey, if unknown, the used seed birthday must be used. If both unknown, 0\n(16x0bit) must be used.\n\n### Gap limit delta\n12 bits, results in a possible range from 0 to 4095.\n\nIf the total decoded serialization length is 275 bits (decode) or if the key\nmaterial is 256 bits (encode), the gap limit must not be present.\n\nThe base gap limit value is 20 (to disallow insane gap limits). The final gap\nlimit is the base value + the gap limit delta stored in those 12 bits.\nKey derivation gap limit must not be exceeded when deriving child keys and must\nbe respected during transaction rescans.\nChild key derivation must not be possible if gap limit is hit.\n\n### Script type restriction\n3 or 5 bits (range 0-7 / 0-31)\n0 no restriction\n1 P2PKH compressed\n2 P2PKH | P2SH\n3 P2WPKH P2WSH nested in P2SH\n4 P2WPKH | P2WSH\n\nIf the total decoded serialization length is 275 bits (decode) or if the key\nmaterial is 256 bits (encode), 3 bits are used for the script type. 5 bits are\nused for key material with the size of 512, 520 bits.\n\nIf the script type restriction is set, the according extended key must only be\nused to derive addresses with the selected script type.\nThis does not stands in contradiction to derivation path proposals ([4]). It\ndoes allow to derive and encode an extended key at a keypath where users assume\nrestricted script types in derivation due to other supported proposals.\n\n\nEncoding\n########\n\nBech32 must be used as encoding format (see the Bech32 rational [5]). Encoding\n545 or 553 bits (results in 109 resp. 111 x 5 bits) will exceed the Bech32 property of a\nguaranteed detection of 4 errors (only 3 are).\nIt is possible that there are more efficient BCH codes, especially for encoding\nextended private keys. Since a Bech32 implementation needs to be present in\nmodern Bitcoin software, re-using Bech32 will allow to migrate to this proposal\nwith a minimal implementation effort.\nForensic, cpu-intense key-recovery (including brute-force techniques) may allow\nto recover keys beyond the guaranteed error detection limits.\n\nBech32 HRPs\nMainnet Private Extended: xp\nMainnet Public Extended: xpu\nTestnet Private Extended: tp\nTestnet Public Extended: tpu\nMainnet Key: pk-\nTestnet Key: tk-\n\nCompatibility\n###########\nOnly new software will be able to use these serialization and encoding format.\n\nReferences\n##########\n\n\n[1] https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki <https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki>\n[2] https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki\n[3] https://github.com/bitcoin/bips/blob/master/bip-0178.mediawiki\n[4] https://github.com/bitcoin/bips/blob/master/bip-0049.mediawiki\n[5] https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki#rationale\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180529/d2332b98/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180529/d2332b98/attachment.sig>"
            },
            {
                "author": "shiva sitamraju",
                "date": "2018-05-30T06:30:25",
                "message_text_only": "The idea to add birthdate and gap limit sounds very good and addresses lots\nof problems users are facing.\n\nHowever, adding birthday to keys breaks two basic properties\n\n- Visually Comparing two keys to find if they are same (Important)\n- Different wallet software could set different birthday/gap limit.\ncreating different xpub/xprv for the same set of mathematically derived\nindividual keys. This removes the decoupling between key and wallet metadata\n\nIn fact, same could be argued to add birthday to WIF private key format to\nlet wallet discover funds faster.\n\n\nIs it possible to have a serialization so that in the encoding, the key\npart is still visually the same ?\n\n\nOn Tue, May 29, 2018 at 5:30 PM, <\nbitcoin-dev-request at lists.linuxfoundation.org> wrote:\n\n> Send bitcoin-dev mailing list submissions to\n>         bitcoin-dev at lists.linuxfoundation.org\n>\n> To subscribe or unsubscribe via the World Wide Web, visit\n>         https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> or, via email, send a message with subject or body 'help' to\n>         bitcoin-dev-request at lists.linuxfoundation.org\n>\n> You can reach the person managing the list at\n>         bitcoin-dev-owner at lists.linuxfoundation.org\n>\n> When replying, please edit your Subject line so it is more specific\n> than \"Re: Contents of bitcoin-dev digest...\"\n>\n>\n> Today's Topics:\n>\n>    1. New serialization/encoding format for key material\n>       (Jonas Schnelli)\n>\n>\n> ----------------------------------------------------------------------\n>\n> Message: 1\n> Date: Tue, 29 May 2018 11:13:37 +0200\n> From: Jonas Schnelli <dev at jonasschnelli.ch>\n> To: Bitcoin Protocol Discussion\n>         <bitcoin-dev at lists.linuxfoundation.org>\n> Subject: [bitcoin-dev] New serialization/encoding format for key\n>         material\n> Message-ID: <03557E21-8CFC-4822-8494-F4A78E23860B at jonasschnelli.ch>\n> Content-Type: text/plain; charset=\"utf-8\"\n>\n> Hi\n>\n> Extended public and private keys are defined in BIP32 [1].\n>\n> Encoded extended private keys should not be confused with a wallet ?seed?\n> (proposals like BIP39) while they can also partially serve the purpose to\n> ?seed? a wallet (there may be an overlap in the use-case).\n>\n> Recovering a wallet by its extended private master key (xpriv; may or may\n> not\n> be at depth 0) is a complex task with risks of failing to recover all\n> available\n> funds.\n>\n> It may be reasonable to consider that recovering a wallet purely based on\n> the\n> existence of an extended private master key is a forensic funds recovery\n> process and should probably be the last resort in case of a backup-recovery\n> situation. A simple example here is, that it was/is possible to have used\n> an\n> xpriv (referring to extended private master key) in production that is/was\n> used\n> to derive BIP45 based P2SH multisig addresses (1of1, used by Bitpays BWS\n> for\n> while), later used for bare BIP45ish multisig 1of1 as well as for P2PKH\n> after\n> BIP44 & vanilla BIP32 P2WPKH (m/0?/k?).\n> I?m not aware of any wallet that would recover 100% of those funds,\n> leading to\n> the risk that forwarding the unspents and destroying the extended master\n> key\n> may result in coins forever lost.\n>\n> The case above may be an edge case, but I?m generally under the assumption\n> that\n> recovering funds based on the sole existence of an xpriv (or seed) without\n> further\n> metadata is a fragile concept.\n>\n> Second, the missing birthday-metadata tend to lead to non-optimal\n> blockchain\n> scans (eventually increased p2p traffic). Recovering funds can take hours.\n>\n> Additionally, the BIP44 gap limit seems to be a weak construct. The\n> current gap\n> limit in BIP44 is set to 20 [2] which basically means, handing out more\n> then 20\n> incoming payment requests (addresses) results in taking the risks that\n> funds\n> may be destroyed (or at least not detected) during a recovery.\n> The Gap limit value may also depend on the use case, but the current\n> proposals\n> do not allow to set an arbitrary value. High load merchants very likely\n> need a\n> different gap limit value then individuals create a transaction once a\n> year.\n>\n> During creation time of an xpriv/xpub, it is impossible to know if the\n> created\n> xpriv will be used for an unforeseen derivation scheme. Future proposals\n> may\n> want to limit an extended key to a single derivation scheme.\n>\n>\n> This is an early draft in order to allow discussion that may lead to a\n> possible\n> proposal.\n> This proposals could also make BIP 178 obsolete since it can be replace the\n> WIF[3] standard.\n>\n>\n> Thanks for feedback\n> /jonas\n>\n>\n> ------------------------------------\n>\n>\n> Titel\n> ######\n> Bech32 encoded key material including metadata\n>\n> Abstract\n> ########\n> An error tolerant encoding format for key material up to 520bits with a\n> minimal\n> amount of metadata.\n>\n> Motivation\n> ##########\n> (See above; intro text)\n>\n>\n> Specification\n> #############\n>\n> ## Serialization format\n>\n> 1 bit version bit\n> 15 bits (bit 1 to 16) key-birthday (0-32767)\n> (12 bit gap limit)\n> 3 or 5 bits script type\n> 256 or 512 or 520 bits key material\n> = Total 275, 545, 553 bits\n>\n> The initial version bit allows extending the serialization-format in\n> future.\n> The encoding format must hint the total length and thus allow to calculate\n> the\n> length of the key material.\n>\n> The total length for 256 or 512 bit key material is optimised for Bech32\n> (power\n> of 5).\n>\n> ### Key material\n> If the key material length is 520 bits, it must contain an extended public\n> key\n> If the key material length is 512 bits, it must contain an extended\n> private key\n> Key material length other then 256, 512, 520 bits and invalid.\n>\n> If 520 bits are present, first 256 bits are the BIP32 chain code, to\n> second 264\n> bits (33 bytes) define the public key (according to BIP32)\n>\n> If 512 bits are present, first 256 bits are the BIP32 chain code, to\n> second 256\n> bits define the private key\n>\n> If 256 bits are present, those bits represent a pure private key (or seed)\n>\n> ### Key birthday\n> A 15 bit timestamp expressed in days since genesis (valid up to ~2098). The\n> birthday must be set to the first possible derivation of the according\n> extended\n> key, if unknown, the used seed birthday must be used. If both unknown, 0\n> (16x0bit) must be used.\n>\n> ### Gap limit delta\n> 12 bits, results in a possible range from 0 to 4095.\n>\n> If the total decoded serialization length is 275 bits (decode) or if the\n> key\n> material is 256 bits (encode), the gap limit must not be present.\n>\n> The base gap limit value is 20 (to disallow insane gap limits). The final\n> gap\n> limit is the base value + the gap limit delta stored in those 12 bits.\n> Key derivation gap limit must not be exceeded when deriving child keys and\n> must\n> be respected during transaction rescans.\n> Child key derivation must not be possible if gap limit is hit.\n>\n> ### Script type restriction\n> 3 or 5 bits (range 0-7 / 0-31)\n> 0 no restriction\n> 1 P2PKH compressed\n> 2 P2PKH | P2SH\n> 3 P2WPKH P2WSH nested in P2SH\n> 4 P2WPKH | P2WSH\n>\n> If the total decoded serialization length is 275 bits (decode) or if the\n> key\n> material is 256 bits (encode), 3 bits are used for the script type. 5 bits\n> are\n> used for key material with the size of 512, 520 bits.\n>\n> If the script type restriction is set, the according extended key must\n> only be\n> used to derive addresses with the selected script type.\n> This does not stands in contradiction to derivation path proposals ([4]).\n> It\n> does allow to derive and encode an extended key at a keypath where users\n> assume\n> restricted script types in derivation due to other supported proposals.\n>\n>\n> Encoding\n> ########\n>\n> Bech32 must be used as encoding format (see the Bech32 rational [5]).\n> Encoding\n> 545 or 553 bits (results in 109 resp. 111 x 5 bits) will exceed the Bech32\n> property of a\n> guaranteed detection of 4 errors (only 3 are).\n> It is possible that there are more efficient BCH codes, especially for\n> encoding\n> extended private keys. Since a Bech32 implementation needs to be present in\n> modern Bitcoin software, re-using Bech32 will allow to migrate to this\n> proposal\n> with a minimal implementation effort.\n> Forensic, cpu-intense key-recovery (including brute-force techniques) may\n> allow\n> to recover keys beyond the guaranteed error detection limits.\n>\n> Bech32 HRPs\n> Mainnet Private Extended: xp\n> Mainnet Public Extended: xpu\n> Testnet Private Extended: tp\n> Testnet Public Extended: tpu\n> Mainnet Key: pk-\n> Testnet Key: tk-\n>\n> Compatibility\n> ###########\n> Only new software will be able to use these serialization and encoding\n> format.\n>\n> References\n> ##########\n>\n>\n> [1] https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki <\n> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki>\n> [2] https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki\n> [3] https://github.com/bitcoin/bips/blob/master/bip-0178.mediawiki\n> [4] https://github.com/bitcoin/bips/blob/master/bip-0049.mediawiki\n> [5] https://github.com/bitcoin/bips/blob/master/bip-0173.\n> mediawiki#rationale\n>\n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> attachments/20180529/d2332b98/attachment-0001.html>\n> -------------- next part --------------\n> A non-text attachment was scrubbed...\n> Name: signature.asc\n> Type: application/pgp-signature\n> Size: 833 bytes\n> Desc: Message signed with OpenPGP\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> attachments/20180529/d2332b98/attachment-0001.sig>\n>\n> ------------------------------\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> End of bitcoin-dev Digest, Vol 36, Issue 46\n> *******************************************\n>\n\n\n\n-- \nShiva S\nCEO @ Blockonomics <https://www.blockonomics.co>\nDecentralized and Permissionless Payments\nKnow more about us here\n<https://www.blockonomics.co/docs/blockonomics-brochure.pdf>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180530/a8d03f62/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2018-05-30T14:08:08",
                "message_text_only": "On Wed, May 30, 2018 at 6:30 AM, shiva sitamraju via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> The idea to add birthdate and gap limit sounds very good and addresses lots\n> of problems users are facing.\n>\n> However, adding birthday to keys breaks two basic properties\n>\n> - Visually Comparing two keys to find if they are same (Important)\n\nCan you explain exactly what you mean there? I can think of to\nplausible meanings (that two valid keys could differ by only a single\nsymbol, which wouldn't be true due to the checksum and could be made\neven stronger if we thought that would be useful or I think you could\nalso be complaining that the same \"key material\" could be encoded two\nways which I think is both harmless and unavoidable for anything\nversioned).\n\n> - Different wallet software could set different birthday/gap limit. creating\n> different xpub/xprv for the same set of mathematically derived individual\n> keys. This removes the decoupling between key and wallet metadata\n\nPersonally, I think it's a mistake to believe that any key format can\nreally make private keying material strongly compatible between\nwallets. At best you can hope for a mostly compatible kind of recovery\nhandling.\n\nBut the lookahead amount may be pretty integral to the design of the\nsoftware, so signaling it may not mean the other side can obey the\nsignal... but that wouldn't make the signal completely useless."
            },
            {
                "author": "Jonas Schnelli",
                "date": "2018-05-30T19:03:46",
                "message_text_only": "Hi\n\n> - Visually Comparing two keys to find if they are same (Important)\n> - Different wallet software could set different birthday/gap limit. creating different xpub/xprv for the same set of mathematically derived individual keys. This removes the decoupling between key and wallet metadata\n\nWhat would be the downside of encoding the same key with different metadata (resulting in different \"visual strings\u201c)?\nIf you import it into the same software, it would be trivial to detect it. If you import it into another software, it probably doesn\u2019t matter.\n\nVisual comparing is eventually a broken concept (agree with Greg) and I doubt that this property is important, and IMHO basic metadata seems more important then this - very likely irrelevant - visual property.\n\nAlso, I think a recovery based on a sole xpriv (or + limited amount of meta-data as described in this proposal) is a disaster recovery (or forensic recovery).\n\nLong term, I would wish, if wallet-metadata including transaction based user metadata would be backed up - after encrypted with a key that can be derived from the seed - in a way, where you need the seed to recover that backup thus it can be stored in cheap, insecure spaces.\n\n> \n> In fact, same could be argued to add birthday to WIF private key format to let wallet discover funds faster.\n> \n\nThe proposal I made can be seen as a replacement for WIF (it can replace WIF and xpriv/xpub) since it can encode a single private key into 275bits (still pretty short Bech32 string).\n\n/jonas\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180530/ff430ac7/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "New serialization/encoding format for key material",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "shiva sitamraju",
                "Gregory Maxwell",
                "Jonas Schnelli"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 20750
        }
    },
    {
        "title": "[bitcoin-dev] BIP suggestion: PoW proportional to block transaction sum",
        "thread_messages": [
            {
                "author": "Darren Weber",
                "date": "2018-05-30T16:17:29",
                "message_text_only": "Apologies for brevity, noob here and just throwing out an idea in case it's\nuseful (probably already covered somewhere, but I haven't got time to do\nall the necessary background research).\n\n>From https://github.com/bitcoin/bitcoin/issues/13342\n\nSuggestion:  To make it more difficult for a malicious attacker to reap\nquick rewards by double-spending large amounts with a relatively brief\nmajority of the network hashing power, introduce a hash workload that is\nproportional to the sum of transactions in a block (probably the sum of the\nabsolute values, and a \"proportionality function\" could be linear or\nexponential).  The motivation is to make it more difficult for malicious\nattacks to hash-power their way through a few large transactions.\nObviously, there are costs in greater transaction delays (and fees?) for\nlarger amounts (absolute value).\n\nIf there is original value in the idea, I can try to make time to follow-up\nwith a better BIP proposal.\n\n-- \nDarren\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20180530/54925552/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP suggestion: PoW proportional to block transaction sum",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Darren Weber"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1150
        }
    },
    {
        "title": "[bitcoin-dev] SIGHASH2 for version 1 witness programme",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2018-05-31T18:35:41",
                "message_text_only": "Since 2016, I have made a number of proposals for the next generation of script. Since then, there has been a lot of exciting development on this topic. The most notable ones are Taproot and Graftroot proposed by Maxwell. It seems the most logical way is to implement MAST and other new script functions inside Taproot and/or Graftroot. Therefore, I substantially simplified my earlier proposal on SIGHASH2. It is a superset of the existing SIGHASH and the BIP118 SIGHASH_NOINPUT, with further flexibility but not being too complicated. It also fixes some minor problems that we found in the late stage of BIP143 review. For example, the theoretical (but not statistical) possibility of having same SignatureHash() results for a legacy and a witness transaction. This is fixed by padding a constant at the end of the message so collision would not be possible.\n\nA formatted version and example code could be found here:\nhttps://github.com/jl2012/bips/blob/sighash2/bip-sighash2.mediawiki\nhttps://github.com/jl2012/bitcoin/commits/sighash2\n\n\n========\n\nBIP: YYY\n  Layer: Consensus (soft fork)\n  Title: Signature checking operations in version 1 witness program\n  Author: Johnson Lau <jl2012 at xbt.hk>\n  Comments-Summary: No comments yet.\n  Comments-URI: https://github.com/bitcoin/bips/wiki/Comments:BIP-0YYY\n  Status: Draft\n  Type: Standards Track\n  Created: 2017-07-19\n  License: BSD-3-Clause\n\n\n*Abstract\n\nThis BIP defines signature checking operations in version 1 witness program.\n\n*Motivation\n\nUse of compact signatures to save space.\n\nMore SIGHASH options, more flexibility\n\n*Specification\n\nThe following specification is applicable to OP_CHECKSIG and OP_CHECKSIGVERIFY in version 1 witness program.\n\n**Public Key Format\n\nThe pubic key MUST be exactly 33 bytes.\n\nIf the first byte of the public key is a 0x02 or 0x03, it MUST be a compressed public key. The signature is a Schnorr signature (To be defined separately)\n\nIf the first byte of the public key is neither 0x02 nor 0x03, the signature is assumed valid. This is for future upgrade.\n\n**Signature Format\n\nThe following rules apply only if the first byte of the public key is a 0x02 or 0x03.\n\nIf the signature size is 64 to 66 byte, it MUST be a valid Schnorr signature or the script execution MUST fail (cf. BIP146 NULLFAIL). The first 32-byte is the R value in big-endian. The next 32-byte is the S value in big-endian. The remaining data, if any, denotes the hashtype in little-endian (0 to 0xffff).\n\nhashtype MUST be minimally encoded. Any trailing zero MUST be removed.\n\nIf the signature size is zero, it is accepted as the \"valid failing\" signature for OP_CHECKSIG to return a FALSE value to the stack. (cf. BIP66)\n\nThe script execution MUST fail with a signature size not 0, 64, 65, or 66-byte.\n\n**New hashtype definitions\n\nhashtype and the SignatureHash function are re-defined:\n\n  Double SHA256 of the serialization of:\n     1. nVersion (4-byte little endian)\n     2. hashPrevouts (32-byte hash)\n     3. hashSequence (32-byte hash)\n     4. outpoint (32-byte hash + 4-byte little endian)\n     5. scriptCode (serialized as scripts inside CTxOuts)\n     6. nAmount (8-byte little endian)\n     7. nSequence (4-byte little endian)\n     8. hashOutputs (32-byte hash)\n     9. nLocktime (4-byte little endian)\n    10. nInputIndex (4-byte little endian)\n    11. nFees (8-byte little endian)\n    12. hashtype (4-byte little endian)\n    13. sigversion (4-byte little endian for the fixed value 0x01000000)\n\nThe bit 0 to 3 of hashtype denotes a value between 0 and 15:\n\n\t\u2022 If the value is 1, the signature is invalid.\n\t\u2022 If the value is 3 or below, hashPrevouts is the hash of all input, same as defined in BIP143. Otherwise, it is 32-byte of 0x0000......0000.\n\t\u2022 If the value is 7 or below, outpoint is the COutPoint of the current input. Otherwise, it is 36-byte of 0x0000......0000.\n\t\u2022 If the value is 0, hashSequence is the hash of all sequence, same as defined in BIP143. Otherwise, it is 32-byte of 0x0000......0000.\n\t\u2022 If the value is even (including 0), nSequence is the nSequence of the current input. Otherwise, it is 0x00000000.\n\t\u2022 If the value is 6, 7, 10, 11, 14, or 15, nInputIndex is 0x00000000. Otherwise, it is the index of the current input.\n\t\u2022 If the value is 11 or below, nAmount is the value of the current input (same as BIP143). Otherwise, it is 0x0000000000000000.\n\nThe bit 4 and 5 of hashtype denotes a value between 0 and 3:\n\n\t\u2022 If the value is 0, hashOutputs is same as the SIGHASH_ALL case in BIP143 as a hash of all outputs.\n\t\u2022 If the value is 1, the signature is invalid.\n\t\u2022 If the value is 2, hashOutputs is same as the SIGHASH_SINGLE case in BIP143 as a hash of the matching output. If a matching output does not exist, hashOutputs is 32-byte of 0x0000......0000.\n\t\u2022 If the value is 3, hashOutputs is 32-byte of 0x0000......0000.\nIf bit 6 is set (SIGHASH2_NOFEE), nFees is 0x0000000000000000. Otherwise, it is the fee paid by the transaction.\nIf bit 7 is set (SIGHASH2_NOLOCKTIME), nLockTime is 0x00000000. Otherwise, it is the transaction nLockTime.\n\nIf bit 8 is set (SIGHASH2_NOVERSION), nVersion is 0x00000000. Otherwise, it is the transaction nVersion.\n\nIf bit 9 is set (SIGHASH2_NOSCRIPTCODE), scriptCode is an empty script. Otherwise, it is same as described in BIP143.\n\nBits 10 to 15 are reserved and ignored, but the signature still commits to their value as hashtype.\n\nhashtype of 0 is also known as SIGHASH2_ALL, which covers all the available options. In this case the singnature MUST be exactly 64-byte.\n\nhashtype of 0x3ff is also known as SIGHASH2_NONE, which covers nothing and is effectively forfeiting the right related to this public key to anyone.\n\n*Rationale\n\n**Signature Format\n\nThe current DER format is a complete waste of block space. The new format saves ~8 bytes per signature.\n\n**New hashtype definitions\n\nThe default and most commonly used case is SIGHASH2_ALL. Making it zero size to save space. As a result, the bit flags are defined in a negative way (e.g. NOLOCKTIME)\n\nWhy decouple INPUT and SEQUENCE? Maybe you want NOINPUT but still have a relative lock-time?\n\nWhy some combinations are missing? To save some bits for useless flags. If you sign all inputs, you must know its index and value. If you sign only this input, you must know its value, but probably don't know its index in the input vector.\n\nWhy only allow signing all SEQUENCE if all INPUT are signed? It doesn't make much sense if you care about their sequence without even knowing what they are.\n\nWhy signing INPUTINDEX? Legacy and BIP143 SINGLE|ANYONECANPAY behaves differently for input index. Better make it explicit and optional.\n\nWhy signing FEE? Sometimes you don't sign all inputs / outputs but still want to make sure the fees amount is correct.\n\nPutting NOVERSION and NOSCRIPTCODE in the second byte makes most signatures below 66 bytes:\n\n\t\u2022 NOVERSION: Currently the only use of transaction version is to enforce BIP68. It could be safely assumed that version 2 is used. The only case one would like to use NOVERSION is to make the signature compatible with some unknown new features that use a different transaction version.\n\t\u2022 NOSCRIPTCODE: It would be very rare if one could make a signature without knowing what the script is (at least they know the public key). The only scenario that a NOSCRIPTCODE is really needed is the public key being reused in different scripts, and the user wants to use a single signature to cover all these scripts.\nReserved bits: These bits are ignored but should normally be unset. Users MUST NOT set these bits until they are defined by a future proposal, or they might lose money.\nWhy sigversion? Make sure the message digest won't collide with SIGHASH schemes in the past (legacy and BIP143) and future (which will use a different sigversion).\n\n*Examples\n\nEquivalent SIGHASH2 value for other SIGHASH schemes:\nLegacy/BIP143 ALL: 0 (commit to everything)\nLegacy/BIP143 SINGLE with matching output: 0x62 (all input, one sequence, one output, no fee)\nLegacy SINGLE without matching output: 0x3ff (Not exactly. Both signatures commit to nothing, but the legacy one is valid only without a matched output. Practically, they are both \"wildcard\" signatures that allow anyone to spend any related UTXO)\nLegacy/BIP143 NONE: 0x72 (all input, one sequence, no output, no fee)\nLegacy/BIP143 ANYONECANPAY|ALL: 0x46 (one input without index, one sequence, all output, no fee)\nLegacy ANYONECANPAY|SINGLE with matching output: 0x64 (one input with index, one sequence, one output, no fee)\nLegacy/BIP143 ANYONECANPAY|NONE: 0x76 (one input without index, one sequence, no output, no fee)\nBIP143 SINGLE without matching output: 0x62 (all input, one sequence, no output, no fee)\nBIP143 ANYONECANPAY|SINGLE with matching output: 0x66 (one input without index, one sequence, one output, no fee)\nBIP143 ANYONECANPAY|SINGLE without matching output: 0x66 (one input without index, one sequence, no output, no fee)\nBIP118 NOINPUT: 0x14b (no input but with value, no index, no sequence, no fee, no scriptcode)\n\nNotes:\n\n1. In legacy and BIP143 SIGHASH, only ALL but not other types implicitly commits to the fee paid.\n2. Legacy SIGHASH always implicitly commits to the input value. BIP143 and BIP118 commits to that explicitly.\n3. Legacy and BIP143 SIGHASH behaves differently in the case of SINGLE without matching output. In legacy SIGHASH it is a true \"wildcard signature\" that allows anyone to spend any related UTXO. In BIP143 such signature applies only to a specific UTXO.\n4. BIP143 ANYONECANPAY never commits to the input index. Legacy ANYONECANPAY|SINGLE implicitly commits to the input index.\n\n*Backward compatibility\n\nThis is a soft-fork.\n\n*Deployment\n\nExact details TBD.\n\n*Reference Implementation\n\nhttps://github.com/jl2012/bitcoin/commits/sighash2 (To be updated)\n\n*Copyright\n\nThis document is licensed as BSD 3-clause."
            }
        ],
        "thread_summary": {
            "title": "SIGHASH2 for version 1 witness programme",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Johnson Lau"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9835
        }
    },
    {
        "title": "[bitcoin-dev] Disallow insecure use of SIGHASH_SINGLE",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2018-05-31T18:53:01",
                "message_text_only": "I\u2019ve made a PR to add a new policy to disallow using SIGHASH_SINGLE without matched output:\n\nhttps://github.com/bitcoin/bitcoin/pull/13360\n\nSignature of this form is insecure, as it commits to no output while users might think it commits to one. It is even worse in non-segwit scripts, which is effectively SIGHASH_NOINPUT|SIGHASH_NONE, so any UTXO of the same key could be stolen. (It\u2019s restricted to only one UTXO in segwit, but it\u2019s still like a SIGHASH_NONE.)\n\nThis is one of the earliest unintended consensus behavior. Since these signatures are inherently unsafe, I think it does no harm to disable this unintended \u201cfeature\u201d with a softfork. But since these signatures are currently allowed, the first step is to make them non-standard."
            }
        ],
        "thread_summary": {
            "title": "Disallow insecure use of SIGHASH_SINGLE",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Johnson Lau"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 742
        }
    }
]