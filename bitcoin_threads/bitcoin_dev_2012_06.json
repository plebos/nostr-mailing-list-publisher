[
    {
        "title": "[Bitcoin-development] Full Clients in the future - Blockchain management",
        "thread_messages": [
            {
                "author": "Alan Reiner",
                "date": "2012-06-02T15:40:27",
                "message_text_only": "Devs,\n\nI have decided to upgrade Armory's blockchain utilities, partly out of \nnecessity due to a poor code decision I made before I even decided I was \nmaking a client.  In an effort to avoid such mistakes again, I want to \ndo it \"right\" this time around, and realize that this is a good \ndiscussion for all the devs that will have to deal with this eventually...\n\nThe part I'm having difficulty with, is the idea that in a few years \nfrom now, it just may not be feasible to hold transactions \nfile-/pointers/ in RAM, because even that would overwhelm standard RAM \nsizes.  Without any degree of blockchain compression, I see that the \nmost general, scalable solution is probably a complicated one.\n\nOn the other hand, where this fails may be where we have already \npredicted that the network will have to split into \"super-nodes\" and \n\"lite nodes.\"  In which case, this discussion is still a good one, but \njust directed more towards the super-nodes.  But, there may still be a \npoint at which super-nodes don't have enough RAM to hold this data...\n\n(1)  As for how small you can get the data:  my original idea was that \nthe entire blockchain is stored on disk as blkXXXX.dat files.  I store \nall transactions as 10-byte \"file-references.\"  10 bytes would be\n\n     -- X in blkX.dat (2 bytes)\n     -- Tx start byte (4 bytes)\n     -- Tx size bytes (4 bytes)\n\nThe file-refs would be stored in a multimap indexed by the first 6 bytes \nof the tx-hash.  In this way, when I search the multimap, I potentially \nget a list of file-refs, and I might have to retrieve a couple of tx \nfrom disk before finding the right one, but it would be a good trade-off \ncompared to storing all 32 bytes (that's assuming that multimap nodes \ndon't have too much overhead).\n\nBut even with this, if there are 1,000,000,000 transactions in the \nblockchain, each node is probably 48 bytes  (16 bytes + map/container \noverhead), then you're talking about 48 GB to track all the data in \nRAM.  mmap() may help here, but I'm not sure it's the right solution\n\n(2) What other ways are there, besides some kind of blockchain \ncompression, to maintain a multi-terabyte blockchain, assuming that \nstoring references to each tx would overwhelm available RAM?   Maybe \nthat assumption isn't necessary, but I think it prepares for the worst.\n\nOr maybe I'm too narrow in my focus.  How do other people envision this \nwill be handled in the future.  I've heard so many vague notions of \n\"well we could do /this/ or /that/, or it wouldn't be hard to do /that/\" \nbut I haven't heard any serious proposals for it.  And while I believe \nthat blockchain compression will become ubiquitous in the future, not \neveryone believes that, and there will undoubtedly be users/devs that \n/want/ to maintain everything under all circumstances.\n\n-Alan\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120602/e99d820c/attachment.html>"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-03T14:17:42",
                "message_text_only": "Yeah, for actually storing transactions the approach Satoshi uses of\nrelying on a database engine makes sense and is what the code already does,\nso I'm not sure why this is a problem.\n\nThe real problem with Satoshis code for scaling down to smaller devices\n(and one day desktops too) is the need to store all the chain headers in\nRAM. BitcoinJ avoids this but just creates more problems for itself in\nother places, partly because we also try to avoid a database engine\n(read/write traffic on phones can be insanely expensive, especially on\nolder ones, and so sqlite is known to be a serious cause of performance\npain on android apps).\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120603/639427f3/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Full Clients in the future - Blockchain management",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Mike Hearn",
                "Alan Reiner"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3800
        }
    },
    {
        "title": "[Bitcoin-development] Fwd: Re: Full Clients in the future - Blockchain management",
        "thread_messages": [
            {
                "author": "Alan Reiner",
                "date": "2012-06-02T17:15:26",
                "message_text_only": "(response from Doug forwarded below)\n\nIt's a very good point.  I have no experience with database engines.  I \nhad assumed that in most cases, data could always be indexed in RAM, and \nwanted to know where to go when that's not the case.  I will look into \nthat solution, further.\n\nI am very interested to solve the blockchain compression problem, and \nthink I've got a great way that will not just compress the blockchain, \nbut improve the network for lightweight clients.  But the idea is not \nfully formed yet, so I was holding off...\n\n\n\n-------- Original Message --------\nSubject: \tRe: [Bitcoin-development] Full Clients in the future - \nBlockchain management\nDate: \tSat, 2 Jun 2012 12:07:44 -0500\nFrom: \tDouglas Huff <mith at jrbobdobbs.org>\nTo: \tAlan Reiner <etotheipi at gmail.com>\n\n\n\nI think you're trying to solve something a little out of scope, really. \nMost of the issues aren't really issues for other clients using \nestablished storage mechanisms (bdb,SQLite,etc) and they're using them \nprecisely because this is a problem that people have been working on for \ndecades and a poor candidate for reinvention.\n\nIf you really look at what you're proposing it's fundamentally how bdb \noperates except your indexing format is usage domain specific and you're \nin charge of all the resource management semantics. While at the same \ntime you'll be missing many of the newer features that make working \nwith/recovering/diagnosing issues in the storage layer easier.\n\nIf you're really wanting to talk about pruning methods to prevent the \nmassive amount of duplicated; but no longer pertinent, data that's a \ndifferent story and please continue. :)\n\n-- \nDouglas Huff\n\nOn Jun 2, 2012, at 10:40, Alan Reiner <etotheipi at gmail.com \n<mailto:etotheipi at gmail.com>> wrote:\n\n> Devs,\n>\n> I have decided to upgrade Armory's blockchain utilities, partly out of \n> necessity due to a poor code decision I made before I even decided I \n> was making a client.  In an effort to avoid such mistakes again, I \n> want to do it \"right\" this time around, and realize that this is a \n> good discussion for all the devs that will have to deal with this \n> eventually...\n>\n> The part I'm having difficulty with, is the idea that in a few years \n> from now, it just may not be feasible to hold transactions \n> file-/pointers/ in RAM, because even that would overwhelm standard RAM \n> sizes.  Without any degree of blockchain compression, I see that the \n> most general, scalable solution is probably a complicated one.\n>\n> On the other hand, where this fails may be where we have already \n> predicted that the network will have to split into \"super-nodes\" and \n> \"lite nodes.\"  In which case, this discussion is still a good one, but \n> just directed more towards the super-nodes.  But, there may still be a \n> point at which super-nodes don't have enough RAM to hold this data...\n>\n> (1)  As for how small you can get the data:  my original idea was that \n> the entire blockchain is stored on disk as blkXXXX.dat files.  I store \n> all transactions as 10-byte \"file-references.\"  10 bytes would be\n>\n>     -- X in blkX.dat (2 bytes)\n>     -- Tx start byte (4 bytes)\n>     -- Tx size bytes (4 bytes)\n>\n> The file-refs would be stored in a multimap indexed by the first 6 \n> bytes of the tx-hash.  In this way, when I search the multimap, I \n> potentially get a list of file-refs, and I might have to retrieve a \n> couple of tx from disk before finding the right one, but it would be a \n> good trade-off compared to storing all 32 bytes (that's assuming that \n> multimap nodes don't have too much overhead).\n>\n> But even with this, if there are 1,000,000,000 transactions in the \n> blockchain, each node is probably 48 bytes  (16 bytes + map/container \n> overhead), then you're talking about 48 GB to track all the data in \n> RAM.  mmap() may help here, but I'm not sure it's the right solution\n>\n> (2) What other ways are there, besides some kind of blockchain \n> compression, to maintain a multi-terabyte blockchain, assuming that \n> storing references to each tx would overwhelm available RAM?   Maybe \n> that assumption isn't necessary, but I think it prepares for the worst.\n>\n> Or maybe I'm too narrow in my focus.  How do other people envision \n> this will be handled in the future.  I've heard so many vague notions \n> of \"well we could do /this/ or /that/, or it wouldn't be hard to do \n> /that/\" but I haven't heard any serious proposals for it.  And while I \n> believe that blockchain compression will become ubiquitous in the \n> future, not everyone believes that, and there will undoubtedly be \n> users/devs that /want/ to maintain everything under all circumstances.\n>\n> -Alan\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net \n> <mailto:Bitcoin-development at lists.sourceforge.net>\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120602/e8fd5daf/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Fwd: Re: Full Clients in the future - Blockchain management",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Alan Reiner"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5519
        }
    },
    {
        "title": "[Bitcoin-development] Defeating the block withholding attack",
        "thread_messages": [
            {
                "author": "Luke-Jr",
                "date": "2012-06-03T00:52:14",
                "message_text_only": "Analysis, comments, constructive criticism, etc welcome for the following:\n\n==Background==\nAt present, an attacker can harm a pool by intentionally NOT submitting shares \nthat are also valid blocks. All pools are vulnerable to this attack, whether \ncentralized or decentralized and regardless of reward system used. The \nattack's effectiveness is proportional to ratio of the attacker's hashrate to \nthe rest of the pool.\n\nThere are obvious solutions that can be used to defeat this attack on \ncentralized pools. For example, including a secret in the coinbase transaction \nthat is accepted by the network as a partial preimage proof-of-work. All these \nsolutions require changes to Bitcoin's proof-of-work acceptance terms, and \nsince centralized pools can be harmful to the network's security, these rule \nchanges are not likely to gain enough acceptance among the greater Bitcoin \ncommunity.\n\n==Proposed Solution==\nPlease comment on the viability of this new proof-of-work algorithm, which I \nthink should be viable for even decentralized pools:\n\nBlocks are accepted at a lower difficulty N (choosable by the pool; eg, the \nshare difficulty) iff they are submitted with a candidate for the next block \nand SHA256(SHA256(NewBlockHash + NextBlockCandidateHash)) meets difficulty M.\nThe relationship between M and N must be comparable to the normal network \ndifficulty; details on the specifics of this can be figured out later, ideally \nby someone more qualified than me. M and N must be chosen prior to searching \nfor the block: it should be safe to steal some always-zero bytes from the \nprevblock header for this.\n\nThis algorithm should guarantee that every share has an equal chance of being \na valid block at the time it is found, and that which ones are actually blocks \ncannot be known until the subsequent block is found. Thus, attackers have no \nway to identify which shares to withhold even while they have full knowledge \nof the shares/blocks themselves.\n\n==Backward Compatibility==\nObviously, this change creates a hard-fork in the blockchain. I propose that \nif it solves the block withholding risk, the gain is sufficient that the \ncommunity may approve a hard-fork to take place 1-2 years from consensus.\n\nSince mining continues to use a double-SHA256 on a fixed 80 byte header, \nexisting miners, FPGAs, etc should work unmodified. Poolservers will need to \nadapt significantly."
            },
            {
                "author": "Peter Vessenes",
                "date": "2012-06-04T01:43:42",
                "message_text_only": "On Sat, Jun 2, 2012 at 8:52 PM, Luke-Jr <luke at dashjr.org> wrote:\n\n> Analysis, comments, constructive criticism, etc welcome for the following:\n>\n> ==Background==\n> At present, an attacker can harm a pool by intentionally NOT submitting\n> shares\n> that are also valid blocks. All pools are vulnerable to this attack,\n> whether\n> centralized or decentralized and regardless of reward system used. The\n> attack's effectiveness is proportional to ratio of the attacker's hashrate\n> to\n> the rest of the pool.\n>\n>\nI'm unclear on the economics of this attack; we spent a bit of time talking\nabout it a few months ago at CoinLab and decided not to worry about it for\nright now.\n\nDoes it have asymmetric payoff for an attacker, that is, over time does it\npay them more to spend their hashes attacking than just mining?\n\nMy gut is that it pays less well than mining, meaning I think this is\nlikely a small problem in the aggregate, and certainly not something we\nshould try and fork the blockchain for until there's real pain.\n\nConsider, for instance, whether it pays better than just mining bitcoins\nand spending those on 'bonuses' for getting users to switch from a pool you\nhate.\n\nWatson, I don't believe the attack signature you mention is a factor here,\nsince the pool controls the merkle, only that pool will benefit from block\nsubmission. The nonce / coinbase combo is worthless otherwise, and so this\nattack is just in brief \"get lucky, but don't submit.\"\n\nSo, can anyone enlighten me as to some actual estimates of badness for this\nattack?\n\nPeter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120603/40300cbd/attachment.html>"
            },
            {
                "author": "Luke-Jr",
                "date": "2012-06-04T02:04:55",
                "message_text_only": "On Monday, June 04, 2012 1:43:42 AM Peter Vessenes wrote:\n> Does it have asymmetric payoff for an attacker, that is, over time does it\n> pay them more to spend their hashes attacking than just mining?\n\nThat depends on the pool's reward scheme. Some complicated forms are capable \nof getting \"bonus\" earnings out of the pool. Under all systems, the attacker \nat least gains the \"hurt the pool\" benefit. Given the frequency of DDoS \nattacks on pools, it is clear there are people who will even pay for attacks \nthat provide no other benefit than harming pools. Under all systems, the \nattacker doesn't lose out in any significant way.\n\n> My gut is that it pays less well than mining, meaning I think this is\n> likely a small problem in the aggregate, and certainly not something we\n> should try and fork the blockchain for until there's real pain.\n\nIf we wait until there's real pain, it will be a painful fork. If we plan it \n1-2 years out, people have time to upgrade on their own before it breaks.\n\n> Consider, for instance, whether it pays better than just mining bitcoins\n> and spending those on 'bonuses' for getting users to switch from a pool you\n> hate.\n\nWith this attack, attackers can hurt the pool's \"luck factor\" *and* spend the \nbitcoins they earn to bribe users away."
            },
            {
                "author": "Mike Koss",
                "date": "2012-06-04T20:49:48",
                "message_text_only": "As I understand the attack, the attacker gets compensated for the shares\nthey earn, but the pool will be denied any valid blocks found.  The\nattacker DOES NOT have access to the Bitcoins earned in the unreported\nblock (only the mining pool has access to the Coinbase address and\ntransactions in the block).\n\nSo it's a zero-net-cost attack for the attacker (but no chance of making a\nprofit) to hurt the pool operator.  The only way to detect such an attack\nnow is to look for \"unlucky\" miners; at the current difficulty, you can't\ndetect this cheat until many millions of shares have been earned w/o a\nqualifying block.  Since an attacker can also create many fake identities,\nthey can avoid detection indefinitely by abandoning each account after a\nmillion earned shares.\n\nI don't understand your proposal for fixing this.  You would have to come\nup with a scheme where:\n\n- The miner can detect a qualifying hash to earn a share.\n- Not be able to tell if the hash is for a valid block.\n\nThe way I would do this is to have a secret part (not shared with the\nminers) of a block that is part of the merkle hash, which is also used in a\nsecondary hash.  Difficulty is then divide into two parts: the first,\nsolved by the miner (earning a \"share\" - e.g., 1 in 4 Billion hashes).  And\na second, solved by the pool (1 in Difficulty shares).  A valid block would\nhave to exhibit a valid Share Hash AND a valid Pool Hash in order to be\naccepted.\n\nThis would be a very major change to the Block structure.  Given that\nattackers do not have direct monetary gain from this attack, I'm not sure\nwe can justify it at this point.\n\nOn Sun, Jun 3, 2012 at 7:04 PM, Luke-Jr <luke at dashjr.org> wrote:\n\n> On Monday, June 04, 2012 1:43:42 AM Peter Vessenes wrote:\n> > Does it have asymmetric payoff for an attacker, that is, over time does\n> it\n> > pay them more to spend their hashes attacking than just mining?\n>\n> That depends on the pool's reward scheme. Some complicated forms are\n> capable\n> of getting \"bonus\" earnings out of the pool. Under all systems, the\n> attacker\n> at least gains the \"hurt the pool\" benefit. Given the frequency of DDoS\n> attacks on pools, it is clear there are people who will even pay for\n> attacks\n> that provide no other benefit than harming pools. Under all systems, the\n> attacker doesn't lose out in any significant way.\n>\n> > My gut is that it pays less well than mining, meaning I think this is\n> > likely a small problem in the aggregate, and certainly not something we\n> > should try and fork the blockchain for until there's real pain.\n>\n> If we wait until there's real pain, it will be a painful fork. If we plan\n> it\n> 1-2 years out, people have time to upgrade on their own before it breaks.\n>\n> > Consider, for instance, whether it pays better than just mining bitcoins\n> > and spending those on 'bonuses' for getting users to switch from a pool\n> you\n> > hate.\n>\n> With this attack, attackers can hurt the pool's \"luck factor\" *and* spend\n> the\n> bitcoins they earn to bribe users away.\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n\n\n\n-- \nMike Koss\nCTO, CoinLab\n(425) 246-7701 (m)\n\nA Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need\nto know about Bitcoins.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120604/ed21dae7/attachment.html>"
            },
            {
                "author": "Luke-Jr",
                "date": "2012-06-04T21:05:25",
                "message_text_only": "On Monday, June 04, 2012 8:49:48 PM Mike Koss wrote:\n> As I understand the attack, the attacker gets compensated for the shares\n> they earn, but the pool will be denied any valid blocks found.  The\n> attacker DOES NOT have access to the Bitcoins earned in the unreported\n> block (only the mining pool has access to the Coinbase address and\n> transactions in the block).\n\nWith decentralized pools, the attacker does have access to the block, and can \npotentially submit it to the Bitcoin network directly bypassing the pool if it \nbenefits them to do so.\n\n> So it's a zero-net-cost attack for the attacker (but no chance of making a\n> profit) to hurt the pool operator. \n\nBecause of the above, there is a possibility an attacker can make a profit.\n\n> The only way to detect such an attack now is to look for \"unlucky\" miners;\n> at the current difficulty, you can't detect this cheat until many millions\n> of shares have been earned w/o a qualifying block.  Since an attacker can\n> also create many fake identities, they can avoid detection indefinitely by\n> abandoning each account after a million earned shares.\n\nThere are other modes of detection, but nobody has bothered to implement them \nsince attackers can easily do a simple workaround in an arms race.\n\n> I don't understand your proposal for fixing this.  You would have to come\n> up with a scheme where:\n> \n> - The miner can detect a qualifying hash to earn a share.\n> - Not be able to tell if the hash is for a valid block.\n\nWith my proposal, miners can find shares, but won't know if it's a valid block \nuntil the subsequent block is also found (that subsequent block might not end \nup being a real block in the big picture).\n\n> The way I would do this is to have a secret part (not shared with the\n> miners) of a block that is part of the merkle hash, which is also used in a\n> secondary hash.  Difficulty is then divide into two parts: the first,\n> solved by the miner (earning a \"share\" - e.g., 1 in 4 Billion hashes).  And\n> a second, solved by the pool (1 in Difficulty shares).  A valid block would\n> have to exhibit a valid Share Hash AND a valid Pool Hash in order to be\n> accepted.\n\nThis only works for centralized pools, which are contrary to the health of the \nBitcoin network. Decentralized pools cannot have a secret."
            },
            {
                "author": "Mike Koss",
                "date": "2012-06-05T00:00:25",
                "message_text_only": "I don't understand how your proposal will work for decentralized pools -\ncan you explain it more concretely?\n\nWhat would the new block header look like?\n\nWhat is required for a share to to be earned?\n\nWhat is required for a block to be valid (added to Block Chain)?\n\nI don't think I understand what you mean by NextBlockCandidate.  Perhaps a\nconcrete example using difficulty 1.7 million would be instructive.\n\nOn Mon, Jun 4, 2012 at 2:05 PM, Luke-Jr <luke at dashjr.org> wrote:\n\n> On Monday, June 04, 2012 8:49:48 PM Mike Koss wrote:\n> > As I understand the attack, the attacker gets compensated for the shares\n> > they earn, but the pool will be denied any valid blocks found.  The\n> > attacker DOES NOT have access to the Bitcoins earned in the unreported\n> > block (only the mining pool has access to the Coinbase address and\n> > transactions in the block).\n>\n> With decentralized pools, the attacker does have access to the block, and\n> can\n> potentially submit it to the Bitcoin network directly bypassing the pool\n> if it\n> benefits them to do so.\n>\n> > So it's a zero-net-cost attack for the attacker (but no chance of making\n> a\n> > profit) to hurt the pool operator.\n>\n> Because of the above, there is a possibility an attacker can make a profit.\n>\n> > The only way to detect such an attack now is to look for \"unlucky\"\n> miners;\n> > at the current difficulty, you can't detect this cheat until many\n> millions\n> > of shares have been earned w/o a qualifying block.  Since an attacker can\n> > also create many fake identities, they can avoid detection indefinitely\n> by\n> > abandoning each account after a million earned shares.\n>\n> There are other modes of detection, but nobody has bothered to implement\n> them\n> since attackers can easily do a simple workaround in an arms race.\n>\n> > I don't understand your proposal for fixing this.  You would have to come\n> > up with a scheme where:\n> >\n> > - The miner can detect a qualifying hash to earn a share.\n> > - Not be able to tell if the hash is for a valid block.\n>\n> With my proposal, miners can find shares, but won't know if it's a valid\n> block\n> until the subsequent block is also found (that subsequent block might not\n> end\n> up being a real block in the big picture).\n>\n> > The way I would do this is to have a secret part (not shared with the\n> > miners) of a block that is part of the merkle hash, which is also used\n> in a\n> > secondary hash.  Difficulty is then divide into two parts: the first,\n> > solved by the miner (earning a \"share\" - e.g., 1 in 4 Billion hashes).\n>  And\n> > a second, solved by the pool (1 in Difficulty shares).  A valid block\n> would\n> > have to exhibit a valid Share Hash AND a valid Pool Hash in order to be\n> > accepted.\n>\n> This only works for centralized pools, which are contrary to the health of\n> the\n> Bitcoin network. Decentralized pools cannot have a secret.\n>\n\n\n\n-- \nMike Koss\nCTO, CoinLab\n(425) 246-7701 (m)\n\nA Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need\nto know about Bitcoins.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120604/2e959c8a/attachment.html>"
            },
            {
                "author": "Luke-Jr",
                "date": "2012-06-05T01:05:18",
                "message_text_only": "On Tuesday, June 05, 2012 12:00:25 AM Mike Koss wrote:\n> I don't understand how your proposal will work for decentralized pools -\n> can you explain it more concretely?\n> \n> What would the new block header look like?\n\nFor example (just a draft; in reality, merged mining would probably be\n             integrated in a hardfork)\n 4 bytes: Block version number = 2\n31 bytes: Hash of the block 2 back, except for the minimum last 8 bits of zero\n 1 byte : Share difficulty (measured in \"zero\" bits)\n 4 bytes: Timestamp\n 4 bytes: \"Bits\" (current target in compact format)\n 4 bytes: Nonce\n\n> What is required for a share to to be earned?\n\nThe final <share difficulty> bits (minimum 32) of the block header are zero.\n\n> What is required for a block to be valid (added to Block Chain)?\n\nThe hash of this block header, concatenated with a valid share candidate for \nthe next block header, must hash to a value less than the current target \noffset against the share difficulty (this algorithm may need adjustment).\n\n> I don't think I understand what you mean by NextBlockCandidate.  Perhaps a\n> concrete example using difficulty 1.7 million would be instructive.\n\nThe first share becomes a block only after a second share is found that \ncombined hashes to meet the real difficulty. That second share becomes a block \nwhen a third is found. Etc."
            }
        ],
        "thread_summary": {
            "title": "Defeating the block withholding attack",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Luke-Jr",
                "Mike Koss",
                "Peter Vessenes"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 16190
        }
    },
    {
        "title": "[Bitcoin-development] Fwd: Defeating the block withholding attack",
        "thread_messages": [
            {
                "author": "Watson Ladd",
                "date": "2012-06-03T03:40:41",
                "message_text_only": "On Sat, Jun 2, 2012 at 7:52 PM, Luke-Jr <luke at dashjr.org> wrote:\n> Analysis, comments, constructive criticism, etc welcome for the following:\n>\n> ==Background==\n> At present, an attacker can harm a pool by intentionally NOT submitting shares\n> that are also valid blocks. All pools are vulnerable to this attack, whether\n> centralized or decentralized and regardless of reward system used. The\n> attack's effectiveness is proportional to ratio of the attacker's hashrate to\n> the rest of the pool.\nThis attack has an obvious signature: getting outworked on the same\nblock as the pool was trying to verify, and always by the same person.\n>\n> There are obvious solutions that can be used to defeat this attack on\n> centralized pools. For example, including a secret in the coinbase transaction\n> that is accepted by the network as a partial preimage proof-of-work. All these\n> solutions require changes to Bitcoin's proof-of-work acceptance terms, and\n> since centralized pools can be harmful to the network's security, these rule\n> changes are not likely to gain enough acceptance among the greater Bitcoin\n> community.\n>\n> ==Proposed Solution==\n> Please comment on the viability of this new proof-of-work algorithm, which I\n> think should be viable for even decentralized pools:\n>\n> Blocks are accepted at a lower difficulty N (choosable by the pool; eg, the\n> share difficulty) iff they are submitted with a candidate for the next block\n> and SHA256(SHA256(NewBlockHash + NextBlockCandidateHash)) meets difficulty M.\n> The relationship between M and N must be comparable to the normal network\n> difficulty; details on the specifics of this can be figured out later, ideally\n> by someone more qualified than me. M and N must be chosen prior to searching\n> for the block: it should be safe to steal some always-zero bytes from the\n> prevblock header for this.\nSo the goal is to prevent the attacker double-dipping by submitting\ncycles to the pool, which if he\nfound a correct answer he could submit himself. I don't see how this\ndoes that: if he finds a valid\nblock he finds a valid block. Only if the operator has a secret is\nthis prevented.\n>\n> This algorithm should guarantee that every share has an equal chance of being\n> a valid block at the time it is found, and that which ones are actually blocks\n> cannot be known until the subsequent block is found. Thus, attackers have no\n> way to identify which shares to withhold even while they have full knowledge\n> of the shares/blocks themselves.\nThis further delays the finalization of a transaction. That's not a good thing.\n>\n> ==Backward Compatibility==\n> Obviously, this change creates a hard-fork in the blockchain. I propose that\n> if it solves the block withholding risk, the gain is sufficient that the\n> community may approve a hard-fork to take place 1-2 years from consensus.\n>\n> Since mining continues to use a double-SHA256 on a fixed 80 byte header,\n> existing miners, FPGAs, etc should work unmodified. Poolservers will need to\n> adapt significantly.\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n\n\n\n--\n\"Those who would give up Essential Liberty to purchase a little\nTemporary Safety deserve neither\u00a0 Liberty nor Safety.\"\n-- Benjamin Franklin\n\n\n-- \n\"Those who would give up Essential Liberty to purchase a little\nTemporary Safety deserve neither\u00a0 Liberty nor Safety.\"\n-- Benjamin Franklin"
            }
        ],
        "thread_summary": {
            "title": "Fwd: Defeating the block withholding attack",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Watson Ladd"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3918
        }
    },
    {
        "title": "[Bitcoin-development] getmemorypool",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2012-06-09T23:10:55",
                "message_text_only": "Hello everyone,\n\nLuke's getmemorypool/BIP22 pull request has been open for a\nlong time, and didn't receive too much discussion.\n\nI think that having a stable and flexible API for negotiating\nblock generation is important to be standardized. The fact that\nit allows moving block generation to specialized programs is a\nstep in the right direction. However, it seems to me that too\nfew people (myself included) understand all the details of\nBIP22 (or don't care enough) to judge their necessity. I gave\nup trying to follow all design decisions some time ago, and as\nit seems I'm not alone, nobody liked merging support for it in\nthe Satoshi client. This is a pity, and I hope the situation\ncan be improved soon.\n\nI'm sorry for being this late with these comments, but I think\nit's essential that the standard is not more complex than\nnecessary (making it as easy as possible to write either\nservers or clients for it), and perhaps even more important,\nthat its purpose and intended use cases are clear.\n\n>From what I understand, the three subrequests are template,\nproposal and submit. The general idea is that \n  1) a miner requests a block template\n  2) builds/modifies a block based on this, and optionally\n     uses propose to check whether the server is willing to\n     accept it before doing work\n  3) submits when valid proof-of-work is found\nI'd like to see this process described in the BIP at least,\nit too me way too long to extract this.\n\nRegarding the block template: is there a particular reason\nfor sending the full transactions (serialized in hex) both in\ntemplates and submissions? The server will always need to have\naccess to the transaction database anyway, and clients will\n(afaics) rarely care about the actual transactions. My\nsuggestion would be to just transfer txids - if the client is\ninterested in the actual transactions, we have the\ngettransaction RPC call already. This seems to be captured by\nthe several \"submit/*\" and \"share/*\" variations, but making\nit optional seems way more complex than just limiting the API\nto that way of working.\n\nThat's another thing that bothers me in the standard: too many\noptional features. In particular, I understand the usefulness of\nhaving some flexibility in what miner clients are allowed to\nmodify, but I'm unconvinced we need 10 individually selectable\nvariations. In particular:\n* coinbase outputs: can we just add a list of required coinbase\n  outputs (amount + scriptPubKey) to the template? If no\n  generation+fee amount remains, nothing can be added.\n* coinbase input: put the required part in the template;\n  miners can always add whatever they like. Is there any known\n  use case where a server would not allow a client to add\n  stuff to the coinbase?\n* noncerange limiting: if coinbase input variation is not\n  limited, there is certainly no reason to limit nonceranges.\n  This adds unnecessary complexity to clients, in my option.\n* time/*: put a minimum and maximum timestamp in the template\n  (i believe those are already there anyway). Anything in\n  between is valid.\n* transactions/add: what is the use case?\n* transactions/remove: i'd just standarize on having all\n  transactions be removable (perhaps except those marked\n  'required').\n* prevblock: one getmemorypool per new block shouldn't be\n  a problem imho, so do a longpoll instead of having the client\n  able to modify prevblock themselves.\n\nOne more thing that I do not like is often several ways for\nspecifying the same behaviour. For example, txrequires specifies\nthat the first N transactions are mandatory, a 'required' field\nin the transaction list itself specifies that that transaction is\nmandatory, and the lack of transactions as variation means that\nthey must not be touched at all. Pick one way that is flexible\nenough, and discard the others.\n\nIn summary, I'd like to see the standard simplified. I have\nno problem merging code that makes getmemorypool compliant to\na standard that is agreed upon, but like to understand it first.\n\nIn my opinion - but I'm certainly open to discussion here - the\nstandard could be simplified to:\n* getblocktemplate: create a new block template, and return it.\n  The result contains:\n  * bits, previousblockhash, version: as to be used in block\n  * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp\n    between (curtime - local_time_at_receipt + local_time),\n    decreased by mintimeoff and increased maxtimeoff\n  * expires, sigoplimit, sizelimit: unchanged\n  * subsidy: amount generated (5000000000 for now)\n  * coinbaseaux: what generated coinbase's scriptSig must start\n    with\n  * coinbaseoutputs: list of objects, each specifying a required\n    coinbase output. Each has fields:\n    * amount: sent amount\n    * scriptPubKey: hex serialized of the output script\n  * transactions: list of object, each specifying a suggested\n    transaction (except for the coinbase) in the generated block.\n    Each has fields:\n    * txid: transaction id\n    * depends: list of dependencies (txids of earlier objects in\n      this same transactions list).\n    * fee: fee generated by this transaction, which increases the\n      max output of the coinbase.\n    * required: if present, transaction may not be dropped.\n* submitblocktemplate: submit an object containing a hex serialized\n  block header, hex serialized coinbase transaction, and a list of\n  txids. Returns true or string describing the problem. Proof of\n  work is checked last, so that error is only returned if there is\n  no other problem with the suggested block (this allows it to\n  replace both propose and submit).\n\nAre there important use cases I'm missing?\n\n-- \nPieter"
            }
        ],
        "thread_summary": {
            "title": "getmemorypool",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Pieter Wuille"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5620
        }
    },
    {
        "title": "[Bitcoin-development] BIP22/getmemorypool",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2012-06-10T09:03:58",
                "message_text_only": "----- Forwarded message from Pieter Wuille <pieter.wuille at gmail.com> -----\n\nDate: Sun, 10 Jun 2012 01:10:54 +0200\nFrom: Pieter Wuille <pieter.wuille at gmail.com>\nTo: bitcoin-development at lists.sourceforge.net\nSubject: getmemorypool\nUser-Agent: Mutt/1.5.20 (2009-06-14)\n\nHello everyone,\n\nLuke's getmemorypool/BIP22 pull request has been open for a long time, and didn't receive too much discussion.\n\nI think that having a stable and flexible API for negotiating block generation is important to be standardized. The fact that it allows moving block generation to specialized programs is a step in the right direction. However, it seems to me that too few people (myself included) understand all the details of BIP22 (or don't care enough) to judge their necessity. I gave up trying to follow all design decisions some time ago, and as it seems I'm not alone, nobody liked merging support for it in the Satoshi client. This is a pity, and I hope the situation can be improved soon.\n\nI'm sorry for being this late with these comments, but I think it's essential that the standard is not more complex than necessary (making it as easy as possible to write either servers or clients for it), and perhaps even more important, that its purpose and intended use cases are clear.\n\n>From what I understand, the three subrequests are template, proposal and submit. The general idea is that \n  1) a miner requests a block template\n  2) builds/modifies a block based on this, and optionally uses propose to check whether the server is willing to accept it before doing work\n  3) submits when valid proof-of-work is found\nI'd like to see this process described in the BIP at least, it too me way too long to extract this.\n\nRegarding the block template: is there a particular reason for sending the full transactions (serialized in hex) both in templates and submissions? The server will always need to have access to the transaction database anyway, and clients will (afaics) rarely care about the actual transactions. My suggestion would be to just transfer txids - if the client is interested in the actual transactions, we have the gettransaction RPC call already. This seems to be captured by the several \"submit/*\" and \"share/*\" variations, but making it optional seems way more complex than just limiting the API to that way of working.\n\nThat's another thing that bothers me in the standard: too many optional features. In particular, I understand the usefulness of having some flexibility in what miner clients are allowed to modify, but I'm unconvinced we need 10 individually selectable variations. In particular: \n* coinbase outputs: can we just add a list of required coinbase outputs (amount + scriptPubKey) to the template? If no generation+fee amount remains, nothing can be added.\n* coinbase input: put the required part in the template; miners can always add whatever they like. Is there any known use case where a server would not allow a client to add stuff to the coinbase?\n* noncerange limiting: if coinbase input variation is not limited, there is certainly no reason to limit nonceranges. This adds unnecessary complexity to clients, in my option.\n* time/*: put a minimum and maximum timestamp in the template (i believe those are already there anyway). Anything in between is valid.\n* transactions/add: what is the use case?\n* transactions/remove: i'd just standarize on having all transactions be removable (perhaps except those marked 'required').\n* prevblock: one getmemorypool per new block shouldn't be a problem imho, so do a longpoll instead of having the client able to modify prevblock themselves.\n\nOne more thing that I do not like is often several ways for specifying the same behaviour. For example, txrequires specifies that the first N transactions are mandatory, a 'required' field in the transaction list itself specifies that that transaction is mandatory, and the lack of transactions as variation means that they must not be touched at all. Pick one way that is flexible enough, and discard the others.\n\nIn summary, I'd like to see the standard simplified. I have no problem merging code that makes getmemorypool compliant to a standard that is agreed upon, but like to understand it first. \n\nIn my opinion - but I'm certainly open to discussion here - the standard could be simplified to:\n* getblocktemplate: create a new block template, and return it. The result contains:\n  * bits, previousblockhash, version: as to be used in block\n  * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp between (curtime - local_time_at_receipt + local_time), decreased by mintimeoff and increased maxtimeoff\n  * expires, sigoplimit, sizelimit: unchanged\n  * subsidy: amount generated (5000000000 for now)\n  * coinbaseaux: what generated coinbase's scriptSig must start with\n  * coinbaseoutputs: list of objects, each specifying a required coinbase output. Each has fields:\n    * amount: sent amount\n    * scriptPubKey: hex serialized of the output script\n  * transactions: list of object, each specifying a suggested transaction (except for the coinbase) in the generated block. Each has fields:\n    * txid: transaction id\n    * depends: list of dependencies (txids of earlier objects in this same transactions list).\n    * fee: fee generated by this transaction, which increases the max output of the coinbase.\n    * required: if present, transaction may not be dropped.\n* submitblocktemplate: submit an object containing a hex serialized block header, hex serialized coinbase transaction, and a list of txids. Returns true or string describing the problem. Proof of work is checked last, so that error is only returned if there is no other problem with the suggested block (this allows it to replace both propose and submit).\n\nAre there important use cases I'm missing?\n\n-- \nPieter"
            },
            {
                "author": "Pieter Wuille",
                "date": "2012-06-10T14:18:47",
                "message_text_only": "Hello everyone,\n\nLuke's getmemorypool/BIP22 pull request has been open for a long time, and\ndidn't receive too much discussion.\n\nI think that having a stable and flexible API for negotiating block\ngeneration is important to be standardized. The fact that it allows moving\nblock generation to specialized programs is a step in the right direction.\nHowever, it seems to me that too few people (myself included) understand\nall the details of BIP22 (or don't care enough) to judge their necessity. I\ngave up trying to follow all design decisions some time ago, and as it\nseems I'm not alone, nobody liked merging support for it in the Satoshi\nclient. This is a pity, and I hope the situation can be improved soon.\n\nI'm sorry for being this late with these comments, but I think it's\nessential that the standard is not more complex than necessary (making it\nas easy as possible to write either servers or clients for it), and perhaps\neven more important, that its purpose and intended use cases are clear.\n\n>From what I understand, the three subrequests are template, proposal and\nsubmit. The general idea is that\n 1) a miner requests a block template\n 2) builds/modifies a block based on this, and optionally uses propose to\ncheck whether the server is willing to accept it before doing work\n 3) submits when valid proof-of-work is found\nI'd like to see this process described in the BIP at least, it too me way\ntoo long to extract this.\n\nRegarding the block template: is there a particular reason for sending the\nfull transactions (serialized in hex) both in templates and submissions?\nThe server will always need to have access to the transaction database\nanyway, and clients will (afaics) rarely care about the actual\ntransactions. My suggestion would be to just transfer txids - if the client\nis interested in the actual transactions, we have the gettransaction RPC\ncall already. This seems to be captured by the several \"submit/*\" and\n\"share/*\" variations, but making it optional seems way more complex than\njust limiting the API to that way of working.\n\nThat's another thing that bothers me in the standard: too many optional\nfeatures. In particular, I understand the usefulness of having some\nflexibility in what miner clients are allowed to modify, but I'm\nunconvinced we need 10 individually selectable variations. In particular:\n* coinbase outputs: can we just add a list of required coinbase outputs\n(amount + scriptPubKey) to the template? If no generation+fee amount\nremains, nothing can be added.\n* coinbase input: put the required part in the template; miners can always\nadd whatever they like. Is there any known use case where a server would\nnot allow a client to add stuff to the coinbase?\n* noncerange limiting: if coinbase input variation is not limited, there is\ncertainly no reason to limit nonceranges. This adds unnecessary complexity\nto clients, in my option.\n* time/*: put a minimum and maximum timestamp in the template (i believe\nthose are already there anyway). Anything in between is valid.\n* transactions/add: what is the use case?\n* transactions/remove: i'd just standarize on having all transactions be\nremovable (perhaps except those marked 'required').\n* prevblock: one getmemorypool per new block shouldn't be a problem imho,\nso do a longpoll instead of having the client able to modify prevblock\nthemselves.\n\nOne more thing that I do not like is often several ways for specifying the\nsame behaviour. For example, txrequires specifies that the first N\ntransactions are mandatory, a 'required' field in the transaction list\nitself specifies that that transaction is mandatory, and the lack of\ntransactions as variation means that they must not be touched at all. Pick\none way that is flexible enough, and discard the others.\n\nIn summary, I'd like to see the standard simplified. I have no problem\nmerging code that makes getmemorypool compliant to a standard that is\nagreed upon, but like to understand it first.\n\nIn my opinion - but I'm certainly open to discussion here - the standard\ncould be simplified to:\n* getblocktemplate: create a new block template, and return it. The result\ncontains:\n * bits, previousblockhash, version: as to be used in block\n * curtime, maxtimeoff, maxtimeoff: client chooses a timestamp between\n(curtime - local_time_at_receipt + local_time), decreased by mintimeoff and\nincreased maxtimeoff\n * expires, sigoplimit, sizelimit: unchanged\n * subsidy: amount generated (5000000000 for now)\n * coinbaseaux: what generated coinbase's scriptSig must start with\n * coinbaseoutputs: list of objects, each specifying a required coinbase\noutput. Each has fields:\n   * amount: sent amount\n   * scriptPubKey: hex serialized of the output script\n * transactions: list of object, each specifying a suggested transaction\n(except for the coinbase) in the generated block. Each has fields:\n   * txid: transaction id\n   * depends: list of dependencies (txids of earlier objects in this same\ntransactions list).\n   * fee: fee generated by this transaction, which increases the max output\nof the coinbase.\n   * required: if present, transaction may not be dropped.\n* submitblocktemplate: submit an object containing a hex serialized block\nheader, hex serialized coinbase transaction, and a list of txids. Returns\ntrue or string describing the problem. Proof of work is checked last, so\nthat error is only returned if there is no other problem with the suggested\nblock (this allows it to replace both propose and submit).\n\nAre there important use cases I'm missing?\n\n--\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120610/84564648/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2012-06-11T17:47:59",
                "message_text_only": "I think the sourceforge mailing list system had the hiccups this\nweekend; sorry for Pieter's messages appearing in your inbox multiple\ntimes, it is not his fault.\n\nI deleted the extra copies from the mailing list archives.\n\n\nAs for the contents of his message, since this mailing list was not\nworking discussion wandered into the pull request:\n  https://github.com/bitcoin/bitcoin/pull/936\n\nAssuming this mailing list is now fixed, I'd like to pull that\ndiscussion back here.  The executive summary:  Pieter and I feel like\nBIP 22 is overly complicated, and would like it to be simpler. I'd\nespecially like to hear what people think will be the \"will be used by\nlots of pool customers\" features and what are the \"will be used by\nless than 5% of pool customers\" features.\n\n\n-- \n--\nGavin Andresen"
            },
            {
                "author": "thomasV1 at gmx.de",
                "date": "2012-06-11T18:10:22",
                "message_text_only": "> discussion back here.  The executive summary:  Pieter and I feel like\n> BIP 22 is overly complicated, and would like it to be simpler. I'd\n> especially like to hear what people think will be the \"will be used by\n> lots of pool customers\" features and what are the \"will be used by\n> less than 5% of pool customers\" features.\n> \n\nwill be used by Electrum servers \n\n-- \nEmpfehlen Sie GMX DSL Ihren Freunden und Bekannten und wir\nbelohnen Sie mit bis zu 50,- Euro! https://freundschaftswerbung.gmx.de"
            },
            {
                "author": "Pieter Wuille",
                "date": "2012-06-12T10:50:40",
                "message_text_only": "On Mon, Jun 11, 2012 at 08:10:22PM +0200, thomasV1 at gmx.de wrote:\n> \n> > discussion back here.  The executive summary:  Pieter and I feel like\n> > BIP 22 is overly complicated, and would like it to be simpler. I'd\n> > especially like to hear what people think will be the \"will be used by\n> > lots of pool customers\" features and what are the \"will be used by\n> > less than 5% of pool customers\" features.\n> \n> will be used by Electrum servers \n\nPlease. This is not about whether getmemorypool is useful (at least I am\na big fan of BIP22's big picture). It's about whether it needs 20 optional\nfeatures.\n\n-- \nPieter"
            }
        ],
        "thread_summary": {
            "title": "BIP22/getmemorypool",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Pieter Wuille",
                "Gavin Andresen",
                "thomasV1 at gmx.de"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 13425
        }
    },
    {
        "title": "[Bitcoin-development] Bootstrapping full nodes post-pruning",
        "thread_messages": [
            {
                "author": "Mike Hearn",
                "date": "2012-06-10T23:06:50",
                "message_text_only": "Apologies if this has been discussed elsewhere. I don't recall us ever\nreaching a solid conclusion on it.\n\nA node that has pruned its block chain cannot serve the chain to new\nnodes. So there are three options for bootstrapping a newly installed\nnode:\n\n1) Have some kind of special archival nodes that never prune\n(advertised via the services field?). Encourage people to run them,\nsomehow.\n\n2) Ship a post-pruning block chain and tx index with the client\ndownloads, so the client starts up already bootstrapped.\n\n3) Some combination of both. It's safe to assume some people will keep\nunpruned chains around no matter what. But for many users (2) is\neasiest and archival nodes would be put under less load if they were\nused only by users who wish to fully bootstrap from only the code.\n\nI remember some people, Greg in particular, who were not a fan of\napproach (2) at all, though it has the benefit of speeding startup for\nnew users as there's no indexing overhead."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-11T15:39:20",
                "message_text_only": "On Sun, Jun 10, 2012 at 7:06 PM, Mike Hearn <mike at plan99.net> wrote:\n> I remember some people, Greg in particular, who were not a fan of\n> approach (2) at all, though it has the benefit of speeding startup for\n> new users as there's no indexing overhead.\n\nI'm not a fan of anything which introduces unauditable single source\nmaterial.  \"Trust us\" is a bad place to be because it would greatly\nincrease the attractiveness of compromising developers.\n\nIf we wanted to go the route of shipping pruned chains I'd prefer to\nhave a deterministic process to produce archival chains and then start\nintroducing commitments to them in the blockchain or something like\nthat.   Then a client doing a reverse header sync[1] would bump into a\ncommitment for an archival chain that they have and would simply stop\nsyncing and use the archival chain for points before that.\n\nThis would leave it so that the distribution of the software could\nstill be audited.\n\nMore generally we should start doing something with the service\nannouncements so that full nodes that don't have enough bandwidth to\nsupport a lot of syncing from new nodes can do so without turning off\nlistening.\n\n\n[1] https://en.bitcoin.it/wiki/User:Gmaxwell/Reverse_header-fetching_sync"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-11T20:36:13",
                "message_text_only": "> If we wanted to go the route of shipping pruned chains I'd prefer to\n> have a deterministic process to produce archival chains\n\nYeah, that sounds reasonable. I mean, I can't see why pruning would\nnot be deterministic. So if you download a binary that contains a\npre-indexed and pruned chain up to block 180,000 or whatever, you\nshould be able to blow away the data files and run with\n\"-syncto=180000 -prune\", then check the hashes of the newly created\nfiles vs what you downloaded.\n\nUnless BDB has some weird behaviour in it, that shouldn't require any\nadditional effort, and anyone could set up a cron job to verify the\ndownloads match what is expected.\n\nEven if a more complex scheme is used whereby commitments are in the\nblock chain, somebody still has to verify the binaries match the\nsource. If that isn't true, the software could do anything and you'd\nnever know."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-11T20:43:20",
                "message_text_only": "On Mon, Jun 11, 2012 at 4:36 PM, Mike Hearn <mike at plan99.net> wrote:\n> Unless BDB has some weird behaviour in it, that shouldn't require any\n\nHAHAHA.   Have you consider doing comedy full time?\n\nActual BDB files are absolutely not deterministic. Nor is the raw\nblockchain itself currently, because blocks aren't always added in the\nsame order (plus they get orphans in them)\n\nBut the serious inter-version compatibility problems as well as poor\nspace efficiency make BDB a poor candidate for read only pruned\nindexes.\n\n> Even if a more complex scheme is used whereby commitments are in the\n> block chain, somebody still has to verify the binaries match the\n> source. If that isn't true, the software could do anything and you'd\n> never know.\n\nThe binaries distributed by bitcoin.org are all already compiled\ndeterministically and validated by multiple independent parties.  In\nthe future there will be a downloader tool (e.g. for updates) which\nwill automatically check for N approvals before accepting an update,\neven for technically unsophisticated users.\n\nThis will produce a full chain of custody which tracks the actual\nbinaries people fetch to specific source code which can be audited, so\nsubstitution attacks will at least in theory always be detectable. Of\ncourse, you're left with Ken Thompson's compiler attack but even that\ncan be substantially closed."
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-11T20:48:36",
                "message_text_only": "> Actual BDB files are absolutely not deterministic. Nor is the raw\n> blockchain itself currently, because blocks aren't always added in the\n> same order (plus they get orphans in them)\n\nThat's true. Though if you prune up to the last checkpoint, orphans\nbefore that point can be safely thrown away.\n\nI wonder if swapping out bdb for LevelDB might make sense at some\npoint. I'm not sure how deterministic that is either though :)"
            }
        ],
        "thread_summary": {
            "title": "Bootstrapping full nodes post-pruning",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Mike Hearn",
                "Gregory Maxwell"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 4871
        }
    },
    {
        "title": "[Bitcoin-development] New P2P commands for diagnostics, SPV clients",
        "thread_messages": [
            {
                "author": "Jeff Garzik",
                "date": "2012-06-13T20:46:37",
                "message_text_only": "An IRC discussion today covered additional needs of lightweight\nclients.  Here is a draft of proposed new P2P commands, and associated\nbehavior changes.  This is not meant to be a formal, detailed\nspecification but rather rough picture of the preferred direction.\n\n     -----\n\nfilterinit(false positive rate, number of elements): initialize\nper-connection bloom filter to the given parameters.  if the\nparameters create a too-large table, the operation fails.  returns a\n'filterparams' message, with bloom filter construction details.\n\nfilterload(data): input a serialized bloom filter table metadata and data.\n\nfilterclear(): remove any filtering associated with current connection.\n\nfilteradd(hash data): add a single hash to the bloom filter.  WARNING:\nalthough easier to use, has privacy implications. filterload shrouds\nthe hash list; filteradd does not.  it is also less efficient to send\na stream of filteradd's to the remote node.\n\nmempool():  list TX's in remote node's memory pool.\n\n     -----\n\n'filterload' and 'filteradd' enable special behavior changes for\n'mempool' and existing P2P commands, whereby only transactions\nmatching the bloom filter will be announced to the connection, and\nonly matching transactions will be sent inside serialized blocks.\n\nA lightweight (\"SPV\") client would issue 'filterload', sync up with\nblocks, then use 'mempool' to sync up to current TX's.  The\n'filterload' command also ensures that the client is only sent 'inv'\nmessages etc. for the TX's it is probably interested in.\n\nThe 'mempool' command is thought to be useful as a diagnostic, even if\na bloom filter is not applied to its output.\n\nA bloom filter match would need to notice activity on existing coins\n(via CTxIn->prevout) and activity on a bitcoin address (via CTxOut).\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-14T11:52:29",
                "message_text_only": "> filterinit(false positive rate, number of elements): initialize\n> filterload(data): input a serialized bloom filter table metadata and data.\n\nWhy not combine these two?\n\n> 'filterload' and 'filteradd' enable special behavior changes for\n> 'mempool' and existing P2P commands, whereby only transactions\n> matching the bloom filter will be announced to the connection, and\n> only matching transactions will be sent inside serialized blocks.\n\nNeed to specify the format of how these arrive. It means that when a\nnew block is found instead of inv<->getdata<->block we'd see something\nlike  inv<->getdata<->merkleblock where a \"merkleblock\" structure is a\nheader + list of transactions + list of merkle branches linking them\nto the root. I think CMerkleTx already knows how to serialize this,\nbut it redundantly includes the block hash which would not be\nnecessary for a merkleblock message."
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-15T11:52:56",
                "message_text_only": "> Need to specify the format of how these arrive. It means that when a\n> new block is found instead of inv<->getdata<->block we'd see something\n> like \u00a0inv<->getdata<->merkleblock where a \"merkleblock\" structure is a\n> header + list of transactions + list of merkle branches linking them\n> to the root.\n\nThinking about it some more and re-reading the Scalability wiki page,\nI remembered that a nice bandwidth optimization to the protocol is to\ndistribute blocks as header+list of tx hashes. If a node has already\nseen that tx before (eg, it's in the mempool) there is no need to send\nit again.\n\nWith the new command to download the contents of the mempool on\nstartup, this means that blocks could potentially propagate across the\nnetwork faster as download time is taken out of the equation, and\nindeed, with the signature cache the hard work of verifying is already\ndone. So this could also help reduce orphan blocks and spurious chain\nsplits.\n\nAre you planning on implementing any of this Jeff? I think we have the\nopportunity to kill a few birds with one or two stones."
            },
            {
                "author": "Matt Corallo",
                "date": "2012-06-15T13:19:06",
                "message_text_only": "On Thu, 2012-06-14 at 13:52 +0200, Mike Hearn wrote:\n> > filterinit(false positive rate, number of elements): initialize\n> > filterload(data): input a serialized bloom filter table metadata and data.\n> \n> Why not combine these two?\nI believe its because it allows the node which will have to use the\nbloom filter to scan transactions to chose how much effort it wants to\nput into each transaction on behalf of the SPV client.  Though its\ngenerally a small amount of CPU time/memory, if we end up with a drastic\nsplit between SPV nodes and only a few large network nodes, those nodes\nmay wish to limit the CPU/memory usage each node is allowed to use,\nwhich may be important if you are serving 1000 SPV peers.  It offers a\nsort of negotiation between SPV client and full node instead of letting\nthe client specify it outright.\n> \n> > 'filterload' and 'filteradd' enable special behavior changes for\n> > 'mempool' and existing P2P commands, whereby only transactions\n> > matching the bloom filter will be announced to the connection, and\n> > only matching transactions will be sent inside serialized blocks.\n> \n> Need to specify the format of how these arrive. It means that when a\n> new block is found instead of inv<->getdata<->block we'd see something\n> like  inv<->getdata<->merkleblock where a \"merkleblock\" structure is a\n> header + list of transactions + list of merkle branches linking them\n> to the root. I think CMerkleTx already knows how to serialize this,\n> but it redundantly includes the block hash which would not be\n> necessary for a merkleblock message.\nA series of CMerkleTx's might also end up redundantly encoding branches\nof the merkle tree, so, yes as a part of the BIP/implementation, I would\nsay we probably want a CFilteredBlock or similar"
            },
            {
                "author": "Jeff Garzik",
                "date": "2012-06-15T13:26:18",
                "message_text_only": "On Thu, Jun 14, 2012 at 7:52 AM, Mike Hearn <mike at plan99.net> wrote:\n>> filterinit(false positive rate, number of elements): initialize\n>> filterload(data): input a serialized bloom filter table metadata and data.\n>\n> Why not combine these two?\n\nThis is a fair point that sipa raised.\n\nConsensus concluded that 'filterload' includes all necessary metadata\nrequired to initialize a bloom filter.  That implies 'filterinit'\nwould only be needed for 'filteradd'.  If we don't think 'filteradd'\nhas a compelling use case, filterinit + filteradd can be dropped.\n\n>> 'filterload' and 'filteradd' enable special behavior changes for\n>> 'mempool' and existing P2P commands, whereby only transactions\n>> matching the bloom filter will be announced to the connection, and\n>> only matching transactions will be sent inside serialized blocks.\n>\n> Need to specify the format of how these arrive. It means that when a\n> new block is found instead of inv<->getdata<->block we'd see something\n> like \u00a0inv<->getdata<->merkleblock where a \"merkleblock\" structure is a\n> header + list of transactions + list of merkle branches linking them\n> to the root. I think CMerkleTx already knows how to serialize this,\n> but it redundantly includes the block hash which would not be\n> necessary for a merkleblock message.\n\nYes, the format is something that must be hashed out (no pun\nintended).  Need input from potential users about what information\nthey might need.\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-15T13:43:06",
                "message_text_only": "> Yes, the format is something that must be hashed out (no pun\n> intended). \u00a0Need input from potential users about what information\n> they might need.\n\nMatts point that a branch-per-transaction may duplicate data is well\nmade, that said, I suspect a format that tries to fix this would be\nmuch more complicated.\n\nHow about see this project as a three part change?\n\nFirst step - add the mempool command and make nodes sync up their\nmempools on startup.\n\nSecond step - if protocol version >= X, the \"block\" message consists\nof a header + num transactions + vector<hash>  instead of the full\ntransactions themselves.\n\nOn receiving such a block, we go look to see which transactions we're\nmissing from the mempool and request them with getdata. Each time we\nreceive a tx message we check to see if it was one we were missing\nfrom a block. Once all transactions in the block message are in\nmemory, we go ahead and assemble the block, then verify as per normal.\nThis should speed up block propagation. Miners have an incentive to\nupgrade because it should reduce wasted work.\n\nThird step - new message, getmerkletx takes a vector<hash> and returns\na merkletx message: \"merkle branch missing the root + transaction data\nitself\" for each requested transaction. The filtering commands are\nadded, so the block message now only lists transaction hashes that\nmatch the filter which can then be requested with getmerkletx."
            },
            {
                "author": "Matt Corallo",
                "date": "2012-06-15T14:56:52",
                "message_text_only": "On Fri, 2012-06-15 at 15:43 +0200, Mike Hearn wrote:\n> > Yes, the format is something that must be hashed out (no pun\n> > intended).  Need input from potential users about what information\n> > they might need.\n> \n> Matts point that a branch-per-transaction may duplicate data is well\n> made, that said, I suspect a format that tries to fix this would be\n> much more complicated.\n> \n> How about see this project as a three part change?\n> \n> First step - add the mempool command and make nodes sync up their\n> mempools on startup.\nACK\n> \n> Second step - if protocol version >= X, the \"block\" message consists\n> of a header + num transactions + vector<hash>  instead of the full\n> transactions themselves.\nIf vector<hash> is sorted in the order of the merkle tree, you dont need\nto forward the merkle tree to non-filtered nodes, further saving some\nsmall amount of bandwidth.  For filtered nodes, you would still need to\nforward merkle branches anyway.\n> \n> On receiving such a block, we go look to see which transactions we're\n> missing from the mempool and request them with getdata. Each time we\n> receive a tx message we check to see if it was one we were missing\n> from a block. Once all transactions in the block message are in\n> memory, we go ahead and assemble the block, then verify as per normal.\n> This should speed up block propagation. Miners have an incentive to\n> upgrade because it should reduce wasted work.\nACK\n> \n> Third step - new message, getmerkletx takes a vector<hash> and returns\n> a merkletx message: \"merkle branch missing the root + transaction data\n> itself\" for each requested transaction. The filtering commands are\n> added, so the block message now only lists transaction hashes that\n> match the filter which can then be requested with getmerkletx.\nI really dont think it would be /that/ difficult to make it getmerkletxs\nvector<hashes>. And then respond with a partial merkle tree to those\ntransactions.\n\nMatt"
            },
            {
                "author": "Jeff Garzik",
                "date": "2012-06-15T15:32:06",
                "message_text_only": "On Fri, Jun 15, 2012 at 9:43 AM, Mike Hearn <mike at plan99.net> wrote:\n> How about see this project as a three part change?\n>\n> First step - add the mempool command and make nodes sync up their\n> mempools on startup.\n\nHere's the \"mempool\" implementation:\nhttps://github.com/bitcoin/bitcoin/pull/1470\n\nSPV nodes would definitely want to sync up their mempool upon startup.\n As for full nodes... I like the organic growth and random nature of\nthe mempools.  On the fence, WRT full node mempool sync at startup.\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com"
            },
            {
                "author": "Matt Corallo",
                "date": "2012-06-15T16:20:27",
                "message_text_only": "On Fri, 2012-06-15 at 11:32 -0400, Jeff Garzik wrote:\n>  As for full nodes... I like the organic growth and random nature of\n> the mempools.  On the fence, WRT full node mempool sync at startup.\n> \nI dont particularly care either way, but I have a feeling miners will\nreally want that so that they can get fee-paying transactions right\naway.\n\nMatt"
            },
            {
                "author": "Amir Taaki",
                "date": "2012-06-15T18:42:32",
                "message_text_only": "Why though? The bottleneck is not network traffic but disk space usage/blockchain validation time.\n\n\n\n----- Original Message -----\nFrom: Mike Hearn <mike at plan99.net>\nTo: Jeff Garzik <jgarzik at exmulti.com>\nCc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\nSent: Friday, June 15, 2012 3:43 PM\nSubject: Re: [Bitcoin-development] New P2P commands for diagnostics, SPV clients\n\n> Yes, the format is something that must be hashed out (no pun\n> intended). \u00a0Need input from potential users about what information\n> they might need.\n\nMatts point that a branch-per-transaction may duplicate data is well\nmade, that said, I suspect a format that tries to fix this would be\nmuch more complicated.\n\nHow about see this project as a three part change?\n\nFirst step - add the mempool command and make nodes sync up their\nmempools on startup.\n\nSecond step - if protocol version >= X, the \"block\" message consists\nof a header + num transactions + vector<hash>\u00a0 instead of the full\ntransactions themselves.\n\nOn receiving such a block, we go look to see which transactions we're\nmissing from the mempool and request them with getdata. Each time we\nreceive a tx message we check to see if it was one we were missing\nfrom a block. Once all transactions in the block message are in\nmemory, we go ahead and assemble the block, then verify as per normal.\nThis should speed up block propagation. Miners have an incentive to\nupgrade because it should reduce wasted work.\n\nThird step - new message, getmerkletx takes a vector<hash> and returns\na merkletx message: \"merkle branch missing the root + transaction data\nitself\" for each requested transaction. The filtering commands are\nadded, so the block message now only lists transaction hashes that\nmatch the filter which can then be requested with getmerkletx.\n\n------------------------------------------------------------------------------\nLive Security Virtual Conference\nExclusive live event will cover all the ways today's security and \nthreat landscape has changed and how IT managers can respond. Discussions \nwill include endpoint security, mobile security and the latest in malware \nthreats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-16T08:25:48",
                "message_text_only": "The bottleneck for the android Bitcoin Wallet app is rapidly becoming\nbandwidth and parse time.\n\nOn Fri, Jun 15, 2012 at 8:42 PM, Amir Taaki <zgenjix at yahoo.com> wrote:\n> Why though? The bottleneck is not network traffic but disk space usage/blockchain validation time.\n>\n>\n>\n> ----- Original Message -----\n> From: Mike Hearn <mike at plan99.net>\n> To: Jeff Garzik <jgarzik at exmulti.com>\n> Cc: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n> Sent: Friday, June 15, 2012 3:43 PM\n> Subject: Re: [Bitcoin-development] New P2P commands for diagnostics, SPV clients\n>\n>> Yes, the format is something that must be hashed out (no pun\n>> intended). \u00a0Need input from potential users about what information\n>> they might need.\n>\n> Matts point that a branch-per-transaction may duplicate data is well\n> made, that said, I suspect a format that tries to fix this would be\n> much more complicated.\n>\n> How about see this project as a three part change?\n>\n> First step - add the mempool command and make nodes sync up their\n> mempools on startup.\n>\n> Second step - if protocol version >= X, the \"block\" message consists\n> of a header + num transactions + vector<hash>\u00a0 instead of the full\n> transactions themselves.\n>\n> On receiving such a block, we go look to see which transactions we're\n> missing from the mempool and request them with getdata. Each time we\n> receive a tx message we check to see if it was one we were missing\n> from a block. Once all transactions in the block message are in\n> memory, we go ahead and assemble the block, then verify as per normal.\n> This should speed up block propagation. Miners have an incentive to\n> upgrade because it should reduce wasted work.\n>\n> Third step - new message, getmerkletx takes a vector<hash> and returns\n> a merkletx message: \"merkle branch missing the root + transaction data\n> itself\" for each requested transaction. The filtering commands are\n> added, so the block message now only lists transaction hashes that\n> match the filter which can then be requested with getmerkletx.\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Simon Barber",
                "date": "2012-06-15T15:43:04",
                "message_text_only": "separate filterinit / filterload - so you can do a new filterload later \non if your list changes, without the privacy implications of filteradd.\n\nSimon\n\n\nOn Thu 14 Jun 2012 04:52:29 AM PDT, Mike Hearn wrote:\n>> filterinit(false positive rate, number of elements): initialize\n>> filterload(data): input a serialized bloom filter table metadata and data.\n>\n> Why not combine these two?\n>\n>> 'filterload' and 'filteradd' enable special behavior changes for\n>> 'mempool' and existing P2P commands, whereby only transactions\n>> matching the bloom filter will be announced to the connection, and\n>> only matching transactions will be sent inside serialized blocks.\n>\n> Need to specify the format of how these arrive. It means that when a\n> new block is found instead of inv<->getdata<->block we'd see something\n> like  inv<->getdata<->merkleblock where a \"merkleblock\" structure is a\n> header + list of transactions + list of merkle branches linking them\n> to the root. I think CMerkleTx already knows how to serialize this,\n> but it redundantly includes the block hash which would not be\n> necessary for a merkleblock message.\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Jeff Garzik",
                "date": "2012-06-15T16:40:46",
                "message_text_only": "On Fri, Jun 15, 2012 at 11:43 AM, Simon Barber <simon at superduper.net> wrote:\n> separate filterinit / filterload - so you can do a new filterload later on\n> if your list changes, without the privacy implications of filteradd.\n\nfilterload loads a whole new bloom filter from scratch, in one atomic\noperation.  Params set, table sized, data input into table.  A\nseparate filterinit does not make sense for filterload.\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-15T13:23:11",
                "message_text_only": "> > Why not combine these two?\n>\n> I believe its because it allows the node which will have to use the\n> bloom filter to scan transactions to chose how much effort it wants to\n> put into each transaction on behalf of the SPV client.\n\nIf that's the case then the negotiation protocol needs to be specified\ntoo. It seems heavy though. If a node is getting overloaded it could\njust disconnect intensive peers or refuse new connections."
            },
            {
                "author": "Matt Corallo",
                "date": "2012-06-15T14:39:44",
                "message_text_only": "On Fri, 2012-06-15 at 15:23 +0200, Mike Hearn wrote:\n> > > Why not combine these two?\n> >\n> > I believe its because it allows the node which will have to use the\n> > bloom filter to scan transactions to chose how much effort it wants to\n> > put into each transaction on behalf of the SPV client.\n> \n> If that's the case then the negotiation protocol needs to be specified\n> too. It seems heavy though. If a node is getting overloaded it could\n> just disconnect intensive peers or refuse new connections.\nIMHO it already is.  A node requests a filter using filterinit by\nspecifying the false positive rate it wants and a guessed number of\nitems.  The node which will have to hold that filter then responds with\nthe closest filter to what the SPV node requested that it is willing to\nprovide.  If the SPV node responds with a filterload command, it has\naccepted the offer, otherwise it will simply disconnect and find a\nbetter full node.  \nI'd much rather have an overloaded node respond with 50% fp rate filters\nas an option if there aren't many full nodes available than simply\ndisconnect SPV clients.\nAt least thats my thinking, but you may be right that it is too heavy\nfor too little gain."
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-16T08:27:09",
                "message_text_only": "> I'd much rather have an overloaded node respond with 50% fp rate filters\n> as an option if there aren't many full nodes available than simply\n> disconnect SPV clients.\n\nI don't think the bloom filter settings have any impact on server-side\nload ... a node still has to check every transaction against the\nfilter regardless of how that filter is configured, which means the\nsame amount of disk io and processing.\n\nHow can you reduce load on a peer by negotiating different filter settings?"
            },
            {
                "author": "Matt Corallo",
                "date": "2012-06-19T19:09:58",
                "message_text_only": "On Sat, 2012-06-16 at 10:27 +0200, Mike Hearn wrote:\n> > I'd much rather have an overloaded node respond with 50% fp rate filters\n> > as an option if there aren't many full nodes available than simply\n> > disconnect SPV clients.\n> \n> I don't think the bloom filter settings have any impact on server-side\n> load ... a node still has to check every transaction against the\n> filter regardless of how that filter is configured, which means the\n> same amount of disk io and processing.\n> \n> How can you reduce load on a peer by negotiating different filter settings?\nAgreed, I was largely giving a reason why one may want to negotiate the\nfilter settings in response to your question as to why it was done.  As\nlong as there are sane limits (you cant make a 1GB filter by specifying\n0% fp and some crazy number of entires), filter negotiation largely isnt\nworth it (also prevents any floats from appearing in the p2p protocol,\nthough in either case it shouldn't be able to cause issues).\n\nMatt"
            }
        ],
        "thread_summary": {
            "title": "New P2P commands for diagnostics, SPV clients",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "Mike Hearn",
                "Simon Barber",
                "Amir Taaki",
                "Matt Corallo"
            ],
            "messages_count": 17,
            "total_messages_chars_count": 22267
        }
    },
    {
        "title": "[Bitcoin-development] Raw Transaction RPC calls for bitcoind",
        "thread_messages": [
            {
                "author": "Gavin Andresen",
                "date": "2012-06-14T13:22:08",
                "message_text_only": "I submitted a pull request yesterday that implements low-level \"raw\"\ntransaction, and am looking for feedback on the API and help with\ntrying to test/break it.\n\nDesign doc:  https://gist.github.com/2839617\nPull request: https://github.com/bitcoin/bitcoin/pull/1456\nTest plan: https://secure.bettermeans.com/projects/4180/wiki/Raw_Transaction_RPC_Test_Plan\n\nPlaying around with this API on the command line I'm pretty happy with\nthe level of abstraction and the way it interacts with existing RPC\ncommands; for example, \"createrawtx\" is just like \"sendmany\" in the\nway outputs are specified.\n\nThe signrawtx method is the key new method; it takes a raw\ntransaction, signs as many inputs as it can, and returns the same raw\ntransaction with signatures. Typical usage would be:\n\nFunds are sitting in a multisignature transaction output, and it is\ntime to gather signatures and spend them.\n\nAssumption: you know the multisignature transaction's [txid,\noutputNumber, amount].\n\nCreate a raw transaction to spend, using createrawtx.\nUse signrawtx to add your signatures (after unlocking the wallet, if necessary).\nGive the transaction to the other person(s) to sign.\nYou or they submit the transaction to the network using sendrawtx.\nI don't imagine anybody but very-early-adopters or ultra-geeks will do\nthis by calling these RPC methods at a command-line. They are really\nintended for people writing services on top of bitcoind. The service\nshould be careful to include an appropriate transaction fee, or the\nsendrawtx method is likely to fail.\n\nI've been asked a couple of times: why doesn't signrawtx handle the\nBIP 0010 (https://en.bitcoin.it/wiki/BIP_0010) transaction format?\n\nI considered parsing/writing BIP 10 format for raw transactions, but\ndecided that reading/writing BIP 10 format should happen at a higher\nlevel and not in the low-level RPC calls. So 'raw transactions' are\nsimply hex-encoded into JSON strings, and encoding/decoding them is\njust a couple of lines of already-written-and-debugged code.\n\n------\n\nHere is the help output and example use for all the new RPC calls:\n\nlistunspent [minconf=1] [maxconf=999999]\nReturns array of unspent transaction outputs\nwith between minconf and maxconf (inclusive) confirmations.\nReturns an array of 4-element arrays, each of which is:\n[transaction id, output, amount, confirmations]\n\nE.g:  listunspent 1 2\nReturns:\n[\n    [\n        \"2881b33a8c0bbdb45b0a65b36aa6611a05201e316ea3ad718762d48ef9588fb3\",\n        0,\n        40.00000000,\n        2\n    ],\n    [\n        \"894a0fc535c7b49f434ceb633d8555ea24c8f9775144efb42da85b853280bcd7\",\n        0,\n        50.00000000,\n        1\n    ]\n]\n\ngetrawtx <txid>\nReturns hexadecimal-encoded, serialized transaction data\nfor <txid>. Returns an error if <txid> is unknown.\n\nE.g.: getrawtx fce46ea2448820f7bb8091b5f5e3fd75b7b267e60b9a22af88a9eeabfb084233\nReturns:\n01000000016d40da062b6a0edcaf643b6e25b943baf103941589d287e39d6f425d84ae8b1c000000004847304402203fb648ff8381d8961e66ef61ab88afe52826a5179b8a7312742c8d93785ca56302204240ea12de1211fffab49686f13ca0e78011d1985765be6e6aa8e747852f897d01ffffffff0100f2052a0100000017a914f96e358e80e8b3660256b211a23ce3377d2f9cb18700000000\n\n\ncreaterawtx [[\"txid\",n],...] {address:amount,...}\nCreate a transaction spending given inputs\n(array of (hex transaction id, output number) pairs),\nsending to given address(es).\nReturns the same information as gettransaction, plus an\nextra \"rawtx\" key with the hex-encoded transaction.\nNote that the transaction's inputs are not signed, and\nit is not stored in the wallet or transmitted to the network.\n\nE.g.: createrawtx '[\n[\"fce46ea2448820f7bb8091b5f5e3fd75b7b267e60b9a22af88a9eeabfb084233\",0]\n]' '{\"mqYmZSQQuAWNQcdwBrDwmtTXg2TLNz748L\":50}'\nReturns:\n{\n    \"version\" : 1,\n    \"locktime\" : 0,\n    \"size\" : 85,\n    \"vin\" : [\n        {\n            \"prevout\" : {\n                \"hash\" :\n\"fce46ea2448820f7bb8091b5f5e3fd75b7b267e60b9a22af88a9eeabfb084233\",\n                \"n\" : 0\n            },\n            \"scriptSig\" : \"\",\n            \"sequence\" : 4294967295\n        }\n    ],\n    \"vout\" : [\n        {\n            \"value\" : 50.00000000,\n            \"scriptPubKey\" : \"OP_DUP OP_HASH160\n6e0920fc26383dc7e6101bc417cf87169d0cedbd OP_EQUALVERIFY OP_CHECKSIG\"\n        }\n    ],\n    \"rawtx\" : \"0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc0000000000ffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000\"\n}\n\nsignrawtx <hex string> [<prevtx1>,<prevtx2>...]\nSign inputs for raw transaction (serialized, hex-encoded).\nSecond argument is an array of raw previous transactions that\nthis transaction depends on but are not yet in the blockchain.\nReturns json object with keys:\n  rawtx : raw transaction with signature(s) (hex-encoded string)\n  complete : 1 if transaction has a complete set of signature (0 if not)\n\nE.g.: signrawtx\n\"0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc0000000000ffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000\"\n'[\"01000000016d40da062b6a0edcaf643b6e25b943baf103941589d287e39d6f425d84ae8b1c000000004847304402203fb648ff8381d8961e66ef61ab88afe52826a5179b8a7312742c8d93785ca56302204240ea12de1211fffab49686f13ca0e78011d1985765be6e6aa8e747852f897d01ffffffff0100f2052a0100000017a914f96e358e80e8b3660256b211a23ce3377d2f9cb18700000000\"]'\nReturns:\n{\n    \"rawtx\" : \"0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc000000009100473044022007f3ba1b8bdc156f2340ef1222eb287c3f5481a8078a8dad43aa09fd289ba19002201cc72e97406d546dc918159978dc78aee8215a6418375956665ee44e6eacc1150147522102894ca6e7a6483d0f8fa6110c77c431035e8d462e3a932255d9dda65e8fada55c2103c556ef01e89a07ee9ba61581658fa007bf442232daed8b465c47c278550d3dab52aeffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000\",\n    \"complete\" : false\n}\n\nsendrawtx <hex string>\nSubmits raw transaction (serialized, hex-encoded) to local node and network.\nE.g.: sendrawtx\n0100000001334208fbabeea988af229a0be667b2b775fde3f5b59180bbf7208844a26ee4fc000000009100473044022007f3ba1b8bdc156f2340ef1222eb287c3f5481a8078a8dad43aa09fd289ba19002201cc72e97406d546dc918159978dc78aee8215a6418375956665ee44e6eacc1150147522102894ca6e7a6483d0f8fa6110c77c431035e8d462e3a932255d9dda65e8fada55c2103c556ef01e89a07ee9ba61581658fa007bf442232daed8b465c47c278550d3dab52aeffffffff0100f2052a010000001976a9146e0920fc26383dc7e6101bc417cf87169d0cedbd88ac00000000\nReturns:\nerror: {\"code\":-22,\"message\":\"TX rejected\"}\n\n(Rejected because it doesn't have all required signatures, if it was\naccepted it would return the transaction id)\n\n-- \n--\nGavin Andresen"
            },
            {
                "author": "Peter Vessenes",
                "date": "2012-06-14T14:37:26",
                "message_text_only": "This is super cool!\n\nI have a feature request: it would be awesome to be able to provide private\nkeys at the command line with the signature, turning the client into a\nwallet-less signature machine.\n\nPeter\n\n\nOn Thu, Jun 14, 2012 at 9:22 AM, Gavin Andresen <gavinandresen at gmail.com>wrote:\n\n> I submitted a pull request yesterday that implements low-level \"raw\"\n> transaction, and am looking for feedback on the API and help with\n> trying to test/break it.\n> ...\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120614/3dde553e/attachment.html>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2012-06-14T20:00:57",
                "message_text_only": "On Thu, Jun 14, 2012 at 10:37 AM, Peter Vessenes <peter at coinlab.com> wrote:\n> This is super cool!\n>\n> I have a feature request: it would be awesome to be able to provide private\n> keys at the command line with the signature, turning the client into a\n> wallet-less signature machine.\n\nI like that idea.\n\nA third argument that is an array of private keys (in the same format\nas the dumpprivkey RPC call) should be easy to support, assuming the\nsemantics are:\n\n+ If third argument given, do not require that the wallet be unlocked,\nand only sign using the private key(s) given (ignore the bitcoind\nwallet entirely).\n+ Private keys would stay in bitcoind memory only for the duration of\nthe RPC call.\n\n-- \n--\nGavin Andresen"
            }
        ],
        "thread_summary": {
            "title": "Raw Transaction RPC calls for bitcoind",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Peter Vessenes",
                "Gavin Andresen"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 7980
        }
    },
    {
        "title": "[Bitcoin-development] A tangent about BIP 10",
        "thread_messages": [
            {
                "author": "Alan Reiner",
                "date": "2012-06-14T14:25:07",
                "message_text_only": "On Thu, Jun 14, 2012 at 9:22 AM, Gavin Andresen <gavinandresen at gmail.com>wrote:\n\n>\n> I've been asked a couple of times: why doesn't signrawtx handle the\n> BIP 0010 (https://en.bitcoin.it/wiki/BIP_0010) transaction format?\n>\n> I considered parsing/writing BIP 10 format for raw transactions, but\n> decided that reading/writing BIP 10 format should happen at a higher\n> level and not in the low-level RPC calls. So 'raw transactions' are\n> simply hex-encoded into JSON strings, and encoding/decoding them is\n> just a couple of lines of already-written-and-debugged code.\n>\n>\nBIP 10 <https://en.bitcoin.it/wiki/BIP_0010> could use some improvement.  I\ncreated it for offline and multi-sig tx but there was no reception to it\nbecause no one was using offline or multi-sig tx at the time except for\nArmory (which only currently implements offline tx).  So I made something\nthat fit my needs, and it has served its purpose well for me. But I also\nthink it could be expanded and improved before there is wider adoption of\nit.  It's a little clunky and not very rigorous.\n\nElements of it that I'd really like to keep:\n\n(1) Some aspects of human-readability -- even if regular users will never\nlook at it, it should be possible for advanced users to manually copy&paste\nthe data around and see what's going on in the transaction and what\nsignatures are present.  I'm thinking of super-high-security situations\nwhere manual handling of such data may even be the norm.\n(2) Should be compact -- I took the concept of ASCII-armoring from PGP/GPG,\nbecause, for the reason above, it's much easier and cleaner to view/select\nwhen copied inline.  If a random user accidentally runs across it, it will\npartially self-identify itself\n(3) Includes all previous transactions so the device can verify transaction\ninputs without the blockchain.\n\n\nThings that could be added:\n\n-- It needs a BIP16 script entry (this was created for vanilla multi-sig\nbefore BIP 16 was created)\n-- Comment lines\n-- Version number\n-- Use base58/64 encoding\n-- Rigorous formatting spec\n-- Binary representation\n-- A better name than \"Tx Distribution Proposal\"\n\nI'll be releasing the Beta version of Armory soon, and after that, I'll\nprobably be thinking about a multi-signature support interface.  That would\nbe a good time for me to tie in a better version of BIP 10 -- one that is\ncompatible with other clients implementing the same thing.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120614/69ba203b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A tangent about BIP 10",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Alan Reiner"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2583
        }
    },
    {
        "title": "[Bitcoin-development] BIP16 backport bug (0.4.x and 0.5.x stuck on block 177617)",
        "thread_messages": [
            {
                "author": "Luke-Jr",
                "date": "2012-06-14T15:00:45",
                "message_text_only": "Block 177618 was rejected by BIP16-enabled backports (0.4.x and 0.5.x) due to \ncontaining a P2SH redemption with over 200 bytes in. Since the BIP16 code uses \nIsPushOnly to check the scriptSig for compliance, and IsPushOnly in these \nversions also enforced the 200-byte \"is standard\" rule, they were effectively \ntreating it as a network rule. This was not a problem in 0.6 because the \noriginal OP_EVAL commit (e679ec9) moved the check outside of IsPushOnly.\n\nThis problem could have been avoided if either IsPushOnly was renamed when its \nsemantics/behaviour changed significantly, or I inspected the OP_EVAL commit \nin detail instead of skipping it over as a new feature and not bugfixes. \nAdditionally, it might have helped, if the commit message mentioned the \nchange, but I'd probably have still missed it as it wasn't relevant until \nmonths later.\n\nI will be releasing 0.4.7 and 0.5.6 hopefully in the next 24 hours to address \nthis bug, along with instructions to get unstuck:\n    1. Ensure you have the minimum required 1280 MB memory available\n    2. Create a new file in your bitcoin directory (the same one with\n       wallet.dat) called DB_CONFIG with the following two lines:\n           set_lk_max_locks   1000000\n           set_lk_max_objects 1000000\n    3. Start bitcoind or Bitcoin-Qt\n    4. WAIT AT LEAST SIX HOURS\n       Your client will NOT show any signs of making progress during this time\n    5. When complete, your client should be up-to-date on block count\n    6. At this time, you may wish to delete the DB_CONFIG file and restart\n       your client, to use less memory\n\nLuke"
            }
        ],
        "thread_summary": {
            "title": "BIP16 backport bug (0.4.x and 0.5.x stuck on block 177617)",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Luke-Jr"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1601
        }
    },
    {
        "title": "[Bitcoin-development] Near-term scalability",
        "thread_messages": [
            {
                "author": "Mike Hearn",
                "date": "2012-06-15T11:29:51",
                "message_text_only": "I had to hit the sack last night as it was 2am CET, but I'd like to\nsum up the discussion we had on IRC about scalability and SatoshiDice\nin particular.\n\nI think we all agreed on the following:\n\n- Having senders/buyers pay no fees is psychologically desirable even\nthough we all understand that eventually, somebody, somewhere will be\npaying fees to use Bitcoin\n\n- In the ideal world Bitcoin would scale perfectly and there would be\nno need for there to be some \"winners\" and some \"losers\" when it comes\nto confirmation time.\n\nThere was discussion of some one-off changes to address the current\nsituation, namely de-ranking transactions that re-use addresses. Gavin\nand myself were not keen on this idea, primarily because it just\navoids the real problem and Bitcoin already has a good way to\nprioritize transactions via the fees mechanism itself. The real issue\nis that SatoshiDice does indeed pay fees and generates a lot of\ntransactions, pushing more traditional traffic out due to artificial\nthrottles.\n\nThe following set of proposals were discussed:\n\n(1) Change the mining code to group transactions together with their\nmempool dependencies and then calculate all fees as a group. A tx with\na fee of 1 BTC that depends on 5 txns with zero fees would result in\nall 6 transactions being considered to have a fee of 1BTC and\ntherefore become prioritized for inclusion. This allows a transition\nto \"receiver pays\" model for fees. There are many advantages. One is\nthat it actually makes sense ... it's always the receiver who wants\nconfirmations because it's the receiver that fears double spends.\nSenders never do. What's more, whilst Bitcoin is designed to operate\non a zero-trust model in the real world trust often exists and it can\nbe used to optimize by passing groups of transactions around with\ntheir dependencies, until that group passes a trust boundary and gets\nbroadcast with a send-to-self tx to add fees. Another advantage is it\nsimplifies usage for end users who primarily buy rather than sell,\nbecause it avoids the need to guess at fees, one of the most\nproblematic parts of Bitcoins design now.\n\nThe disadvantages are that it can result in extra transactions that\nexist only for adding fees, and it requires a more modern payment\nprotocol than the direct-IP protocol Satoshi designed.\n\nIt would help address the current situation by avoiding angry users\nwho want to buy things, but don't know what fee to set and so their\ntransactions get stuck.\n\n(2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to\navoid paying excessive fees and queue-jumping. Guess that's on my\nplate.\n\n(3) Scalability improvements seem like a no brainer to everyone, it's\njust a case of how complicated they are.\n\n(4) Making the block size limit float is better than picking a new\narbitrary threshold.\n\nOn the forums Matt stated that block chain pruning was a no-go because\n\"it makes bitcoin more centralized\". I think we've thrashed this one\nout sufficiently well by now that there should be a united opinion on\nit. There are technical ways to implement it such that there is no\nchange of trust requirements. All the other issues (finding archival\nnodes, etc) can be again addressed with sufficient programming.\n\nFor the case of huge blocks slowing down end user syncing and wasting\ntheir resources, SPV clients like MultiBit and Android Wallet already\nexist and will get better with time. If Jeff implements the bloom\nfiltering p2p commands I'll make bitcoinj use them and that'll knock\nout excessive bandwidth usage and parse overheads from end users who\nare on these clients. At some point Bitcoin-Qt can have a dual mode,\nbut who knows when that'll get implemented.\n\nDoes that all sound reasonable?"
            },
            {
                "author": "Matt Corallo",
                "date": "2012-06-15T13:08:55",
                "message_text_only": "On Fri, 2012-06-15 at 13:29 +0200, Mike Hearn wrote:\n> I had to hit the sack last night as it was 2am CET, but I'd like to\n> sum up the discussion we had on IRC about scalability and SatoshiDice\n> in particular.\n> \n> I think we all agreed on the following:\n> \n> - Having senders/buyers pay no fees is psychologically desirable even\n> though we all understand that eventually, somebody, somewhere will be\n> paying fees to use Bitcoin\n> \n> - In the ideal world Bitcoin would scale perfectly and there would be\n> no need for there to be some \"winners\" and some \"losers\" when it comes\n> to confirmation time.\n> \n> There was discussion of some one-off changes to address the current\n> situation, namely de-ranking transactions that re-use addresses. Gavin\n> and myself were not keen on this idea, primarily because it just\n> avoids the real problem and Bitcoin already has a good way to\n> prioritize transactions via the fees mechanism itself. The real issue\n> is that SatoshiDice does indeed pay fees and generates a lot of\n> transactions, pushing more traditional traffic out due to artificial\n> throttles.\nThe idea can be more generalized in that there are many cases where the\ngenerator of a transaction doesn't care about confirmation times, and\nwould really be willing to make their transaction lower priority than\nother 0-fee transactions.  This enables the first point with lower\nconfirmation times for a while longer.\nAs it turns out, we already have an indication that someone is willing\nto wait longer for confirmations - rapid reuse of an address.  \n1) Green Addresses: The whole point of a green address is that you are\ntrusted based on your address, not necessarily based on confirmations of\nyour transactions.  In this case, you are generally willing to wait a\nbit longer for confirmations than the average user depositing coins into\ntheir Mt. Gox account.  \n2) Donation Addresses: If you are using a publicized donation address,\nyou probably aren't depending on getting your coins *now* to turn around\nand ship a product and, again, you are a bit more willing to tolerate\nlonger confirmation times.\n3) Lazy (or overworked) coders: If, for whatever reason, someone\ndesigning a bitcoin site decides that it is simply easier to make users\npay to a single address for everything, such actions should generally be\ndiscouraged.  Such a setup is worse for end-user privacy.  Also, such\nlaziness (or likely just overworked and not having time to fix the\nissue) is likely also laziness across the board including ignoring\nmultisend for payouts.  If you discourage such address use forcing site\ndesigners to implement more sane policies, hopefully they will do enough\nresearch to also do multisend.  Note that though this is where one\naddresses sites like SatoshiDice, its also the one where we are likely\nto have the least impact...\n\nOne of the ways to implement such deprioritization of rapidly-reused\naddresses is to limit the count of address re-uses by default in memory\npool.  By limiting relaying of such transactions, you a) give nodes\nacross the network some small say in the transactions which they have to\ndeal with relaying outside of blocks, instead of relying on miners to\nmake decisions which are good for the total network load, but which are\nworse for them.  b) You allow sites which wish to re-use addresses to do\nso initially to keep the time-to-launch the same as it is today, but\nforce them to re-think their design decisions as they grow to\n(hopefully) decrease their impact on the average Bitcoin full-node\noperator.  Sites which begin to see their transactions rate-limited have\nseveral options:\n1) Make a deal with a miner to feed them their list of now-non-relayed\ntransactions outside of the regular p2p network and have them manually\nadded to blocks.  Id argue that such setups are going to become more\ncommon in the future and such out-of-band transaction relaying should be\nencouraged.  This also shifts the delay for other transactions from a\nconstant delay getting into blocks until there is room for additional\n0-fee transactions to a spike on each block from the given miner.  I\nhighly prefer this, as you would see usually only one or two block delay\ngetting your transaction confirmed at the worst case, instead of a very\nfuzzy unknown delay that could stretch on for some time.\n2) Use rotating addresses.  This is likely the simplest to implement,\nand I would absolutely think this is what most sites would end up doing.\nThough it doesn't result in a decreased load on the transaction-relaying\nnodes, it does at least allow for a minor improvement in user privacy.  \n\nIn the end, it boils down to an optional transaction deprioritization.\n> \n> The following set of proposals were discussed:\n> \n> (1) Change the mining code to group transactions together with their\n> mempool dependencies and then calculate all fees as a group. A tx with\n> a fee of 1 BTC that depends on 5 txns with zero fees would result in\n> all 6 transactions being considered to have a fee of 1BTC and\n> therefore become prioritized for inclusion. This allows a transition\n> to \"receiver pays\" model for fees. There are many advantages. One is\n> that it actually makes sense ... it's always the receiver who wants\n> confirmations because it's the receiver that fears double spends.\n> Senders never do. What's more, whilst Bitcoin is designed to operate\n> on a zero-trust model in the real world trust often exists and it can\n> be used to optimize by passing groups of transactions around with\n> their dependencies, until that group passes a trust boundary and gets\n> broadcast with a send-to-self tx to add fees. Another advantage is it\n> simplifies usage for end users who primarily buy rather than sell,\n> because it avoids the need to guess at fees, one of the most\n> problematic parts of Bitcoins design now.\n> \n> The disadvantages are that it can result in extra transactions that\n> exist only for adding fees, and it requires a more modern payment\n> protocol than the direct-IP protocol Satoshi designed.\n> \n> It would help address the current situation by avoiding angry users\n> who want to buy things, but don't know what fee to set and so their\n> transactions get stuck.\n> \n> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to\n> avoid paying excessive fees and queue-jumping. Guess that's on my\n> plate.\n> \n> (3) Scalability improvements seem like a no brainer to everyone, it's\n> just a case of how complicated they are.\nI think all of the above are largely no brianers to everyone.\n> \n> (4) Making the block size limit float is better than picking a new\n> arbitrary threshold.\nDefinitely something that is very appealing as we need to scale up.\n> \n> On the forums Matt stated that block chain pruning was a no-go because\n> \"it makes bitcoin more centralized\". I think we've thrashed this one\n> out sufficiently well by now that there should be a united opinion on\n> it. There are technical ways to implement it such that there is no\n> change of trust requirements. All the other issues (finding archival\n> nodes, etc) can be again addressed with sufficient programming.\nMy point was that the easiest way to do it would be to ship a pruned\nsnapshot with Bitcoin, and such a system, while verifiable, would\nincrease Bitocin's centralization.  Though it is quite possible to prune\nthe chain while downloading at checkpoints or when blocks are N deep, it\ncomplicates the initial download if no one has the chain to begin with. \n\nAnother point I made was that by doing chain pruning by default, we may\nsee a decrease in non-fClient nodes (for compatibility, I would assume\npruned nodes have to set fClient) which is what old clients look for to\nconnect to, possibly complicating using Bitcoin for clients that either\nwish to run a full IBD or older clients which need a non-fClient node\nbefore they are happy (which could be an issue when you look at the very\npoor \"upgrade-apathy\" in the Bitcoin community with people running\nlong-outdated versions because they don't feel like upgrading).\n\nAll that said, I do believe pruning will eventually have to come to\nencourage p2pool and other getmemorypool-based pool mining, but\n(obviously) its something that needs careful consideration in its\noverall effects across the network before its applied.\n> \n> For the case of huge blocks slowing down end user syncing and wasting\n> their resources, SPV clients like MultiBit and Android Wallet already\n> exist and will get better with time. If Jeff implements the bloom\n> filtering p2p commands I'll make bitcoinj use them and that'll knock\n> out excessive bandwidth usage and parse overheads from end users who\n> are on these clients. At some point Bitcoin-Qt can have a dual mode,\n> but who knows when that'll get implemented.\n> \n> Does that all sound reasonable?"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-15T13:34:19",
                "message_text_only": "> The idea can be more generalized in that there are many cases where the\n> generator of a transaction doesn't care about confirmation times, and\n> would really be willing to make their transaction lower priority than\n> other 0-fee transactions.\n\nJust to be clear, I think this solution is a hack and don't support it\nbecause it's yet another change of network rules. Some random people\nwill get whacked because of a heuristic \"rule of thumb\".\n\nIf it's implemented, SD could/would switch to fresh addresses and\nnothing would have been achieved except making an already complex\nsystem more complex.\n\nI disagree with the notion that you need \"less important than free\".\nIf you care about the confirmation time of a transaction that was sent\nto you and you need space in a limited resource, you can pay for it.\nIt's an auction like any other. Besides, the idea that transactions\nare free today is just a psychological trick befitting governments but\nnot us - transactions are funded by aggressive hyperinflation. I would\nnever describe Bitcoin as a free system and I suggest nobody else does\neither.\n\nIf grouped fee calculations are implemented, we can keep the nice\nproperty that the person who cares about double spending risk pays the\nfees, and if you assume most transactions are hub-and-spoke from\nbuyers to merchants, rather than a pure p2p graph, in practice it'll\nwork out to seeming free most of the time even if seen globally it\ndoesn't make much difference.\n\n> My point was that the easiest way to do it would be to ship a pruned\n> snapshot with Bitcoin, and such a system, while verifiable, would\n> increase Bitocin's centralization.\n\nI'm not sure why. If you want to audit everything from scratch, after\nchecking the code you could just blow away the included files and then\n\"-connect=archive.bitcoin.org\" or something like that. After\nrebuilding the chain from scratch, check the databases for consistency\nwith the included data.\n\nIt reduces the number of nodes with full copies of the block chain,\nyes, but as long as there's at least one copy of the old data in an\naccessible location new nodes can still bootstrap just fine.\n\nI'm sure we can find organizations willing to host full chains for\npeople who want to rebuild their databases from scratch, given how\ncheap disk space is.\n\n> connect to, possibly complicating using Bitcoin for clients that either\n> wish to run a full IBD or older clients which need a non-fClient node\n\nYes, but old nodes probably have a copy of the chain already, so it\nwouldn't affect them. New blocks would still be fully distributed,\nright?\n\nThe only case where it'd cause issues is if you install a fresh copy\nof a very old node. Not a common occurrence, and those nodes will have\nto wait until they find an archival node announcing itself. Those\nnodes could be made to announce more frequently than normal, if need\nbe."
            },
            {
                "author": "Matt Corallo",
                "date": "2012-06-15T16:18:36",
                "message_text_only": "On Fri, 2012-06-15 at 15:34 +0200, Mike Hearn wrote:\n> > The idea can be more generalized in that there are many cases where the\n> > generator of a transaction doesn't care about confirmation times, and\n> > would really be willing to make their transaction lower priority than\n> > other 0-fee transactions.\n> \n> Just to be clear, I think this solution is a hack and don't support it\n> because it's yet another change of network rules. Some random people\n> will get whacked because of a heuristic \"rule of thumb\".\nIts arguably not a change to network rules as its something that users\ncan already do today by patching their clients.  Obviously any\nimplementation would have sane defaults which allowed for a significant\nnumber of transactions to/from a given address at a time, avoiding\nwhacking random people unless they are large enough that they should\nreally already be fully aware of how bitcoin works.\n> \n> If it's implemented, SD could/would switch to fresh addresses and\n> nothing would have been achieved except making an already complex\n> system more complex.\nI would think SD would switch to using fresh addresses for each bet.\nBut even that is a good thing, at least where user privacy is concerned.\nHowever, I would hope that SD would see the rule tweak and, in order to\navoid having to generate a number of new addresses per second (or, if\nthey went the pool route, having a huge pool of many thousands of\naddresses), they would consider implementing sendmulti support.\n> \n> I disagree with the notion that you need \"less important than free\".\n> If you care about the confirmation time of a transaction that was sent\n> to you and you need space in a limited resource, you can pay for it.\n> It's an auction like any other. Besides, the idea that transactions\n> are free today is just a psychological trick befitting governments but\n> not us - transactions are funded by aggressive hyperinflation. I would\n> never describe Bitcoin as a free system and I suggest nobody else does\n> either.\nI agree, free transactions isnt something we should aggressively push as\na feature of Bitcoin, its simply not.  However, in the current system\nfree transactions are usually confirmed within a small number of blocks,\nand for a number of users, that is an important feature that draws them\nto get through the initial hurdles of converting money to Bitcoin and\nunderstanding enough of the system to trust it.  I believe that if we\ncan incentive large transaction creators to avoid delaying free\ntransactions, we should and giving them the option to delay their own\ntransactions seems like a perfectly reasonable way to do so.  Even if\nyou drop all the per-address limit stuff, allowing transaction creators\nto add a simple flag to transactions seems reasonable when they want to\nencourage Bitcoin to continue to grow as it does today.  Obviously\nkeeping free transactions confirming won't be possible forever, but\nhopefully that will be as a result of natural growth which can encourage\nfurther growth without the need for free transactions and not as a\nresult of a few actors in the community creating a transaction volume\nsignificantly greater than their user-base.\n> \n> If grouped fee calculations are implemented, we can keep the nice\n> property that the person who cares about double spending risk pays the\n> fees, and if you assume most transactions are hub-and-spoke from\n> buyers to merchants, rather than a pure p2p graph, in practice it'll\n> work out to seeming free most of the time even if seen globally it\n> doesn't make much difference.\nACK, thats an important thing to implement IMO, but I really dont see it\nas something that replaces the option to deprioritize your own\ntransactions to below 0-fee transactions.  It could even allow users who\nreceive payouts which are below 0-fee transactions to place a fee on the\nsubsequent transactions to allow the payouts to confirm quicker (if done\nright).\n> \n> > My point was that the easiest way to do it would be to ship a pruned\n> > snapshot with Bitcoin, and such a system, while verifiable, would\n> > increase Bitocin's centralization.\n> \n> I'm not sure why. If you want to audit everything from scratch, after\n> checking the code you could just blow away the included files and then\n> \"-connect=archive.bitcoin.org\" or something like that. After\n> rebuilding the chain from scratch, check the databases for consistency\n> with the included data.\nI would be surprised if more than a handful of devs audit such a thing.\nAnd I would say that does define an increase in centralization.\n> \n> It reduces the number of nodes with full copies of the block chain,\n> yes, but as long as there's at least one copy of the old data in an\n> accessible location new nodes can still bootstrap just fine.\nSadly, old nodes do not know where to look for such data, and I'm fairly\ncertain people running old nodes don't read the forums enough to catch\nwhen it is announced that old nodes should make sure to\n-connect=archive.bitcoin.org in order to avoid initially having horrible\ninitial bootstrap times and eventually not being able to connect to\nfull-chain-serving nodes at all.\n> \n> I'm sure we can find organizations willing to host full chains for\n> people who want to rebuild their databases from scratch, given how\n> cheap disk space is.\nSadly, disk space isnt the issue.  Each connection to bitcoind (not that\nit cant be fixed, but currently) eats a nice chunk of memory.  An\norganization that wants to provide nodes for old nodes to connect to\nwould need to have a significant number of open incoming connection\nslots, have plenty of bandwidth for nodes that are in IBD and have\nplenty of memory and CPU to manage all the connections.\n\n> \n> > connect to, possibly complicating using Bitcoin for clients that either\n> > wish to run a full IBD or older clients which need a non-fClient node\n> \n> Yes, but old nodes probably have a copy of the chain already, so it\n> wouldn't affect them. New blocks would still be fully distributed,\n> right?\nSadly, BDB's infamous database corrupted messages appear all too often,\nand the usual response is \"delete the chain and resync.\"  I have a hard\ntime believing that old nodes will rarely be in IBD.  \n> \n> The only case where it'd cause issues is if you install a fresh copy\n> of a very old node. Not a common occurrence, and those nodes will have\n> to wait until they find an archival node announcing itself. Those\n> nodes could be made to announce more frequently than normal, if need\n> be.\nI agree that its very possible to have archival nodes available and to\nmake it work, but I have yet to see anyone doing any work to actually\nget commitments to run archival nodes and I have yet to see any\ndiscussion of what, exactly, that would entail.\n\nMatt"
            },
            {
                "author": "Stefan Thomas",
                "date": "2012-06-15T16:56:38",
                "message_text_only": "Thanks Mike for the writeup - I'm very sad to have missed the discussion\non IRC since fee economics are probably my favorite topic, but I'll try\nto contribute to the email discussion instead.\n\n> (4) Making the block size limit float is better than picking a new\n> arbitrary threshold.\n\nFees are a product of both real and artificial limits to transaction\nvalidation.\n\nThe artificial limits like the block size limit are essentially putting\na floor on prices by limiting supply beyond what it would otherwise be.\nE.g. the network could confirm more transactions theoretically, but the\nblock size limit prevents it.\n\nThe real limits are the bandwidth, computing and memory resources of\nparticipating nodes. For the sake of argument suppose a 1 TB block was\nreleased into the network right now and we'll also assume there was no\nblock size limit of any kind. Many nodes would likely not be able to\nsuccessfully download this block in under 10-30 minutes, so there is a\nvery good chance that other miners will have generated two blocks before\nthis block makes its way to them.\n\nWhat does this mean? The miner generating a 1 TB block knows this would\nhappen. So in terms of economic self interest he will generate the\nlargest possible block that he is still confident that other miners will\naccept and process. A miner who receives a block will also consider\nwhether to build on it based on whether they think other miners will be\nable to download it. In other words, if I receive a large block I may\ndecide not to mine on it, because I believe that the majority of mining\npower will not mine on it - because it is either too large for them to\ndownload or because their rules against large blocks reject it.\n\nIt's important to understand that in practice economic actors tend to\nplan ahead. In other words, if there is no block size limit that doesn't\nmean that there will be constant forks and total chaos. Rather, no miner\nwill ever want to have a block rejected due to size, there is plenty of\nincentive to be conservative with your limits. Even if there are forks,\nthis simply means that miners have decided that they can make more money\nby including more transactions at the cost of the occasional dud.\n\nTherefore, from an economic perspective, we do not need a global block\nsize limit of any kind. As \"guardians of the network\" the only thing we\nneed to do is to let miners figure out what they wanna do.\n\nHOWEVER, the existing economic incentives won't manifest unless somebody\ntranslates them into code. We have to give our users (miners & endusers)\nthe tools to create a genuine fee-based verification market.\n\nOn the miner side: I would make the block size limit configurable with a\nrelatively high default. If the default is too low few people will\nbother changing it, which means that it is not worth changing (because a\nmajority uses the default anyway), which means even fewer people will\nchange it and so on.\n\nThe block size limit should also be a soft rather than a hard limit -\nhere are some ideas for this:\n\n- The default limit for accepting blocks from others should always be\nsignificantly greater than the default limit for blocks that the client\nitself will generate.\n\n- There should be different size limits for side chains that are longer\nthan the currently active chain. In other words, I might reject a block\nfor being slightly too large, but if everyone else accepts it I should\neventually accept it too, and my client should also consider\nautomatically raising my size limit if this happens a lot.\n\nThe rationale for the soft limit is to allow for gradual upward\nadjustment. It needs to be risky for individual miners to raise the size\nof their blocks to new heights, but ideally there won't be one solid\nwall for them to run into.\n\nOn the user side: I would display the fee on the Send Coins dialog and\nallow users to choose a different fee per transaction. We also talked\nabout adding some UI feedback where the client tries to estimate how\nlong a transaction will take to confirm given a certain fee, based on\nrecent information about what it observed from the network. If the fee\ncan be changed on the Send Coins tab, then this could be a red, yellow,\ngreen visual indication whether the fee is sufficient, adequate or\ndangerously low.\n\nA criticism one might raise is: \"The block size limit is not to protect\nminers, but to protect end users who may have less resources than miners\nand can't download gigantic block chains.\" - That's a viewpoint that is\ncertainly valid. I believe that we will be able to do a lot just with\nefficiency improvements, pruning, compression and whatnot. But when it\ncomes down to it, I'd prefer a large network with cheap\nmicrotransactions even if that means that consumer hardware can't\noperate as a standalone validating node anymore. Headers-only mode is\nalready a much-requested feature anyway and there are many ways of\nimproving the security of various header-only or lightweight protocols.\n\n(I just saw Greg's message advocating the opposite viewpoint, I'll\nrespond to that as soon as I can.)\n\n\n> (1) Change the mining code to group transactions together with their\n> mempool dependencies and then calculate all fees as a group.\n\n+1 Very good change. This would allow miners to maximize their revenue\nand in doing so better represent the existing priorities that users\nexpress through fees.\n\n\n> There was discussion of some one-off changes to address the current\n> situation, namely de-ranking transactions that re-use addresses.\n\nDiscouraging address reuse will not change the amount of transactions, I\nthink we all agree on that. As for whether it improves the\nprioritization, I'm not sure. Use cases that we seek to discourage may\nsimply switch to random addresses and I don't agree in and of itself\nthis is a benefit (see item 4 below). Here are a few reasons one might\nbe against this proposal:\n\n1) Certain use cases like green addresses will be forced to become more\ncomplicated than they would otherwise need to be.\n\n2) It will be harder to read information straight out of the block\nchain, for example right now we can pretty easily see how much volume is\ncaused by Satoshi Dice, perhaps allowing us to make better decisions.\n\n3) The address index that is used by block explorers and lightweight\nclient servers will grow unnecessarily (an address -> tx index will be\nlarger if the number of unique addresses increases given the same number\nof txs), so for people like myself who work on that type of software\nyou're actually making our scalability equation slightly worse.\n\n4) You're forcing people into privacy best practices which you think are\ngood, but others may not subscribe to. For example I have absolutely\nzero interest in privacy, anyone who cares that I buy Bitcoins with my\nsalary and spend them on paragliding is welcome to know about it.\nFrankly, if I cared about privacy I wouldn't be using Bitcoin. If other\npeople want to use mixing services and randomize their addresses and\ncommunicate through Tor that's fine, but the client shouldn't force me\nto do those things if I don't want to by \"deprioritizing\" my transactions.\n\n5) We may not like firstbits, but the fact remains that for now they are\nextremely popular, because they improve the user experience where we\nfailed to do so. If you deprioritize transactions to reused addresses\nyou'll for example deprioritize all/most of Girls Gone Bitcoin, which\n(again, like it or not) is one of the few practical, sustainable niches\nthat Bitcoin has managed to carve out for itself so far.\n\n\n> Having senders/buyers pay no fees is psychologically desirable even\n> though we all understand that eventually, somebody, somewhere will be\n> paying fees to use Bitcoin\n\nFree is just an extreme form of cheap, so if we can make transactions\nvery cheap (through efficiency and very large blocks) then it will be\neasier for charitable miners to include free transactions. In practice,\nmy prediction is that free transactions on the open network will simply\nnot be possible in the long run. Dirty hacks aside there is simply no\nway of distinguishing a spam transaction from a charity-worthy\ntransaction. So the way I envision free transactions in the future is\nthat there may be miners in partnership with wallet providers like\nBlockChain.info that let you submit feeless transactions straight to\nthem based on maybe a captcha or some ads. (For the purist, the captcha\nchallenge and response could be communicated across the bitcoin network,\nbut I think we agree that such things should ideally take place\nout-of-band.)\n\nThat way, the available charity of miners who wish to include feeless\ntransactions would go to human users as opposed to the potentially\ninfinite demand of auto-generated feeless transactions.\n\n\n\nOn 6/15/2012 1:29 PM, Mike Hearn wrote:\n> I had to hit the sack last night as it was 2am CET, but I'd like to\n> sum up the discussion we had on IRC about scalability and SatoshiDice\n> in particular.\n>\n> I think we all agreed on the following:\n>\n> - Having senders/buyers pay no fees is psychologically desirable even\n> though we all understand that eventually, somebody, somewhere will be\n> paying fees to use Bitcoin\n>\n> - In the ideal world Bitcoin would scale perfectly and there would be\n> no need for there to be some \"winners\" and some \"losers\" when it comes\n> to confirmation time.\n>\n> There was discussion of some one-off changes to address the current\n> situation, namely de-ranking transactions that re-use addresses. Gavin\n> and myself were not keen on this idea, primarily because it just\n> avoids the real problem and Bitcoin already has a good way to\n> prioritize transactions via the fees mechanism itself. The real issue\n> is that SatoshiDice does indeed pay fees and generates a lot of\n> transactions, pushing more traditional traffic out due to artificial\n> throttles.\n>\n> The following set of proposals were discussed:\n>\n> (1) Change the mining code to group transactions together with their\n> mempool dependencies and then calculate all fees as a group. A tx with\n> a fee of 1 BTC that depends on 5 txns with zero fees would result in\n> all 6 transactions being considered to have a fee of 1BTC and\n> therefore become prioritized for inclusion. This allows a transition\n> to \"receiver pays\" model for fees. There are many advantages. One is\n> that it actually makes sense ... it's always the receiver who wants\n> confirmations because it's the receiver that fears double spends.\n> Senders never do. What's more, whilst Bitcoin is designed to operate\n> on a zero-trust model in the real world trust often exists and it can\n> be used to optimize by passing groups of transactions around with\n> their dependencies, until that group passes a trust boundary and gets\n> broadcast with a send-to-self tx to add fees. Another advantage is it\n> simplifies usage for end users who primarily buy rather than sell,\n> because it avoids the need to guess at fees, one of the most\n> problematic parts of Bitcoins design now.\n>\n> The disadvantages are that it can result in extra transactions that\n> exist only for adding fees, and it requires a more modern payment\n> protocol than the direct-IP protocol Satoshi designed.\n>\n> It would help address the current situation by avoiding angry users\n> who want to buy things, but don't know what fee to set and so their\n> transactions get stuck.\n>\n> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to\n> avoid paying excessive fees and queue-jumping. Guess that's on my\n> plate.\n>\n> (3) Scalability improvements seem like a no brainer to everyone, it's\n> just a case of how complicated they are.\n>\n> (4) Making the block size limit float is better than picking a new\n> arbitrary threshold.\n>\n> On the forums Matt stated that block chain pruning was a no-go because\n> \"it makes bitcoin more centralized\". I think we've thrashed this one\n> out sufficiently well by now that there should be a united opinion on\n> it. There are technical ways to implement it such that there is no\n> change of trust requirements. All the other issues (finding archival\n> nodes, etc) can be again addressed with sufficient programming.\n>\n> For the case of huge blocks slowing down end user syncing and wasting\n> their resources, SPV clients like MultiBit and Android Wallet already\n> exist and will get better with time. If Jeff implements the bloom\n> filtering p2p commands I'll make bitcoinj use them and that'll knock\n> out excessive bandwidth usage and parse overheads from end users who\n> are on these clients. At some point Bitcoin-Qt can have a dual mode,\n> but who knows when that'll get implemented.\n>\n> Does that all sound reasonable?\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and \n> threat landscape has changed and how IT managers can respond. Discussions \n> will include endpoint security, mobile security and the latest in malware \n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Mike Koss",
                "date": "2012-06-15T17:37:04",
                "message_text_only": "Grouping mempool transactions based on fees of the group seems\nan unnecessary complexity; it makes it harder to predict if an isolated\ntransaction has enough \"juice\" to be included in the next Block.\n\nGiven your point about economic actors adapting to conditions, would it not\nbe simpler to use a individual \"fee per byte\" priority algorithm and let\ntransaction generators distribute their fees accordingly (and more\npredictably)?\n\nThis simpler algorithm will prune arbitrary transactions sub-optimally, but\nhas the benefit of being more understandable and predictable from the point\nof view of transaction generators.\n\nOn Fri, Jun 15, 2012 at 9:56 AM, Stefan Thomas <moon at justmoon.de> wrote:\n\n> Thanks Mike for the writeup - I'm very sad to have missed the discussion\n> on IRC since fee economics are probably my favorite topic, but I'll try\n> to contribute to the email discussion instead.\n>\n> > (4) Making the block size limit float is better than picking a new\n> > arbitrary threshold.\n>\n> Fees are a product of both real and artificial limits to transaction\n> validation.\n>\n> The artificial limits like the block size limit are essentially putting\n> a floor on prices by limiting supply beyond what it would otherwise be.\n> E.g. the network could confirm more transactions theoretically, but the\n> block size limit prevents it.\n>\n> The real limits are the bandwidth, computing and memory resources of\n> participating nodes. For the sake of argument suppose a 1 TB block was\n> released into the network right now and we'll also assume there was no\n> block size limit of any kind. Many nodes would likely not be able to\n> successfully download this block in under 10-30 minutes, so there is a\n> very good chance that other miners will have generated two blocks before\n> this block makes its way to them.\n>\n> What does this mean? The miner generating a 1 TB block knows this would\n> happen. So in terms of economic self interest he will generate the\n> largest possible block that he is still confident that other miners will\n> accept and process. A miner who receives a block will also consider\n> whether to build on it based on whether they think other miners will be\n> able to download it. In other words, if I receive a large block I may\n> decide not to mine on it, because I believe that the majority of mining\n> power will not mine on it - because it is either too large for them to\n> download or because their rules against large blocks reject it.\n>\n> It's important to understand that in practice economic actors tend to\n> plan ahead. In other words, if there is no block size limit that doesn't\n> mean that there will be constant forks and total chaos. Rather, no miner\n> will ever want to have a block rejected due to size, there is plenty of\n> incentive to be conservative with your limits. Even if there are forks,\n> this simply means that miners have decided that they can make more money\n> by including more transactions at the cost of the occasional dud.\n>\n> Therefore, from an economic perspective, we do not need a global block\n> size limit of any kind. As \"guardians of the network\" the only thing we\n> need to do is to let miners figure out what they wanna do.\n>\n> HOWEVER, the existing economic incentives won't manifest unless somebody\n> translates them into code. We have to give our users (miners & endusers)\n> the tools to create a genuine fee-based verification market.\n>\n> On the miner side: I would make the block size limit configurable with a\n> relatively high default. If the default is too low few people will\n> bother changing it, which means that it is not worth changing (because a\n> majority uses the default anyway), which means even fewer people will\n> change it and so on.\n>\n> The block size limit should also be a soft rather than a hard limit -\n> here are some ideas for this:\n>\n> - The default limit for accepting blocks from others should always be\n> significantly greater than the default limit for blocks that the client\n> itself will generate.\n>\n> - There should be different size limits for side chains that are longer\n> than the currently active chain. In other words, I might reject a block\n> for being slightly too large, but if everyone else accepts it I should\n> eventually accept it too, and my client should also consider\n> automatically raising my size limit if this happens a lot.\n>\n> The rationale for the soft limit is to allow for gradual upward\n> adjustment. It needs to be risky for individual miners to raise the size\n> of their blocks to new heights, but ideally there won't be one solid\n> wall for them to run into.\n>\n> On the user side: I would display the fee on the Send Coins dialog and\n> allow users to choose a different fee per transaction. We also talked\n> about adding some UI feedback where the client tries to estimate how\n> long a transaction will take to confirm given a certain fee, based on\n> recent information about what it observed from the network. If the fee\n> can be changed on the Send Coins tab, then this could be a red, yellow,\n> green visual indication whether the fee is sufficient, adequate or\n> dangerously low.\n>\n> A criticism one might raise is: \"The block size limit is not to protect\n> miners, but to protect end users who may have less resources than miners\n> and can't download gigantic block chains.\" - That's a viewpoint that is\n> certainly valid. I believe that we will be able to do a lot just with\n> efficiency improvements, pruning, compression and whatnot. But when it\n> comes down to it, I'd prefer a large network with cheap\n> microtransactions even if that means that consumer hardware can't\n> operate as a standalone validating node anymore. Headers-only mode is\n> already a much-requested feature anyway and there are many ways of\n> improving the security of various header-only or lightweight protocols.\n>\n> (I just saw Greg's message advocating the opposite viewpoint, I'll\n> respond to that as soon as I can.)\n>\n>\n> > (1) Change the mining code to group transactions together with their\n> > mempool dependencies and then calculate all fees as a group.\n>\n> +1 Very good change. This would allow miners to maximize their revenue\n> and in doing so better represent the existing priorities that users\n> express through fees.\n>\n>\n> > There was discussion of some one-off changes to address the current\n> > situation, namely de-ranking transactions that re-use addresses.\n>\n> Discouraging address reuse will not change the amount of transactions, I\n> think we all agree on that. As for whether it improves the\n> prioritization, I'm not sure. Use cases that we seek to discourage may\n> simply switch to random addresses and I don't agree in and of itself\n> this is a benefit (see item 4 below). Here are a few reasons one might\n> be against this proposal:\n>\n> 1) Certain use cases like green addresses will be forced to become more\n> complicated than they would otherwise need to be.\n>\n> 2) It will be harder to read information straight out of the block\n> chain, for example right now we can pretty easily see how much volume is\n> caused by Satoshi Dice, perhaps allowing us to make better decisions.\n>\n> 3) The address index that is used by block explorers and lightweight\n> client servers will grow unnecessarily (an address -> tx index will be\n> larger if the number of unique addresses increases given the same number\n> of txs), so for people like myself who work on that type of software\n> you're actually making our scalability equation slightly worse.\n>\n> 4) You're forcing people into privacy best practices which you think are\n> good, but others may not subscribe to. For example I have absolutely\n> zero interest in privacy, anyone who cares that I buy Bitcoins with my\n> salary and spend them on paragliding is welcome to know about it.\n> Frankly, if I cared about privacy I wouldn't be using Bitcoin. If other\n> people want to use mixing services and randomize their addresses and\n> communicate through Tor that's fine, but the client shouldn't force me\n> to do those things if I don't want to by \"deprioritizing\" my transactions.\n>\n> 5) We may not like firstbits, but the fact remains that for now they are\n> extremely popular, because they improve the user experience where we\n> failed to do so. If you deprioritize transactions to reused addresses\n> you'll for example deprioritize all/most of Girls Gone Bitcoin, which\n> (again, like it or not) is one of the few practical, sustainable niches\n> that Bitcoin has managed to carve out for itself so far.\n>\n>\n> > Having senders/buyers pay no fees is psychologically desirable even\n> > though we all understand that eventually, somebody, somewhere will be\n> > paying fees to use Bitcoin\n>\n> Free is just an extreme form of cheap, so if we can make transactions\n> very cheap (through efficiency and very large blocks) then it will be\n> easier for charitable miners to include free transactions. In practice,\n> my prediction is that free transactions on the open network will simply\n> not be possible in the long run. Dirty hacks aside there is simply no\n> way of distinguishing a spam transaction from a charity-worthy\n> transaction. So the way I envision free transactions in the future is\n> that there may be miners in partnership with wallet providers like\n> BlockChain.info that let you submit feeless transactions straight to\n> them based on maybe a captcha or some ads. (For the purist, the captcha\n> challenge and response could be communicated across the bitcoin network,\n> but I think we agree that such things should ideally take place\n> out-of-band.)\n>\n> That way, the available charity of miners who wish to include feeless\n> transactions would go to human users as opposed to the potentially\n> infinite demand of auto-generated feeless transactions.\n>\n>\n>\n> On 6/15/2012 1:29 PM, Mike Hearn wrote:\n> > I had to hit the sack last night as it was 2am CET, but I'd like to\n> > sum up the discussion we had on IRC about scalability and SatoshiDice\n> > in particular.\n> >\n> > I think we all agreed on the following:\n> >\n> > - Having senders/buyers pay no fees is psychologically desirable even\n> > though we all understand that eventually, somebody, somewhere will be\n> > paying fees to use Bitcoin\n> >\n> > - In the ideal world Bitcoin would scale perfectly and there would be\n> > no need for there to be some \"winners\" and some \"losers\" when it comes\n> > to confirmation time.\n> >\n> > There was discussion of some one-off changes to address the current\n> > situation, namely de-ranking transactions that re-use addresses. Gavin\n> > and myself were not keen on this idea, primarily because it just\n> > avoids the real problem and Bitcoin already has a good way to\n> > prioritize transactions via the fees mechanism itself. The real issue\n> > is that SatoshiDice does indeed pay fees and generates a lot of\n> > transactions, pushing more traditional traffic out due to artificial\n> > throttles.\n> >\n> > The following set of proposals were discussed:\n> >\n> > (1) Change the mining code to group transactions together with their\n> > mempool dependencies and then calculate all fees as a group. A tx with\n> > a fee of 1 BTC that depends on 5 txns with zero fees would result in\n> > all 6 transactions being considered to have a fee of 1BTC and\n> > therefore become prioritized for inclusion. This allows a transition\n> > to \"receiver pays\" model for fees. There are many advantages. One is\n> > that it actually makes sense ... it's always the receiver who wants\n> > confirmations because it's the receiver that fears double spends.\n> > Senders never do. What's more, whilst Bitcoin is designed to operate\n> > on a zero-trust model in the real world trust often exists and it can\n> > be used to optimize by passing groups of transactions around with\n> > their dependencies, until that group passes a trust boundary and gets\n> > broadcast with a send-to-self tx to add fees. Another advantage is it\n> > simplifies usage for end users who primarily buy rather than sell,\n> > because it avoids the need to guess at fees, one of the most\n> > problematic parts of Bitcoins design now.\n> >\n> > The disadvantages are that it can result in extra transactions that\n> > exist only for adding fees, and it requires a more modern payment\n> > protocol than the direct-IP protocol Satoshi designed.\n> >\n> > It would help address the current situation by avoiding angry users\n> > who want to buy things, but don't know what fee to set and so their\n> > transactions get stuck.\n> >\n> > (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to\n> > avoid paying excessive fees and queue-jumping. Guess that's on my\n> > plate.\n> >\n> > (3) Scalability improvements seem like a no brainer to everyone, it's\n> > just a case of how complicated they are.\n> >\n> > (4) Making the block size limit float is better than picking a new\n> > arbitrary threshold.\n> >\n> > On the forums Matt stated that block chain pruning was a no-go because\n> > \"it makes bitcoin more centralized\". I think we've thrashed this one\n> > out sufficiently well by now that there should be a united opinion on\n> > it. There are technical ways to implement it such that there is no\n> > change of trust requirements. All the other issues (finding archival\n> > nodes, etc) can be again addressed with sufficient programming.\n> >\n> > For the case of huge blocks slowing down end user syncing and wasting\n> > their resources, SPV clients like MultiBit and Android Wallet already\n> > exist and will get better with time. If Jeff implements the bloom\n> > filtering p2p commands I'll make bitcoinj use them and that'll knock\n> > out excessive bandwidth usage and parse overheads from end users who\n> > are on these clients. At some point Bitcoin-Qt can have a dual mode,\n> > but who knows when that'll get implemented.\n> >\n> > Does that all sound reasonable?\n> >\n> >\n> ------------------------------------------------------------------------------\n> > Live Security Virtual Conference\n> > Exclusive live event will cover all the ways today's security and\n> > threat landscape has changed and how IT managers can respond. Discussions\n> > will include endpoint security, mobile security and the latest in malware\n> > threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> > _______________________________________________\n> > Bitcoin-development mailing list\n> > Bitcoin-development at lists.sourceforge.net\n> > https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n> >\n>\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n\n\n\n-- \nMike Koss\nCTO, CoinLab\n(425) 246-7701 (m)\n\nA Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need\nto know about Bitcoins.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120615/1bc049d6/attachment.html>"
            },
            {
                "author": "Amir Taaki",
                "date": "2012-06-15T18:38:20",
                "message_text_only": "Forcing users to switch addresses per received payment to work around a bad fee system would be a braindead decision. You might love software and playing with web plugins, but not everyone does. Artists like Rap News can right now simply throw up an address and begin accepting donations. That's a hugely powerful and impactful selling point for Bitcoin.\n\nI don't really see these problems as a concern. Stefan made an excellent post which touched on this, in that miners have an incentive to keep block sizes low so that their blocks propagate. The real problem here is not about block propagation but the user experience. The way I see it, Bitcoin is becoming more specialised over time and part of that process is abstraction. In the past we all used the Satoshi client for mining, merchant functions, validating blocks and personal uses. These are rapidly diverging, and managing the blockchain is not something that user clients should be doing.\n\nMike is right when he says the network only needs a few thousand nodes to function fairly. I am not worried about Bitcoin becoming corrupted because of it being a network \"by bankers for bankers\" because unlike the conventional finance industry, there are no artificial barriers to entry beyond the base cost. This network would always be competitive and strictly operate based on market dynamics.\n\nCase in point: http://en.wikipedia.org/wiki/Coase_theorem\n\nWith strict property rights and zero (or low) transaction costs, the allocation of a system does not matter. The system will make efficient use of its resources. I don't see why a cabal would try to corrupt Bitcoin at expense to themselves when a new competitor can enter the market and undercut them. It's why we expect the ROI on mining to be 0 or negative.\n\n\nI figured out that if you trust data from a blockchain service and only accept data with multiple confirms from each connected service, then you can trivially calculate the probability of being fed corrupt data (assuming a fixed chance per server). In this way, the model is a fault tolerant byzantine system. The chance of being manipulated falls expontentially as you add more servers. And these services can be made highly scalable if you see my BIP 33.\n\nhttps://en.bitcoin.it/wiki/BIP_0033\n\n________________________________\nFrom: Mike Koss <mike at coinlab.com>\nTo: Stefan Thomas <moon at justmoon.de> \nCc: bitcoin-development at lists.sourceforge.net \nSent: Friday, June 15, 2012 7:37 PM\nSubject: Re: [Bitcoin-development] Near-term scalability\n\n\nGrouping mempool transactions based on fees of the group seems an\u00a0unnecessary\u00a0complexity; it makes it harder to predict if an isolated transaction has enough \"juice\" to be included in the next Block.\n\nGiven your point about economic actors adapting to conditions, would it not be simpler to use a individual \"fee per byte\" priority algorithm and let transaction generators distribute their fees accordingly (and more predictably)?\n\nThis simpler algorithm will prune arbitrary transactions sub-optimally, but has the benefit of being more understandable and predictable from the point of view of transaction generators.\n\n\n\nOn Fri, Jun 15, 2012 at 9:56 AM, Stefan Thomas <moon at justmoon.de> wrote:\n\nThanks Mike for the writeup - I'm very sad to have missed the discussion\n>on IRC since fee economics are probably my favorite topic, but I'll try\n>to contribute to the email discussion instead.\n>\n>\n>> (4) Making the block size limit float is better than picking a new\n>> arbitrary threshold.\n>\n>Fees are a product of both real and artificial limits to transaction\n>validation.\n>\n>The artificial limits like the block size limit are essentially putting\n>a floor on prices by limiting supply beyond what it would otherwise be.\n>E.g. the network could confirm more transactions theoretically, but the\n>block size limit prevents it.\n>\n>The real limits are the bandwidth, computing and memory resources of\n>participating nodes. For the sake of argument suppose a 1 TB block was\n>released into the network right now and we'll also assume there was no\n>block size limit of any kind. Many nodes would likely not be able to\n>successfully download this block in under 10-30 minutes, so there is a\n>very good chance that other miners will have generated two blocks before\n>this block makes its way to them.\n>\n>What does this mean? The miner generating a 1 TB block knows this would\n>happen. So in terms of economic self interest he will generate the\n>largest possible block that he is still confident that other miners will\n>accept and process. A miner who receives a block will also consider\n>whether to build on it based on whether they think other miners will be\n>able to download it. In other words, if I receive a large block I may\n>decide not to mine on it, because I believe that the majority of mining\n>power will not mine on it - because it is either too large for them to\n>download or because their rules against large blocks reject it.\n>\n>It's important to understand that in practice economic actors tend to\n>plan ahead. In other words, if there is no block size limit that doesn't\n>mean that there will be constant forks and total chaos. Rather, no miner\n>will ever want to have a block rejected due to size, there is plenty of\n>incentive to be conservative with your limits. Even if there are forks,\n>this simply means that miners have decided that they can make more money\n>by including more transactions at the cost of the occasional dud.\n>\n>Therefore, from an economic perspective, we do not need a global block\n>size limit of any kind. As \"guardians of the network\" the only thing we\n>need to do is to let miners figure out what they wanna do.\n>\n>HOWEVER, the existing economic incentives won't manifest unless somebody\n>translates them into code. We have to give our users (miners & endusers)\n>the tools to create a genuine fee-based verification market.\n>\n>On the miner side: I would make the block size limit configurable with a\n>relatively high default. If the default is too low few people will\n>bother changing it, which means that it is not worth changing (because a\n>majority uses the default anyway), which means even fewer people will\n>change it and so on.\n>\n>The block size limit should also be a soft rather than a hard limit -\n>here are some ideas for this:\n>\n>- The default limit for accepting blocks from others should always be\n>significantly greater than the default limit for blocks that the client\n>itself will generate.\n>\n>- There should be different size limits for side chains that are longer\n>than the currently active chain. In other words, I might reject a block\n>for being slightly too large, but if everyone else accepts it I should\n>eventually accept it too, and my client should also consider\n>automatically raising my size limit if this happens a lot.\n>\n>The rationale for the soft limit is to allow for gradual upward\n>adjustment. It needs to be risky for individual miners to raise the size\n>of their blocks to new heights, but ideally there won't be one solid\n>wall for them to run into.\n>\n>On the user side: I would display the fee on the Send Coins dialog and\n>allow users to choose a different fee per transaction. We also talked\n>about adding some UI feedback where the client tries to estimate how\n>long a transaction will take to confirm given a certain fee, based on\n>recent information about what it observed from the network. If the fee\n>can be changed on the Send Coins tab, then this could be a red, yellow,\n>green visual indication whether the fee is sufficient, adequate or\n>dangerously low.\n>\n>A criticism one might raise is: \"The block size limit is not to protect\n>miners, but to protect end users who may have less resources than miners\n>and can't download gigantic block chains.\" - That's a viewpoint that is\n>certainly valid. I believe that we will be able to do a lot just with\n>efficiency improvements, pruning, compression and whatnot. But when it\n>comes down to it, I'd prefer a large network with cheap\n>microtransactions even if that means that consumer hardware can't\n>operate as a standalone validating node anymore. Headers-only mode is\n>already a much-requested feature anyway and there are many ways of\n>improving the security of various header-only or lightweight protocols.\n>\n>(I just saw Greg's message advocating the opposite viewpoint, I'll\n>respond to that as soon as I can.)\n>\n>\n>\n>> (1) Change the mining code to group transactions together with their\n>> mempool dependencies and then calculate all fees as a group.\n>\n>+1 Very good change. This would allow miners to maximize their revenue\n>and in doing so better represent the existing priorities that users\n>express through fees.\n>\n>\n>\n>> There was discussion of some one-off changes to address the current\n>> situation, namely de-ranking transactions that re-use addresses.\n>\n>Discouraging address reuse will not change the amount of transactions, I\n>think we all agree on that. As for whether it improves the\n>prioritization, I'm not sure. Use cases that we seek to discourage may\n>simply switch to random addresses and I don't agree in and of itself\n>this is a benefit (see item 4 below). Here are a few reasons one might\n>be against this proposal:\n>\n>1) Certain use cases like green addresses will be forced to become more\n>complicated than they would otherwise need to be.\n>\n>2) It will be harder to read information straight out of the block\n>chain, for example right now we can pretty easily see how much volume is\n>caused by Satoshi Dice, perhaps allowing us to make better decisions.\n>\n>3) The address index that is used by block explorers and lightweight\n>client servers will grow unnecessarily (an address -> tx index will be\n>larger if the number of unique addresses increases given the same number\n>of txs), so for people like myself who work on that type of software\n>you're actually making our scalability equation slightly worse.\n>\n>4) You're forcing people into privacy best practices which you think are\n>good, but others may not subscribe to. For example I have absolutely\n>zero interest in privacy, anyone who cares that I buy Bitcoins with my\n>salary and spend them on paragliding is welcome to know about it.\n>Frankly, if I cared about privacy I wouldn't be using Bitcoin. If other\n>people want to use mixing services and randomize their addresses and\n>communicate through Tor that's fine, but the client shouldn't force me\n>to do those things if I don't want to by \"deprioritizing\" my transactions.\n>\n>5) We may not like firstbits, but the fact remains that for now they are\n>extremely popular, because they improve the user experience where we\n>failed to do so. If you deprioritize transactions to reused addresses\n>you'll for example deprioritize all/most of Girls Gone Bitcoin, which\n>(again, like it or not) is one of the few practical, sustainable niches\n>that Bitcoin has managed to carve out for itself so far.\n>\n>\n>\n>> Having senders/buyers pay no fees is psychologically desirable even\n>> though we all understand that eventually, somebody, somewhere will be\n>> paying fees to use Bitcoin\n>\n>Free is just an extreme form of cheap, so if we can make transactions\n>very cheap (through efficiency and very large blocks) then it will be\n>easier for charitable miners to include free transactions. In practice,\n>my prediction is that free transactions on the open network will simply\n>not be possible in the long run. Dirty hacks aside there is simply no\n>way of distinguishing a spam transaction from a charity-worthy\n>transaction. So the way I envision free transactions in the future is\n>that there may be miners in partnership with wallet providers like\n>BlockChain.info that let you submit feeless transactions straight to\n>them based on maybe a captcha or some ads. (For the purist, the captcha\n>challenge and response could be communicated across the bitcoin network,\n>but I think we agree that such things should ideally take place\n>out-of-band.)\n>\n>That way, the available charity of miners who wish to include feeless\n>transactions would go to human users as opposed to the potentially\n>infinite demand of auto-generated feeless transactions.\n>\n>\n>\n>\n>On 6/15/2012 1:29 PM, Mike Hearn wrote:\n>> I had to hit the sack last night as it was 2am CET, but I'd like to\n>> sum up the discussion we had on IRC about scalability and SatoshiDice\n>> in particular.\n>>\n>> I think we all agreed on the following:\n>>\n>> - Having senders/buyers pay no fees is psychologically desirable even\n>> though we all understand that eventually, somebody, somewhere will be\n>> paying fees to use Bitcoin\n>>\n>> - In the ideal world Bitcoin would scale perfectly and there would be\n>> no need for there to be some \"winners\" and some \"losers\" when it comes\n>> to confirmation time.\n>>\n>> There was discussion of some one-off changes to address the current\n>> situation, namely de-ranking transactions that re-use addresses. Gavin\n>> and myself were not keen on this idea, primarily because it just\n>> avoids the real problem and Bitcoin already has a good way to\n>> prioritize transactions via the fees mechanism itself. The real issue\n>> is that SatoshiDice does indeed pay fees and generates a lot of\n>> transactions, pushing more traditional traffic out due to artificial\n>> throttles.\n>>\n>> The following set of proposals were discussed:\n>>\n>> (1) Change the mining code to group transactions together with their\n>> mempool dependencies and then calculate all fees as a group. A tx with\n>> a fee of 1 BTC that depends on 5 txns with zero fees would result in\n>> all 6 transactions being considered to have a fee of 1BTC and\n>> therefore become prioritized for inclusion. This allows a transition\n>> to \"receiver pays\" model for fees. There are many advantages. One is\n>> that it actually makes sense ... it's always the receiver who wants\n>> confirmations because it's the receiver that fears double spends.\n>> Senders never do. What's more, whilst Bitcoin is designed to operate\n>> on a zero-trust model in the real world trust often exists and it can\n>> be used to optimize by passing groups of transactions around with\n>> their dependencies, until that group passes a trust boundary and gets\n>> broadcast with a send-to-self tx to add fees. Another advantage is it\n>> simplifies usage for end users who primarily buy rather than sell,\n>> because it avoids the need to guess at fees, one of the most\n>> problematic parts of Bitcoins design now.\n>>\n>> The disadvantages are that it can result in extra transactions that\n>> exist only for adding fees, and it requires a more modern payment\n>> protocol than the direct-IP protocol Satoshi designed.\n>>\n>> It would help address the current situation by avoiding angry users\n>> who want to buy things, but don't know what fee to set and so their\n>> transactions get stuck.\n>>\n>> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to\n>> avoid paying excessive fees and queue-jumping. Guess that's on my\n>> plate.\n>>\n>> (3) Scalability improvements seem like a no brainer to everyone, it's\n>> just a case of how complicated they are.\n>>\n>> (4) Making the block size limit float is better than picking a new\n>> arbitrary threshold.\n>>\n>> On the forums Matt stated that block chain pruning was a no-go because\n>> \"it makes bitcoin more centralized\". I think we've thrashed this one\n>> out sufficiently well by now that there should be a united opinion on\n>> it. There are technical ways to implement it such that there is no\n>> change of trust requirements. All the other issues (finding archival\n>> nodes, etc) can be again addressed with sufficient programming.\n>>\n>> For the case of huge blocks slowing down end user syncing and wasting\n>> their resources, SPV clients like MultiBit and Android Wallet already\n>> exist and will get better with time. If Jeff implements the bloom\n>> filtering p2p commands I'll make bitcoinj use them and that'll knock\n>> out excessive bandwidth usage and parse overheads from end users who\n>> are on these clients. At some point Bitcoin-Qt can have a dual mode,\n>> but who knows when that'll get implemented.\n>>\n>> Does that all sound reasonable?\n>>\n>> ------------------------------------------------------------------------------\n>> Live Security Virtual Conference\n>> Exclusive live event will cover all the ways today's security and\n>> threat landscape has changed and how IT managers can respond. Discussions\n>> will include endpoint security, mobile security and the latest in malware\n>> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n>\n>\n>------------------------------------------------------------------------------\n>Live Security Virtual Conference\n>Exclusive live event will cover all the ways today's security and\n>threat landscape has changed and how IT managers can respond. Discussions\n>will include endpoint security, mobile security and the latest in malware\n>threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n>_______________________________________________\n>Bitcoin-development mailing list\n>Bitcoin-development at lists.sourceforge.net\n>https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n\n\n-- \nMike Koss\nCTO, CoinLab\n(425) 246-7701 (m)\n\nA Bitcoin Primer\u00a0- What you need to know about Bitcoins.\n\n------------------------------------------------------------------------------\nLive Security Virtual Conference\nExclusive live event will cover all the ways today's security and \nthreat landscape has changed and how IT managers can respond. Discussions \nwill include endpoint security, mobile security and the latest in malware \nthreats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Gavin Andresen",
                "date": "2012-06-15T20:56:25",
                "message_text_only": "> (1) Change the mining code to group transactions together with their\n> mempool dependencies and then calculate all fees as a group.\n\nI think there is general consensus this is a good idea.\n\n> (2) SatoshiDice should use the same fee algorithms as Bitcoin-Qt to\n> avoid paying excessive fees and queue-jumping. Guess that's on my\n> plate.\n\nI don't think the problem is with SatoshiDice, but is with the current\nfixed/inflexible fee rules:\n\n----------------------------\n\nI've been thinking about fees a lot the last couple of days, and I'm\nclose to making a formal proposal. Here are my thoughts so far:\n\nIt seems to me there are two typical use cases:\n\nCase 1:  I want my transaction to be confirmed quickly, and am willing\nto pay a fee to make that happen.\n\nCase 2: I want my transaction to be confirmed eventually. I'd rather\nnot pay, unless I have to.\n\nI don't think the current code handles those two cases as well as it\ncould; here's a proposal to fix that:\n\no Let miners decide on how many free transactions they'll support, by\nletting them specify how much of each block to set aside for 'free'\ntransactions (bytes) and what the threshold for 'free' is\n(bitcoins/kilobyte). I think a new RPC call to get/set the parameters\ndynamically is the right way to go.\n\no Change the block creation code to calculate a\nbitcoin-fee-per-kilobyte for each transaction, where the fee and size\nare both calculated based on the transaction and it's dependent\ndescendants (so we get the receiver-can-opt-to-pay-the-fee behavior we\nwant). Order transactions so highest-paying get into the non-free\nspace first.\n\no Fill up the \"free\" space (if any) with the highest-priority\ntransactions, where priority is a function of transaction size, age of\ninputs, number of bitcoins... and ratio of inputs to outputs (to\nencourage combining inputs so more pruning is possible).\n\nThe fee-paying space in a block lets Use Case #1 users compete to make\ntheir fees high enough to get into the block.\n\nThe free space should let non-spamming Use Case #2 users (who don't\nsend a lot of transactions, and so have well-aged, high-priority\ninputs) send transactions for free, at least as long as there are\nminers willing to accept free transactions.\n\nThe question is: how do clients suggest fees to users if miners might\nhave very different fee policies?\n\nI think full, transaction-verifying clients can watch how long\ntransactions stay in the memory pool to figure it out. I'm gathering\nstatistics right now to test a couple of simple heuristic algorithms\nfor reasonable fee/priority policies.\n\nBut that won't work for newly started clients that haven't seen a lot\nof transactions enter/exit the memory pool, or SPV clients that can't\nlookup transaction inputs (so can't calculate what fees are being paid\n-- and once we have bloom filters may not be paying attention anything\nbut their own transactions, anyway).\n\nI'm still thinking about that.\n\nMaybe a new p2p network command: you give me a list of block hashes, I\ntell you average fees paid per kilobyte for fee-paying transactions in\nthose blocks, and minimum and average priority of free transactions in\nthose blocks.\n\nMaybe the big mining pools all publish their fee policies and that\ninformation somehow gets to clients (encoded in the coinbase? ... but\nthey have a strong incentive to lie to try put upward pressure on\nfees... ).\n\nMaybe each client developer runs a \"fee policy server\" and clients\nperiodically ask it for reasonable fee rules (HTTP fetch info from a\nweb page that is updated as often or infrequently as is convenient,\nmaybe). I think I like this solution the best, it should let clients\ncompete to have the smartest/bestest algorithms for saving their\nuser's money on transaction fees.\n\n-- \n--\nGavin Andresen"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-16T07:55:55",
                "message_text_only": "[resend, sorry gavin]\n\nI think these ideas all make a ton of sense, some have been floating\naround for a while in various forms but it's good to draw them\ntogether coherently.\n\n> o Fill up the \"free\" space (if any) with the highest-priority\n> transactions, where priority is a function of transaction size, age of\n> inputs, number of bitcoins... and ratio of inputs to outputs (to\n> encourage combining inputs so more pruning is possible).\n\nIs more incentive needed? If you have tons of tiny outputs you already\nhave incentives to merge them because otherwise your txns will become\nlarge and the fees needed to overcome the DoS limits and gain priority\nwill rise.\n\nThe code to do it is a bit irritating as you really want to de-frag\nwallets in the background when the user is not likely to need the\noutputs quickly, and I suspect over time transaction volumes will\nbecome diurnal so it'd be cheaper to do that at night time, but it's\nall possible.\n\n> But that won't work for newly started clients that haven't seen a lot\n> of transactions enter/exit the memory pool\n\nPeers could provide first-seen timestamps for transactions when\nannounced or when downloaded with Jeffs proposed command, but the\ntimestamps are not necessarily trustable. Not sure if that'd open up\nnew attacks.\n\n> or SPV clients that can't lookup transaction inputs\n\nSPV clients can do it by getdata-ing on the relevant inputs, but it's\nvery bandwidth intensive just to guesstimate fees.\n\n> Maybe each client developer runs a \"fee policy server\"\n\nThat's reasonable. I don't believe this case is worth worrying about\nright now. For the common cases of\n\na) Customer buys from merchant (runs full node)\nb) Trusted person sends money to trusting person (does not need confirms)\n\nit wouldn't matter after the changes to the block creation code. It's\nonly really an issue when a user running an SPV client wishes to\naccept money from somebody they do not trust, and they want it to\nconfirm quick-ish (within an hour), but can tolerate delays up to\nthat. I think this is likely to be rare.\n\nMuch more common is that you want to accept the payment immediately,\nwhich is an oft discussed but different problem."
            },
            {
                "author": "Amir Taaki",
                "date": "2012-06-15T18:50:47",
                "message_text_only": "> less expensive. This is no more \"real\" or less \"artificial\" then an\n> imposed licensing fee or the like and it is not subject to market\n> forces.\n\nSure, the market is not always efficient nor desirable. This seems more like a social question though about choice and information. I do strongly feel that users should have more control over their technology, and a say in how Bitcoin operates. It is our job to present the choices and inform them to make good decisions. If we think how to implement this with a social component of the users operating the network rather than hard and fast rules, I think that's the preferrable way.\n\nPart of the problem is that Satoshi didn't totally anticipate the growth of the network. The block reward (the subsidy) is too high, which is why transactions can afford to be so cheap. What would happen if blocks required a cumulative fee of XN BTC for N transactions before being accepted?\n\n\n\n----- Original Message -----\nFrom: Gregory Maxwell <gmaxwell at gmail.com>\nTo: Amir Taaki <zgenjix at yahoo.com>\nCc: \nSent: Friday, June 15, 2012 8:43 PM\nSubject: Re: [Bitcoin-development] Near-term scalability\n\nOn Fri, Jun 15, 2012 at 2:38 PM, Amir Taaki <zgenjix at yahoo.com> wrote:\n> Forcing users to switch addresses per received payment to work around a bad fee system would be a braindead decision. You might love software and playing with web plugins, but not everyone does. Artists like Rap News can right now simply throw up an address and begin accepting donations. That's a hugely powerful and impactful selling point for Bitcoin.\n\nAnd that use case does not need fast confirmations!\n\nThis is making the point.\n\n>there are no artificial barriers to entry beyond the base cost. This network would always be competitive and strictly operate based on market dynamics.\n\nThe users of bitcoin can collectively choose how expensive operating a\nfull node is by accepting validation rules that allow it to be more or\nless expensive. This is no more \"real\" or less \"artificial\" then an\nimposed licensing fee or the like and it is not subject to market\nforces."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-15T18:55:52",
                "message_text_only": "On Fri, Jun 15, 2012 at 2:50 PM, Amir Taaki <zgenjix at yahoo.com> wrote:\n> Part of the problem is that Satoshi didn't totally anticipate the growth of the network. The block reward (the subsidy) is too high, which is why transactions can afford to be so cheap. What would happen if blocks required a cumulative fee of XN BTC for N transactions before being accepted?\n\nI would take the last block I solved and use it to write a transaction\nto nowhere which which gave all 50 BTC out in fee.  This pays for as\nmany transactions in the block as I like for any value of X you want\nto choose.\n\nYou should read the bitcointalk forums more often: variants on that\nidea are frequently suggested and dismantled. There is a lot of noise\nthere but also a lot of ideas and knowing what doesn't work is good\ntoo."
            }
        ],
        "thread_summary": {
            "title": "Near-term scalability",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Mike Hearn",
                "Amir Taaki",
                "Gregory Maxwell",
                "Matt Corallo",
                "Gavin Andresen",
                "Stefan Thomas",
                "Mike Koss"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 77527
        }
    },
    {
        "title": "[Bitcoin-development] Suggestion for Simplifying development work",
        "thread_messages": [
            {
                "author": "Peter Vessenes",
                "date": "2012-06-15T14:59:40",
                "message_text_only": "Hi all,\n\nI've been wondering about whether it would be possible to wipe out the GUI\ncompletely from the satoshi client, and reimplement any necessary data\nrequests as RPC calls, allowing us to fork -QT and other GUIs over and\n(hopefully) dramatically simplifying the codebase that you all have to work\non.\n\nAny thoughts about this? Once a week at least I find myself wanting to find\nways to help speed up development, this seems like it could be a big win.\n\nPeter\n\n-- \nPeter J. Vessenes\nCEO, CoinLab\nM: 206.595.9839\nSkype: vessenes\nGoogle+ <https://plus.google.com/112885659993091300749>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120615/79d6e61a/attachment.html>"
            },
            {
                "author": "Wladimir",
                "date": "2012-06-15T15:59:02",
                "message_text_only": "On Fri, Jun 15, 2012 at 4:59 PM, Peter Vessenes <peter at coinlab.com> wrote:\n\n> Hi all,\n>\n> I've been wondering about whether it would be possible to wipe out the GUI\n> completely from the satoshi client, and reimplement any necessary data\n> requests as RPC calls, allowing us to fork -QT and other GUIs over and\n> (hopefully) dramatically simplifying the codebase that you all have to work\n> on.\n>\n\nSplitting the UI into a seperate *process* is a long-term goal. The UI code\nis structured so that all communication with the core happens through a\n\"bottleneck\" (consisting of the model classes), so preparation has been\nunder way.\n\nHowever, the current RPC calls don't suffice to implement a full-featured,\nresponsive UI. I'm not even sure JSON-RPC is a good fit for a UI<->core\nprotocol, as it doesn't support bidirectional communication (at least\nwithout pretty ugly hacks).\n\nBut what exactly is the problem with having a GUI as part of the main\nclient project? I don't see how it would \"speed up development\" to split\nthe project. By far most of the users use the program through the UI so it\nis one of the drivers for requirements on the core, and I'd think it is\npretty important to keep it a first-class citizen.\n\nWladimir\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120615/f4a07b7b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Suggestion for Simplifying development work",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Peter Vessenes",
                "Wladimir"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2182
        }
    },
    {
        "title": "[Bitcoin-development]  Near-term scalability",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-15T16:53:13",
                "message_text_only": "[I originally sent an earlier version of this message to Mike off\nlist, but I figure it's worth adding to the public discussion]\n\nOn Fri, Jun 15, 2012 at 7:29 AM, Mike Hearn <mike at plan99.net> wrote:\n> (4) Making the block size limit float is better than picking a new\n> arbitrary threshold.\n> On the forums Matt stated that block chain pruning was a no-go because\n> \"it makes bitcoin more centralized\". I think we've thrashed this one\n> out sufficiently well by now that there should be a united opinion on\n> it.\n\nBy itself letting the size float has non-trivial existential risk. \u00a0A\nBitcoin with expensive transactions due to competition for space in\nblocks can be front-ended with fast payment systems and still provide\nthe promised decentralized currency. Bitcoin with a very large\nblockchain and blocks does not. \u00a0It would do the bitcoin users no good\nto increase the transaction volume while concurrently making Bitcoin\nmore or less pointless over the alternatives.\n\nScalability must be improved, we can unite on that opinion. \u00a0But\nscalability can't come at the expense of what made Bitcoin worth\nhaving in the first place.\n\nFortunately it appear to be possible to greatly increase the\nscalability without compromising on keeping the costs of operating a\nfully validating node very low, \u00a0for example Pieter's experimentation\nwith txout+txid indexing (for the 'flip the chain' proposals)\nindicates that the data required right now to validate further\ntransactions is only about 85MiB\u2014 and that would be somewhat smaller\nwith compression and with clients which intentionally try to reduce\nthe set of unspent transactions. \u00a0 Commitments to these indexes in the\nchain would allow almost-full validating nodes with fairly limited\nresources.  (Almost-full meaning they would not validate the history\nlong before they started, they'd trusted header difficulty for that. They\ncould still mine and otherwise act as full nodes).\n\nAchieving scalability improvements without breaking the radical\ndecentralization will be a lot harder than just improving scalability\nbut it's effort that is justified if the scalability is actually\nneeded.\n\nHow much decentralization is needed in the end?  That isn't clear\u2014 \"As\nmuch as possible\" should generally be the goal.  Modern currencies\naren't controlled by single parties but by tens of thousands of\nparties locked in economic, legal, and political compromise that\nlimits their control.  In Bitcoin the traditional controls that keep\nparties honest are non-existent and if they were just directly applied\nwe'd potentially lose the properties that make Bitcoin distinct and\nuseful (e.g. make all miners mine only with FED permission and you\njust have a really bandwidth inefficient interface to the dollar).\nInstead we have aggressive decentralization and autonomous rule\nenforcement.\n\nMike pointed out that  \"Before he left Satoshi made a comment saying\nhe used to think Bitcoin would need millions of nodes if it became\nreally popular, but in the end he thought it could do fine with just\ntens of thousands\"    I'm not so sure\u2014 and I think the truth is in\nbetween.  Tens of thousands of nodes\u2014 run by a self-selecting bunch of\npeople who reap the greatest rewards from controlling the validation\nof Bitcoin, who by that criteria necessarily have a lot in common with\neach other and perhaps not with the regular users\u2014 could easily be an\noutcome where control is _less_ publicly vested than popular\ngovernment controlled currencies.   We probably don't need the raw\nnumbers of nodes, but we need a distribution of ownership and a\ndistribution of interest (e.g. not a system by bankers for bankers) of\nthose nodes which I think can only be achieved by making them cheap to\noperate and having a lot more than we actually need. \u2014 though not so\nmuch that it has to run on every laptop.\n\nThe core challenge is that the only obvious ways to justify the cost\nof maintaining expensive validation infrastructure is because you\nintend to manipulate the currency using it or because you intend to\nprevent other people from manipulating the currency.  The latter\nmotivation is potentially subject to a tragedy of the commons\u2014 you\ndon't need to run a full validating node as long as 'enough' other\npeople do, and enough is a nice slippery slope to zero.   Right now\njust the random computers I\u2014 some random geek\u2014 had at home prior to\nBitcoin could store over a hundred years of max size blocks and\nprocess the maximum rate of transactions.   With the costs so low\nthere isn't any real question about a consolidation of validation\nmaking Bitcoin pointless.  You could probably increase the scale 10x\nwithout breaking that analysis  but beyond that unless the\ncost-per-scale goes down a highly consolidated future seems likely.\n40 years from now why would people use Bitcoin over centralized\nprivate banknotes like paypal or democratic government controlled\ncurrencies?\n\nPerhaps Bitcoin transaction could transition to being more of the\nsame\u2014 controlled by a consortium of banks, exchanging gigabyte blocks\nover terabit ethernet, but I think that would be sad.  An alternative\nwhich was autonomous and decentralized even if the transactions were\nsomewhat slow or costly would be excellent competition for everything\nelse, and it's something I think man kind ought to have."
            }
        ],
        "thread_summary": {
            "title": "Near-term scalability",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Gregory Maxwell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5295
        }
    },
    {
        "title": "[Bitcoin-development] SatoshiDice and Near-term scalability",
        "thread_messages": [
            {
                "author": "Jeff Garzik",
                "date": "2012-06-15T17:17:18",
                "message_text_only": "On Fri, Jun 15, 2012 at 12:56 PM, Stefan Thomas <moon at justmoon.de> wrote:\n> The artificial limits like the block size limit are essentially putting\n[...]\n\nChanging the block size is an item for the hard-fork list.  The chance\nof the block size limit changing in the short term seems rather low...\n it is a \"nuclear option.\"\n\nHard-fork requires a very high level of community buy-in, because it\nshuts out older clients who will simply refuse to consider >1MB blocks\nvalid.\n\nAnything approaching that level of change would need some good, hard\ndata indicating that SatoshiDice was shutting out the majority of\nother traffic.  Does anyone measure mainnet \"normal tx\" confirmation\ntimes on a regular basis?  Any other hard data?\n\nClearly SatoshiDice is a heavy user of the network, but there is a\nvast difference between a good stress test and a network flood that is\nshutting out non-SD users.\n\nCan someone please help quantify the situation?  kthanks :)\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com"
            },
            {
                "author": "Stefan Thomas",
                "date": "2012-06-15T17:52:10",
                "message_text_only": "I do agree that changing/lifting the block size limit is a hard fork\nmeasure, but Mike raised the point and I do believe that whatever we\ndecide to do now will be informed by our long term plan as well. So I\nthink it is relevant to the discussion.\n\n> Can someone please help quantify the situation?  kthanks :)\n\nAccording to BlockChain.info we seem to have lots of small blocks of\n0-50KB and some larger 200-300 KB blocks. So in terms of near term\nmeasure one thing I would like to know is why miners (i.e. no miners at\nall) are fully exhausting the available block size despite thousands of\ntransactions in the memory pool. I'm not too familiar with the default\ninclusion rules, so that would certainly be interesting to understand.\nThere are probably some low hanging fruit here.\n\nThe fact that SatoshiDice is able to afford to pay 0.0005 BTC fees and\nfill up the memory pool means that either users who care about speedy\nconfirmation have to pay higher fees, the average actual block size has\nto go up or prioritization has to get smarter. If load increases more\nthen we need more of any of these three tendencies as well. (Note that\nthe last one is only a very limited fix, because as the high priority\ntransactions get confirmed faster, the low priority ones take even longer.)\n\n\nOn 6/15/2012 7:17 PM, Jeff Garzik wrote:\n> On Fri, Jun 15, 2012 at 12:56 PM, Stefan Thomas <moon at justmoon.de> wrote:\n>> The artificial limits like the block size limit are essentially putting\n> [...]\n>\n> Changing the block size is an item for the hard-fork list.  The chance\n> of the block size limit changing in the short term seems rather low...\n>  it is a \"nuclear option.\"\n>\n> Hard-fork requires a very high level of community buy-in, because it\n> shuts out older clients who will simply refuse to consider >1MB blocks\n> valid.\n>\n> Anything approaching that level of change would need some good, hard\n> data indicating that SatoshiDice was shutting out the majority of\n> other traffic.  Does anyone measure mainnet \"normal tx\" confirmation\n> times on a regular basis?  Any other hard data?\n>\n> Clearly SatoshiDice is a heavy user of the network, but there is a\n> vast difference between a good stress test and a network flood that is\n> shutting out non-SD users.\n>\n> Can someone please help quantify the situation?  kthanks :)\n>"
            },
            {
                "author": "Jonathan Warren",
                "date": "2012-06-16T02:35:13",
                "message_text_only": "Yes, I measure mainnet confirmation times on a regular basis.\nhttp://bitcoinstats.org/post/tx-confirmation-times-June2012.png\n\nBefore fairly recently, fee-paying transactions never took anywhere close to\nthis long to be confirmed. \n\nJonathan Warren\n(Bitcointalk: Atheros)\n\n-----Original Message-----\nFrom: Jeff Garzik [mailto:jgarzik at exmulti.com] \nSent: Friday, June 15, 2012 1:17 PM\nTo: bitcoin-development at lists.sourceforge.net\nSubject: [Bitcoin-development] SatoshiDice and Near-term scalability\n\nHard-fork requires a very high level of community buy-in, because it shuts\nout older clients who will simply refuse to consider >1MB blocks valid.\n\nAnything approaching that level of change would need some good, hard data\nindicating that SatoshiDice was shutting out the majority of other traffic.\nDoes anyone measure mainnet \"normal tx\" confirmation times on a regular\nbasis?  Any other hard data?"
            },
            {
                "author": "Amir Taaki",
                "date": "2012-06-16T04:33:31",
                "message_text_only": "Did anyone try sending them an email asking them to stop or offering help to fix their site? What did they say? I'm sure they would try to be accomodating.\n\n\n\n----- Original Message -----\nFrom: Jonathan Warren <jonathan at bitcoinstats.org>\nTo: bitcoin-development at lists.sourceforge.net\nCc: \nSent: Saturday, June 16, 2012 4:35 AM\nSubject: Re: [Bitcoin-development] SatoshiDice and Near-term scalability\n\nYes, I measure mainnet confirmation times on a regular basis.\nhttp://bitcoinstats.org/post/tx-confirmation-times-June2012.png\n\nBefore fairly recently, fee-paying transactions never took anywhere close to\nthis long to be confirmed. \n\nJonathan Warren\n(Bitcointalk: Atheros)\n\n-----Original Message-----\nFrom: Jeff Garzik [mailto:jgarzik at exmulti.com] \nSent: Friday, June 15, 2012 1:17 PM\nTo: bitcoin-development at lists.sourceforge.net\nSubject: [Bitcoin-development] SatoshiDice and Near-term scalability\n\nHard-fork requires a very high level of community buy-in, because it shuts\nout older clients who will simply refuse to consider >1MB blocks valid.\n\nAnything approaching that level of change would need some good, hard data\nindicating that SatoshiDice was shutting out the majority of other traffic.\nDoes anyone measure mainnet \"normal tx\" confirmation times on a regular\nbasis?\u00a0 Any other hard data?\n\n\n\n------------------------------------------------------------------------------\nLive Security Virtual Conference\nExclusive live event will cover all the ways today's security and \nthreat landscape has changed and how IT managers can respond. Discussions \nwill include endpoint security, mobile security and the latest in malware \nthreats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-16T08:30:30",
                "message_text_only": "Joseph is quite accommodating and doesn't want to hurt the network.\nThat said \"asking him to stop\" seems like the worst possible solution\npossible. His site is quite reasonable.\n\nI think if I fix bitcoinj to have smarter fee code he might stop\nattaching a small fee to every TX, but I'm not sure.\n\nOn Sat, Jun 16, 2012 at 6:33 AM, Amir Taaki <zgenjix at yahoo.com> wrote:\n> Did anyone try sending them an email asking them to stop or offering help to fix their site? What did they say? I'm sure they would try to be accomodating.\n>\n>\n>\n> ----- Original Message -----\n> From: Jonathan Warren <jonathan at bitcoinstats.org>\n> To: bitcoin-development at lists.sourceforge.net\n> Cc:\n> Sent: Saturday, June 16, 2012 4:35 AM\n> Subject: Re: [Bitcoin-development] SatoshiDice and Near-term scalability\n>\n> Yes, I measure mainnet confirmation times on a regular basis.\n> http://bitcoinstats.org/post/tx-confirmation-times-June2012.png\n>\n> Before fairly recently, fee-paying transactions never took anywhere close to\n> this long to be confirmed.\n>\n> Jonathan Warren\n> (Bitcointalk: Atheros)\n>\n> -----Original Message-----\n> From: Jeff Garzik [mailto:jgarzik at exmulti.com]\n> Sent: Friday, June 15, 2012 1:17 PM\n> To: bitcoin-development at lists.sourceforge.net\n> Subject: [Bitcoin-development] SatoshiDice and Near-term scalability\n>\n> Hard-fork requires a very high level of community buy-in, because it shuts\n> out older clients who will simply refuse to consider >1MB blocks valid.\n>\n> Anything approaching that level of change would need some good, hard data\n> indicating that SatoshiDice was shutting out the majority of other traffic.\n> Does anyone measure mainnet \"normal tx\" confirmation times on a regular\n> basis?\u00a0 Any other hard data?\n>\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            }
        ],
        "thread_summary": {
            "title": "SatoshiDice and Near-term scalability",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "Mike Hearn",
                "Jonathan Warren",
                "Amir Taaki",
                "Stefan Thomas"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 9088
        }
    },
    {
        "title": "[Bitcoin-development] RPC and signals - processing priority",
        "thread_messages": [
            {
                "author": "grarpamp",
                "date": "2012-06-15T20:55:41",
                "message_text_only": "While happily processing these:\nreceived block ...\nSetBestChain: new best=...  height=...  work=...\nProcessBlock: ACCEPTED\n\nbitcoind very often refuses to answer rpc queries such as getinfo/stop,\nor signals such as kill/ctrl-c. It even registers:\n ThreadRPCServer method=getinfo/stop\nin the debug log. But the action doesn't happen as expected.\n\nShouldn't it be checking and processing all user interrupts like\nonce per block and doing the chain in the background?\n\nHow do busy commerce servers deal with this poor rpc handling?\n\nIs there a way to increase the priority of user scheduled tasks?\nWhat's going on? Thanks."
            },
            {
                "author": "Wladimir",
                "date": "2012-06-16T07:04:34",
                "message_text_only": "On Fri, Jun 15, 2012 at 10:55 PM, grarpamp <grarpamp at gmail.com> wrote:\n\n> While happily processing these:\n> received block ...\n> SetBestChain: new best=...  height=...  work=...\n> ProcessBlock: ACCEPTED\n>\n> bitcoind very often refuses to answer rpc queries such as getinfo/stop,\n> or signals such as kill/ctrl-c. It even registers:\n>  ThreadRPCServer method=getinfo/stop\n> in the debug log. But the action doesn't happen as expected.\n>\n> Shouldn't it be checking and processing all user interrupts like\n> once per block and doing the chain in the background?\n>\n\n\nThis has nothing to do with priority and \"user interrupts\", but with the\nlocks on the wallet and client. Every RPC command takes both locks, and\nreleases them only when finished.\n\nShutting down also requires both locks, so the operations will be\nserialized.\n\nThis protects the database and critical data structures. Sure, there might\nbe some cases in which the locks are not necessary, or read/write locks\ncould be used instead to improve concurrency, but this has to be approached\nreally carefully.\n\nWladimir\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120616/aca1635e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "RPC and signals - processing priority",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Wladimir",
                "grarpamp"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1877
        }
    },
    {
        "title": "[Bitcoin-development] Manual file cleanup on exit, safe? [coredump backtrace]",
        "thread_messages": [
            {
                "author": "grarpamp",
                "date": "2012-06-15T20:58:55",
                "message_text_only": "When bitcoind exits cleanly, it does not seem safe for the blockchain\nto clean up the following hierarchy with rm -r ?\n\ndatabase/\ndb.log\n.lock\ndebug.log\naddr.dat\nwallet.dat\n\nAnd what about adding to the above list the following files when\nbitcoind crashes:\n\n__db.*\n\nIs there an option to make bitcoind roll/flush the above files on\nexit so they can be removed/ported?\n\nNo matter the answers, bitcoind should not be dumping core.\n\n\nBitcoin version v0.6.2.2-unk-beta ()\nDefault data directory /.../.bitcoin\nLoading addresses...\ndbenv.open LogDir=/.../.bitcoin/database ErrorFile=/.../.bitcoin/db.log\n\n************************\nEXCEPTION: 11DbException\nDb::open: Invalid argument\nbitcoin in AppInit()\nterminate called after throwing an instance of 'DbException'\n  what():  Db::open: Invalid argument\nsh: abort (core dumped)\n\nfile unknown has LSN 38/7968116, past end of log at 1/28\nCommonly caused by moving a database from one database environment\nto another without clearing the database LSNs, or by removing all of\nthe log files from a database environment\n__db_meta_setup: /.../.bitcoin/addr.dat: unexpected file type or format\n\n\n[New Thread 28801140 (LWP 100964/initial thread)]\n(gdb) bt\n#0  0x2873e9a7 in kill () from /lib/libc.so.7\n#1  0x2852d397 in raise () from /lib/libthr.so.3\n#2  0x2873d4da in abort () from /lib/libc.so.7\n#3  0x285a0880 in __gnu_cxx::__verbose_terminate_handler () from\n/usr/lib/libstdc++.so.6\n#4  0x285a508a in std::set_unexpected () from /usr/lib/libstdc++.so.6\n#5  0x285a50d2 in std::terminate () from /usr/lib/libstdc++.so.6\n#6  0x285a4f58 in __cxa_rethrow () from /usr/lib/libstdc++.so.6\n#7  0x0816d2ea in PrintException (pex=0x288251b0, pszThread=0x82f4cfa\n\"AppInit()\") at util.cpp:792\n#8  0x08087625 in AppInit (argc=2, argv=0xbfbfedf0) at init.cpp:113\n#9  0x0808766d in main (argc=Cannot access memory at address 0x3) at init.cpp:96"
            },
            {
                "author": "Pieter Wuille",
                "date": "2012-06-15T23:11:39",
                "message_text_only": "On Fri, Jun 15, 2012 at 04:58:55PM -0400, grarpamp wrote:\n> When bitcoind exits cleanly, it does not seem safe for the blockchain\n> to clean up the following hierarchy with rm -r ?\n\nUse -detachdb if you want to detach the blockchain database files from the\ndatabase environment at exit. This was turned off by default in 0.6.0 to\nspeed up the shutdown process very significantly, and few people have a need\nto manually fiddle with their blockchain database files.\n\n-- \nPieter"
            }
        ],
        "thread_summary": {
            "title": "Manual file cleanup on exit, safe? ",
            "categories": [
                "Bitcoin-development",
                "coredump backtrace"
            ],
            "authors": [
                "Pieter Wuille",
                "grarpamp"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2341
        }
    },
    {
        "title": "[Bitcoin-development] Proposed new P2P command and response: getcmds, cmdlist",
        "thread_messages": [
            {
                "author": "Jeff Garzik",
                "date": "2012-06-16T00:13:21",
                "message_text_only": "Outside of major features advertised network-wide in nService bits,\nP2P protocol lacks a good method of enumerating minor features or\nextensions.  The version number increment is coarse-grained, and is\nnot self-documenting.  A simple extension which lists supported\ncommands is added, as demonstrated in this pull request:\n\n     https://github.com/bitcoin/bitcoin/pull/1471\n\nAnother option is for verack to return this information at login,\neliminating the need for a separate command/response.\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com"
            },
            {
                "author": "Amir Taaki",
                "date": "2012-06-16T01:34:53",
                "message_text_only": "Introspection/command discovery is nice, but I would prefer it to be immediately done in the first version exchange so no assumptions as to how a network is operating need to be made.\n\nI like the idea of a flat list of commands. It might make sense to have \"meta\"-commands that alias to groups of commands. i.e \"original\" for the current core subset up to (and including) \"pong\". The aliases could exist in a text definition file which is held on github or bitcoin.org/\n\n\n----- Original Message -----\nFrom: Jeff Garzik <jgarzik at exmulti.com>\nTo: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\nCc: \nSent: Saturday, June 16, 2012 2:13 AM\nSubject: [Bitcoin-development] Proposed new P2P command and response: getcmds, cmdlist\n\nOutside of major features advertised network-wide in nService bits,\nP2P protocol lacks a good method of enumerating minor features or\nextensions.\u00a0 The version number increment is coarse-grained, and is\nnot self-documenting.\u00a0 A simple extension which lists supported\ncommands is added, as demonstrated in this pull request:\n\n\u00a0 \u00a0 https://github.com/bitcoin/bitcoin/pull/1471\n\nAnother option is for verack to return this information at login,\neliminating the need for a separate command/response.\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com\n\n------------------------------------------------------------------------------\nLive Security Virtual Conference\nExclusive live event will cover all the ways today's security and \nthreat landscape has changed and how IT managers can respond. Discussions \nwill include endpoint security, mobile security and the latest in malware \nthreats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Wladimir",
                "date": "2012-06-16T06:45:00",
                "message_text_only": "As replied on the github issue:\n\nPersonally I still think it's better to have a clear standardized \"protocol\nversion\", that implies what capabilities are supported, instead of a\ncapability-based system that explicitly lists them.\n\nCapability-based systems (just look at OpenGL) tend to become horrendously\ncomplex, as you have to take into account all possible combinations of\npossible interactions, and constantly check for support of specific\nfeatures instead of just comparing a version number.\n\nSure, it can be necessary to distinguish between different types of nodes,\nbut there is no need to make it this fine-grained.\n\nWladimir\n\nOn Sat, Jun 16, 2012 at 3:34 AM, Amir Taaki <zgenjix at yahoo.com> wrote:\n\n> Introspection/command discovery is nice, but I would prefer it to be\n> immediately done in the first version exchange so no assumptions as to how\n> a network is operating need to be made.\n>\n> I like the idea of a flat list of commands. It might make sense to have\n> \"meta\"-commands that alias to groups of commands. i.e \"original\" for the\n> current core subset up to (and including) \"pong\". The aliases could exist\n> in a text definition file which is held on github or bitcoin.org/\n>\n>\n> ----- Original Message -----\n> From: Jeff Garzik <jgarzik at exmulti.com>\n> To: Bitcoin Development <bitcoin-development at lists.sourceforge.net>\n> Cc:\n> Sent: Saturday, June 16, 2012 2:13 AM\n> Subject: [Bitcoin-development] Proposed new P2P command and response:\n> getcmds, cmdlist\n>\n> Outside of major features advertised network-wide in nService bits,\n> P2P protocol lacks a good method of enumerating minor features or\n> extensions.  The version number increment is coarse-grained, and is\n> not self-documenting.  A simple extension which lists supported\n> commands is added, as demonstrated in this pull request:\n>\n>     https://github.com/bitcoin/bitcoin/pull/1471\n>\n> Another option is for verack to return this information at login,\n> eliminating the need for a separate command/response.\n>\n> --\n> Jeff Garzik\n> exMULTI, Inc.\n> jgarzik at exmulti.com\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120616/d990c59b/attachment.html>"
            },
            {
                "author": "Andy Parkins",
                "date": "2012-06-16T08:16:24",
                "message_text_only": "On Saturday 16 Jun 2012 07:45:00 Wladimir wrote:\n> As replied on the github issue:\n> \n> Personally I still think it's better to have a clear standardized\n> \"protocol version\", that implies what capabilities are supported,\n> instead of a capability-based system that explicitly lists them.\n> \n> Capability-based systems (just look at OpenGL) tend to become\n> horrendously complex, as you have to take into account all possible\n> combinations of possible interactions, and constantly check for support\n> of specific features instead of just comparing a version number.\n> \n> Sure, it can be necessary to distinguish between different types of\n> nodes, but there is no need to make it this fine-grained.\n\nIt's less of a problem in a (nearly) stateless protocol like Bitcoin.\n\nI like the idea of a capabilities command; as time goes on and the ecosystem \nof thin/spv/semi-thin/headers-only/blocks-on-demand/reverse-search-\nblockchain/memory-pool-query clients becomes more varied, it's going to be \nmore an more important.  The particular example that occurs is thin clients \nconnecting to the network are going to want to ensure they are connected to \nat least one non-thin client.\n\n\n\nAndy\n\n-- \nDr Andy Parkins\nandyparkins at gmail.com"
            },
            {
                "author": "Wladimir",
                "date": "2012-06-16T08:42:21",
                "message_text_only": "On Sat, Jun 16, 2012 at 10:16 AM, Andy Parkins <andyparkins at gmail.com>wrote:\n\n>\n> It's less of a problem in a (nearly) stateless protocol like Bitcoin.\n>\n\nIt's currently (nearly) stateless, however it would be short-sighted to\nthink it will stay that way. State is being introduced as we speak; for\nexample, connection-specific filters.\n\nI like the idea of a capabilities command; as time goes on and the ecosystem\n> of thin/spv/semi-thin/headers-only/blocks-on-demand/reverse-search-\n> blockchain/memory-pool-query clients becomes more varied, it's going to be\n> more an more important.  The particular example that occurs is thin clients\n> connecting to the network are going to want to ensure they are connected to\n> at least one non-thin client.\n>\n\nWhich is a perfectly reasonable requirement. However, one could simply\nstandardize what a 'thin client' and what a 'thick client' does and offers\n(at a certain version level), without having to explicitly enumerate\neverything over the protocol.\n\nThis also makes it easier to deprecate (lack of) certain features later on.\nYou can simply drop support for protocol versions before a certain number\n(which has happened before). With the extension system this is much harder,\nwhich likely means you keep certain workarounds forever.\n\nLetting the node know of each others capabilities at connection time helps\nsomewhat. It'd allow refusing clients that do not implement a certain\nfeature. Then again, to me it's unclear what this wins compared to\nincremental protocol versions with clear requirements.\n\nI'm just afraid that the currently simple P2P protocol will turn into a zoo\nof complicated (and potentially buggy/insecure) interactions.\n\nSo maybe a capability system is a good idea but then the granularity should\nbe large, not command-level. The interaction between protocol versions and\ncapabilities needs to be defined as well. Does offering \"getdata\" at\nprotocol version 10 mean the same as offering it at protocol version 11\"?\nProbably not guaranteed. The arguments might have changed. So it's not\nentirely self-documenting either.\n\nWladimir\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120616/2a6e50dc/attachment.html>"
            },
            {
                "author": "Amir Taaki",
                "date": "2012-06-16T09:16:46",
                "message_text_only": "> I'm just afraid that the currently simple P2P protocol will turn into a \nzoo of complicated (and potentially buggy/insecure) interactions. \n\n\nThis is my biggest fear too. I would rather be extremely conservative in making any changes to the protocol unless absolutely needed. That includes the bloom filters which take away the fact that Bitcoin is stateless.\n\nI was discussing this with another developer who mentioned something interesting: that always in the lifecycle of system's development, you see increasing complexity during its initial lifecycle as the field is being explored. At some later point, the technology matures and becomes standardised. At that point enough is known that the system snaps together and the cruft can be cut away to reduce the system down to core principles.\n\nIt's an interesting viewpoint to consider. I do however advise erring on the side of caution. Maybe there needs to a minimum schedule time before a new extension can be added to the protocol (except security fixes). If we're not careful, the protocol will become enormously huge and kludgy. However maybe as that developer pointed out, trying to stall the inevitable is slowing the long-term evolution of Bitcoin down.\n\n\n________________________________\nFrom: Wladimir <laanwj at gmail.com>\nTo: Andy Parkins <andyparkins at gmail.com> \nCc: bitcoin-development at lists.sourceforge.net \nSent: Saturday, June 16, 2012 10:42 AM\nSubject: Re: [Bitcoin-development] Proposed new P2P command and response: getcmds, cmdlist\n\n\nOn Sat, Jun 16, 2012 at 10:16 AM, Andy Parkins <andyparkins at gmail.com> wrote:\n\n\n>It's less of a problem in a (nearly) stateless protocol like Bitcoin.\n>\n\nIt's currently (nearly) stateless, however it would be short-sighted to think it will stay that way. State is being introduced as we speak; for example, connection-specific filters.\n\nI like the idea of a capabilities command; as time goes on and the ecosystem\n>of thin/spv/semi-thin/headers-only/blocks-on-demand/reverse-search-\n>blockchain/memory-pool-query clients becomes more varied, it's going to be\n>more an more important. \u00a0The particular example that occurs is thin clients\n>connecting to the network are going to want to ensure they are connected to\n>at least one non-thin client.\n>\n\nWhich is a perfectly reasonable requirement. However, one could simply standardize what a 'thin client' and what a 'thick client' does and offers (at a certain version level), without having to explicitly enumerate everything over the protocol.\u00a0\n\nThis also makes it easier to deprecate (lack of) certain features later on. You can simply drop support for protocol versions before a certain number (which has happened before). With the extension system this is much harder, which likely means you keep certain workarounds forever.\u00a0\n\nLetting the node know of each others capabilities at connection time helps somewhat. It'd allow refusing clients that do not implement a certain feature. Then again, to me it's unclear what this wins compared to incremental protocol versions with clear requirements.\u00a0\n\nI'm just afraid that the currently simple P2P protocol will turn into a zoo of complicated (and potentially buggy/insecure) interactions.\u00a0\n\nSo maybe a capability system is a good idea but then the granularity should be large, not command-level. The interaction between protocol versions and capabilities needs to be defined as well. Does offering \"getdata\" at protocol version 10 mean the same as offering it at protocol version 11\"? Probably not guaranteed. The arguments might have changed. So it's not entirely self-documenting either.\n\nWladimir\n\n------------------------------------------------------------------------------\nLive Security Virtual Conference\nExclusive live event will cover all the ways today's security and \nthreat landscape has changed and how IT managers can respond. Discussions \nwill include endpoint security, mobile security and the latest in malware \nthreats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Andy Parkins",
                "date": "2012-06-16T09:54:11",
                "message_text_only": "On Saturday 16 Jun 2012 09:42:21 Wladimir wrote:\n\n> Which is a perfectly reasonable requirement. However, one could simply\n> standardize what a 'thin client' and what a 'thick client' does and\n> offers (at a certain version level), without having to explicitly\n> enumerate everything over the protocol.\n\nMy problem is that that I suspect the spectrum of clients will be far more \nthan simply \"thin\" or \"thick\".  What about thick-pruned, thick-full?  What \nabout thin-blocks-on-demand and thin-headers-on-demand?  These are just what \nI can think of now; it seems unwise to limit the functionality of clients \nnot yet designed with a binary designation.  So... we make a field that can \nhold more than just a bit; with each possible value representing a specific \n(possibly overlapping) set of features?  Why not just enumerate the features \nthen?\n\nI did write responses to each of your following points; but they just \nsounded like me being contrary.  The short version is that I think too much \nemphasis is being placed on defining a specific set of feature->version \nmapping.  That's going to make it hard for future clients that want to \nimplement some of the features but not all, and yet still want to be good \nbitcoin citizens and be able to tell their peers what they don't support.  \nFor example, there is no easy way for a node to tell another that it doesn't \nhave the whole block chain available, so requesting it from it will fail. \n\n> I'm just afraid that the currently simple P2P protocol will turn into a\n> zoo of complicated (and potentially buggy/insecure) interactions.\n\nFair enough.\n\n> So maybe a capability system is a good idea but then the granularity\n> should be large, not command-level. The interaction between protocol\n> versions and capabilities needs to be defined as well. Does offering\n> \"getdata\" at protocol version 10 mean the same as offering it at\n> protocol version 11\"? Probably not guaranteed. The arguments might have\n> changed. So it's not entirely self-documenting either.\n\nThat problem doesn't go away just because you don't have a capabilities \nsystem.  Either version 11 can speak version 10 or it can't.  I don't see \nhow having a system for finding out that fact changes anything other than \nremoving a load of protocol noise.\n\n\"I support getdata10\" makes it far easier to discover that the peer supports \ngetdata10 than sending getdata11 and watching it fail does.\n\n\n\nAndy\n-- \nDr Andy Parkins\nandyparkins at gmail.com"
            },
            {
                "author": "Jeff Garzik",
                "date": "2012-06-17T15:19:53",
                "message_text_only": "On Sat, Jun 16, 2012 at 4:42 AM, Wladimir <laanwj at gmail.com> wrote:\n> Which is a perfectly reasonable requirement. However, one could simply\n> standardize what a 'thin client' and what a 'thick client' does and offers\n> (at a certain version level), without having to explicitly enumerate\n> everything over the protocol.\n>\n> This also makes it easier to deprecate (lack of) certain features later on.\n> You can simply drop support for protocol versions before a certain number\n> (which has happened before). With the extension system this is much harder,\n> which likely means you keep certain workarounds forever.\n>\n> Letting the node know of each others capabilities at connection time helps\n> somewhat. It'd allow refusing clients that do not implement a certain\n> feature. Then again, to me it's unclear what this wins compared to\n> incremental protocol versions with clear requirements.\n>\n> I'm just afraid that the currently simple P2P protocol will turn into a zoo\n> of complicated (and potentially buggy/insecure) interactions.\n\nWhat is missing here is some perspective on the current situation.  It\nis -very- easy to make a protocol change and bump PROTOCOL_VERSION in\nthe Satoshi client.\n\nBut for anyone maintaining a non-Satoshi codebase, the P2P protocol is\nalready filled with all sorts of magic numbers, arbitrarily versioned\nbinary data structures..  already an unfriendly zoo of complicated and\npotentially buggy interactions.  There is scant, incomplete\ndocumentation on the wiki -- the Satoshi source code is really the\nonly true reference.\n\nI see these problems personally, trying to keep ArtForz' half-a-node\nrunning on mainnet (distributed as 'blkmond' with pushpool).\n\nIn an era of HTTP and JSON, NFS and iSCSI, bitcoin's P2P protocol is\nwoefully backwards, fragile, limited and inflexible when it comes to\nparameter/extension exchange and negotiation.  Even iSCSI, that which\nis implemented on hard drive firmware, has the ability to exchange\nkey=value  parameters between local and remote sides of the RPC\nconnection.\n\nCalling the current P2P protocol \"simple\" belies all the\nimplementation details you absolutely -must- get right, to run on\nmainnet today.  Satoshi client devs almost never see the fragility and\ncomplexity inherent in the current legacy codebase, built up over\ntime.\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com"
            },
            {
                "author": "Amir Taaki",
                "date": "2012-06-17T16:30:41",
                "message_text_only": "As the only person to have created and maintaining a full reimplementation of the Bitcoin protocol/standard, I do think Bitcoin is filled with arbitrary endianness and magic numbers. However it is a tiny and simple protocol.\n\nThe big problem is not implementing the Bitcoin protocol, but the fact that once you have created a codebase, you want to perfect and fine-tune the design. During the meantime, the Bitcoin protocol is being changed. Change to the Bitcoin protocol is far more damaging to people that want to implement the protocol than any issues with the current protocol.\n\nThat's not to say, I disagree with changes to the protocol. I think changes should be a lot more conservative and have a longer time frame than they do currently. Usually changes suddenly get added to the Satoshi client and I notice them in the commit log or announcements. Then it's like \"oh I have to add this\" and I spend a week working to implement the change without proper consideration or reflection which ends up with me having to compromise on design choices. That is to remain compatible with the protocol.\n\nHowever it is not my intent to slow down progress so I usually try to hedge against that kind of feeling towards conservatism.\n\n\n\n----- Original Message -----\nFrom: Jeff Garzik <jgarzik at exmulti.com>\nTo: Wladimir <laanwj at gmail.com>\nCc: bitcoin-development at lists.sourceforge.net\nSent: Sunday, June 17, 2012 5:19 PM\nSubject: Re: [Bitcoin-development] Proposed new P2P command and response: getcmds, cmdlist\n\nOn Sat, Jun 16, 2012 at 4:42 AM, Wladimir <laanwj at gmail.com> wrote:\n> Which is a perfectly reasonable requirement. However, one could simply\n> standardize what a 'thin client' and what a 'thick client' does and offers\n> (at a certain version level), without having to explicitly enumerate\n> everything over the protocol.\n>\n> This also makes it easier to deprecate (lack of) certain features later on.\n> You can simply drop support for protocol versions before a certain number\n> (which has happened before). With the extension system this is much harder,\n> which likely means you keep certain workarounds forever.\n>\n> Letting the node know of each others capabilities at connection time helps\n> somewhat. It'd allow refusing clients that do not implement a certain\n> feature. Then again, to me it's unclear what this wins compared to\n> incremental protocol versions with clear requirements.\n>\n> I'm just afraid that the currently simple P2P protocol will turn into a zoo\n> of complicated (and potentially buggy/insecure) interactions.\n\nWhat is missing here is some perspective on the current situation.\u00a0 It\nis -very- easy to make a protocol change and bump PROTOCOL_VERSION in\nthe Satoshi client.\n\nBut for anyone maintaining a non-Satoshi codebase, the P2P protocol is\nalready filled with all sorts of magic numbers, arbitrarily versioned\nbinary data structures..\u00a0 already an unfriendly zoo of complicated and\npotentially buggy interactions.\u00a0 There is scant, incomplete\ndocumentation on the wiki -- the Satoshi source code is really the\nonly true reference.\n\nI see these problems personally, trying to keep ArtForz' half-a-node\nrunning on mainnet (distributed as 'blkmond' with pushpool).\n\nIn an era of HTTP and JSON, NFS and iSCSI, bitcoin's P2P protocol is\nwoefully backwards, fragile, limited and inflexible when it comes to\nparameter/extension exchange and negotiation.\u00a0 Even iSCSI, that which\nis implemented on hard drive firmware, has the ability to exchange\nkey=value\u00a0 parameters between local and remote sides of the RPC\nconnection.\n\nCalling the current P2P protocol \"simple\" belies all the\nimplementation details you absolutely -must- get right, to run on\nmainnet today.\u00a0 Satoshi client devs almost never see the fragility and\ncomplexity inherent in the current legacy codebase, built up over\ntime.\n\n-- \nJeff Garzik\nexMULTI, Inc.\njgarzik at exmulti.com\n\n------------------------------------------------------------------------------\nLive Security Virtual Conference\nExclusive live event will cover all the ways today's security and \nthreat landscape has changed and how IT managers can respond. Discussions \nwill include endpoint security, mobile security and the latest in malware \nthreats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n_______________________________________________\nBitcoin-development mailing list\nBitcoin-development at lists.sourceforge.net\nhttps://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2012-06-18T01:27:39",
                "message_text_only": "Sorry for the duplication Amir, I meant to send this to everyone:\n\nBitTorrent might be an example to look to here. It's a peer-to-peer network\nthat has undergone many significant protocol upgrades over the years while\nmaintaining compatibility. More recent clients have had the ability to\nexpose the capabilities of connected peers and modify behavior accordingly,\nand overall it has worked very well.\n\nCapability-based systems do work, and provide an excellent means of trying\nout new algorithms, adding new features for upgraded clients, and when\nnecessary reverting protocol changes (by depreciating or removing\nextensions).\n\nThe problem with OpenGL was and continues to be that the two superpowers of\nthat industry develop and maintain competing proposals for similar\nfunctionality, which are thrust upon developers which must support both if\nthey want access to the latest and greatest features, until such time that\nthe ARB arbitrarily choses one to standardize upon (in the process creating\nyet another extension of the form ARB_* that may be different and must be\nexplicitly supported by developers).\n\nI think the BitTorrent example shows that a loosely organized, open-source\ncommunity *can* maintain a capability-based extension system without\nfalling into capability-hell.\n\nMark\n\nOn Sun, Jun 17, 2012 at 9:30 AM, Amir Taaki <zgenjix at yahoo.com> wrote:\n\n> As the only person to have created and maintaining a full reimplementation\n> of the Bitcoin protocol/standard, I do think Bitcoin is filled with\n> arbitrary endianness and magic numbers. However it is a tiny and simple\n> protocol.\n>\n> The big problem is not implementing the Bitcoin protocol, but the fact\n> that once you have created a codebase, you want to perfect and fine-tune\n> the design. During the meantime, the Bitcoin protocol is being changed.\n> Change to the Bitcoin protocol is far more damaging to people that want to\n> implement the protocol than any issues with the current protocol.\n>\n> That's not to say, I disagree with changes to the protocol. I think\n> changes should be a lot more conservative and have a longer time frame than\n> they do currently. Usually changes suddenly get added to the Satoshi client\n> and I notice them in the commit log or announcements. Then it's like \"oh I\n> have to add this\" and I spend a week working to implement the change\n> without proper consideration or reflection which ends up with me having to\n> compromise on design choices. That is to remain compatible with the\n> protocol.\n>\n> However it is not my intent to slow down progress so I usually try to\n> hedge against that kind of feeling towards conservatism.\n>\n>\n>\n> ----- Original Message -----\n> From: Jeff Garzik <jgarzik at exmulti.com>\n> To: Wladimir <laanwj at gmail.com>\n> Cc: bitcoin-development at lists.sourceforge.net\n> Sent: Sunday, June 17, 2012 5:19 PM\n> Subject: Re: [Bitcoin-development] Proposed new P2P command and response:\n> getcmds, cmdlist\n>\n> On Sat, Jun 16, 2012 at 4:42 AM, Wladimir <laanwj at gmail.com> wrote:\n> > Which is a perfectly reasonable requirement. However, one could simply\n> > standardize what a 'thin client' and what a 'thick client' does and\n> offers\n> > (at a certain version level), without having to explicitly enumerate\n> > everything over the protocol.\n> >\n> > This also makes it easier to deprecate (lack of) certain features later\n> on.\n> > You can simply drop support for protocol versions before a certain number\n> > (which has happened before). With the extension system this is much\n> harder,\n> > which likely means you keep certain workarounds forever.\n> >\n> > Letting the node know of each others capabilities at connection time\n> helps\n> > somewhat. It'd allow refusing clients that do not implement a certain\n> > feature. Then again, to me it's unclear what this wins compared to\n> > incremental protocol versions with clear requirements.\n> >\n> > I'm just afraid that the currently simple P2P protocol will turn into a\n> zoo\n> > of complicated (and potentially buggy/insecure) interactions.\n>\n> What is missing here is some perspective on the current situation.  It\n> is -very- easy to make a protocol change and bump PROTOCOL_VERSION in\n> the Satoshi client.\n>\n> But for anyone maintaining a non-Satoshi codebase, the P2P protocol is\n> already filled with all sorts of magic numbers, arbitrarily versioned\n> binary data structures..  already an unfriendly zoo of complicated and\n> potentially buggy interactions.  There is scant, incomplete\n> documentation on the wiki -- the Satoshi source code is really the\n> only true reference.\n>\n> I see these problems personally, trying to keep ArtForz' half-a-node\n> running on mainnet (distributed as 'blkmond' with pushpool).\n>\n> In an era of HTTP and JSON, NFS and iSCSI, bitcoin's P2P protocol is\n> woefully backwards, fragile, limited and inflexible when it comes to\n> parameter/extension exchange and negotiation.  Even iSCSI, that which\n> is implemented on hard drive firmware, has the ability to exchange\n> key=value  parameters between local and remote sides of the RPC\n> connection.\n>\n> Calling the current P2P protocol \"simple\" belies all the\n> implementation details you absolutely -must- get right, to run on\n> mainnet today.  Satoshi client devs almost never see the fragility and\n> complexity inherent in the current legacy codebase, built up over\n> time.\n>\n> --\n> Jeff Garzik\n> exMULTI, Inc.\n> jgarzik at exmulti.com\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/37f28afb/attachment.html>"
            },
            {
                "author": "Andy Parkins",
                "date": "2012-06-16T08:17:39",
                "message_text_only": "On Saturday 16 Jun 2012 02:34:53 Amir Taaki wrote:\n> Introspection/command discovery is nice, but I would prefer it to be\n> immediately done in the first version exchange so no assumptions as to\n> how a network is operating need to be made.\n\nThat would need a change of the current version message.  So why not make \nthe change be simply: one of the service bits indicates that \"getcmds\" is \navailable?\n\nThen the version message doesn't need any on-the-wire change.\n\n\n\nAndy\n\n-- \nDr Andy Parkins\nandyparkins at gmail.com"
            }
        ],
        "thread_summary": {
            "title": "Proposed new P2P command and response: getcmds, cmdlist",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Jeff Garzik",
                "Wladimir",
                "Amir Taaki",
                "Andy Parkins",
                "Mark Friedenbach"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 30237
        }
    },
    {
        "title": "[Bitcoin-development] After compressed pubkeys: hybrid pubkeys",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2012-06-16T19:26:52",
                "message_text_only": "Hello all,\n\nwhile OpenSSL's silent support for compressed public keys allowed us to\nenable them in a fully backward-compatible way, it seems OpenSSL supports yet\nanother (and non-standard, and apparently useless) encoding for public keys.\n\nAs these are supported by (almost all?) fully validating clients on the\nnetwork, I believe alternative implementations should be willing to handle\nthem as well. No hybrid keys are used in the main chain, but I did test them\nin testnet3, and they work as expected.\n\nIn total, the following encodings exist:\n* 0x00: point at infinity; not a valid public key\n* 0x02 [32-byte X coord]: compressed format for even Y coords\n* 0x03 [32-byte X coord]: compressed format for odd Y coords\n* 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format\n* 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y coords\n* 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y coords\n\nHandling them is trivial: if you see a public key starting with a 0x06 or\n0x07, use it as if there was a 0x04 instead.\n\nI suppose we could decide to forbid these after a certain date/block height,\nand try to get sufficient mining power to enforce that before that date.\nAny opinions? Forbidding it certainly makes alternative implementation\nslightly easier in the future, but I'm not sure the hassle of a network\nrule change is worth it.\n\n-- \nPieter"
            },
            {
                "author": "Gavin Andresen",
                "date": "2012-06-16T21:41:52",
                "message_text_only": "RE: 0x06/0x07 'hybrid' public keys:\n\n> Any opinions? Forbidding it certainly makes alternative implementation\n> slightly easier in the future, but I'm not sure the hassle of a network\n> rule change is worth it.\n\nI say treat any transactions that use them as 'non-standard' -- don't\nrelay/mine them by default, but accept blocks that happen to contain\nthem.\n\nI agree that a rule change isn't worth it right now, but making them\nnon-standard now is easy and should make a rule change in the future\neasier.\n\n-- \n--\nGavin Andresen"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-16T23:39:00",
                "message_text_only": "On Sat, Jun 16, 2012 at 5:41 PM, Gavin Andresen <gavinandresen at gmail.com> wrote:\n> RE: 0x06/0x07 'hybrid' public keys:\n>\n>> Any opinions? Forbidding it certainly makes alternative implementation\n>> slightly easier in the future, but I'm not sure the hassle of a network\n>> rule change is worth it.\n>\n> I say treat any transactions that use them as 'non-standard' -- don't\n> relay/mine them by default, but accept blocks that happen to contain\n> them.\n>\n> I agree that a rule change isn't worth it right now, but making them\n> non-standard now is easy and should make a rule change in the future\n> easier.\n\nACK.  Hopefully no one will mine these before we can merge denying\nthem into another rule change. But if they do, oh well."
            },
            {
                "author": "Luke-Jr",
                "date": "2012-06-17T01:15:54",
                "message_text_only": "On Saturday, June 16, 2012 11:39:00 PM Gregory Maxwell wrote:\n> On Sat, Jun 16, 2012 at 5:41 PM, Gavin Andresen <gavinandresen at gmail.com> \nwrote:\n> > RE: 0x06/0x07 'hybrid' public keys:\n> >> Any opinions? Forbidding it certainly makes alternative implementation\n> >> slightly easier in the future, but I'm not sure the hassle of a network\n> >> rule change is worth it.\n> > \n> > I say treat any transactions that use them as 'non-standard' -- don't\n> > relay/mine them by default, but accept blocks that happen to contain\n> > them.\n> > \n> > I agree that a rule change isn't worth it right now, but making them\n> > non-standard now is easy and should make a rule change in the future\n> > easier.\n> \n> ACK.  Hopefully no one will mine these before we can merge denying\n> them into another rule change. But if they do, oh well.\n\nI'm willing to make Eligius reject these as well, if someone provides a patch \nthat doesn't depend on IsStandard being enforced...\n\nSame goes for rejecting OP_NOP<n> - I can't see any legitimate reason we'd \nwant these on mainnet right now.\n\nLuke"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-17T11:01:12",
                "message_text_only": "> * 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format\n> * 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y coords\n> * 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y coords\n\nSo what's the actual difference in format? Is there any at all, or\nit's just the first number that's different?"
            },
            {
                "author": "Pieter Wuille",
                "date": "2012-06-17T12:04:48",
                "message_text_only": "On Sun, Jun 17, 2012 at 01:01:12PM +0200, Mike Hearn wrote:\n> > * 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format\n> > * 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y coords\n> > * 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y coords\n> \n> So what's the actual difference in format? Is there any at all, or\n> it's just the first number that's different?\n\n>From what I understand, that is indeed the only difference.\n\n-- \nPieter"
            },
            {
                "author": "Wladimir",
                "date": "2012-06-17T15:16:13",
                "message_text_only": "On Sun, Jun 17, 2012 at 2:04 PM, Pieter Wuille <pieter.wuille at gmail.com>wrote:\n\n> On Sun, Jun 17, 2012 at 01:01:12PM +0200, Mike Hearn wrote:\n> > > * 0x04 [32-byte X coord] [32-byte Y coord]: uncompressed format\n> > > * 0x06 [32-byte X coord] [32-byte Y coord]: hybrid format for even Y\n> coords\n> > > * 0x07 [32-byte X coord] [32-byte Y coord]: hybrid format for odd Y\n> coords\n> >\n> > So what's the actual difference in format? Is there any at all, or\n> > it's just the first number that's different?\n>\n> >From what I understand, that is indeed the only difference.\n>\n>\nTo prevent surprises in the future, in case OpenSSL decides to add more,\ncan we disable all other key formats in advance?\n\nWladimir\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/09b5f3b1/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "After compressed pubkeys: hybrid pubkeys",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Luke-Jr",
                "Wladimir",
                "Mike Hearn",
                "Gregory Maxwell",
                "Gavin Andresen",
                "Pieter Wuille"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 5416
        }
    },
    {
        "title": "[Bitcoin-development] 0.6.x - detachdb in wrong place",
        "thread_messages": [
            {
                "author": "grarpamp",
                "date": "2012-06-17T09:22:20",
                "message_text_only": "Well, detachdb doesn't appear in the -\\? help\nbecause it's stuffed under pnp, which is not set\nin my build. please fix for people, tx :)\n\n#ifdef USE_UPNP\n#if USE_UPNP\n            \"  -upnp            \\t  \"   + _(\"Use Universal Plug and\nPlay to map the listening port (default: 1)\") + \"\\n\" +\n#else\n            \"  -upnp            \\t  \"   + _(\"Use Universal Plug and\nPlay to map the listening port (default: 0)\") + \"\\n\" +\n#endif\n            \"  -detachdb        \\t  \"   + _(\"Detach block and address\ndatabases. Increases shutdown time (default: 0)\") + \"\\n\" +\n#endif"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-17T10:17:04",
                "message_text_only": "On Sun, Jun 17, 2012 at 5:22 AM, grarpamp <grarpamp at gmail.com> wrote:\n> Well, detachdb doesn't appear in the -\\? help\n> because it's stuffed under pnp, which is not set\n> in my build. please fix for people, tx :)\n\nIt isn't inside the ifdef in bitcoin git master.\n\n(For future reference this sort of request is probably best opened as\nan issue in the github issue tracker instead of posted to the list)."
            },
            {
                "author": "grarpamp",
                "date": "2012-06-17T21:35:46",
                "message_text_only": "> It isn't inside the ifdef in bitcoin git master.\n\nOh, hmm, well then, what is the difference or usage\nbetween these two repositories in regards to the project?\n\nWhich one are the formal releases tagged (tbz's cut) in?\n\nWhich one has the branches with the commits that will\nmake it into the next formal release? ie: tracking along\n0.5.x, 0.6.x, HEAD/master (to be branched for 0.7.x).\n\nhttps://github.com/bitcoin/bitcoin\nhttps://git.gitorious.org/bitcoin/bitcoind-stable\n\nI seem to be seeing more tags in the former, and\nmore maintained branches in the latter?"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-17T21:52:18",
                "message_text_only": "On Sun, Jun 17, 2012 at 5:35 PM, grarpamp <grarpamp at gmail.com> wrote:\n>> It isn't inside the ifdef in bitcoin git master.\n>\n> Oh, hmm, well then, what is the difference or usage\n> between these two repositories in regards to the project?\n> Which one are the formal releases tagged (tbz's cut) in?\n>\n> Which one has the branches with the commits that will\n> make it into the next formal release? ie: tracking along\n> 0.5.x, 0.6.x, HEAD/master (to be branched for 0.7.x).\n>\n> https://github.com/bitcoin/bitcoin\n> https://git.gitorious.org/bitcoin/bitcoind-stable\n\nThe latter is Luke's backports of security and stability fixes to\notherwise unmaintained old versions."
            },
            {
                "author": "grarpamp",
                "date": "2012-06-17T23:04:42",
                "message_text_only": ">> https://github.com/bitcoin/bitcoin\n>> https://git.gitorious.org/bitcoin/bitcoind-stable\n>\n> The latter is Luke's backports of security and stability fixes to\n> otherwise unmaintained old versions.\n\nAh ok, coming from cvs/svn, it's a bit different to find things.\nThere's something to be said for maintenance of pior branches.\nThough I see some things I can use in github and my work would\nbe more useful there, so maybe I'll stwitch to that from gitorius/0.6.x.\n\nPresumably the github/0.6.2 branch is safe for production?\n\nWhat degree of caution about wallet eating should be\nmade for those using github/master?"
            },
            {
                "author": "Luke-Jr",
                "date": "2012-06-18T00:02:41",
                "message_text_only": "On Sunday, June 17, 2012 11:04:42 PM grarpamp wrote:\n> >> https://github.com/bitcoin/bitcoin\n> >> https://git.gitorious.org/bitcoin/bitcoind-stable\n> > \n> > The latter is Luke's backports of security and stability fixes to\n> > otherwise unmaintained old versions.\n> \n> Ah ok, coming from cvs/svn, it's a bit different to find things.\n> There's something to be said for maintenance of pior branches.\n> Though I see some things I can use in github and my work would\n> be more useful there, so maybe I'll stwitch to that from gitorius/0.6.x.\n> \n> Presumably the github/0.6.2 branch is safe for production?\n\nNo, that was a temporary branch of what became the stable 0.6.x branch.\nGitHub/master is bleeding edge. For production, you usually want the stable \nbranches/releases (which are on Gitorious).\n\nThe fix to -detachdb's position in -help was just merged to master, and should \nbe backported sometime in the next few days.\n\nLuke"
            },
            {
                "author": "grarpamp",
                "date": "2012-06-18T03:27:52",
                "message_text_only": "> be sure to have good backups that never touched the new code...\n> We have at various times had bugs in master that would corrupt\n> wallets\n\nSorry, that's to be expected, I shouldn't have asked.\n\n> It would be very helpful if anyone offering bitcoin services would\n> setup parallel toy versions of your sites on testnet...\n\nGood point.\n\n> we don't currently have enough testing activity on master.\n\nI usually test compile / report current and stable of things I use.\n\n\nSo I get that github/master is the obvious top of things.\nBut in looking at where the tags are between repositories,\nit's still not clear to me what the workflow is.\n\nExample...\n\nThere are these release tarballs on sourceforge, which all have\ntags in github, yet no tags in gitorious. There are no 'x' branches\non github, yet there is one release applicable branch on gitorious.\n\nI guess I'd expect to see, that if as hinted by Luke that gitorious\nhas the stable trees, that there would be release tags there, laid\ndown at some comfy point in time on the 'x' stable branches there.\n(The stable branches inheriting new code from master). But there\nare no such tags.\n\nAnd the releases/tags seem to magically appear from nowhere on\ngithub :) Again, I'm trying to extricate myself from CVS here.\n\n\n# sourceforge tarballs\n0.6.0\n0.6.1\n0.6.2\n0.6.2.2\n\n# github branches\n  remotes/origin/master        432d28d Merge pull request #1477 from\ngmaxwell/master\n  remotes/origin/0.6.2         40fd689 Bump version to 0.6.2.2 for\nosx-special build\n# github tags\nv0.6.0\nv0.6.1\nv0.6.2\nv0.6.2.1\nv0.6.2.2\n\n# gitorius branches\n  remotes/origin/0.6.0.x d354f94 Merge branch '0.5.x' into 0.6.0.x\n  remotes/origin/0.6.x   5e322a7 Merge branch '0.6.0.x' into 0.6.x\n# gitorious tags\nv0.6.0.7"
            },
            {
                "author": "Luke-Jr",
                "date": "2012-06-18T03:57:11",
                "message_text_only": "On Monday, June 18, 2012 3:27:52 AM grarpamp wrote:\n> So I get that github/master is the obvious top of things.\n> But in looking at where the tags are between repositories,\n> it's still not clear to me what the workflow is.\n\nWorkflow is all new development takes place in master during release windows. \nEventually, those windows close and master is cleaned up and bugfix'd for the \nnext 0.x release. Occasionally, when 0.N.0 has some problem before the next \nrelease window opens, Gavin will use it to roll a 0.N.1 (and recently even a \n0.N.2 and 0.N.2.2). Once the release window for the next 0.N version opens,\nI import the (last bugfix-only commit after the final 0.N.M release made in \nmaster) into the stable repository as the 0.N.x branch, and begin applying \nbackports. When there's significant backports, I'll tag another 0.N.M from the \nbranch and possibly release Windows binaries. Usually this happens around the \nsame time as master becomes the next 0.N.0 release.\n\n> Example...\n> \n> There are these release tarballs on sourceforge, which all have\n> tags in github, yet no tags in gitorious. There are no 'x' branches\n> on github, yet there is one release applicable branch on gitorious.\n> \n> I guess I'd expect to see, that if as hinted by Luke that gitorious\n> has the stable trees, that there would be release tags there, laid\n> down at some comfy point in time on the 'x' stable branches there.\n> (The stable branches inheriting new code from master). But there\n> are no such tags.\n\nI guess I've been neglecting to update the stable repo with releases tagged in \nmaster. It should be fixed now.\n\nLuke"
            },
            {
                "author": "grarpamp",
                "date": "2012-06-18T08:09:56",
                "message_text_only": "> Workflow is ...\n\nThanks very much, I think that helps me/others. I did not realize\nthere were release windows in master and thought it more as the\ntypical full time dev slush. That also explains the presence of all\nthe release tags in github repo. And even, in a divergent way, the\npresence of github/0.6.2 as path to gitorius/0.6.x. And I agree\nwith the (last bugfix after release) -> import/maintain model, it\nwould be similar in solo repo.\n\n> I guess I've been neglecting to update the stable repo with\n> releases tagged in master. It should be fixed now.\n\nYes, that has helped! Now git'ers can easily compare the release\ntags to stable 'x' branches on gitorious. I don't know how to do\nthat across repos yet, save manuel diff of checkouts from each,\nwhich would have been required prior to this update you made.\n\nAlso, these declarations of defunctness help sort out too.\n\n# git branch -vv -a\n\"This stable branch is no longer maintained.\"\n\n\nOk, so for my works I will now track github/master (edge) and\ngitorious/0.bigN(eg: 6).x (stable) against gitorious/bigTagRelease\n(latest public). Thanks guys, and Luke :)\n\nI hope other with similar questions find this thread. Apology for\nsubverting its subject somehows."
            },
            {
                "author": "Luke-Jr",
                "date": "2012-06-18T13:25:21",
                "message_text_only": "On Monday, June 18, 2012 8:09:56 AM grarpamp wrote:\n> > I guess I've been neglecting to update the stable repo with\n> > releases tagged in master. It should be fixed now.\n> \n> Yes, that has helped! Now git'ers can easily compare the release\n> tags to stable 'x' branches on gitorious. I don't know how to do\n> that across repos yet, save manuel diff of checkouts from each,\n> which would have been required prior to this update you made.\n\nThis is the work model I use:\n  git clone git://github.com/bitcoin/bitcoin.git\n  cd bitcoin\n  git remote add stable git://gitorious.org/+bitcoin-stable-developers/bitcoin/bitcoind-stable.git\n  git remote add personal git at github.com:YOURNAME/bitcoin.git\n\nWith this, you can use \"git fetch --all\" to update your copy of the remote\nbranches, and access them as \"origin/master\", \"stable/0.6.x\", etc; and push\npersonal branches using \"git push personal <branch>\"\n\nLuke"
            },
            {
                "author": "grarpamp",
                "date": "2012-06-20T09:06:53",
                "message_text_only": "> This is the work model I use:\n\nWill try all these things out this weekend. Thanks."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-18T00:07:45",
                "message_text_only": "On Sun, Jun 17, 2012 at 7:04 PM, grarpamp <grarpamp at gmail.com> wrote:\n> Presumably the github/0.6.2 branch is safe for production?\n\n0.6.2 is very widely used, more so than the other acceptably updated backports.\n\n> What degree of caution about wallet eating should be\n> made for those using github/master?\n\nI can't speak for anyone but myself:\n\nI don't run master on wallets with large amounts of (non-testnet) coin\nin them, except for a few times when I needed access to this feature\nor that or just in a isolated capacity for testing.  In any use with\nreal wallets I'd be sure to have good backups that never touched the\nnew code.\n\nWe have at various times had bugs in master that would corrupt wallets\n(though IIRC not too severely) and have bugs that would burn coin both\nin mining and in transactions (though again, I think not too\nseverely).  My caution is not due to the risk being exceptionally\ngreat but just because there is probably no remedy if things go wrong,\nthis caution is magnified by the fact that we don't currently have\nenough testing activity on master.\n\nTestnet exists so that people can test without fear of losing a lot of\nfunds and with the 0.7.0(git master) testnet reboot it should be more\nusable than it has been.   It would be very helpful if anyone offering\nbitcoin services would setup parallel toy versions of your sites on\ntestnet\u2014 it would bring more attention to your real services, it would\ngive you an opportunity to get more testing done of your real\nservices, it would show some more commitment to software quality, and\nit would let you take a more active role in advancing bitcoin\ndevelopment by doing a little testing yourself that you couldn't do on\nyour production systems."
            }
        ],
        "thread_summary": {
            "title": "0.6.x - detachdb in wrong place",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Luke-Jr",
                "Gregory Maxwell",
                "grarpamp"
            ],
            "messages_count": 12,
            "total_messages_chars_count": 11014
        }
    },
    {
        "title": "[Bitcoin-development] Ultimate Blockchain Compression w/ trust-free lite nodes",
        "thread_messages": [
            {
                "author": "Alan Reiner",
                "date": "2012-06-17T18:39:28",
                "message_text_only": "All,\n\nWith the flurry of discussion about blockchain compression, I thought it \nwas time to put forward my final, most-advanced idea, into a single, \nwell-thought-out, *illustrated*, forum post.     Please check it out: \nhttps://bitcointalk.org/index.php?topic=88208.0\n\nThis is a huge undertaking, but it has some pretty huge benefits.  And \nit's actually feasible because it can be implemented without disrupting \nthe main network.  I'm sure there's lots of issues with it, but I'm \nputting it out there to see how it might be improved and actually executed.\n\n----\n*Summary:\n\n*/Use a special tree data structure to organize all unspent-TxOuts on \nthe network, and use the root of this tree to communicate its \n\"signature\" between nodes.  The leaves of this tree actually correspond \nto addresses/scripts, and the data at the leaf is actually a root of the \nunspent-TxOut list for that address/script.  To maintain security of the \ntree signatures, it will be included in the header of an alternate \nblockchain, which will be secured by merged mining.\n\nThis provides the same compression as the simpler unspent-TxOut merkle \ntree, but also gives nodes a way to download just the unspent-TxOut list \nfor each address in their wallet, and verify that list directly against \nthe blockheaders.  Therefore, even lightweight nodes can get full \naddress information, from any untrusted peer, and with only a tiny \namount of downloaded data (a few kB). /*\n*----\n\nAlright, tear it up!\n-Alan\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/c6a13a8c/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2012-06-17T19:05:11",
                "message_text_only": "On Sun, Jun 17, 2012 at 02:39:28PM -0400, Alan Reiner wrote:\n> All,\n> \n> With the flurry of discussion about blockchain compression, I\n> thought it was time to put forward my final, most-advanced idea,\n> into a single, well-thought-out, *illustrated*, forum post.\n> Please check it out: https://bitcointalk.org/index.php?topic=88208.0\n> \n> This is a huge undertaking, but it has some pretty huge benefits.\n> And it's actually feasible because it can be implemented without\n> disrupting the main network.  I'm sure there's lots of issues with\n> it, but I'm putting it out there to see how it might be improved and\n> actually executed.\n> \n> ----\n> *Summary:\n> \n> */Use a special tree data structure to organize all unspent-TxOuts\n> on the network, and use the root of this tree to communicate its\n> \"signature\" between nodes.  The leaves of this tree actually\n> correspond to addresses/scripts, and the data at the leaf is\n> actually a root of the unspent-TxOut list for that address/script.\n> To maintain security of the tree signatures, it will be included in\n> the header of an alternate blockchain, which will be secured by\n> merged mining.\n> \n> This provides the same compression as the simpler unspent-TxOut\n> merkle tree, but also gives nodes a way to download just the\n> unspent-TxOut list for each address in their wallet, and verify that\n> list directly against the blockheaders.  Therefore, even lightweight\n> nodes can get full address information, from any untrusted peer, and\n> with only a tiny amount of downloaded data (a few kB). /*\n\nHow are you going to prevent people from delibrately unbalancing the\ntree with addresses with chosen hashes?\n\nOne idea that comes to mind, which unfortunately would make for a\npseudo-network rule, is to simply say that any *new* address whose hash\nhappens to be deeper in the tree than, say, 10*log(n), indicating it was\nprobably chosen to be unbalanced, gets discarded. The \"new address\" part\nof the rule would be required, or else you could use the rule to get\nother people's addresses discarded.\n\nHaving said that, such a rule just means that anyone playing games will\nfind they can't spend *their* money, and only with pruning clients.\nUnrelated people will not be effected. The coins can also always be\nspent with a non-pruning client to an acceptable address, which can\nlater re-spend on a pruning client.\n\n\nIt also comes to mind is that with the popularity of firstbits it may be\na good idea to use a comparison function that works last bit first...\n\n\nIt's merkles all the way down...\n\n-- \n'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120617/f3a21565/attachment.sig>"
            },
            {
                "author": "Alberto Torres",
                "date": "2012-06-17T22:46:47",
                "message_text_only": "Hi,\n\nI did describe a very similar thing back in January (also illustrated,\nand, if I'm not mistaken, more simple and efficient to recalculate),\nand I wanted to do a prototype, but I have been very busy with other\nprojects since then.\n\nhttps://en.bitcoin.it/wiki/User:DiThi/MTUT\n\nI just saw Gavin left a comment in the talk page, I'm sorry I haven't\nseen it earlier.\n\nI think armory is the perfect client to implement such an idea. I sort\nof waited it to be able to run in my laptop with 2 GB of RAM before\nbeing sucked into other projects. I even lost track of its\ndevelopment.\n\nI hope this gets developed. I will be able to help after summer if\nthis is still not done.\n\nDiThi\n\nP.S: Sorry Peter, I've sent you the message privately by mistake.\nAlso, I don't quite understand your concern of \"unbalancing\" the tree.\n\n2012/6/17 Peter Todd <pete at petertodd.org>:\n> On Sun, Jun 17, 2012 at 02:39:28PM -0400, Alan Reiner wrote:\n>> All,\n>>\n>> With the flurry of discussion about blockchain compression, I\n>> thought it was time to put forward my final, most-advanced idea,\n>> into a single, well-thought-out, *illustrated*, forum post.\n>> Please check it out: https://bitcointalk.org/index.php?topic=88208.0\n>>\n>> This is a huge undertaking, but it has some pretty huge benefits.\n>> And it's actually feasible because it can be implemented without\n>> disrupting the main network. \u00a0I'm sure there's lots of issues with\n>> it, but I'm putting it out there to see how it might be improved and\n>> actually executed.\n>>\n>> ----\n>> *Summary:\n>>\n>> */Use a special tree data structure to organize all unspent-TxOuts\n>> on the network, and use the root of this tree to communicate its\n>> \"signature\" between nodes. \u00a0The leaves of this tree actually\n>> correspond to addresses/scripts, and the data at the leaf is\n>> actually a root of the unspent-TxOut list for that address/script.\n>> To maintain security of the tree signatures, it will be included in\n>> the header of an alternate blockchain, which will be secured by\n>> merged mining.\n>>\n>> This provides the same compression as the simpler unspent-TxOut\n>> merkle tree, but also gives nodes a way to download just the\n>> unspent-TxOut list for each address in their wallet, and verify that\n>> list directly against the blockheaders. \u00a0Therefore, even lightweight\n>> nodes can get full address information, from any untrusted peer, and\n>> with only a tiny amount of downloaded data (a few kB). /*\n>\n> How are you going to prevent people from delibrately unbalancing the\n> tree with addresses with chosen hashes?\n>\n> One idea that comes to mind, which unfortunately would make for a\n> pseudo-network rule, is to simply say that any *new* address whose hash\n> happens to be deeper in the tree than, say, 10*log(n), indicating it was\n> probably chosen to be unbalanced, gets discarded. The \"new address\" part\n> of the rule would be required, or else you could use the rule to get\n> other people's addresses discarded.\n>\n> Having said that, such a rule just means that anyone playing games will\n> find they can't spend *their* money, and only with pruning clients.\n> Unrelated people will not be effected. The coins can also always be\n> spent with a non-pruning client to an acceptable address, which can\n> later re-spend on a pruning client.\n>\n>\n> It also comes to mind is that with the popularity of firstbits it may be\n> a good idea to use a comparison function that works last bit first...\n>\n>\n> It's merkles all the way down...\n>\n> --\n> 'peter'[:-1]@petertodd.org\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            },
            {
                "author": "Alan Reiner",
                "date": "2012-06-17T23:17:23",
                "message_text_only": "Hi Alberto,\n\nYour thread was part of the inspiration for the idea that I proposed.  \nBut as I read it more, I see that I originally misunderstood it \n(mistaking it for a simpler unspent-TxOut tree idea).  Even after \nreading it, I'm not entirely clear how your proposal would work, but I \nsee that you proposed something very similar.  I just want to clarify \nthat there are two, major orthogonal pieces to both proposals:\n\n(1) The method for creating unspent-TxOut-tree roots/fingerprints for \nverification\n(2) Using an alternate blockchain to maintain and distribute those \nfingerprints\n\nThere are multiple ways to do both of those.  You proposed a different \ntree structure (which I haven't entirely figured out, yet), and putting \nthose \"fingerprints\" in the main chain header.\n\nIn my proposal, (2) is to avoid inducing a blockchain fork, or even \nchanging the protocol at all.  By using a separate blockchain, it can be \ndone non-disruptively, and could even be thrown out and re-worked if we \nwere to find an issue with it later.  The availability of merged mining \nmakes it possible to get [almost] the same security as changing the \nprotocol, but without the disruption of hard-forking.  (I expect that if \nthere's not too much computational overhead and the software is already \nwritten, most miners would sign on)\n\nI'll read into your page a little more.  I don't want to take credit \naway from you, since you clearly had a comparable idea developed long \nbefore me :)\n\n-Alan\n\n\nOn 06/17/2012 06:46 PM, Alberto Torres wrote:\n> Hi,\n>\n> I did describe a very similar thing back in January (also illustrated,\n> and, if I'm not mistaken, more simple and efficient to recalculate),\n> and I wanted to do a prototype, but I have been very busy with other\n> projects since then.\n>\n> https://en.bitcoin.it/wiki/User:DiThi/MTUT\n>\n> I just saw Gavin left a comment in the talk page, I'm sorry I haven't\n> seen it earlier.\n>\n> I think armory is the perfect client to implement such an idea. I sort\n> of waited it to be able to run in my laptop with 2 GB of RAM before\n> being sucked into other projects. I even lost track of its\n> development.\n>\n> I hope this gets developed. I will be able to help after summer if\n> this is still not done.\n>\n> DiThi\n>\n> P.S: Sorry Peter, I've sent you the message privately by mistake.\n> Also, I don't quite understand your concern of \"unbalancing\" the tree.\n>\n> 2012/6/17 Peter Todd<pete at petertodd.org>:\n>> On Sun, Jun 17, 2012 at 02:39:28PM -0400, Alan Reiner wrote:\n>>> All,\n>>>\n>>> With the flurry of discussion about blockchain compression, I\n>>> thought it was time to put forward my final, most-advanced idea,\n>>> into a single, well-thought-out, *illustrated*, forum post.\n>>> Please check it out: https://bitcointalk.org/index.php?topic=88208.0\n>>>\n>>> This is a huge undertaking, but it has some pretty huge benefits.\n>>> And it's actually feasible because it can be implemented without\n>>> disrupting the main network.  I'm sure there's lots of issues with\n>>> it, but I'm putting it out there to see how it might be improved and\n>>> actually executed.\n>>>\n>>> ----\n>>> *Summary:\n>>>\n>>> */Use a special tree data structure to organize all unspent-TxOuts\n>>> on the network, and use the root of this tree to communicate its\n>>> \"signature\" between nodes.  The leaves of this tree actually\n>>> correspond to addresses/scripts, and the data at the leaf is\n>>> actually a root of the unspent-TxOut list for that address/script.\n>>> To maintain security of the tree signatures, it will be included in\n>>> the header of an alternate blockchain, which will be secured by\n>>> merged mining.\n>>>\n>>> This provides the same compression as the simpler unspent-TxOut\n>>> merkle tree, but also gives nodes a way to download just the\n>>> unspent-TxOut list for each address in their wallet, and verify that\n>>> list directly against the blockheaders.  Therefore, even lightweight\n>>> nodes can get full address information, from any untrusted peer, and\n>>> with only a tiny amount of downloaded data (a few kB). /*\n>> How are you going to prevent people from delibrately unbalancing the\n>> tree with addresses with chosen hashes?\n>>\n>> One idea that comes to mind, which unfortunately would make for a\n>> pseudo-network rule, is to simply say that any *new* address whose hash\n>> happens to be deeper in the tree than, say, 10*log(n), indicating it was\n>> probably chosen to be unbalanced, gets discarded. The \"new address\" part\n>> of the rule would be required, or else you could use the rule to get\n>> other people's addresses discarded.\n>>\n>> Having said that, such a rule just means that anyone playing games will\n>> find they can't spend *their* money, and only with pruning clients.\n>> Unrelated people will not be effected. The coins can also always be\n>> spent with a non-pruning client to an acceptable address, which can\n>> later re-spend on a pruning client.\n>>\n>>\n>> It also comes to mind is that with the popularity of firstbits it may be\n>> a good idea to use a comparison function that works last bit first...\n>>\n>>\n>> It's merkles all the way down...\n>>\n>> --\n>> 'peter'[:-1]@petertodd.org\n>>\n>> ------------------------------------------------------------------------------\n>> Live Security Virtual Conference\n>> Exclusive live event will cover all the ways today's security and\n>> threat landscape has changed and how IT managers can respond. Discussions\n>> will include endpoint security, mobile security and the latest in malware\n>> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n>> _______________________________________________\n>> Bitcoin-development mailing list\n>> Bitcoin-development at lists.sourceforge.net\n>> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Peter Todd",
                "date": "2012-06-18T10:14:41",
                "message_text_only": "On Mon, Jun 18, 2012 at 12:46:47AM +0200, Alberto Torres wrote:\n> Hi,\n> \n> I did describe a very similar thing back in January (also illustrated,\n> and, if I'm not mistaken, more simple and efficient to recalculate),\n> and I wanted to do a prototype, but I have been very busy with other\n> projects since then.\n> \n> https://en.bitcoin.it/wiki/User:DiThi/MTUT\n> \n> I just saw Gavin left a comment in the talk page, I'm sorry I haven't\n> seen it earlier.\n> \n> I think armory is the perfect client to implement such an idea. I sort\n> of waited it to be able to run in my laptop with 2 GB of RAM before\n> being sucked into other projects. I even lost track of its\n> development.\n\nI strongly disagree on that point. What you're proposing needs miner\nsupport to work, and miners generally run either the satoshi client as a\ndaemon, or some other custom code. Implementing the idea in armory\ndoesn't give those miners a nice upgrade path.\n\nThat said, *using* the hash tree is something that can be implemented in\nany client, but a lot of the code will be shared between calculating it\nand using it anyway, so again implementing in the satoshi client makes\nsense.\n\n> I hope this gets developed. I will be able to help after summer if\n> this is still not done.\n> \n> DiThi\n> \n> P.S: Sorry Peter, I've sent you the message privately by mistake.\n> Also, I don't quite understand your concern of \"unbalancing\" the tree.\n\nLets suppose we're trying to make a tree consisting of real numbers:\n\n    /\\\n   /  \\\n   *   \\\n  / \\   \\\n /   \\   \\\n *   *   *\n/ \\ / \\ / \\\n1 2 3 4 5 6\n\nIf the numbers are evenly distributed, as will happen with hashes of\narbitrary data, any number will be at most log(n) steps away from the\nhead of the tree.\n\nSuppose though some malicious actor adds the following numbers to that\ntree: 3.001 3.002 3.003\n\n    /\\\n   /  \\\n   *   \\\n  / \\   \\\n /   \\   \\\n *   *   *\n/ \\ / \\ / \\\n1 2 * 4 5 6\n   / \\\n   |  \\\n   *   *\n  / \\ / \\\n  0 1 2 3 <- (3.000 to 3.003)\n\nOoops, the tree suddenly became a lot higher, with an associated\ndecrease in retrieval performance and an increase in memory usage.\n\nOf course the exact details depend on what rules there are for\nconstructing the tree, but essentially the attacker can either force the\na big increase in the depth of the tree, or a large number of vertexes\nto be re-organizationed to create the tree, or both.\n\nNow, to be exact, since the key of each vertex is a transaction hash,\nthis malicious actor will need to brute chosen prefix hash collisions,\nbut this is bitcoin: the whole system is about efficiently brute forcing\nchosen prefix hash collisions. Besides, you would only need something\nlike k*n collisions to product an n increase in tree depth, with some\nsmall k.\n\n\nMy solution was to simply state that vertexes that happened to cause the\ntree to be unbalanced would be discarded, and set the depth of inbalance\nsuch that this would be extremely unlikely to happen by accident. I'd\nrather see someone come up with something better though.\n\n\nAnother naive option would be to hash each vertex key (the transaction\nhash) with a nonce known only to the creator of that particular merkle\ntree, but then the whole tree has to be recreatred from scratch each\ntime, which is worse than the problem... Interestingly in a\n*non-distributed* system this idea is actually quite feasible feasible,\nas the nonce could be kept secret.\n\n-- \n'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120618/36d8e486/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Ultimate Blockchain Compression w/ trust-free lite nodes",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Alberto Torres",
                "Alan Reiner",
                "Peter Todd"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 18668
        }
    },
    {
        "title": "[Bitcoin-development] Block preview for faster relaying",
        "thread_messages": [
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-18T00:24:27",
                "message_text_only": "Right now we're seeing cases where block propagation is sometimes\ntaking minutes.\nThis doesn't cause much of a problem for general Bitcoin users but for\nminers its problematic because it potentially increases the risk for\norphaning.\n\nThere are probably many contributing factors which can be improved\nhere but one of the most obvious is that nodes fully validate blocks\nbefore relaying them. The validation is IO intensive and can currently\ntake a minute alone on sufficiently slow nodes with sufficiently large\nblocks and larger blocks require more data to be transmitted.  Because\nthis slowness is proportional to the size of the block this risks\ncreating mismatched incentives where miners are better off not mining\n(many) transactions in order to maximize their income.\n\nThe validation speed can and should be improved but there is at least\none short term improvement that can be made at the protocol level:\nMake it possible to relay blocks to other nodes before fully\nvalidating them.\n\nThis can be reasonable secure because basic validation (such as the\ndifficulty, previous block identity, and timestamps) can be done first\nso an attacker would need to burn enormous amounts of computing power\njust to make very modest trouble with it... and it's a change which\nwould be beneficial even after any other performance improvements were\nmade.\n\nLuke has been working on a patch for this:\n\nhttps://github.com/luke-jr/bitcoin/commit/0ce6f590dc2b9cbb46ceecd7320220f55d814bca\n\nOne aspect of it that I wanted to see more comments on was the use of\na new message for the preview-blocks instead of just announcing them\nlike normal. The reason for this is two-fold: To prevent existing full\nnodes from blacklisting nodes sending a bad preview block due to the\nexisting misbehavior checks, otherwise an attacker could burn one\nblock to partition the network,  and also so that SPV nodes which\naren't able to fully validate the block themselves can opt-out or at\nleast know that the data is not yet validated by the peer.\n\nI don't see any better way to address this but I thought other people\nmight have comments."
            }
        ],
        "thread_summary": {
            "title": "Block preview for faster relaying",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Gregory Maxwell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2104
        }
    },
    {
        "title": "[Bitcoin-development] LevelDB benchmarking",
        "thread_messages": [
            {
                "author": "Mike Hearn",
                "date": "2012-06-18T18:41:43",
                "message_text_only": "I switched the transaction database to use the Google LevelDB library,\nwhich is a refactored out part of BigTable.\n\nHere are my results. All tests are done on this hard disk:\n\n  http://wdc.custhelp.com/app/answers/detail/a_id/1409/~/specifications-for-the-500-gb-caviar-blue-and-caviar-se16-serial-ata-drive\n\nwhich has an average 8.9msec seek time. It is a 6 core Ubuntu machine.\n\nI used -loadblock on a chain with with 185127 blocks in it, so it has\nlots of SatoshiDice traffic.\n\n8.9 ms (average) seek time\n\n>> Regular BDB as we have today:\nreal\t96m6.836s\nuser\t49m55.220s\nsys\t2m29.850s\n\nThroughput usually 4-5MB/sec according to iotop, pauses of 8-10\nseconds for \u201cFlushing wallet ...\u201d. 611mb of blkindex.dat\n\n>> BDB without sig checking\nThroughput, 12-17mb/sec\nreal\t42m51.508s\nuser\t11m52.700s\nsys\t2m36.590s\n\nDisabling EC verification halves running time.\n\n>> LevelDB no customized options\n(I ran the wrong time command here, hence the different format)\n3184.73user 181.02system 51:20.81elapsed 109%CPU (0avgtext+0avgdata\n1220096maxresident)k\n1104inputs+125851776outputs (293569major+37436202minor)pagefaults 0swaps\n\nSo, 50 minutes. Throughput often in range of 20-30mb/sec. 397MB of data files.\n\n>> LevelDB w/ 10 bit per key bloom filter\nreal\t50m52.740s\nuser\t53m38.870s\nsys\t3m4.990s\n\n424mb of data files\n\nNo change.\n\n>> LevelDB w/ 10 bit per key bloom filter + 30mb cache (up from 8mb by default)\nreal\t50m53.054s\nuser\t53m26.910s\nsys\t3m10.720s\n\nNo change. The reason is, signature checking is the bottleneck not IO.\n\n>> LevelDB w/10 bit per key bloom filter, 30mb cache, no sigs\nreal\t12m58.998s\nuser\t11m42.330s\nsys\t2m5.670s\n\n12 minutes vs 42 minutes for BDB on the same benchmark.\n\n\nConclusion: LevelDB is a clear win, taking a sync in the absence of\nnetwork delays from 95 minutes to 50, at which point signature\nchecking becomes the bottleneck. It is nearly 4x as fast when\nsignature checks are not done (ie, when receiving a block containing\nonly mempool transactions you already verified)."
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-19T09:05:20",
                "message_text_only": "+list\n\nOn Mon, Jun 18, 2012 at 9:07 PM, Gregory Maxwell <gmaxwell at gmail.com> wrote:\n> In addition to the ECDSA caching, \u00a0ECDSA can can easily be run on\n> multiple cores for basically a linear speedup.. so even with the\n> checking in place once ECDSA was using multiple threads we'd be back\n> to the DB being the bottleneck for this kind of case.\n\nMaybe ... looking again I think I may be wrong about being IO bound in\nthe last benchmark. The core running the main Bitcoin thread is still\npegged and the LevelDB background thread is only spending around 20%\nof its time in iowait. An oprofile shows most of the time being spent\ninside a std::map.\n\nOK, to make progress on this work I need a few decisions (Gavin?)\n\n1) Shall we do it?\n\n2) LevelDB is obscure, new and has a very minimalist build system. It\nsupports \"make\" but not \"make install\", for example, and is unlikely\nto be packaged. It's also not very large. I suggest we just check the\nsource into the main Bitcoin tree and link it statically rather than\ncomplicate the build.\n\n3) As the DB format would change and a slow migration period\nnecessary, any other tweaks to db format we could make at the same\ntime? Right now the key/values are the same as before, though using\nsatoshi serialization for everything is a bit odd.\n\nWe'd need UI for migration as well."
            },
            {
                "author": "Pieter Wuille",
                "date": "2012-06-19T11:38:59",
                "message_text_only": "On Tue, Jun 19, 2012 at 11:05:20AM +0200, Mike Hearn wrote:\n> OK, to make progress on this work I need a few decisions (Gavin?)\n> \n> 1) Shall we do it?\n\nI'm all for moving away from BDB. It's a very good system for what it is\nintended for, but that is not how we use it. The fact that it is tied to\na database environment (but people want to copy the files themselves\nbetween systems), that is provides consistency in case of failures (but\nbecause we remove old log files, we still see very frequent corrupted\nsystems), the fact that its environments are sometimes not even forward-\ncompatible, ...\n\nAssuming LevelDB is an improvement in these areas as well as resulting in\na speed improvement, I like it.\n\n> 2) LevelDB is obscure, new and has a very minimalist build system. It\n> supports \"make\" but not \"make install\", for example, and is unlikely\n> to be packaged. It's also not very large. I suggest we just check the\n> source into the main Bitcoin tree and link it statically rather than\n> complicate the build.\n\nHow portable is LevelDB? How well tested is it? What compatibility\nguarantees exist between versions of the system?\n\nI don't mind including the source code; it doesn't seem particularly\nlarge, and the 2-clause BSD license shouldn't be a problem.\n\n> 3) As the DB format would change and a slow migration period\n> necessary, any other tweaks to db format we could make at the same\n> time? Right now the key/values are the same as before, though using\n> satoshi serialization for everything is a bit odd.\n> \n> We'd need UI for migration as well.\n\nJeff was working on splitting the database into several files earlier, and\nI'm working on the database/validation logic as well. Each of these will\nrequire a rebuild of the databases anyway. If possible, we should try to\nget them in a single release, so people only need to rebuild once. \n\nPS: can we see the code?\n\n-- \nPieter"
            },
            {
                "author": "Gavin Andresen",
                "date": "2012-06-19T15:05:21",
                "message_text_only": "> OK, to make progress on this work I need a few decisions (Gavin?)\n>\n> 1) Shall we do it?\n\nWhat problem does it solve?\n\nIf the problem it will solve is \"it will only take 4 hours to download\nthe entire blockchain next year instead of taking 16 hours\" then no, I\ndon't think we should do it, both 4 and 16 hours to get fully up and\nrunning is too long.\n\nIf the problem it will solve is the \"too easy to get a DB_RUNRECOVERY\nerror\" because bdb is fragile when it comes to its environment... then\nLevelDB looks very interesting.\n\nIf the problem is bdb is creaky and old and has obscure semantics and\na hard-to-work-with API, then yes, lets switch (I'm easily seduced by\na pretty API and blazing fast performance).\n\n> 2) LevelDB is obscure, new and has a very minimalist build system. It\n> supports \"make\" but not \"make install\", for example, and is unlikely\n> to be packaged. It's also not very large. I suggest we just check the\n> source into the main Bitcoin tree and link it statically rather than\n> complicate the build.\n\nAs long as it compiles and runs on mac/windows/linux that doesn't\nreally worry me. I just tried it, and it compiled quickly with no\ncomplaints on my mac.\n\nLack of infrastructure because it is new does worry me; for example,\ncould I rework bitcointools to read the LevelDB blockchain?  (are\nthere python bindings for LevelDB?)\n\n> 3) As the DB format would change and a slow migration period\n> necessary, any other tweaks to db format we could make at the same\n> time? Right now the key/values are the same as before, though using\n> satoshi serialization for everything is a bit odd.\n\nSatoshi rolled his own network serialization because he didn't trust\nexisting serialization solutions to be 100% secure against remote\nexploits. Then it made sense to use the same solution for disk\nserialization; I don't see a compelling reason to switch to some other\nserialization scheme.\n\nModifying the database schema during migration to better support\napplications like InstaWallet (tens of thousands of separate wallets)\nor something like Pieter's ultra-pruning makes sense.\n\n-- \n--\nGavin Andresen"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-19T16:06:30",
                "message_text_only": "> What problem does it solve?\n\nPrimarily that block verification and therefore propagation is too\nslow because it's very CPU and IO intensive. The CPU work can be\nmulti-threaded. The IO work, not as much. As Bitcoin grows we need to\nscale the nodes. Eventually there may be multi-machine nodes, but for\nnow we can buy more time by making the existing nodes faster.\n\nI don't see this as a replacement for moving users to SPV clients.\nObviously, otherwise I would not be writing one ;)\n\n> If the problem it will solve is the \"too easy to get a DB_RUNRECOVERY\n> error\" because bdb is fragile when it comes to its environment... then\n> LevelDB looks very interesting.\n\nI have no experience with how robust LevelDB is. It has an API call to\ntry and repair the database and I know from experience that BigTable\nis pretty solid. But that doesn't mean LevelDB is.\n\n> If the problem is bdb is creaky and old and has obscure semantics and\n> a hard-to-work-with API, then yes, lets switch (I'm easily seduced by\n> a pretty API and blazing fast performance).\n\nThe code is a lot simpler for sure.\n\n> As long as it compiles and runs on mac/windows/linux that doesn't\n> really worry me.\n\nIt was refactored out of BigTable and made standalone for usage in\nChrome. Therefore it's as portable as Chrome is. Mac/Windows/Linux\nshould all work. Solaris, I believe, may need 64 bit binaries to avoid\nlow FD limits.\n\n> Lack of infrastructure because it is new does worry me; for example,\n> could I rework bitcointools to read the LevelDB blockchain? \u00a0(are\n> there python bindings for LevelDB?)\n\nYes: http://code.google.com/p/py-leveldb/\n\nFirst look at the code is here, but it's not ready for a pull req yet,\nand I'll force push over it a few times to get it into shape. So don't\nbranch:\n\nhttps://github.com/mikehearn/bitcoin/commit/2b601dd4a0093f834084241735d84d84e484f183\n\nIt has misc other changes I made whilst profiling, isn't well\ncommented enough, etc."
            },
            {
                "author": "Stefan Thomas",
                "date": "2012-06-19T19:22:15",
                "message_text_only": "Here are my 2 cents after using LevelDB as the default backend for\nBitcoinJS for about a year.\n\nLevelDB was written to power IndexedDB in Chrome which is a JavaScript\nAPI. That means that LevelDB doesn't really give you a lot of options,\nbecause they assume that on the C++ layer you don't know any more than\nthey do, because the actual application is on the JavaScript layer. For\nexample whereas BDB supports hashtables, b-trees, queues, etc., LevelDB\nuses one database type, LSM trees which is an ordered data structure\nthat is pretty good at everything.\n\nAnother gotcha was the number of file descriptors, LevelDB defaults to\n1000 per DB. We originally used multiple DBs, one for each of the\nindices, but it was easy enough to combine everything into one table,\nthereby solving the fd issue. (Lowering the file descriptor limit also\nworks of course, but if you lower it too much, LevelDB will start to\nspend a lot of time opening and closing files, so I believe combining\nyour tables into one is the better option.)\n\nOverall, LevelDB is a fantastic solution for desktop software that is\nfaced with multiple use cases that aren't known at compile time. It\nisn't really designed for something like Bitcoin which doesn't need\nordered access, has relatively predictable characteristics and - at\nleast some of the time - runs on servers.\n\nThat said, it does seem to work well for the Bitcoin use case anyway.\nThanks to the LSM trees, It's very quick at doing bulk inserts and we\ndon't seem to need any of the bells and whistles that BDB offers. So I\ncan't think of a reason not to switch, just make sure you all understand\nthe deal, LevelDB unlike Tokyo/Kyoto Cabinet is *not* intended as a\ncompetitor or replacement for BDB, it's something quite different.\n\n\n\nOn 6/19/2012 6:06 PM, Mike Hearn wrote:\n>> What problem does it solve?\n> Primarily that block verification and therefore propagation is too\n> slow because it's very CPU and IO intensive. The CPU work can be\n> multi-threaded. The IO work, not as much. As Bitcoin grows we need to\n> scale the nodes. Eventually there may be multi-machine nodes, but for\n> now we can buy more time by making the existing nodes faster.\n>\n> I don't see this as a replacement for moving users to SPV clients.\n> Obviously, otherwise I would not be writing one ;)\n>\n>> If the problem it will solve is the \"too easy to get a DB_RUNRECOVERY\n>> error\" because bdb is fragile when it comes to its environment... then\n>> LevelDB looks very interesting.\n> I have no experience with how robust LevelDB is. It has an API call to\n> try and repair the database and I know from experience that BigTable\n> is pretty solid. But that doesn't mean LevelDB is.\n>\n>> If the problem is bdb is creaky and old and has obscure semantics and\n>> a hard-to-work-with API, then yes, lets switch (I'm easily seduced by\n>> a pretty API and blazing fast performance).\n> The code is a lot simpler for sure.\n>\n>> As long as it compiles and runs on mac/windows/linux that doesn't\n>> really worry me.\n> It was refactored out of BigTable and made standalone for usage in\n> Chrome. Therefore it's as portable as Chrome is. Mac/Windows/Linux\n> should all work. Solaris, I believe, may need 64 bit binaries to avoid\n> low FD limits.\n>\n>> Lack of infrastructure because it is new does worry me; for example,\n>> could I rework bitcointools to read the LevelDB blockchain?  (are\n>> there python bindings for LevelDB?)\n> Yes: http://code.google.com/p/py-leveldb/\n>\n> First look at the code is here, but it's not ready for a pull req yet,\n> and I'll force push over it a few times to get it into shape. So don't\n> branch:\n>\n> https://github.com/mikehearn/bitcoin/commit/2b601dd4a0093f834084241735d84d84e484f183\n>\n> It has misc other changes I made whilst profiling, isn't well\n> commented enough, etc.\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and \n> threat landscape has changed and how IT managers can respond. Discussions \n> will include endpoint security, mobile security and the latest in malware \n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-20T09:44:48",
                "message_text_only": "Thanks, I didn't realize BitcoinJS used LevelDB already.\n\nJust one minor thing - LevelDB was definitely designed for servers, as\nit comes from BigTable. It happens to be used in Chrome today, and\nthat was the motivation for open sourcing it, but that's not where the\ndesign came from.\n\nIf anything it's going to get less and less optimal for desktops and\nlaptops over time because they're moving towards SSDs, where the\nminimal-seeks design of LevelDB doesn't necessarily help. Servers are\nmoving too of course but I anticipate most Bitcoin nodes on servers to\nbe HDD based for the forseeable future.\n\nAlso, Satoshis code does use ordered access/iteration in at least one\nplace, where it looks up the \"owner transactions\" of a tx. I'm not\ntotally sure what that code is used for, but it's there. Whether it's\nactually the best way to solve the problem is another question :-)"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-20T09:53:32",
                "message_text_only": "There's an interesting post here about block propagation times:\n\nhttps://bitcointalk.org/index.php?topic=88302.msg975343#msg975343\n\nLooks like the regular network is reliably 0-60 seconds behind p2pool\nin propagating new blocks.\n\nSo optimizing IO load (and after that, threading tx verification)\nseems like an important win. Lukes preview functionality would also be\nuseful."
            },
            {
                "author": "Pieter Wuille",
                "date": "2012-06-20T11:37:46",
                "message_text_only": "On Wed, Jun 20, 2012 at 11:44:48AM +0200, Mike Hearn wrote:\n> Also, Satoshis code does use ordered access/iteration in at least one\n> place, where it looks up the \"owner transactions\" of a tx. I'm not\n> totally sure what that code is used for, but it's there. Whether it's\n> actually the best way to solve the problem is another question :-)\n\nTwo days ago on #bitcoin-dev:\n21:01:19< sipa> what was CTxDB::ReadOwnerTxes ever used for?\n21:01:31< sipa> maybe it predates the wallet logic\n\n(read: it's not used anywhere in the code, and apparently wasn't ever, even in 0.1.5)\n\n-- \nPieter"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-20T12:41:30",
                "message_text_only": "> Two days ago on #bitcoin-dev:\n> 21:01:19< sipa> what was CTxDB::ReadOwnerTxes ever used for?\n> 21:01:31< sipa> maybe it predates the wallet logic\n>\n> (read: it's not used anywhere in the code, and apparently wasn't ever, even in 0.1.5)\n\nGreat, in that case Stefan is right and I'll delete that code when I\nnext work on the patch."
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-25T16:32:56",
                "message_text_only": "I've added some more commits:\n\nhttps://github.com/mikehearn/bitcoin/commits/leveldb\n\nIt's still not ready for a pull req but is a lot closer:\n\n1) Auto-migration is there but not well tested enough (I only tested\nwith empty wallets).\n2) Migration progress UI is there so you have something to watch for\nthe few minutes it takes. Script execution is disabled during\nmigration\n3) LevelDB source is checked in to the main tree, bitcoin-qt.pro\nupdated to use it\n4) LevelDB is conditionally compiled so if there's some unexpected\nissue or regression on some platform it can be switched back to BDB\n\nStill to go:\n\n1) More testing, eg, with actual wallets :-)\n2) Update the non-Qt makefiles\n3) On Windows it's currently de-activated due to some missing files\nfrom leveldb + I didn't test it\n\nIf you want to help out, some testing and makefile work would be\nuseful. I may not get a chance to work on this again until next week.\n\nOn Wed, Jun 20, 2012 at 2:41 PM, Mike Hearn <mike at plan99.net> wrote:\n>> Two days ago on #bitcoin-dev:\n>> 21:01:19< sipa> what was CTxDB::ReadOwnerTxes ever used for?\n>> 21:01:31< sipa> maybe it predates the wallet logic\n>>\n>> (read: it's not used anywhere in the code, and apparently wasn't ever, even in 0.1.5)\n>\n> Great, in that case Stefan is right and I'll delete that code when I\n> next work on the patch."
            }
        ],
        "thread_summary": {
            "title": "LevelDB benchmarking",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Gavin Andresen",
                "Mike Hearn",
                "Stefan Thomas",
                "Pieter Wuille"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 17147
        }
    },
    {
        "title": "[Bitcoin-development] Ultimate Blockchain Compression w/ trust-free lite node",
        "thread_messages": [
            {
                "author": "Andrew Miller",
                "date": "2012-06-19T16:46:52",
                "message_text_only": "> Peter Todd wrote:\n> My solution was to simply state that vertexes that happened to cause the\n> tree to be unbalanced would be discarded, and set the depth of inbalance\n> such that this would be extremely unlikely to happen by accident. I'd\n> rather see someone come up with something better though.\n\nHere is a simpler solution. (most of this message repeats the content\nof my reply to the forum)\n\nSuppose we were talking about a binary search tree, rather than a\nMerkle tree. It's important to balance a binary search tree, so that\nthe worst-case maximum length from the root to a leaf is bounded by\nO(log N). AVL trees were the original algorithm to do this, Red-Black\ntrees are also popular, and there are many similar methods. All\ninvolve storing some form of 'balancing metadata' at each node. In a\nRedBlack tree, this is a single bit (red or black). Every operation on\nthese trees, including search, inserting, deleting, and rebalancing,\nrequires a worst-case effort of O(log N).\n\nAny (acyclic) recursive data structure can be Merkle-ized, simply by\nadding a hash of the child node alongside each link/pointer. This way,\nyou can verify the data for each node very naturally, as you traverse\nthe structure.\n\nIn fact, as long as a lite-client knows the O(1) root hash, the rest\nof the storage burden can be delegated to an untrusted helper server.\nSuppose a lite-client wants to insert and rebalance its tree. This\nrequires accessing at most O(log N) nodes. The client can request only\nthe data relevant to these nodes, and it knows the hash for each chunk\nof data in advance of accessing it. After computing the updated root\nhash, the client can even discard the data it processed.\n\nThis technique has been well discussed in the academic literature,\ne.g. [1,2], although since I am not aware of any existing\nimplementation, I made my own, intended as an explanatory aid:\nhttps://github.com/amiller/redblackmerkle/blob/master/redblack.py\n\n\n[1] Certificate Revocation and Update\n    Naor and Nissim. 1998\n    http://static.usenix.org/publications/library/proceedings/sec98/full_papers/nissim/nissim.pdf\n\n[2] A General Model for Authenticated Data Structures\n    Martel, Nuckolls, Devanbu, Michael Gertz, Kwong, Stubblebine. 2004\n    http://truthsayer.cs.ucdavis.edu/algorithmica.pdf\n\n--\nAndrew Miller"
            },
            {
                "author": "Alan Reiner",
                "date": "2012-06-19T17:33:31",
                "message_text_only": "I hope that someone else here would chime in on the issue raised in the \nthread, about using a tree-structure that has multiple valid \nconfigurations for the same set of unspent-TxOuts.  If you use any \nbinary tree, you must replay the entire history of insertions and \ndeletions in the correct order to get the tree structure and correct \nroot.  Along those lines, using something like a red-black tree, while \ntheoretically well-known, could be subject to implementation errors.  \nOne implementation of a red-black tree may do the rebalancing \ndifferently, and still work for it's intended purpose in the majority of \napplications where it doesn't matter.  One app developer updates their \nRB tree code which updated the RB-tree optimizations/rebalancing, and \nnow a significant portion of the network can't agree on the correct \nroot.  Not only would that be disruptive, it would be a disaster to \ntrack down.\n\nIf we were to use a raw trie structure, then we'd have all the above \nissues solved:  a trie has the same configuration no matter how elements \nare inserted or deleted, and accesses to elements in the tree are \nconstant time -- O(1).  There is no such thing as an unbalanced trie.  \nBut overall space-efficiency is an issue.\n\nA PATRICIA tree/trie would be ideal, in my mind, as it also has a \ncompletely deterministic structure, and is an order-of-magnitude more \nspace-efficient.  Insert, delete and query times are still O(1).    \nHowever, it is not a trivial implementation.  I have occasionally looked \nfor implementations, but not found any that were satisfactory.\n\nSo, I don't have a good all-around solution, within my own stated \nconstraints. But perhaps I'm being too demanding of this solution.\n\n-Alan\n\n\n\nOn 06/19/2012 12:46 PM, Andrew Miller wrote:\n>> Peter Todd wrote:\n>> My solution was to simply state that vertexes that happened to cause the\n>> tree to be unbalanced would be discarded, and set the depth of inbalance\n>> such that this would be extremely unlikely to happen by accident. I'd\n>> rather see someone come up with something better though.\n> Here is a simpler solution. (most of this message repeats the content\n> of my reply to the forum)\n>\n> Suppose we were talking about a binary search tree, rather than a\n> Merkle tree. It's important to balance a binary search tree, so that\n> the worst-case maximum length from the root to a leaf is bounded by\n> O(log N). AVL trees were the original algorithm to do this, Red-Black\n> trees are also popular, and there are many similar methods. All\n> involve storing some form of 'balancing metadata' at each node. In a\n> RedBlack tree, this is a single bit (red or black). Every operation on\n> these trees, including search, inserting, deleting, and rebalancing,\n> requires a worst-case effort of O(log N).\n>\n> Any (acyclic) recursive data structure can be Merkle-ized, simply by\n> adding a hash of the child node alongside each link/pointer. This way,\n> you can verify the data for each node very naturally, as you traverse\n> the structure.\n>\n> In fact, as long as a lite-client knows the O(1) root hash, the rest\n> of the storage burden can be delegated to an untrusted helper server.\n> Suppose a lite-client wants to insert and rebalance its tree. This\n> requires accessing at most O(log N) nodes. The client can request only\n> the data relevant to these nodes, and it knows the hash for each chunk\n> of data in advance of accessing it. After computing the updated root\n> hash, the client can even discard the data it processed.\n>\n> This technique has been well discussed in the academic literature,\n> e.g. [1,2], although since I am not aware of any existing\n> implementation, I made my own, intended as an explanatory aid:\n> https://github.com/amiller/redblackmerkle/blob/master/redblack.py\n>\n>\n> [1] Certificate Revocation and Update\n>      Naor and Nissim. 1998\n>      http://static.usenix.org/publications/library/proceedings/sec98/full_papers/nissim/nissim.pdf\n>\n> [2] A General Model for Authenticated Data Structures\n>      Martel, Nuckolls, Devanbu, Michael Gertz, Kwong, Stubblebine. 2004\n>      http://truthsayer.cs.ucdavis.edu/algorithmica.pdf\n>\n> --\n> Andrew Miller\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-19T17:59:04",
                "message_text_only": "On Tue, Jun 19, 2012 at 1:33 PM, Alan Reiner <etotheipi at gmail.com> wrote:\n>\u00a0One app developer updates their\n> RB tree code which updated the RB-tree optimizations/rebalancing, and\n> now a significant portion of the network can't agree on the correct\n> root. \u00a0Not only would that be disruptive, it would be a disaster to\n> track down.\n\nThis is why good comprehensive tests and a well specified algorithim\nare important. The tree update algorithm would be normative in that\nscheme. Worrying that implementers might get it wrong would be like\nworrying that they'd get SHA256 wrong.\n\n> A PATRICIA tree/trie would be ideal, in my mind, as it also has a\n> completely deterministic structure, and is an order-of-magnitude more\n\nProvable libJudy trees. Oh boy."
            },
            {
                "author": "Alan Reiner",
                "date": "2012-06-19T18:12:19",
                "message_text_only": "On 06/19/2012 01:59 PM, Gregory Maxwell wrote:\n> On Tue, Jun 19, 2012 at 1:33 PM, Alan Reiner<etotheipi at gmail.com>  wrote:\n>>   One app developer updates their\n>> RB tree code which updated the RB-tree optimizations/rebalancing, and\n>> now a significant portion of the network can't agree on the correct\n>> root.  Not only would that be disruptive, it would be a disaster to\n>> track down.\n> This is why good comprehensive tests and a well specified algorithim\n> are important. The tree update algorithm would be normative in that\n> scheme. Worrying that implementers might get it wrong would be like\n> worrying that they'd get SHA256 wrong.\n\nThe point is not that they get it *wrong*, it's that the implement it \n*differently*.  Given a set of 100 TxOuts, there's a seemingly-infinite \nnumber of ways to construct a binary tree.  Put them in in a different \norder, and you get a different tree. *They're all correct and legal* in \nterms of satisfying expectations of insert, delete and query runtime -- \nbut they will produce different root hashes.   And the differences in \nunderlying structure are completely transparent to the calling code.\n\nI'm extremely uncomfortable with the idea the you can have all the nodes \nin the tree, but have to replay X years of blockchain history just to \nget the same tree configuration as someone else.  However, a trie \nconfiguration is history-independent -- given an unspent-TxOut list, \nthere's only one way to construct that tree.  That's an important \nproperty to me.\n\nI can't tell if you're joking about Judy structures: I've never heard of \nthem.  But I'll look into it anyway...\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120619/5426be15/attachment.html>"
            },
            {
                "author": "Mark Friedenbach",
                "date": "2012-06-19T18:18:07",
                "message_text_only": "On Tue, Jun 19, 2012 at 10:33 AM, Alan Reiner <etotheipi at gmail.com> wrote:\n\n> I hope that someone else here would chime in on the issue raised in the\n> thread, about using a tree-structure that has multiple valid\n> configurations for the same set of unspent-TxOuts.  If you use any\n> binary tree, you must replay the entire history of insertions and\n> deletions in the correct order to get the tree structure and correct\n> root.  Along those lines, using something like a red-black tree, while\n> theoretically well-known, could be subject to implementation errors.\n> One implementation of a red-black tree may do the rebalancing\n> differently, and still work for it's intended purpose in the majority of\n> applications where it doesn't matter.  One app developer updates their\n> RB tree code which updated the RB-tree optimizations/rebalancing, and\n> now a significant portion of the network can't agree on the correct\n> root.  Not only would that be disruptive, it would be a disaster to\n> track down.\n>\n\nThen use a 2-3-4 tree (aka self-balancing B-tree of order 4), which is a\ngeneralization of RB-trees that doesn't allow for implementation choices in\nbalancing (assuming ordered insertion and deletion).\n\nAs gmaxwell points out, this is an trivially fixable 'problem'. Choose a\nstandard, mandate it, and write test cases.\n\nIf we were to use a raw trie structure, then we'd have all the above\n> issues solved:  a trie has the same configuration no matter how elements\n> are inserted or deleted, and accesses to elements in the tree are\n> constant time -- O(1).  There is no such thing as an unbalanced trie.\n> But overall space-efficiency is an issue.\n>\n> A PATRICIA tree/trie would be ideal, in my mind, as it also has a\n> completely deterministic structure, and is an order-of-magnitude more\n> space-efficient.  Insert, delete and query times are still O(1).\n> However, it is not a trivial implementation.  I have occasionally looked\n> for implementations, but not found any that were satisfactory.\n>\n\nNo, a trie of any sort is dependent upon distribution of input data for\nbalancing. As Peter Todd points out, a malicious actor could construct\ntransaction or address hashes in such a way as to grow some segment of the\ntrie in an unbalanced fashion. It's not much of an attack, but in principle\nexploitable under particular timing-sensitive circumstances.\n\nSelf-balancing search trees (KVL, RB, 2-3-4, whatever) don't suffer from\nthis problem.\n\nMark\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120619/8d09b88b/attachment.html>"
            },
            {
                "author": "Alan Reiner",
                "date": "2012-06-19T18:30:16",
                "message_text_only": "On 06/19/2012 02:18 PM, Mark Friedenbach wrote:\n> On Tue, Jun 19, 2012 at 10:33 AM, Alan Reiner <etotheipi at gmail.com \n> <mailto:etotheipi at gmail.com>> wrote:\n>\n>     If we were to use a raw trie structure, then we'd have all the above\n>     issues solved:  a trie has the same configuration no matter how\n>     elements\n>     are inserted or deleted, and accesses to elements in the tree are\n>     constant time -- O(1).  There is no such thing as an unbalanced trie.\n>     But overall space-efficiency is an issue.\n>\n>     A PATRICIA tree/trie would be ideal, in my mind, as it also has a\n>     completely deterministic structure, and is an order-of-magnitude more\n>     space-efficient.  Insert, delete and query times are still O(1).\n>     However, it is not a trivial implementation.  I have occasionally\n>     looked\n>     for implementations, but not found any that were satisfactory.\n>\n>\n> No, a trie of any sort is dependent upon distribution of input data \n> for balancing. As Peter Todd points out, a malicious actor could \n> construct transaction or address hashes in such a way as to grow some \n> segment of the trie in an unbalanced fashion. It's not much of an \n> attack, but in principle exploitable under particular timing-sensitive \n> circumstances.\n>\n> Self-balancing search trees (KVL, RB, 2-3-4, whatever) don't suffer \n> from this problem.\n>\n> Mark\n\nI was using \"unbalanced\" to refer to \"query time\" (and also \ninsert/delete time).  If your trie nodes branch based on the next byte \nof your key hash, then the max depth of your trie is 32.  Period.  No \none can do anything to ever make you do more than 32 hops to \nfind/insert/delete your data.   And if you're using a raw trie, you'll \nalways use /exactly/ 32 hops regardless of the distribution of the \nunderlying data.  Hence, the trie structure is deterministic \n(history-independent) and cannot become unbalanced in terms of access time.\n\nMy first concern was that a malicious actor could linearize parts of the \ntree and cause access requests to take much longer than log(N) time.  \nWith the trie, that's not only impossible, you're actually accessing in \nO(1) time.\n\nHowever, you are right that disk space can be affected by a malicious \nactor.  The more branching he can induce, the more branch nodes that are \ncreated to support branches with only one leaf.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120619/2548286d/attachment.html>"
            },
            {
                "author": "Mike Koss",
                "date": "2012-06-21T21:42:58",
                "message_text_only": "Are we just talking about pruning the spent transactions from an old block?\n We already have a data structure that allows us to replace any un-needed\ntransaction by just it's hash - and possibly a whole sub-tree if we get\nlucky in that the un-needed transaction all fall within a common node of\nthe merkle tree.\n\nIf a lite client only cares to retain a single transaction in a block (the\nmost common case) - it will only need O(log2(T)) merkle hashes plus the\ntransaction it cares about.\n\nDoes it really make sense to adopt a more complex data-structure than the\nmerkle tree for inclusing in the bticoin protocol?  And we're not talking\nabout blocks with millions of transactions in them - I don't understand the\nrelevance of Order statistics for random access to a transaction given its\nblock.\n\nOn Tue, Jun 19, 2012 at 11:30 AM, Alan Reiner <etotheipi at gmail.com> wrote:\n\n>  On 06/19/2012 02:18 PM, Mark Friedenbach wrote:\n>\n> On Tue, Jun 19, 2012 at 10:33 AM, Alan Reiner <etotheipi at gmail.com> wrote:\n>\n>  If we were to use a raw trie structure, then we'd have all the above\n>> issues solved:  a trie has the same configuration no matter how elements\n>> are inserted or deleted, and accesses to elements in the tree are\n>> constant time -- O(1).  There is no such thing as an unbalanced trie.\n>> But overall space-efficiency is an issue.\n>>\n>> A PATRICIA tree/trie would be ideal, in my mind, as it also has a\n>> completely deterministic structure, and is an order-of-magnitude more\n>> space-efficient.  Insert, delete and query times are still O(1).\n>> However, it is not a trivial implementation.  I have occasionally looked\n>> for implementations, but not found any that were satisfactory.\n>>\n>\n>  No, a trie of any sort is dependent upon distribution of input data for\n> balancing. As Peter Todd points out, a malicious actor could construct\n> transaction or address hashes in such a way as to grow some segment of the\n> trie in an unbalanced fashion. It's not much of an attack, but in principle\n> exploitable under particular timing-sensitive circumstances.\n>\n>  Self-balancing search trees (KVL, RB, 2-3-4, whatever) don't suffer from\n> this problem.\n>\n>  Mark\n>\n>\n> I was using \"unbalanced\" to refer to \"query time\" (and also insert/delete\n> time).  If your trie nodes branch based on the next byte of your key hash,\n> then the max depth of your trie is 32.  Period.  No one can do anything to\n> ever make you do more than 32 hops to find/insert/delete your data.   And\n> if you're using a raw trie, you'll always use *exactly* 32 hops\n> regardless of the distribution of the underlying data.  Hence, the trie\n> structure is deterministic (history-independent) and cannot become\n> unbalanced in terms of access time.\n>\n> My first concern was that a malicious actor could linearize parts of the\n> tree and cause access requests to take much longer than log(N) time.  With\n> the trie, that's not only impossible, you're actually accessing in O(1)\n> time.\n>\n> However, you are right that disk space can be affected by a malicious\n> actor.  The more branching he can induce, the more branch nodes that are\n> created to support branches with only one leaf.\n>\n>\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and\n> threat landscape has changed and how IT managers can respond. Discussions\n> will include endpoint security, mobile security and the latest in malware\n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>\n>\n\n\n-- \nMike Koss\nCTO, CoinLab\n(425) 246-7701 (m)\n\nA Bitcoin Primer <http://coinlab.com/a-bitcoin-primer.pdf> - What you need\nto know about Bitcoins.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120621/84612184/attachment.html>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-21T22:02:27",
                "message_text_only": "On Thu, Jun 21, 2012 at 5:42 PM, Mike Koss <mike at coinlab.com> wrote:\n> Are we just talking about pruning the spent transactions from an old block?\n\nNo.\n\nWe're talking about commitments to the state of _unspent_ transactions\nwhich would allow ~memoryless nodes to engage in full validation\nwithout having to trust anything with the help of some untrusted\nnon-memoryless peers.  Also, talking about being able to securely\ninitialize new pruned nodes (not memoryless but reduced memory)\nwithout exposing them to the old history of the chain. In both cases\nthis is possible without substantially degrading the full node\nsecurity model (rule violations prior to where they begin are only\nundetectable with a conspiracy of the entire network).\n\nBut it requires a new data structure for managing these trees of\nunspent transactions in a secure, scalable, and DOS resistant manner.\nFortunately there are lots of possibilities here.\n\n> Does it really make sense to adopt a more complex data-structure than the merkle tree for inclusing in the bticoin protocol?\n\nYes. Though this is obviously not an ultra short term thing."
            },
            {
                "author": "Andrew Miller",
                "date": "2012-06-19T18:29:45",
                "message_text_only": "Alan Reiner wrote:\n> A PATRICIA tree/trie would be ideal, in my mind, as it also has a\n> completely deterministic structure, and is an order-of-magnitude more\n> space-efficient. \u00a0Insert, delete and query times are still O(1).\n> However, it is not a trivial implementation. \u00a0I have occasionally looked\n> for implementations, but not found any that were satisfactory.\n\nPATRICIA Tries (aka Radix trees) have worst-case O(k), where k is the\nnumber of bits in the key. Notice that since we would storing k-bit\nhashes, the number of elements must be less than 2^k, or else by\nbirthday paradox we would have a hash collision! So O(log N) <= O(k).\n\nYou're right, though, that such a trie would have the property that\nany two trees containing the same data (leaves) will be identical. I\ncan't think of any reason why this is useful, although I am hoping we\ncan figure out what is triggering your intuition to desire this! I am\nindeed assuming that the tree will be incrementally constructed\naccording to the canonical (blockchain) ordering of transactions, and\nthat the balancing rules are agreed on as part of the protocol.\n\n-- \nAndrew Miller"
            }
        ],
        "thread_summary": {
            "title": "Ultimate Blockchain Compression w/ trust-free lite node",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Andrew Miller",
                "Alan Reiner",
                "Gregory Maxwell",
                "Mark Friedenbach",
                "Mike Koss"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 21170
        }
    },
    {
        "title": "[Bitcoin-development] Berlin Bitcoin Hackathon",
        "thread_messages": [
            {
                "author": "Amir Taaki",
                "date": "2012-06-21T23:03:58",
                "message_text_only": "This is happening in Berlin if anyone is around: http://bitcoin-hackathon.com/\n\nI am happy to host if space is needed."
            },
            {
                "author": "Stefan Thomas",
                "date": "2012-06-22T06:39:24",
                "message_text_only": "Flights booked. Mike Hearn and I will be there. :)\n\nOn 6/22/2012 1:03 AM, Amir Taaki wrote:\n> This is happening in Berlin if anyone is around: http://bitcoin-hackathon.com/\n>\n> I am happy to host if space is needed.\n>\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and \n> threat landscape has changed and how IT managers can respond. Discussions \n> will include endpoint security, mobile security and the latest in malware \n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development\n>"
            }
        ],
        "thread_summary": {
            "title": "Berlin Bitcoin Hackathon",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Stefan Thomas",
                "Amir Taaki"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 946
        }
    },
    {
        "title": "[Bitcoin-development] Wallet related bug?",
        "thread_messages": [
            {
                "author": "grarpamp",
                "date": "2012-06-22T11:01:19",
                "message_text_only": "I think there may be an ideal order of ops bug around rescan,\nwallet upgrades/import and last block markers.\n\nI dropped an old wallet in a current blockchain.\n\nFirst ran - in rescan mode.\nIt said old walletver.\nThen rescanned whole chain.\nAddToWallet some blockhash, blocks out of range, invalid/nonwallet txid's,\nwhich were already in there as legit ones in the old logs.\n\nSecond run in plain mode.\nNew wallet ver logged.\nRescanned  the last 20k blocks or so,\nwhich might have been the marker last time the old wallet was used.\n\nThird and later runs... duplicates the second.\n\nNever did say 'upgrading wallet' as it sometimes does.\n\nRunning detach=1 always.\n\nWhy scan the last 20k every time? Shouldn't have to if\nwhole chain was scanned. And certainly no more than\nonce if not.\n\nAlso...\nDumping the run params (bitcoin.conf, cmdline) to the log would be good.\n\nAnd not automatically truncate the log when big but just append or roll it."
            },
            {
                "author": "grarpamp",
                "date": "2012-06-23T00:10:57",
                "message_text_only": "I had previously commented on this.\nMy references to wallet ver were really to nFileVersion.\nAnd I've since been able to make that, and the\nreal walletversion become current.\n\nHowever it is still doing this every invocation...\n Rescanning last 14xxx blocks (from block 170xxx)...\nWhich seems unneeded more than 1x. I cannot yet explain.\n\nSo I expect to avoid all by send the balance from old\nwallets to new wallet soon instead.\n\nThe old wallets were ver 10500."
            }
        ],
        "thread_summary": {
            "title": "Wallet related bug?",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "grarpamp"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 1398
        }
    },
    {
        "title": "[Bitcoin-development] Enforcing inflation rules for SPV clients",
        "thread_messages": [
            {
                "author": "Mike Hearn",
                "date": "2012-06-24T12:45:53",
                "message_text_only": "I've been having a discussion with d'aniel from the forums\u00a0about how\nto handle the possibility of a majority-miner conspiracy to raise\ninflation, if most economic actors use SPV clients.\n\nBecause of how blocks are formatted you cannot check the coinbase of a\ntransaction without knowing the fees in the block, and the fees can\nonly be calculated if you have all the input transactions for every\ntransaction in that block. Because the attack scenario is an attempted\ntakeover of the economy by miners, attempting to put hints into the\nblocks won't work - we have to assume the hardest chain is in fact\nwrong according to the rules signed up to by the Bitcoin user.\n\nThe most obvious goal for a cartel of miners is to change the\ninflation formula, either for purely selfish reasons (they want more\nmoney than can be obtained by fees) or due to coercion by\ngovernments/central banks who still subscribe to the \"inflation is\ngood\" idea.\n\nWhilst \"good\" nodes (still on the old ruleset) won't relay blocks that\nviolate the rules no matter how hard they are, in a situation where an\nSPV client DOES hear about the bad best chain, it would switch to it\nautomatically. And who knows how the network might look in future -\nperhaps most nodes would end up run by miners, or other entities that\nupgrade to the new ruleset for other reasons.\n\nd'aniel made a good proposal - having good nodes broadcast\nannouncements when they detect a rule that breaks the rules, along\nwith a proof that it did so. Checking the proof might be very\nexpensive, but it would only have to be done for split points,\nlimiting the potential for DoS. If a node announces that it has a\nweaker chain and that the split point is a rule-breaker, the SPV\nclient would download the headers for the side chain to verify the\nsplit, then download all the transactions in the split block along\nwith all their inputs, and the merkle branches linking the inputs to\nthe associated block headers. In this way the fee can be calculated,\nthe inflation formula applied and the coinbase value checked.\n\nIf the block is indeed found to be a rule-breaker, it'd be blacklisted\nand chains from that point forward ignored.\n\nMiners may decide to allow themselves to create money with\nnon-index-zero transactions to work around this. In that case the good\nnode can announce that a given tx in the rule-breaker block is\ninvalid. The SPV node would then challenge nodes announcing the longer\nchain to provide the inputs for the bad tx all the way back to a\npre-split coinbase.\n\nDoing these checks would be rather time consuming with huge blocks,\nbut it's a last resort only. In the absence of bugs, the mere presence\nof the mechanism should ensure it never has to be used."
            },
            {
                "author": "Stefan Thomas",
                "date": "2012-06-24T16:51:26",
                "message_text_only": "Very interesting for you to bring this up. I had a similar idea for a\ntotally different use case. Greg recently pointed out an interesting\ndilemma saying that (significantly) larger blocks would lead to\ncentralization. So I've been working on a design for a decentralized\npool that can handle gigabyte sized blocks by splitting up the work\namong its members.\n\nAt the moment P2Pool nodes all verify all transactions in all blocks.\nBut it seems feasible to create a system where miners who have over the\nlast 10000 blocks contributed to the pool's proof-of-work are allocated\na proportional piece of verification work with redundancy and\ndeterministic randomness that makes manipulation of the allocation\nextremely difficult. Such a pool would be very unlikely to accept an\ninvalid block or transaction in practice.\n\nHowever, with these block sizes obviously non miners are going to have\nto be SPV, so even just a 0.0001% chance of an invalid block being\naccepted has profound implications for the network. If a decentralized\npool like that had more than 50% of the hashing power and it accepted a\nsingle invalid block, that tainted chain would be forever regarded as\nvalid by SPV clients. There needs to be some way to recover once an\ninvalid block has been accidentally accepted by an imperfect miner.\n\nBased on that I also started to think about proofs of invalidity that\nwould circulate. Basically you would add a new network message that\nwould contain the proof that a specific signature and therefore the\nwhole block is invalid.\n\nAs long as the block's proof-of-work is valid and the block's parent is\none of the last n = 50000 blocks, the message is relayed (subject to a\ncooldown, warnings would be less frequently relayed the older the\noffending block is.)\n\nThe mechanism works in exactly the way Mike mentions: It allows even SPV\nclients to punish any miner who is dishonest or negligent with their\nverification work. That gives miners a good reason not to be dishonest\nor negligent in the first place.\n\n\n(Motivation:\n\nProcessing more transactions means that hashing is a smaller part of the\noverall cost for miners. For example, paying for 50 BTC worth of hashing\nper block costs 0.05 BTC per tx at 1000 tx/block, but only 0.0005 BTC at\n100000 tx/block.\n\nNumber of transactions is a lever that lets us have lower fees and more\nnetwork security at the same time. Like Greg correctly pointed out, this\nis not worth having if we have to sacrifice decentralization. But if we\ndon't, it becomes a no-brainer.\n\nMy IMTUO proposal [1] showed a way where miners don't need a copy of the\nset of unspent outputs at all. This means the minimum storage\nrequirements per node no longer grow with the number of transactions.\n\nHowever, the price for this was about five times greater bandwidth usage\nper verified transaction. Since every miner still had to verify every\ntransaction it looked like bandwidth would become an even bigger problem\nwith IMTUO than storage would have been without. However, if a small\nminer can do less than 100% verifications and still contribute, suddenly\nIMTUO may become viable. That would accomplish the holy grail of Bitcoin\nscalability where the network successfully runs on trust-atomic entities\nall of which can choose to store only a small fraction of the block\nchain, verify a small fraction of transactions and perform a small\nfraction of the hashing.)\n\n\n[1] https://en.bitcoin.it/wiki/User:Justmoon/IMTUO\n\nOn 6/24/2012 2:45 PM, Mike Hearn wrote:\n> I've been having a discussion with d'aniel from the forums about how\n> to handle the possibility of a majority-miner conspiracy to raise\n> inflation, if most economic actors use SPV clients.\n>\n> Because of how blocks are formatted you cannot check the coinbase of a\n> transaction without knowing the fees in the block, and the fees can\n> only be calculated if you have all the input transactions for every\n> transaction in that block. Because the attack scenario is an attempted\n> takeover of the economy by miners, attempting to put hints into the\n> blocks won't work - we have to assume the hardest chain is in fact\n> wrong according to the rules signed up to by the Bitcoin user.\n>\n> The most obvious goal for a cartel of miners is to change the\n> inflation formula, either for purely selfish reasons (they want more\n> money than can be obtained by fees) or due to coercion by\n> governments/central banks who still subscribe to the \"inflation is\n> good\" idea.\n>\n> Whilst \"good\" nodes (still on the old ruleset) won't relay blocks that\n> violate the rules no matter how hard they are, in a situation where an\n> SPV client DOES hear about the bad best chain, it would switch to it\n> automatically. And who knows how the network might look in future -\n> perhaps most nodes would end up run by miners, or other entities that\n> upgrade to the new ruleset for other reasons.\n>\n> d'aniel made a good proposal - having good nodes broadcast\n> announcements when they detect a rule that breaks the rules, along\n> with a proof that it did so. Checking the proof might be very\n> expensive, but it would only have to be done for split points,\n> limiting the potential for DoS. If a node announces that it has a\n> weaker chain and that the split point is a rule-breaker, the SPV\n> client would download the headers for the side chain to verify the\n> split, then download all the transactions in the split block along\n> with all their inputs, and the merkle branches linking the inputs to\n> the associated block headers. In this way the fee can be calculated,\n> the inflation formula applied and the coinbase value checked.\n>\n> If the block is indeed found to be a rule-breaker, it'd be blacklisted\n> and chains from that point forward ignored.\n>\n> Miners may decide to allow themselves to create money with\n> non-index-zero transactions to work around this. In that case the good\n> node can announce that a given tx in the rule-breaker block is\n> invalid. The SPV node would then challenge nodes announcing the longer\n> chain to provide the inputs for the bad tx all the way back to a\n> pre-split coinbase.\n>\n> Doing these checks would be rather time consuming with huge blocks,\n> but it's a last resort only. In the absence of bugs, the mere presence\n> of the mechanism should ensure it never has to be used.\n>\n> ------------------------------------------------------------------------------\n> Live Security Virtual Conference\n> Exclusive live event will cover all the ways today's security and \n> threat landscape has changed and how IT managers can respond. Discussions \n> will include endpoint security, mobile security and the latest in malware \n> threats. http://www.accelacomm.com/jaw/sfrnl04242012/114/50122263/\n> _______________________________________________\n> Bitcoin-development mailing list\n> Bitcoin-development at lists.sourceforge.net\n> https://lists.sourceforge.net/lists/listinfo/bitcoin-development"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-25T08:44:50",
                "message_text_only": "> Very interesting for you to bring this up. I had a similar idea for a\n> totally different use case. Greg recently pointed out an interesting\n> dilemma saying that (significantly) larger blocks would lead to\n> centralization.\n\nYeah. I am still unsure that this really holds. Bitcoin moves fast,\nbut even so, unless there are a few more SatoshiDice-like events and\nthe way people use transactions changes dramatically we're a long way\nfrom gigabyte sized blocks.  And once we get there, technology will\nprobably have improved to the point where it doesn't seem like a big\ndeal anymore.\n\nOf course we have debated this many times already. Maybe again at the\nnext meetup :-)"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-24T18:03:10",
                "message_text_only": "On Sun, Jun 24, 2012 at 8:45 AM, Mike Hearn <mike at plan99.net> wrote:\n> d'aniel made a good proposal - having good nodes broadcast\n> announcements when they detect a rule that breaks the rules, along\n> with a proof that it did so. Checking the proof might be very\n\nLink?\n\nI also proposed this on this list (see the response in the tree\ndatastructures thread) along with more elaboration on IRC. If multiple\npeople are coming up with it thats a good sign that it it might\nactually be viable. :)\n\nI was going for a slightly different angle and pointing out that the\nproofs would mean that a node doing validation with TxOUT tree which\nhasn't personally wittnessed the complete history of Bitcoin actually\nhas basically the same security\u2014 including resistance to miners\ncreating fake coin in the past\u2014 as a full node today because in order\nto get away with a lie every single node must conspire: It's adequate\nthat only one honest node wittness the lie because once it has the\nproof information is hard to suppress.\n\nTo save people from having to dig through the public IRC logs for what\nI wrote there:\n\n--- Day changed Thu Jun 21 2012\n15:10 < gmaxwell> etotheipi_: amiller: an interesting point with all\nthis txout tree stuff is that if you join the network late and just\ntrust that the history is correct based on the headers, any other node\nwho has witnessed a rule violation in the past can prepare a small\nmessage which you would take to be conclusive proof of a rule\nviolation and then ignore that chain.\n15:11 < gmaxwell> e.g. if someone doublespends I just take the\nconflicting transactions out and the segments connecting them to the\nchain... and show them to you. And without trusting me you can now\nignore the entire child chain past that point.\n15:13 < gmaxwell> This fits nicely with the Satoshi comment \"It takes\nadvantage of the nature of information being easy to spread but hard\nto stifle\" ...  it would be safe to late-join a txout tree chain,\nbecause if there is only a single other honest node in the world who\nwas around long enough to wittness the cheating, he could still tell\nyou and it would be as good as if you saw it yourself.\n15:17 < gmaxwell> (this is akin to the provable doublespend alert\nstuff we talked about before, but applied to blocks)"
            },
            {
                "author": "Mike Hearn",
                "date": "2012-06-25T08:42:02",
                "message_text_only": "> Link?\n\nIt was a private conversation for some reason.\n\n> I also proposed this on this list (see the response in the tree\n> datastructures thread) along with more elaboration on IRC.\n\nAh OK. I wasn't paying much attention to those threads."
            },
            {
                "author": "Daniel Lidstrom",
                "date": "2012-06-25T23:21:14",
                "message_text_only": "Here's the conversation I had with Mike that Gregory requested a link to:\n\n\nThanks!\n\nBad or hacked client devs is indeed a huge, worrying problem. The \nofficial client is addressing this with a system called gitian, where \nmultiple developers all compile the same source to the same binary and \nthen sign the results. Multi-signatures raise the bar for releasing \nhacked clients a lot. We're starting to investigate this with bitcoinj \ntoo, but it's a lot of work.\n\nGenerally, the more people you have to involve in a conspiracy, the less \nlikely it is to succeed. If a few miners started to dominate the system \nthey have strong financial incentives to cheat, alternatively, they may \nbe subjected to government pressure. Having to get the client developers \ninvolved too makes it much harder, especially as users have to actually \nupgrade.\n\nI started a thread on the development mailing list with your suggestion, \nby the way.\n\nOn Mon, Jun 25, 2012 at 1:00 AM, Daniel Lidstrom <lidstrom83 at gmail.com \n<mailto:lidstrom83 at gmail.com>> wrote:\n\n    Hey Mike,\n\n    I put our conversation in the email for easy reference.\n\n    In the unlikely event of a miner conspiracy to print money, is it\n    really so much of a further stretch to think the developers of a\n    widely used client could also be involved?  (Well, maybe, since\n    miners are unaccountable and developers are not.  OTOH if most users\n    are apathetic...)  Also, isn't the advantage for lightweight clients\n    of SPV over the server-client model that you don't have to trust any\n    operator?  Maybe I'm being too much of a purist here...\n\n    Regarding errors being cheap to send and expensive to verify,\n    compartmentalizing them the way I suggested before would make them\n    individually cheaper to verify.  Just throwing around ideas:\n    requiring the error message be received by a quorum of peers before\n    checking, and dropping misbehaving or unreliable peers could help.\n    Also, not verifying error messages unless the peers relaying them\n    are willing to send all the data necessary to do so would help. \n    Hashcash could also be used to balance the costs to send and to\n    verify a given type of error message.  I like your idea to only\n    check errors in blocks that are split points, and the length of the\n    split could also be a consideration.\n\n>     Can we move further conversations to email please? SMF kind of\n>     sucks as an inbox.\n>\n>     Anyway, yes, your proposal makes a lot of sense, although I think\n>     in practice this is unlikely to be an issue. If a majority of\n>     miners did start mining on a chain with new rules, even if SPV\n>     clients couldn't detect the switch automatically it's very likely\n>     the developers of those clients would notify the users out of band\n>     in some way. For example, by pushing an update to users that\n>     explains the new rules to them and tells them how they can cash\n>     out of the Bitcoin economy if they disagree with the new consensus.\n>\n>     If users are on the losing side of a rule change and want to stay\n>     there (eg, maybe most non-miners want to stay on the slower\n>     chain), then the client can just checkpoint the first block after\n>     the rule change occurred. Now even though there's a harder chain\n>     with the new rules, the client will stay with the old rules\n>     despite being blind to them. There's nothing that says checkpoints\n>     have to be hard coded - clients could poll the client developers\n>     every day to get new ones. So as long as the SPV devs are on the\n>     ball, most users would stay on the old rules even if the software\n>     can't do it by itself.\n>\n>     All that said, broadcasting messages proving a block broke the\n>     rules is a nice backstop if it can be done without excessive\n>     complexity. There are some details to think about. These messages\n>     would be cheap to create and expensive to verify. There has to be\n>     something that stops me claiming to SPV clients that every single\n>     block is invalid and forcing them to do tons of useless work.\n>     Perhaps only blocks that are split points would be eligible. Also,\n>     currently, SPV clients do not form their own P2P network. They are\n>     always leaves of full nodes. So propagation of the messages might\n>     prove difficult unless that was changed.\n\n>     Hi Mike,\n>\n>     Thanks for your reply.  It was actually an old post of yours on\n>     the forum that made me understand the importance of lightweight\n>     clients being able to audit the coinbase tx.\n>\n>     Re: input tx download, I think that splitting the \"invalid\n>     coinbase\" error notification into separate \"input n in tx m is\n>     invalid\" and \"invalid fee arithmetic\" errors would mean mobile\n>     clients would only ever have to download at most one input tx to\n>     verify an invalid coinbase.\n>\n>     The \"invalid fee arithmetic\" error could also be compartmentalized\n>     into \"invalid fee arithmetic in tx batch n\", where the fee\n>     subtotals are recorded in the block, so as to be separately\n>     verifiable.  Then the necessary tx downloads would be completely\n>     capped.\n>\n>     Anyway, I think most of my fears about lifting the block size\n>     limit are put to rest by these lines of thinking Smiley\n\n>     Hey Daniel,\n>\n>     I think you're thinking along the right lines here. We should be\n>     looking at cheap and backwards compatible ways to upgrade the SPV\n>     trust model to more than just going along with the majority consensus.\n>\n>     For the coinbase issue, it's actually really important because if\n>     you can't calculate fees, you can't check the size of the coinbase\n>     value and therefore a conspiracy of miners could try and change\n>     the inflation schedule. The worst that can happen today if miners\n>     decide to try and fork the ruleset against the best interests of\n>     other users, is that they produce chains that are not accepted by\n>     regular merchant/exchange/end-user nodes and tx confirmation slows\n>     down. But if most users are on SPV nodes they will happily accept\n>     the new blocks and the miner conspiracy will have successfully\n>     seized control of the money supply for the majority of end users.\n>\n>     The problem with calculating fees is you actually need not only\n>     every tx in a block, but all the input txns too! You can't know\n>     the fee of a transaction without the input transactions being\n>     available too.\n>\n>     So there's good news and bad news. The good news is you don't have\n>     to actually store every transaction on disk or check signatures,\n>     which is the expensive part, if you're an SPV node - you can still\n>     check the coinbase value asynchronously after receiving a block by\n>     requesting the entire contents and all the input transactions\n>     (+branches, of course). If you are in an environment that is\n>     always-on and not bandwidth constrained this can make a lot of\n>     sense. For example, if you're running an SPV client on a regular\n>     PC that is sitting in a shop somewhere, just downloading a lot of\n>     data is still very cheap compared to indexing it all and verifying\n>     all the signatures. I'm implementing support for this kind of\n>     async verification in bitcoinj at the moment.\n>\n>     Where it doesn't really make sense is mobile clients, and your\n>     ideas of notification can help a lot there. The first type of\n>     error broadcast I'd add is actually for detected double spends\n>     because there's a paper from researchers at ETH which show this is\n>     needed to address a feasible attack on todays infrastructure. But\n>     adding more later to detect invalid blocks is not a bad idea.\n\n>     Hi Mike,\n>\n>     I wanted to post this on the forum and figured that since, based\n>     on the subject, you'd probably end up reading it anyway, I thought\n>     I'd run it by you first to see if there's any merit to it.\n>\n>     Quote\n>     I think similar ideas are currently being tossed around.\n>\n>     The motivation here is to keep rule enforcement maximally\n>     decentralized while scaling Bitcoin. Basically, SPV clients could\n>     subscribe to peers' announcements of /invalid/ blocks + their\n>     specific failure mode.  AFAICT, most of the possible failure modes\n>     are easily verifiable by smart phone SPV clients, even at scale. \n>     For example, inclusion of double spends or otherwise invalid txs\n>     are easy to verify /given the necessary data/, and Merkle tree\n>     nodes can be deemed invalid /if no peer can produce the data that\n>     hashes to it/ (gotta be careful to detect and deal with false\n>     positives here).  The only failure mode I can think of that isn't\n>     easily verifiable at scale is an invalid quantity of fees spent in\n>     the coinbase tx, since that currently requires the download of the\n>     whole block. (I think I have an idea to fix this below.)\n>\n>     This trust model relies upon having a single fully validating peer\n>     that will reliably announce invalid blocks.  To help this, one or\n>     more peers could be periodically cycled, and new ones asked if the\n>     main chain contains any invalid blocks.  Worst case scenario, an\n>     SPV client reverts to the current trust model, so this idea can\n>     only improve the situation. Best case, it takes advantage of the\n>     fact that public information is hard to suppress.\n>\n>     *Verifying coinbase tx invalidity at scale with SPV clients*\n>\n>     While this is currently not prohibitive for a smart phone if done\n>     only occasionally, this will not be the case if the block size\n>     limit is lifted.  The idea is to split up the summing of the\n>     spendable fees into verifiable pieces.  A new merkle tree is\n>     calculated where the i'th leaf's data is the total fees in the\n>     i'th chunk of 1024 txs, along with these txs' Merkle root (to\n>     prevent collisions).  The Merkle root of this new tree is then\n>     included somewhere in the block.\n>\n>     To verify a claim of invalidity of one of these leaves requires\n>     the download of ~1MB of tx data (along with a small amount of\n>     Merkle tree data to verify inclusion).  If no claims are made\n>     against it, the leaf data is assumed to be valid.  If this is the\n>     case, but shenanigans are called on the coinbase tx, downloading\n>     all of the leaf data, verifying inclusion, and calculating the\n>     total spendable fees is easily doable for a smart phone, even at\n>     very large tx volumes.\n>\n>     Thanks for any thoughts/suggestions!\n>\n>     Cheers,\n>     Daniel\n\n\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120625/bf142529/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: not available\nType: image/gif\nSize: 382 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120625/bf142529/attachment.gif>"
            }
        ],
        "thread_summary": {
            "title": "Enforcing inflation rules for SPV clients",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Stefan Thomas",
                "Mike Hearn",
                "Gregory Maxwell",
                "Daniel Lidstrom"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 23853
        }
    },
    {
        "title": "[Bitcoin-development] Tor hidden service support",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2012-06-26T14:11:29",
                "message_text_only": "Hello everyone,\n\na few days ago we merged Tor hidden service support in mainline. This means\nthat it's now possible to run a hidden service bitcoin node, and connect to\nother bitcoin hidden services (via a Tor proxy) when running git HEAD. See\ndoc/Tor.txt for more information. This is expected to be included in the 0.7\nrelease.\n\nAdditionally, such addresses are exchanged and relayed via the P2P network.\nTo do so, we reused the fd87:d87e:eb43::/48 IPv6 range. Each address in this\n80-bit range is mapped to an onion address, and treated as belonging to a\nseparate network. This network range is the same as used by the OnionCat\napplication (though we do not use OnionCat in any way), and is part of the\nRFC4193 Unique Local IPv6 range, which is normally not globally routable.\n\nOther clients that wish to implement similar functionality, can use this\ntest case: 5wyqrzbvrdsumnok.onion == FD87:D87E:EB43:edb1:8e4:3588:e546:35ca.\nThe conversion is simply decoding the base32 onion address, and storing the\nresulting 80 bits of data as low-order bits of an IPv6 address, prefixed by\nfd87:d87e:eb43:. As this range is not routable, there should be no\ncompatibility problems: any unaware IPv6-capable code will immediately fail\nwhen trying to connect.\n\n-- \nPieter\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120626/9b5e2a09/attachment.html>"
            },
            {
                "author": "grarpamp",
                "date": "2012-06-26T23:01:14",
                "message_text_only": "> Additionally, such addresses are exchanged and relayed via the P2P network.\n> To do so, we reused the fd87:d87e:eb43::/48 IPv6 range. Each address in this\n> 80-bit range is mapped to an onion address, and treated as belonging to a\n> separate network. This network range is the same as used by the OnionCat\n> application (though we do not use OnionCat in any way), and is part of the\n> RFC4193 Unique Local IPv6 range, which is normally not globally routable.\n>\n> Other clients that wish to implement similar functionality, can use this\n> test case: 5wyqrzbvrdsumnok.onion == FD87:D87E:EB43:edb1:8e4:3588:e546:35ca.\n> The conversion is simply decoding the base32 onion address, and storing the\n> resulting 80 bits of data as low-order bits of an IPv6 address, prefixed by\n> fd87:d87e:eb43:. As this range is not routable, there should be no\n> compatibility problems: any unaware IPv6-capable code will immediately fail\n> when trying to connect.\n\nYou are going to want to include the block of the Phatom project as well:\nhttps://code.google.com/p/phantom/\nfd00:2522:3493::/48\n\nAnd the one for 'garlicat' for I2P, which might be more complex due\nto I2P's addressing:\nfd60:db4d:ddb5::/48\n\nNote that while these blocks are not expected to be routable, that\npeople may in fact have interfaces, routing tables and packet filters\non their machines configured with up to all three of those networks\nfor the purposes therein."
            },
            {
                "author": "Gregory Maxwell",
                "date": "2012-06-27T00:14:08",
                "message_text_only": "On Tue, Jun 26, 2012 at 7:01 PM, grarpamp <grarpamp at gmail.com> wrote:\n> You are going to want to include the block of the Phatom project as well:\n> https://code.google.com/p/phantom/\n> fd00:2522:3493::/48\n\nPerhaps some argument to add blocks to the IsRoutable check is in\norder?  Then people who use overlay networks that are actually\nroutable but which use otherwise private space can just add the\nrelevant blocks.\n\n> Note that while these blocks are not expected to be routable, that\n> people may in fact have interfaces, routing tables and packet filters\n> on their machines configured with up to all three of those networks\n> for the purposes therein.\n\nNote that while the hidden service support in bitcoin uses a\ncompatible IPv6 mapping with onioncat,  it is _not_ onioncat, does not\nuse onioncat, does not need onioncat, and wouldn't benefit from\nonioncat.  The onioncat style advertisement is used because our\nprotocol already relays IPv6 addresses. The connections are regular\ntor hidden service connections, not the more-risky and low performance\nip in tcp onioncat stuff."
            },
            {
                "author": "Andy Parkins",
                "date": "2012-06-27T08:47:01",
                "message_text_only": "On 2012 June 26 Tuesday, Pieter Wuille wrote:\n\n> Additionally, such addresses are exchanged and relayed via the P2P network.\n> To do so, we reused the fd87:d87e:eb43::/48 IPv6 range. Each address in\n\nYuck.  Can't we pinch a few of the addr.services bits to store an address \nfamily?  AF_INET, AF_INET6, AF_CUSTOM_TOR, and leave space for a few more \nwould be, say, four bits out of 64 mostly unused.\n\n\nAndy\n\n-- \nDr Andy Parkins\nandyparkins at gmail.com\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 198 bytes\nDesc: This is a digitally signed message part.\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20120627/00ea91b2/attachment.sig>"
            },
            {
                "author": "grarpamp",
                "date": "2012-06-27T19:51:08",
                "message_text_only": "Forward past automoderation...\n\n\n> Reading https://github.com/bitcoin/bitcoin/blob/master/doc/Tor.txt\n\n> Is bitcoin software going to incorporate tor binaries within the\n> application standard application and automatically create a Tor Hidden\n> Service on behalf of end-user?\n>\n> Are there any direction regarding this kind of integration?\n\nThe document (Tor.txt) assumes the bitcoin user has taken care of\nthat. So no bi-direction needed (I am not TorProject).\n\n> Regarding the addressing, why not use directly the .onion address?\n> They represent in parallel:\n> - Routing information (providing a path to the destination)\n> - Proof of identity (owning the private RSA key)\n> Which is the reason to map it to an IPv6 address?\n\nSeems it's used only within bitcoin code to distinguish which proxy\nor native IPvN path to send bitcoin traffic to (or receive from).\nIt might be simpler than managing onions, i2p's and whatever else\nthroughout the code and the private bitcoin p2p mesh.\n\nThough I don't suspect it will conflict [1] with anyone's use of\nOnionCat, GarliCat, or Phantom... it would just feel odd configuring\nbitcoin to use Tor or I2P proxy ports (or Phantom native) when you\ncould conceivably just dump the IPv6 traffic to the OS stack for\nhandling once you have the *Cat shims and Phantom set up. They do\nhave a point about about ocat as a shim for their purposes. And\nPhantom is a special case in that it's all native IPv6 interface,\nno proxy or shim needed or provided.\n\nI will quote an additional note from bitcoin-devel...\n\n\"Note that while the hidden service support in bitcoin uses a\ncompatible IPv6 mapping with onioncat, it is _not_ onioncat, does not\nuse onioncat, does not need onioncat, and wouldn't benefit from\nonioncat. The onioncat style advertisement is used because our\nprotocol already relays IPv6 addresses. The connections are regular\ntor hidden service connections, not the more-risky and low performance\nip in tcp onioncat stuff.\"\n\nFYI. There have been a dozen or so onion:8333 nodes and maybe some\non I2P long before this work. But I think could only be used as\n-connect or -addnode seeds with some extra host setup. Never tried\nit since -proxy was sufficient. Seems this is a simpler and full\nsolution.\n\n[1] Well bitcoin wouldn't know to offload traffic to any of those\nblocks, or a specific host on them, if you had them set up locally\nvia *Cat or Phantom... for bitcoin use. It would probably end up\nhalf useful similar to the above FYI. But that would just affect\nbitcoin, not whatever else you were running on them."
            }
        ],
        "thread_summary": {
            "title": "Tor hidden service support",
            "categories": [
                "Bitcoin-development"
            ],
            "authors": [
                "Pieter Wuille",
                "Gregory Maxwell",
                "grarpamp",
                "Andy Parkins"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 7251
        }
    },
    {
        "title": "[Bitcoin-development] [tor-talk]  Tor hidden service support",
        "thread_messages": [
            {
                "author": "grarpamp",
                "date": "2012-06-27T09:25:15",
                "message_text_only": "GregM, wasn't sure how to answer your question, and as to\nconflicts [1]. I think I grasped it in my reply to something on\ntor-talk, which is on its way here pending moderation due to bcc.\nI put that part below. The FYI referred to seednodes as\nthey exist on Tor / I2P today.\n\n> You are going to want to include the block of the Phatom project as well:\n>> https://code.google.com/p/phantom/\n>> fd00:2522:3493::/48\n\n> Perhaps some argument to add blocks to the IsRoutable check is in\n> order?  Then people who use overlay networks that are actually\n> routable but which use otherwise private space can just add the\n> relevant blocks.\n\n/ [1] Well bitcoin wouldn't know to offload traffic to any of those\n/ blocks, or a specific host on them, if you had them set up locally\n/ via *Cat or Phantom... for bitcoin use. It would probably end up\n/ half useful similar to the above FYI. But that would just affect\n/ bitcoin, not whatever else you were running on them."
            }
        ],
        "thread_summary": {
            "title": "Tor hidden service support",
            "categories": [
                "Bitcoin-development",
                "tor-talk"
            ],
            "authors": [
                "grarpamp"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 958
        }
    }
]