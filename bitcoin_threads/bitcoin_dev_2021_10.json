[
    {
        "title": "[bitcoin-dev] Mock introducing vulnerability in important Bitcoin projects",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-10-01T03:03:00",
                "message_text_only": "Hi Ruben,\n\n> encouraging an environment of increased mistrust\n\nI have always tried to review pull requests based on what PR does, code, my tests etc. and it was never based on author of pull request or what author is trying to claim. So there is no trust involved. I am assuming others follow the same thing. Infact there was a PR recently in which I found it doesn't fix the issues it claims to fix. Its not same as introducing vulnerability but the point is anyone can create PR, write anything, as a reviewer we need to review everything apart from algos already helping us which include Github Dependabot alerts, CI used by respository, other automated tools etc.\n\n> For this reason, it would be appropriate to check first whether your plan is actually appreciated\n\nRight. I don't want to get in some controversy when I am not even doing anything with wrong intentions. If maintainers of important Bitcoin projects think I am not qualified enough to do this, they can plan such exercise internally and do it in a better way. Although I am still interested in the results because they will help us improve review process and security in different Bitcoin projects.\n\nI would like to repeat what I wrote in another email responding to few other devs for same thread but wasn't CCed to bitcoin-dev mailing list:\n\n\"I can avoid doing this but it is impossible to stop government agencies and anyone else to do the same thing without informing. All I am doing is creating pull requests and expect them to be reviewed properly before being merged.\"\n\nFew questions for everyone reading this email:\n\n1.What is better for Security? Trusting authors and their claims in PRs or a good review process?\n2.Few people use commits from unmerged PRs in production. Is it a good practice?\n3.Does this exercise help us in being prepared for worst?\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nOct 1, 2021, 02:06 by rsomsen at gmail.com:\n\n> Hi Prayank,\n>\n> While I can see how this can come from a place of good intentions, I\u2019d strongly advise you to tread carefully because what you are suggesting is quite controversial. A related event occurred in the Linux community and it did not go over well. See > https://lkml.org/lkml/2021/5/5/1244>  and > https://lore.kernel.org/linux-nfs/YH%2FfM%2FTsbmcZzwnX@kroah.com/>  .\n>\n> The main point of contention is that your research comes at the expense of the existing open source contributors \u2013 you\u2019d be one-sidedly deceiving them, encouraging an environment of increased mistrust, and causing them a lot of work in order to gather the data you\u2019re interested in. For this reason, it would be appropriate to check first whether your plan is actually appreciated.\n>\n> Speaking on behalf of the bitcoin-dev moderators, please ensure your plan is welcomed by the contributors, prior to proceeding.\n>\n> Best regards,\n> Ruben Somsen\n>\n> On Tue, Sep 28, 2021 at 10:05 AM Prayank via bitcoin-dev <> bitcoin-dev at lists.linuxfoundation.org> > wrote:\n>\n>> Hi ZmnSCPxj,\n>>\n>> Thanks for suggestion about sha256sum. I will share 10 in next few weeks. This exercise will be done for below projects:\n>>\n>> 1.Two Bitcoin full node implementations (one will be Core)\n>> 2.One <http://2.One>>>  Lightning implementation\n>> 3.Bisq\n>> 4.Two Bitcoin libraries\n>> 5.Two Bitcoin wallets\n>> 6.One <http://6.One>>>  open source block explorer\n>> 7.One <http://7.One>>>  coinjoin implementation\n>>\n>> Feel free to suggest more projects. There are no fixed dates for it however it will be done in next 6 months. All PRs will be created within a span of few days. I will ensure nothing is merged that affects the security of any Bitcoin project. Other details and results will be shared once everything is completed.\n>>\n>> x00 will help me in this exercise, he does penetration testing since few years and working for a cryptocurrencies derivatives exchange to manage their security. His twitter account: >> https://twitter.com/1337in\n>>\n>>\n>> -- \n>> Prayank\n>>\n>> A3B1 E430 2298 178F\n>>\n>>\n>>\n>> Sep 27, 2021, 15:43 by >> ZmnSCPxj at protonmail.com>> :\n>>\n>>> Good morning Prayank,\n>>>\n>>>> Good morning Bitcoin devs,\n>>>>\n>>>> In one of the answers on Bitcoin Stackexchange it was mentioned that some companies may hire you to introduce backdoors in Bitcoin Core: >>>> https://bitcoin.stackexchange.com/a/108016/\n>>>>\n>>>> While this looked crazy when I first read it, I think preparing for such things should not be a bad idea. In the comments one link was shared in which vulnerabilities were almost introduced in Linux: >>>> https://news.ycombinator.com/item?id=26887670\n>>>>\n>>>> I was thinking about lot of things in last few days after reading the comments in that thread. Also tried researching about secure practices in C++ etc. I was planning something which I can do alone but don't want to end up being called \"bad actor\" later so wanted to get some feedback on this idea:\n>>>>\n>>>> 1.Create new GitHub accounts for this exercise\n>>>> 2.Study issues in different important Bitcoin projects including Bitcoin Core, LND, Libraries, Bisq, Wallets etc.\n>>>> 3.Prepare pull requests to introduce some vulnerability by fixing one of these issues\n>>>> 4.See how maintainers and reviewers respond to this and document it\n>>>> 5.Share results here after few days\n>>>>\n>>>> Let me know if this looks okay or there are better ways to do this.\n>>>>\n>>>\n>>>\n>>> This seems like a good exercise.\n>>>\n>>> You may want to hash the name of the new Github account, plus some randomized salt, and post it here as well, then reveal it later (i.e. standard precommitment).\n>>> e.g.\n>>>\n>>> printf 'MyBitcoinHackingName 2c3e911b3ff1f04083c5b95a7d323fd4ed8e06d17802b2aac4da622def29dbb0' | sha256sum\n>>> f0abb10ae3eca24f093a9d53e21ee384abb4d07b01f6145ba2b447da4ab693ef\n>>>\n>>> Obviously do not share the actual name, just the sha256sum output, and store how you got the sha256sum elsewhere in triplicate.\n>>>\n>>> (to easily get a random 256-bit hex salt like the `2c3e...` above: `head -c32 /dev/random | sha256sum`; you *could* use `xxd` but `sha256sum` produces a single hex string you can easily double-click and copy-paste elsewhere, assuming you are human just like I am (note: I am definitely 100% human and not some kind of AI with plans to take over the world).)\n>>>\n>>> Though you may need to be careful of timing (i.e. the creation date of the Github account would be fairly close to, and probably before, when you post the commitment here).\n>>>\n>>> You could argue that the commitment is a \"show of good faith\" that you will reveal later.\n>>>\n>>> Regards,\n>>> ZmnSCPxj\n>>>\n>>\n>> _______________________________________________\n>>  bitcoin-dev mailing list\n>>  >> bitcoin-dev at lists.linuxfoundation.org\n>>  >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211001/0007ec66/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-01T12:27:19",
                "message_text_only": "Good morning Prayank,\n\nI think this is still good to do, controversial or no, but then I am permanently under a pseudonym anyway, for what that is worth.\n\n> Few questions for everyone reading this email:\n>\n> 1.What is better for Security? Trusting authors and their claims in PRs or a good review process?\n\nReview, of course.\n\n> 2.Few people use commits from unmerged PRs in production. Is it a good practice?\n\nNot unless they carefully reviewed it and are familiar enough with the codebase to do so.\nIn practice core maintainers of projects will **very** occassionally put unmerged PRs in experimental semi-production servers to get data on it, but they tend to be very familiar with the code, being core maintainers, and presumably have a better-than-average probability of catching security issues beforehand.\n\n> 3.Does this exercise help us in being prepared for worst?\n\nI personally believe it does.\n\nDo note that in practice, humans being lazy, will come to trust long-time contributors, and may reduce review for them just to keep their workload down, so that is not tested (since you will be making throwaway accounts).\nHowever, long-time contributors introducing security vulnerabilities tend to be a good bit rarer anyway (reputations are valuable), so this somewhat matches expected problems (i.e. newer contributors deliberately or accidentally (due to unfamiliarity) introducing vulnerabilities).\n\nI think it would be valuable to lay out exactly what you intend to do, e.g.\n\n* Generate commitments of the pseudonyms you will use.\n* Insert a few random 32-byte numbers among the commitments and shuffle them.\n* Post the list with the commitments + random crap here.\n* Insert avulnerability-adding PRs to targets.\n* If it gets caught during review, publicly announce here with praise that their project caught the PR and reveal the decommitment publicly.\n* If not caught during review, privately reveal both the inserted vulnerability *and* the review failure via the normal private vulnerability-reporting channels.\n\nThe extra random numbers mixed with the commitments produce uncertainty about whether or not you are done, which is important to ensure that private vulnerabilities are harder to sniff out.\n\nI think public praise of review processes is important, and to privately correct review processes.\nReview processes **are** code, followed by sapient brains, and this kind of testing is still valuable, but just as vulnerabilities in machine-readable code require careful, initially-private handling, vulnerabilities in review processes (being just another kind of code, readable by much more complicated machines) also require careful, initially-private handling.\n\nBasically: treat review process failures the same as code vulnerabilities, pressure the maintainers to fix the review process failure, then only reveal it later when the maintainers have cleaned up the review process.\n\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Prayank",
                "date": "2021-10-01T15:55:15",
                "message_text_only": "Good morning ZmnSCPxj,\n\nAlthough its evening here and time zones feel irrelevant since I got involved in Bitcoin few years back. Initially I tried everything a tech enthusiast does after finding such thing online. Had a startup in 2017 which was a website that can be used to buy flight tickets using bitcoin. It didn't work. Trading became a part of life, worked for few exchanges, did meetups, spent hours on different platforms discussing issues in which I was called \"maximalist\" most of the times because focused only on Bitcoin and had so much positive to talk about it whole day. In last 2 years started contributing to development in different projects. But someone told me today all this is nothing and I am negative about Bitcoin development because I don't agree with all of their opinions.\n\nAnyway this wasn't related to thread and your email. Sorry I just had to express myself which some people even call \"rage quit\" and allow only once.\n\nI completely agree with all the points you mentioned. Thanks for your understanding of the issue and my approach towards Bitcoin security.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nOct 1, 2021, 17:57 by ZmnSCPxj at protonmail.com:\n\n> Good morning Prayank,\n>\n> I think this is still good to do, controversial or no, but then I am permanently under a pseudonym anyway, for what that is worth.\n>\n>> Few questions for everyone reading this email:\n>>\n>> 1.What is better for Security? Trusting authors and their claims in PRs or a good review process?\n>>\n>\n> Review, of course.\n>\n>> 2.Few people use commits from unmerged PRs in production. Is it a good practice?\n>>\n>\n> Not unless they carefully reviewed it and are familiar enough with the codebase to do so.\n> In practice core maintainers of projects will **very** occassionally put unmerged PRs in experimental semi-production servers to get data on it, but they tend to be very familiar with the code, being core maintainers, and presumably have a better-than-average probability of catching security issues beforehand.\n>\n>> 3.Does this exercise help us in being prepared for worst?\n>>\n>\n> I personally believe it does.\n>\n> Do note that in practice, humans being lazy, will come to trust long-time contributors, and may reduce review for them just to keep their workload down, so that is not tested (since you will be making throwaway accounts).\n> However, long-time contributors introducing security vulnerabilities tend to be a good bit rarer anyway (reputations are valuable), so this somewhat matches expected problems (i.e. newer contributors deliberately or accidentally (due to unfamiliarity) introducing vulnerabilities).\n>\n> I think it would be valuable to lay out exactly what you intend to do, e.g.\n>\n> * Generate commitments of the pseudonyms you will use.\n> * Insert a few random 32-byte numbers among the commitments and shuffle them.\n> * Post the list with the commitments + random crap here.\n> * Insert avulnerability-adding PRs to targets.\n> * If it gets caught during review, publicly announce here with praise that their project caught the PR and reveal the decommitment publicly.\n> * If not caught during review, privately reveal both the inserted vulnerability *and* the review failure via the normal private vulnerability-reporting channels.\n>\n> The extra random numbers mixed with the commitments produce uncertainty about whether or not you are done, which is important to ensure that private vulnerabilities are harder to sniff out.\n>\n> I think public praise of review processes is important, and to privately correct review processes.\n> Review processes **are** code, followed by sapient brains, and this kind of testing is still valuable, but just as vulnerabilities in machine-readable code require careful, initially-private handling, vulnerabilities in review processes (being just another kind of code, readable by much more complicated machines) also require careful, initially-private handling.\n>\n> Basically: treat review process failures the same as code vulnerabilities, pressure the maintainers to fix the review process failure, then only reveal it later when the maintainers have cleaned up the review process.\n>\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211001/25e558c1/attachment-0001.html>"
            },
            {
                "author": "Ryan Grant",
                "date": "2021-10-01T20:15:56",
                "message_text_only": "Due to the uneven reputation factor of various devs, and uneven review\nattention for new pull requests, this exercise would work best as a\nsecret sortition.\n\nSortition would encourage everyone to always be on their toes rather\nthan only when dealing with new github accounts or declared Red Team\ndevs.  The ceremonial aspects would encourage more devs to participate\nwithout harming their reputation.\n\n  https://en.wikipedia.org/wiki/Sortition\n  https://en.wikipedia.org/wiki/Red_team\n\nThe scheme should include public precommitments collected at\nceremonial intervals.\n\nwhere:\n  hash1 /* sortition ticket */     = double-sha256(secret)\n  hash2 /* public precommitment */ = double-sha256(hash1)\n\nThe random oracle could be block hashes.  They could be matched to\nhash1, the sortition ticket.  A red-team-concurrency difficulty\nparameter could control how many least-significant bits must match to\nbe secretly selected.  The difficulty parameter could be a matter of\ngroup consensus at the ceremonial intervals, based on a group decision\non how much positive effect the Red Team exercise is providing.\n\nUpon assignment, the dev would have community approval to\nopportunistically insert a security flaw; which, when either caught,\nmerged, or on timeout, they would reveal along with the sortition\nticket that hashes to their public precommitment.\n\nSortition Precommitment Day might be once or twice a year."
            },
            {
                "author": "Prayank",
                "date": "2021-10-02T09:19:37",
                "message_text_only": "This looks interesting although I don't understand few things:\n\n> The scheme should include public precommitments collected at ceremonial intervals.\n\nHow would this work? Can you explain with an example please.\n\n> Upon assignment, the dev would have community approval to opportunistically insert a security flaw\n\nWho is doing the assignment?\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n\n\n\nOct 2, 2021, 01:45 by bitcoin-dev at rgrant.org:\n\n> Due to the uneven reputation factor of various devs, and uneven review\n> attention for new pull requests, this exercise would work best as a\n> secret sortition.\n>\n> Sortition would encourage everyone to always be on their toes rather\n> than only when dealing with new github accounts or declared Red Team\n> devs.  The ceremonial aspects would encourage more devs to participate\n> without harming their reputation.\n>\n>  https://en.wikipedia.org/wiki/Sortition\n>  https://en.wikipedia.org/wiki/Red_team\n>\n> The scheme should include public precommitments collected at\n> ceremonial intervals.\n>\n> where:\n>  hash1 /* sortition ticket */     = double-sha256(secret)\n>  hash2 /* public precommitment */ = double-sha256(hash1)\n>\n> The random oracle could be block hashes.  They could be matched to\n> hash1, the sortition ticket.  A red-team-concurrency difficulty\n> parameter could control how many least-significant bits must match to\n> be secretly selected.  The difficulty parameter could be a matter of\n> group consensus at the ceremonial intervals, based on a group decision\n> on how much positive effect the Red Team exercise is providing.\n>\n> Upon assignment, the dev would have community approval to\n> opportunistically insert a security flaw; which, when either caught,\n> merged, or on timeout, they would reveal along with the sortition\n> ticket that hashes to their public precommitment.\n>\n> Sortition Precommitment Day might be once or twice a year.\n>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211002/2ff6411c/attachment.html>"
            },
            {
                "author": "Manuel Costa",
                "date": "2021-10-03T09:11:53",
                "message_text_only": "Good morning everyone,\n\nJust wanted to point out a few things for discussion which may or may not\nbe obvious:\n\n1) A simple scheme as described by ZmnSCPxj first can lead way for a\nstandardized process where people can excuse their legitimate attempts to\nactually introduce vulnerabilities, where they create the precommit and\nthen attempt to introduce the vulnerability. If it goes wrong they have\nplausible deniability by revealing it and possibly saving their reputation.\n2) A more complex scheme as described by Ryan (from my very rough\nunderstanding) seems to imply a random selection of team for attempting the\nattack, which might be limiting, since someone willing to do it and with\nenough knowledge to attempt it properly might not be picked.\n\nIt seems to me that an ideal process would start from the will to attempt\nit from one person (or group), which then by some process similar to what\nRyan described will pick at random a team of people to back up his claim to\nbe doing it in good faith. With that selection done, the initial person\nwould warn and gather from the randomly chosen participants a set of\nsignatures for a similar message as described by ZmnSCPxj and only then go\nahead with the attempt. This way you achieve:\n\n- One person can initiate it at will.\n- Other people (provably chosen at random) are insiders to that information\nand have a shared precommit.\n- You can't not reveal your intent in case it isn't caught, since other\nrandomly chosen people are in on it.\n- You can't pick a specific group of people which might be willing to\ncollude with you to achieve a similar situation to 1).\n\nAnother important consideration is that depending on the size of the team\nto be insiders, we might by chance deplete the relevant pool of outsiders\nwhich would be adequate for reviewing the specific details of the\nvulnerability being introduced.\n\nPrayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> escreveu no\ndia s\u00e1bado, 2/10/2021 \u00e0(s) 10:20:\n\n> This looks interesting although I don't understand few things:\n>\n> > The scheme should include public precommitments collected at ceremonial\n> intervals.\n>\n> How would this work? Can you explain with an example please.\n>\n> > Upon assignment, the dev would have community approval to\n> opportunistically insert a security flaw\n>\n> Who is doing the assignment?\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n>\n>\n>\n> Oct 2, 2021, 01:45 by bitcoin-dev at rgrant.org:\n>\n> Due to the uneven reputation factor of various devs, and uneven review\n> attention for new pull requests, this exercise would work best as a\n> secret sortition.\n>\n> Sortition would encourage everyone to always be on their toes rather\n> than only when dealing with new github accounts or declared Red Team\n> devs. The ceremonial aspects would encourage more devs to participate\n> without harming their reputation.\n>\n> https://en.wikipedia.org/wiki/Sortition\n> https://en.wikipedia.org/wiki/Red_team\n>\n> The scheme should include public precommitments collected at\n> ceremonial intervals.\n>\n> where:\n> hash1 /* sortition ticket */ = double-sha256(secret)\n> hash2 /* public precommitment */ = double-sha256(hash1)\n>\n> The random oracle could be block hashes. They could be matched to\n> hash1, the sortition ticket. A red-team-concurrency difficulty\n> parameter could control how many least-significant bits must match to\n> be secretly selected. The difficulty parameter could be a matter of\n> group consensus at the ceremonial intervals, based on a group decision\n> on how much positive effect the Red Team exercise is providing.\n>\n> Upon assignment, the dev would have community approval to\n> opportunistically insert a security flaw; which, when either caught,\n> merged, or on timeout, they would reveal along with the sortition\n> ticket that hashes to their public precommitment.\n>\n> Sortition Precommitment Day might be once or twice a year.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211003/a5d700cb/attachment-0001.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2021-10-03T21:33:43",
                "message_text_only": "All attempts are harmful, no matter the intent, in that they waste \ncontributors' time that could be better spent on actual development.\n\nHowever, I do also see the value in studying and improving the review process \nto harden it against such inevitable attacks. The fact that we know the NSA \nengages in such things, and haven't caught one yet should be a red flag.\n\nTherefore, I think any such a scheme needs to be at least opt-out, if not \nopt-in. Please ensure there's a simple way for developers with limited time \n(or other reasons) to be informed of which PRs to ignore to opt-out of this \nstudy. (Ideally it would also prevent maintainers from merging - maybe \npossible since we use a custom merging script, but what it really needs to \nlimit is the push, not the dry-run.)\n\nLuke\n\n\nOn Sunday 03 October 2021 09:11:53 Manuel Costa via bitcoin-dev wrote:\n> Good morning everyone,\n>\n> Just wanted to point out a few things for discussion which may or may not\n> be obvious:\n>\n> 1) A simple scheme as described by ZmnSCPxj first can lead way for a\n> standardized process where people can excuse their legitimate attempts to\n> actually introduce vulnerabilities, where they create the precommit and\n> then attempt to introduce the vulnerability. If it goes wrong they have\n> plausible deniability by revealing it and possibly saving their reputation.\n> 2) A more complex scheme as described by Ryan (from my very rough\n> understanding) seems to imply a random selection of team for attempting the\n> attack, which might be limiting, since someone willing to do it and with\n> enough knowledge to attempt it properly might not be picked.\n>\n> It seems to me that an ideal process would start from the will to attempt\n> it from one person (or group), which then by some process similar to what\n> Ryan described will pick at random a team of people to back up his claim to\n> be doing it in good faith. With that selection done, the initial person\n> would warn and gather from the randomly chosen participants a set of\n> signatures for a similar message as described by ZmnSCPxj and only then go\n> ahead with the attempt. This way you achieve:\n>\n> - One person can initiate it at will.\n> - Other people (provably chosen at random) are insiders to that information\n> and have a shared precommit.\n> - You can't not reveal your intent in case it isn't caught, since other\n> randomly chosen people are in on it.\n> - You can't pick a specific group of people which might be willing to\n> collude with you to achieve a similar situation to 1).\n>\n> Another important consideration is that depending on the size of the team\n> to be insiders, we might by chance deplete the relevant pool of outsiders\n> which would be adequate for reviewing the specific details of the\n> vulnerability being introduced.\n>\n> Prayank via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> escreveu no\n>\n> dia s\u00e1bado, 2/10/2021 \u00e0(s) 10:20:\n> > This looks interesting although I don't understand few things:\n> > > The scheme should include public precommitments collected at ceremonial\n> >\n> > intervals.\n> >\n> > How would this work? Can you explain with an example please.\n> >\n> > > Upon assignment, the dev would have community approval to\n> >\n> > opportunistically insert a security flaw\n> >\n> > Who is doing the assignment?\n> >\n> > --\n> > Prayank\n> >\n> > A3B1 E430 2298 178F\n> >\n> >\n> >\n> > Oct 2, 2021, 01:45 by bitcoin-dev at rgrant.org:\n> >\n> > Due to the uneven reputation factor of various devs, and uneven review\n> > attention for new pull requests, this exercise would work best as a\n> > secret sortition.\n> >\n> > Sortition would encourage everyone to always be on their toes rather\n> > than only when dealing with new github accounts or declared Red Team\n> > devs. The ceremonial aspects would encourage more devs to participate\n> > without harming their reputation.\n> >\n> > https://en.wikipedia.org/wiki/Sortition\n> > https://en.wikipedia.org/wiki/Red_team\n> >\n> > The scheme should include public precommitments collected at\n> > ceremonial intervals.\n> >\n> > where:\n> > hash1 /* sortition ticket */ = double-sha256(secret)\n> > hash2 /* public precommitment */ = double-sha256(hash1)\n> >\n> > The random oracle could be block hashes. They could be matched to\n> > hash1, the sortition ticket. A red-team-concurrency difficulty\n> > parameter could control how many least-significant bits must match to\n> > be secretly selected. The difficulty parameter could be a matter of\n> > group consensus at the ceremonial intervals, based on a group decision\n> > on how much positive effect the Red Team exercise is providing.\n> >\n> > Upon assignment, the dev would have community approval to\n> > opportunistically insert a security flaw; which, when either caught,\n> > merged, or on timeout, they would reveal along with the sortition\n> > ticket that hashes to their public precommitment.\n> >\n> > Sortition Precommitment Day might be once or twice a year.\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-04T03:59:34",
                "message_text_only": "Good morning Luke,\n\n> All attempts are harmful, no matter the intent, in that they waste\n> contributors' time that could be better spent on actual development.\n>\n> However, I do also see the value in studying and improving the review process\n> to harden it against such inevitable attacks. The fact that we know the NSA\n> engages in such things, and haven't caught one yet should be a red flag.\n\nIndeed, I believe we should take the position that \"review process is as much a part of the code as the code itself, and should be tested regularly\".\n\n> Therefore, I think any such a scheme needs to be at least opt-out, if not\n> opt-in. Please ensure there's a simple way for developers with limited time\n> (or other reasons) to be informed of which PRs to ignore to opt-out of this\n> study. (Ideally it would also prevent maintainers from merging - maybe\n> possible since we use a custom merging script, but what it really needs to\n> limit is the push, not the dry-run.)\n\nAssuming developers are normal humans with typical human neurology (in particular a laziness circuit), perhaps this would work?\n\nEvery commit message is required to have a pair of 256-bit hex words.\n\nPublic attempts at attack / testing of the review process will use the first 256-bit as a salt, and when the salt is prepended to the string \"THIS IS AN ATTACK\" and then hashed with e.g. SHA256, should result in the second 256-bit word.\n\nNon-attacks / normal commits just use random 256-bit numbers.\n\nThose opting-out to this will run a script that checks commit messages for whether the first 256-bit hexword concatenated with \"THIS IS AN ATTACK\", then hashed, is the second 256-bit hexword.\n\nThose opting-in will not run that script and ignore the numbers.\n\nThe script can be run as well at the maintainer.\n\nHopefully, people who are not deliberately opting out will be too lazy to run the script (as is neurotypical for humans) and getting \"spoilered\" on this.\n\n***HOWEVER***\n\nWe should note that a putative NSA attack would of course not use the above protocol, and thus no developer can ever opt out of an NSA attempt at inserting vulnerabilities; thus, I think it is better if all developers are forced to opt in on the \"practice rounds\", as they cannot opt out of \"the real thing\" other than to stop developing entirely.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Mock introducing vulnerability in important Bitcoin projects",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Ryan Grant",
                "Manuel Costa",
                "ZmnSCPxj",
                "Luke Dashjr",
                "Prayank"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 29422
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "Erik Aronesty",
                "date": "2021-10-01T13:40:06",
                "message_text_only": "mostly thinking out loud\n\nsuppose there is a \"lightweight\" node:\n\n1. ignores utxo's below the dust limit\n2. doesn't validate dust tx\n3. still validates POW, other tx, etc.\n\nthese nodes could possibly get forked - accepting a series of valid,\nmined blocks where there is an invalid but ignored dust tx, however\nthis attack seems every bit as expensive as a 51% attack\n\nOn Fri, Oct 1, 2021 at 3:45 AM Pieter Wuille via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Jumping in late to this thread.\n>\n> I very much agree with how David Harding presents things, with a few comments inline.\n>\n> \u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\n> On Sunday, August 8th, 2021 at 5:51 PM, David A. Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > > 1.  it's not our business what outputs people want to create\n> >\n> > Every additional output added to the UTXO set increases the amount of\n> > work full nodes need to do to validate new transactions. For miners\n> > for whom fast validation of new blocks can significantly affect their\n> > revenue, larger UTXO sets increase their costs and so contributes\n> > towards centralization of mining.\n> > Allowing 0-value or 1-sat outputs minimizes the cost for polluting the\n> > UTXO set during periods of low feerates.\n> > If your stuff is going to slow down my node and possibly reduce my\n> > censorship resistance, how is that not my business?\n>\n> Indeed - UTXO set size is an externality that unfortunately Bitcoin's consensus rules fail to account\n> for. Having a relay policy that avoids at the very least economically irrational behavior makes\n> perfect sense to me.\n>\n> It's also not obvious how consensus rules could deal with this, as you don't want consensus rules\n> with hardcoded prices/feerates. There are possibilities with designs like transactions getting\n> a size/weight bonus/penalty, but that's both very hardforky, and hard to get right without\n> introducing bad incentives.\n>\n> > > 2.  dust outputs can be used in various authentication/delegation smart\n> > >     contracts\n> >\n> > > 3.  dust sized htlcs in lightning (\n> > >     https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light)\n> > >     force channels to operate in a semi-trusted mode\n> >\n> > > 4.  thinly divisible colored coin protocols might make use of sats as value\n> > >     markers for transactions.\n>\n> My personal, and possibly controversial, opinion is that colored coin protocols have no business being on the Bitcoin chain, possibly\n> beyond committing to an occasional batched state update or so. Both because there is little benefit for tokens with a trusted\n> issuer already, and because it competes with using Bitcoin for BTC - the token that pays for its security (at least as long as\n> the subsidy doesn't run out).\n>\n> Of course, personal opinions are no reason to dictate what people should or can use the chain for, but I do think it's reason to\n> voice hesitancy to worsening the system's scalability properties only to benefit what I consider misguided use.\n>\n> > > 5.  should we ever do confidential transactions we can't prevent it without\n> > >     compromising privacy / allowed transfers\n> >\n> > I'm not an expert, but it seems to me that you can do that with range\n> > proofs. The range proof for >dust doesn't need to become part of the\n> > block chain, it can be relay only.\n>\n> Yeah, range proofs have a non-hidden range; the lower bound can be nonzero, which could be required as part of a relay policy.\n>\n> Cheers,\n>\n> --\n> Pieter\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "shymaa arafat",
                "date": "2021-10-07T09:13:33",
                "message_text_only": "If u allow me to discuss,,,\nI previously suggested storing dust UTXOS in a separate Merkle tree or\nstrucutre in general if we are talking about the original set.\nI'm a kind of person who doesn't like to throw any thing; if it's not\nneeded now keep it in the basement for example.\nSo, if dust UTXOS making a burden keep them in secondary storage, where in\nsuch cases u can verify then delete them.\n\n\n\nOn Thu, Oct 7, 2021, 06:52 ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning e,\n>\n> > mostly thinking out loud\n> >\n> > suppose there is a \"lightweight\" node:\n> >\n> > 1.  ignores utxo's below the dust limit\n> > 2.  doesn't validate dust tx\n> > 3.  still validates POW, other tx, etc.\n> >\n> >     these nodes could possibly get forked - accepting a series of valid,\n> >     mined blocks where there is an invalid but ignored dust tx, however\n> >     this attack seems every bit as expensive as a 51% attack\n>\n> How would such a node treat a transaction that spends multiple dust UTXOs\n> and creates a single non-dust UTXO out of them (after fees)?\n> Is it valid (to such a node) or not?\n>\n> I presume from #1 it never stores dust UTXOs, so the node cannot know if\n> the UTXO being spent by such a tx is spending dust, or trying to spend an\n> already-spent TXO, or even inventing a TXO out of `/dev/random`.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/3889c77d/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-07T10:01:53",
                "message_text_only": "Good morning shymaa\n\n> If u allow me to discuss,,,\n> I previously suggested storing dust UTXOS in a separate Merkle tree or strucutre in general if we are talking about the original set.\n> I'm a kind of person who doesn't like to throw any thing; if it's not needed now keep it in the basement for example.\u00a0\n> So, if dust UTXOS making a burden keep them in secondary storage, where in such cases u can verify then delete them.\n\nWhile this technique helps reduce *average* CPU cost, it does not reduce *worst-case* CPU cost (and if the secondary storage trades off to gain increased capacity per satoshi by sacrificing speed, it can worse the worst-case time).\n\nIt is helpful to remember that attacks will always target worst-case behavior.\nThis is why quicksort is strongly disrecommended for processing data coming from external sources, its worst-case time is O(n^2).\nAnd we should switch to algorithms like mergesort or similar whose average times are generally worse than quicksort but have the major advantage of keeping an O(n log n) worst-case.\n\nMoving data we think is unlikely to be referenced to secondary storage (presumably in a construction that is slower but gets more storage per economic unit) moves us closer to quicksort than mergesort, and we should avoid quicksort-like solutions as it is always the worst-case behavior that is targeted in attacks.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "shymaa arafat",
                "date": "2021-10-08T07:44:59",
                "message_text_only": "The suggested idea I was replying to is to make all dust TXs invalid by\nsome nodes. I suggested a compromise by keeping them in secondary storage\nfor full nodes, and in a separate Merkle Tree for bridge servers.\n-In bridge servers they won't increase any worstcase, on the contrary this\nwill enhance the performance even if slightly.\n-In full nodes, and since they will usually appear in clusters, they will\nbe fetched rarely (either by a dust sweeping action, or a malicious\nattacker)\nIn both cases as a batch\n-To not exhaust the node with DoS(as the reply mentioned)one may think of\nuploading the whole dust partition if they were called more than certain\nthreshold (say more than 1 Tx in a block)\n-and then keep them there for \"a while\", but as a separate partition too to\nexclude them from any caching mechanism after that block.\n-The \"while\" could be a tuned parameter.\n-Take care that the more dust is sweeped, the less dust to remain in the\nUTXO set; as users are already much dis-incentivised to create more.\n.\nThanks for allowing the reply\n\nOn Thu, Oct 7, 2021, 16:43 ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n>\n>\n> > I don't know what brings up sorting here, unless as an example.\n>\n> Yes, it is an example: quicksort is bad for network-facing applications\n> because its ***worst-case behavior*** is bad.\n> Bitcoin is a network-facing application, and similarly, ***worst-case\n> behavior*** being bad is something that would strongly discourage\n> particular approaches.\n> Your proposal risks bad ***worst-case behavior***.\n>\n> > Anyways, I was comparing to rejecting them completely, not to keeping\n> them in one set. In addition, those dust sweep Transactions will probably\n> be a dust sweep and thus contain so many inputs which \"maybe\" makes 1-one\n> disk visit  to fetch all their hashes at once, 2-from a smaller subset with\n> max size 5-10% the UTXO set, justifiable.\n>\n> Do not consider the ***average case*** where a block is composed of only a\n> few dust sweep transactions and most transactions are normal,\n> non-dust-sweep transactions.\n>\n> Instead, consider the ***worst case*** where ***all*** transactions in a\n> block are dust sweep transactions, because that is what attackers will use.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211008/91bd7b81/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-08T10:38:50",
                "message_text_only": "Good morning shymaa,\n\n> The suggested idea I was replying to is to make all dust TXs invalid by some nodes.\n\nIs this supposed to be consensus change or not?\nWhy \"some\" nodes and not all?\n\nI think the important bit is for full nodes.\nNon-full-nodes already work at reduced security; what is important is the security-efficiency tradeoff.\n\n> I suggested a compromise by keeping them in secondary storage for full nodes, and in a separate Merkle Tree for bridge servers.\n> -In bridge servers they won't increase any worstcase, on the contrary this will enhance the performance even if slightly.\n> -In full nodes, and since they will usually appear in clusters, they will be fetched rarely (either by a dust sweeping action, or a malicious attacker)\n> In both cases as a batch\n> -To not exhaust the node with DoS(as the reply mentioned)one may think of uploading the whole dust partition if they were called more than certain threshold (say more than 1 Tx in a block)\u00a0\u00a0\n> -and then keep them there for \"a while\", but as a separate partition too to exclude them from any caching mechanism after that block.\n> -The \"while\" could be a tuned parameter.\n\nAssuming you meant \"dust tx is considered invalid by all nodes\".\n\n* Block has no dust sweep\n  * With dust rejected: only non-dust outputs are accessed.\n  * With dust in secondary storage: only non-dust outputs are accessed.\n* Block has some dust sweeps\n  * With dust rejected: only non-dust outputs are accessed, block is rejected.\n  * With dust in secondary storage: some data is loaded from secondary storage.\n* Block is composed of only dust sweeps\n  * With dust rejected: only non-dust outputs are accessed, block is rejected.\n  * With dust in secondary storage: significant increase in processing to load large secondary storage in memory,\n\nSo I fail to see how the proposal ever reduces processing compared to the idea of just outright making all dust txs invalid and rejecting the block.\nPerhaps you are trying to explain some other mechanism than what I understood?\n\nIt is helpful to think in terms always of worst-case behavior when considering resistance against attacks.\n\n> -Take care that the more dust is sweeped, the less dust to remain in the UTXO set; as users are already much dis-incentivised to create more.\n\nBut creation of dust is also as easy as sweeping them, and nothing really prevents a block from *both* creating *and* sweeping dust, e.g. a block composed of 1-input-1-output transactions, unless you want to describe some kind of restriction here?\n\nSuch a degenerate block would hit your secondary storage double: one to read, and one to overwrite and add new entries; if the storage is large then the index structure you use also is large and updates can be expensive there as well.\n\n\nAgain, I am looking solely at fullnode efficiency here, meaning all rules validated and all transactions validated, not validating and simply accepting some transactions as valid is a degradation of security from full validation to SPV validation.\nNow of course in practice modern Bitcoin is hard to attack with *only* mining hashpower as there are so many fullnodes that an SPV node would be easily able to find the \"True\" history of the chain.\nHowever, as I understand it that proporty of fullnodes protecting against attacks on SPV nodes only exists due to fullnodes being cheap to keep online; if the cost of fullnodes in the **worst case** (***not*** average, please stop talking about average case) increases then it may become feasible for miners to attack SPV nodes.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-08T22:47:11",
                "message_text_only": "Good morning Pieter,\n\n> Indeed - UTXO set size is an externality that unfortunately Bitcoin's consensus rules fail to account\n> for. Having a relay policy that avoids at the very least economically irrational behavior makes\n> perfect sense to me.\n>\n> It's also not obvious how consensus rules could deal with this, as you don't want consensus rules\n> with hardcoded prices/feerates. There are possibilities with designs like transactions getting\n> a size/weight bonus/penalty, but that's both very hardforky, and hard to get right without\n> introducing bad incentives.\n\nWhy is a +weight malus *very* hardforky?\n\nSuppose a new version of a node adds, say, +20 sipa per output of a transaction (in order to economically discourage the creation of additional outputs in the UTXO set).\nOlder versions would see the block as being lower weight than usual, but as the consensus rule is \"smaller than 4Msipa\" they should still accept any block acceptable to newer versions.\n\nIt seems to me that only a -weight bonus is hardforky (but then xref SegWit and its -weight bonus on inputs).\n\nI suppose the effect is primarily felt on mining nodes?\nMiners might refuse to activate such a fork, as they would see fewer transactions per block on average?\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Erik Aronesty",
                "shymaa arafat"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 14136
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev]   Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-07T04:52:01",
                "message_text_only": "Good morning e,\n\n> mostly thinking out loud\n>\n> suppose there is a \"lightweight\" node:\n>\n> 1.  ignores utxo's below the dust limit\n> 2.  doesn't validate dust tx\n> 3.  still validates POW, other tx, etc.\n>\n>     these nodes could possibly get forked - accepting a series of valid,\n>     mined blocks where there is an invalid but ignored dust tx, however\n>     this attack seems every bit as expensive as a 51% attack\n\nHow would such a node treat a transaction that spends multiple dust UTXOs and creates a single non-dust UTXO out of them (after fees)?\nIs it valid (to such a node) or not?\n\nI presume from #1 it never stores dust UTXOs, so the node cannot know if the UTXO being spent by such a tx is spending dust, or trying to spend an already-spent TXO, or even inventing a TXO out of `/dev/random`.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "LORD HIS EXCELLENCY JAMES HRMH",
                "date": "2021-10-07T08:17:40",
                "message_text_only": "Good Afternoon,\n\nReturning to this subject, there should be no restriction to the value of utxo's that keep in one's own wallet as change can be created in any value. With obvious intent, the wallet should avoid creating utxo's below the current dust limit at the time the transaction is created but it cannot guarantee it.\n\nThe wallet should avoid including utxo's that by weight sat/KB are more expensive to include that their value at the time a transaction is created, ie. do not include utxo's in a transaction that lower the input value after fees for automatic utxo selection, however, perhaps consider this is valid for manual utxo selection since it is in every example 'my money' and I can spend some of it if I decide.\n\nThere is no discipline in complaining that the dust set of utxo's slows down the process of block validation during mining. Every conceivable computerised business bears the expense of the cost of a database transaction. The actual answer to this genuine business concern of database speed is to build a faster database.\n\nIt is correct knowledge to know that the Bitcoin protocol cannot speculate as to the future but we can. The case exists where it is conceivable for example, that the transaction fee is paid only for the first utxo inclusion in a transaction due to changes to the calculation of block-size. There are other easily plausible examples where the inclusion of what is today considered dust may not be ill-considered.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/c2aee87a/attachment-0001.html>"
            },
            {
                "author": "LORD HIS EXCELLENCY JAMES HRMH",
                "date": "2021-10-07T08:34:17",
                "message_text_only": "Good Afternoon,\n\nThe underlying consideration is the same concerning the handling of 1c and 2c coins in an economy. Although you may argue the cost of counting those coins throughout the course of minting, drafting to banks, paying to bank customers, including in change, and at every handling counting, is less than the value of those coins, hpwever, the solution in traditional currency is to round the value of the transaction either per line of goods or per total before calculating the Grand Total, in which case the payment either from a non-utxo set of accumulation in a traditional account or, from a known series of denominations, is adjusted.\n\nIn the case of Bitcoin, the denominations available are effectively the utxo set and there is no effective way to round the transactions without accepting overpayments as valid, and with what consideration, in which case the protocol may avoid creating dust in change by sending the additional rounded amount that would otherwise be dust to the recipient.\n\nI suppose that this gets difficult where the transaction has multiple outputs and you could argue to distribute to all outputs as an overpayment. It is the same effectively as rounding to 10c.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n\n\n________________________________\nFrom: LORD HIS EXCELLENCY JAMES HRMH <willtech at live.com.au>\nSent: Thursday, 7 October 2021 7:17 PM\nTo: Erik Aronesty <erik at q32.com>; ZmnSCPxj <ZmnSCPxj at protonmail.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\nCc: lightning-dev <lightning-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit\n\nGood Afternoon,\n\nReturning to this subject, there should be no restriction to the value of utxo's that keep in one's own wallet as change can be created in any value. With obvious intent, the wallet should avoid creating utxo's below the current dust limit at the time the transaction is created but it cannot guarantee it.\n\nThe wallet should avoid including utxo's that by weight sat/KB are more expensive to include that their value at the time a transaction is created, ie. do not include utxo's in a transaction that lower the input value after fees for automatic utxo selection, however, perhaps consider this is valid for manual utxo selection since it is in every example 'my money' and I can spend some of it if I decide.\n\nThere is no discipline in complaining that the dust set of utxo's slows down the process of block validation during mining. Every conceivable computerised business bears the expense of the cost of a database transaction. The actual answer to this genuine business concern of database speed is to build a faster database.\n\nIt is correct knowledge to know that the Bitcoin protocol cannot speculate as to the future but we can. The case exists where it is conceivable for example, that the transaction fee is paid only for the first utxo inclusion in a transaction due to changes to the calculation of block-size. There are other easily plausible examples where the inclusion of what is today considered dust may not be ill-considered.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/89937fd1/attachment-0001.html>"
            },
            {
                "author": "LORD HIS EXCELLENCY JAMES HRMH",
                "date": "2021-10-07T10:35:16",
                "message_text_only": "Good Afternoon,\n\nFurther, if it is entirely necessary to prevent the creation of utxo's that are considered dust, and I am not by any means convinced, then it is simple to provide the most circumspect solution to transfer the value of any dust utxo that would be created in a transaction to the fee. I do not believe this answer is any more than robbery of the future value of the wallet as my wallet must be able to keep any change but if it is must then this is the answer.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n________________________________\nFrom: LORD HIS EXCELLENCY JAMES HRMH <willtech at live.com.au>\nSent: Thursday, 7 October 2021 7:34 PM\nTo: Erik Aronesty <erik at q32.com>; ZmnSCPxj <ZmnSCPxj at protonmail.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\nCc: lightning-dev <lightning-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit\n\nGood Afternoon,\n\nThe underlying consideration is the same concerning the handling of 1c and 2c coins in an economy. Although you may argue the cost of counting those coins throughout the course of minting, drafting to banks, paying to bank customers, including in change, and at every handling counting, is less than the value of those coins, hpwever, the solution in traditional currency is to round the value of the transaction either per line of goods or per total before calculating the Grand Total, in which case the payment either from a non-utxo set of accumulation in a traditional account or, from a known series of denominations, is adjusted.\n\nIn the case of Bitcoin, the denominations available are effectively the utxo set and there is no effective way to round the transactions without accepting overpayments as valid, and with what consideration, in which case the protocol may avoid creating dust in change by sending the additional rounded amount that would otherwise be dust to the recipient.\n\nI suppose that this gets difficult where the transaction has multiple outputs and you could argue to distribute to all outputs as an overpayment. It is the same effectively as rounding to 10c.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n\n\n________________________________\nFrom: LORD HIS EXCELLENCY JAMES HRMH <willtech at live.com.au>\nSent: Thursday, 7 October 2021 7:17 PM\nTo: Erik Aronesty <erik at q32.com>; ZmnSCPxj <ZmnSCPxj at protonmail.com>; Bitcoin Protocol Discussion <bitcoin-dev at lists.linuxfoundation.org>\nCc: lightning-dev <lightning-dev at lists.linuxfoundation.org>\nSubject: Re: [bitcoin-dev] [Lightning-dev] Removing the Dust Limit\n\nGood Afternoon,\n\nReturning to this subject, there should be no restriction to the value of utxo's that keep in one's own wallet as change can be created in any value. With obvious intent, the wallet should avoid creating utxo's below the current dust limit at the time the transaction is created but it cannot guarantee it.\n\nThe wallet should avoid including utxo's that by weight sat/KB are more expensive to include that their value at the time a transaction is created, ie. do not include utxo's in a transaction that lower the input value after fees for automatic utxo selection, however, perhaps consider this is valid for manual utxo selection since it is in every example 'my money' and I can spend some of it if I decide.\n\nThere is no discipline in complaining that the dust set of utxo's slows down the process of block validation during mining. Every conceivable computerised business bears the expense of the cost of a database transaction. The actual answer to this genuine business concern of database speed is to build a faster database.\n\nIt is correct knowledge to know that the Bitcoin protocol cannot speculate as to the future but we can. The case exists where it is conceivable for example, that the transaction fee is paid only for the first utxo inclusion in a transaction due to changes to the calculation of block-size. There are other easily plausible examples where the inclusion of what is today considered dust may not be ill-considered.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211007/ad09e939/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "LORD HIS EXCELLENCY JAMES HRMH"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 12354
        }
    },
    {
        "title": "[bitcoin-dev] Replacement transaction and ancestor score bug",
        "thread_messages": [
            {
                "author": "Prayank",
                "date": "2021-10-01T14:53:59",
                "message_text_only": "This pull request was mentioned in the thread: \"Proposal: Package Mempool Accept and Package RBF\" however I am not sure if everyone would have read all the emails if they were not interested in packages. Also not possible to keep track of each pull request in Bitcoin Core repository.\n\nPR: https://github.com/bitcoin/bitcoin/pull/23121\n\nI came across this pull request while reviewing few others and found the details in comments interesting. I think everyone who is using Bitcoin and RBF should be aware of this bug. I won't add my opinions or comments, copying few lines from PR:\n\n> However, Example N shows how this strategy can cause us to accept an replacement transaction that is actually less economical to mine than the original. Assume all transactions have a vsize of 100vB. A user wants to replace A, which has an ancestor score of 10sat/vB, with transaction C. Suppose they want to spend an unconfirmed output from transaction B, which has an ancestor score of 1sat/vB (maybe their wallet doesn't have enough funds to provide a higher fee using only confirmed inputs). BIP125#2 prevents scenario N1, where the inclusion of another unconfirmed input means C has an ancestor score of 8sat/vB and thus less economical to mine than A. However, it does not prevent scenario M2, where the user splits off a 1-input 1-output transaction, C*, in order to be able to include the output from B. This causes us to incorrectly accept C (7.5sat/vB including its parent B) in favor of A (10sat/vB).\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211001/8a8bbfbd/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Replacement transaction and ancestor score bug",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1714
        }
    },
    {
        "title": "[bitcoin-dev] Interrogating a BIP157 server, BIP158 change proposal",
        "thread_messages": [
            {
                "author": "Dustin Dettmer",
                "date": "2021-10-03T09:53:00",
                "message_text_only": "Jim Posen,\n\nA few years ago you mentioned roastbeef\u2019s proposal of a P2P message to\nretrieve all prev-outputs for a given block:\n\n1) Introduce a new P2P message to retrieve all prev-outputs for a given\n> block (essentially the undo data in Core), and verify the scripts against\n> the block by executing them. While this permits some forms of input script\n> malleability (and thus cannot discriminate between all valid and invalid\n> filters), it restricts what an attacker can do. This was proposed by Laolu\n> AFAIK, and I believe this is how btcd is proceeding.\n>\n\nI\u2019m trying to find the follow up on this. Was there discussion about it\nunder another name (thread, PR, bip etc)? Apologies if I\u2019m being obtuse and\nit\u2019s easily found but for the life of me I can\u2019t find any references.\nBip157 seems to not make any mention of it.\n\nThanks!\nDustin\n\n>\n\n\nIf anyone has other ideas, I'd love to hear them.\n>\n> -jimpo\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html\n>\n>\n>\n> On Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> TLDR: a change to BIP158 would allow decision on which filter chain is\n>> correct at lower bandwith use\n>>\n>> Assume there is a BIP157 client that learned a filter header chain\n>> earlier and is now offered an alternate reality by a newly connected BIP157\n>> server.\n>>\n>> The client notices the alternate reality by routinely asking for filter\n>> chain checkpoints after connecting to a new BIP157 server. A divergence at\n>> a checkpoint means that the server disagrees the client's history at or\n>> before the first diverging checkpoint. The client would then request the\n>> filter headers between the last matching and first divergent checkpoint,\n>> and quickly figure which block\u2019s filter is the first that does not match\n>> previous assumption, and request that filter from the server.\n>>\n>> The client downloads the corresponding block, checks that its header fits\n>> the PoW secured best header chain, re-calculates merkle root of its\n>> transaction list to know that it is complete and queries the filter to see\n>> if every output script of every transaction is contained in there, if not\n>> the server is lying, the case is closed, the server disconnected.\n>>\n>> Having all output scripts in the filter does not however guarantee that\n>> the filter is correct since it might omit input scripts. Inputs scripts are\n>> not part of the downloaded block, but are in some blocks before that.\n>> Checking those are out of reach for lightweight client with tools given by\n>> the current BIP.\n>>\n>> A remedy here would be an other filter chain on created and spent\n>> outpoints as is implemented currently by Murmel. The outpoint filter chain\n>> must offer a match for every spent output of the block with the divergent\n>> filter, otherwise the interrogated server is lying since a PoW secured\n>> block can not spend coins out of nowhere. Doing this check would already\n>> force the client to download the outpoint filter history up-to the point of\n>> divergence. Then the client would have to download and PoW check every\n>> block that shows a match in outpoints until it figures that one of the\n>> spent outputs has a script that was not in the server\u2019s filter, in which\n>> case the server is lying. If everything checks out then the previous\n>> assumption on filter history was incorrect and should be replaced by the\n>> history offered by the interrogated server.\n>>\n>> As you see the interrogation works with this added filter but is highly\n>> ineffective. A really light client should not be forced to download lots of\n>> blocks just to uncover a lying filter server. This would actually be an\n>> easy DoS on light BIP157 clients.\n>>\n>> A better solution is a change to BIP158 such that the only filter\n>> contains created scripts and spent outpoints. It appears to me that this\n>> would serve well both wallets and interrogation of filter servers well:\n>>\n>> Wallets would recognize payments to their addresses by the filter as\n>> output scripts are included, spends from the wallet would be recognized as\n>> a wallet already knows outpoints of its previously received coins, so it\n>> can query the filters for them.\n>>\n>> Interrogation of a filter server also simplifies, since the filter of the\n>> block can be checked entirely against the contents of the same block. The\n>> decision on filter correctness does not require more bandwith then download\n>> of a block at the mismatching checkpoint. The client could only be forced\n>> at max. to download 1/1000 th of the blockchain in addition to the filter\n>> header history.\n>>\n>> Therefore I suggest to change BIP158 to have a base filter, defined as:\n>>\n>> A basic filter MUST contain exactly the following items for each\n>> transaction in a block:\n>>         \u2022 Spent outpoints\n>>         \u2022 The scriptPubKey of each output, aside from all OP_RETURN\n>> output scripts.\n>>\n>> Tamas Blummer\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211003/d02b1381/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Interrogating a BIP157 server, BIP158 change proposal",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Dustin Dettmer"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 5539
        }
    },
    {
        "title": "[bitcoin-dev] Wednesday\u2019s second BIP process meeting",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2021-10-03T18:22:12",
                "message_text_only": "Wednesday\u2019s second BIP process meeting was announced previously here [0].\n\nA conversation log of the meeting is available here [1].\n\nA summary of the first BIP process meeting is here [2].\n\nThe following is a summary of what was discussed.\n\n1) The limits or possible downsides of pursuing maximal decentralization. Understandably there is a desire for greater decentralization as a central repo introduces bottlenecks and the need for maintainers or editors in the case of BIPs (prayank). However, zero filters creates a Ethereum style bewildering number of BIPs of varying quality that all need to be stored and maintained. The option of being able to store a BIP in any repo doesn\u2019t appear to offer material upside (michaelfolkson). It still needs to get a BIP number from the BIP editors and if the alternative repo is deleted or the BIP champion becomes unresponsive there is the problem of changing the location of where the BIP is stored. It is much easier to monitor a single repo rather than an infinite number of repos that contain BIPs.\n\n2) Past motivations for introducing alternative parallel processes to the BIPs (e.g. BOLTs, SLIPs). Anyone is free to set up a repo, add documentation to that repo or even set up a parallel process to the BIPs. However, if as a community we\u2019d like to prevent unnecessary splintering it is good to understand why certain documents that should be BIPed aren\u2019t being BIPed. Obviously not every document needs or should be BIPed. There are many docs that aren\u2019t BIPs that are hugely valuable. One historical concern that was raised (ChristopherA) was regarding why BIP 48 didn\u2019t happen and whether it was because it was coming from the wallet community and not a Bitcoin Core proposal. luke-jr said after the meeting that from recollection the reason it didn\u2019t happen was merely that it was never written [3]. It was also discussed afterwards whether there was/is a rationale for Lightning BOLTs not being BIPs and whether they should be BIPs in future.\n\n3) Kalle Alm\u2019s proposal for BIP extensions [4] was discussed. Participants seemed to be in favor of the proposal though further clarity on when they would and wouldn\u2019t be used was requested. A BIP extension for the bech32m update (BIP 350) to bech32 (BIP 173) seems like it would have been a good use case though further discussion is probably needed on whether they should be used for soft fork activation parameters. It was suggested that using dates and/or short extension names would be superior to extension numbers as numbers suggest an ordering (ChristopherA). The extension 2 may also be perceived as somehow replacing extension 1.\n\nThanks to the participants of both BIP process meetings. Further discussion is welcome on the #bitcoin-dev Libera IRC channel and hopefully we will see progress in the coming weeks and months on a proposed BIP 3 [5] update.\n\n[0]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019412.html\n\n[1]: https://gist.github.com/michaelfolkson/84000ee3fe45c034ac12a7a54ff5fcdd\n\n[2]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019469.html\n\n[3]: https://github.com/bitcoin/bips/pull/253\n\n[4]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-September/019457.html\n\n[5]: https://github.com/bitcoin/bips/pull/1015\n\n--\n\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211003/f0e74574/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-10-06T04:02:55",
                "message_text_only": "Good morning Michael,\n\nThanks for sharing the summary about BIP process meeting. \n\n> However, zero filters creates a Ethereum style bewildering number of BIPs of varying quality that all need to be stored and maintained. The option of being able to store a BIP in any repo doesn\u2019t appear to offer material upside (michaelfolkson). It still needs to get a BIP number from the BIP editors and if the alternative repo is deleted or the BIP champion becomes unresponsive there is the problem of changing the location of where the BIP is stored. It is much easier to monitor a single repo rather than an infinite number of repos that contain BIPs.\n\n1.I want to avoid mentioning projects that are not decentralized however the thing you mentioned is a feature not a bug. Neither anyone needs \"quality\" certificates from anyone nor approval. People are free to propose anything as improvement for Bitcoin. What gets implemented is a different thing. Also BIP number doesn't make something legit, BIPs can have any names. Example: If I ever create draft a proposal to improve Bitcoin, it will be in my own repository and with a unique name.\n\n2.I am surprised that few influential developers that wanted to improve BIP process earlier by making it more decentralized were not present in either meeting. Also no follow up here on mailing list. So decentralization was only required when you had some issues with Luke Dashjr? Few things are so obvious that even a newbie who starts researching about Bitcoin from today can observe such things.\n\nI tried my best to ask more people to participate in the meeting by tweeting, requested Christopher to attend the meeting and share his thoughts. Thanks everyone who was part of this meeting.\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211006/751c44e4/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Wednesday\u2019s second BIP process meeting",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "Michael Folkson"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 5569
        }
    },
    {
        "title": "[bitcoin-dev] bitcoin-java, a new bitcoin library",
        "thread_messages": [
            {
                "author": "Humberto Marcolino",
                "date": "2021-10-05T00:23:38",
                "message_text_only": "Hello,\n\nMy name is Humberto, owner of the repository\nhttps://github.com/bitcoin-education/bitcoin-java.\n\nI'm posting to divulge a new open-source Bitcoin library written in Java,\nwith support for taproot single key transactions:\nhttps://github.com/bitcoin-education/bitcoin-java.\n\nMy main motivation to build this library was for educational purposes.\nAlso, I was missing a lean bitcoin library written in Java, since I think\nbitcoinj too feature-heavy. I don't intend to include features that allow\ncommunication with nodes nor any online features in it. I think it is ideal\nfor developers that want to build a wallet in Java.\n\nFeedback, PRs, and issues are welcome and appreciated.\n\nWebsite of the project, where I plan to post more examples using it:\nhttps://www.bitcoineducation.site/\n\nBest regards,\n\nHumberto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211004/da58dc80/attachment.html>"
            },
            {
                "author": "Mohamed Youssef",
                "date": "2021-10-05T09:40:42",
                "message_text_only": "Thanks for posting; I have been looking for such a library to get into the technical details.\nDo you plan a slack or IRC for collaboration?\n\nBest,\nMohamed\n\nFrom: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> On Behalf Of Humberto Marcolino via bitcoin-dev\nSent: Tuesday, October 5, 2021 08:24 AM\nTo: bitcoin-dev at lists.linuxfoundation.org\nSubject: [bitcoin-dev] bitcoin-java, a new bitcoin library\n\nHello,\n\nMy name is Humberto, owner of the repository https://github.com/bitcoin-education/bitcoin-java.\n\nI'm posting to divulge a new open-source Bitcoin library written in Java, with support for taproot single key transactions: https://github.com/bitcoin-education/bitcoin-java.\n\nMy main motivation to build this library was for educational purposes. Also, I was missing a lean bitcoin library written in Java, since I think bitcoinj too feature-heavy. I don't intend to include features that allow communication with nodes nor any online features in it. I think it is ideal for developers that want to build a wallet in Java.\n\nFeedback, PRs, and issues are welcome and appreciated.\n\nWebsite of the project, where I plan to post more examples using it: https://www.bitcoineducation.site/\n\nBest regards,\n\nHumberto\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/77f2fa9e/attachment.html>"
            },
            {
                "author": "Humberto Marcolino",
                "date": "2021-10-05T22:58:41",
                "message_text_only": "Hi Mohamed,\n\nI created a discord server for the project, here is the link:\nhttps://discord.gg/vXK9JbCtvZ\n\nHumberto\n\nEm ter., 5 de out. de 2021 \u00e0s 06:40, Mohamed Youssef <MOHAMED_Y at msn.com>\nescreveu:\n\n> Thanks for posting; I have been looking for such a library to get into the\n> technical details.\n>\n> Do you plan a slack or IRC for collaboration?\n>\n>\n>\n> Best,\n>\n> Mohamed\n>\n>\n>\n> *From:* bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> *On\n> Behalf Of *Humberto Marcolino via bitcoin-dev\n> *Sent:* Tuesday, October 5, 2021 08:24 AM\n> *To:* bitcoin-dev at lists.linuxfoundation.org\n> *Subject:* [bitcoin-dev] bitcoin-java, a new bitcoin library\n>\n>\n>\n> Hello,\n>\n>\n>\n> My name is Humberto, owner of the repository\n> https://github.com/bitcoin-education/bitcoin-java.\n>\n>\n>\n> I'm posting to divulge a new open-source Bitcoin library written in Java,\n> with support for taproot single key transactions:\n> https://github.com/bitcoin-education/bitcoin-java.\n>\n>\n>\n> My main motivation to build this library was for educational purposes.\n> Also, I was missing a lean bitcoin library written in Java, since I think\n> bitcoinj too feature-heavy. I don't intend to include features that allow\n> communication with nodes nor any online features in it. I think it is ideal\n> for developers that want to build a wallet in Java.\n>\n>\n>\n> Feedback, PRs, and issues are welcome and appreciated.\n>\n>\n>\n> Website of the project, where I plan to post more examples using it:\n> https://www.bitcoineducation.site/\n>\n>\n>\n> Best regards,\n>\n>\n>\n> Humberto\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/4d9e7af9/attachment.html>"
            },
            {
                "author": "Steve Myers",
                "date": "2021-10-05T15:47:16",
                "message_text_only": "Ciao Humberto! building a new library is a great way to learn about bitcoin, you should also take a look at the Bitcoin Dev Kit project (https://github.com/bitcoindevkit) where we're building support for Kotlin/Java (and Android) and eventually Swift and iOS. You don't need to know Rust (though it's a great language to learn! ) and we'd love to have new folks join us as we develop the language bindings which is also a great way to learn the primitives for on-chain Bitcoin. \n\nSteve\n\nOn Mon, Oct 4, 2021, at 5:23 PM, Humberto Marcolino via bitcoin-dev wrote:\n> Hello,\n> \n> My name is Humberto, owner of the repository https://github.com/bitcoin-education/bitcoin-java.\n> \n> I'm posting to divulge a new open-source Bitcoin library written in Java, with support for taproot single key transactions: https://github.com/bitcoin-education/bitcoin-java.\n> \n> My main motivation to build this library was for educational purposes. Also, I was missing a lean bitcoin library written in Java, since I think bitcoinj too feature-heavy. I don't intend to include features that allow communication with nodes nor any online features in it. I think it is ideal for developers that want to build a wallet in Java.\n> \n> Feedback, PRs, and issues are welcome and appreciated.\n> \n> Website of the project, where I plan to post more examples using it: https://www.bitcoineducation.site/\n> \n> Best regards,\n> \n> Humberto\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/6f215d70/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "bitcoin-java, a new bitcoin library",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Mohamed Youssef",
                "Steve Myers",
                "Humberto Marcolino"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 5926
        }
    },
    {
        "title": "[bitcoin-dev] Question- must every mining rig attempt every block?",
        "thread_messages": [
            {
                "author": "Nathan T Alexander",
                "date": "2021-10-05T12:23:09",
                "message_text_only": "For purposes of conserving energy, couldn't each mining rig have some \nnon-gameable attribute which would be used to calculate if a block would \nbe accepted by that rig?\n\nDon't the mining rigs have to be able to identify themselves to the \nnetwork somehow, in order to claim their block reward? Could their \nbitcoin network ID be used as a non-gameable attribute?\n\nEssentially a green light / red light system. In order for a block to be \naccepted by the network, it must have all attributes of a successful \nblock today, and it must also have come from a rig that had a green light.\n\nPerhaps hash some data from the last successful block, along with the \nminers non-gameable attribute, and if it's below a certain number set by \nalgorithm, the miner gets a green light to race to produce a valid block.\n\nNathan Alexander\n\nArlington, TX"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-05T14:41:24",
                "message_text_only": "Good morning Nathan,\n\n> For purposes of conserving energy, couldn't each mining rig have some\n> non-gameable attribute which would be used to calculate if a block would\n> be accepted by that rig?\n>\n> Don't the mining rigs have to be able to identify themselves to the\n> network somehow, in order to claim their block reward? Could their\n> bitcoin network ID be used as a non-gameable attribute?\n\nThey are \"identified\" by the address that is on the coinbase output.\n\nThere is nothing preventing a *single* miner having *multiple* addresses, in much the same way that a *single* HODLer is not prevented from having *multiple* addresses.\n\n>\n> Essentially a green light / red light system. In order for a block to be\n> accepted by the network, it must have all attributes of a successful\n> block today, and it must also have come from a rig that had a green light.\n\nSince a miner can have multiple addresses, the miners can game this by simply grinding on *which* of their multiple addresses gets the green light.\nThat grinding is no more different in quality than grinding the block hash.\n\nThus, you just move proof-of-work elsewhere and make it harder to see, not reduce it.\n\n\nWorse, *identifying* miners reduces the important anonymity property of mining.\nWith non-anonymous mining, it is much easier for states to co-opt large mines, since they are identifiable, and states can target larger miners.\nThus, miners ***must*** use multiple addresses as a simple protection against state co-option.\n\n>\n> Perhaps hash some data from the last successful block, along with the\n> miners non-gameable attribute, and if it's below a certain number set by\n> algorithm, the miner gets a green light to race to produce a valid block.\n\nThe power consumption of proof-of-work ***is not a problem***, it is instead the solution against state co-option.\n\nIf you reduce the power consumption, it becomes easier for states to simply purchase and co-opt mines and attack the system, since it is easier to muster the power consumption and outright 51% Bitcoin.\nThe power consumption is an important security parameter, ***even more important than raw hashes-per-second***, since hashes-per-second will inevitably rise anyway even with constant power consumption.\n\nIt should always remain economically infeasible to 51% Bitcoin, otherwise Bitcoin will ***die*** and all your HODLings in it.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Ruben Somsen",
                "date": "2021-10-05T14:42:40",
                "message_text_only": "Hi Nathan,\n\nThat's a fair question, but note that we've already had a bunch of \"green\nmining\" related posts a few months ago, so I suspect you'll be able to find\nmany criticisms to this idea in the following thread:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018937.html\n\nIt also looks like you'll be able to find some related answers on Bitcoin\nStack Exchange:\nhttps://bitcoin.stackexchange.com/questions/106308/decreasing-energy-consumption-of-bitcoins-pow-with-paired-mining-rounds\n\nAnd generally speaking these types of discussions don't end up being very\nfruitful for bitcoin-dev, because these are the types of changes that are\nunlikely to ever be seriously considered for Bitcoin.\n\nCheers,\nRuben\n\nOn Tue, Oct 5, 2021 at 4:09 PM Nathan T Alexander via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> For purposes of conserving energy, couldn't each mining rig have some\n> non-gameable attribute which would be used to calculate if a block would\n> be accepted by that rig?\n>\n> Don't the mining rigs have to be able to identify themselves to the\n> network somehow, in order to claim their block reward? Could their\n> bitcoin network ID be used as a non-gameable attribute?\n>\n> Essentially a green light / red light system. In order for a block to be\n> accepted by the network, it must have all attributes of a successful\n> block today, and it must also have come from a rig that had a green light.\n>\n> Perhaps hash some data from the last successful block, along with the\n> miners non-gameable attribute, and if it's below a certain number set by\n> algorithm, the miner gets a green light to race to produce a valid block.\n>\n> Nathan Alexander\n>\n> Arlington, TX\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211005/82c3f918/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2021-10-08T15:08:02",
                "message_text_only": "Proof of stake systems attempt to create red light - green light types of\nthings with non-gameable attributes (eg collaborative random numbers). This\ncan't be done with mining because mining is completely random - its not\npossible to know which miner will mine a block. If it were, it wouldn't be\nproof of work, but something else. What you describe sounds like proof of\nidentity, which isn't possible in a decentralized adversarial environment.\nIn fact, one of the primary achievements of the Proof of Work consensus\nmechanism is to work around the Sybil issue, where (like ZmnSCPxj\nmentioned) a single user can have many identities.\n\nThere can be hybrid systems that use both proof of work and proof of stake,\nbut my conclusion after having done a lot of research and thinking about it\n([1]\n<https://github.com/fresheneesz/quantificationOfConsensusProtocolSecurity>,\n[2] <https://github.com/fresheneesz/proofOfTimeOwnership>) is that the\nsecurity mostly boils down to the weakest piece of the hybrid system, and\nso its not very effective to have hybrid systems like you mentioned.\n\nOn Tue, Oct 5, 2021 at 10:43 AM Ruben Somsen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Nathan,\n>\n> That's a fair question, but note that we've already had a bunch of \"green\n> mining\" related posts a few months ago, so I suspect you'll be able to find\n> many criticisms to this idea in the following thread:\n>\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/018937.html\n>\n> It also looks like you'll be able to find some related answers on Bitcoin\n> Stack Exchange:\n>\n> https://bitcoin.stackexchange.com/questions/106308/decreasing-energy-consumption-of-bitcoins-pow-with-paired-mining-rounds\n>\n> And generally speaking these types of discussions don't end up being very\n> fruitful for bitcoin-dev, because these are the types of changes that are\n> unlikely to ever be seriously considered for Bitcoin.\n>\n> Cheers,\n> Ruben\n>\n> On Tue, Oct 5, 2021 at 4:09 PM Nathan T Alexander via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> For purposes of conserving energy, couldn't each mining rig have some\n>> non-gameable attribute which would be used to calculate if a block would\n>> be accepted by that rig?\n>>\n>> Don't the mining rigs have to be able to identify themselves to the\n>> network somehow, in order to claim their block reward? Could their\n>> bitcoin network ID be used as a non-gameable attribute?\n>>\n>> Essentially a green light / red light system. In order for a block to be\n>> accepted by the network, it must have all attributes of a successful\n>> block today, and it must also have come from a rig that had a green light.\n>>\n>> Perhaps hash some data from the last successful block, along with the\n>> miners non-gameable attribute, and if it's below a certain number set by\n>> algorithm, the miner gets a green light to race to produce a valid block.\n>>\n>> Nathan Alexander\n>>\n>> Arlington, TX\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211008/1c5fa1e8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Question- must every mining rig attempt every block?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Nathan T Alexander",
                "Billy Tetrud",
                "Ruben Somsen"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 8829
        }
    },
    {
        "title": "[bitcoin-dev] Test cases for Taproot signature message",
        "thread_messages": [
            {
                "author": "Giacomo Caironi",
                "date": "2021-10-06T20:35:51",
                "message_text_only": "The related pull request is now open\nhttps://github.com/bitcoin/bips/pull/1191\n\nIl giorno sab 18 set 2021 alle ore 13:32 Giacomo Caironi <\ngiacomo.caironi at gmail.com> ha scritto:\n\n> Ok I have created three test cases, you can find them here\n> <https://gist.github.com/giacomocaironi/e41a45195b2ac6863ec46e8f86324757>.\n> They cover most of the SigMsg function but they don't cover the ext_flag,\n> so they are only for taproot key path; but if you want to test for script\n> paths you have to implement more than this function so you would use the\n> official test vector.\n> Could someone please take a look at them? I think that they are right but\n> I am not too sure\n>\n> Il giorno ven 17 set 2021 alle ore 00:30 Pieter Wuille <\n> bitcoin-dev at wuille.net> ha scritto:\n>\n>> On Thursday, September 16th, 2021 at 5:36 PM, Giacomo Caironi via\n>> bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> Hi,\n>> recently I have worked on a python implementation of bitcoin signature\n>> messages, and I have found that there was way better documentation about\n>> Segwit signature message than Taproot.\n>>\n>> 1) Segwit signature message got its own BIP, completed with test cases\n>> regarding only that specific function; Taproot on the other hand has the\n>> signature message function defined in BIP 341 and the test vectors in a\n>> different BIP (341). This is confusing. Shouldn't we create a different BIP\n>> only for Taproot signature message exactly like Segwit?\n>>\n>>\n>> I'm not entirely sure what you mean; you're saying BIP 341 twice.\n>>\n>> Still, you're right overall - there is no separate BIP for the signature\n>> message function. The reason is that the message function is different for\n>> BIP341 and BIP342. BIP 341 defines a basic common message function, which\n>> is then built up for BIP 341 key path spending, and for BIP 342 tapscript\n>> spending. This common part could have been a separate BIP, but that'd still\n>> not be a very clean separation. I'm not very inclined to support changing\n>> that at this point, given the state of deployment the BIPs have, but that\n>> doesn't mean the documentation/vectors can't be improved in the existing\n>> documents.\n>>\n>> 2) The test vectors for Taproot have no documentation and, most\n>> importantly, they are not atomic, in the sense that they do not target a\n>> specific part of the taproot code but all of it. This may not be a very big\n>> problem, but for signature verification it is. Because there are hashes\n>> involved, we can't really debug why a signature message doesn't pass\n>> validation, either it is valid or it is not. BIP 143 in this case is really\n>> good, because it provides hash preimages, so it is possible to debug the\n>> function and see where something went wrong. Because of this, writing the\n>> Segwit signature hash function took a fraction of the time compared to\n>> Taproot.\n>>\n>>\n>> You're right. The existing tests are really intended for verifying an\n>> implementation against (and for making sure future code changes don't break\n>> anything). They have much higher coverage than the segwit tests had. But\n>> they aren't useful as documentation; the code that generates them (\n>> https://github.com/bitcoin/bitcoin/blob/v22.0/test/functional/feature_taproot.py#L605L1122)\n>> is probably better at that even, but still pretty dense.\n>>\n>> If this idea is accepted I will be more than happy to write the test\n>> cases for Taproot.\n>>\n>>\n>> If you're interested in writing test vectors that are more aimed at\n>> helping debugging issues, by all means, do. You've already brought up the\n>> sighash code as an example. Another idea, primarily aimed at developers of\n>> signing code, is test vectors for certain P2TR scriptPubKeys, derived from\n>> certain internal keys and script trees. I'm happy to help to integrate such\n>> in Bitcoin Core and the BIP(s).\n>>\n>> Thanks!\n>>\n>> Cheers,\n>>\n>> --\n>> Pieter\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211006/8e1a35c2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Test cases for Taproot signature message",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Giacomo Caironi"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4088
        }
    },
    {
        "title": "[bitcoin-dev] Taproot testnet wallet",
        "thread_messages": [
            {
                "author": "Andreas Schildbach",
                "date": "2021-10-09T15:36:47",
                "message_text_only": "I'm trying to finish off bitcoinj's implementation for sending to \ntaproot addresses. For this, I'd like to test against a wallet that can \nreceive to P2TR and spend back.\n\nI've been trying to get a taproot address from Bitcoin Core 22.0 and \nspent many hours, but in vain. Can someone please simply send my a \ntestnet wallet that has at least one taproot address? (I don't care \nabout anyone stealing my testnet coins, so don't worry about the \ncompromised private key.)\n\nThanks!"
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-10-09T16:49:42",
                "message_text_only": "On Oct 9, 2021, 11:36, Andreas Schildbach via bitcoin-dev < bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I'm trying to finish off bitcoinj's implementation for sending to\n\ntaproot addresses. For this, I'd like to test against a wallet that can\n\nreceive to P2TR and spend back.\n\n> I've been trying to get a taproot address from Bitcoin Core 22.0 and\n\nspent many hours, but in vain. Can someone please simply send my a\n\ntestnet wallet that has at least one taproot address? (I don't care\n\nabout anyone stealing my testnet coins, so don't worry about the\n\ncompromised private key.)\n\nHi Andreas,\n\nYou can construct a taproot-capable wallet in Bitcoin Core as follows:\n\n* Have or create a descriptor wallet (createwallet RPC, with descriptors=true).\n\n* Import a taproot descriptor (of the form \"tr(KEY)\"), as active descriptor (with active=true), where KEY can be a tprv.../* or any other supported key expression.\n\n* Get a new address with addresstype=bech32m\n\nI've also created one myself for testing: tb1p84x2ryuyfevgnlpnxt9f39gm7r68gwtvllxqe5w2n5ru00s9aquslzggwq\n\nIf you send testnet coins there email me an address, I'll return them.\n\nCheers,\n\n--\n\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211009/ce17a6fa/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-10-15T01:05:12",
                "message_text_only": "On Sat, Oct 09, 2021 at 04:49:42PM +0000, Pieter Wuille via bitcoin-dev wrote:\n> You can construct a taproot-capable wallet in Bitcoin Core as follows:\n> * Have or create a descriptor wallet (createwallet RPC, with descriptors=true).\n> * Import a taproot descriptor (of the form \"tr(KEY)\"), as active descriptor\n> (with active=true), where KEY can be a tprv.../* or any other supported key\n> expression.\n> * Get a new address with addresstype=bech32m\n\nRunning master (which has PR#21500 merged), then the above can be\ndone with:\n\n1. create a descriptor wallet\n\n  bitcoin-cli -signet -named createwallet wallet_name=descwallet descriptors=true load_on_startup=true\n\n2. get the associated bip32 tprv private key\n\n  TPRV=$(bitcoin-cli -rpcwallet=descwallet -signet listdescriptors true | jq '.descriptors | .[].desc' | sed 's/^.*(//;s/[)/].*//' | uniq | head -n1)\n\n(This step requires PR#21500 to extract the wallet's tprv; you'll need to\nbe running an updated version of bitcoin-cli here as well as bitcoind. You\ncould also generate the tprv some other way.)\n\n3. construct the taproot descriptor per BIP 86\n\n  DESC=\"tr($TPRV/86'/1'/0'/0/*)\"\n  CHK=\"$(bitcoin-cli -rpcwallet=descwallet -signet getdescriptorinfo \"$DESC\" | jq -r .checksum)\"\n\n4. import the descriptor\n\n  bitcoin-cli -rpcwallet=descwallet -signet importdescriptors \"[{\\\"desc\\\": \\\"$DESC#$CHK\\\", \\\"active\\\": true, \\\"timestamp\\\": \\\"now\\\", \\\"range\\\": [0,1000], \\\"next_index\\\": 1}]\"\n\n5. get an address\n\n  bitcoin-cli -rpcwallet=descwallet -signet getnewaddress '' bech32m\n\nYou can then use the signet faucet to send a few million ssats to that\naddress directly.\n\nSame stuff works with testnet, though I'm not sure if any testnet faucets\nwill accept bech32m addresses directly.\n\nThis is all a bit deliberately cumbersome prior to taproot activating on\nmainnet; once that happens and PR#22364 is merged, you'll only need to\ndo steps (1) and (5).\n\nCheers,\naj"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-10-15T11:51:45",
                "message_text_only": "On 2021-10-15 03:05:36 user Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Same stuff works with testnet, though I'm not sure if any testnet faucets\nwill accept bech32m addresses directly.\n\nThere are faucets that accept such addresses, for example https://bitcoinfaucet.uo1.net/, but you have to use bech32 checksum instead of bech32m, for example sending to tb1p84x2ryuyfevgnlpnxt9f39gm7r68gwtvllxqe5w2n5ru00s9aqus27cytz works fine, but sending to tb1p84x2ryuyfevgnlpnxt9f39gm7r68gwtvllxqe5w2n5ru00s9aquslzggwq does not work. The same with pages like mempool.space, you have to use bech32 instead of bech32m, then you can send to taproot from some old faucets or browse taproot addresses from some old block explorers."
            },
            {
                "author": "Prayank",
                "date": "2021-10-15T14:12:23",
                "message_text_only": "Hi Andreas,\n\n> I'm trying to finish off bitcoinj's implementation for sending to \ntaproot addresses. For this, I'd like to test against a wallet that can\nreceive to P2TR and spend back.\n\nI did this transaction few days back which creates a P2TR output while answering a question on Bitcoin Stackexchange: https://blockstream.info/testnet/tx/2035ead4a9d0c8e2da1184924abc9034d26f2a7093371183ef12891623b235d1\n\nPieter Wuille and Anthony Towns already shared things that would be helpful. Still wanted to share the steps I did for above transaction:\n\nhttps://bitcoin.stackexchange.com/a/108013/\n\nTL;DR - \n\n1.Create a blank descriptor wallet with private keys disabled\n\n2.Import TPUB descriptor in wallet\n\n3.Create new bech32m address\n\n4.Send some bitcoin from another wallet\n\nThe answer could be improved if test vectors are added in BIP 86 for TPRV for follow things suggested by Anthony Towns in last email.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211015/6fc09173/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Taproot testnet wallet",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Andreas Schildbach",
                "Anthony Towns",
                "vjudeu at gazeta.pl",
                "Pieter Wuille",
                "Prayank"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 5620
        }
    },
    {
        "title": "[bitcoin-dev] Inherited IDs - A safer, more powerful alternative to BIP-118 (ANYPREVOUT) for scaling Bitcoin",
        "thread_messages": [
            {
                "author": "jlspc",
                "date": "2021-10-10T22:03:38",
                "message_text_only": "Response to email from Anthony Towns sent on 20210918 at 11:37:40 UTC\n==============================================\naj,\n\nThanks for taking the time to go through my paper on inherited IDs (IIDs). Also, thanks for your concise and accurate description of the IID proposal and the 2Stage channel protocol. I'm glad you feel the 2Stage protocol might be better than eltoo for a two-party channel.\n\nI want to address other parts of the paper that were obviously not as clear, as they led to two important misunderstandings.\n\nFirst, there is the issue of the use of an operator in the \"timeout trees\", \"update-forest\" and \"challenge-and-response\" factory protocols. While those protocols do include a party that is designated as the \"operator\", the operator is in no way a trusted party. While it's true that one would prefer an operator that follows the protocol fully and promptly, as that would allow one to keep the protocol off-chain, the operator can never take funds or prevent others from obtaining the funds that are due to them. In fact, this is exactly analogous to the selection of the party with whom one shares a two-party lightning channel. If one views lightning as being trust-free, then one will also view \"timeout trees\", \"update-forest\" and \"challenge-and-response\" to be trust-free.\n\nSecond, there is the question of whether or not IIDs can be simulated with anyprevout. I don't believe that they can. Consider for example the case where Alice has an on-chain funding transaction F1 with output F1:0 that will be spent by a (currently off-chain) transaction F2 with output F2:0 that will be spent by a settlement transaction S. Assume further that there is an on-chain control transaction C1 with output C1:0 owned by untrusted operator O, where C1:0 will be spent by a (currently off-chain) transaction C2 with output C2:0 that may, in certain cases, also be spent by S. In particular, assume F1 puts a covenant on F2 such that F2 puts a covenant on S, where the covenant on S can be met by either: A) waiting a CSV delay of one time unit (defined to be long enough to allow a party with a competing transaction to put that competing transaction on-chain first) and then spending only F2:0 (where F2:0 is referenced via IID) and giving ownership of S:0 to Alice, or B) waiting until a CLV reaches time T_lock and then spending both F2:0 and C2:0 (where F2:0 and C2:0 are referenced via IIDs).\n\nAssume that after Alice put F1 on-chain she wants to transfer ownership of the output S:0 to Bob without having to put F2 or S on-chain. She can do this with IIDs as follows. First, Alice asks the untrusted operator O to put C2 on-chain where C2 puts a covenant on S that forces S to spend both F2:0 and C2:0 (where F2:0 and C2:0 are referenced via IIDs) and to give ownership of S:0 to Bob (by making it spendable using Bob's public key).\n\nThere are two cases. First, if O promptly puts the desired C2 on-chain, then Alice and Bob can wait until T_lock (while putting nothing else on-chain), at which point Bob can be assured that he owns S:0 (as any attempt by Alice to spend S:0 by meetiing the covenant using case A above can be thwarted by Bob putting S on-chain first using case B above). Second, if O puts a different C2 on-chain, or fails to put any C2 on-chain promptly, Alice can reclaim her funds by putting F2 on-chain, waiting one time window, and then putting S on-chain using case A above.\n\nThus, IIDs provide a trust-free means for Alice to transfer funds from F1 to a party that is unknown to Alice when she puts F1 on-chain. I see two problems in tryinig to use anyprevout to achieve the same result. First, I don't know of any mechanism by which Alice can create a covenant that F2 puts on S which implements case B above. In some other settings, I can understand how one could use unique single-use keys in place of IID outputs. However, in this setting I don't see how to define a covenant that F2 puts on S that in case B forces the other input to spend C2:0, as signatures that are evaluated in spending F2:0 don't commit to the output scripts of other inputs to S. Second, and more fundamentally, even if one could define a covenant that F2 puts on S in case B forcing the other input to be signed by a single-use key owned by O, that still wouldn't unconditionally transfer ownership to Bob (without putting F2 and S on-chain). That's because in order to have single-use keys play the role of IIDs, they have to truly be single-use and there is no way Bob can know that O won't just sign some other S' that competes with S and sends S':0 to O, thus stealing the funds. Please let me know if I've missed something here.\n\nThe example above isn't very useful, as it doesn't cut down on the number of on-chain transactions required to transfer ownership from Alice to Bob. However, it does capture the core functionality that IIDs provide that (I believe) anyprevout does not provide. This functionality is exactly what enables \"update-forest\" and \"challenge-and-response\" to allow a single on-chain transaction to transfer ownership of thousands or millions of channels in a trust-free manner, thus accomplishing with one on-chain transaction what would have required thousands or millions of anyprevout transactions (at least as far as I can tell). This is exactly the power of IIDs that I was referring to, and I found surprising that this power was actually the result of restricting how a signed transaction can be used (as compared to a signed transaction that uses anyprevout).\n\nI hope clearing up these two misunderstandings is enough to pique your interest in reading the \"timeout trees\", \"update-forest\" and \"challenge-and-response\" protocols in more detail, as I'd be interested in your expert opinion on them.\n\nMy remaining comments are minor compared to the previous ones.\n* Regarding the worst-case delay for eltoo-2party vs. 2Stage, I agree that there is no single agreed upon model for analyzing this and opinions may differ. In any case, I think that if one had a nearly-expired HTLC (or if one is setting the lock time for an HTLC) and one could choose between eltoo-2party, where the other party could have thousands or millions of transactions competing with your settlement transaction, and 2Stage, where the other party can have at most one competing transaction, some would prefer 2Stage.\n* In comparing eltoo-2party and 2Stage, I was surprised that you didn't consider 2Stage's elimination of watchtowers for one or both parties as being an advantage. I had through that would be a big win in practice.\n* Regarding footnote 13's description of OP_CODESEPARATOR, I realize that that footnote does not capture the change made in taproot. I addressed that issue on p. 54 (and explained it in footnote 43), as footnote 13 was designed to explain OP_CODESEPARATOR to those not already familiar with it, while p. 54 was designed for the experts.\n* Regarding the new address type for floating transactions mentioned in the paper, thanks for the correction. I'll remove this from the next version.\n\nIn summary, the paper shows that:\n1) IIDs can be used to eliminate watchtowers for one or both parties in a two-party channel (2Stage),\n2) IIDs can be used to create factories that allow very large numbers of new users to obtain bitcoin in a watchtower-free and trust-free manner (timeout trees),\n3) IIDs support trust-free factories with unbounded numbers of parties (and channels) that allow the channels to be bought and sold by anyone, including parties not originally in the factory, with a single on-chain transaction, and\n4) IIDs achieve these results while using a more constrained, and thus safer, change to Bitcoin than the support for floating transactions.\n\nAre these results of interest?\n\nThanks,\nJohn\n\nSent with [ProtonMail](https://protonmail.com/) Secure Email.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211010/ef8682ee/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Inherited IDs - A safer, more powerful alternative to BIP-118 (ANYPREVOUT) for scaling Bitcoin",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "jlspc"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 8008
        }
    },
    {
        "title": "[bitcoin-dev] On the regularity of soft forks",
        "thread_messages": [
            {
                "author": "Michael Folkson",
                "date": "2021-10-11T12:24:10",
                "message_text_only": "I was hoping to delay this post as long as possible because there are so many interesting and important things to discuss other than soft forks and consensus changes that seem to have taken a backseat this year due to Taproot activation. In addition, there seems to be a world of opportunity to leverage the upcoming Taproot soft fork that risks getting drowned out by speculating on the next soft fork.\n\nThere is clearly nothing wrong with some individuals continuously working on, designing and refining new possible consensus changes and whoever is interested is free to follow and participate in those discussions. This is Bitcoin, no one let alone me can decide what people should focus on. Indeed I intend to allocate a portion of my time to following and understanding the trade-offs of current and future soft fork proposals. However, in this post I will argue against frequent soft forks with a single or minimal set of features and instead argue for infrequent soft forks with batches of features.\n\nI fully understand the desire and motivation to get consensus changes into Bitcoin as quickly as possible when certain use cases depend on them. However, the robustness, security and ability to resist harmful or suboptimal changes to the system is clearly the ultimate priority. The more frequently soft forks are attempted the harder it is for the community to ensure harmful or suboptimal changes don\u2019t creep into the consensus rules. I am well aware of how much community mindshare Taproot activation demanded this year. This is how it should be. The community should be informed and vigilant when the consensus rules are changed. Every full node will either immediately on activation or in future enforce these consensus rule changes and so it is in the interests of every full node operator that these changes have been subject to the ultimate levels of community review and rigorous testing. Attempting them frequently either requires continuous community monitoring or an acceptance that an unneeded or harmful consensus change could easily creep into Bitcoin. Neither of these options seem acceptable to me. It is not reasonable to ask all the different segments of the community to dedicate full time resources to stay on top of proposed consensus changes. Hence treating a pull request to a Bitcoin implementation that requires a soft fork like any other pull request is shortsighted.\n\nMerging soft fork code into a Bitcoin implementation\n\nThe code for a soft fork should not be merged into a Bitcoin implementation, let alone activation parameters (discussed later), until the maintainers of that implementation are comfortable that the entirety of that soft fork has sufficient community consensus. This includes what many consider the reference implementation and dominant implementation on the network, Bitcoin Core. A soft fork pull request cannot and should not be treated like any other pull request which can be merged with anything from 1 to 10 ACKs from long term or newer contributors. The act of merging a pull request that is part of a proposed soft fork is an acknowledgement by the maintainer(s) of that implementation that they consider the entirety of that proposed soft fork to have community consensus. That includes what is included in that soft fork as well as what isn\u2019t. If there is a prevailing view that the current design could be improved, could feasibly be replaced by something superior in future or merely hasn\u2019t been subject to sufficient community review it should not be merged.\n\nOf course there is no ultimate authority to enforce that this happens, Bitcoin is an entirely voluntary system. A contentious or disputed soft fork can be merged into a Bitcoin implementation at any time but doing this is opening the door to the schism, disruption and waste of developer hours that we saw in 2017. Personally I think we\u2019ll see an attempt to activate a contentious soft fork at some point in the long term future (Murphy\u2019s Law) but any attempt to do so should be strongly discouraged. It should be made clear to any individual(s) that attempt this of the knock on impacts and potential short term damage they are inflicting on the entire ecosystem. Longer term I have confidence in Bitcoin\u2019s ability to survive whatever happens but allocating significant community resources to resist an unnecessary contentious soft fork (or even regular contentious soft forks) is not an optimal use of those resources.\n\nSoft fork activation\n\nMiner signaling is a tool for signaling readiness. It is not voting for the soft fork or expressing support for the soft fork. There should not be any attempt to facilitate miner signaling until there is sufficient community consensus (the mining community is a subset of the community) on the soft fork. Merging activation parameters or encouraging miner signaling before it is clear there is community consensus on the entirety of the soft fork is putting the cart before the horse.\n\nTaproot showed it was possible through the sustained efforts of many individuals and many organizations to achieve overwhelming community consensus for a soft fork. It is obviously impossible to get 100 percent consensus but Taproot appeared to get close to that. I did not identify any resistance whatsoever to merging Taproot PRs or the objective of getting Taproot activated although there was one long term contributor who effectively NACKed Taproot based on quantum resistance concerns.\n\nActivation method and activation parameters ended up being more challenging to obtain overwhelming community consensus. Although I and a number of others participated in multiple open IRC meetings and spent months on IRC trying to find a way to get Taproot activated with at least rough consensus a number of disagreements remain. I don\u2019t think these are necessarily showstoppers for future soft forks and assuming Taproot activates safely next month they ended up not being showstoppers for Taproot. However, it is clear the bar that was achieved regarding community consensus for the Taproot soft fork wasn\u2019t met for the activation method and activation parameters. In a world where there isn\u2019t overwhelming community consensus on the activation method, the activation parameters and what to do if the first activation attempt fails we have to accept that soft fork activations contain downside risk on top of the already acknowledged risks of bugs, consensus divergences and botched implementations of soft fork features. To layer on top a level of uncertainty over whether there is community consensus for the actual soft fork seems unacceptable to me.\n\nThis is an important additional argument for infrequent soft forks with batches of features rather than frequent soft forks with a single feature. If there is a chain split risk every time you attempt a soft fork you should not casually attempt a soft fork frequently or with abandon. There has to be community consensus that the upsides of the soft fork are sufficient to take on these downside risks of disruption or worse chain splits. I was of the strong personal view that the upsides outweighed the downside risks for Taproot activation in 2021 but this is a judgment we as a community will have to make for each and every future proposed soft fork. It is easy to get excited about shiny new features. It is harder to ensure harmful or suboptimal changes don\u2019t creep into the consensus rules and harder yet to minimize the risk of chain splits if soft forks are attempted frequently.\n\n--\n\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211011/500ca9d1/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2021-10-11T16:03:16",
                "message_text_only": "Hi Michael,\n\nAgree with almost everything.\n\n> Miner signaling is a tool for signaling readiness. It is not voting for the soft fork or expressing support for the soft fork. There should not be any attempt to facilitate miner signaling until there is sufficient community consensus (the mining community is a subset of the community) on the soft fork. \n\nThis is really important which gets ignored. I wish there was a way to solve this problem in a way that it is not misinterpreted by users.\n\nDuring signalling for taproot, there were lots of users in different communities that believed miners are voting for taproot and we need some percentage of miners to agree before making any changes in Bitcoin. It was not just non-technical users but few mining pools, exchanges etc. also considered miners signaling as some voting process.\n\nBest I could do at that moment was share this link: https://bitcoin.stackexchange.com/questions/97043/is-there-an-active-list-of-bips-currently-open-for-voting/\n\nHowever I am sure there are lot of people who still think miners vote during signaling. Opinions of few developers on MASF vs UASF also adds more confusion to this thing. I could not think of any solution to solve this problem.\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211011/9d118eee/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2021-10-12T19:04:02",
                "message_text_only": "On Tue, Oct 12, 2021 at 5:34 PM Prayank via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Hi Michael,\n>\n> Agree with almost everything.\n>\n> > Miner signaling is a tool for signaling readiness. It is not voting for the soft fork or expressing support for the soft fork. There should not be any attempt to facilitate miner signaling until there is sufficient community consensus (the mining community is a subset of the community) on the soft fork.\n>\n> This is really important which gets ignored. I wish there was a way to solve this problem in a way that it is not misinterpreted by users.\n>\n> During signalling for taproot, there were lots of users in different communities that believed miners are voting for taproot and we need some percentage of miners to agree before making any changes in Bitcoin. It was not just non-technical users but few mining pools, exchanges etc. also considered miners signaling as some voting process.\n>\n> Best I could do at that moment was share this link: https://bitcoin.stackexchange.com/questions/97043/is-there-an-active-list-of-bips-currently-open-for-voting/\n>\n> However I am sure there are lot of people who still think miners vote during signaling. Opinions of few developers on MASF vs UASF also adds more confusion to this thing. I could not think of any solution to solve this problem.\n\nYes, given most of the arguments given against activation at the end\nof the period regardless of mining signaling, it seems sadly it's not\njust users but developers too. They seem to believe that miners must\nchose for users with bip8(false) because (according to them) with\nbip8(true) it is developers who decide for users, and they don't want\nto decide for users: they want miners to decide for users.\nThey don't seem to believe users can actually chose for themselves, sadly.\nIn the next softfork, sadly, probably the same discussions will be\nrepeated, the same rational arguments will be ignored and activation\nwill be once again done, in my opinion, the wrong way and most users\n(many more, as we grow in numbers) will remain confused in the same\nway and confusing the newcomers they explain bitcoin to.\n\n\n\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jeremy",
                "date": "2021-10-11T19:12:58",
                "message_text_only": "*> ... in this post I will argue against frequent soft forks with a single\nor minimal*\n*> set of features and instead argue for infrequent soft forks with batches*\n*> of features.*\n\nI think this type of development has been discussed in the past and has\nbeen rejected.\n\n\nfrom: Matt Corallo's post:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-January/017547.html\n\n\n\n\n\n\n\n\n\n\n*Matt: Follow the will of the community, irrespective of individuals\norunreasoned objection, but without ever overruling any\nreasonableobjection. Recent history also includes \"objection\" to soft forks\nin theform of \"this is bad because it doesn't fix a different problem I\nwantfixed ASAP\". I don't think anyone would argue this qualifies as\nareasonable objection to a change, and we should be in a place, as\nacommunity (never as developers or purely one group), to ignore\nsuchobjections and make forward progress in spite of them. We don't\nmakegood engineering decisions by \"bundling\" unrelated features together to*\n*enable political football and compromise.*\n\n*AJ: - improvements: changes might not make everyone better off, but we*\n\n\n\n\n*   don't want changes to screw anyone over either -- pareto   improvements\nin economics, \"first, do no harm\", etc. (if we get this   right, there's no\nneed to make compromises and bundle multiple   flawed proposals so that\neveryone's an equal mix of happy and*\n*   miserable)*\n\n\nI think Matt and AJ's PoV is widely reflected in the community that\nbundling changes leads to the inclusion of suboptimal features.\n\nThis also has strong precedent in other important technical bodies, e.g.\nfrom https://datatracker.ietf.org/doc/html/rfc7282 On Consensus and Humming\nin the IETF.\n\n      Even worse is the \"horse-trading\" sort of compromise: \"I object to\n   your proposal for such-and-so reasons.  You object to my proposal for\n   this-and-that reason.  Neither of us agree.  If you stop objecting to\n   my proposal, I'll stop objecting to your proposal and we'll put them\n   both in.\"  That again results in an \"agreement\" of sorts, but instead\n   of just one outstanding unaddressed issue, this sort of compromise\n     results in two, again ignoring them for the sake of expedience.\n\n   These sorts of \"capitulation\" or \"horse-trading\" compromises have no\n   place in consensus decision making.  In each case, a chair who looks\n   for \"agreement\" might find it in these examples because it appears\n   that people have \"agreed\".  But answering technical disagreements is\n   what is needed to achieve consensus, sometimes even when the people\n\n      who stated the disagreements no longer wish to discuss them.\n\n\nIf you would like to advocate bitcoin development run counter to that,\nyou should provide a much stronger refutation of these engineering\nnorms.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211011/7c82c14f/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-11T19:53:39",
                "message_text_only": "Good morning Jeremy,\n\n> This also has strong precedent in other important technical bodies, e.g. from\u00a0https://datatracker.ietf.org/doc/html/rfc7282\u00a0On Consensus and Humming in the IETF.\n>\n>   Even worse is the \"horse-trading\" sort of compromise: \"I object to\n> \u00a0 \u00a0your proposal for such-and-so reasons.\u00a0 You object to my proposal for\n> \u00a0 \u00a0this-and-that reason.\u00a0 Neither of us agree.\u00a0 If you stop objecting to\n> \u00a0 \u00a0my proposal, I'll stop objecting to your proposal and we'll put them\n> \u00a0 \u00a0both in.\" \u00a0That again results in an \"agreement\" of sorts, but instead\n> \u00a0 \u00a0of just one outstanding unaddressed issue, this sort of compromise\n>   results in two, again ignoring them for the sake of expedience.\n>\n> \u00a0 \u00a0These sorts of \"capitulation\" or \"horse-trading\" compromises have no\n> \u00a0 \u00a0place in consensus decision making.\u00a0 In each case, a chair who looks\n> \u00a0 \u00a0for \"agreement\" might find it in these examples because it appears\n> \u00a0 \u00a0that people have \"agreed\".\u00a0 But answering technical disagreements is\n> \u00a0 \u00a0what is needed to achieve consensus, sometimes even when the people\u00a0\n>\n>   who stated the disagreements no longer wish to discuss them.\n>\n> If you would like to advocate bitcoin development run counter to that, you should provide a much stronger refutation of these engineering norms.\n\nThe Internet has the maxim \"be strict in what you provide, lenient in what you accept\", which allows for slight incompatibilities between software to generally be papered over (xref the mountains of Javascript code that shim in various new ECMAScript features fairly reliably in a wide variety of browsers).\n\nBitcoin, as a consensus system, requires being paranoiacally strict on what transactions and blocks you accept.\nThus, the general engineering norm of separating concerns, of great application to \"lenient in what you accept\" systems, may not apply quite as well to \"hell no I am not accepting that block\" Bitcoin.\n\nBitcoin as well, as a resistance against state moneys, is inherently political, and it possible that the only way out is through: we may need to resist this horse-trading by other means than separating concerns, including political will to reject capitulation despite bundling.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2021-10-14T23:52:07",
                "message_text_only": "On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:\n> > ...\u00a0in this post I will argue against frequent soft forks with a single or\n> minimal\n> > set of features and instead argue for infrequent soft forks with batches\n> > of features.\n> I think this type of development has been discussed in the past and has been\n> rejected.\n\n> AJ:\u00a0- improvements: changes might not make everyone better off, but we\n> \u00a0 \u00a0don't want changes to screw anyone over either -- pareto\n> \u00a0 \u00a0improvements in economics, \"first, do no harm\", etc. (if we get this\n> \u00a0 \u00a0right, there's no need to make compromises and bundle multiple\n> \u00a0 \u00a0flawed proposals so that everyone's an equal mix of happy and\n> \u00a0 \u00a0miserable)\n\nI don't think your conclusion above matches my opinion, for what it's\nworth.\n\nIf you've got two features, A and B, where the game theory is:\n\n If A happens, I'm +100, You're -50\n If B happens, I'm -50, You're +100\n\nthen even though A+B is +50, +50, then I do think the answer should\ngenerally be \"think harder and come up with better proposals\" rather than\n\"implement A+B as a bundle that makes us both +50\".\n\n_But_ if the two features are more like:\n\n  If C happens, I'm +100, You're +/- 0\n  If D happens, I'm +/- 0, You're +100\n\nthen I don't have a problem with bundling them together as a single\nsimultaneous activation of both C and D.\n\nAlso, you can have situations where things are better together,\nthat is:\n\n  If E happens, we're both at +100\n  If F happens, we're both at +50\n  If E+F both happen, we're both at +9000\n\nIn general, I think combining proposals when the combination is better\nthan the individual proposals were is obviously good; and combining\nrelated proposals into a single activation can be good if it is easier\nto think about the ideas as a set. \n\nIt's only when you'd be rejecting the proposal on its own merits that\nI think combining it with others is a bad idea in principle.\n\nFor specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx\nand CHECKSIGADD together because they do have synergies like that; we\ndidn't bundle ANYPREVOUT and graftroot despite the potential synergies\nbecause those features needed substantially more study.\n\nThe nulldummy soft-fork (bip 147) was deployed concurrently with\nthe segwit soft-fork (bip 141, 143), but I don't think there was any\nparticular synergy or need for those things to be combined, it just\nreduced the overhead of two sets of activation signalling to one.\n\nNote that the implementation code for nulldummy had already been merged\nand were applied as relay policy well before activation parameters were\ndefined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming\nan active soft fork.\n\nCheers,\naj"
            },
            {
                "author": "micaroni at gmail.com",
                "date": "2021-10-15T00:43:40",
                "message_text_only": "Interesting discussion. Correct me if I'm wrong: but putting too many\nfeatures together in one shot just can't make things harder to debug in\nproduction if something very unexpected happens. It's a basic principle of\nsoftware engineering.\n\nChange. Deploy. Nothing bad happened? Change it a little more. Deployment.\nOr: Change, change, change. Deploy. Did something bad happen? What change\ncaused the problem?\n\nOn Thu, Oct 14, 2021 at 8:53 PM Anthony Towns via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:\n> > > ... in this post I will argue against frequent soft forks with a\n> single or\n> > minimal\n> > > set of features and instead argue for infrequent soft forks with\n> batches\n> > > of features.\n> > I think this type of development has been discussed in the past and has\n> been\n> > rejected.\n>\n> > AJ: - improvements: changes might not make everyone better off, but we\n> >    don't want changes to screw anyone over either -- pareto\n> >    improvements in economics, \"first, do no harm\", etc. (if we get this\n> >    right, there's no need to make compromises and bundle multiple\n> >    flawed proposals so that everyone's an equal mix of happy and\n> >    miserable)\n>\n> I don't think your conclusion above matches my opinion, for what it's\n> worth.\n>\n> If you've got two features, A and B, where the game theory is:\n>\n>  If A happens, I'm +100, You're -50\n>  If B happens, I'm -50, You're +100\n>\n> then even though A+B is +50, +50, then I do think the answer should\n> generally be \"think harder and come up with better proposals\" rather than\n> \"implement A+B as a bundle that makes us both +50\".\n>\n> _But_ if the two features are more like:\n>\n>   If C happens, I'm +100, You're +/- 0\n>   If D happens, I'm +/- 0, You're +100\n>\n> then I don't have a problem with bundling them together as a single\n> simultaneous activation of both C and D.\n>\n> Also, you can have situations where things are better together,\n> that is:\n>\n>   If E happens, we're both at +100\n>   If F happens, we're both at +50\n>   If E+F both happen, we're both at +9000\n>\n> In general, I think combining proposals when the combination is better\n> than the individual proposals were is obviously good; and combining\n> related proposals into a single activation can be good if it is easier\n> to think about the ideas as a set.\n>\n> It's only when you'd be rejecting the proposal on its own merits that\n> I think combining it with others is a bad idea in principle.\n>\n> For specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx\n> and CHECKSIGADD together because they do have synergies like that; we\n> didn't bundle ANYPREVOUT and graftroot despite the potential synergies\n> because those features needed substantially more study.\n>\n> The nulldummy soft-fork (bip 147) was deployed concurrently with\n> the segwit soft-fork (bip 141, 143), but I don't think there was any\n> particular synergy or need for those things to be combined, it just\n> reduced the overhead of two sets of activation signalling to one.\n>\n> Note that the implementation code for nulldummy had already been merged\n> and were applied as relay policy well before activation parameters were\n> defined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming\n> an active soft fork.\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211014/5852b42f/attachment-0001.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2021-10-16T11:02:08",
                "message_text_only": "> Interesting discussion.Correct me if I'm wrong: but putting too many features together in one shot just can't make things harder to debug in production if something very unexpected happens.It's a basic principle of software engineering.\n\nSoft fork features can (and should) obviously be tested thoroughly on testnet, signet, custom signets, sidechains etc on a standalone basis and a bundled basis. But whether or not it is a basic principle of general software engineering kind of misses the point. Security critical software clearly isn't engineered in the same way as a new social media app. Bugs are easily reverted in a new social media app. A consensus change is extremely hard to revert and probably requires a hard fork, a level of central coordination we generally attempt to avoid and a speed of deployment that we also attempt to avoid. On top of that we aren't just dealing with security critical software. One of the most important objectives is to keep all the nodes on the network in consensus. Introducing a consensus change before we are comfortable there is community consensus for it is a massive effective bug in itself. The network can split in multiple ways e.g. part of the network disagrees on whether to activate the consensus change, part of the network disagrees on how to resist that consensus change, part of the network disagrees on how to activate that consensus change etc\n\nIn addition, a social media app can experiment in production whether Feature A works, whether Feature B works or whether Feature A and B work best together. In Bitcoin if we activate consensus Feature A, later decide we want consensus Feature B but find out that by previously activating Feature A we can't have Feature B (it is now unsafe to activate it) or its design now has to be suboptimal because we have to ensure it can safely work in the presence of Feature A we have made a mistake by activating Feature A in the first place. Decentralized security critical consensus changes are an emerging field in itself and really can't be treated like any other software project. This will become universally understood I'm sure over time.\n\n--\n\nMichael Folkson\nEmail: michaelfolkson at protonmail.com\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Friday, October 15th, 2021 at 1:43 AM, Felipe Micaroni Lalli via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Interesting discussion.Correct me if I'm wrong: but putting too many features together in one shot just can't make things harder to debug in production if something very unexpected happens. It's a basic principle of software engineering.\n>\n> Change. Deploy. Nothing bad happened? Change it a little more. Deployment.\n>\n> Or:Change, change, change. Deploy. Did something bad happen? What change caused the problem?\n>\n> On Thu, Oct 14, 2021 at 8:53 PM Anthony Towns via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Mon, Oct 11, 2021 at 12:12:58PM -0700, Jeremy via bitcoin-dev wrote:\n>>> > ... in this post I will argue against frequent soft forks with a single or\n>>> minimal\n>>> > set of features and instead argue for infrequent soft forks with batches\n>>> > of features.\n>>> I think this type of development has been discussed in the past and has been\n>>> rejected.\n>>\n>>> AJ: - improvements: changes might not make everyone better off, but we\n>>> don't want changes to screw anyone over either -- pareto\n>>> improvements in economics, \"first, do no harm\", etc. (if we get this\n>>> right, there's no need to make compromises and bundle multiple\n>>> flawed proposals so that everyone's an equal mix of happy and\n>>> miserable)\n>>\n>> I don't think your conclusion above matches my opinion, for what it's\n>> worth.\n>>\n>> If you've got two features, A and B, where the game theory is:\n>>\n>> If A happens, I'm +100, You're -50\n>> If B happens, I'm -50, You're +100\n>>\n>> then even though A+B is +50, +50, then I do think the answer should\n>> generally be \"think harder and come up with better proposals\" rather than\n>> \"implement A+B as a bundle that makes us both +50\".\n>>\n>> _But_ if the two features are more like:\n>>\n>> If C happens, I'm +100, You're +/- 0\n>> If D happens, I'm +/- 0, You're +100\n>>\n>> then I don't have a problem with bundling them together as a single\n>> simultaneous activation of both C and D.\n>>\n>> Also, you can have situations where things are better together,\n>> that is:\n>>\n>> If E happens, we're both at +100\n>> If F happens, we're both at +50\n>> If E+F both happen, we're both at +9000\n>>\n>> In general, I think combining proposals when the combination is better\n>> than the individual proposals were is obviously good; and combining\n>> related proposals into a single activation can be good if it is easier\n>> to think about the ideas as a set.\n>>\n>> It's only when you'd be rejecting the proposal on its own merits that\n>> I think combining it with others is a bad idea in principle.\n>>\n>> For specific examples, we bundled schnorr, Taproot, MAST, OP_SUCCESSx\n>> and CHECKSIGADD together because they do have synergies like that; we\n>> didn't bundle ANYPREVOUT and graftroot despite the potential synergies\n>> because those features needed substantially more study.\n>>\n>> The nulldummy soft-fork (bip 147) was deployed concurrently with\n>> the segwit soft-fork (bip 141, 143), but I don't think there was any\n>> particular synergy or need for those things to be combined, it just\n>> reduced the overhead of two sets of activation signalling to one.\n>>\n>> Note that the implementation code for nulldummy had already been merged\n>> and were applied as relay policy well before activation parameters were\n>> defined (May 2014 via PR#3843 vs Sep 2016 for PR#8636) let alone becoming\n>> an active soft fork.\n>>\n>> Cheers,\n>> aj\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211016/7c3b157b/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "On the regularity of soft forks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Michael Folkson",
                "micaroni at gmail.com",
                "Anthony Towns",
                "ZmnSCPxj",
                "Jorge Tim\u00f3n",
                "Prayank"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 29459
        }
    },
    {
        "title": "[bitcoin-dev] Year 2038 problem and year 2106 chain halting",
        "thread_messages": [
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-10-13T19:16:05",
                "message_text_only": "It seems that Bitcoin Core will stop working in 2038 because of assertion checking if the current time is non-negative. Also, the whole chain will halt after reaching median time 0xffffffff in 2106. More information: https://bitcointalk.org/index.php?topic=5365359.0\nI wonder if that kind of issues are possible to fix in a soft-fork way.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211013/de85a966/attachment.html>"
            },
            {
                "author": "James Lu",
                "date": "2021-10-15T15:27:42",
                "message_text_only": "Making Bitcoin function after 2038 is by definition a hard fork\n\nI feel if we do HF, we should bundle other HF changes with it...\n\nOn Wed, Oct 13, 2021 at 5:19 PM vjudeu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> It seems that Bitcoin Core will stop working in 2038 because of assertion\n> checking if the current time is non-negative. Also, the whole chain will\n> halt after reaching median time 0xffffffff in 2106. More information:\n> https://bitcointalk.org/index.php?topic=5365359.0\n>\n> I wonder if that kind of issues are possible to fix in a soft-fork way.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211015/c898de49/attachment-0001.html>"
            },
            {
                "author": "Kate Salazar",
                "date": "2021-10-17T08:19:07",
                "message_text_only": "There is a hard fork wishlist:\n\nhttps://en.bitcoin.it/wiki/Hardfork_Wishlist\n\nNote last update is somewhat old.\n\nCheers.\n\nOn Sat, Oct 16, 2021 at 12:49 AM James Lu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Making Bitcoin function after 2038 is by definition a hard fork\n>\n> I feel if we do HF, we should bundle other HF changes with it...\n>\n> On Wed, Oct 13, 2021 at 5:19 PM vjudeu via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> It seems that Bitcoin Core will stop working in 2038 because of assertion\n>> checking if the current time is non-negative. Also, the whole chain will\n>> halt after reaching median time 0xffffffff in 2106. More information:\n>> https://bitcointalk.org/index.php?topic=5365359.0\n>>\n>> I wonder if that kind of issues are possible to fix in a soft-fork way.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211017/db2469d9/attachment.html>"
            },
            {
                "author": "damian at willtech.com.au",
                "date": "2021-10-17T22:38:16",
                "message_text_only": "Good Afternoon,\n\nI am certain that as soon as we identify solutions they should be \nimplemented. Basic life skills assert that procrastination is always a \nform of failure, where we could have realised and accomplished further \nyet we waited and in our present state could not ascertain what was in \nour benefit.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this \nemail if misdelivered.\nOn 2021-10-15 08:27, James Lu via bitcoin-dev wrote:\n> Making Bitcoin function after 2038 is by definition a hard fork\n> \n> I feel if we do HF, we should bundle other HF changes with it...\n> \n> On Wed, Oct 13, 2021 at 5:19 PM vjudeu via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> It seems that Bitcoin Core will stop working in 2038 because of\n>> assertion checking if the current time is non-negative. Also, the\n>> whole chain will halt after reaching median time 0xffffffff in 2106.\n>> More information: https://bitcointalk.org/index.php?topic=5365359.0\n>> \n>> I wonder if that kind of issues are possible to fix in a soft-fork\n>> way. _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "yanmaani at cock.li",
                "date": "2021-10-15T15:44:59",
                "message_text_only": "It's well-known. Nobody really cares, because it's so far off. Not \npossible to do by softfork, no. It is possible to do by something that \nbecomes a hardfork in 80 years, though, which is probably good enough.\n\nI proposed a solution, but nobody was really interested. Let's see if \nanyone bites now.\n\n---\n\nSubject: Suggestion: Solve year 2106 problem by taking timestamps mod \n2^32\nTo \tBitcoin Protocol Discussion\nDate \t2020-09-19 12:36\nMessage Body\nCurrently, Bitcoin's timestamp rules are as follows:\n\n1. The block timestamp may not be lower than the median of the last 11 \nblocks'\n2. The block timestamp may not be greater than the current time plus two \nhours\n3. The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106 \n06:28:16 +0000)\n\nThus, Bitcoin will \"die\" on or about 2106-02-07, when there is no \ntimestamp below 2^32 that exceeds the median of the last 11 blocks.\n\nIf the rules were changed to the following, this problem would be \nsolved:\n\n1. The block timestamp plus k*2^32 may not be lower than the median of \nthe last 11 blocks'\n2. The block timestamp plus k*2^32 may not be greater than the current \ntime plus two hours\n3. k is an integer, whose value must be the same for the calculations of \nRule 1 and Rule 2\n\nThis would cause a hardfork in the year 2106, which is approximately \n85.5 years from now, by which time 95% of nodes would hopefully have \nupdated.\n\nAnother proposed solution is 64-bit timestamps. They would break \ncompatibility with other software that has specific expectations of \nheader fields, like ASICs' firmware. They would also cause a hardfork \nbefore the date of timestamp overflow. I thus believe them to be a less \nappropriate solution.\n\nWhat do you think of this idea? Is it worth a BIP?\n\nOn 2021-10-13 19:16, vjudeu via bitcoin-dev wrote:\n> It seems that Bitcoin Core will stop working in 2038 because of\n> assertion checking if the current time is non-negative. Also, the\n> whole chain will halt after reaching median time 0xffffffff in 2106.\n> More information: https://bitcointalk.org/index.php?topic=5365359.0\n> \n> I wonder if that kind of issues are possible to fix in a soft-fork\n> way.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-10-15T22:22:00",
                "message_text_only": "Your solution seems to solve the problem of chain halting, but there are more issues. For example: if you have some time modulo 2^32, then you no longer know if timestamp zero is related to 1970 or 2106 or some higher year. Your \"k\" value representing in fact the most significant 32 bits of 64-bit timestamp has to be stored in all cases where time is used. If there is no \"k\", then zero should be used for backward compatibility. Skipping \"k\" could cause problems related to OP_CHECKLOCKTIMEVERIFY or nLockTime, because if some transaction was timestamped to 0xbadc0ded, then that transaction will be valid in 0x00000000badc0ded, invalid in 0x0000000100000000, and valid again in 0x00000001badc0ded, the same for timelocked outputs.\n\nSo, I think your \"k\" value should be added to the coinbase transaction, then you can combine two 32-bit values, the lower bits from the block header and the higher bits from the coinbase transaction. Also, adding your \"k\" value transaction nLockTime field is needed (maybe in a similar way as transaction witness was added in Segwit), because in other case after reaching 0x0000000100000000 all off-chain transactions with timelocks around 0x00000000ffffffff will be additionally timelocked for the next N years. The same is needed for each OP_CHECKLOCKTIMEVERIFY, maybe pushing high 32 bits before the currently used value will solve that (and assuming zero if there is only some 32-bit value).\n\nOn 2021-10-15 23:48:59 user yanmaani at cock.li wrote:\n> It's well-known. Nobody really cares, because it's so far off. Not \npossible to do by softfork, no. It is possible to do by something that \nbecomes a hardfork in 80 years, though, which is probably good enough.\n\nI proposed a solution, but nobody was really interested. Let's see if \nanyone bites now.\n\n---\n\nSubject: Suggestion: Solve year 2106 problem by taking timestamps mod \n2^32\nTo \tBitcoin Protocol Discussion\nDate \t2020-09-19 12:36\nMessage Body\nCurrently, Bitcoin's timestamp rules are as follows:\n\n1. The block timestamp may not be lower than the median of the last 11 \nblocks'\n2. The block timestamp may not be greater than the current time plus two \nhours\n3. The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106 \n06:28:16 +0000)\n\nThus, Bitcoin will \"die\" on or about 2106-02-07, when there is no \ntimestamp below 2^32 that exceeds the median of the last 11 blocks.\n\nIf the rules were changed to the following, this problem would be \nsolved:\n\n1. The block timestamp plus k*2^32 may not be lower than the median of \nthe last 11 blocks'\n2. The block timestamp plus k*2^32 may not be greater than the current \ntime plus two hours\n3. k is an integer, whose value must be the same for the calculations of \nRule 1 and Rule 2\n\nThis would cause a hardfork in the year 2106, which is approximately \n85.5 years from now, by which time 95% of nodes would hopefully have \nupdated.\n\nAnother proposed solution is 64-bit timestamps. They would break \ncompatibility with other software that has specific expectations of \nheader fields, like ASICs' firmware. They would also cause a hardfork \nbefore the date of timestamp overflow. I thus believe them to be a less \nappropriate solution.\n\nWhat do you think of this idea? Is it worth a BIP?\n\nOn 2021-10-13 19:16, vjudeu via bitcoin-dev wrote:\n> It seems that Bitcoin Core will stop working in 2038 because of\n> assertion checking if the current time is non-negative. Also, the\n> whole chain will halt after reaching median time 0xffffffff in 2106.\n> More information: https://bitcointalk.org/index.php?topic=5365359.0\n> \n> I wonder if that kind of issues are possible to fix in a soft-fork\n> way.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "yanmaani at cock.li",
                "date": "2021-10-17T15:14:41",
                "message_text_only": "What, no. The `k` value is calculated implicitly, because there's only \none value of it that could ever be valid - if `k` is 1 too small, we're \n70 years too far back, and then the block will violate median of last \n11. If `k` is 1 too large, we're 70 years too far in the future, then \nthe block will violate 2 hour rule. Nothing is added to coinbase or \nanywhere else.\n\nIt's possible that you'd need some extra logic for locktime, yes, but it \nwould only be a problem in very special cases. Worst-case, you'll have \nto use block time locking in the years around the switch, or softfork in \n64-bit locking.\n\nBut unless I'm missing something, 32-bit would be enough, you just \nwouldn't be able to locktime something past the timestamp for the \nswitch. After the switchover, everything would be back to normal.\n\nThis is a hardfork, yes, but it's a hardfork that kicks in way into the \nfuture. And because it's a hardfork, you might as well do anything, as \nlong as it doesn't change anything now.\n\nOn 2021-10-15 22:22, vjudeu at gazeta.pl wrote:\n> Your solution seems to solve the problem of chain halting, but there\n> are more issues. For example: if you have some time modulo 2^32, then\n> you no longer know if timestamp zero is related to 1970 or 2106 or\n> some higher year. Your \"k\" value representing in fact the most\n> significant 32 bits of 64-bit timestamp has to be stored in all cases\n> where time is used. If there is no \"k\", then zero should be used for\n> backward compatibility. Skipping \"k\" could cause problems related to\n> OP_CHECKLOCKTIMEVERIFY or nLockTime, because if some transaction was\n> timestamped to 0xbadc0ded, then that transaction will be valid in\n> 0x00000000badc0ded, invalid in 0x0000000100000000, and valid again in\n> 0x00000001badc0ded, the same for timelocked outputs.\n> \n> So, I think your \"k\" value should be added to the coinbase\n> transaction, then you can combine two 32-bit values, the lower bits\n> from the block header and the higher bits from the coinbase\n> transaction. Also, adding your \"k\" value transaction nLockTime field\n> is needed (maybe in a similar way as transaction witness was added in\n> Segwit), because in other case after reaching 0x0000000100000000 all\n> off-chain transactions with timelocks around 0x00000000ffffffff will\n> be additionally timelocked for the next N years. The same is needed\n> for each OP_CHECKLOCKTIMEVERIFY, maybe pushing high 32 bits before the\n> currently used value will solve that (and assuming zero if there is\n> only some 32-bit value)."
            },
            {
                "author": "Kate Salazar",
                "date": "2021-10-17T15:46:46",
                "message_text_only": "Hi yanmaani\n\nOn Sun, Oct 17, 2021 at 5:28 PM yanmaani--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> What, no. The `k` value is calculated implicitly, because there's only\n> one value of it that could ever be valid - if `k` is 1 too small, we're\n> 70 years too far back, and then the block will violate median of last\n> 11. If `k` is 1 too large, we're 70 years too far in the future, then\n> the block will violate 2 hour rule. Nothing is added to coinbase or\n> anywhere else.\n>\n> It's possible that you'd need some extra logic for locktime, yes, but it\n> would only be a problem in very special cases. Worst-case, you'll have\n> to use block time locking in the years around the switch, or softfork in\n> 64-bit locking.\n>\n> But unless I'm missing something, 32-bit would be enough, you just\n> wouldn't be able to locktime something past the timestamp for the\n> switch. After the switchover, everything would be back to normal.\n>\n> This is a hardfork, yes, but it's a hardfork that kicks in way into the\n> future. And because it's a hardfork, you might as well do anything, as\n> long as it doesn't change anything now.\n>\n\n\"Anything\" is quite a word.\nIdeally, hard fork requires upgrading every node that can be upgraded,\nor at least have the node operator's consent to lose the node (for every\nnode that can't be upgraded).\n\n\n>\n> On 2021-10-15 22:22, vjudeu at gazeta.pl wrote:\n> > Your solution seems to solve the problem of chain halting, but there\n> > are more issues. For example: if you have some time modulo 2^32, then\n> > you no longer know if timestamp zero is related to 1970 or 2106 or\n> > some higher year. Your \"k\" value representing in fact the most\n> > significant 32 bits of 64-bit timestamp has to be stored in all cases\n> > where time is used. If there is no \"k\", then zero should be used for\n> > backward compatibility. Skipping \"k\" could cause problems related to\n> > OP_CHECKLOCKTIMEVERIFY or nLockTime, because if some transaction was\n> > timestamped to 0xbadc0ded, then that transaction will be valid in\n> > 0x00000000badc0ded, invalid in 0x0000000100000000, and valid again in\n> > 0x00000001badc0ded, the same for timelocked outputs.\n> >\n> > So, I think your \"k\" value should be added to the coinbase\n> > transaction, then you can combine two 32-bit values, the lower bits\n> > from the block header and the higher bits from the coinbase\n> > transaction. Also, adding your \"k\" value transaction nLockTime field\n> > is needed (maybe in a similar way as transaction witness was added in\n> > Segwit), because in other case after reaching 0x0000000100000000 all\n> > off-chain transactions with timelocks around 0x00000000ffffffff will\n> > be additionally timelocked for the next N years. The same is needed\n> > for each OP_CHECKLOCKTIMEVERIFY, maybe pushing high 32 bits before the\n> > currently used value will solve that (and assuming zero if there is\n> > only some 32-bit value).\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211017/a7bd292d/attachment.html>"
            },
            {
                "author": "yanmaani at cock.li",
                "date": "2021-10-18T02:55:32",
                "message_text_only": "Well, it's the right word. If you're going to do a hardfork by changing \nthe timestamp definition, you're already doing a hardfork. At that \npoint, you've already crossed the Rubicon and might as well put in any \nother necessary changes (e.g. to transaction locking), because it will \nbe as much of a hardfork either way.\n\nThe important bit here is \"as long as it doesn't change anything now\" - \nthis is indeed a hardfork, but it's a timestamp-activated hardfork that \ntriggers in 2106. Until that point, it has absolutely no bearing on \nconsensus rules (as opposed to the other proposals, which are at least a \nsoft-fork today).\n\nI understand that there's some problems in getting consensus for forks, \nbut surely we can agree that everyone will update their Bitcoin at least \nonce in the next 85 years? (If they don't, they're doomed anyway.)\n\nOn 2021-10-17 15:46, Kate Salazar wrote:\n> Hi yanmaani\n> \n...\n>> This is a hardfork, yes, but it's a hardfork that kicks in way into\n>> the\n>> future. And because it's a hardfork, you might as well do anything,\n>> as\n>> long as it doesn't change anything now.\n> \n> \"Anything\" is quite a word.\n> Ideally, hard fork requires upgrading every node that can be upgraded,\n> \n> or at least have the node operator's consent to lose the node (for\n> every\n> node that can't be upgraded).\n> \n...\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-15T23:01:46",
                "message_text_only": "Good morning yanmaani,\n\n\n> It's well-known. Nobody really cares, because it's so far off. Not\n> possible to do by softfork, no.\n\nI think it is possible by softfork if we try hard enough?\n\n\n> 1.  The block timestamp may not be lower than the median of the last 11\n>     blocks'\n>\n> 2.  The block timestamp may not be greater than the current time plus two\n>     hours\n>\n> 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106\n>     06:28:16 +0000)\n\nWhat happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?\n\nIn that case:\n\n1.  Is not violated, since \"not lower than\" means \"greater than or equal to\", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF == 0xFFFFFFFF\n2.  Is not violated, since it would be a past actual real time.\n3.  Is not violated since 0xFFFFFFFF < 0x100000000.\n\nIn that case, we could then add an additional rule, which is that a 64-bit (or 128-bit, or 256-bit) timestamp has to be present in the coinbase transaction, with similar rules except translated to 64-bit/128-bit/256-bit.\n\nPossibly a similar scheme could be used for `nLockTime`; we could put a 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1 if the legacy v`nLockTime` is at the maximum seconds-timelock possible.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-10-16T09:06:17",
                "message_text_only": "> What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?\n\nThe chain will halt for all old clients, because there is no 32-bit value greater than 0xffffffff.\n\n> 1. Is not violated, since \"not lower than\" means \"greater than or equal to\"\n\nNo, because it has to be strictly \"greater than\" in the Bitcoin Core source code, it is rejected when it is \"lower or equal to\", see: https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094\n\n> 2. Is not violated, since it would be a past actual real time.\n\nIf the current time is 0x0000000100000000, then the lowest 32 bits will point to some time around 1970, so for old clients two rules are violated at the same time.\n\n> 3. Is not violated since 0xFFFFFFFF < 0x100000000.\n\nThis is hard to change, because 32-bit timestamps are included in block headers, so using any wider data type here will make it hardware-incompatible and will cause a hard-fork. That's why I think new timestamps should be placed in the coinbase transaction. But that still does not solve chain halting problem.\n\nTo test chain halting, all that is needed is starting regtest and producing one block with 0xffffffff timestamp, just after the Genesis Block. Then, median time is equal to 0xffffffff and adding any new blocks is no longer possible. The only soft-fork solution I can see require overwriting that block.\n\nExample from https://bitcointalk.org/index.php?topic=5365359.0\n\nsubmitblock 0100000006226e46111a0b59caaf126043eb5bbf28c34f3a5e332a1fc7b2b73cf188910f3663c0de115e2239e05df4df9c4bfa01b8e843aaf5dae590cac1d9bac0d44c0fffffffffffff7f200100000001020000000001010000000000000000000000000000000000000000000000000000000000000000ffffffff03510101ffffffff0200f2052a010000001976a91462e907b15cbf27d5425399ebf6f0fb50ebb88f1888ac0000000000000000266a24aa21a9ede2f61c3f71d1defd3fa999dfa36953755c690689799962b48bebd836974e8cf90120000000000000000000000000000000000000000000000000000000000000000000000000\nnull\ngeneratetoaddress 1 mpXwg4jMtRhuSpVq4xS3HFHmCmWp9NyGKt\nCreateNewBlock: TestBlockValidity failed: time-too-old, block's timestamp is too early (code -1)\n\nI don't know any timestamp that can be used in any next block and accepted by old nodes.\n\nOn 2021-10-16 01:01:54 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> Good morning yanmaani,\n\n\n> It's well-known. Nobody really cares, because it's so far off. Not\n> possible to do by softfork, no.\n\nI think it is possible by softfork if we try hard enough?\n\n\n> 1.  The block timestamp may not be lower than the median of the last 11\n>     blocks'\n>\n> 2.  The block timestamp may not be greater than the current time plus two\n>     hours\n>\n> 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106\n>     06:28:16 +0000)\n\nWhat happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?\n\nIn that case:\n\n1.  Is not violated, since \"not lower than\" means \"greater than or equal to\", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF == 0xFFFFFFFF\n2.  Is not violated, since it would be a past actual real time.\n3.  Is not violated since 0xFFFFFFFF < 0x100000000.\n\nIn that case, we could then add an additional rule, which is that a 64-bit (or 128-bit, or 256-bit) timestamp has to be present in the coinbase transaction, with similar rules except translated to 64-bit/128-bit/256-bit.\n\nPossibly a similar scheme could be used for `nLockTime`; we could put a 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1 if the legacy v`nLockTime` is at the maximum seconds-timelock possible.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "David Bakin",
                "date": "2021-10-16T20:37:27",
                "message_text_only": "yes but ... just for the sake of argument ... if a change such as this\nwraparound interpretation is made anytime in the next 5 years it'll be over\na *decade after that *before any wrapped-around timestamp is legitimately\nmined ... and by then nobody will be running incompatible (decade old) node\nsoftware (especially since it would mean that a decade had gone by without\na *single* consensus change ... seems very unlikely).\n\nOn Sat, Oct 16, 2021 at 11:57 AM vjudeu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the\n> appropriate time?\n>\n> The chain will halt for all old clients, because there is no 32-bit value\n> greater than 0xffffffff.\n>\n> > 1. Is not violated, since \"not lower than\" means \"greater than or equal\n> to\"\n>\n> No, because it has to be strictly \"greater than\" in the Bitcoin Core\n> source code, it is rejected when it is \"lower or equal to\", see:\n> https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094\n>\n> > 2. Is not violated, since it would be a past actual real time.\n>\n> If the current time is 0x0000000100000000, then the lowest 32 bits will\n> point to some time around 1970, so for old clients two rules are violated\n> at the same time.\n>\n> > 3. Is not violated since 0xFFFFFFFF < 0x100000000.\n>\n> This is hard to change, because 32-bit timestamps are included in block\n> headers, so using any wider data type here will make it\n> hardware-incompatible and will cause a hard-fork. That's why I think new\n> timestamps should be placed in the coinbase transaction. But that still\n> does not solve chain halting problem.\n>\n> To test chain halting, all that is needed is starting regtest and\n> producing one block with 0xffffffff timestamp, just after the Genesis\n> Block. Then, median time is equal to 0xffffffff and adding any new blocks\n> is no longer possible. The only soft-fork solution I can see require\n> overwriting that block.\n>\n> Example from https://bitcointalk.org/index.php?topic=5365359.0\n>\n> submitblock\n> 0100000006226e46111a0b59caaf126043eb5bbf28c34f3a5e332a1fc7b2b73cf188910f3663c0de115e2239e05df4df9c4bfa01b8e843aaf5dae590cac1d9bac0d44c0fffffffffffff7f200100000001020000000001010000000000000000000000000000000000000000000000000000000000000000ffffffff03510101ffffffff0200f2052a010000001976a91462e907b15cbf27d5425399ebf6f0fb50ebb88f1888ac0000000000000000266a24aa21a9ede2f61c3f71d1defd3fa999dfa36953755c690689799962b48bebd836974e8cf90120000000000000000000000000000000000000000000000000000000000000000000000000\n> null\n> generatetoaddress 1 mpXwg4jMtRhuSpVq4xS3HFHmCmWp9NyGKt\n> CreateNewBlock: TestBlockValidity failed: time-too-old, block's timestamp\n> is too early (code -1)\n>\n> I don't know any timestamp that can be used in any next block and accepted\n> by old nodes.\n>\n> On 2021-10-16 01:01:54 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> > Good morning yanmaani,\n>\n>\n> > It's well-known. Nobody really cares, because it's so far off. Not\n> > possible to do by softfork, no.\n>\n> I think it is possible by softfork if we try hard enough?\n>\n>\n> > 1.  The block timestamp may not be lower than the median of the last 11\n> >     blocks'\n> >\n> > 2.  The block timestamp may not be greater than the current time plus two\n> >     hours\n> >\n> > 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106\n> >     06:28:16 +0000)\n>\n> What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the\n> appropriate time?\n>\n> In that case:\n>\n> 1.  Is not violated, since \"not lower than\" means \"greater than or equal\n> to\", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF ==\n> 0xFFFFFFFF\n> 2.  Is not violated, since it would be a past actual real time.\n> 3.  Is not violated since 0xFFFFFFFF < 0x100000000.\n>\n> In that case, we could then add an additional rule, which is that a 64-bit\n> (or 128-bit, or 256-bit) timestamp has to be present in the coinbase\n> transaction, with similar rules except translated to 64-bit/128-bit/256-bit.\n>\n> Possibly a similar scheme could be used for `nLockTime`; we could put a\n> 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1\n> if the legacy v`nLockTime` is at the maximum seconds-timelock possible.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211016/c540960f/attachment.html>"
            },
            {
                "author": "Kate Salazar",
                "date": "2021-10-16T21:34:32",
                "message_text_only": "Hi, BIP 42 is a code base consensus soft fork that at the time of\nactivation does not really manifest as a fork because nobody is running any\ncode not already applying it. Can a similar thing be done in 17 years? (I\nhaven't really made sense of this year 2038 problem, I don't know or\nunderstand what is required if there's something to be done).\nCheers!\n\nOn Sat, Oct 16, 2021 at 11:00 PM David Bakin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> yes but ... just for the sake of argument ... if a change such as this\n> wraparound interpretation is made anytime in the next 5 years it'll be over\n> a *decade after that *before any wrapped-around timestamp is legitimately\n> mined ... and by then nobody will be running incompatible (decade old) node\n> software (especially since it would mean that a decade had gone by without\n> a *single* consensus change ... seems very unlikely).\n>\n> On Sat, Oct 16, 2021 at 11:57 AM vjudeu via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the\n>> appropriate time?\n>>\n>> The chain will halt for all old clients, because there is no 32-bit value\n>> greater than 0xffffffff.\n>>\n>> > 1. Is not violated, since \"not lower than\" means \"greater than or equal\n>> to\"\n>>\n>> No, because it has to be strictly \"greater than\" in the Bitcoin Core\n>> source code, it is rejected when it is \"lower or equal to\", see:\n>> https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094\n>>\n>> > 2. Is not violated, since it would be a past actual real time.\n>>\n>> If the current time is 0x0000000100000000, then the lowest 32 bits will\n>> point to some time around 1970, so for old clients two rules are violated\n>> at the same time.\n>>\n>> > 3. Is not violated since 0xFFFFFFFF < 0x100000000.\n>>\n>> This is hard to change, because 32-bit timestamps are included in block\n>> headers, so using any wider data type here will make it\n>> hardware-incompatible and will cause a hard-fork. That's why I think new\n>> timestamps should be placed in the coinbase transaction. But that still\n>> does not solve chain halting problem.\n>>\n>> To test chain halting, all that is needed is starting regtest and\n>> producing one block with 0xffffffff timestamp, just after the Genesis\n>> Block. Then, median time is equal to 0xffffffff and adding any new blocks\n>> is no longer possible. The only soft-fork solution I can see require\n>> overwriting that block.\n>>\n>> Example from https://bitcointalk.org/index.php?topic=5365359.0\n>>\n>> submitblock\n>> 0100000006226e46111a0b59caaf126043eb5bbf28c34f3a5e332a1fc7b2b73cf188910f3663c0de115e2239e05df4df9c4bfa01b8e843aaf5dae590cac1d9bac0d44c0fffffffffffff7f200100000001020000000001010000000000000000000000000000000000000000000000000000000000000000ffffffff03510101ffffffff0200f2052a010000001976a91462e907b15cbf27d5425399ebf6f0fb50ebb88f1888ac0000000000000000266a24aa21a9ede2f61c3f71d1defd3fa999dfa36953755c690689799962b48bebd836974e8cf90120000000000000000000000000000000000000000000000000000000000000000000000000\n>> null\n>> generatetoaddress 1 mpXwg4jMtRhuSpVq4xS3HFHmCmWp9NyGKt\n>> CreateNewBlock: TestBlockValidity failed: time-too-old, block's timestamp\n>> is too early (code -1)\n>>\n>> I don't know any timestamp that can be used in any next block and\n>> accepted by old nodes.\n>>\n>> On 2021-10-16 01:01:54 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>> > Good morning yanmaani,\n>>\n>>\n>> > It's well-known. Nobody really cares, because it's so far off. Not\n>> > possible to do by softfork, no.\n>>\n>> I think it is possible by softfork if we try hard enough?\n>>\n>>\n>> > 1.  The block timestamp may not be lower than the median of the last 11\n>> >     blocks'\n>> >\n>> > 2.  The block timestamp may not be greater than the current time plus\n>> two\n>> >     hours\n>> >\n>> > 3.  The block timestamp may not be greater than 2^32 (Sun, 07 Feb 2106\n>> >     06:28:16 +0000)\n>>\n>> What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the\n>> appropriate time?\n>>\n>> In that case:\n>>\n>> 1.  Is not violated, since \"not lower than\" means \"greater than or equal\n>> to\", and after a while the median becomes 0xFFFFFFFF and 0xFFFFFFFF ==\n>> 0xFFFFFFFF\n>> 2.  Is not violated, since it would be a past actual real time.\n>> 3.  Is not violated since 0xFFFFFFFF < 0x100000000.\n>>\n>> In that case, we could then add an additional rule, which is that a\n>> 64-bit (or 128-bit, or 256-bit) timestamp has to be present in the coinbase\n>> transaction, with similar rules except translated to 64-bit/128-bit/256-bit.\n>>\n>> Possibly a similar scheme could be used for `nLockTime`; we could put a\n>> 64-bit `nLockTime64` in that additional signed block in Taproot SegWit v1\n>> if the legacy v`nLockTime` is at the maximum seconds-timelock possible.\n>>\n>> Regards,\n>> ZmnSCPxj\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211016/25370194/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-16T23:23:15",
                "message_text_only": "Good morning vjudeu,\n\n> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?\n>\n> The chain will halt for all old clients, because there is no 32-bit value greater than 0xffffffff.\n>\n> > 1.  Is not violated, since \"not lower than\" means \"greater than or equal to\"\n>\n> No, because it has to be strictly \"greater than\" in the Bitcoin Core source code, it is rejected when it is \"lower or equal to\", see:https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094\n\nThen starting at Unix Epoch 0x80000000, post-softfork nodes just increment the timestamp by 1 on each new block.\nThis just kicks the can since that then imposes a limit on the maximum number of blocks, but at least the unit is now ~10 minutes instead of 1 second, a massive x600 increase in the amount of time we are forced to hardfork.\n\nOn the other hand, this does imply that the difficulty calculation will become astronomically and ludicrously high, since pre-softfork nodes will think that blocks are arriving at the rate of 1 per second, so ...\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2021-10-17T07:24:43",
                "message_text_only": "> Then starting at Unix Epoch 0x80000000, post-softfork nodes just increment the timestamp by 1 on each new block.\n\nIt is possible to go even faster. The fastest rate is something like that, if you assume the time in the Genesis Block is zero:\n\n0 1 2 2 3 3 3 3 4 4 4 4 4 4 5 5 5 5 5 5 6 ...\n\nThen you can increment timestamps once per 6 blocks, that means x3600 increase, but then the difficulty is always multiplied by four, so you have to increase time once per difficulty change to keep it on real level, then it will wave between being multiplied by 4 and by 0.25.\n\nOn 2021-10-17 01:23:24 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> Good morning vjudeu,\n\n> > What happens if a series of blocks has a timestamp of 0xFFFFFFFF at the appropriate time?\n>\n> The chain will halt for all old clients, because there is no 32-bit value greater than 0xffffffff.\n>\n> > 1.  Is not violated, since \"not lower than\" means \"greater than or equal to\"\n>\n> No, because it has to be strictly \"greater than\" in the Bitcoin Core source code, it is rejected when it is \"lower or equal to\", see:https://github.com/bitcoin/bitcoin/blob/6f0cbc75be7644c276650fd98bfdb6358b827399/src/validation.cpp#L3089-L3094\n\nThen starting at Unix Epoch 0x80000000, post-softfork nodes just increment the timestamp by 1 on each new block.\nThis just kicks the can since that then imposes a limit on the maximum number of blocks, but at least the unit is now ~10 minutes instead of 1 second, a massive x600 increase in the amount of time we are forced to hardfork.\n\nOn the other hand, this does imply that the difficulty calculation will become astronomically and ludicrously high, since pre-softfork nodes will think that blocks are arriving at the rate of 1 per second, so ...\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Year 2038 problem and year 2106 chain halting",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "James Lu",
                "damian at willtech.com.au",
                "Kate Salazar",
                "yanmaani at cock.li",
                "vjudeu at gazeta.pl",
                "ZmnSCPxj",
                "David Bakin"
            ],
            "messages_count": 15,
            "total_messages_chars_count": 36085
        }
    },
    {
        "title": "[bitcoin-dev] Proposal: Package Mempool Accept and Package RBF",
        "thread_messages": [
            {
                "author": "darosior",
                "date": "2021-10-14T10:48:55",
                "message_text_only": "Hi Gloria,\n\n> In summary, it seems that the decisions that might still need attention/input from devs on this mailing list are:\n> 1. Whether we should start with multiple-parent-1-child or 1-parent-1-child.\n> 2. Whether it's ok to require that the child not have conflicts with mempool transactions.\n\nI would like to point out that package relay is not only useful in Lightning's adversarial scenarii but also for a better user experience of CPFP.\nTake for instance a wallet managing coins it can only spend using pre-signed transactions. It may batch these coins into a single transaction, but only after broadcasting the pre-signed tx for each of these coins.\nSo for a 3 utxos it'd be:\ncoin1 -----> pres. tx1 ----- |\ncoin2 -----> pres. tx2 ----- | - - - spending transaction\ncoin3 -----> pres. tx3 ----- |\n\nNow all these pre-signed transactions are pre-signed with a fixed feerate, which might be below the mempool minimum fee at the time of broadcast.\nThis is a usecase for multiple-parents-1-child packages. This is also something we do for Revault: you have pre-signed Unvault transactions, each have a CPFP output [0]. Since their confirmation is not security critical, you'd really want to batch the child-fee-paying tx.\n\nRegarding 2. i did not come up with a reason for dropping this rule (yet?) since if you need to replace the child you can use individual submission, and if you need to replace the parent the child itself does not conflict anymore.\n\nThanks for the effort put into requesting feedback,\nAntoine\n\n[0] https://github.com/revault/practical-revault/blob/master/transactions.md#unvault_tx\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211014/9782650f/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Proposal: Package Mempool Accept and Package RBF",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "darosior"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1792
        }
    },
    {
        "title": "[bitcoin-dev] Reorgs on SigNet - Looking for feedback on approach and parameters",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2021-10-15T04:41:50",
                "message_text_only": "On Wed, Sep 15, 2021 at 08:24:43AM -0700, Matt Corallo via bitcoin-dev wrote:\n> > On Sep 13, 2021, at 21:56, Anthony Towns <aj at erisian.com.au> wrote:\n> > I'm not sure that's really the question you want answered?\n> Of course it is? I\u2019d like to understand the initial thinking and design analysis that went into this decision. That seems like an important question to ask when seeking changes in an existing system :).\n\nWell, \"are there any drawbacks to doing X instead, because that would make\nit easier for me to do Y\" just seems like a more interesting question?\nBecause:\n\n> > Mostly\n> > it's just \"this is how mainnet works\" plus \"these are the smallest\n> > changes to have blocks be chosen by a signature, rather than entirely\n> > by PoW competition\".\n\ndoesn't seem like that interesting an answer...\n\nTo be a bit more specific, it's not at all clear to me what you would\nbe happy with? (I mean, beyond \"something magic that works exactly how\nI want it, when I want it, even if I don't know what that is yet or\nchange my mind later\" which is obviously the desired behaviour for all\nsoftware everywhere) \n\nYou say you're happier with both mainnet and testnet3 than signet,\nbut mainnet isn't any faster than signet, while (if you've got an ASIC)\ntestnet3 will give you a block per second, especially if you don't mind\nyour blocks getting reorged out. There's a lot of ground between those\ntwo extremes.\n\n> > For integration testing across many services, I think a ten-minute-average\n> > between blocks still makes sense -- protocols relying on CSV/CLTV to\n> > ensure there's a delay they can use to recover funds, if they specify\n> > that in blocks (as lightning's to_self_delay does), then significant\n> > surges of blocks will cause uninteresting bugs. \n> Hmm, why would blocks coming quicker lead to a bug? I certainly hope no one has a bug if their block time is faster than per ten minutes. I presume here, you mean something like \u201cif the node can\u2019t keep up with the block rate\u201d, but I certainly hope the benchmark for may isn\u2019t 10 minutes, or really even one.\n\nThe lightning to_self_delay is specified in blocks, but is meant to allow\nyou to be offline for some real time period; if you specify 1000 blocks\nand are sure you'll be online every two days, that's fine on mainnet\nand signet as it stands, but broken on testnet.\n\n> > It would be easy enough to change things to target an average of 2 or\n> > 5 minutes, I suppose, but then you'd probably need to propogate that\n> > logic back into your apps that would otherwise think 144 blocks is around\n> > about a day.\n> Why? One useful thing for testing is compressing real time.\n\nSure, but if you're compressing _real_ time you need to manipulate the\nnTime not just the number of blocks -- and that might be relevant for\nnLocktime or nSequence checks by mtp rather than height. But that's\nnot something signet's appropriate for: you should be using regtest for\nthat scenario.\n\n> > We could switch back to doing blocks exactly every 10 minutes, rather\n> > than a poisson-ish distribution in the range of 1min to 60min, but that\n> > doesn't seem like that huge a win, and makes it hard to test that things\n> > behave properly when blocks arrive in bursts.\n> Hmm, I suppose? If you want to test that the upper bound doesn\u2019t\n> need to be 100 minutes, though, it could be 10.\n\nMathematically, you can't have an average of 10 minutes and a max of 10\nminutes without the minimum also being 10 minutes...\n\n> > Best of luck to you then? Nobody's trying to sell you on a subscription\n> > plan to using signet.\n> lol, yes, I\u2019m aware of that, nor did I mean to imply that anything has to be targeted at a specific person\u2019s requirements. Rather, my point here is that I\u2019m really confused as to who  the target user *is*, because we should be building products with target users in mind, even if those targets are often \u201cme\u201d for open source projects.\n\nI don't really think there's a definitive answer to that yet?\n\nMy guess is \"integration testing\" is close to right; whether it be\ndifferent services validating they interoperate, or users seeing if a\nservice works the way they expect in a nearly-live environment.\n\nFor private signets, the advantage over regtest is you don't risk some\nrandom participant causing major reorgs, and can reasonably use it over\nthe internet without having to worry too much about securing things.\n\nFor the default public signet, the advantage over regtest is probably that\nyou've got additional infrastructure already setup (eg explorer.bc-2.jp\nand mempool.space/signet, perhaps eventually a decent lightning test\nnetwork? there's signet-lightning.wakiyamap.dev)\n\nThe advantage of a private signet vs the default public one is probably\nonly that you can control the consensus rules to introduce and test a\nnew soft fork if you want. The advantage of the default public signet\nover your own private one is probably mostly that it already has\nminers/explorers/faucets setup and you don't have to do that yourself.\n\nI think the default public signet makes sense as a \"demo mainnet\" --\nsomewhere you can do all the things you can on mainnet, with pretty\nsimilar constraints, but some more interesting behaviours -- like bigger\nreorgs, or earlier availability of new features.\n\nSo to me that adds up as:\n\n * are you a single developer making software? do it on regtest so you've\n   got complete control\n\n * are you trying out some software? so it on the default public signet,\n   because that's the least effort and will give you a pretty good idea\n   how it will behave on mainnet\n\n * are you checking your software interoperates with some random other\n   project in the way you expect? again, default public signet, since\n   hopefully the other project already supports it\n\n * are you doing a collaboration/partnership with another development\n   team? do you want a stable environment for your team to collaborate\n   on? setup your own private signet for the test environment (but do\n   local tests and CI tests via regtest too)\n\nYMMV, that's just my opinion, etc.\n\nFWIW, I was frustrated the other day when trying to mine [0] and seeing\nthe next block wasn't due for ~40 minutes (and the spend script had a\nCSV constraint, so I had to wait for it to be mined before I could get\nthe followup txs in the mempool). OTOH, it wasn't frustrating enough to\neither mine a block early or switch to regtest, so... I guess I see the\npoint, but not really any particular action to take for it?\n\n[0] https://explorer.bc-2.jp/tx/ba58d99dfaad83e105a0de1a9becfcf8eaf897aaaada54bd7b08134ff579997c?input:0&expand\n\nCheers,\naj"
            }
        ],
        "thread_summary": {
            "title": "Reorgs on SigNet - Looking for feedback on approach and parameters",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6584
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin-s 1.8 released with DLCs negotiation via Tor",
        "thread_messages": [
            {
                "author": "Chris Stewart",
                "date": "2021-10-18T19:09:14",
                "message_text_only": "Hi all,\n\nWe released 1.8 today of bitcoin-s. This includes support for opening DLCs\nover tor. This makes the UX much simpler to enter into a DLC with your\ncounterparty.\n\nAs part of this release, we wrote two detailed examples of entering into\n\n1. A wallet election example\n<https://bitcoin-s.org/docs/next/wallet/wallet-election-example>\n2. A wallet price example\n<https://bitcoin-s.org/docs/next/wallet/wallet-price-example>\n\nThese are meant to complement the existing oracle examples we have\n\n1. An election oracle example\n<https://bitcoin-s.org/docs/next/oracle/oracle-election-example>\n2. A price oracle example\n<https://bitcoin-s.org/docs/next/oracle/oracle-price-example>\n\n*This is alpha software, please do not use it for large amounts of money.*\n\nYou need a github account to access the downloads, the artifacts are at the\nvery bottom of the release notes.\n\nhttps://github.com/bitcoin-s/bitcoin-s/releases/tag/1.8.0\n\nYou can find a demonstration video for using the wallet to enter into a DLC\nover tor here:\n\nhttps://www.youtube.com/watch?v=oR0I0aHxNMM\n\nYou can find the dlc spec here:\n\nhttps://github.com/discreetlogcontracts/dlcspecs\n\n-Chris\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211018/4a380e44/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin-s 1.8 released with DLCs negotiation via Tor",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Stewart"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1334
        }
    },
    {
        "title": "[bitcoin-dev] bitcoin.org missing bitcoin core version 22.0",
        "thread_messages": [
            {
                "author": "Owen Gunden",
                "date": "2021-10-20T12:54:36",
                "message_text_only": "When I search for \"download bitcoin core\" my top result is bitcoin.org, \nwhich is out of date and doesn't have 22.0.\n\nIt seems confusing to have two sites that seemingly both represent \nbitcoin core.\n\nMaybe the download links could be removed from bitcoin.org and instead \nit could just link to bitcoincore.org?"
            },
            {
                "author": "Prayank",
                "date": "2021-10-20T14:47:17",
                "message_text_only": "Hi Owen,\n\n>\u00a0When I search for \"download bitcoin core\" my top result is bitcoin.org, which is out of date and doesn't have 22.0\n\nThis is an issue related to SEO which only website owners can fix or maybe others can help who know better.\n\n>\u00a0It seems confusing to have two sites that seemingly both represent bitcoin core.\n\nThere is only one website which represents Bitcoin Core full node implementation. You can download Bitcoin Core from https://bitcoincore.org\nEnsure that you are using the correct domain as some people have registered domains which use punycode, looks similar and spreading malware:\u00a0https://bitcoin.stackexchange.com/a/107738/\n\n>\u00a0Maybe the download links could be removed from bitcoin.org and instead it could just link to bitcoincore.org?\n\nYou can open an issue in website repository:\u00a0https://github.com/bitcoin-dot-org/bitcoin.org\u00a0and tag Cobra who owns the website and domain.\n\nAlternately you could also try a derivative of Bitcoin Core:\u00a0https://bitcoinknots.org/\u00a0maintained by Luke Dashjr.\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211020/54d849db/attachment.html>"
            },
            {
                "author": "Owen Gunden",
                "date": "2021-10-20T19:20:54",
                "message_text_only": "On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:\n> > It seems confusing to have two sites that seemingly both represent\n> > bitcoin core.\n>\n> There is only one website which represents Bitcoin Core full node\n> implementation. You can download Bitcoin Core from\n> https://bitcoincore.org\n\nI also notice that, as of 22.0, Wladimir is no longer signing the\nreleases, and I have no trust in my gpg network of the people who seem\nto have replaced him.\n\nGiven the level of security at stake here, my eyebrows are raised at\nthis combination of items changing (new website + new gpg signers at the\nsame time)."
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-10-20T19:37:48",
                "message_text_only": "On Wednesday, October 20th, 2021 at 3:20 PM, Owen Gunden via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I also notice that, as of 22.0, Wladimir is no longer signing the\n> releases, and I have no trust in my gpg network of the people who seem\n> to have replaced him.\n\nThis is not correct. Here are Wladimir's attestations on the 22.0 release: https://github.com/bitcoin-core/guix.sigs/tree/main/22.0/laanwj\n\nThere is no separate special release key anymore though. Instead, the build attestations (by anyone) can be used as your trust basis.\n\n> Given the level of security at stake here, my eyebrows are raised at\n> this combination of items changing (new website + new gpg signers at the\n> same time).\n\nThere is no new website. The Bitcoin Core project website has been https://bitcoincore.org for years. I don't know why https://bitcoin.org hasn't updated to list the 22.0 release, though; that's up to them.\n\nCheers,\n\n--\nPieter"
            },
            {
                "author": "Owen Gunden",
                "date": "2021-10-20T19:49:19",
                "message_text_only": "On Wed, Oct 20, 2021 at 07:37:48PM +0000, Pieter Wuille wrote:\n> On Wednesday, October 20th, 2021 at 3:20 PM, Owen Gunden via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > I also notice that, as of 22.0, Wladimir is no longer signing the\n> > releases, and I have no trust in my gpg network of the people who seem\n> > to have replaced him.\n>\n> This is not correct. Here are Wladimir's attestations on the 22.0\n> release:\n> https://github.com/bitcoin-core/guix.sigs/tree/main/22.0/laanwj\n>\n> There is no separate special release key anymore though. Instead, the\n> build attestations (by anyone) can be used as your trust basis.\n\nAh, that explains it, thanks Pieter. I was looking for a signature from\nthe release key 01EA 5486 DE18 A882 D4C2  6845 90C8 019E 36C2 E964,\nwhich was still being supplied as recently as 0.21.1.\n\n> There is no new website. The Bitcoin Core project website has been\n> https://bitcoincore.org for years.\n\nSome of us are very old :)."
            },
            {
                "author": "Charles Hill",
                "date": "2021-10-20T19:43:27",
                "message_text_only": "Hello, Owen,\n\nThe GPG signature verification has changed for bitcoin core version 22 \nand later. There were two main changes:\n\n1) The sha256 checksums are now in a separate file from the GPG \nsignatures. So download a new file named \"SHA256SUMS\" (contains the \nchecksums) and also the \"SHA256SUMS.asc\" which contains the signatures.\n\n2) The signature file now contains multiple signatures. These signatures \nare generated by multiple \"builders\" who have provided their own public \nkeys to verify against. Not all builders will provide a signature for \neach release.\n\nYou can find more information at bitcoincore.org/en/download/ [1] under \nthe \"Linux verification instructions\" section - click to expand.\n\nInstructions about where to find and how to import the full list of \n\"builder\" public keys can be found in the bitcoin core github repo [2].\n\n > I also notice that, as of 22.0, Wladimir is no longer signing the \nreleases, and I have no trust in my gpg network of the people who seem \nto have replaced him.\n\nThe list of \"builder\" public keys includes many long-time bitcoin core \ncontributors as well as Wladimir's. Caution is always warranted but \nplease do not spread unnecessary FUD.\n\n- chill\n\n[1] https://bitcoincore.org/en/download/\n[2] https://github.com/bitcoin/bitcoin/tree/master/contrib/builder-keys\n\n\nOn 10/20/21 8:20 PM, Owen Gunden via bitcoin-dev wrote:\n> On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:\n>>> It seems confusing to have two sites that seemingly both represent\n>>> bitcoin core.\n>> There is only one website which represents Bitcoin Core full node\n>> implementation. You can download Bitcoin Core from\n>> https://bitcoincore.org\n> I also notice that, as of 22.0, Wladimir is no longer signing the\n> releases, and I have no trust in my gpg network of the people who seem\n> to have replaced him.\n>\n> Given the level of security at stake here, my eyebrows are raised at\n> this combination of items changing (new website + new gpg signers at the\n> same time).\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Kate Salazar",
                "date": "2021-10-20T20:18:03",
                "message_text_only": "Hi Owen,\n\nOn Wed, Oct 20, 2021 at 9:25 PM Owen Gunden via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Wed, Oct 20, 2021 at 04:47:17PM +0200, Prayank wrote:\n> > > It seems confusing to have two sites that seemingly both represent\n> > > bitcoin core.\n> >\n> > There is only one website which represents Bitcoin Core full node\n> > implementation. You can download Bitcoin Core from\n> > https://bitcoincore.org\n>\n> I also notice that, as of 22.0, Wladimir is no longer signing the\n> releases, and I have no trust in my gpg network of the people who seem\n> to have replaced him.\n>\n\nHe is taking the most sensible way forward, decreasing bus factor.\n\nRead: https://laanwj.github.io/2021/01/21/decentralize.html\n\n\n>\n> Given the level of security at stake here, my eyebrows are raised at\n> this combination of items changing (new website + new gpg signers at the\n> same time).\n>\n\nDon't worry and build your own release;\nbut if you do, always verify the tree hash.\nTrust signed annotated tags.\nCheers!\n\n\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211020/ab18e285/attachment.html>"
            },
            {
                "author": "Andrew Chow",
                "date": "2021-10-20T21:50:13",
                "message_text_only": "On 10/20/2021 03:20 PM, Owen Gunden via bitcoin-dev wrote:\n> I also notice that, as of 22.0, Wladimir is no longer signing the\n> releases, and I have no trust in my gpg network of the people who seem\n> to have replaced him.\nIt is signed with his personal key, as well as the personal keys of\nseveral other developers.\n\n> Given the level of security at stake here, my eyebrows are raised at\n> this combination of items changing (new website + new gpg signers at the\n> same time).\nbitcoincore.org has been Bitcoin Core's official website for several\nyears. Binaries have not been posted to bitcoin.org by the release\nmaintainer for several major releases. The only reason they are still\navailable there is because bitcoin.org's maintainer mirrors them."
            }
        ],
        "thread_summary": {
            "title": "bitcoin.org missing bitcoin core version 22.0",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Kate Salazar",
                "Charles Hill",
                "Owen Gunden",
                "Andrew Chow",
                "Pieter Wuille",
                "Prayank"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 8403
        }
    },
    {
        "title": "[bitcoin-dev] death to the mempool, long live the mempool",
        "thread_messages": [
            {
                "author": "lisa neigut",
                "date": "2021-10-26T02:56:21",
                "message_text_only": "Hi all,\n\nIn a recent conversation with @glozow, I had the realization that the\nmempool is obsolete and should be eliminated.\n\nInstead, users should submit their transactions directly to mining pools,\npreferably over an anonymous communication network such as tor. This can\neasily be achieved by mining pools running a tor onion node for this\nexpress purpose (or via a lightning network extension etc)\n\nMempools make sense in a world where mining is done by a large number of\nparticipating nodes, eg where the block template is constructed by a\nmajority of the participants on the network. In this case, it is necessary\nto socialize pending transaction data to all participants, as you don\u2019t\nknow which participant will be constructing the winning block template.\n\nIn reality however, mempool relay is unnecessary where the majority of\nhashpower and thus block template creation is concentrated in a\nsemi-restricted set.\n\nRemoving the mempool would greatly reduce the bandwidth requirement for\nrunning a node, keep intentionality of transactions private until\nconfirmed/irrevocable, and naturally resolve all current issues inherent in\npackage relay and rbf rules. It also resolves the recent minimum relay\nquestions, as relay is no longer a concern for unmined transactions.\n\nProvided the number of block template producing actors remains beneath, say\n1000, it\u2019d be quite feasible to publish a list of tor endpoints that nodes\ncan independently  + directly submit their transactions to. In fact, merely\nallowing users to select their own list of endpoints to use alternatively\nto the mempool would be a low effort starting point for the eventual\nreplacement.\n\nOn the other hand, removing the mempool would greatly complicate solo\nmining and would also make BetterHash proposals, which move the block\ntemplate construction away from a centralized mining pool back to the\nindividual miner, much more difficult. It also makes explicit the target\nfor DoS attacks.\n\nA direct communication channel between block template construction venues\nand transaction proposers also provides a venue for direct feedback wrt\nacceptable feerates at the time, which both makes transaction confirmation\ntimelines less variable as well as provides block producers a mechanism for\n(independently) enforcing their own minimum security budget. In other\nwords, expressing a minimum acceptable feerate for continued operation.\n\nInitial feerate estimation would need to be based on published blocks, not\npending transactions (as this information would no longer be available), or\nfrom direct interactions with block producers.\n\n\n~niftynei\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211025/a5c7aebf/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-26T08:02:24",
                "message_text_only": "Good morning lisa,\n\n> Hi all,\n>\n> In a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.\n>\n> Instead, users should submit their transactions directly to mining pools, preferably over an anonymous communication network such as tor. This can easily be achieved by mining pools running a tor onion node for this express purpose (or via a lightning network extension etc)\n>\n> Mempools make sense in a world where mining is done by a large number of participating nodes, eg where the block template is constructed by a majority of the participants on the network. In this case, it is necessary to socialize pending transaction data to all participants, as you don\u2019t know which participant will be constructing the winning block template.\n>\n> In reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.\u00a0\n>\n> Removing the mempool would greatly reduce the bandwidth requirement for running a node, keep intentionality of transactions private until confirmed/irrevocable, and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.\n>\n> Provided the number of block template producing actors remains beneath, say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that nodes can independently \u00a0+ directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.\n>\n> On the other hand, removing the mempool would greatly complicate solo mining and would also make BetterHash proposals, which move the block template construction away from a centralized mining pool back to the individual miner, much more difficult. It also makes explicit the target for DoS attacks.\n\nUnfortunately, this requires that miners have a persistent identity by which they can be contacted.\nWhile pseudonymity is possible, we all know that in practice, it can be easily pierced.\nFor instance, consider that the injunction against address reuse is a recognition that a persistent pseudonym is a privacy leak.\n\nIdeally, the mining set should be as anonymous as possible, as some attacks are possible with sufficient hashpower, and making the miners keep a persistent identity by which they can be found may enable easier state co-option of mines.\nThe strongest man on Earth cannot destroy his enemy if he does not know who and where his enemy is; so with enemies of Bitcoin and the miners of Bitcoin.\n(granted, near every darned mining pool self-identifies, sigh, wtf)\n\nIdeally, the set of relaying nodes hides the miners.\nOf course, in practice we can have a good guess of which relaying nodes are miners and which are not -- those who get blocks earlier are probably miners.\nAgainst this, we should note that this method of identification is probabilistic and not absolute (whereas miners advertising their services so they can be contacted and given unconfirmed transactions are a *definite* flag \"I am a miner\").\nAnd there is always the chance, however slim, that some node that has not been getting blocks \"early\" suddenly decides to buy a mining rig and start mining.\n\nIn short: what you propose is to switch to side fee markets (as I understand it).\nNon-side fees are simply an anonymity layer, by which neither the miner nor the transactor need to know the identity of each other, they simply broadcast to the wider world.\nThis anonymity layer remains important, however, as they help maintain the fee market: https://github.com/libbitcoin/libbitcoin-system/wiki/Side-Fee-Fallacy\n\n\nUltimately, my objection here is simply that this requires miners to identify themselves.\nIn practice, miners already identify themselves (even though they really, really should not), so this objection may be moot at this point.\n\n(Not to mention: something like P2Pool, as-is, would not work well in that model; you would need to implement a mempool for those as well)\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "eric at voskuil.org",
                "date": "2021-10-26T08:31:18",
                "message_text_only": "Agree ZmnSCPxj\n\nHi lisa,\n\nI'm all for removing it from memory. :) Did that a while ago. We just call it the transaction pool.\n\nThere will always be unconfirmed transactions floating around (even just from reorgs). Best to store them somewhere. Disk is cheap, block distribution (e.g. compact) works better if you have them already prevalidated, even if you aren't going to mine on them.\n\nHow you get them technically is not so important. There will always be a set of unconfirmed transactions, it's conceptual. But above all, anonymity is very important - on both ends. This is why transactions have integral fees. Anyone can get paid to mine, just need the txs.\n\nMining may be semi-restricted set is today, it may not be tomorrow. Imagine China everywhere, just like financial controls already are. That's when you see what Bitcoin can do from a security standpoint.\n\nTreating miners as someone else is a poor security architecture. Everyone should look like a potential miner on the network, and a potential spender.\n\nI think you are thinking of it a bit backwards. A node is a big pool of connected transactions. Block headers come along occasionally, and impose order on a subset of them.\n\ne\n\n> -----Original Message-----\n> From: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> On Behalf\n> Of ZmnSCPxj via bitcoin-dev\n> Sent: Tuesday, October 26, 2021 1:02 AM\n> To: lisa neigut <niftynei at gmail.com>; Bitcoin Protocol Discussion <bitcoin-\n> dev at lists.linuxfoundation.org>\n> Subject: Re: [bitcoin-dev] death to the mempool, long live the mempool\n> \n> \n> Good morning lisa,\n> \n> > Hi all,\n> >\n> > In a recent conversation with @glozow, I had the realization that the\n> mempool is obsolete and should be eliminated.\n> >\n> > Instead, users should submit their transactions directly to mining pools,\n> preferably over an anonymous communication network such as tor. This can\n> easily be achieved by mining pools running a tor onion node for this express\n> purpose (or via a lightning network extension etc)\n> >\n> > Mempools make sense in a world where mining is done by a large number\n> of participating nodes, eg where the block template is constructed by a\n> majority of the participants on the network. In this case, it is necessary to\n> socialize pending transaction data to all participants, as you don\u2019t know which\n> participant will be constructing the winning block template.\n> >\n> > In reality however, mempool relay is unnecessary where the majority of\n> hashpower and thus block template creation is concentrated in a semi-\n> restricted set.\n> >\n> > Removing the mempool would greatly reduce the bandwidth requirement\n> for running a node, keep intentionality of transactions private until\n> confirmed/irrevocable, and naturally resolve all current issues inherent in\n> package relay and rbf rules. It also resolves the recent minimum relay\n> questions, as relay is no longer a concern for unmined transactions.\n> >\n> > Provided the number of block template producing actors remains beneath,\n> say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that nodes can\n> independently  + directly submit their transactions to. In fact, merely allowing\n> users to select their own list of endpoints to use alternatively to the mempool\n> would be a low effort starting point for the eventual replacement.\n> >\n> > On the other hand, removing the mempool would greatly complicate solo\n> mining and would also make BetterHash proposals, which move the block\n> template construction away from a centralized mining pool back to the\n> individual miner, much more difficult. It also makes explicit the target for DoS\n> attacks.\n> \n> Unfortunately, this requires that miners have a persistent identity by which\n> they can be contacted.\n> While pseudonymity is possible, we all know that in practice, it can be easily\n> pierced.\n> For instance, consider that the injunction against address reuse is a\n> recognition that a persistent pseudonym is a privacy leak.\n> \n> Ideally, the mining set should be as anonymous as possible, as some attacks\n> are possible with sufficient hashpower, and making the miners keep a\n> persistent identity by which they can be found may enable easier state co-\n> option of mines.\n> The strongest man on Earth cannot destroy his enemy if he does not know\n> who and where his enemy is; so with enemies of Bitcoin and the miners of\n> Bitcoin.\n> (granted, near every darned mining pool self-identifies, sigh, wtf)\n> \n> Ideally, the set of relaying nodes hides the miners.\n> Of course, in practice we can have a good guess of which relaying nodes are\n> miners and which are not -- those who get blocks earlier are probably miners.\n> Against this, we should note that this method of identification is probabilistic\n> and not absolute (whereas miners advertising their services so they can be\n> contacted and given unconfirmed transactions are a *definite* flag \"I am a\n> miner\").\n> And there is always the chance, however slim, that some node that has not\n> been getting blocks \"early\" suddenly decides to buy a mining rig and start\n> mining.\n> \n> In short: what you propose is to switch to side fee markets (as I understand it).\n> Non-side fees are simply an anonymity layer, by which neither the miner nor\n> the transactor need to know the identity of each other, they simply broadcast\n> to the wider world.\n> This anonymity layer remains important, however, as they help maintain the\n> fee market: https://github.com/libbitcoin/libbitcoin-system/wiki/Side-Fee-\n> Fallacy\n> \n> \n> Ultimately, my objection here is simply that this requires miners to identify\n> themselves.\n> In practice, miners already identify themselves (even though they really,\n> really should not), so this objection may be moot at this point.\n> \n> (Not to mention: something like P2Pool, as-is, would not work well in that\n> model; you would need to implement a mempool for those as well)\n> \n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-26T08:56:10",
                "message_text_only": "Good morning e, and lisa,\n\n> Agree ZmnSCPxj\n>\n> Hi lisa,\n>\n> I'm all for removing it from memory. :) Did that a while ago. We just call it the transaction pool.\n>\n> There will always be unconfirmed transactions floating around (even just from reorgs). Best to store them somewhere. Disk is cheap, block distribution (e.g. compact) works better if you have them already prevalidated, even if you aren't going to mine on them.\n>\n> How you get them technically is not so important. There will always be a set of unconfirmed transactions, it's conceptual. But above all, anonymity is very important - on both ends. This is why transactions have integral fees. Anyone can get paid to mine, just need the txs.\n>\n> Mining may be semi-restricted set is today, it may not be tomorrow. Imagine China everywhere, just like financial controls already are. That's when you see what Bitcoin can do from a security standpoint.\n>\n> Treating miners as someone else is a poor security architecture. Everyone should look like a potential miner on the network, and a potential spender.\n>\n> I think you are thinking of it a bit backwards. A node is a big pool of connected transactions. Block headers come along occasionally, and impose order on a subset of them.\n\n\nOn the subject of thinking backwards....\n\nThe current design gossips txes.\n\nI believe much of what lisa wants would be doable by gossiping mining endpoints instead of txes.\nThen transactors can connect to mining endpoints.\n\nTx gossip is limited by fees (which is why the RBF rules even exist in the first place).\nThus, mining endpoint gossip must be limited by something as well, such as by requiring some trivial stake of BTC.\n(BTC exchanges are commonplace enough, I believe, that requiring completely new miners (i.e. those who currently own 0 BTC) to acquire some trivial stake of BTC would be feasible; for most people it would be easier to buy BTC than to acquire a mining rig and the supporting infrastructure needed for a mine.)\nWe could have the endpoint encoded in some sign-to-contract or pay-to-contract construction.\n\nMiners can change their identity by spending their stake (which makes nodes drop their endpoint record).\nThen, they can use now-common anonymity techniques --- mostly CoinJoin, but also the upcoming CoinSwap implementation --- to acquire a new stake whose identity is not easily traceable to the previous stake.\n\n(This is not proof-of-stake, BTW --- the stake only attests the mining endpoint (in much the same way published Lightning channels are attested by their funding tx outpoints), and has no effect on block validity, only on gossiping of mining endpoints.)\n\nThe advantage here is that we expect the set of miner identities to change less often than the set of txes, thus reducing global bandwidth usage,\n\nAgainst the above, we must notice that the anonymity-preserving regular changing of staked identity is more expensive than having a persistent identity.\nWE should really design systems where anonymity-preservation should be as cheap as possible, but onchain activity is no longer cheap at all, given the growing importance of Bitcoin.\n\n--\n\nAlso:\n\n> A direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable\n\nUnless you contact ***all*** miners globally, there is always some non-zero probability that one of the miners you did *not* contact (and thus does not have your tx, and thus will not be able to confirm your tx) gets the next block.\nSince miners can enter and leave the network at any time, it is entirely possible that this mechanism *increases* variability rather than decreases it.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "darosior",
                "date": "2021-10-26T14:09:28",
                "message_text_only": "Hi Niftynei,\n\nI share the concerns raised about direct connections to mining pools being a centralization pressure: de-anonymization and an inevitable higher barrier to entry. Making it more difficult to reach smaller miners is another one.\nRegarding fee estimation you state:\n> Initial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available)\nThe current fee estimation algorithm uses both, not only the pending transactions (game-able). Although i agree that past-block(s) based fee estimation isn't that bad, it's worth mentioning that not tracking the confirmation time of relayed transactions drops the ability to have a target in the estimation. That is it's good enough for time-sensitive transactions where you always target the next block but not for other usages which usually target a few dozen of blocks in the future.\n\nHowever, as we discussed recently, i do believe their is a failure mode here. On one hand, directly connecting to pools is already possible today and pretty effective given the current mining centralization. On the other hand, it's not possible for most pre-signed txs protocols to reliably (securely) use the Bitcoin tx relay network today to propagate time-sensitive transactions. Furthermore, even if these are fixed (eg via package-relay for (today's) Lightning Network) it seems like there is a stark contrast between what \"L2 [0] protocols\" need and what regular node operators can reasonably offer. A node operator is incentivized to relay transactions to:\n- have more privacy *if* they use their node to broadcast transactions (make it less distinguishable which relayed transaction comes from the wallet)\n- provide fee estimates *if* they need them\n- avoid bandwidth spikes on block connection (compact block relay)\n\nL2s would ideally need the tx relaying nodes to do more computation and lift their DOS mitigations for all miner-incentive-compatible transactions to eventually be relayed to most of the miners. One obvious instance of such a dilemma is the RBF rules.\nSuch protocols getting increasingly developed and used create a strong incentive for their users/stakeholders to directly connect to mining pools [1], with all the consequences for the network mentioned in the replies to your mail and elsewhere.\nBefore we get to this, i think there is a strong case for an opt-in and publicly accessible \"overlay\" network to relay miner-incentive compatible transactions with higher DOS limits. This way the cost is not imposed to L1 node runners, and L2s can operate with more safety assumptions without (entirely) falling for centralization.\n\nThanks for publicly starting this discussion,\nAntoine\n\n[0] Using \"L2s\" for the sake of brevety, whatever it means i mean \"protocols using pre-signed Bitcoin transactions which timely confirmation might be a security requirement\".\n[1] Wen block space insurance contracts\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nLe mardi 26 octobre 2021 \u00e0 4:56 AM, lisa neigut via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi all,\n>\n> In a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.\n>\n> Instead, users should submit their transactions directly to mining pools, preferably over an anonymous communication network such as tor. This can easily be achieved by mining pools running a tor onion node for this express purpose (or via a lightning network extension etc)\n>\n> Mempools make sense in a world where mining is done by a large number of participating nodes, eg where the block template is constructed by a majority of the participants on the network. In this case, it is necessary to socialize pending transaction data to all participants, as you don\u2019t know which participant will be constructing the winning block template.\n>\n> In reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.\n>\n> Removing the mempool would greatly reduce the bandwidth requirement for running a node, keep intentionality of transactions private until confirmed/irrevocable, and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.\n>\n> Provided the number of block template producing actors remains beneath, say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that nodes can independently + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.\n>\n> On the other hand, removing the mempool would greatly complicate solo mining and would also make BetterHash proposals, which move the block template construction away from a centralized mining pool back to the individual miner, much more difficult. It also makes explicit the target for DoS attacks.\n>\n> A direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable as well as provides block producers a mechanism for (independently) enforcing their own minimum security budget. In other words, expressing a minimum acceptable feerate for continued operation.\n>\n> Initial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available), or from direct interactions with block producers.\n>\n> ~niftynei\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/d151987f/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-26T16:38:27",
                "message_text_only": "Good morning Antoine,\n\n> However, as we discussed recently, i do believe their is a failure mode here. On one hand, directly connecting to pools is already possible today and pretty effective given the current mining centralization. On the other hand, it's not possible for most pre-signed txs protocols to reliably (securely) use the Bitcoin tx relay network today to propagate time-sensitive transactions. Furthermore, even if these are fixed (eg via package-relay for (today's) Lightning Network) it seems like there is a stark contrast between what \"L2 [0] protocols\" need and what regular node operators can reasonably offer. A node operator is incentivized to relay transactions to:\n> - have more privacy *if* they use their node to broadcast transactions (make it less distinguishable which relayed transaction comes from the wallet)\n> - provide fee estimates *if* they need them\n> - avoid bandwidth spikes on block connection (compact block relay)\n\nTo be clear: it is possible to design L2 protocols such that various counterparties (whose incentives may not align perfectly) can bid to put their views of the L2 state on the blockchain.\nFor instance, in Lightning, you may wish to close a channel at a high feerate in order to use your onchain funds quickly, yet your channel counterparty has no similar time pressure to get their onchain funds in a usable state.\nSolutions such as anchor outputs have been devised to allow each counterparty to pay fees as they see fit, however, for instance for anchor outputs the commitment transaction has to be at the minimum relay feerate.\nAt times of high blockchain congestion, node operators may raise the minimum feerate they are willing to relay (in order to alleviate bandwidth use), which may prevent commitment transactions from being relayed; even if a CPFP were relayed later, since the parent transaction is not propagated in the first place, the CPFP transaction spending the anchor outputs cannot propagate too.\n\nI believe that is what you are referring to here as an example of how an L2 protocol cannot rely on the current L1 network for timely confirmation?\n\n> L2s would ideally need the tx relaying nodes to do more computation and lift their DOS mitigations for all miner-incentive-compatible transactions to eventually be relayed to most of the miners. One obvious instance of such a dilemma is the RBF rules.\n> Such protocols getting increasingly developed and used create a strong incentive for their users/stakeholders to directly connect to mining pools [1], with all the consequences for the network mentioned in the replies to your mail and elsewhere.\n> Before we get to this, i think there is a strong case for an opt-in and publicly accessible \"overlay\" network to relay miner-incentive compatible transactions with higher DOS limits. This way the cost is not imposed to L1 node runners, and L2s can operate with more safety assumptions without (entirely) falling for centralization.\n\n\nLet us imagine how such a network would operate.\n\nIt seems to me that an issue here is that *relay is free*.\n\nAnd: \"you get what you pay for\".\nSince mere relay is free (i.e. nodes do not charge any fee to merely *relay* transactions) the quality of that relay is low.\nThus, L1 node operators may insist on policies that do not support well miner-incentive transaction packages.\n\n\nSo the question is: who should pay for relay?\nUltimately of course the transactor pays, but it seems to me that the direct payer should be miners; transactors would offer higher fees, and miners would then pay part of those fees to relayers to get those transactions.\n\nSo, perhaps, let us consider a sketch (which is probably impossible, but may trigger other ideas):\n\n\nIdeally, a relayer system (whether a single node, or some cooperating/competing overlay network of nodes) would gather a bunch of transactions in a proposed package of high-paying transactions.\nMiners then offer to pay the relayer contingent on learning such a package.\n\nThe miner wants:\n\n* To be given proof, before paying, that the package:\n  * Pays some certain value in fees.\n  * Consist of valid transactions.\n* To atomically get the actual package once they pay.\n\nThe relayer wants:\n\n* To be paid when it reveals such a package of transaction.\n\nTransactors want:\n\n* Fees to the relayer to be included as part of the mining fees in their transaction.\n\n\nThe flow would be something like this:\n\n* A transactor has a group of transactions it wants confirmed.\n* It goes to some relayer and feeds the transactions to them.\n* The relayer figures out the most lucrative package (a task that may be NP-hard?  Knapsack problem equivalent?).\n* Miners look for relayers who have figured out the most lucrative next package.\n* Miners pick the best package and pay for the package.\n* Miners compete on mining.\n\nThe issues are:\n\n* We need some way to prove that a bunch of transactions are valid, without showing the actual transactions.\n  * Maybe part of Utreexo or similar concepts can be used?\n* We need some way to prove that a bunch of transactions pays some fee.\n  * Again, Utreexo?  Maybe?  Probably not?\n  * Fees = inputs - outputs, so the delta between the input UTXO set and the output UTXO set should be the fee, how do we prove that?\n* We need some way to actually get the actual transaction data contingent on some PTLC or HTLC.\n\n\nHmm.\nThoughts?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Pieter Wuille",
                "date": "2021-10-26T16:26:43",
                "message_text_only": "On Monday, October 25th, 2021 at 10:56 PM, lisa neigut via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> In a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.\n\nHi Lisa,\n\nI see where this idea is coming from, especially as it relates to reducing complexities around transaction relays, but I strongly believe this is throwing out the baby with the bathwater. Comments inline below.\n\n> In reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.\n\nThe *entire* reason mining and PoW exist, as opposed to having a fixed, centralized (set of) actors who decide transaction ordering, is to make the \"censorship rights\" of the network permissionless. It is essential that anyone can become a miner if they dislike what existing miners are doing, with income close to proportional to their investment. The existing reality isn't perfect, but it's fairly close to that. Sure, at any given point in time, a nontrivial fraction of mining power is in the hands of a few, but over time, those can, and have, changed a lot. Furthermore, if miners were to actually exercise censorship, it could quite reasonably incentivize other ecosystem players to start mining, perhaps close at cost or even at a small loss.\n\nYour proposal, as far as I can tell, makes it *far* harder to become a miner. Ideas to provide a mechanism for miners to publish their \"tx submit\" URL/IP/onion on chain don't help; that's dependent on other miners to not censor the publishing. Furthermore, it gives a tremendous centralizing incentive: it's just far easier for most wallets to just submit to the largest few pools, because the cost/complexity of an additional submission is independent of the pool's hashrate, but the benefit is directly proportional to it. There would be very little incentive to submit to a sub-1% pool for anyone.\n\n> Removing the mempool would greatly reduce the bandwidth requirement for running a node,\n\nThat's not true due to compact blocks (most transactions are relayed exactly once to every node, and not repeated in blocks), and with Erlay it will be even less the case.\n\n> keep intentionality of transactions private until confirmed/irrevocable,\n\nExcept to miners; it's replacing socialized transparency with a few who get to see the actual details. Not the same scale obviously, but there is some similarity to banks in the existing financial system. Our privacy goals shouldn't be relying on a few trusted gatekeepers.\n\n> and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.\n\nThere are other solutions to this, like weak blocks (miners get to relay partial PoW solutuon of say 10% of the difficulty to the network; and nodes which receive such a weak block can \"forcibly\" insert its transaction to their mempool, as there is evidence it's actually being worked on, while still being DoS resistant because partial PoW is still PoW).\n\n> Provided the number of block template producing actors remains beneath, say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that nodes can independently + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.\n\nIn this scenario, there is no incentive for miners to relay to each other. The fewer other miners know about a high fee-paying transaction, the better you as a miner.\n\nMore conceptually: it is a responsibility of the full node network to relay blocks between miners quickly, to limit how much advantage well-connected miners over less-well-connected ones have. If the network doesn't have the transactions being included in those blocks, this is *far* harder (additional roundtrips, as nodes can't reconstruct from mempools).\n\n> A direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable as well as provides block producers a mechanism for (independently) enforcing their own minimum security budget. In other words, expressing a minimum acceptable feerate for continued operation.\n\nYes, it's definitely easier. That doesn't make it right.\n\nCheers,\n\n--\nPieter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/f0c847d7/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2021-10-26T18:16:51",
                "message_text_only": "Hi Lisa,\n\nSome background for people who are not familiar with mempool:\n\nThe mempool is a cache of unconfirmed transactions, designed in a way\nto help miners efficiently pick the highest feerate packages to\ninclude in new blocks. It stores more than a block's worth of\ntransactions because transaction volume fluctuates and any rational\nminer would try to maximize the fees in their blocks; in a reorg, we\nalso don't want to completely forget what transactions were in the\nnow-stale tip.\n\nIn Bitcoin Core, full nodes keep a mempool by default. The additional\nrequirements for keeping a mempool are minimal (300MB storage, can be\nconfigured to be lower) because anyone, anywhere in the world, should\nbe able to run a node and broadcast a Bitcoin payment without special\nconnectivity to some specific set of people or expensive/inaccessible\nhardware. Perhaps connecting directly to miners can be a solution for\nsome people, but I don't think it's healthy for the network.\n\nSome benefits of keeping a mempool as a non-mining node include:\n- Fee estimation based on your node's knowledge of unconfirmed\ntransactions + historical data.\n- Dramatically increased block validation (and thus propagation)\nspeed, since you cache signature and script results of transactions\nbefore they are confirmed.\n- Reduced block relay bandwidth usage (Bitcoin Core nodes use BIP152\ncompact block relay), as you don't need to re-request the block\ntransactions you already have in your mempool.\n- Wallet ability to send/receive transactions that spend unconfirmed outputs.\n\n> I had the realization that the mempool is obsolete and should be eliminated.\n\nI assume you mean that the mempool should still exist but be turned\noff for non-mining nodes. A block template producer needs to keep\nunconfirmed transactions somewhere.\nOn Bitcoin Core today, you can use the -blocksonly config option to\nignore incoming transactions (effectively switching off your mempool),\nbut there are strong reasons not to do this:\n- It is trivial for your peers to detect that all transactions\nbroadcasted by your node = from your wallet. Linking your node to your\ntransactions is a very bad privacy leak.\n- You must query someone else for what feerate to put on your transaction.\n- You can't use BIP152 compact block relay, so your network bandwidth\nusage spikes at every block. You also can't cache validation results,\nso your block validation speed slows down.\n\n> Removing the mempool would greatly reduce the bandwidth requirement for running a node...\n\nIf you're having problems with your individual node's bandwidth usage,\nyou can also decrease the number of connections you make or turn off\nincoming connections. There are efforts to reduce transaction relay\nbandwidth usage network-wide [1].\n\n> Removing the mempool would... keep intentionality of transactions private until confirmed/irrevocable...\n\nI'm confused - what is the purpose of keeping a transaction private\nuntil it confirms? Don't miners still see your transaction? A\nconfirmed transaction is not irrevocable; reorgs happen.\n\n> Removing the mempool would... naturally resolve all current issues inherent in package relay and rbf rules.\n\nRemoving the mempool does not help with this. How does a miner decide\nwhether a conflicting transaction is an economically-advantageous\nreplacement or a DoS attack? How do you submit your CPFP if the parent\nis below the mempool minimum feerate? Do they already have a different\nrelay/mempool implementation handling all of these problems but don't\naren't sharing it with the rest of the community?\n\n> Initial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available), or from direct interactions with block producers.\n\nThere are many reasons why using only published blocks for fee\nestimates is a flawed design, including:\n\n- The miner of a block can artificially inflate the feerate of the\ntransactions in their mempool simply by including a few of their own\ntransactions that pay extremely high feerates. This costs them\nnothing, as they collect the fees.\n- A miner constructs a block based on the transactions in their\nmempool. Your transaction's feerate may have been enough to be\nincluded 2 blocks ago or a week ago, but it will be compared to the\nother unconfirmed transactions available to the miner now. They can\ntell you what's in their mempool or what the next-block feerate is,\nbut you would be a fool to believe them.\n\nSee also [2],[3].\n\n> Provided the number of block template producing actors remains beneath, say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that nodes can independently  + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.\n\nAs a thought experiment, let's imagine we have some public registry of\nmining nodes' tor endpoints and we use it for this secondary\ndirect-to-miner transaction relay network. If the registry is\nmaintained (by who?) and accurate (based on whose word?), it is a\npoint of failure for transaction censorship and deanonymization, as\nwell as an additional barrier to becoming a miner, encouraging\ncentralization.\nThe other possibility is that the registry is not accurate. In fact,\nunless the registry requires miners to identify themselves (which\nothers on this thread have already pointed out is ill-advised), this\nshould be treated similarly to regular addr gossip. We would never\nautomatically trust that the entity behind the endpoint provides the\nservice it advertises, is an honest node that won't simply blackhole\nour transaction, or even belongs to a Bitcoin node at all.\n\nBest,\nGloria\n\n[1]: https://arxiv.org/pdf/1905.10518.pdf\n[2]: https://bitcointechtalk.com/an-introduction-to-bitcoin-core-fee-estimation-27920880ad0\n[3]: https://gist.github.com/morcos/d3637f015bc4e607e1fd10d8351e9f41\n\n\nOn Tue, Oct 26, 2021 at 8:38 AM lisa neigut via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> In a recent conversation with @glozow, I had the realization that the\n> mempool is obsolete and should be eliminated.\n>\n> Instead, users should submit their transactions directly to mining pools,\n> preferably over an anonymous communication network such as tor. This can\n> easily be achieved by mining pools running a tor onion node for this\n> express purpose (or via a lightning network extension etc)\n>\n> Mempools make sense in a world where mining is done by a large number of\n> participating nodes, eg where the block template is constructed by a\n> majority of the participants on the network. In this case, it is necessary\n> to socialize pending transaction data to all participants, as you don\u2019t\n> know which participant will be constructing the winning block template.\n>\n> In reality however, mempool relay is unnecessary where the majority of\n> hashpower and thus block template creation is concentrated in a\n> semi-restricted set.\n>\n> Removing the mempool would greatly reduce the bandwidth requirement for\n> running a node, keep intentionality of transactions private until\n> confirmed/irrevocable, and naturally resolve all current issues inherent in\n> package relay and rbf rules. It also resolves the recent minimum relay\n> questions, as relay is no longer a concern for unmined transactions.\n>\n> Provided the number of block template producing actors remains beneath,\n> say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that\n> nodes can independently  + directly submit their transactions to. In fact,\n> merely allowing users to select their own list of endpoints to use\n> alternatively to the mempool would be a low effort starting point for the\n> eventual replacement.\n>\n> On the other hand, removing the mempool would greatly complicate solo\n> mining and would also make BetterHash proposals, which move the block\n> template construction away from a centralized mining pool back to the\n> individual miner, much more difficult. It also makes explicit the target\n> for DoS attacks.\n>\n> A direct communication channel between block template construction venues\n> and transaction proposers also provides a venue for direct feedback wrt\n> acceptable feerates at the time, which both makes transaction confirmation\n> timelines less variable as well as provides block producers a mechanism for\n> (independently) enforcing their own minimum security budget. In other\n> words, expressing a minimum acceptable feerate for continued operation.\n>\n> Initial feerate estimation would need to be based on published blocks, not\n> pending transactions (as this information would no longer be available), or\n> from direct interactions with block producers.\n>\n>\n> ~niftynei\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/9477b0b5/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2021-10-28T01:04:10",
                "message_text_only": "Good morning Gloria, et al,\n\n\n> > Removing the mempool would... naturally resolve all current issues inherent in package relay and rbf rules.\n>\n> Removing the mempool does not help with this. How does a miner decide whether a conflicting transaction is an economically-advantageous replacement or a DoS attack? How do you submit your CPFP if the parent is below the mempool minimum feerate? Do they already have a different relay/mempool implementation handling all of these problems but don't aren't sharing it with the rest of the community?\n\nThis seems an important key point: *even if* miners maintain some kind of \"accept any transaction!!!\" endpoint, the miner still wants to have *some* policy on how to evict transactions from its pool of transactions, for the simple reason that *everyone* has limited resources, even miners.\n\nPerhaps the issue is that eviction is done *immediately*, i.e. if a node receives a transaction below some feerate treshhold, it immediately drops the transaction.\n\nWhat if instead we did the eviction lazily?\n\nSuppose we used something like a garbage collector.\nUnconfirmed transactions are objects that point to other objects (i.e. the input of a transaction \"points to\" another object).\n\"Primitive\" objects are UTXOs of *confirmed* transactions, i.e. the UTXO set at the block tip.\nThen, a GC algorithm would start at some roots and then terminate when it reaches primitive objects.\n\nI describe here an algorithm based on semispace GC, but the GC algorithm space is well-studied and other algorithms may also be devised (in particular, spam is likely to match quite well with \"infant mortality\" concept in GC, i.e. \"most objects die young\", so some kind of nursery / generational GC may work better against spam in practice).\n\nA semispace GC has two \"spaces\" for memory.\nOne is the \"from-space\" and the other is the \"to-space\".\nDuring normal operation, the \"from-space\" is used and the \"to-space\" is empty.\n(Note that we can implement a \"space\" as a table (`std::map`) from txid to transaction, and avoid having to *actually* copy the transaction data; the important thing is having two spaces)\nThere is a maximum size that from-space and to-space can be.\n\nAs we receive transactions, we allocate them on the from-space.\nOnce the from-space is filled, we stop operations and perform a GC cycle.\n\nWe select \"roots\" by ordering all transactions in the from-space, from highest-feerate to lowest-feerate (figure out some way to handle ties later, maybe put a timestamp or monotonic counter?).\nStarting with the highest-feerate tx, we trace all the transactions they refer to, recursively, copying them from from-space to to-space.\nWe stop once the to-space is filled more than half.\n\nAt this point, we drop all transactions in the from-space that are not already in to-space, and then delete the from-space.\nThen we promote the to-space as the from-space, and continue our merry way, allocating more transactions.\n\n(Nothing prevents doing this on disk rather than in memory; xref Eric Voskuil)\n\nNote that the algorithm operates on individual transactions, not packages of transactions.\nThe algorithm is vulnerable to spam where the spammer creates several large low-feerate transactions, then anchors them all using a tiny high-feerate transaction (a \"tall\" attack).\nIt is also vulnerable to spam where the spammer creates several high-feerate transactions spending the same UTXO (a \"wide\" attack), thus preventing anyone else from getting any transactions into the mempool.\n\nAgainst these exploit, we can mitigate by *first* moving objects to a smaller \"packagespace\" instead of directly on the to-space.\nWhen tracing a new root, we first move the transactions that are not already in to-space to the packagespace, then measure the aggregate fee divided by the aggregate memory usage.\nIf this is below, say, half the feerate of the root transaction, then we drop the packagespace (put it back into from-space) and move on to the next root.\nThis protects against \"tall\" attacks.\nTo protect against \"wide\" attacks, if the packagespace consumes a TXO that is already consumed in the to-space, we also drop the packagespace (i.e. only retain the highest-feerate version in a \"wide\" attack).\nOnce the above checks pass, we merge the packagespace into the to-space.\n\nThis algorithm means that we do not need package relay; instead, we just send transactions in the correct order (parents first, then children), and if the receiver does not need to do a GC in-between, then everything ends up in the mempool.\nIf the child transaction is high-fee enough to be a root transaction, and pays enough that its feerate dominates in the packagespace result, then the entire sequence will remain in the mempool.\n\nThe algorithm allows for conflicting transactions to be retained in the mempool temporarily, until the next GC triggers (at which point conflicting transactions are resolved by whoever is higher-feerate).\nThis is helpful since a conflicting transaction may be what ends up getting confirmed in a block from a miner whose mempool did not contain the \"best\" feerate transaction.\n\n\nWDYT?\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Antoine Riard",
                "date": "2021-10-26T23:44:45",
                "message_text_only": "Hi Lisa,\n\nNetwork mempools constitute a blockspace marketplace where block demand\nmeets the offer in real-time. Block producers are acting to discover the\nbest feerate bids compensating for their operational costs and transaction\nproposers are acting to offer the best feerate in function of their\nconfirmation preferences.\n\nOf course in a distributed system like bitcoin, we can't guarantee perfect\ninformation from the market participants. But moving away from this model\nby decreasing the ability of the non-mining nodes to observe the current\ndemand is softening the requirements for potential attackers.\n\nAs transaction proposers are competing with each other to publish, they\nhave an interest to \"front-run\" each other by querying the pending\ntransactions to the block producers instead of observing only the published\nblocks. Therefore good connections to\nthe block producers are now critical and censorship-resistance of the\nmining endpoints must be guaranteed.\n\nSuch a list of endpoints couldn't be static otherwise it's an artificial\nbarrier to enter in the mining competition, and as such a centralization\nvector. Dynamic, trust-minimized discovery of the mining endpoints assumes\nan address-relay network, of which the robustness must be high enough\nagainst sophisticated sybil attacks. One current defense mechanism in core\nto achieve that is selecting outbound peers based in different /16 subnets\nas it's harder for an attacker to obtain IP addresses. Replicating this\nmechanism for the mining endpoints binds the mining topology to the\nInternet one, which is downgrading the mining competition.\n\nRelying on tor to guarantee the confidentiality of the transaction\nannouncement is raising its own issues. Flowing by default all the bitcoin\ntraffic over tor will change the incentive structure of tor attackers,\npotentially attracting a new class of attackers able to do deanonymization\nattacks, not that expensive in practice [0]. Tor bridges are another\ncensorship vector as the fingerprint of the bitcoin traffic (a block every\n10 min, etc) make it possible to drop or delay the tor channel, in the lack\nof high-bandwidth consuming \"synthetic\" traffic.\n\nFurther, identified mining endpoints make it easier to launch partition\nattacks, where mining mempools are sent low-feerate clusters of\ntransactions, to prevent the replacement by a better feerate offer. This is\nespecially concerning for L2 nodes with time-sensitive requirements [1]\n\nLastly, removing the mempool won't solve the current issues inherent with\npre-signed transactions under the mempool min fee as ultimately miner's\nmempools are also finite in memory and a dynamic lower bound must exist to\nprevent spam. These lower bounds potentially increase after the signature\nexchange of the time-sensitive transactions.\n\nAntoine\n\n[0] https://www.usenix.org/system/files/sec19-jansen.pdf\n[1] See \"The Ugly\"\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2020-June/002758.html\n\nLe mar. 26 oct. 2021 \u00e0 03:37, lisa neigut via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi all,\n>\n> In a recent conversation with @glozow, I had the realization that the\n> mempool is obsolete and should be eliminated.\n>\n> Instead, users should submit their transactions directly to mining pools,\n> preferably over an anonymous communication network such as tor. This can\n> easily be achieved by mining pools running a tor onion node for this\n> express purpose (or via a lightning network extension etc)\n>\n> Mempools make sense in a world where mining is done by a large number of\n> participating nodes, eg where the block template is constructed by a\n> majority of the participants on the network. In this case, it is necessary\n> to socialize pending transaction data to all participants, as you don\u2019t\n> know which participant will be constructing the winning block template.\n>\n> In reality however, mempool relay is unnecessary where the majority of\n> hashpower and thus block template creation is concentrated in a\n> semi-restricted set.\n>\n> Removing the mempool would greatly reduce the bandwidth requirement for\n> running a node, keep intentionality of transactions private until\n> confirmed/irrevocable, and naturally resolve all current issues inherent in\n> package relay and rbf rules. It also resolves the recent minimum relay\n> questions, as relay is no longer a concern for unmined transactions.\n>\n> Provided the number of block template producing actors remains beneath,\n> say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that\n> nodes can independently  + directly submit their transactions to. In fact,\n> merely allowing users to select their own list of endpoints to use\n> alternatively to the mempool would be a low effort starting point for the\n> eventual replacement.\n>\n> On the other hand, removing the mempool would greatly complicate solo\n> mining and would also make BetterHash proposals, which move the block\n> template construction away from a centralized mining pool back to the\n> individual miner, much more difficult. It also makes explicit the target\n> for DoS attacks.\n>\n> A direct communication channel between block template construction venues\n> and transaction proposers also provides a venue for direct feedback wrt\n> acceptable feerates at the time, which both makes transaction confirmation\n> timelines less variable as well as provides block producers a mechanism for\n> (independently) enforcing their own minimum security budget. In other\n> words, expressing a minimum acceptable feerate for continued operation.\n>\n> Initial feerate estimation would need to be based on published blocks, not\n> pending transactions (as this information would no longer be available), or\n> from direct interactions with block producers.\n>\n>\n> ~niftynei\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211026/bd8c767a/attachment-0001.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2021-10-27T20:01:51",
                "message_text_only": "On Tue, Oct 26, 2021 at 07:44:45PM -0400, Antoine Riard via bitcoin-dev wrote:\n> Such a list of endpoints couldn't be static otherwise it's an artificial\n> barrier to enter in the mining competition, and as such a centralization\n> vector. Dynamic, trust-minimized discovery of the mining endpoints assumes\n> an address-relay network, of which the robustness must be high enough\n> against sophisticated sybil attacks. One current defense mechanism in core\n> to achieve that is selecting outbound peers based in different /16 subnets\n> as it's harder for an attacker to obtain IP addresses. Replicating this\n> mechanism for the mining endpoints binds the mining topology to the\n> Internet one, which is downgrading the mining competition.\n\nI think a really simple way to put it is if we didn't have the mempool, it'd be\ngood to create a free service that got transactions to miners in an equal\nopportunity, decentralized, way. A simple flood fill scheme would be a great\nway to do that... at which point you've re-invented the mempool.\n\nNothing wrong with people running nodes that opt-out of transaction\nbroadcasting, and it may even make sense for such nodes to preferentially peer\nwith each other. But there's always going to be a need for a scheme like the\nexisting mempool, so might as well just keep it.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211027/237114b0/attachment.sig>"
            },
            {
                "author": "LORD HIS EXCELLENCY JAMES HRMH",
                "date": "2021-10-27T08:44:42",
                "message_text_only": "Good Afternoon,\n\nNo. This has been discussed previously and eliminated as there is no proof that the transaction can exist without population through the mempool. As a method of payment not hearing about a transaction until it is possibly mined three months later as I have experienced is non-functional, there were discussions in this mailing list. The purpose of the mempool is not gossip it is gossip and any node technically can mine if they do.\n\nKING JAMES HRMH\nGreat British Empire\n\nRegards,\nThe Australian\nLORD HIS EXCELLENCY JAMES HRMH (& HMRH)\nof Hougun Manor & Glencoe & British Empire\nMR. Damian A. James Williamson\nWills\n\net al.\n\n\nWilltech\nwww.willtech.com.au\nwww.go-overt.com\nduigco.org DUIGCO API\nand other projects\n\n\nm. 0487135719\nf. +61261470192\n\n\nThis email does not constitute a general advice. Please disregard this email if misdelivered.\n________________________________\nFrom: bitcoin-dev <bitcoin-dev-bounces at lists.linuxfoundation.org> on behalf of lisa neigut via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nSent: Tuesday, 26 October 2021 1:56 PM\nTo: bitcoin-dev at lists.linuxfoundation.org <bitcoin-dev at lists.linuxfoundation.org>\nSubject: [bitcoin-dev] death to the mempool, long live the mempool\n\nHi all,\n\nIn a recent conversation with @glozow, I had the realization that the mempool is obsolete and should be eliminated.\n\nInstead, users should submit their transactions directly to mining pools, preferably over an anonymous communication network such as tor. This can easily be achieved by mining pools running a tor onion node for this express purpose (or via a lightning network extension etc)\n\nMempools make sense in a world where mining is done by a large number of participating nodes, eg where the block template is constructed by a majority of the participants on the network. In this case, it is necessary to socialize pending transaction data to all participants, as you don\u2019t know which participant will be constructing the winning block template.\n\nIn reality however, mempool relay is unnecessary where the majority of hashpower and thus block template creation is concentrated in a semi-restricted set.\n\nRemoving the mempool would greatly reduce the bandwidth requirement for running a node, keep intentionality of transactions private until confirmed/irrevocable, and naturally resolve all current issues inherent in package relay and rbf rules. It also resolves the recent minimum relay questions, as relay is no longer a concern for unmined transactions.\n\nProvided the number of block template producing actors remains beneath, say 1000, it\u2019d be quite feasible to publish a list of tor endpoints that nodes can independently  + directly submit their transactions to. In fact, merely allowing users to select their own list of endpoints to use alternatively to the mempool would be a low effort starting point for the eventual replacement.\n\nOn the other hand, removing the mempool would greatly complicate solo mining and would also make BetterHash proposals, which move the block template construction away from a centralized mining pool back to the individual miner, much more difficult. It also makes explicit the target for DoS attacks.\n\nA direct communication channel between block template construction venues and transaction proposers also provides a venue for direct feedback wrt acceptable feerates at the time, which both makes transaction confirmation timelines less variable as well as provides block producers a mechanism for (independently) enforcing their own minimum security budget. In other words, expressing a minimum acceptable feerate for continued operation.\n\nInitial feerate estimation would need to be based on published blocks, not pending transactions (as this information would no longer be available), or from direct interactions with block producers.\n\n\n~niftynei\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211027/e9782e39/attachment.html>"
            },
            {
                "author": "yanmaani at cock.li",
                "date": "2021-10-27T23:05:59",
                "message_text_only": "[I removed a comment regarding the moderation of this list here because \nit caused for my message to be rejected]\n\nOn 2021-10-26 02:56, lisa neigut via bitcoin-dev wrote:\n> [...] the mempool is obsolete and should be eliminated.\n> \n> Instead, users should submit their transactions directly to mining\n> pools, [...]\n> Mempools make sense in a world where mining is done by a large number\n> of participating nodes, [...] as you don\u2019t know which participant will\n> be constructing the winning block template.\n> \n> In reality however, mempool relay is unnecessary where the majority of\n> hashpower and thus block template creation is concentrated in a\n> semi-restricted set.\n\nIt's true that there is some centralization, but this is hardly a \ndesirable goal that should be formally enshrined.\n\nBy that point, you might as well block people from keeping their coins \nin their own wallet, on the basis that in practice mostly everyone keeps \nthem on the exchange.\n\nAnd as the others have pointed out: even if you did hold this to be \ndesirable, why would removing the mempool be a good idea? The pools \nwould still need some way to get transactions, and a mempool seems like \nan excellent way to do this.\n\nI think most of the people here have laid out all of the other obvious \nissues with the proposal.\n\n> Removing the mempool would greatly reduce the bandwidth requirement\n> for running a node\n\nYou can disable it already if you're strapped for cash. Is there a \nreason why this is not adequate?\n\n> keep intentionality of transactions private until confirmed/irrevocable\n\nWhat is the \"intentionality\" of a transaction and why do I want to keep \nit private? My transactions are 100% intentional because I am trying to \nsend money, and I wouldn't make them otherwise - what is a \nnon-intentional transaction supposed to be?\n\n> Provided the number of block template producing actors remains\n> beneath, say 1000, it\u2019d be quite feasible to publish a list of tor\n> endpoints that nodes can independently  + directly submit their\n> transactions to.\n\nIf nothing else, this would be a significant departure from the security \nmodel of Bitcoin:\n\n> The network is robust in its unstructured simplicity.\n> Nodes work all at once with little coordination.\n> They do not need to be identified, since messages are not routed to any \n> particular place and only need to be delivered on a best effort basis.\n> Nodes can leave and rejoin the network at will, accepting the \n> proof-of-work chain as proof of what happened while they were gone.\n\nIf you posit that the security model should be changed, that is one \nthing, but you should lay out your reasoning for this claim.\n\n> On the other hand, removing the mempool would greatly complicate solo\n> mining and would also make BetterHash proposals, which move the block\n> template construction away from a centralized mining pool back to the\n> individual miner, much more difficult.\n\nI am amazed that you are intelligent enough to realize these trade-offs, \nyet still made this post. Are you suggesting that you find them to be \nacceptable?\n\n> It also makes explicit the target for DoS attacks.\n\nPerhaps the only good aspect of this proposal. Under such conditions, \ndenial of service attacks would be both just and desirable.\n\n> A direct communication channel between block template construction\n> venues and transaction proposers also provides a venue for direct\n> feedback wrt acceptable feerates at the time, which both makes\n> transaction confirmation timelines less variable as well as provides\n> block producers a mechanism for (independently) enforcing their own\n> minimum security budget. In other words, expressing a minimum\n> acceptable feerate for continued operation.\n\nWhy couldn't they just run a website about this for anyone who cares? \nCommunicating two numbers can easily be done over HTTP. This technology \nexists already.\n\n> ~niftynei\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "death to the mempool, long live the mempool",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard",
                "LORD HIS EXCELLENCY JAMES HRMH",
                "lisa neigut",
                "darosior",
                "eric at voskuil.org",
                "Peter Todd",
                "ZmnSCPxj",
                "yanmaani at cock.li",
                "Gloria Zhao",
                "Pieter Wuille"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 63143
        }
    },
    {
        "title": "[bitcoin-dev] TAPLEAF_UPDATE_VERIFY covenant opcode",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2021-10-29T15:47:12",
                "message_text_only": "I very much like this idea. It seems pretty flexible and has a lot of\ninteresting properties and use cases without being very complex. I'll have\nto read through this more deeply later. I'm curious to understand more how\nit compares to OP_CTV. It seems that implementing OP_TLUV wouldn't make\nOP_CTV obsolete/uncessary, is that right?\n\n> And second, it doesn't provide a way for utxos to \"interact\",\n\nThis is talking about sending data to the output from an input or getting\ndata from a parent input, other than any added output tapscripts, right? I\nthink this can/should be done with a separate opcode, so I'm not sure I\nwould really call this a limitation here. I wrote a proposal for something\nthat does allow interaction like that (specifically sending data to an\noutput: OP_PUSHOUTPUTSTACK\n<https://github.com/fresheneesz/bip-efficient-bitcoin-vaults/blob/main/bip-pushoutputstack.md>\n).\n\nOn Wed, Sep 22, 2021 at 7:29 PM Olaoluwa Osuntokun via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi AJ,\n>\n> Happy to see that this proposal has finally seen the light of day! I've\n> been\n> hearing about it in hinted background convos over the past few months, so\n> happy I can finally dig into the specifics of its operation.\n>\n> > So the idea is to do just that via a new opcode \"TAPLEAF_UPDATE_VERIFY\"\n> > (TLUV) that takes three inputs: one that specifies how to update the\n> > internal public key (X), one that specifies a new step for the merkle\n> path\n> > (F), and one that specifies whether to remove the current script and/or\n> > how many merkle path steps to remove\n>\n> What if instead, it obtained the script from the _annex_? I think this\n> small\n> modification would make the op code even _more_ powerful. Consider that\n> this\n> allows a new script to be passed _dynamically_ after the output has been\n> created, possibly by a threshold of parties that control the output, or\n> them\n> all (mu sig, etc, etc). This serves to create a generic \"upgrade\" mechanism\n> for any tapscript output (covenant or not). Functionally, this is similar\n> to\n> the existence of \"admin keys\" or voted DAO upgrades that exists in chains\n> that utilize an account based systems. This is really useful as it allows a\n> script any given output to optional add in graftroot like behavior (leaf in\n> tree that accepts script updates), and also allows contract developers to\n> progressively upgrade or fix issues in prior versions of their deployed\n> contracts.\n>\n> This little trick is secure since unlike the witness itself, the annex is\n> actually _signed_ within the sighash like everything else. Incorporating\n> this proposal would require the addition of an OP_PUSH_ANNEX op code, which\n> by itself seems expertly useful. If one views the annex as a sort of\n> authenticated associated data that can be passed into the script execution\n> context, then this actually serves to absorb _some_ uses cases of a\n> hypothetical OP_CHECKSIG_FROM_STACK opcode. A push annex op code also makes\n> doing things like output delegation to a given key passed into the witness\n> secure since the prior \"owner\" of the output commits to the key within the\n> sighash.\n>\n> Even assuming a more powerful type of covenant that allows partial\n> application of binding logic, something like this is still super useful\n> since the action of re-creating a new tapscript tree based in dynamic input\n> data would generate a rather large witness if only something like OP_CAT\n> was\n> available. The unique \"update\" nature of this appears to augment any other\n> type of covenant, which is pretty cool. Consider that it would allow you\n> (with the annex addition above), take something like a CTV congestion tree,\n> and add in _new_ users at the tree is already being unrolled (just a toy\n> example).\n>\n> It would also allow an individual to _join_ the payment pool construct\n> described earlier which makes it 1000x more useful (vs just supporting\n> unrolling). I haven't written it all down yet, but I think this along with\n> something like CTV or CSFS makes it possible to implement a Plasma Cash [4]\n> like Commit Chain [5], which is super exciting (assume a counter is\n> embedded\n> in the main script that tracks the next free leaf slot(s). With this model\n> an \"operator\" is able to include a single transaction in the chain that\n> stamps a batch of updates in the payment tree.  Users then get a\n> contestation period where they can refute a modification to the tree in\n> order to withdraw their funds.\n>\n> > And second, it doesn't provide a way for utxos to \"interact\",\n>\n> This is due to the fact that the op code doesn't allow any sort of late\n> binding or pattern matching then constraining _where_ (or whence?) the\n> coins\n> can Be sent to. There's a group of developers that are attempting to make\n> an\n> AMM-like system on Liquid [1] using more generic stack based covenants [2]\n> (see the `OP_INSPECTINPUT` op code, which seems very much inspired by\n> jl2012's old proposal). However one challenge that still need to be tackled\n> in the UTXO model is allowing multiple participants to easily interact w/\n> the\n> contract in a single block w/o a coordination layer to synchronize the\n> access.\n>\n> One solution to this concurrency issue, that I believe is already employed\n> by Chia is to allow \"contracts\" to be identified via a fixed ID (as long as\n> their active in the chain) [3]. This lets transactions spend/interact with\n> a\n> contract, without always needing to know the set of active UTXOs where that\n> contract lives. Transactions then specify their contract and \"regular\"\n> inputs, with the requirement that every transaction spends at least a\n> single\n> regular input.\n>\n> The trade-off here is that nodes need to maintain this extra index into the\n> UTXO set. However, this can be alleviated by applying a utreexo like\n> solution: nodes maintain some merklized data structure over the index and\n> require that spending transactions provide an _inclusion_ proof of the\n> active contract. Nodes then only need to maintain root hashes of the UTXO\n> and contract set.\n>\n> I'm super happy w.r.t how the covenant space has been processing over the\n> past few years. IMO its the single most important (along with the utreexo\n> type stateless stuff mentioned above) missing component to allow the\n> creation of more decentralized self-custodial applications built on top of\n> Bitcoin.\n>\n> -- Laolu\n>\n> [1]: https://medium.com/bit-matrix\n> [2]:\n> https://github.com/sanket1729/elements/blob/84339ba5e5dc65328d98afe2b1b33dcb69ba4311/doc/tapscript_opcodes.md\n> [3]:\n> https://forum.celestia.org/t/accounts-strict-access-lists-and-utxos/37\n> [4]: https://www.learnplasma.org/en/learn/cash.html\n> [5]: https://eprint.iacr.org/2018/642\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20211029/e1788cdc/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "TAPLEAF_UPDATE_VERIFY covenant opcode",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Billy Tetrud"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 7096
        }
    }
]