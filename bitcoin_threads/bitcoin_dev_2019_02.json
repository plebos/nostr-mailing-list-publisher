[
    {
        "title": "[bitcoin-dev] Proof-of-Stake Bitcoin Sidechains",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-02-01T09:19:00",
                "message_text_only": "Good morning Matt Bell,\n\nThinking of this further, I observe that there are limits on the number of operations in a SCRIPT (I believe 201 non-push operations, and maybe a smaller number of CHECKSIG operations?).\nThis implies that the number of signatories of the sidechain funds in the mainchain are limited.\nThis is an important care point.\nI am uncertain what is the best way to solve this issue.\n\n\n---\n\nIn any case, I now present a design for a proof-of-mainstake sidechain, now without any modifications to Bitcoin mainchain.\n\n---\n\nI observe that a blockchain is, stripped to its barest minimum, nothing more than a Merklized singly-linked list.\nEach block header is a node in a singly-linked list.\nIt commits to the previous block header, and also commits to the block data (traditionally a Merkle binary tree).\n\nVia such block headers, a chain of blocks --- a blockchain --- is formed.\n\n---\n\nI observe that a (non-coinbase) transaction in Bitcoin refers to at least one existing transaction output.\nA representation of that transaction output must be committed to in the transaction.\n\nIf we create single-input single-output transactions, a chain of transactions is formed.\n\n---\n\nThus the idea: the sidechain *is* the transaction chain.\n\nI observe that the mainchain *must* contain some UTXO(s) that are purportedly controlled by the sidechain rules.\n\nIt is also possible that the sidechain funds be a single UTXO, with deposits and withdrawals requiring that the single UTXO be spent, in order to maintain the invariant that the sidechains funds are handled completely in a single UTXO.\nIn addition, it is possible for a transaction to commit to some data arbitrarily, either via `OP_RETURN`, or via some technique such as pay-to-contract (which reduces space utilization on the mainchain compared to `OP_RETURN`).\n\nWhen we use the above technique (i.e. the sidechain only \"owns\" a single mainchain UTXO at each block of the mainchain):\n\n1.  Each transaction commits to the previous transaction (via spending the output of the previous transaction).\n2.  Each transaction may commit to some other data (via `OP_RETURN` or other technique).\n\nI observe also that under a blockchain:\n\n1.  Each block header commits to the previous block header.\n2.  Each block header commits to the block data.\n\n>From this, the analogy is simple and obvious.\nThe sidechain \"blockchain\" *is* the transaction chain.\n\n---\n\nUnder certain forms of proof-of-stake, the block must be signed by some set of signatories of the stakers.\nUnder transaction rules, the transaction must be signed according to the SCRIPT, and the SCRIPT may indicate that some set of signatories must sign.\n\nThus, it becomes possible to simply embed the sidechain block headers on the mainchain directly, by spending the previous transaction (== sidechain block header).\nThis spend requires that the transaction be signed in order to authorize the spend.\nHowever, these same signatures are in fact also the signatures that, under proof-of-stake, prove that a sufficient signatory set of the stakers has authorized a particular block of the proof-of-stake blockchain.\n\nThe magic here is that, on the mainchain, a transaction may only be spent once.\nThus, nothing-at-stake and stake-grinding problems disappear.\n\n---\n\nNow let us introduce some details.\n\nWe have two mainchain-to-sidechain requests:\n\n1.  An indication that a mainchain coin owner wants to stake coins to the sidechain.\n2.  An indication that a mainchain coin owner wants to transfer coins to the sidechain.\n\n>From within the sidechain, sidechain-to-mainchain withdrawals must somehow be signalled, but that is up to the sidechain to define.\nWe shall ignore it here.\n\nWhen a sidechain receives a request to add stake, then the current stakers create a mainchain transaction, spending the sidechain UTXO, plus the  staked coins, and outputting the next sidechain UTXO (with the signatory set modified appropriately), plus a stake UTXO that locks the coins.\nWhen a sidechain receives a request to transfer coins from mainchain to sidechain, then the current stakers create a mainchain transaction, spending the sidechain UTXO, plus the transferred coins, and outputting the next sidechain UTXO (with the same signatory set).\n\nMultiple such requests can be processed for each transaction (i.e. sidechain block).\nThis simply consumes the sidechain UTXO, any stake or transfer coins, and creates the next sidechain UTXO and any stake UTXOs.\n\nNow, the indication to stake is a UTXO with a special script.\nIt has two branches:\n\n1.  A signature from the current signatory set.\n2.  Or, 2 OP_CSV and the staker signature.\n\nThe intent of the latter branch is to ensure that, if the current signatories ignore the incoming staker, the incoming staker can still recover its funds.\n\nIf the current set of stakers accepts the incoming staker (which requires both that they change the signatory set, and put the money being staked into a stake UTXO which is simply a long CSV and the staker signature), then the first branch is performed and the coin is an input of the sidechain block header (== sidechain managing transaction).\n\nA similar technique is used for mainchain-to-sidechain transfers.\nIf the mainchain-to-sidechain transfer is ignored (either deliberately, or by an accident of disrupted communication from the mainchain trasnferrer to the sidechain network), the mainchain transferrer can recover its money and try again.\n\n---\n\nIdeally, every mainchain block would have a sidechain managing transaction (== sidechain block header).\nOf course, we must consider fees.\nObviously the sidechain itself must charge fees within the sidechain.\nSome fraction of those fees will be spent in order for the sidechain managing transaction to be confirmed on the mainchain.\n\nNow it may happen that the sidechain managing transaction is not confirmed immediately on the mainchain.\nThe sidechain stakers (the signatory set) may elect to sacrifice even more of their fees to increase the fees of the sidechain managing transaction via RBF.\n\n---\n\nNow perhaps the sidechain may wish to have a faster (or more regular) block rate than the mainchain.\nIn such a case, it may maintain a \"real\" blockchain (i.e. headers that commit to a single previous header, and commits the block data).\nThen each sidechain managing transaction would commit to the latest sidechain block header agreed upon by the stakers.\n\nThese sidechain blocks need not be signed; they only become \"real\" if they are committed to, directly or indirectly, in a sidechain managing transaction.\n\nAt each sidechain block, the signatory set (the stakers) create a sidechain managing transaction.\nIf it is immediately put into a mainchain block, then the next sidechain managing transaction spends it.\nOtherwise, if it is not put into a mainchain block, then the stakers just recreate the sidechain managing transaction with RBF.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Proof-of-Stake Bitcoin Sidechains",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6905
        }
    },
    {
        "title": "[bitcoin-dev] Safer NOINPUT with output tagging",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2019-02-01T09:36:50",
                "message_text_only": "Good morning aj,\n\nI certainly agree.\nI hope that PSBT support becomes much, much, much more widespread.\n\nRegards,\nZmnSCPxj\n\n\nSent with ProtonMail Secure Email.\n\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010 Original Message \u2010\u2010\u2010\u2010\u2010\u2010\u2010\nOn Thursday, January 31, 2019 2:04 PM, Anthony Towns <aj at erisian.com.au> wrote:\n\n> On Mon, Dec 24, 2018 at 11:47:38AM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > A boutique protocol would reduce the number of existing onchain wallets that could be integrated in such UI.\n>\n> Seems like PSBT would be a sufficient protocol:\n>\n> 0) lightning node generates a PSBT for a new channel,\n> with no inputs and a single output of the 2-of-2 address\n>\n> 1.  wallet funds the PSBT but doesn't sign it, adding a change address\n>     if necessary, and could combine with other tx's bustapay style\n>\n> 2.  lightning determines txid from PSBT, and creates update/settlement\n>     tx's for funding tx so funds can be recovered\n>\n> 3.  wallet signs and publishes the PSBT\n> 4.  lightning sees tx on chain and channel is open\n>\n>     That's a bit more convoluted than \"(0) lightning generates an address and\n>     value, and creates NOINPUT update/settlement tx's for that address/value;\n>     (1) wallet funds address to exactly that value; (2) lightning monitors\n>     blockchain for payment to that address\" of course.\n>\n>     But it avoids letting users get into the habit of passing NOINPUT\n>     addresses around, or the risk of a user typo'ing the value and losing\n>     money immediately, and it has the benefit that the wallet can tweak the\n>     value if (eg) that avoids a change address or enhances privacy (iirc,\n>     c-lightning tweaks payment values for that reason). If the channel's\n>     closed cooperatively, it also avoids ever needing to publish a NOINPUT\n>     sig (or NOINPUT tagged output).\n>\n>     Does that seem a fair trade off?\n>\n>     Cheers,\n>     aj\n>"
            },
            {
                "author": "Jonas Nick",
                "date": "2019-02-08T19:01:40",
                "message_text_only": "Output tagging may result in reduced fungibility in multiparty eltoo channels.\nIf one party is unresponsive, the remaining participants want to remove\nthe party from the channel without downtime. This is possible by creating\nsettlement transactions which pay off the unresponsive party and fund a new\nchannel with the remaining participants.\n\nWhen the party becomes unresponsive, the channel is closed by broadcasting the\nupdate transaction as usual. As soon as that happens the remaining\nparticipants can start to update their new channel. Their update signatures\nmust use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\nfinal (because update tx is not confirmed and may have to rebind to another\noutput). Therefore, the funding output of the new channel must be NOINPUT\ntagged. Assuming the remaining parties later settle cooperatively, this loss\nof fungibility would not have happened without output tagging.\n\nfunding output          update output                                    settlement outputs              update output\n[ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n                                                                           C' ]\nIf the expectation is that the unresponsive party returns, fungibility is\nnot reduced due to output tagging because the above scheme can be used\noff-chain until the original channel can be continued.\n\nSide note: I was not able to come up with an similar, eltoo-like protocol that works\nif you can't predict in advance who will become absent.\n\nOn 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this norm any time soon, if possible at all.\n> \n> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n> \n> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.\n> \n> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n> \n> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n> \n> 1. A certain bit in the tx version must be set\n> 2. A certain bit in the scriptPubKey must be set\n> \n> I will analyse the pros and cons later.\n> \n> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.\n> \n> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to other people.\n> \n> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n> \n> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.\n> \n> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.\n> \n> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n> \n> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n> \n> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Alejandro Ranchal Pedrosa",
                "date": "2019-02-09T10:01:20",
                "message_text_only": "Hi all,\n>\n> Side note: I was not able to come up with an similar, eltoo-like \n> protocol that works\n> if you can't predict in advance who will become absent.\n>\nAn eltoo-like protocol that works (without going on-chain) if you can't \npredict in advance who will become absent would be a childchain. If the \noff-chain protocol can continue updating in the abscence of other \nparties, it means that other parties' signatures must not be required \nwhen they are not involved in the off-chain state update. If other \nparties' signatures must not be required, there must be a way of having \na common verifiable 'last state' to prevent a party to simultaneously \n'fork' the state with two different parties, and double-spend. A \nsolution for this is a childchain for Bitcoin. An example of this is \nwhat is known as a 'Broken Factory' attack [1] \n(https://bitcoin.stackexchange.com/questions/77434/how-does-channel-factory-act/81005#81005)\n\n> If the expectation is that the unresponsive party returns, fungibility is\n> not reduced due to output tagging because the above scheme can be used\n> off-chain until the original channel can be continued.\n\nI believe that in many cases other parties won't be able to continue \nuntil the unresponsive parties go back online. That might be true in \nparticular scenarios, but generally speaking, the party might have gone \nunresponsive during a factory-level update (i.e. off-chain closing and \nopening of channels), while some parties might have given out their \nsignature for the update without receiving a fully signed transaction. \nIn this case they do not even know which channel they have open (the one \nof the old state that they have fully signed, or the one for the new \nstate that they have given out their signature for). This is known as a \n'Stale Factory', and can be exploited by an adversary in a 'Stale \nFactory' attack [1]. Even if they knew which state they are in (i.e. the \nparty went unresponsive but not during a factory-level update), some of \nthem might have run out of funds in some of their channels of the \nfactory, and might want to update, while they will not be willing to \nwait for a party to go back online (something for which they also have \nzero guarantees of).\n\nAn eltoo-like protocol that works (allowing going on-chain) if you can't \nin advance who will become absent, then this is precisely why \n'Transaction Fragments' have been suggested. They allow an eltoo-like \nprotocol even when one cannot predict in advance who will become absent, \nor malicious (by publishing invalid states), cause the non-absent \nparties can unite their fragments and create a valid spendable \nfactory-level transaction that effectively kicks out the malicious \nparties, while leaving the rest of the factory as it was. To the best of \nmy understanding, the eltoo original proposal also allows this though.\n\nBest,\n\nAlejandro.\n\n[1]: Scalable Lightning Factories for Bitcoin, \nhttps://eprint.iacr.org/2018/918.pdf\n\n\nOn 08/02/2019 20:01, Jonas Nick via bitcoin-dev wrote:\n> Output tagging may result in reduced fungibility in multiparty eltoo channels.\n> If one party is unresponsive, the remaining participants want to remove\n> the party from the channel without downtime. This is possible by creating\n> settlement transactions which pay off the unresponsive party and fund a new\n> channel with the remaining participants.\n>\n> When the party becomes unresponsive, the channel is closed by broadcasting the\n> update transaction as usual. As soon as that happens the remaining\n> participants can start to update their new channel. Their update signatures\n> must use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\n> final (because update tx is not confirmed and may have to rebind to another\n> output). Therefore, the funding output of the new channel must be NOINPUT\n> tagged. Assuming the remaining parties later settle cooperatively, this loss\n> of fungibility would not have happened without output tagging.\n>\n> funding output          update output                                    settlement outputs              update output\n> [ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n>                                                                             C' ]\n> If the expectation is that the unresponsive party returns, fungibility is\n> not reduced due to output tagging because the above scheme can be used\n> off-chain until the original channel can be continued.\n>\n> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n> if you can't predict in advance who will become absent.\n>\n> On 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n>> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this norm any time soon, if possible at all.\n>>\n>> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n>>\n>> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.\n>>\n>> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n>>\n>> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n>>\n>> 1. A certain bit in the tx version must be set\n>> 2. A certain bit in the scriptPubKey must be set\n>>\n>> I will analyse the pros and cons later.\n>>\n>> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.\n>>\n>> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to other people.\n>>\n>> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n>>\n>> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.\n>>\n>> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.\n>>\n>> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n>>\n>> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n>>\n>> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Johnson Lau",
                "date": "2019-02-09T16:48:40",
                "message_text_only": "In a 3 parties channel, let\u2019s say the balance for A, B, C is 2, 3, 6BTC respectively, there are few ways they could make the settlement tx.\n\nThe first type we may call it \u201csimple settlement\u201d, which has 3 outputs with A=2, B=3, C=6.\n\nThe second type we may call it \u201cfully combinatorial settlement\u201d, which has 3 outputs with (A & B), (B & C), and (A & C). The value distribution is flexible, but never exceed the total balance of the involved parties. For example, (A & B) may have any value between 0 and 5BTC. For the following example, I will use (A & B) = 3; (B & C) = 6; (A & C) = 2, but there are infinitely many valid combinations.\n\nThe third type we may call it \u201cpartially combinatorial settlement\u201d. It may have 2 multi-sig outputs, for example, (A & B) = 4 and (B & C) = 7; or 1 multi-sig output and 1 single-sig output, for example, (A & B) = 5 and C=6 (known as \"semi-cooperative channel closing\u201d SCCC in my last post)\n\nI\u2019ll just focus on the fully combinatorial settlement. The partial type works in the same way, with benefits and limitations.\n\nIn a combinatorial settlement, the multi-sig outputs are actually eltoo-style scripts. Therefore, A and B will further distribute the value of (A & B) by a 2-party eltoo channel (\u201cbranch channels\"). Again, there are infinitely many valid ways to distribute the values. If the AB branch channel is distributed as A=1 and B=2, then the BC channel must be B=1 and C=5, and the AC channel must be A=1 and C=1.\n\nA clear benefit of this model is that any 2 parties could trade with each other, in the absence of any other party(s), as long as there is enough liquidity in their branch channel. There is also no way to \u201cfork\u201d the state, because liquidity is restricted to each branch channel. In some way, this is like the existing lightning network where the 3 parties have direct channel with each other. However, this is superior to lightning network, because when the 3 parties are online simultaneously, they could re-distribute the channel capacities without closing any channels. They could even change it to a partially combinatorial settlement. If they find that A and C rarely trade with each other, they could remove the (A & C) output, and improve the capacities of the remaining channels. If C is going offline for a week, they could make it (A & B), C, (aka. SCCC) which will maximise the capacity of the AB branch channel, and minimise the cost in case C is not coming back.\n\nA problem with combinatorial settlement is the increased costs of uncooperative settlement. It is more expensive, as more parties are missing. Simple settlement has the same settlement cost for any number of missing party. However, even if one party is missing, a simple settled channel will cease to function and force an immediate on-chain settlement. In combinatorial settlement, the surviving parties may keep trading, may or may not with reduced capacity depending on the exact settlement model, and in the meantime hope that the missing parties may return.\n\nIt requires 6 outputs for 4 parties doing fully combinatorial settlement, 10 outputs for 5 parties, 15 outputs for 6 parties, etc. However, in a many parties situation, not every parties might want to trade with all the other parties, and those branch channels might be omitted to improve the capacities of the other channels. If some pairs want to trade without a direct branch channel, they might try to find a third (internal) party to forward the tx. When the next time all parties are online, they could rearrange the branch channel capacities at no cost.\n\nThe combinatorial settlement model could be generalised to a hierarchical settlement model, where we might have 4 settlement outputs (A&B&C), (A&B&D), (A&C&D), (B&C&D) for a 4-party channel, and each settlement output will have 3 branch channels. If A is missing, for example, we will still have one BC branch channel, one BD branch channel, one CD branch channel, and one BCD 3-party branch channel. The benefit of having a BCD 3-party branch channel is the 3 parties could rearrange the channel capacities without involving A. Let\u2019s say D is going for vacation, he could do a SCCC in the BCD branch channel to maximise the capacity of its BC channel. Without the involvement of A, however, the capacities of the other BC, BD, and CD branch channels are not modifiable, and B and C\u2019s balance in the BD/CD channels are frozen during the absence of D.\n\nAs the number of parties increase, the number of settlement txs will grow factorially in a fully hierarchical settlement model, and will soon be out-of-control. The result could be catastrophic if many parties are gone. So the group needs to continuously evaluate the risks of each party being missing, and modify the settlement model accordingly.\n\n\n\n> On 9 Feb 2019, at 6:01 PM, Alejandro Ranchal Pedrosa via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Hi all,\n>> \n>> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n>> if you can't predict in advance who will become absent.\n>> \n> An eltoo-like protocol that works (without going on-chain) if you can't predict in advance who will become absent would be a childchain. If the off-chain protocol can continue updating in the abscence of other parties, it means that other parties' signatures must not be required when they are not involved in the off-chain state update. If other parties' signatures must not be required, there must be a way of having a common verifiable 'last state' to prevent a party to simultaneously 'fork' the state with two different parties, and double-spend. A solution for this is a childchain for Bitcoin. An example of this is what is known as a 'Broken Factory' attack [1] (https://bitcoin.stackexchange.com/questions/77434/how-does-channel-factory-act/81005#81005)\n> \n>> If the expectation is that the unresponsive party returns, fungibility is\n>> not reduced due to output tagging because the above scheme can be used\n>> off-chain until the original channel can be continued.\n> \n> I believe that in many cases other parties won't be able to continue until the unresponsive parties go back online. That might be true in particular scenarios, but generally speaking, the party might have gone unresponsive during a factory-level update (i.e. off-chain closing and opening of channels), while some parties might have given out their signature for the update without receiving a fully signed transaction. In this case they do not even know which channel they have open (the one of the old state that they have fully signed, or the one for the new state that they have given out their signature for). This is known as a 'Stale Factory', and can be exploited by an adversary in a 'Stale Factory' attack [1]. Even if they knew which state they are in (i.e. the party went unresponsive but not during a factory-level update), some of them might have run out of funds in some of their channels of the factory, and might want to update, while they will not be willing to wait for a party to go back online (something for which they also have zero guarantees of).\n> \n> An eltoo-like protocol that works (allowing going on-chain) if you can't in advance who will become absent, then this is precisely why 'Transaction Fragments' have been suggested. They allow an eltoo-like protocol even when one cannot predict in advance who will become absent, or malicious (by publishing invalid states), cause the non-absent parties can unite their fragments and create a valid spendable factory-level transaction that effectively kicks out the malicious parties, while leaving the rest of the factory as it was. To the best of my understanding, the eltoo original proposal also allows this though.\n> \n> Best,\n> \n> Alejandro.\n> \n> [1]: Scalable Lightning Factories for Bitcoin, https://eprint.iacr.org/2018/918.pdf\n> \n> \n> On 08/02/2019 20:01, Jonas Nick via bitcoin-dev wrote:\n>> Output tagging may result in reduced fungibility in multiparty eltoo channels.\n>> If one party is unresponsive, the remaining participants want to remove\n>> the party from the channel without downtime. This is possible by creating\n>> settlement transactions which pay off the unresponsive party and fund a new\n>> channel with the remaining participants.\n>> \n>> When the party becomes unresponsive, the channel is closed by broadcasting the\n>> update transaction as usual. As soon as that happens the remaining\n>> participants can start to update their new channel. Their update signatures\n>> must use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\n>> final (because update tx is not confirmed and may have to rebind to another\n>> output). Therefore, the funding output of the new channel must be NOINPUT\n>> tagged. Assuming the remaining parties later settle cooperatively, this loss\n>> of fungibility would not have happened without output tagging.\n>> \n>> funding output          update output                                    settlement outputs              update output\n>> [ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n>>                                                                            C' ]\n>> If the expectation is that the unresponsive party returns, fungibility is\n>> not reduced due to output tagging because the above scheme can be used\n>> off-chain until the original channel can be continued.\n>> \n>> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n>> if you can't predict in advance who will become absent.\n>> \n>> On 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n>>> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this norm any time soon, if possible at all.\n>>> \n>>> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n>>> \n>>> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.\n>>> \n>>> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n>>> \n>>> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n>>> \n>>> 1. A certain bit in the tx version must be set\n>>> 2. A certain bit in the scriptPubKey must be set\n>>> \n>>> I will analyse the pros and cons later.\n>>> \n>>> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.\n>>> \n>>> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to other people.\n>>> \n>>> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n>>> \n>>> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.\n>>> \n>>> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.\n>>> \n>>> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n>>> \n>>> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n>>> \n>>> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>> \n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Anthony Towns",
                "date": "2019-02-10T04:46:30",
                "message_text_only": "On Sun, Feb 10, 2019 at 12:48:40AM +0800, Johnson Lau wrote:\n> In a 3 parties channel, let\u2019s say the balance for A, B, C is 2, 3, 6BTC respectively, there are few ways they could make the settlement tx.\n\nThe way I look at this is:\n\n * you can have a \"channel factory\" of 3 or more members (A,B,C,...)\n * it's protected by an n-of-n multisig output\n * it contains some combination of:\n    - spends directly to members\n    - lightning channels between pairs of members\n    - channel factories between subgroups of members\n * when initially setup, the factory just has direct spends to each\n   member matching the amount they contributed to the factory\n * whether you create a lightning channel or a sub-factory is the same\n   decision as whether you create a lightning channel or a factory\n   on-chain, so there's no combinatorial explosion.\n\nYou can close any channel factory by publishing it (and any higher level\nchannel factories it was a subgroup of) to the blockchain (at which point\nthe lower level channel factories and lightning channels remain open),\nor you can update a channel factory off-chain by having everyone agree\nto a new state -- which is only possible if everyone is online, of course.\n\nUpdates to transactions in a lightning channel in a factory, or updates\nto a subfactory, don't generally involve updating the containing factory\nat all, I think.\n\nI don't think there's much use to having sub-factories -- maybe if you\nhave a subgroup that's much more active and wants to change channel\nbalances between each other more frequently than the least active member\nof the main factory is online?\n\nAs far as NOINPUT goes; this impacts channel factories because cheating\ncould be by any member of the group, so you can't easily penalise the\ncheater. So an eltoo-esque setup where you publish a commitment to the\nstate that's spendable only by any later state, and is then redeemed\nafter a timelock seems workable. In that case closing a factory when\nyou can't get all group members to cooperatively close looks like:\n\n   funding tx: n-of-n multisig\n\n   state commitment: n-of-n multisig\n      spends funding tx or earlier state commitment\n      spendable by later state commitment or settlement\n\n   settlement: n-of-n multisig\n      relative timelock\n      spends state commitment\n      spends to members, channels or sub-factories\n\nThe settlement tx has to spend with a NOINPUT sig, because the state\ncommitment could have had to spend different things. If it's a\nsub-factory, the funding tx will have been in a factory, so the state\ncommitment would also have had to be a NOINPUT spend. So tagging\nNOINPUT-spendable outputs would mean:\n\n - tagging state commitment outputs (which will be spent shortly with\n   NOINPUT by the settlement tx, so no real loss here)\n\n - tagging settlement tx outputs if they're lightning channels or\n   sub-factories (which is something of a privacy loss, I think, since\n   they could continue off-chain for an indefinite period before being\n   spent)\n\n\n\nI think Johnson's suggested elsewhere that if you spend an input with a\nNOINPUT signature, you should make all the outputs be tagged NOINPUT (as\na \"best practice rule\", rather than consensus-enforced or standardness).\nThat would avoid the privacy loss here, I think, but might be confusing.\n\nIf you wanted to close your factory and send your funds to an external\nthird-party (a cold-wallet, custodial wallet, or just paying someone\nfor something), you'd presumably do that via a cooperative close of the\nfactory, which doesn't require the state/settlement pair or NOINPUT\nspends, so the NOINPUT-in means NOINPUT-tagged-outputs doesn't cause\na problem for that use case.\n\n\n\nFWIW, I think an interesting way to improve this model might be to *add*\ncentralisation and trust; so that instead of having the factory have\nan n-of-n multisig, have it be protected by k-of-n plus a trusted third\nparty. If you have the trusted third party check that the only balances\nthat change in the factory are from the \"k\" signers, that allows (n-k)\nmembers to be offline at any time, but the remaining members to rebalance\ntheir channels happily. (Theoretically you could do this trustlessly\nwith covenants, but the spending proofs on chain would be much larger)\n\nOf course, this allows k-signers plus the trusted party to steal funds.\nIt might be possible for the trusted party to store audit logs of the\npartial signatures from each of the k-signers for each transaction to\nprovide accountability -- where the lack of such logs implies the\ntrusted third party was cheating.\n\nCheers,\naj\n\n> \n> \n> \n> > On 9 Feb 2019, at 6:01 PM, Alejandro Ranchal Pedrosa via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > \n> > Hi all,\n> >> \n> >> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n> >> if you can't predict in advance who will become absent.\n> >> \n> > An eltoo-like protocol that works (without going on-chain) if you can't predict in advance who will become absent would be a childchain. If the off-chain protocol can continue updating in the abscence of other parties, it means that other parties' signatures must not be required when they are not involved in the off-chain state update. If other parties' signatures must not be required, there must be a way of having a common verifiable 'last state' to prevent a party to simultaneously 'fork' the state with two different parties, and double-spend. A solution for this is a childchain for Bitcoin. An example of this is what is known as a 'Broken Factory' attack [1] (https://bitcoin.stackexchange.com/questions/77434/how-does-channel-factory-act/81005#81005)\n> > \n> >> If the expectation is that the unresponsive party returns, fungibility is\n> >> not reduced due to output tagging because the above scheme can be used\n> >> off-chain until the original channel can be continued.\n> > \n> > I believe that in many cases other parties won't be able to continue until the unresponsive parties go back online. That might be true in particular scenarios, but generally speaking, the party might have gone unresponsive during a factory-level update (i.e. off-chain closing and opening of channels), while some parties might have given out their signature for the update without receiving a fully signed transaction. In this case they do not even know which channel they have open (the one of the old state that they have fully signed, or the one for the new state that they have given out their signature for). This is known as a 'Stale Factory', and can be exploited by an adversary in a 'Stale Factory' attack [1]. Even if they knew which state they are in (i.e. the party went unresponsive but not during a factory-level update), some of them might have run out of funds in some of their channels of the factory, and might want to update, while they will not be willing to wait for a party to go back online (something for which they also have zero guarantees of).\n> > \n> > An eltoo-like protocol that works (allowing going on-chain) if you can't in advance who will become absent, then this is precisely why 'Transaction Fragments' have been suggested. They allow an eltoo-like protocol even when one cannot predict in advance who will become absent, or malicious (by publishing invalid states), cause the non-absent parties can unite their fragments and create a valid spendable factory-level transaction that effectively kicks out the malicious parties, while leaving the rest of the factory as it was. To the best of my understanding, the eltoo original proposal also allows this though.\n> > \n> > Best,\n> > \n> > Alejandro.\n> > \n> > [1]: Scalable Lightning Factories for Bitcoin, https://eprint.iacr.org/2018/918.pdf\n> > \n> > \n> > On 08/02/2019 20:01, Jonas Nick via bitcoin-dev wrote:\n> >> Output tagging may result in reduced fungibility in multiparty eltoo channels.\n> >> If one party is unresponsive, the remaining participants want to remove\n> >> the party from the channel without downtime. This is possible by creating\n> >> settlement transactions which pay off the unresponsive party and fund a new\n> >> channel with the remaining participants.\n> >> \n> >> When the party becomes unresponsive, the channel is closed by broadcasting the\n> >> update transaction as usual. As soon as that happens the remaining\n> >> participants can start to update their new channel. Their update signatures\n> >> must use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\n> >> final (because update tx is not confirmed and may have to rebind to another\n> >> output). Therefore, the funding output of the new channel must be NOINPUT\n> >> tagged. Assuming the remaining parties later settle cooperatively, this loss\n> >> of fungibility would not have happened without output tagging.\n> >> \n> >> funding output          update output                                    settlement outputs              update output\n> >> [ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n> >>                                                                            C' ]\n> >> If the expectation is that the unresponsive party returns, fungibility is\n> >> not reduced due to output tagging because the above scheme can be used\n> >> off-chain until the original channel can be continued.\n> >> \n> >> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n> >> if you can't predict in advance who will become absent.\n> >> \n> >> On 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n> >>> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this norm any time soon, if possible at all.\n> >>> \n> >>> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n> >>> \n> >>> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.\n> >>> \n> >>> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n> >>> \n> >>> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n> >>> \n> >>> 1. A certain bit in the tx version must be set\n> >>> 2. A certain bit in the scriptPubKey must be set\n> >>> \n> >>> I will analyse the pros and cons later.\n> >>> \n> >>> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.\n> >>> \n> >>> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to other people.\n> >>> \n> >>> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n> >>> \n> >>> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.\n> >>> \n> >>> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.\n> >>> \n> >>> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n> >>> \n> >>> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n> >>> \n> >>> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n> >>> _______________________________________________\n> >>> bitcoin-dev mailing list\n> >>> bitcoin-dev at lists.linuxfoundation.org\n> >>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>> \n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> \n>"
            },
            {
                "author": "Jonas Nick",
                "date": "2019-02-09T16:54:09",
                "message_text_only": "<--- not replying to list as this is off-topic ---->\n\nHey Alejandro,\n\nthanks for the pointer. Is there a summary of how the opcode you're proposing would look like?\nIs pairing crypto strictly necessary or would interactive key aggregation schemes like Bellare-Neven\nwork as well?\n\nBest,\nJonas\n\nOn 2/9/19 10:01 AM, Alejandro Ranchal Pedrosa via bitcoin-dev wrote:\n> Hi all,\n>>\n>> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n>> if you can't predict in advance who will become absent.\n>>\n> An eltoo-like protocol that works (without going on-chain) if you can't predict in advance who will become absent would be a childchain. If the off-chain protocol can continue updating in the abscence\n> of other parties, it means that other parties' signatures must not be required when they are not involved in the off-chain state update. If other parties' signatures must not be required, there must\n> be a way of having a common verifiable 'last state' to prevent a party to simultaneously 'fork' the state with two different parties, and double-spend. A solution for this is a childchain for Bitcoin.\n> An example of this is what is known as a 'Broken Factory' attack [1] (https://bitcoin.stackexchange.com/questions/77434/how-does-channel-factory-act/81005#81005)\n> \n>> If the expectation is that the unresponsive party returns, fungibility is\n>> not reduced due to output tagging because the above scheme can be used\n>> off-chain until the original channel can be continued.\n> \n> I believe that in many cases other parties won't be able to continue until the unresponsive parties go back online. That might be true in particular scenarios, but generally speaking, the party might\n> have gone unresponsive during a factory-level update (i.e. off-chain closing and opening of channels), while some parties might have given out their signature for the update without receiving a fully\n> signed transaction. In this case they do not even know which channel they have open (the one of the old state that they have fully signed, or the one for the new state that they have given out their\n> signature for). This is known as a 'Stale Factory', and can be exploited by an adversary in a 'Stale Factory' attack [1]. Even if they knew which state they are in (i.e. the party went unresponsive\n> but not during a factory-level update), some of them might have run out of funds in some of their channels of the factory, and might want to update, while they will not be willing to wait for a party\n> to go back online (something for which they also have zero guarantees of).\n> \n> An eltoo-like protocol that works (allowing going on-chain) if you can't in advance who will become absent, then this is precisely why 'Transaction Fragments' have been suggested. They allow an\n> eltoo-like protocol even when one cannot predict in advance who will become absent, or malicious (by publishing invalid states), cause the non-absent parties can unite their fragments and create a\n> valid spendable factory-level transaction that effectively kicks out the malicious parties, while leaving the rest of the factory as it was. To the best of my understanding, the eltoo original\n> proposal also allows this though.\n> \n> Best,\n> \n> Alejandro.\n> \n> [1]: Scalable Lightning Factories for Bitcoin, https://eprint.iacr.org/2018/918.pdf\n> \n> \n> On 08/02/2019 20:01, Jonas Nick via bitcoin-dev wrote:\n>> Output tagging may result in reduced fungibility in multiparty eltoo channels.\n>> If one party is unresponsive, the remaining participants want to remove\n>> the party from the channel without downtime. This is possible by creating\n>> settlement transactions which pay off the unresponsive party and fund a new\n>> channel with the remaining participants.\n>>\n>> When the party becomes unresponsive, the channel is closed by broadcasting the\n>> update transaction as usual. As soon as that happens the remaining\n>> participants can start to update their new channel. Their update signatures\n>> must use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\n>> final (because update tx is not confirmed and may have to rebind to another\n>> output). Therefore, the funding output of the new channel must be NOINPUT\n>> tagged. Assuming the remaining parties later settle cooperatively, this loss\n>> of fungibility would not have happened without output tagging.\n>>\n>> funding output\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 update output\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 settlement outputs\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 update output\n>> [ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n>> \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 C' ]\n>> If the expectation is that the unresponsive party returns, fungibility is\n>> not reduced due to output tagging because the above scheme can be used\n>> off-chain until the original channel can be continued.\n>>\n>> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n>> if you can't predict in advance who will become absent.\n>>\n>> On 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n>>> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address.\n>>> Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this\n>>> norm any time soon, if possible at all.\n>>>\n>>> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO,\n>>> Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and\n>>> \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n>>>\n>>> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want,\n>>> while minimising the risks of misuse.\n>>>\n>>> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not\n>>> uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers\n>>> (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n>>>\n>>> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n>>>\n>>> 1. A certain bit in the tx version must be set\n>>> 2. A certain bit in the scriptPubKey must be set\n>>>\n>>> I will analyse the pros and cons later.\n>>>\n>>> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo,\n>>> should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT.\n>>> As the final destination, there is no need to tag in the settlement tx.\n>>>\n>>> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to\n>>> other people.\n>>>\n>>> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n>>>\n>>> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow\n>>> NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more\n>>> expensive.\n>>>\n>>> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for\n>>> them to refuse sending to tagged addresses by default.\n>>>\n>>> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or\n>>> none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n>>>\n>>> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always\n>>> uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n>>>\n>>> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Johnson Lau",
                "date": "2019-02-09T10:15:17",
                "message_text_only": "This is really interesting. If I get it correctly, I think the fungibility hit could be avoided, just by making one more signature, and not affecting the blockchain space usage.\n\nJust some terminology first. In a 3-party channel, \u201cmain channel\u201d means the one requires all parties to update, and \u201cbranch channel\u201d requires only 2 parties to update.\n\nBy what you describe, I think the most realistic scenario is \u201cC is going to offline soon, and may or may not return. So the group wants to keep the main channel open, and create a branch channel for A and B, during the absence of C\u201d. I guess this is what you mean by being able to \"predict in advance who will become absent\u201d\n\nI call this process as \u201csemi-cooperative channel closing\u201d (SCCC). During a SCCC, the settlement tx will have 2 outputs: one as (A & B), one as (C). Therefore, a branch channel could be opened with the (A & B) output. The channel opening must use NOINPUT signature, since we don\u2019t know the txid of the settlement tx. With the output tagging requirement, (A & B) must be tagged, and lead to the fungibility loss you described.\n\nHowever, it is possible to make 2 settlement txs during SCCC. Outputs of the settlement tx X are tagged(A&B) and C. Outputs of the settlement tx Y are untagged(A&B) and C. Both X and Y are BIP68 relative-time-locked, but Y has a longer time lock.\n\nThe branch channel is opened on top of the tagged output of tx X. If A and B want to close the channel without C, they need to publish the last update tx of the main channel. Once the update tx is confirmed, its txid becomes permanent, so are the txids of X and Y. If A and B decide to close the channel cooperatively, they could do it on top of the untagged output of tx Y, without using NOINPUT. There won\u2019t be any fungibility loss. Other people will only see the uncooperative closing of the main channel, and couldn\u2019t even tell the number of parties in the main channel. Unfortunately, the unusual long lock time of Y might still tell something.\n\nIf anything goes wrong, A or B could publish X before the lock time of Y, and settle it through the usual eltoo style. Since this is an uncooperative closing anyway, the extra fungibility loss due to tagging is next to nothing. However, it may suggest that the main channel was a multi-party one.\n\nFor C, the last update tx of the main channel and the settlement tx Y are the only things he needs to get the money back. C has to sign tx X, but he shouldn\u2019t get the complete tx X. Otherwise, C might have an incentive to publish X in order to get the money back earlier, at the cost of fungibility loss of the branch channel.\n\nTo minimise the fungibility loss, we\u2019d better make it a social norm: if you sign your tx with NOINPUT, always try to make all outputs tagged to be NOINPUT-spendable. (NOTE: you can still spend tagged outputs with normal signatures, so this won\u2019t permanently taint your coins as NOINPUT-spendable) It makes sense because the use of NOINPUT signature strongly suggests that you don\u2019t know the txid of the parent tx, so you may most likely want your outputs to be NOINPUT-spendable as well. I thought of making this a policy or consensus rule, but may be it\u2019s just overkill.\n\n\n\n> On 9 Feb 2019, at 3:01 AM, Jonas Nick <jonasdnick at gmail.com> wrote:\n> \n> Output tagging may result in reduced fungibility in multiparty eltoo channels.\n> If one party is unresponsive, the remaining participants want to remove\n> the party from the channel without downtime. This is possible by creating\n> settlement transactions which pay off the unresponsive party and fund a new\n> channel with the remaining participants.\n> \n> When the party becomes unresponsive, the channel is closed by broadcasting the\n> update transaction as usual. As soon as that happens the remaining\n> participants can start to update their new channel. Their update signatures\n> must use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\n> final (because update tx is not confirmed and may have to rebind to another\n> output). Therefore, the funding output of the new channel must be NOINPUT\n> tagged. Assuming the remaining parties later settle cooperatively, this loss\n> of fungibility would not have happened without output tagging.\n> \n> funding output          update output                                    settlement outputs              update output\n> [ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n>                                                                           C' ]\n> If the expectation is that the unresponsive party returns, fungibility is\n> not reduced due to output tagging because the above scheme can be used\n> off-chain until the original channel can be continued.\n> \n> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n> if you can't predict in advance who will become absent.\n> \n> On 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n>> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this norm any time soon, if possible at all.\n>> \n>> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n>> \n>> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.\n>> \n>> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n>> \n>> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n>> \n>> 1. A certain bit in the tx version must be set\n>> 2. A certain bit in the scriptPubKey must be set\n>> \n>> I will analyse the pros and cons later.\n>> \n>> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.\n>> \n>> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to other people.\n>> \n>> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n>> \n>> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.\n>> \n>> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.\n>> \n>> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n>> \n>> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n>> \n>> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>"
            },
            {
                "author": "Jonas Nick",
                "date": "2019-02-09T16:52:07",
                "message_text_only": "Johnson's modification solves the issue I pointed out.\n\nMoreover, as Johnson and I discussed in private, using different locktimes for\nX and Y is not necessary. They can have the same relative locktime. If A and B\nwould only sign Y as soon as the update tx is confirmed, there is no risk of Y\nchanging its txid and therefore invalidating updates built on it.\n\n\nOn 2/9/19 10:15 AM, Johnson Lau wrote:\n> This is really interesting. If I get it correctly, I think the fungibility hit could be avoided, just by making one more signature, and not affecting the blockchain space usage.\n> \n> Just some terminology first. In a 3-party channel, \u201cmain channel\u201d means the one requires all parties to update, and \u201cbranch channel\u201d requires only 2 parties to update.\n> \n> By what you describe, I think the most realistic scenario is \u201cC is going to offline soon, and may or may not return. So the group wants to keep the main channel open, and create a branch channel for A and B, during the absence of C\u201d. I guess this is what you mean by being able to \"predict in advance who will become absent\u201d\n> \n> I call this process as \u201csemi-cooperative channel closing\u201d (SCCC). During a SCCC, the settlement tx will have 2 outputs: one as (A & B), one as (C). Therefore, a branch channel could be opened with the (A & B) output. The channel opening must use NOINPUT signature, since we don\u2019t know the txid of the settlement tx. With the output tagging requirement, (A & B) must be tagged, and lead to the fungibility loss you described.\n> \n> However, it is possible to make 2 settlement txs during SCCC. Outputs of the settlement tx X are tagged(A&B) and C. Outputs of the settlement tx Y are untagged(A&B) and C. Both X and Y are BIP68 relative-time-locked, but Y has a longer time lock.\n> \n> The branch channel is opened on top of the tagged output of tx X. If A and B want to close the channel without C, they need to publish the last update tx of the main channel. Once the update tx is confirmed, its txid becomes permanent, so are the txids of X and Y. If A and B decide to close the channel cooperatively, they could do it on top of the untagged output of tx Y, without using NOINPUT. There won\u2019t be any fungibility loss. Other people will only see the uncooperative closing of the main channel, and couldn\u2019t even tell the number of parties in the main channel. Unfortunately, the unusual long lock time of Y might still tell something.\n> \n> If anything goes wrong, A or B could publish X before the lock time of Y, and settle it through the usual eltoo style. Since this is an uncooperative closing anyway, the extra fungibility loss due to tagging is next to nothing. However, it may suggest that the main channel was a multi-party one.\n> \n> For C, the last update tx of the main channel and the settlement tx Y are the only things he needs to get the money back. C has to sign tx X, but he shouldn\u2019t get the complete tx X. Otherwise, C might have an incentive to publish X in order to get the money back earlier, at the cost of fungibility loss of the branch channel.\n> \n> To minimise the fungibility loss, we\u2019d better make it a social norm: if you sign your tx with NOINPUT, always try to make all outputs tagged to be NOINPUT-spendable. (NOTE: you can still spend tagged outputs with normal signatures, so this won\u2019t permanently taint your coins as NOINPUT-spendable) It makes sense because the use of NOINPUT signature strongly suggests that you don\u2019t know the txid of the parent tx, so you may most likely want your outputs to be NOINPUT-spendable as well. I thought of making this a policy or consensus rule, but may be it\u2019s just overkill.\n> \n> \n> \n>> On 9 Feb 2019, at 3:01 AM, Jonas Nick <jonasdnick at gmail.com> wrote:\n>>\n>> Output tagging may result in reduced fungibility in multiparty eltoo channels.\n>> If one party is unresponsive, the remaining participants want to remove\n>> the party from the channel without downtime. This is possible by creating\n>> settlement transactions which pay off the unresponsive party and fund a new\n>> channel with the remaining participants.\n>>\n>> When the party becomes unresponsive, the channel is closed by broadcasting the\n>> update transaction as usual. As soon as that happens the remaining\n>> participants can start to update their new channel. Their update signatures\n>> must use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\n>> final (because update tx is not confirmed and may have to rebind to another\n>> output). Therefore, the funding output of the new channel must be NOINPUT\n>> tagged. Assuming the remaining parties later settle cooperatively, this loss\n>> of fungibility would not have happened without output tagging.\n>>\n>> funding output          update output                                    settlement outputs              update output\n>> [ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n>>                                                                           C' ]\n>> If the expectation is that the unresponsive party returns, fungibility is\n>> not reduced due to output tagging because the above scheme can be used\n>> off-chain until the original channel can be continued.\n>>\n>> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n>> if you can't predict in advance who will become absent.\n>>\n>> On 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n>>> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this norm any time soon, if possible at all.\n>>>\n>>> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n>>>\n>>> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.\n>>>\n>>> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n>>>\n>>> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n>>>\n>>> 1. A certain bit in the tx version must be set\n>>> 2. A certain bit in the scriptPubKey must be set\n>>>\n>>> I will analyse the pros and cons later.\n>>>\n>>> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.\n>>>\n>>> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to other people.\n>>>\n>>> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n>>>\n>>> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.\n>>>\n>>> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.\n>>>\n>>> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n>>>\n>>> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n>>>\n>>> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n> \n>"
            },
            {
                "author": "Johnson Lau",
                "date": "2019-02-09T17:43:50",
                "message_text_only": "And this scheme could be generalised to the combinatorial settlement model in my earlier post.\n\nLet\u2019s say the settlement tx has 3 outputs: (A&B),(B&C),(A&C). There will be 4 versions of this tx:\n\ntx-X: all 3 outputs are tagged, signed by all 3 parties\ntx-Y-AB: output (A&B) is untagged, the other 2 outputs are tagged. Signed only by C\ntx-Y-AC: output (A&C) is untagged, the other 2 outputs are tagged. Signed only by B\ntx-Y-BC: \u2026\u2026\u2026\n\nAll 4 txs will have the same relative-lock-time\n\nIf C is missing at the time of settlement, A and B will settle upon tx-Y-AB with a simple signature\n\nIf B and C are missing, A will settle upon tx-X\n\nHowever, I think this is just an overkill, and hardly improves fungibility. It is very clear that this is an uncooperative eltoo closing (due to the update tx of the main channel), and this is a multi-party channel (due to multiple settlement outputs). There is little doubt that the remaining parties would like to continue trading. So there is actually no secret to hide, and it might be easier to just tag all outputs\n\nNonetheless, this example shows that the fungibility impact of output tagging is quite manageable. Most likely you just need to prepare more versions of intermediate txs, and only use the tagged one when things go against you.\n\n> On 10 Feb 2019, at 12:52 AM, Jonas Nick <jonasdnick at gmail.com> wrote:\n> \n> Johnson's modification solves the issue I pointed out.\n> \n> Moreover, as Johnson and I discussed in private, using different locktimes for\n> X and Y is not necessary. They can have the same relative locktime. If A and B\n> would only sign Y as soon as the update tx is confirmed, there is no risk of Y\n> changing its txid and therefore invalidating updates built on it.\n> \n> \n> On 2/9/19 10:15 AM, Johnson Lau wrote:\n>> This is really interesting. If I get it correctly, I think the fungibility hit could be avoided, just by making one more signature, and not affecting the blockchain space usage.\n>> \n>> Just some terminology first. In a 3-party channel, \u201cmain channel\u201d means the one requires all parties to update, and \u201cbranch channel\u201d requires only 2 parties to update.\n>> \n>> By what you describe, I think the most realistic scenario is \u201cC is going to offline soon, and may or may not return. So the group wants to keep the main channel open, and create a branch channel for A and B, during the absence of C\u201d. I guess this is what you mean by being able to \"predict in advance who will become absent\u201d\n>> \n>> I call this process as \u201csemi-cooperative channel closing\u201d (SCCC). During a SCCC, the settlement tx will have 2 outputs: one as (A & B), one as (C). Therefore, a branch channel could be opened with the (A & B) output. The channel opening must use NOINPUT signature, since we don\u2019t know the txid of the settlement tx. With the output tagging requirement, (A & B) must be tagged, and lead to the fungibility loss you described.\n>> \n>> However, it is possible to make 2 settlement txs during SCCC. Outputs of the settlement tx X are tagged(A&B) and C. Outputs of the settlement tx Y are untagged(A&B) and C. Both X and Y are BIP68 relative-time-locked, but Y has a longer time lock.\n>> \n>> The branch channel is opened on top of the tagged output of tx X. If A and B want to close the channel without C, they need to publish the last update tx of the main channel. Once the update tx is confirmed, its txid becomes permanent, so are the txids of X and Y. If A and B decide to close the channel cooperatively, they could do it on top of the untagged output of tx Y, without using NOINPUT. There won\u2019t be any fungibility loss. Other people will only see the uncooperative closing of the main channel, and couldn\u2019t even tell the number of parties in the main channel. Unfortunately, the unusual long lock time of Y might still tell something.\n>> \n>> If anything goes wrong, A or B could publish X before the lock time of Y, and settle it through the usual eltoo style. Since this is an uncooperative closing anyway, the extra fungibility loss due to tagging is next to nothing. However, it may suggest that the main channel was a multi-party one.\n>> \n>> For C, the last update tx of the main channel and the settlement tx Y are the only things he needs to get the money back. C has to sign tx X, but he shouldn\u2019t get the complete tx X. Otherwise, C might have an incentive to publish X in order to get the money back earlier, at the cost of fungibility loss of the branch channel.\n>> \n>> To minimise the fungibility loss, we\u2019d better make it a social norm: if you sign your tx with NOINPUT, always try to make all outputs tagged to be NOINPUT-spendable. (NOTE: you can still spend tagged outputs with normal signatures, so this won\u2019t permanently taint your coins as NOINPUT-spendable) It makes sense because the use of NOINPUT signature strongly suggests that you don\u2019t know the txid of the parent tx, so you may most likely want your outputs to be NOINPUT-spendable as well. I thought of making this a policy or consensus rule, but may be it\u2019s just overkill.\n>> \n>> \n>> \n>>> On 9 Feb 2019, at 3:01 AM, Jonas Nick <jonasdnick at gmail.com> wrote:\n>>> \n>>> Output tagging may result in reduced fungibility in multiparty eltoo channels.\n>>> If one party is unresponsive, the remaining participants want to remove\n>>> the party from the channel without downtime. This is possible by creating\n>>> settlement transactions which pay off the unresponsive party and fund a new\n>>> channel with the remaining participants.\n>>> \n>>> When the party becomes unresponsive, the channel is closed by broadcasting the\n>>> update transaction as usual. As soon as that happens the remaining\n>>> participants can start to update their new channel. Their update signatures\n>>> must use SIGHASH_NOINPUT. This is because in eltoo the settlement txid is not\n>>> final (because update tx is not confirmed and may have to rebind to another\n>>> output). Therefore, the funding output of the new channel must be NOINPUT\n>>> tagged. Assuming the remaining parties later settle cooperatively, this loss\n>>> of fungibility would not have happened without output tagging.\n>>> \n>>> funding output          update output                                    settlement outputs              update output\n>>> [ A & B & C ] -> ... -> [ (A & B & C & state CLTV) | (As & Bs & Cs) ] -> [ NOINPUT tagged: (A' & B'), -> ...\n>>>                                                                          C' ]\n>>> If the expectation is that the unresponsive party returns, fungibility is\n>>> not reduced due to output tagging because the above scheme can be used\n>>> off-chain until the original channel can be continued.\n>>> \n>>> Side note: I was not able to come up with an similar, eltoo-like protocol that works\n>>> if you can't predict in advance who will become absent.\n>>> \n>>> On 12/13/18 12:32 PM, Johnson Lau via bitcoin-dev wrote:\n>>>> NOINPUT is very powerful, but the tradeoff is the risks of signature replay. While the key holders are expected not to reuse key pair, little could be done to stop payers to reuse an address. Unfortunately, key-pair reuse has been a social and technical norm since the creation of Bitcoin (the first tx made in block 170 reused the previous public key). I don\u2019t see any hope to change this norm any time soon, if possible at all.\n>>>> \n>>>> As the people who are designing the layer-1 protocol, we could always blame the payer and/or payee for their stupidity, just like those people laughed at victims of Ethereum dumb contracts (DAO, Parity multisig, etc). The existing bitcoin script language is so restrictive. It disallows many useful smart contracts, but at the same time prevented many dumb contracts. After all, \u201csmart\u201d and \u201cdumb\u201d are non-technical judgement. The DAO contract has always been faithfully executed. It\u2019s dumb only for those invested in the project. For me, it was just a comedy show.\n>>>> \n>>>> So NOINPUT brings us more smart contract capacity, and at the same time we are one step closer to dumb contracts. The target is to find a design that exactly enables the smart contracts we want, while minimising the risks of misuse.\n>>>> \n>>>> The risk I am trying to mitigate is a payer mistakenly pay to a previous address with the exactly same amount, and the previous UTXO has been spent using NOINPUT. Accidental double payment is not uncommon. Even if the payee was honest and willing to refund, the money might have been spent with a replayed NOINPUT signature. Once people lost a significant amount of money this way, payers (mostly exchanges) may refuse to send money to anything other than P2PKH, native-P2WPKH and native-P2WSH (as the only 3 types without possibility of NOINPUT)\n>>>> \n>>>> The proposed solution is that an output must be \u201ctagged\u201d for it to be spendable with NOINPUT, and the \u201ctag\u201d must be made explicitly by the payer. There are 2 possible ways to do the tagging:\n>>>> \n>>>> 1. A certain bit in the tx version must be set\n>>>> 2. A certain bit in the scriptPubKey must be set\n>>>> \n>>>> I will analyse the pros and cons later.\n>>>> \n>>>> Using eltoo as example. The setup utxo is a simple 2-of-2 multisig, and should not be tagged. This makes it indistinguishable from normal 1-of-1 utxo. The trigger tx, which spends the setup utxo, should be tagged, so the update txs could spend the trigger utxo with NOINPUT. Similarly, all update txs should be tagged, so they could be spent by other update txs and settlement tx with NOINPUT. As the final destination, there is no need to tag in the settlement tx.\n>>>> \n>>>> In payer\u2019s perspective, tagging means \u201cI believe this address is for one-time-use only\u201d Since we can\u2019t control how other people manage their addresses, we should never do tagging when paying to other people.\n>>>> \n>>>> I mentioned 2 ways of tagging, and they have pros and cons. First of all, tagging in either way should not complicate the eltoo protocol in anyway, nor bring extra block space overhead.\n>>>> \n>>>> A clear advantage of tagging with scriptPubKey is we could tag on a per-output basis. However, scriptPubKey tagging is only possible with native-segwit, not P2SH. That means we have to disallow NOINPUT in P2SH-segwit (Otherwise, *all* P2SH addresses would become \u201crisky\u201d for payers) This should be ok for eltoo, since it has no reason to use P2SH-segwit in intermediate txs, which is more expensive.\n>>>> \n>>>> Another problem with scriptPubKey tagging is all the existing bech32 implementations will not understand the special tag, and will pay to a tagged address as usual. An upgrade would be needed for them to refuse sending to tagged addresses by default.\n>>>> \n>>>> On the other hand, tagging with tx version will also protect P2SH-segwit, and all existing wallets are protected by default. However, it is somewhat a layer violation and you could only tag all or none output in the same tx. Also, as Bitcoin Core has just removed the tx version from the UTXO database, adding it back could be a little bit annoying, but doable.\n>>>> \n>>>> There is an extension to the version tagging, which could make NOINPUT even safer. In addition to tagging requirement, NOINPUT will also sign the version of the previous tx. If the wallet always uses a randomised tx version, it makes accidental replay very unlikely. However, that will burn a few more bits in the tx version field.\n>>>> \n>>>> While this seems fully compatible with eltoo, is there any other proposals require NOINPUT, and is adversely affected by either way of tagging?\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>> \n>> \n>>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2019-02-19T19:04:03",
                "message_text_only": "On Thursday 13 December 2018 12:32:44 Johnson Lau via bitcoin-dev wrote:\n> While this seems fully compatible with eltoo, is there any other proposals\n> require NOINPUT, and is adversely affected by either way of tagging?\n\nYes, this seems to break the situation where a wallet wants to use NOINPUT for \neverything, including normal L1 payments. For example, in the scenario where \naddress reuse will be rejected/ignored by the recipient unconditionally, and \nthe payee is considered to have burned their bitcoins by attempting it.\n\nLuke"
            },
            {
                "author": "Johnson Lau",
                "date": "2019-02-19T19:22:07",
                "message_text_only": "This only depends on the contract between the payer and payee. If the contract says address reuse is unacceptable, it\u2019s unacceptable. It has nothing to do with how the payee spends the coin. We can\u2019t ban address reuse at protocol level (unless we never prune the chain), so address reuse could only be prevented at social level.\n\nUsing NOINPUT is also a very weak excuse: NOINPUT always commit to the value. If the payer reused an address but for different amount, the payee can\u2019t claim the coin is lost due to previous NOINPUT use. A much stronger way is to publish the key after a coin is well confirmed.\n\n> On 20 Feb 2019, at 3:04 AM, Luke Dashjr <luke at dashjr.org> wrote:\n> \n> On Thursday 13 December 2018 12:32:44 Johnson Lau via bitcoin-dev wrote:\n>> While this seems fully compatible with eltoo, is there any other proposals\n>> require NOINPUT, and is adversely affected by either way of tagging?\n> \n> Yes, this seems to break the situation where a wallet wants to use NOINPUT for \n> everything, including normal L1 payments. For example, in the scenario where \n> address reuse will be rejected/ignored by the recipient unconditionally, and \n> the payee is considered to have burned their bitcoins by attempting it.\n> \n> Luke"
            },
            {
                "author": "Luke Dashjr",
                "date": "2019-02-19T20:24:12",
                "message_text_only": "Even besides NOINPUT, such a wallet would simply never show a second payment \nto the same address (or at least never show it as confirmed, until \nsuccessfully spent).\n\nAt least if tx versions are used, it isn't possible to indicate this \nrequirement in current Bitcoin L1 addresses. scriptPubKey might not be \nimpossible to encode, but it isn't really clear what the purpose of doing so \nis.\n\nIf people don't want to use NOINPUT, they should just not use it. Trying to \nimplement a nanny in the protocol is inappropriate and limits what developers \ncan do who actually want the features.\n\nLuke\n\n\nOn Tuesday 19 February 2019 19:22:07 Johnson Lau wrote:\n> This only depends on the contract between the payer and payee. If the\n> contract says address reuse is unacceptable, it\u2019s unacceptable. It has\n> nothing to do with how the payee spends the coin. We can\u2019t ban address\n> reuse at protocol level (unless we never prune the chain), so address reuse\n> could only be prevented at social level.\n>\n> Using NOINPUT is also a very weak excuse: NOINPUT always commit to the\n> value. If the payer reused an address but for different amount, the payee\n> can\u2019t claim the coin is lost due to previous NOINPUT use. A much stronger\n> way is to publish the key after a coin is well confirmed.\n>\n> > On 20 Feb 2019, at 3:04 AM, Luke Dashjr <luke at dashjr.org> wrote:\n> >\n> > On Thursday 13 December 2018 12:32:44 Johnson Lau via bitcoin-dev wrote:\n> >> While this seems fully compatible with eltoo, is there any other\n> >> proposals require NOINPUT, and is adversely affected by either way of\n> >> tagging?\n> >\n> > Yes, this seems to break the situation where a wallet wants to use\n> > NOINPUT for everything, including normal L1 payments. For example, in the\n> > scenario where address reuse will be rejected/ignored by the recipient\n> > unconditionally, and the payee is considered to have burned their\n> > bitcoins by attempting it.\n> >\n> > Luke"
            },
            {
                "author": "Johnson Lau",
                "date": "2019-02-19T20:36:51",
                "message_text_only": "> On 20 Feb 2019, at 4:24 AM, Luke Dashjr <luke at dashjr.org> wrote:\n> \n> Even besides NOINPUT, such a wallet would simply never show a second payment \n> to the same address (or at least never show it as confirmed, until \n> successfully spent).\n\nThis is totally unrelated to NOINPUT. You can make a wallet like this today already, and tell your payer not to reuse address.\n\n\n> \n> At least if tx versions are used, it isn't possible to indicate this \n> requirement in current Bitcoin L1 addresses. scriptPubKey might not be \n> impossible to encode, but it isn't really clear what the purpose of doing so \n> is.\n\nIt sounds like you actually want to tag such outputs as scriptPubKey, so you could encode this requirement in the address?\n\nIf we allow NOINPUT unconditionally (i.e. all v1 addresses are spendable with NOINPUT), you may only create a different proposal to indicate such special requirements \n\n> \n> If people don't want to use NOINPUT, they should just not use it. Trying to \n> implement a nanny in the protocol is inappropriate and limits what developers \n> can do who actually want the features.\n> \n> Luke\n> \n> \n> On Tuesday 19 February 2019 19:22:07 Johnson Lau wrote:\n>> This only depends on the contract between the payer and payee. If the\n>> contract says address reuse is unacceptable, it\u2019s unacceptable. It has\n>> nothing to do with how the payee spends the coin. We can\u2019t ban address\n>> reuse at protocol level (unless we never prune the chain), so address reuse\n>> could only be prevented at social level.\n>> \n>> Using NOINPUT is also a very weak excuse: NOINPUT always commit to the\n>> value. If the payer reused an address but for different amount, the payee\n>> can\u2019t claim the coin is lost due to previous NOINPUT use. A much stronger\n>> way is to publish the key after a coin is well confirmed.\n>> \n>>> On 20 Feb 2019, at 3:04 AM, Luke Dashjr <luke at dashjr.org> wrote:\n>>> \n>>> On Thursday 13 December 2018 12:32:44 Johnson Lau via bitcoin-dev wrote:\n>>>> While this seems fully compatible with eltoo, is there any other\n>>>> proposals require NOINPUT, and is adversely affected by either way of\n>>>> tagging?\n>>> \n>>> Yes, this seems to break the situation where a wallet wants to use\n>>> NOINPUT for everything, including normal L1 payments. For example, in the\n>>> scenario where address reuse will be rejected/ignored by the recipient\n>>> unconditionally, and the payee is considered to have burned their\n>>> bitcoins by attempting it.\n>>> \n>>> Luke\n>"
            }
        ],
        "thread_summary": {
            "title": "Safer NOINPUT with output tagging",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "Johnson Lau",
                "Alejandro Ranchal Pedrosa",
                "ZmnSCPxj",
                "Luke Dashjr",
                "Jonas Nick"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 95544
        }
    },
    {
        "title": "[bitcoin-dev] Predicate Tree in ZkVM: a variant of Taproot/G'root",
        "thread_messages": [
            {
                "author": "Oleg Andreev",
                "date": "2019-02-01T17:56:49",
                "message_text_only": "A follow-up comment: I've have sent this email right before Pieter's talk on miniscript at Stanford yesterday. I want to express my appreciation to the thinking about scripts/contracts that Pieter, Andy, Greg have been promoting for long time. These ideas influenced a lot the design decisions in ZkVM: \"blockchain as a court\", very limited functionality and clarity of scripts, and, as Pieter laid out yesterday, composition of policies. These are all the same values that I'm trying to reflect in ZkVM, that's why i think it might be interesting to this mailing list.\n\nAlso, Neha Narula asked a question this morning:\n\n> Isn't this a DoS vector, in that an attacker could generate a lot of expensive code to execute in the VM which would then be rejected once the checks get executed?  If so, how critical is this deferred execution of point operations to your design? \n\nThe answer: hopefully it's not a DoS vector, we are working on this right now. Programs for `call` and `delegate` have to be statically built into the transaction bytecode string, and cannot be constructed within the VM (so it's very similar to P2SH). ZkVM is similar to Bitcoin Script in that the execution cost is proportional to the program length: one cannot make a short program that would use loops or recursion into dynamically constructed programs to exhibit arbitrary validation cost. For those familiar with TxVM released last year, we are removing loops and dynamic program construction, and gas-like \"runlimit\" with them from ZkVM.\n\nAnother feature is inspired by old proposal by Pieter (IIRC) to treat checksig as all-or-nothing. ZkVM does not do dynamic branching based on outcomes of expensive operations. Signature checks, predicate tree traversal - all have to unconditionally succeed.\n\n1. This makes the program execution (w/o ECC ops) very fast and proportional to the length of the program.\n2. Then, all the collected ECC ops give precise metric of how expensive the rest of the validation would be.\n3. Plus, the constraint system proof blob (that comes with the transaction) by itself gives an exact measurement of the bulletproofs validation cost.\n\nThe upstream protocol (\"blockchain rules\") can have soft- or hard- caps on both program length and amount of ECC operations (similar to the limit on sig checks per block in Bitcoin). That said, we haven't drilled into specifics what these caps should be and how they should be enforced, that's still in the works.\n\n\n> On Jan 31, 2019, at 15:44 , Oleg Andreev <oleganza at gmail.com> wrote:\n> \n> Hi,\n> \n> We've been working for a thing called ZkVM [1] for the last few weeks. It is a \"blockchain virtual machine\" in the spirit of Bitcoin, with multi-asset transfers and zero-knowledge programmable constraints.\n> \n> As a part of its design, there is a \"Predicate Tree\" \u2014 a variant of Taproot by Greg Maxwell [2] and G'root by Anthony Towns [3] that I would like to present here. Hopefully it is useful to the discussion, and I would appreciate any feedback.\n> \n> ## Background\n> \n> In ZkVM there are linear types Contract and Value (in addition to plain data types), where Contract also implements \"object capabilities\" pattern: Contract \"locks\" a collection of data and Values under a \"predicate\" which is represented by a single group element (\"point\" in ECC terms). The predicate can be \"satisfied\" in a number of allowed ways which makes the contract unlock its contents, e.g. release the stored Value which can then be locked in a new unspent output.\n> \n> ## Predicate Tree\n> \n> Predicate is a point that represents one of three things, which allows composing conditions in an arbitrary tree:\n> \n> 1. Public key\n> 2. Program\n> 3. Disjunction of two other predicates\n> \n> Public key allows representing N-of-N signing conditions (and M-of-N with proper threshold key setup, although small combinations like 2-of-3 can be non-interactively represented as a tree of 3 combinations of 2-of-2 conditions):\n> \n>   P = x*B  (x is a secret, B is a primary generator point)\n> \n> Program commitment is a P2SH-like commitment:\n> \n>   P = hash2scalar(program)*B2   (B2 is orthogonal to B, so one cannot sign for P, but must reveal the program)\n> \n> Disjunction (asymmetric to allow happy-path signing with the left predicate):\n> \n>   P = L + hash2scalar(L,R)*B\n> \n> \n> ## VM instructions\n> \n> To use the predicate trees, ZkVM provides 4 instructions:\n> \n> 1. `signtx` to verify the signature over the transaction ID treating the predicate as a pubkey.\n> 2. `call` to reveal the committed program and execute it.\n> 3. `left`/`right` to replace the contract's predicate with one of the sub-predicates in a disjunction.\n> 4. `delegate` to check a signature over a program and execute that program (pay-to-signed-program pattern).\n> \n> More details are in the ZkVM spec: https://github.com/interstellar/zkvm/blob/main/spec/ZkVM.md#signtx\n> \n> `call` and `delegate` differ in that `call` reveals and runs a pre-arranged program (like in P2SH), while `delegate` allows choosing the program later which can be signed with a pre-arranged public key. `delegate` also enables use cases for SIGHASH: if a specific output or outputs or constraints must be signed, they can be represented by such program snippet. Likewise, a \"revocation token\" for the payment channel (LN) can be implemented with `delegate` instruction.\n> \n> \n> ## Performance\n> \n> For performance, the following rules are built into ZkVM:\n> \n> 1. All point operations are deferred. Signature checks, disjunction proofs, program commitment proofs - are not executed right away, but deferred and verified in a batch after the VM execution is complete. This enables significant savings, especially since half or third of the terms reuse static points B and B2.\n> 2. `signtx` does not accept individual signatures, but uses a single aggregated signature for the whole transaction. All the pubkeys are remembered in a separate set and combined via MuSig-style [4] protocol to check the single 64-byte signature over txid in the end of the VM execution. In other words, signature aggregation is not optional for `signtx` (signatures over txid). Note: the `delegate` instruction permits arbitrary programs, so it uses one signature per program.\n> \n> \n> ## What is different from Taproot/G'root\n> \n> (1) No pure hash-based MAST: each time one peels off a layer of a tree, there's an ECC check which is more expensive than pure-hash merkle path check, but makes the design simpler and all ECC ops are batched alongside bulletproofs R1CS verification statement, making the performance difference unimportant.\n> \n> (2) There is no designated blinding factor or a pubkey with the program commitment like in G'root. This is not something i'm particularly sure about, but will lay out the current rationale:\n> 1. The happy-path one-time-key normally acts as a sufficient blinding factor for the program.\n> 2. If the program needs a blinding factor, it can be embedded as a `<garbage> drop`.\n> 3. The combo of \"sign + programmatic constraints\" is done by having instructions inside the program that wrap the value(s) in a transient contract with the required pubkey and leaving it on the stack.\n> \n> \n> ## References\n> \n> [1] https://github.com/interstellar/zkvm\n> [2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-January/015614.html\n> [3] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-July/016249.html\n> [4] https://blockstream.com/2018/01/23/musig-key-aggregation-schnorr-signatures/\n> \n> \n> \n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190201/20c16a4b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Predicate Tree in ZkVM: a variant of Taproot/G'root",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Oleg Andreev"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 7686
        }
    },
    {
        "title": "[bitcoin-dev] Card Shuffle To Bitcoin Seed",
        "thread_messages": [
            {
                "author": "rhavar at protonmail.com",
                "date": "2019-02-02T19:51:59",
                "message_text_only": "More of a shower-thought than a BIP, but it's something I've long wish (hardware) wallets supported:\n\n---\n\nAbstract: Bitcoin Wallets generally ask us to trust their seed generation is both correct and honest. Especially for hardware and air gapped wallets, this is both a big ask and more or less impossible to practically verify. So we propose a bring-your-own-entropy approach in which the wallet can function completely deterministically. Our method is based on shuffling physical deck of cards. There are 52!  (2^219.88) different shuffle order, which is a big enough space to be secure against collision and brute force attacks. Conveniently a shuffled deck of cards also can serve as a physical backup which is easy to hide in plain sight with great plausible deniability.\n\nRepresentation:\n\nEach card has a suit which can be represented by one of SCHD (spades, clubs, hearts, diamonds) and a value of one of 23456789TJQKA where the numbers are obvious and (T=ten, J=jack, Q=queen, K=king, A=ace) so \"7 of clubs\" would be represented by \"7C\" and a \"Ten of Hearts\" would be represented with \"TH\".\n\nAn deck of cards looks like:\n\n2S,3S,4S,5S,6S,7S,8S,9S,TS,JS,QS,KS,AS,2C,3C,4C,5C,6C,7C,8C,9C,TC,JC,QC,KC,AC,2H,3H,4H,5H,6H,7H,8H,9H,TH,JH,QH,KH,AH,2D,3D,4D,5D,6D,7D,8D,9D,TD,JD,QD,KD,AD\n\nAnd can be verified by making sure that every one of the 52 cards appears exactly once.\n\nStep 1.  Shuffle your deck of cards\n\nThis is a lot harder than you'd imagine, so do it quite a few times, with quite a few different techniques. It is advised to do at *least* 7 good quality shuffles to achieve a true cryptographically secure shuffle. Do not look at the cards while shuffling (to avoid biasing) and don't be afraid to also shuffle them face down on the table. Err on the side over over-shuffling.\nSee also: https://en.wikipedia.org/wiki/Shuffling#Sufficient_number_of_shuffles\n\nStep 2. Write out the order (comma separated)\n\nAnd example shuffle is:\n\n5C,7C,4C,AS,3C,KC,AD,QS,7S,2S,5H,4D,AC,9C,3H,6H,9D,4S,8D,TD,2H,7H,JD,QD,2D,JC,KH,9S,9H,4H,6C,7D,3D,6S,2C,AH,QC,TH,TC,JS,6D,8H,8C,JH,8S,KD,QH,5D,5S,KS,TS,3S\n\nStep 3.  Sha512 it to create a seed\n\nIn the example above you should get:\ndc04e4c331b1bd347581d4361841335fe0b090d39dfe5e1c258c547255cd5cf1545e2387d8a7c4dc53e03cacca049a414a9269a2ac6954429955476c56038498\n\nStep 4. Interpret it\n\ne.g. For bip32 you would treat the first 32 bytes as the private key, and the second 32 bytes as as the extension code.\n\n-Ryan\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190202/5489fb15/attachment.html>"
            },
            {
                "author": "Adam Ficsor",
                "date": "2019-02-04T06:49:27",
                "message_text_only": "Unlike mouse movement it works in a CLI software, which is great. However,\nisn't there something else you can use instead of cards? Something with\ninvariant culture and maybe more common.\n\nOn Sun, Feb 3, 2019 at 7:27 PM Ryan Havar via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> More of a shower-thought than a BIP, but it's something I've long wish\n> (hardware) wallets supported:\n>\n> ---\n>\n> Abstract: Bitcoin Wallets generally ask us to trust their seed generation\n> is both correct and honest. Especially for hardware and air gapped wallets,\n> this is both a big ask and more or less impossible to practically verify.\n> So we propose a bring-your-own-entropy approach in which the wallet can\n> function completely deterministically. Our method is based on shuffling\n> physical deck of cards. There are 52!  (2^219.88) different shuffle order,\n> which is a big enough space to be secure against collision and brute force\n> attacks. Conveniently a shuffled deck of cards also can serve as a physical\n> backup which is easy to hide in plain sight with great plausible\n> deniability.\n>\n>\n> Representation:\n>\n> Each card has a suit which can be represented by one of SCHD (spades,\n> clubs, hearts, diamonds) and a value of one of 23456789TJQKA where the\n> numbers are obvious and (T=ten, J=jack, Q=queen, K=king, A=ace) so \"7 of\n> clubs\" would be represented by \"7C\" and a \"Ten of Hearts\" would be\n> represented with \"TH\".\n>\n> An deck of cards looks like:\n>\n>\n> 2S,3S,4S,5S,6S,7S,8S,9S,TS,JS,QS,KS,AS,2C,3C,4C,5C,6C,7C,8C,9C,TC,JC,QC,KC,AC,2H,3H,4H,5H,6H,7H,8H,9H,TH,JH,QH,KH,AH,2D,3D,4D,5D,6D,7D,8D,9D,TD,JD,QD,KD,AD\n>\n> And can be verified by making sure that every one of the 52 cards appears\n> exactly once.\n>\n>\n> Step 1.  Shuffle your deck of cards\n>\n> This is a lot harder than you'd imagine, so do it quite a few times, with\n> quite a few different techniques. It is advised to do at *least* 7 good\n> quality shuffles to achieve a true cryptographically secure shuffle. Do not\n> look at the cards while shuffling (to avoid biasing) and don't be afraid to\n> also shuffle them face down on the table. Err on the side over\n> over-shuffling.\n> See also:\n> https://en.wikipedia.org/wiki/Shuffling#Sufficient_number_of_shuffles\n>\n> Step 2. Write out the order (comma separated)\n>\n> And example shuffle is:\n>\n>\n> 5C,7C,4C,AS,3C,KC,AD,QS,7S,2S,5H,4D,AC,9C,3H,6H,9D,4S,8D,TD,2H,7H,JD,QD,2D,JC,KH,9S,9H,4H,6C,7D,3D,6S,2C,AH,QC,TH,TC,JS,6D,8H,8C,JH,8S,KD,QH,5D,5S,KS,TS,3S\n>\n> Step 3.  Sha512 it to create a seed\n>\n> In the example above you should get:\n>\n> dc04e4c331b1bd347581d4361841335fe0b090d39dfe5e1c258c547255cd5cf1545e2387d8a7c4dc53e03cacca049a414a9269a2ac6954429955476c56038498\n>\n> Step 4. Interpret it\n>\n> e.g. For bip32 you would treat the first 32 bytes as the private key, and\n> the second 32 bytes as as the extension code.\n>\n>\n>\n>\n> -Ryan\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nBest,\n\u00c1d\u00e1m\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190204/0bc324ae/attachment.html>"
            },
            {
                "author": "James MacWhyte",
                "date": "2019-02-04T21:05:41",
                "message_text_only": "James\n\n\nOn Sun, Feb 3, 2019 at 10:27 AM Ryan Havar via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Conveniently a shuffled deck of cards also can serve as a physical backup\n> which is easy to hide in plain sight with great plausible deniability.\n>\n\nTo make sure someone doesn't play with your cards and mix up the order, use\na permanent marker to draw a diagonal line on the side of the deck from\ncorner to corner. If the cards ever get mixed up, you can put them back in\norder by making sure the diagonal line matches up.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190204/67e4ffe2/attachment.html>"
            },
            {
                "author": "Devrandom",
                "date": "2019-02-05T01:37:16",
                "message_text_only": "I would suggest 50+ 6-sided dice rolls, giving about 128 bits of entropy.\nCompared to a shuffle, it's easier to be sure that you got the right amount\nof entropy, even if the dice are somewhat biased.\n\n\nOn Mon, Feb 4, 2019 at 2:33 PM James MacWhyte via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> James\n>\n>\n> On Sun, Feb 3, 2019 at 10:27 AM Ryan Havar via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Conveniently a shuffled deck of cards also can serve as a physical backup\n>> which is easy to hide in plain sight with great plausible deniability.\n>>\n>\n> To make sure someone doesn't play with your cards and mix up the order,\n> use a permanent marker to draw a diagonal line on the side of the deck from\n> corner to corner. If the cards ever get mixed up, you can put them back in\n> order by making sure the diagonal line matches up.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190204/11466a14/attachment-0001.html>"
            },
            {
                "author": "Alan Evans",
                "date": "2019-02-06T13:48:25",
                "message_text_only": "It's not quite enough to just do SHA512, you missed out this condition\n(incredibly rare as it is):\n\n> In case IL is 0 or \u2265n, the master key is invalid.\n\nAlso I can't see how I would use this to seed a hardware wallet that\nrequires a BIP39 seed as mentioned in your abstract.\n\nFor both of those reasons, you may want to just invent/formalize a scheme\nthat takes Cards -> Entropy.\n>From that Entropy one can generate BIP39, and non-BIP39 fans can just\ncontinue, generate and store their root xprv.\n\nPrior art: Note that Ian Coleman's BIP39 site already supports Cards (and\nDice), see the logic here:\nhttps://github.com/iancoleman/bip39/blob/master/src/js/entropy.js\n\n[image: image.png]\n\nNote it detected \"full deck\". It also calculates the Total Bits of Entropy\nand can handle card replacement and multiple decks.\n\nPS, you're a bit out on your entropy calculation, log2(52!) ~= 225.58 bits,\nnot 219.\n\n\nOn Tue, 5 Feb 2019 at 02:08, Devrandom via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I would suggest 50+ 6-sided dice rolls, giving about 128 bits of entropy.\n> Compared to a shuffle, it's easier to be sure that you got the right amount\n> of entropy, even if the dice are somewhat biased.\n>\n>\n> On Mon, Feb 4, 2019 at 2:33 PM James MacWhyte via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>> James\n>>\n>>\n>> On Sun, Feb 3, 2019 at 10:27 AM Ryan Havar via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Conveniently a shuffled deck of cards also can serve as a physical\n>>> backup which is easy to hide in plain sight with great plausible\n>>> deniability.\n>>>\n>>\n>> To make sure someone doesn't play with your cards and mix up the order,\n>> use a permanent marker to draw a diagonal line on the side of the deck from\n>> corner to corner. If the cards ever get mixed up, you can put them back in\n>> order by making sure the diagonal line matches up.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190206/ae1b7d9b/attachment-0001.html>"
            },
            {
                "author": "Alan Evans",
                "date": "2019-02-06T13:51:59",
                "message_text_only": "Image didn't seem to attach:\n[image: image.png]\n\nOn Wed, 6 Feb 2019 at 09:48, Alan Evans <thealanevans at gmail.com> wrote:\n\n> It's not quite enough to just do SHA512, you missed out this condition\n> (incredibly rare as it is):\n>\n> > In case IL is 0 or \u2265n, the master key is invalid.\n>\n> Also I can't see how I would use this to seed a hardware wallet that\n> requires a BIP39 seed as mentioned in your abstract.\n>\n> For both of those reasons, you may want to just invent/formalize a scheme\n> that takes Cards -> Entropy.\n> From that Entropy one can generate BIP39, and non-BIP39 fans can just\n> continue, generate and store their root xprv.\n>\n> Prior art: Note that Ian Coleman's BIP39 site already supports Cards (and\n> Dice), see the logic here:\n> https://github.com/iancoleman/bip39/blob/master/src/js/entropy.js\n>\n> [image: image.png]\n>\n> Note it detected \"full deck\". It also calculates the Total Bits of Entropy\n> and can handle card replacement and multiple decks.\n>\n> PS, you're a bit out on your entropy calculation, log2(52!) ~= 225.58\n> bits, not 219.\n>\n>\n> On Tue, 5 Feb 2019 at 02:08, Devrandom via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> I would suggest 50+ 6-sided dice rolls, giving about 128 bits of\n>> entropy.  Compared to a shuffle, it's easier to be sure that you got the\n>> right amount of entropy, even if the dice are somewhat biased.\n>>\n>>\n>> On Mon, Feb 4, 2019 at 2:33 PM James MacWhyte via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>>\n>>> James\n>>>\n>>>\n>>> On Sun, Feb 3, 2019 at 10:27 AM Ryan Havar via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Conveniently a shuffled deck of cards also can serve as a physical\n>>>> backup which is easy to hide in plain sight with great plausible\n>>>> deniability.\n>>>>\n>>>\n>>> To make sure someone doesn't play with your cards and mix up the order,\n>>> use a permanent marker to draw a diagonal line on the side of the deck from\n>>> corner to corner. If the cards ever get mixed up, you can put them back in\n>>> order by making sure the diagonal line matches up.\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190206/d8bfbbc6/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: image.png\nType: image/png\nSize: 176797 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190206/d8bfbbc6/attachment-0001.png>"
            },
            {
                "author": "James MacWhyte",
                "date": "2019-02-07T02:42:55",
                "message_text_only": "Oooh, that's cool. I didn't realize Ian's support for cards looks so slick\nnow!\n\nThanks for the image.\n\nJames\n\n\nOn Wed, Feb 6, 2019 at 7:55 AM Alan Evans via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Image didn't seem to attach:\n> [image: image.png]\n>\n> On Wed, 6 Feb 2019 at 09:48, Alan Evans <thealanevans at gmail.com> wrote:\n>\n>> It's not quite enough to just do SHA512, you missed out this condition\n>> (incredibly rare as it is):\n>>\n>> > In case IL is 0 or \u2265n, the master key is invalid.\n>>\n>> Also I can't see how I would use this to seed a hardware wallet that\n>> requires a BIP39 seed as mentioned in your abstract.\n>>\n>> For both of those reasons, you may want to just invent/formalize a scheme\n>> that takes Cards -> Entropy.\n>> From that Entropy one can generate BIP39, and non-BIP39 fans can just\n>> continue, generate and store their root xprv.\n>>\n>> Prior art: Note that Ian Coleman's BIP39 site already supports Cards (and\n>> Dice), see the logic here:\n>> https://github.com/iancoleman/bip39/blob/master/src/js/entropy.js\n>>\n>> [image: image.png]\n>>\n>> Note it detected \"full deck\". It also calculates the Total Bits of\n>> Entropy and can handle card replacement and multiple decks.\n>>\n>> PS, you're a bit out on your entropy calculation, log2(52!) ~= 225.58\n>> bits, not 219.\n>>\n>>\n>> On Tue, 5 Feb 2019 at 02:08, Devrandom via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> I would suggest 50+ 6-sided dice rolls, giving about 128 bits of\n>>> entropy.  Compared to a shuffle, it's easier to be sure that you got the\n>>> right amount of entropy, even if the dice are somewhat biased.\n>>>\n>>>\n>>> On Mon, Feb 4, 2019 at 2:33 PM James MacWhyte via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>>\n>>>> James\n>>>>\n>>>>\n>>>> On Sun, Feb 3, 2019 at 10:27 AM Ryan Havar via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>>> Conveniently a shuffled deck of cards also can serve as a physical\n>>>>> backup which is easy to hide in plain sight with great plausible\n>>>>> deniability.\n>>>>>\n>>>>\n>>>> To make sure someone doesn't play with your cards and mix up the order,\n>>>> use a permanent marker to draw a diagonal line on the side of the deck from\n>>>> corner to corner. If the cards ever get mixed up, you can put them back in\n>>>> order by making sure the diagonal line matches up.\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190206/8923303d/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: image.png\nType: image/png\nSize: 176797 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190206/8923303d/attachment-0001.png>"
            }
        ],
        "thread_summary": {
            "title": "Card Shuffle To Bitcoin Seed",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Adam Ficsor",
                "rhavar at protonmail.com",
                "Devrandom",
                "James MacWhyte",
                "Alan Evans"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 16769
        }
    },
    {
        "title": "[bitcoin-dev] BIP157 server Murmel introduced, enchancement suggestion to BIP158",
        "thread_messages": [
            {
                "author": "Tamas Blummer",
                "date": "2019-02-03T20:15:25",
                "message_text_only": "TLDR: I suggest to add outpoint filter to BIP158 as it proved to be useful while developing a filter server and allows further checks in filter client.\n\nMurmel is my project within the rust-bitcoin community. https://github.com/rust-bitcoin/murmel\n\nIts goal is to provide a lightweight, at least SPV security, settlement layer for the Lightning Network implementation in Rust. https://github.com/rust-bitcoin/rust-lightning\n\nMurmel relies on BIP157 (Client Side Block Filtering). Since Bitcoin Core does not yet support this protocol extension, I also added filter and block server functionality to Murmel and this might be useful for \ndevelopment purposes any other BIP157 client project.\n\nYou may compile and run a Murmel filter server to support your client development. It bootstraps within a few hours. Follow the instructions at: https://github.com/rust-bitcoin/murmel\n\nWhile implementing both client and server side I made an observation that should be considered for BIP158:\n\nBIP158 specifies base filter containing all scripts spent or created by a block (except those with OP_RETURN). I found it useful to also compute a filter on spent and created outpoints.\nThe Murmel filter server consults these outpoint filters to find the transactions with the spent scripts while computing the base script filter. Since outpoints usually getting spent shortly after created, this approach works well enough to\nkeep up with the blockchain, although far too slow to rely on it while bootstrapping. An advantage of this approach of looking up UTXO is that there is nothing to be recomputed at re-org, filters are consulted following\nthe path from current tip back to genesis. This fits well with Murmel\u2019s storage, that is my other project Hammersbald https://github.com/rust-bitcoin/hammersbald, a highly efficient truly append only blockchain store in Rust.\nFilter matching is also nicely parallelizable looking up subsets of spent outputs in parallel. \n\nA lightweight client can use outpoint filters to efficiently validate spent coins or miner reward, which goes beyond SPV guarantees. This is probabilistically possible now, and definitely once filters are committed.\n\nFor above reasons I suggest to also add outpoint filter to BIP158, so filter servers may support it, as does Murmel. Murmel is moving quickly; I tagged the version as of this mail with DEVLIST for later reference.\n\nTamas Blummer"
            }
        ],
        "thread_summary": {
            "title": "BIP157 server Murmel introduced, enchancement suggestion to BIP158",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tamas Blummer"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2399
        }
    },
    {
        "title": "[bitcoin-dev] Interrogating a BIP157 server, BIP158 change proposal",
        "thread_messages": [
            {
                "author": "Tamas Blummer",
                "date": "2019-02-04T11:41:20",
                "message_text_only": "TLDR: a change to BIP158 would allow decision on which filter chain is correct at lower bandwith use\n\nAssume there is a BIP157 client that learned a filter header chain earlier and is now offered an alternate reality by a newly connected BIP157 server.\n\nThe client notices the alternate reality by routinely asking for filter chain checkpoints after connecting to a new BIP157 server. A divergence at a checkpoint means that the server disagrees the client's history at or before the first diverging checkpoint. The client would then request the filter headers between the last matching and first divergent checkpoint, and quickly figure which block\u2019s filter is the first that does not match previous assumption, and request that filter from the server.\n\nThe client downloads the corresponding block, checks that its header fits the PoW secured best header chain, re-calculates merkle root of its transaction list to know that it is complete and queries the filter to see if every output script of every transaction is contained in there, if not the server is lying, the case is closed, the server disconnected.\n\nHaving all output scripts in the filter does not however guarantee that the filter is correct since it might omit input scripts. Inputs scripts are not part of the downloaded block, but are in some blocks before that. Checking those are out of reach for lightweight client with tools given by the current BIP.\n\nA remedy here would be an other filter chain on created and spent outpoints as is implemented currently by Murmel. The outpoint filter chain must offer a match for every spent output of the block with the divergent filter, otherwise the interrogated server is lying since a PoW secured block can not spend coins out of nowhere. Doing this check would already force the client to download the outpoint filter history up-to the point of divergence. Then the client would have to download and PoW check every block that shows a match in outpoints until it figures that one of the spent outputs has a script that was not in the server\u2019s filter, in which case the server is lying. If everything checks out then the previous assumption on filter history was incorrect and should be replaced by the history offered by the interrogated server. \n\nAs you see the interrogation works with this added filter but is highly ineffective. A really light client should not be forced to download lots of blocks just to uncover a lying filter server. This would actually be an easy DoS on light BIP157 clients.\n\nA better solution is a change to BIP158 such that the only filter contains created scripts and spent outpoints. It appears to me that this would serve well both wallets and interrogation of filter servers well:\n\nWallets would recognize payments to their addresses by the filter as output scripts are included, spends from the wallet would be recognized as a wallet already knows outpoints of its previously received coins, so it can query the filters for them.\n\nInterrogation of a filter server also simplifies, since the filter of the block can be checked entirely against the contents of the same block. The decision on filter correctness does not require more bandwith then download of a block at the mismatching checkpoint. The client could only be forced at max. to download 1/1000 th of the blockchain in addition to the filter header history.\n\nTherefore I suggest to change BIP158 to have a base filter, defined as:\n\nA basic filter MUST contain exactly the following items for each transaction in a block:\n\t\u2022 Spent outpoints\n\t\u2022 The scriptPubKey of each output, aside from all OP_RETURN output scripts.\n\nTamas Blummer"
            },
            {
                "author": "Jim Posen",
                "date": "2019-02-04T20:18:08",
                "message_text_only": "Please see the thread \"BIP 158 Flexibility and Filter Size\" from 2018\nregarding the decision to remove outpoints from the filter [1].\n\nThanks for bringing this up though, because more discussion is needed on\nthe client protocol given that clients cannot reliably determine the\nintegrity of a block filter in a bandwidth-efficient manner (due to the\ninclusion of input scripts).\n\nI see three possibilities:\n1) Introduce a new P2P message to retrieve all prev-outputs for a given\nblock (essentially the undo data in Core), and verify the scripts against\nthe block by executing them. While this permits some forms of input script\nmalleability (and thus cannot discriminate between all valid and invalid\nfilters), it restricts what an attacker can do. This was proposed by Laolu\nAFAIK, and I believe this is how btcd is proceeding.\n2) Clients track multiple possible filter header chains and essentially\nconsider the union of their matches. So if any filter received for a\nparticular block header matches, the client downloads the block. The client\ncan ban a peer if they 1) ever return a filter omitting some data that is\nobserved in the downloaded block, 2) repeatedly serve filters that trigger\nfalse positive block downloads where such a number of false positives is\nstatistically unlikely, or 3) repeatedly serves filters that are\nsignificantly larger than the expected size (essentially padding the actual\nfilters with garbage to waste bandwidth). I have not done the analysis yet,\nbut we should be able to come up with some fairly simple banning heuristics\nusing Chernoff bounds. The main downside is that the client logic to track\nmultiple possible filter chains and filters per block is more complex and\nbandwidth increases if connected to a malicious server. I first heard about\nthis idea from David Harding.\n3) Rush straight to committing the filters into the chain (via witness\nreserved value or coinbase OP_RETURN) and give up on the pre-softfork BIP\n157 P2P mode.\n\nI'm in favor of option #2 despite the downsides since it requires the\nsmallest number of changes and is supported by the BIP 157 P2P protocol as\ncurrently written. (Though the recommended client protocol in the BIP needs\nto be updated to account for this). Another benefit of it is that it\nremoves some synchronicity assumptions where a peer with the correct\nfilters keeps timing out and is assumed to be dishonest, while the\ndishonest peer is assumed to be OK because it is responsive.\n\nIf anyone has other ideas, I'd love to hear them.\n\n-jimpo\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html\n\n\n\nOn Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> TLDR: a change to BIP158 would allow decision on which filter chain is\n> correct at lower bandwith use\n>\n> Assume there is a BIP157 client that learned a filter header chain earlier\n> and is now offered an alternate reality by a newly connected BIP157 server.\n>\n> The client notices the alternate reality by routinely asking for filter\n> chain checkpoints after connecting to a new BIP157 server. A divergence at\n> a checkpoint means that the server disagrees the client's history at or\n> before the first diverging checkpoint. The client would then request the\n> filter headers between the last matching and first divergent checkpoint,\n> and quickly figure which block\u2019s filter is the first that does not match\n> previous assumption, and request that filter from the server.\n>\n> The client downloads the corresponding block, checks that its header fits\n> the PoW secured best header chain, re-calculates merkle root of its\n> transaction list to know that it is complete and queries the filter to see\n> if every output script of every transaction is contained in there, if not\n> the server is lying, the case is closed, the server disconnected.\n>\n> Having all output scripts in the filter does not however guarantee that\n> the filter is correct since it might omit input scripts. Inputs scripts are\n> not part of the downloaded block, but are in some blocks before that.\n> Checking those are out of reach for lightweight client with tools given by\n> the current BIP.\n>\n> A remedy here would be an other filter chain on created and spent\n> outpoints as is implemented currently by Murmel. The outpoint filter chain\n> must offer a match for every spent output of the block with the divergent\n> filter, otherwise the interrogated server is lying since a PoW secured\n> block can not spend coins out of nowhere. Doing this check would already\n> force the client to download the outpoint filter history up-to the point of\n> divergence. Then the client would have to download and PoW check every\n> block that shows a match in outpoints until it figures that one of the\n> spent outputs has a script that was not in the server\u2019s filter, in which\n> case the server is lying. If everything checks out then the previous\n> assumption on filter history was incorrect and should be replaced by the\n> history offered by the interrogated server.\n>\n> As you see the interrogation works with this added filter but is highly\n> ineffective. A really light client should not be forced to download lots of\n> blocks just to uncover a lying filter server. This would actually be an\n> easy DoS on light BIP157 clients.\n>\n> A better solution is a change to BIP158 such that the only filter contains\n> created scripts and spent outpoints. It appears to me that this would serve\n> well both wallets and interrogation of filter servers well:\n>\n> Wallets would recognize payments to their addresses by the filter as\n> output scripts are included, spends from the wallet would be recognized as\n> a wallet already knows outpoints of its previously received coins, so it\n> can query the filters for them.\n>\n> Interrogation of a filter server also simplifies, since the filter of the\n> block can be checked entirely against the contents of the same block. The\n> decision on filter correctness does not require more bandwith then download\n> of a block at the mismatching checkpoint. The client could only be forced\n> at max. to download 1/1000 th of the blockchain in addition to the filter\n> header history.\n>\n> Therefore I suggest to change BIP158 to have a base filter, defined as:\n>\n> A basic filter MUST contain exactly the following items for each\n> transaction in a block:\n>         \u2022 Spent outpoints\n>         \u2022 The scriptPubKey of each output, aside from all OP_RETURN output\n> scripts.\n>\n> Tamas Blummer\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190204/70fc17ef/attachment.html>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2019-02-04T20:59:44",
                "message_text_only": "I participated in that discussion in 2018, but have not had the insight gathered by now though writing both client and server implementation of BIP157/158\n\nPieter Wuille considered the design choice I am now suggesting here as alternative (a) in: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016064.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016064.html>\nIn his evaluation he recognized that a filter having spent output and output scripts would allow decision on filter correctness by knowing the block only.\nHe did not evaluate the usefulness in the context of checkpoints, which I think are an important shortcut here.\n\nYes, a filter that is collecting input and output scripts is shorter if script re-use is frequent, but I showed back in 2018 in the same thread that this saving is not that significant in recent history as address reuse is no longer that frequent.\n\nA filter on spent outpoint is just as useful for wallets as is one on spent script, since they naturally scan the blockchain forward and thereby learn about their coins by the output script before they need to check spends of those outpoints.\n\nIt seems to me that implementing an interrogation by evtl. downloading blocks at checkpoints is much simpler than following multiple possible filter paths.\n\nA spent outpoint filter allows us to decide on coin availability based on immutable store, without updated and eventually rolled back UTXO store. The availability could be decided by following the filter path to current tip to genesis and\ncheck is the outpoint was spent earlier. False positives can be sorted out with a block download. Murmel implements this if running in server mode, where blocks are already there.\n\nTherefore I ask for a BIP change based on better insight gained through implementation.\n\nTamas Blummer\n\n> On Feb 4, 2019, at 21:18, Jim Posen <jim.posen at gmail.com> wrote:\n> \n> Please see the thread \"BIP 158 Flexibility and Filter Size\" from 2018 regarding the decision to remove outpoints from the filter [1].\n> \n> Thanks for bringing this up though, because more discussion is needed on the client protocol given that clients cannot reliably determine the integrity of a block filter in a bandwidth-efficient manner (due to the inclusion of input scripts).\n> \n> I see three possibilities:\n> 1) Introduce a new P2P message to retrieve all prev-outputs for a given block (essentially the undo data in Core), and verify the scripts against the block by executing them. While this permits some forms of input script malleability (and thus cannot discriminate between all valid and invalid filters), it restricts what an attacker can do. This was proposed by Laolu AFAIK, and I believe this is how btcd is proceeding.\n> 2) Clients track multiple possible filter header chains and essentially consider the union of their matches. So if any filter received for a particular block header matches, the client downloads the block. The client can ban a peer if they 1) ever return a filter omitting some data that is observed in the downloaded block, 2) repeatedly serve filters that trigger false positive block downloads where such a number of false positives is statistically unlikely, or 3) repeatedly serves filters that are significantly larger than the expected size (essentially padding the actual filters with garbage to waste bandwidth). I have not done the analysis yet, but we should be able to come up with some fairly simple banning heuristics using Chernoff bounds. The main downside is that the client logic to track multiple possible filter chains and filters per block is more complex and bandwidth increases if connected to a malicious server. I first heard about this idea from David Harding.\n> 3) Rush straight to committing the filters into the chain (via witness reserved value or coinbase OP_RETURN) and give up on the pre-softfork BIP 157 P2P mode.\n> \n> I'm in favor of option #2 despite the downsides since it requires the smallest number of changes and is supported by the BIP 157 P2P protocol as currently written. (Though the recommended client protocol in the BIP needs to be updated to account for this). Another benefit of it is that it removes some synchronicity assumptions where a peer with the correct filters keeps timing out and is assumed to be dishonest, while the dishonest peer is assumed to be OK because it is responsive.\n> \n> If anyone has other ideas, I'd love to hear them.\n> \n> -jimpo\n> \n> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html>\n> \n> \n> \n> On Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> TLDR: a change to BIP158 would allow decision on which filter chain is correct at lower bandwith use\n> \n> Assume there is a BIP157 client that learned a filter header chain earlier and is now offered an alternate reality by a newly connected BIP157 server.\n> \n> The client notices the alternate reality by routinely asking for filter chain checkpoints after connecting to a new BIP157 server. A divergence at a checkpoint means that the server disagrees the client's history at or before the first diverging checkpoint. The client would then request the filter headers between the last matching and first divergent checkpoint, and quickly figure which block\u2019s filter is the first that does not match previous assumption, and request that filter from the server.\n> \n> The client downloads the corresponding block, checks that its header fits the PoW secured best header chain, re-calculates merkle root of its transaction list to know that it is complete and queries the filter to see if every output script of every transaction is contained in there, if not the server is lying, the case is closed, the server disconnected.\n> \n> Having all output scripts in the filter does not however guarantee that the filter is correct since it might omit input scripts. Inputs scripts are not part of the downloaded block, but are in some blocks before that. Checking those are out of reach for lightweight client with tools given by the current BIP.\n> \n> A remedy here would be an other filter chain on created and spent outpoints as is implemented currently by Murmel. The outpoint filter chain must offer a match for every spent output of the block with the divergent filter, otherwise the interrogated server is lying since a PoW secured block can not spend coins out of nowhere. Doing this check would already force the client to download the outpoint filter history up-to the point of divergence. Then the client would have to download and PoW check every block that shows a match in outpoints until it figures that one of the spent outputs has a script that was not in the server\u2019s filter, in which case the server is lying. If everything checks out then the previous assumption on filter history was incorrect and should be replaced by the history offered by the interrogated server.\n> \n> As you see the interrogation works with this added filter but is highly ineffective. A really light client should not be forced to download lots of blocks just to uncover a lying filter server. This would actually be an easy DoS on light BIP157 clients.\n> \n> A better solution is a change to BIP158 such that the only filter contains created scripts and spent outpoints. It appears to me that this would serve well both wallets and interrogation of filter servers well:\n> \n> Wallets would recognize payments to their addresses by the filter as output scripts are included, spends from the wallet would be recognized as a wallet already knows outpoints of its previously received coins, so it can query the filters for them.\n> \n> Interrogation of a filter server also simplifies, since the filter of the block can be checked entirely against the contents of the same block. The decision on filter correctness does not require more bandwith then download of a block at the mismatching checkpoint. The client could only be forced at max. to download 1/1000 th of the blockchain in addition to the filter header history.\n> \n> Therefore I suggest to change BIP158 to have a base filter, defined as:\n> \n> A basic filter MUST contain exactly the following items for each transaction in a block:\n>         \u2022 Spent outpoints\n>         \u2022 The scriptPubKey of each output, aside from all OP_RETURN output scripts.\n> \n> Tamas Blummer\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190204/4d87cc77/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 488 bytes\nDesc: Message signed with OpenPGP\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190204/4d87cc77/attachment-0001.sig>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2019-02-05T01:42:57",
                "message_text_only": "Hi Tamas,\n\nThis is how the filter worked before the switch over to optimize for a\nfilter containing the minimal items needed for a regular wallet to function.\nWhen this was proposed, I had already implemented the entire proposal from\nwallet to full-node. At that point, we all more or less decided that the\nspace savings (along with intra-block compression) were worthwhile, we\nweren't cutting off any anticipated application level use cases (at that\npoint we had already comprehensively integrated both filters into lnd), and\nthat once committed the security loss would disappear.\n\nI think it's too late into the current deployment of the BIPs to change\nthings around yet again. Instead, the BIP already has measures in place for\nadding _new_ filter types in the future. This along with a few other filter\ntypes may be worthwhile additions as new filter types.\n\n-- Laolu\n\nOn Mon, Feb 4, 2019 at 12:59 PM Tamas Blummer <tamas.blummer at gmail.com>\nwrote:\n\n> I participated in that discussion in 2018, but have not had the insight\n> gathered by now though writing both client and server implementation of\n> BIP157/158\n>\n> Pieter Wuille considered the design choice I am now suggesting here as\n> alternative (a) in:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016064.html\n> In his evaluation he recognized that a filter having spent output and\n> output scripts would allow decision on filter correctness by knowing the\n> block only.\n> He did not evaluate the usefulness in the context of checkpoints, which I\n> think are an important shortcut here.\n>\n> Yes, a filter that is collecting input and output scripts is shorter if\n> script re-use is frequent, but I showed back in 2018 in the same thread\n> that this saving is not that significant in recent history as address reuse\n> is no longer that frequent.\n>\n> A filter on spent outpoint is just as useful for wallets as is one on\n> spent script, since they naturally scan the blockchain forward and thereby\n> learn about their coins by the output script before they need to check\n> spends of those outpoints.\n>\n> It seems to me that implementing an interrogation by evtl. downloading\n> blocks at checkpoints is much simpler than following multiple possible\n> filter paths.\n>\n> A spent outpoint filter allows us to decide on coin availability based on\n> immutable store, without updated and eventually rolled back UTXO store. The\n> availability could be decided by following the filter path to current tip\n> to genesis and\n> check is the outpoint was spent earlier. False positives can be sorted out\n> with a block download. Murmel implements this if running in server mode,\n> where blocks are already there.\n>\n> Therefore I ask for a BIP change based on better insight gained through\n> implementation.\n>\n> Tamas Blummer\n>\n> On Feb 4, 2019, at 21:18, Jim Posen <jim.posen at gmail.com> wrote:\n>\n> Please see the thread \"BIP 158 Flexibility and Filter Size\" from 2018\n> regarding the decision to remove outpoints from the filter [1].\n>\n> Thanks for bringing this up though, because more discussion is needed on\n> the client protocol given that clients cannot reliably determine the\n> integrity of a block filter in a bandwidth-efficient manner (due to the\n> inclusion of input scripts).\n>\n> I see three possibilities:\n> 1) Introduce a new P2P message to retrieve all prev-outputs for a given\n> block (essentially the undo data in Core), and verify the scripts against\n> the block by executing them. While this permits some forms of input script\n> malleability (and thus cannot discriminate between all valid and invalid\n> filters), it restricts what an attacker can do. This was proposed by Laolu\n> AFAIK, and I believe this is how btcd is proceeding.\n> 2) Clients track multiple possible filter header chains and essentially\n> consider the union of their matches. So if any filter received for a\n> particular block header matches, the client downloads the block. The client\n> can ban a peer if they 1) ever return a filter omitting some data that is\n> observed in the downloaded block, 2) repeatedly serve filters that trigger\n> false positive block downloads where such a number of false positives is\n> statistically unlikely, or 3) repeatedly serves filters that are\n> significantly larger than the expected size (essentially padding the actual\n> filters with garbage to waste bandwidth). I have not done the analysis yet,\n> but we should be able to come up with some fairly simple banning heuristics\n> using Chernoff bounds. The main downside is that the client logic to track\n> multiple possible filter chains and filters per block is more complex and\n> bandwidth increases if connected to a malicious server. I first heard about\n> this idea from David Harding.\n> 3) Rush straight to committing the filters into the chain (via witness\n> reserved value or coinbase OP_RETURN) and give up on the pre-softfork BIP\n> 157 P2P mode.\n>\n> I'm in favor of option #2 despite the downsides since it requires the\n> smallest number of changes and is supported by the BIP 157 P2P protocol as\n> currently written. (Though the recommended client protocol in the BIP needs\n> to be updated to account for this). Another benefit of it is that it\n> removes some synchronicity assumptions where a peer with the correct\n> filters keeps timing out and is assumed to be dishonest, while the\n> dishonest peer is assumed to be OK because it is responsive.\n>\n> If anyone has other ideas, I'd love to hear them.\n>\n> -jimpo\n>\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html\n>\n>\n>\n> On Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> TLDR: a change to BIP158 would allow decision on which filter chain is\n>> correct at lower bandwith use\n>>\n>> Assume there is a BIP157 client that learned a filter header chain\n>> earlier and is now offered an alternate reality by a newly connected BIP157\n>> server.\n>>\n>> The client notices the alternate reality by routinely asking for filter\n>> chain checkpoints after connecting to a new BIP157 server. A divergence at\n>> a checkpoint means that the server disagrees the client's history at or\n>> before the first diverging checkpoint. The client would then request the\n>> filter headers between the last matching and first divergent checkpoint,\n>> and quickly figure which block\u2019s filter is the first that does not match\n>> previous assumption, and request that filter from the server.\n>>\n>> The client downloads the corresponding block, checks that its header fits\n>> the PoW secured best header chain, re-calculates merkle root of its\n>> transaction list to know that it is complete and queries the filter to see\n>> if every output script of every transaction is contained in there, if not\n>> the server is lying, the case is closed, the server disconnected.\n>>\n>> Having all output scripts in the filter does not however guarantee that\n>> the filter is correct since it might omit input scripts. Inputs scripts are\n>> not part of the downloaded block, but are in some blocks before that.\n>> Checking those are out of reach for lightweight client with tools given by\n>> the current BIP.\n>>\n>> A remedy here would be an other filter chain on created and spent\n>> outpoints as is implemented currently by Murmel. The outpoint filter chain\n>> must offer a match for every spent output of the block with the divergent\n>> filter, otherwise the interrogated server is lying since a PoW secured\n>> block can not spend coins out of nowhere. Doing this check would already\n>> force the client to download the outpoint filter history up-to the point of\n>> divergence. Then the client would have to download and PoW check every\n>> block that shows a match in outpoints until it figures that one of the\n>> spent outputs has a script that was not in the server\u2019s filter, in which\n>> case the server is lying. If everything checks out then the previous\n>> assumption on filter history was incorrect and should be replaced by the\n>> history offered by the interrogated server.\n>>\n>> As you see the interrogation works with this added filter but is highly\n>> ineffective. A really light client should not be forced to download lots of\n>> blocks just to uncover a lying filter server. This would actually be an\n>> easy DoS on light BIP157 clients.\n>>\n>> A better solution is a change to BIP158 such that the only filter\n>> contains created scripts and spent outpoints. It appears to me that this\n>> would serve well both wallets and interrogation of filter servers well:\n>>\n>> Wallets would recognize payments to their addresses by the filter as\n>> output scripts are included, spends from the wallet would be recognized as\n>> a wallet already knows outpoints of its previously received coins, so it\n>> can query the filters for them.\n>>\n>> Interrogation of a filter server also simplifies, since the filter of the\n>> block can be checked entirely against the contents of the same block. The\n>> decision on filter correctness does not require more bandwith then download\n>> of a block at the mismatching checkpoint. The client could only be forced\n>> at max. to download 1/1000 th of the blockchain in addition to the filter\n>> header history.\n>>\n>> Therefore I suggest to change BIP158 to have a base filter, defined as:\n>>\n>> A basic filter MUST contain exactly the following items for each\n>> transaction in a block:\n>>         \u2022 Spent outpoints\n>>         \u2022 The scriptPubKey of each output, aside from all OP_RETURN\n>> output scripts.\n>>\n>> Tamas Blummer\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190204/84289ae9/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2019-02-05T12:21:45",
                "message_text_only": "On 2/4/19 8:18 PM, Jim Posen via bitcoin-dev wrote:\n- snip -\n > 1) Introduce a new P2P message to retrieve all prev-outputs for a given\n > block (essentially the undo data in Core), and verify the scripts\n > against the block by executing them. While this permits some forms of\n > input script malleability (and thus cannot discriminate between all\n > valid and invalid filters), it restricts what an attacker can do. This\n > was proposed by Laolu AFAIK, and I believe this is how btcd is \nproceeding.\n\nI'm somewhat confused by this - how does the undo data help you without \nseeing the full (mistate compressed) transaction? In (the realistic) \nthread model where an attacker is trying to blind you from some output, \nthey can simply give you \"undo data\" where scriptPubKeys are OP_TRUE \ninstead of the real script and you'd be none the wiser.\n\nOn 2/5/19 1:42 AM, Olaoluwa Osuntokun via bitcoin-dev wrote:\n- snip -\n> I think it's too late into the current deployment of the BIPs to change\n> things around yet again. Instead, the BIP already has measures in place for\n> adding _new_ filter types in the future. This along with a few other filter\n> types may be worthwhile additions as new filter types.\n- snip -\n\nHuh? I don't think we should seriously consider \nonly-one-codebase-has-deployed-anything-with-very-limited-in-the-wild-use \nas \"too late into the current deployment\"?\n\nMatt"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2019-02-06T00:05:57",
                "message_text_only": "Hi Matt,\n\n> In (the realistic) thread model where an attacker is trying to blind you\n> from some output, they can simply give you \"undo data\" where scriptPubKeys\n> are OP_TRUE instead of the real script and you'd be none the wiser.\n\nIt depends on the input. If I'm trying to verify an input that's P2WSH,\nsince the witness script is included in the witness (the last element), I\ncan easily verify that the pkScript given is the proper witness program.\n\n> Huh? I don't think we should seriously consider\n> only-one-codebase-has-deployed-anything-with-very-limited-in-the-wild-use\nas\n> \"too late into the current deployment\"?\n\nI'd wager that most developers reading this email right now are familiar\nwith neutrino as a project. Many even routinely use \"neutrino\" to refer to\nBIP 157+158. There are several projects in the wild that have already\ndeployed applications built on lnd+neutrino live on mainnet. lnd+neutrino is\nalso the only project (as far as I'm aware) that has fully integrated the\np2p BIP 157+158 into a wallet, and also uses the filters for higher level\napplications.\n\nI'm no stranger to this argument, as I made the exact same one 7 months ago\nwhen the change was originally discussed. Since then I realized that using\ninput scripts can be even _more_ flexible as light clients can use them as\nset up or triggers for multi-party protocols such as atomic swaps. Using\nscripts also allows for faster rescans if one knows all their keys ahead of\ntime, as the checks can be parallelized. Additionally, the current filter\nalso lends better to an eventual commitment as you literally can't remove\nanything from it, and still have it be useful for the traditional wallet use\ncase.\n\nAs I mentioned in my last email, this can be added as an additional filter\ntype, leaving it up the full node implementations that have deployed the\nbase protocol to integrate it or not.\n\n-- Laolu\n\n\nOn Tue, Feb 5, 2019 at 4:21 AM Matt Corallo <lf-lists at mattcorallo.com>\nwrote:\n\n>\n> On 2/4/19 8:18 PM, Jim Posen via bitcoin-dev wrote:\n> - snip -\n>  > 1) Introduce a new P2P message to retrieve all prev-outputs for a given\n>  > block (essentially the undo data in Core), and verify the scripts\n>  > against the block by executing them. While this permits some forms of\n>  > input script malleability (and thus cannot discriminate between all\n>  > valid and invalid filters), it restricts what an attacker can do. This\n>  > was proposed by Laolu AFAIK, and I believe this is how btcd is\n> proceeding.\n>\n> I'm somewhat confused by this - how does the undo data help you without\n> seeing the full (mistate compressed) transaction? In (the realistic)\n> thread model where an attacker is trying to blind you from some output,\n> they can simply give you \"undo data\" where scriptPubKeys are OP_TRUE\n> instead of the real script and you'd be none the wiser.\n>\n> On 2/5/19 1:42 AM, Olaoluwa Osuntokun via bitcoin-dev wrote:\n> - snip -\n> > I think it's too late into the current deployment of the BIPs to change\n> > things around yet again. Instead, the BIP already has measures in place\n> for\n> > adding _new_ filter types in the future. This along with a few other\n> filter\n> > types may be worthwhile additions as new filter types.\n> - snip -\n>\n> Huh? I don't think we should seriously consider\n> only-one-codebase-has-deployed-anything-with-very-limited-in-the-wild-use\n> as \"too late into the current deployment\"?\n>\n> Matt\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190205/0e7cb8db/attachment-0001.html>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2019-02-05T20:10:09",
                "message_text_only": "Hi Laolu,\n\nThe only advantage I see in the current design choice is filter size, but even that is less\nimpressive in recent history and going forward, as address re-use is much less frequent nowadays\nthan it was Bitcoin\u2019s early days.\n\nI calculated total filter sizes since block 500,000:\n\ninput script + output script (current BIP): 1.09 GB \nspent outpoint + output script: 1.26 GB\n\nBoth filters are equally useful for a wallet to discover relevant transactions, but the current design\nchoice seriously limits, practically disables a light client, to prove that the filter is correct. \n\nClear advantages of moving to spent outpoint + output script filter:\n\n1. Filter correctness can be proven by downloading the block in question only.\n2. Calculation of the filter on server side does not need UTXO.\n3. Spent outpoints in the filter enable light clients to do further probabilistic checks and even more if committed.\n\nThe current design choice offers lower security than now attainable. This certainly improves with \na commitment, but that is not even on the roadmap yet, or is it?\n\nShould a filter be committed that contains spent outpoints, then such filter would be even more useful:\nA client could decide on availability of spent coins of a transaction without maintaining the UTXO set, by \nchecking the filters if the coin was spent after its origin proven in an SPV manner, evtl. eliminating false positives \nwith a block download. This would be slower than having UTXO but require only immutable store, no unwinds and \nonly download of a few blocks.\n\nSince Bitcoin Core is not yet serving any filters, I do not think this discussion is too late.\n\nTamas Blummer\n\n\n> On Feb 5, 2019, at 02:42, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n> \n> Hi Tamas, \n> \n> This is how the filter worked before the switch over to optimize for a\n> filter containing the minimal items needed for a regular wallet to function.\n> When this was proposed, I had already implemented the entire proposal from\n> wallet to full-node. At that point, we all more or less decided that the\n> space savings (along with intra-block compression) were worthwhile, we\n> weren't cutting off any anticipated application level use cases (at that\n> point we had already comprehensively integrated both filters into lnd), and\n> that once committed the security loss would disappear.\n> \n> I think it's too late into the current deployment of the BIPs to change\n> things around yet again. Instead, the BIP already has measures in place for\n> adding _new_ filter types in the future. This along with a few other filter\n> types may be worthwhile additions as new filter types.\n> \n> -- Laolu\n> \n> On Mon, Feb 4, 2019 at 12:59 PM Tamas Blummer <tamas.blummer at gmail.com> wrote:\n> I participated in that discussion in 2018, but have not had the insight gathered by now though writing both client and server implementation of BIP157/158\n> \n> Pieter Wuille considered the design choice I am now suggesting here as alternative (a) in: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016064.html\n> In his evaluation he recognized that a filter having spent output and output scripts would allow decision on filter correctness by knowing the block only.\n> He did not evaluate the usefulness in the context of checkpoints, which I think are an important shortcut here.\n> \n> Yes, a filter that is collecting input and output scripts is shorter if script re-use is frequent, but I showed back in 2018 in the same thread that this saving is not that significant in recent history as address reuse is no longer that frequent.\n> \n> A filter on spent outpoint is just as useful for wallets as is one on spent script, since they naturally scan the blockchain forward and thereby learn about their coins by the output script before they need to check spends of those outpoints.\n> \n> It seems to me that implementing an interrogation by evtl. downloading blocks at checkpoints is much simpler than following multiple possible filter paths.\n> \n> A spent outpoint filter allows us to decide on coin availability based on immutable store, without updated and eventually rolled back UTXO store. The availability could be decided by following the filter path to current tip to genesis and\n> check is the outpoint was spent earlier. False positives can be sorted out with a block download. Murmel implements this if running in server mode, where blocks are already there.\n> \n> Therefore I ask for a BIP change based on better insight gained through implementation.\n> \n> Tamas Blummer\n> \n>> On Feb 4, 2019, at 21:18, Jim Posen <jim.posen at gmail.com> wrote:\n>> \n>> Please see the thread \"BIP 158 Flexibility and Filter Size\" from 2018 regarding the decision to remove outpoints from the filter [1].\n>> \n>> Thanks for bringing this up though, because more discussion is needed on the client protocol given that clients cannot reliably determine the integrity of a block filter in a bandwidth-efficient manner (due to the inclusion of input scripts).\n>> \n>> I see three possibilities:\n>> 1) Introduce a new P2P message to retrieve all prev-outputs for a given block (essentially the undo data in Core), and verify the scripts against the block by executing them. While this permits some forms of input script malleability (and thus cannot discriminate between all valid and invalid filters), it restricts what an attacker can do. This was proposed by Laolu AFAIK, and I believe this is how btcd is proceeding.\n>> 2) Clients track multiple possible filter header chains and essentially consider the union of their matches. So if any filter received for a particular block header matches, the client downloads the block. The client can ban a peer if they 1) ever return a filter omitting some data that is observed in the downloaded block, 2) repeatedly serve filters that trigger false positive block downloads where such a number of false positives is statistically unlikely, or 3) repeatedly serves filters that are significantly larger than the expected size (essentially padding the actual filters with garbage to waste bandwidth). I have not done the analysis yet, but we should be able to come up with some fairly simple banning heuristics using Chernoff bounds. The main downside is that the client logic to track multiple possible filter chains and filters per block is more complex and bandwidth increases if connected to a malicious server. I first heard about this idea from David Harding.\n>> 3) Rush straight to committing the filters into the chain (via witness reserved value or coinbase OP_RETURN) and give up on the pre-softfork BIP 157 P2P mode.\n>> \n>> I'm in favor of option #2 despite the downsides since it requires the smallest number of changes and is supported by the BIP 157 P2P protocol as currently written. (Though the recommended client protocol in the BIP needs to be updated to account for this). Another benefit of it is that it removes some synchronicity assumptions where a peer with the correct filters keeps timing out and is assumed to be dishonest, while the dishonest peer is assumed to be OK because it is responsive.\n>> \n>> If anyone has other ideas, I'd love to hear them.\n>> \n>> -jimpo\n>> \n>> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html\n>> \n>> \n>> \n>> On Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> TLDR: a change to BIP158 would allow decision on which filter chain is correct at lower bandwith use\n>> \n>> Assume there is a BIP157 client that learned a filter header chain earlier and is now offered an alternate reality by a newly connected BIP157 server.\n>> \n>> The client notices the alternate reality by routinely asking for filter chain checkpoints after connecting to a new BIP157 server. A divergence at a checkpoint means that the server disagrees the client's history at or before the first diverging checkpoint. The client would then request the filter headers between the last matching and first divergent checkpoint, and quickly figure which block\u2019s filter is the first that does not match previous assumption, and request that filter from the server.\n>> \n>> The client downloads the corresponding block, checks that its header fits the PoW secured best header chain, re-calculates merkle root of its transaction list to know that it is complete and queries the filter to see if every output script of every transaction is contained in there, if not the server is lying, the case is closed, the server disconnected.\n>> \n>> Having all output scripts in the filter does not however guarantee that the filter is correct since it might omit input scripts. Inputs scripts are not part of the downloaded block, but are in some blocks before that. Checking those are out of reach for lightweight client with tools given by the current BIP.\n>> \n>> A remedy here would be an other filter chain on created and spent outpoints as is implemented currently by Murmel. The outpoint filter chain must offer a match for every spent output of the block with the divergent filter, otherwise the interrogated server is lying since a PoW secured block can not spend coins out of nowhere. Doing this check would already force the client to download the outpoint filter history up-to the point of divergence. Then the client would have to download and PoW check every block that shows a match in outpoints until it figures that one of the spent outputs has a script that was not in the server\u2019s filter, in which case the server is lying. If everything checks out then the previous assumption on filter history was incorrect and should be replaced by the history offered by the interrogated server. \n>> \n>> As you see the interrogation works with this added filter but is highly ineffective. A really light client should not be forced to download lots of blocks just to uncover a lying filter server. This would actually be an easy DoS on light BIP157 clients.\n>> \n>> A better solution is a change to BIP158 such that the only filter contains created scripts and spent outpoints. It appears to me that this would serve well both wallets and interrogation of filter servers well:\n>> \n>> Wallets would recognize payments to their addresses by the filter as output scripts are included, spends from the wallet would be recognized as a wallet already knows outpoints of its previously received coins, so it can query the filters for them.\n>> \n>> Interrogation of a filter server also simplifies, since the filter of the block can be checked entirely against the contents of the same block. The decision on filter correctness does not require more bandwith then download of a block at the mismatching checkpoint. The client could only be forced at max. to download 1/1000 th of the blockchain in addition to the filter header history.\n>> \n>> Therefore I suggest to change BIP158 to have a base filter, defined as:\n>> \n>> A basic filter MUST contain exactly the following items for each transaction in a block:\n>>         \u2022 Spent outpoints\n>>         \u2022 The scriptPubKey of each output, aside from all OP_RETURN output scripts.\n>> \n>> Tamas Blummer\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Olaoluwa Osuntokun",
                "date": "2019-02-06T00:17:05",
                "message_text_only": "Hi Tamas,\n\n> The only advantage I see in the current design choice is filter size, but\n> even that is less impressive in recent history and going forward, as\naddress\n> re-use is much less frequent nowadays than it was Bitcoin\u2019s early days.\n\nGains aren't only had with address re-use, it's also the case that if an\ninput is spent in the same block as it was created, then only a single items\nis inserted into the filter. Filters spanning across several blocks would\nalso see savings due to the usage of input scripts.\n\nAnother advantage of using input scripts is that it allows rescans where all\nkeys are known ahead of time to proceed in parallel, which can serve to\ngreatly speed up rescans in bitcoind. Additionally, it allows light clients\nto participate in protocols like atomic swaps using the input scripts as\ntriggers for state transitions. If outpoints were used, then the party that\ninitiated the swap would need to send the cooperating party all possible\ntxid's that may be generated due to fee bumps (RBF or sighash single\ntricks). Using the script, the light client simply waits for it to be\nrevealed in a block (P2WSH) and then it can carry on the protocol.\n\n> Clear advantages of moving to spent outpoint + output script filter:\n\n> 1. Filter correctness can be proven by downloading the block in question\nonly.\n\nYep, as is they can verify half the filter. With auxiliary data, they can\nverify the entire thing. Once committed, they don't need to verify at all.\nWe're repeating a discussion that played out 7 months ago with no new\ninformation or context.\n\n> 2. Calculation of the filter on server side does not need UTXO.\n\nThis is incorrect. Filter calculation can use the spentness journal (or undo\nblocks) that many full node implementations utilize.\n\n> This certainly improves with a commitment, but that is not even on the\n> roadmap yet, or is it?\n\nI don't really know of any sort of roadmaps in Bitcoin development. However,\nI think there's relatively strong support to adding a commitment, once the\ncurrent protocol gets more usage in the wild, which it already is today on\nmainnet.\n\n> Should a filter be committed that contains spent outpoints, then such\n> filter would be even more useful\n\nIndeed, this can be added as a new filter type, optionally adding created\noutpoints as you referenced in your prior email.\n\n> Since Bitcoin Core is not yet serving any filters, I do not think this\n> discussion is too late.\n\nSee my reply to Matt on the current state of deployment. It's also the case\nthat bitcoind isn't the only full node implementation used in the wild.\nFurther changes would also serve to delay inclusion into bitcoind. The\nindividuals proposing these PRs to bitcoind has participated in this\ndiscussion 7 months ago (along with many of the contributors to this\nproject). Based in this conversation 7 months ago, it's my understanding\nthat all parties are aware of the options and tradeoffs to be had.\n\n-- Laolu\n\n\nOn Tue, Feb 5, 2019 at 12:10 PM Tamas Blummer <tamas.blummer at gmail.com>\nwrote:\n\n> Hi Laolu,\n>\n> The only advantage I see in the current design choice is filter size, but\n> even that is less\n> impressive in recent history and going forward, as address re-use is much\n> less frequent nowadays\n> than it was Bitcoin\u2019s early days.\n>\n> I calculated total filter sizes since block 500,000:\n>\n> input script + output script (current BIP): 1.09 GB\n> spent outpoint + output script: 1.26 GB\n>\n> Both filters are equally useful for a wallet to discover relevant\n> transactions, but the current design\n> choice seriously limits, practically disables a light client, to prove\n> that the filter is correct.\n>\n> Clear advantages of moving to spent outpoint + output script filter:\n>\n> 1. Filter correctness can be proven by downloading the block in question\n> only.\n> 2. Calculation of the filter on server side does not need UTXO.\n> 3. Spent outpoints in the filter enable light clients to do further\n> probabilistic checks and even more if committed.\n>\n> The current design choice offers lower security than now attainable. This\n> certainly improves with\n> a commitment, but that is not even on the roadmap yet, or is it?\n>\n> Should a filter be committed that contains spent outpoints, then such\n> filter would be even more useful:\n> A client could decide on availability of spent coins of a transaction\n> without maintaining the UTXO set, by\n> checking the filters if the coin was spent after its origin proven in an\n> SPV manner, evtl. eliminating false positives\n> with a block download. This would be slower than having UTXO but require\n> only immutable store, no unwinds and\n> only download of a few blocks.\n>\n> Since Bitcoin Core is not yet serving any filters, I do not think this\n> discussion is too late.\n>\n> Tamas Blummer\n>\n>\n> > On Feb 5, 2019, at 02:42, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n> >\n> > Hi Tamas,\n> >\n> > This is how the filter worked before the switch over to optimize for a\n> > filter containing the minimal items needed for a regular wallet to\n> function.\n> > When this was proposed, I had already implemented the entire proposal\n> from\n> > wallet to full-node. At that point, we all more or less decided that the\n> > space savings (along with intra-block compression) were worthwhile, we\n> > weren't cutting off any anticipated application level use cases (at that\n> > point we had already comprehensively integrated both filters into lnd),\n> and\n> > that once committed the security loss would disappear.\n> >\n> > I think it's too late into the current deployment of the BIPs to change\n> > things around yet again. Instead, the BIP already has measures in place\n> for\n> > adding _new_ filter types in the future. This along with a few other\n> filter\n> > types may be worthwhile additions as new filter types.\n> >\n> > -- Laolu\n> >\n> > On Mon, Feb 4, 2019 at 12:59 PM Tamas Blummer <tamas.blummer at gmail.com>\n> wrote:\n> > I participated in that discussion in 2018, but have not had the insight\n> gathered by now though writing both client and server implementation of\n> BIP157/158\n> >\n> > Pieter Wuille considered the design choice I am now suggesting here as\n> alternative (a) in:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016064.html\n> > In his evaluation he recognized that a filter having spent output and\n> output scripts would allow decision on filter correctness by knowing the\n> block only.\n> > He did not evaluate the usefulness in the context of checkpoints, which\n> I think are an important shortcut here.\n> >\n> > Yes, a filter that is collecting input and output scripts is shorter if\n> script re-use is frequent, but I showed back in 2018 in the same thread\n> that this saving is not that significant in recent history as address reuse\n> is no longer that frequent.\n> >\n> > A filter on spent outpoint is just as useful for wallets as is one on\n> spent script, since they naturally scan the blockchain forward and thereby\n> learn about their coins by the output script before they need to check\n> spends of those outpoints.\n> >\n> > It seems to me that implementing an interrogation by evtl. downloading\n> blocks at checkpoints is much simpler than following multiple possible\n> filter paths.\n> >\n> > A spent outpoint filter allows us to decide on coin availability based\n> on immutable store, without updated and eventually rolled back UTXO store.\n> The availability could be decided by following the filter path to current\n> tip to genesis and\n> > check is the outpoint was spent earlier. False positives can be sorted\n> out with a block download. Murmel implements this if running in server\n> mode, where blocks are already there.\n> >\n> > Therefore I ask for a BIP change based on better insight gained through\n> implementation.\n> >\n> > Tamas Blummer\n> >\n> >> On Feb 4, 2019, at 21:18, Jim Posen <jim.posen at gmail.com> wrote:\n> >>\n> >> Please see the thread \"BIP 158 Flexibility and Filter Size\" from 2018\n> regarding the decision to remove outpoints from the filter [1].\n> >>\n> >> Thanks for bringing this up though, because more discussion is needed\n> on the client protocol given that clients cannot reliably determine the\n> integrity of a block filter in a bandwidth-efficient manner (due to the\n> inclusion of input scripts).\n> >>\n> >> I see three possibilities:\n> >> 1) Introduce a new P2P message to retrieve all prev-outputs for a given\n> block (essentially the undo data in Core), and verify the scripts against\n> the block by executing them. While this permits some forms of input script\n> malleability (and thus cannot discriminate between all valid and invalid\n> filters), it restricts what an attacker can do. This was proposed by Laolu\n> AFAIK, and I believe this is how btcd is proceeding.\n> >> 2) Clients track multiple possible filter header chains and essentially\n> consider the union of their matches. So if any filter received for a\n> particular block header matches, the client downloads the block. The client\n> can ban a peer if they 1) ever return a filter omitting some data that is\n> observed in the downloaded block, 2) repeatedly serve filters that trigger\n> false positive block downloads where such a number of false positives is\n> statistically unlikely, or 3) repeatedly serves filters that are\n> significantly larger than the expected size (essentially padding the actual\n> filters with garbage to waste bandwidth). I have not done the analysis yet,\n> but we should be able to come up with some fairly simple banning heuristics\n> using Chernoff bounds. The main downside is that the client logic to track\n> multiple possible filter chains and filters per block is more complex and\n> bandwidth increases if connected to a malicious server. I first heard about\n> this idea from David Harding.\n> >> 3) Rush straight to committing the filters into the chain (via witness\n> reserved value or coinbase OP_RETURN) and give up on the pre-softfork BIP\n> 157 P2P mode.\n> >>\n> >> I'm in favor of option #2 despite the downsides since it requires the\n> smallest number of changes and is supported by the BIP 157 P2P protocol as\n> currently written. (Though the recommended client protocol in the BIP needs\n> to be updated to account for this). Another benefit of it is that it\n> removes some synchronicity assumptions where a peer with the correct\n> filters keeps timing out and is assumed to be dishonest, while the\n> dishonest peer is assumed to be OK because it is responsive.\n> >>\n> >> If anyone has other ideas, I'd love to hear them.\n> >>\n> >> -jimpo\n> >>\n> >> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html\n> >>\n> >>\n> >>\n> >> On Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >> TLDR: a change to BIP158 would allow decision on which filter chain is\n> correct at lower bandwith use\n> >>\n> >> Assume there is a BIP157 client that learned a filter header chain\n> earlier and is now offered an alternate reality by a newly connected BIP157\n> server.\n> >>\n> >> The client notices the alternate reality by routinely asking for filter\n> chain checkpoints after connecting to a new BIP157 server. A divergence at\n> a checkpoint means that the server disagrees the client's history at or\n> before the first diverging checkpoint. The client would then request the\n> filter headers between the last matching and first divergent checkpoint,\n> and quickly figure which block\u2019s filter is the first that does not match\n> previous assumption, and request that filter from the server.\n> >>\n> >> The client downloads the corresponding block, checks that its header\n> fits the PoW secured best header chain, re-calculates merkle root of its\n> transaction list to know that it is complete and queries the filter to see\n> if every output script of every transaction is contained in there, if not\n> the server is lying, the case is closed, the server disconnected.\n> >>\n> >> Having all output scripts in the filter does not however guarantee that\n> the filter is correct since it might omit input scripts. Inputs scripts are\n> not part of the downloaded block, but are in some blocks before that.\n> Checking those are out of reach for lightweight client with tools given by\n> the current BIP.\n> >>\n> >> A remedy here would be an other filter chain on created and spent\n> outpoints as is implemented currently by Murmel. The outpoint filter chain\n> must offer a match for every spent output of the block with the divergent\n> filter, otherwise the interrogated server is lying since a PoW secured\n> block can not spend coins out of nowhere. Doing this check would already\n> force the client to download the outpoint filter history up-to the point of\n> divergence. Then the client would have to download and PoW check every\n> block that shows a match in outpoints until it figures that one of the\n> spent outputs has a script that was not in the server\u2019s filter, in which\n> case the server is lying. If everything checks out then the previous\n> assumption on filter history was incorrect and should be replaced by the\n> history offered by the interrogated server.\n> >>\n> >> As you see the interrogation works with this added filter but is highly\n> ineffective. A really light client should not be forced to download lots of\n> blocks just to uncover a lying filter server. This would actually be an\n> easy DoS on light BIP157 clients.\n> >>\n> >> A better solution is a change to BIP158 such that the only filter\n> contains created scripts and spent outpoints. It appears to me that this\n> would serve well both wallets and interrogation of filter servers well:\n> >>\n> >> Wallets would recognize payments to their addresses by the filter as\n> output scripts are included, spends from the wallet would be recognized as\n> a wallet already knows outpoints of its previously received coins, so it\n> can query the filters for them.\n> >>\n> >> Interrogation of a filter server also simplifies, since the filter of\n> the block can be checked entirely against the contents of the same block.\n> The decision on filter correctness does not require more bandwith then\n> download of a block at the mismatching checkpoint. The client could only be\n> forced at max. to download 1/1000 th of the blockchain in addition to the\n> filter header history.\n> >>\n> >> Therefore I suggest to change BIP158 to have a base filter, defined as:\n> >>\n> >> A basic filter MUST contain exactly the following items for each\n> transaction in a block:\n> >>         \u2022 Spent outpoints\n> >>         \u2022 The scriptPubKey of each output, aside from all OP_RETURN\n> output scripts.\n> >>\n> >> Tamas Blummer\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190205/45f445a3/attachment-0001.html>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2019-02-06T08:09:55",
                "message_text_only": "Hi Laolu,\n\nspace savings come with the rather serious current disadvantage, that a light client is not \nin the position to check the filter. Also the advanced uses you mention are subject to this, for now. \nBuilding more on a shaky fundament does not make it look better.\n\nNow that we have seen advantages of both filters, what keeps us from offering both by Core?\n\nComputing the addional spent-outpoint output-script filter is cheaper than the current one as \nit can be done with the block as only context, it does not need UTXO nor undo blocks no journals or \nwhatever else. I do not see how my statement regarding this was incorrect.\n\nThere is a political issue though, why I favor better provable uncommitted filter:\n\nI am skeptical that commitment of any filter will come into Core soon.\nThe reason of my skepticism is political, not technical.\n\nA committed filter makes light clients much more reliable and attractive, for some taste too much more.\n\nClients that follow PoW are not significant on the current network. Core nodes enforce many more rules,\nsome as important as miners' reward. A committed filter would strengthen light clients \nsignificantly, such that perhabs too many were compelled using them instead of a Core node. \nWould the remaining Core nodes be sufficient to enforce checks not covered? I see how this is a dilemma.\n\nIs this a dilemma because we think black-white? Light(er) clients might implement checks that are more \nthan blind PoW trust even if less than all Core checks. Has the time come to allow for this?\n\nTamas Blummer\n\n\n\n\n> On Feb 6, 2019, at 01:17, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n> \n> Hi Tamas, \n> \n> > The only advantage I see in the current design choice is filter size, but\n> > even that is less impressive in recent history and going forward, as address\n> > re-use is much less frequent nowadays than it was Bitcoin\u2019s early days.\n> \n> Gains aren't only had with address re-use, it's also the case that if an\n> input is spent in the same block as it was created, then only a single items\n> is inserted into the filter. Filters spanning across several blocks would\n> also see savings due to the usage of input scripts.\n> \n> Another advantage of using input scripts is that it allows rescans where all\n> keys are known ahead of time to proceed in parallel, which can serve to\n> greatly speed up rescans in bitcoind. Additionally, it allows light clients\n> to participate in protocols like atomic swaps using the input scripts as\n> triggers for state transitions. If outpoints were used, then the party that\n> initiated the swap would need to send the cooperating party all possible\n> txid's that may be generated due to fee bumps (RBF or sighash single\n> tricks). Using the script, the light client simply waits for it to be\n> revealed in a block (P2WSH) and then it can carry on the protocol.\n> \n> > Clear advantages of moving to spent outpoint + output script filter:\n> \n> > 1. Filter correctness can be proven by downloading the block in question only.\n> \n> Yep, as is they can verify half the filter. With auxiliary data, they can\n> verify the entire thing. Once committed, they don't need to verify at all.\n> We're repeating a discussion that played out 7 months ago with no new\n> information or context.\n> \n> > 2. Calculation of the filter on server side does not need UTXO.\n> \n> This is incorrect. Filter calculation can use the spentness journal (or undo\n> blocks) that many full node implementations utilize.\n> \n> > This certainly improves with a commitment, but that is not even on the\n> > roadmap yet, or is it?\n> \n> I don't really know of any sort of roadmaps in Bitcoin development. However,\n> I think there's relatively strong support to adding a commitment, once the\n> current protocol gets more usage in the wild, which it already is today on\n> mainnet.\n> \n> > Should a filter be committed that contains spent outpoints, then such\n> > filter would be even more useful\n> \n> Indeed, this can be added as a new filter type, optionally adding created\n> outpoints as you referenced in your prior email.\n> \n> > Since Bitcoin Core is not yet serving any filters, I do not think this\n> > discussion is too late.\n> \n> See my reply to Matt on the current state of deployment. It's also the case\n> that bitcoind isn't the only full node implementation used in the wild.\n> Further changes would also serve to delay inclusion into bitcoind. The\n> individuals proposing these PRs to bitcoind has participated in this\n> discussion 7 months ago (along with many of the contributors to this\n> project). Based in this conversation 7 months ago, it's my understanding\n> that all parties are aware of the options and tradeoffs to be had.\n> \n> -- Laolu\n> \n> \n> On Tue, Feb 5, 2019 at 12:10 PM Tamas Blummer <tamas.blummer at gmail.com> wrote:\n> Hi Laolu,\n> \n> The only advantage I see in the current design choice is filter size, but even that is less\n> impressive in recent history and going forward, as address re-use is much less frequent nowadays\n> than it was Bitcoin\u2019s early days.\n> \n> I calculated total filter sizes since block 500,000:\n> \n> input script + output script (current BIP): 1.09 GB \n> spent outpoint + output script: 1.26 GB\n> \n> Both filters are equally useful for a wallet to discover relevant transactions, but the current design\n> choice seriously limits, practically disables a light client, to prove that the filter is correct. \n> \n> Clear advantages of moving to spent outpoint + output script filter:\n> \n> 1. Filter correctness can be proven by downloading the block in question only.\n> 2. Calculation of the filter on server side does not need UTXO.\n> 3. Spent outpoints in the filter enable light clients to do further probabilistic checks and even more if committed.\n> \n> The current design choice offers lower security than now attainable. This certainly improves with \n> a commitment, but that is not even on the roadmap yet, or is it?\n> \n> Should a filter be committed that contains spent outpoints, then such filter would be even more useful:\n> A client could decide on availability of spent coins of a transaction without maintaining the UTXO set, by \n> checking the filters if the coin was spent after its origin proven in an SPV manner, evtl. eliminating false positives \n> with a block download. This would be slower than having UTXO but require only immutable store, no unwinds and \n> only download of a few blocks.\n> \n> Since Bitcoin Core is not yet serving any filters, I do not think this discussion is too late.\n> \n> Tamas Blummer\n> \n> \n> > On Feb 5, 2019, at 02:42, Olaoluwa Osuntokun <laolu32 at gmail.com> wrote:\n> > \n> > Hi Tamas, \n> > \n> > This is how the filter worked before the switch over to optimize for a\n> > filter containing the minimal items needed for a regular wallet to function.\n> > When this was proposed, I had already implemented the entire proposal from\n> > wallet to full-node. At that point, we all more or less decided that the\n> > space savings (along with intra-block compression) were worthwhile, we\n> > weren't cutting off any anticipated application level use cases (at that\n> > point we had already comprehensively integrated both filters into lnd), and\n> > that once committed the security loss would disappear.\n> > \n> > I think it's too late into the current deployment of the BIPs to change\n> > things around yet again. Instead, the BIP already has measures in place for\n> > adding _new_ filter types in the future. This along with a few other filter\n> > types may be worthwhile additions as new filter types.\n> > \n> > -- Laolu\n> > \n> > On Mon, Feb 4, 2019 at 12:59 PM Tamas Blummer <tamas.blummer at gmail.com> wrote:\n> > I participated in that discussion in 2018, but have not had the insight gathered by now though writing both client and server implementation of BIP157/158\n> > \n> > Pieter Wuille considered the design choice I am now suggesting here as alternative (a) in: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016064.html\n> > In his evaluation he recognized that a filter having spent output and output scripts would allow decision on filter correctness by knowing the block only.\n> > He did not evaluate the usefulness in the context of checkpoints, which I think are an important shortcut here.\n> > \n> > Yes, a filter that is collecting input and output scripts is shorter if script re-use is frequent, but I showed back in 2018 in the same thread that this saving is not that significant in recent history as address reuse is no longer that frequent.\n> > \n> > A filter on spent outpoint is just as useful for wallets as is one on spent script, since they naturally scan the blockchain forward and thereby learn about their coins by the output script before they need to check spends of those outpoints.\n> > \n> > It seems to me that implementing an interrogation by evtl. downloading blocks at checkpoints is much simpler than following multiple possible filter paths.\n> > \n> > A spent outpoint filter allows us to decide on coin availability based on immutable store, without updated and eventually rolled back UTXO store. The availability could be decided by following the filter path to current tip to genesis and\n> > check is the outpoint was spent earlier. False positives can be sorted out with a block download. Murmel implements this if running in server mode, where blocks are already there.\n> > \n> > Therefore I ask for a BIP change based on better insight gained through implementation.\n> > \n> > Tamas Blummer\n> > \n> >> On Feb 4, 2019, at 21:18, Jim Posen <jim.posen at gmail.com> wrote:\n> >> \n> >> Please see the thread \"BIP 158 Flexibility and Filter Size\" from 2018 regarding the decision to remove outpoints from the filter [1].\n> >> \n> >> Thanks for bringing this up though, because more discussion is needed on the client protocol given that clients cannot reliably determine the integrity of a block filter in a bandwidth-efficient manner (due to the inclusion of input scripts).\n> >> \n> >> I see three possibilities:\n> >> 1) Introduce a new P2P message to retrieve all prev-outputs for a given block (essentially the undo data in Core), and verify the scripts against the block by executing them. While this permits some forms of input script malleability (and thus cannot discriminate between all valid and invalid filters), it restricts what an attacker can do. This was proposed by Laolu AFAIK, and I believe this is how btcd is proceeding.\n> >> 2) Clients track multiple possible filter header chains and essentially consider the union of their matches. So if any filter received for a particular block header matches, the client downloads the block. The client can ban a peer if they 1) ever return a filter omitting some data that is observed in the downloaded block, 2) repeatedly serve filters that trigger false positive block downloads where such a number of false positives is statistically unlikely, or 3) repeatedly serves filters that are significantly larger than the expected size (essentially padding the actual filters with garbage to waste bandwidth). I have not done the analysis yet, but we should be able to come up with some fairly simple banning heuristics using Chernoff bounds. The main downside is that the client logic to track multiple possible filter chains and filters per block is more complex and bandwidth increases if connected to a malicious server. I first heard about this idea from David Harding.\n> >> 3) Rush straight to committing the filters into the chain (via witness reserved value or coinbase OP_RETURN) and give up on the pre-softfork BIP 157 P2P mode.\n> >> \n> >> I'm in favor of option #2 despite the downsides since it requires the smallest number of changes and is supported by the BIP 157 P2P protocol as currently written. (Though the recommended client protocol in the BIP needs to be updated to account for this). Another benefit of it is that it removes some synchronicity assumptions where a peer with the correct filters keeps timing out and is assumed to be dishonest, while the dishonest peer is assumed to be OK because it is responsive.\n> >> \n> >> If anyone has other ideas, I'd love to hear them.\n> >> \n> >> -jimpo\n> >> \n> >> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016057.html\n> >> \n> >> \n> >> \n> >> On Mon, Feb 4, 2019 at 10:53 AM Tamas Blummer via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >> TLDR: a change to BIP158 would allow decision on which filter chain is correct at lower bandwith use\n> >> \n> >> Assume there is a BIP157 client that learned a filter header chain earlier and is now offered an alternate reality by a newly connected BIP157 server.\n> >> \n> >> The client notices the alternate reality by routinely asking for filter chain checkpoints after connecting to a new BIP157 server. A divergence at a checkpoint means that the server disagrees the client's history at or before the first diverging checkpoint. The client would then request the filter headers between the last matching and first divergent checkpoint, and quickly figure which block\u2019s filter is the first that does not match previous assumption, and request that filter from the server.\n> >> \n> >> The client downloads the corresponding block, checks that its header fits the PoW secured best header chain, re-calculates merkle root of its transaction list to know that it is complete and queries the filter to see if every output script of every transaction is contained in there, if not the server is lying, the case is closed, the server disconnected.\n> >> \n> >> Having all output scripts in the filter does not however guarantee that the filter is correct since it might omit input scripts. Inputs scripts are not part of the downloaded block, but are in some blocks before that. Checking those are out of reach for lightweight client with tools given by the current BIP.\n> >> \n> >> A remedy here would be an other filter chain on created and spent outpoints as is implemented currently by Murmel. The outpoint filter chain must offer a match for every spent output of the block with the divergent filter, otherwise the interrogated server is lying since a PoW secured block can not spend coins out of nowhere. Doing this check would already force the client to download the outpoint filter history up-to the point of divergence. Then the client would have to download and PoW check every block that shows a match in outpoints until it figures that one of the spent outputs has a script that was not in the server\u2019s filter, in which case the server is lying. If everything checks out then the previous assumption on filter history was incorrect and should be replaced by the history offered by the interrogated server. \n> >> \n> >> As you see the interrogation works with this added filter but is highly ineffective. A really light client should not be forced to download lots of blocks just to uncover a lying filter server. This would actually be an easy DoS on light BIP157 clients.\n> >> \n> >> A better solution is a change to BIP158 such that the only filter contains created scripts and spent outpoints. It appears to me that this would serve well both wallets and interrogation of filter servers well:\n> >> \n> >> Wallets would recognize payments to their addresses by the filter as output scripts are included, spends from the wallet would be recognized as a wallet already knows outpoints of its previously received coins, so it can query the filters for them.\n> >> \n> >> Interrogation of a filter server also simplifies, since the filter of the block can be checked entirely against the contents of the same block. The decision on filter correctness does not require more bandwith then download of a block at the mismatching checkpoint. The client could only be forced at max. to download 1/1000 th of the blockchain in addition to the filter header history.\n> >> \n> >> Therefore I suggest to change BIP158 to have a base filter, defined as:\n> >> \n> >> A basic filter MUST contain exactly the following items for each transaction in a block:\n> >>         \u2022 Spent outpoints\n> >>         \u2022 The scriptPubKey of each output, aside from all OP_RETURN output scripts.\n> >> \n> >> Tamas Blummer\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> > \n>"
            },
            {
                "author": "Gregory Maxwell",
                "date": "2019-02-06T18:17:11",
                "message_text_only": "On Wed, Feb 6, 2019 at 8:10 AM Tamas Blummer <tamas.blummer at gmail.com> wrote:\n> I am skeptical that commitment of any filter will come into Core soon. [...] A committed filter makes light clients much more reliable and attractive, for some taste too much more.\n\nYou keep repeating this smear. Please stop.\n\nIf you would actually bother reading the threads where this was\ndiscussed previously you will see that there was significant interest\nfrom bitcoin developers to eventually commit an output filter, and a\nsignificant investment of effort in improving the proposal to that\nend.  It is really disheartening to see you continue to repeat your\nnegative assumptions about other people's wishes when you haven't even\ninvested the time required to read their words."
            },
            {
                "author": "Tamas Blummer",
                "date": "2019-02-06T19:48:09",
                "message_text_only": "I do not think this ad hominem attack of you on me was justified.\n\nI wrote code, gathered and shared data now and back in 2018. I showed\nunderstanding of non technical issues. Is there an actual action that\ndefies my observation that a commitment is not yet in sight?\n\nIs there anything technically wrong in what I wrote?\n\nIf not you should stop.\n\nTamas Blummer\n\n\nOn Wed, 6 Feb 2019, 18:17 Gregory Maxwell <greg at xiph.org wrote:\n\n> On Wed, Feb 6, 2019 at 8:10 AM Tamas Blummer <tamas.blummer at gmail.com>\n> wrote:\n> > I am skeptical that commitment of any filter will come into Core soon.\n> [...] A committed filter makes light clients much more reliable and\n> attractive, for some taste too much more.\n>\n> You keep repeating this smear. Please stop.\n>\n> If you would actually bother reading the threads where this was\n> discussed previously you will see that there was significant interest\n> from bitcoin developers to eventually commit an output filter, and a\n> significant investment of effort in improving the proposal to that\n> end.  It is really disheartening to see you continue to repeat your\n> negative assumptions about other people's wishes when you haven't even\n> invested the time required to read their words.\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190206/8a032707/attachment-0001.html>"
            },
            {
                "author": "Tamas Blummer",
                "date": "2019-02-06T21:17:17",
                "message_text_only": "The attack was in your implication that I would assume  ill intent of those\ncontributed to the proposal. That is not my position. I explained why, I\nthink, rolling out a commitment could face opposition. This foreseable\nopposition, that must not come from you makes me prefer a provable\nuncommitted filter for now.\n\nI am myself concerned of the implications if many nodes would blindly\nfollow POW.\n\nI did restart the discussion which I read and participated in at its first\ninstance because implementing the current proposal taught me how\nproblematic as is until not committed and because I have not seen a sign to\nassume commitment was imminent.\n\nThis is not just missing code. AFAIK we do not even have a consensus on how\nany future soft fork would be activated.\n\nWhile trying to build a useful software I have to make assumtions on the\ntimeline of dependencies and in my personal evaluation commitment is not\nyet to build on.\n\nI and others learned in this new discussion new arguments such as that of\natomic swaps by Laolu. If nothing else, this was worth of learning.\n\nIt appears me that it is rather you assuming ill intent on my side, which\nhurts given that I do contribute to the ecosystem since many years and have\nnot ever been caught of hurting the project.\n\nTamas Blummer\n\n\nOn Wed, 6 Feb 2019, 20:16 Gregory Maxwell <gmaxwell at gmail.com wrote:\n\n> On Wed, Feb 6, 2019 at 7:48 PM Tamas Blummer <tamas.blummer at gmail.com>\n> wrote:\n> > I do not think this ad hominem attack of you on me was justified.\n>\n> I apologize if I have offended you, but I am at a loss to find in my\n> words you found to be an attack. Can you help me out?\n>\n> On reread the only thing I'm saying is that you hadn't even read the\n> prior discussion. Am I mistaken?  If so, why did you simply propose\n> reverting prior improvements without addressing the arguments given\n> the first time around or even acknowledging that you were rehashing an\n> old discussion?\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190206/2f3af794/attachment-0001.html>"
            },
            {
                "author": "Pieter Wuille",
                "date": "2019-02-07T20:36:25",
                "message_text_only": "On Thu, 7 Feb 2019 at 12:19, Tamas Blummer via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I did restart the discussion which I read and participated in at its first instance because implementing the current proposal taught me how problematic as is until not committed and because I have not seen a sign to assume commitment was imminent.\n\nHi Tamas,\n\nI think you're confusing the lack of sign of imminent commitment for a\nsign it isn't the end goal. Changes in consensus rules take a while,\nand I think adoption of BIP157 in a limited setting where offered by\ntrusted nodes is necessary before we will see a big push for that.\n\nIn my personal view (and I respect other opinions in this regard),\nBIP157 as a public network-facing service offered by untrusted full\nnodes is fair uninteresting. If the goal wasn't to have it eventually\nas a commitment, I don't think I would be interested in helping\nimproving it. There are certainly heuristics that reduce the risk of\nusing it without, but they come at the cost of software complexity,\nextra bandwidth, and a number of assumptions on the types of scripts\ninvolved in the transactions. I appreciate work in exploring more\npossibilities, but for a BIP157-that-eventually-becomes-a-commitment,\nI think they're a distraction. Unless you feel that changes actually\nbenefit that end goal, I think the current BIP157 filter definition\nshould be kept.\n\nThere is no problem however in optionally supporting other filters,\nwhich make different trade-offs, which are intended to be offered by\n(semi) trusted nodes. Still, for the reasons above I would very much\nlike to keep those discussions separate from the\nto-be-committed-filter.\n\nCheers,\n\n-- \nPieter"
            }
        ],
        "thread_summary": {
            "title": "Interrogating a BIP157 server, BIP158 change proposal",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tamas Blummer",
                "Olaoluwa Osuntokun",
                "Gregory Maxwell",
                "Matt Corallo",
                "Pieter Wuille",
                "Jim Posen"
            ],
            "messages_count": 13,
            "total_messages_chars_count": 83428
        }
    },
    {
        "title": "[bitcoin-dev] Implementing Confidential Transactions in extension blocks",
        "thread_messages": [
            {
                "author": "Kenshiro []",
                "date": "2019-02-08T10:12:06",
                "message_text_only": "Greetings,\n\nWhat do you think about implementing Confidential Transactions in extension blocks? CT transactions go from extension block to extension block passing through normal blocks. It looks the perfect solution:\n\n- Soft fork: old nodes see CT transactions as \"sendtoany\" transactions\n- Safe: if there is a software bug in CT it's impossible to create new coins because the coins move from normal block to normal block as public transactions\n- Legal: Exchanges can use public transactions so regulators can monitor their activity\n- Capacity increase: the CT signature is stored in the extension block, so CT transactions increase the maximum number of transactions per block\n\nRegards\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190208/76fc982c/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2019-02-11T04:29:42",
                "message_text_only": "Good morning Kenshiro,\n\n> - Soft fork: old nodes see CT transactions as \"sendtoany\" transactions\n\nThere is a position that fullnodes must be able to get a view of the UTXO set, and extension blocks (which are invisible to pre-extension-block fullnodes) means that fullnodes no longer have an accurate view of the UTXO set.\nSegWit still provides pre-SegWit fullnodes with a view of the UTXO set, although pre-SegWit fullnodes could be convinced that a particular UTXO is anyone-can-spend even though they are no longer anyone-can-spend.\n\nUnder this point-of-view, then, extension block is \"not\" soft fork.\nIt is \"evil\" soft fork since older nodes are forced to upgrade as their intended functionality becomes impossible.\nIn this point-of-view, it is no better than a hard fork, which at least is very noisy about how older fullnode versions will simply stop working.\n\n> - Safe: if there is a software bug in CT it's impossible to create new coins because the coins move from normal block to normal block as public transactions\n\nI think more relevant here is the issue of a future quantum computing breach of the algorithms used to implement confidentiality.\n\nI believe this is also achievable with a non-extension-block approach by implementing a globally-verified publicly-visible counter of the total amount in all confidential transaction outputs.\nThen it becomes impossible to move from confidential to public transactions with a value more than this counter, thus preventing inflation even if a future QC breach allows confidential transaction value commitments to be opened to any value.\n\n(do note that a non-extension-block approach is a definite hardfork)\n\n> - Capacity increase: the CT signature is stored in the extension block, so CT transactions increase the maximum number of transactions per block\n\nThis is not an unalloyed positive: block size increase, even via extension block, translates to greater network capacity usage globally on all fullnodes.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Kenshiro []",
                "date": "2019-02-11T10:19:15",
                "message_text_only": "Good morning ZmnSCPxj,\n\nThank you for your answer.\n\nThere is a position that fullnodes must be able to get a view of the UTXO set, and extension blocks (which are invisible to pre-extension-block fullnodes) means that fullnodes no longer have an accurate view of the UTXO set.\n\nI think old nodes don't need to know the CT part of the UTXO set. It would be possible to move coins from normal address to CT address and the opposite, it would be written as \"anyone-can-spend\" transactions in the main block so old nodes are fully aware of these transactions. Miners would enforce that \"anyone-can-spend\" transactions are true. The full details of the transactions involving CT would be in the extension block. CT to CT transactions don't need to be written in the main block. Maybe I'm missing some technical detail here but it looks good for me.\n\n\n> - Capacity increase: the CT signature is stored in the extension block, so CT transactions  increase the maximum number of transactions per block\n\nThis is not an unalloyed positive: block size increase, even via extension block, translates to greater network capacity usage globally on all fullnodes.\n\nYes, there is an increase in block size and network usage but I think it would still be possible for people with regular computers to run a full node, an people in developing countries could use light wallets.\n\nRegards\n\n\n________________________________\nFrom: ZmnSCPxj <ZmnSCPxj at protonmail.com>\nSent: Monday, February 11, 2019 5:29\nTo: Kenshiro \\[\\]; Bitcoin Protocol Discussion\nSubject: Re: [bitcoin-dev] Implementing Confidential Transactions in extension blocks\n\nGood morning Kenshiro,\n\n> - Soft fork: old nodes see CT transactions as \"sendtoany\" transactions\n\nThere is a position that fullnodes must be able to get a view of the UTXO set, and extension blocks (which are invisible to pre-extension-block fullnodes) means that fullnodes no longer have an accurate view of the UTXO set.\nSegWit still provides pre-SegWit fullnodes with a view of the UTXO set, although pre-SegWit fullnodes could be convinced that a particular UTXO is anyone-can-spend even though they are no longer anyone-can-spend.\n\nUnder this point-of-view, then, extension block is \"not\" soft fork.\nIt is \"evil\" soft fork since older nodes are forced to upgrade as their intended functionality becomes impossible.\nIn this point-of-view, it is no better than a hard fork, which at least is very noisy about how older fullnode versions will simply stop working.\n\n> - Safe: if there is a software bug in CT it's impossible to create new coins because the coins move from normal block to normal block as public transactions\n\nI think more relevant here is the issue of a future quantum computing breach of the algorithms used to implement confidentiality.\n\nI believe this is also achievable with a non-extension-block approach by implementing a globally-verified publicly-visible counter of the total amount in all confidential transaction outputs.\nThen it becomes impossible to move from confidential to public transactions with a value more than this counter, thus preventing inflation even if a future QC breach allows confidential transaction value commitments to be opened to any value.\n\n(do note that a non-extension-block approach is a definite hardfork)\n\n> - Capacity increase: the CT signature is stored in the extension block, so CT transactions increase the maximum number of transactions per block\n\nThis is not an unalloyed positive: block size increase, even via extension block, translates to greater network capacity usage globally on all fullnodes.\n\nRegards,\nZmnSCPxj\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190211/ab2f22c4/attachment-0001.html>"
            },
            {
                "author": "Trey Del Bonis",
                "date": "2019-02-12T17:27:32",
                "message_text_only": ">Under this point-of-view, then, extension block is \"not\" soft fork.\n>It is \"evil\" soft fork since older nodes are forced to upgrade as their intended functionality becomes impossible.\n>In this point-of-view, it is no better than a hard fork, which at least is very noisy about how older fullnode versions will simply stop working\n\nOfftopic: I believe that this kind of \"evil soft fork\" where nodes who\ndon't upgrade can continue to read the blockchain, update their\nutxoset, etc. but can't actually spend some or all of the coins they\nhave has been referred to as a \"firm fork\".  I think this is a pretty\nuseful term to pass around when talking about potential future forks.\n\nThe earliest reference I can find to that term from a quick search is\nthis talk from 2016 by Adam Back:\nhttp://diyhpl.us/wiki/transcripts/adam3us-bitcoin-scaling-tradeoffs/\n\n-Trey Del Bonis\n\nOn Tue, Feb 12, 2019 at 7:48 AM ZmnSCPxj via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Good morning Kenshiro,\n>\n> > - Soft fork: old nodes see CT transactions as \"sendtoany\" transactions\n>\n> There is a position that fullnodes must be able to get a view of the UTXO set, and extension blocks (which are invisible to pre-extension-block fullnodes) means that fullnodes no longer have an accurate view of the UTXO set.\n> SegWit still provides pre-SegWit fullnodes with a view of the UTXO set, although pre-SegWit fullnodes could be convinced that a particular UTXO is anyone-can-spend even though they are no longer anyone-can-spend.\n>\n> Under this point-of-view, then, extension block is \"not\" soft fork.\n> It is \"evil\" soft fork since older nodes are forced to upgrade as their intended functionality becomes impossible.\n> In this point-of-view, it is no better than a hard fork, which at least is very noisy about how older fullnode versions will simply stop working.\n>\n> > - Safe: if there is a software bug in CT it's impossible to create new coins because the coins move from normal block to normal block as public transactions\n>\n> I think more relevant here is the issue of a future quantum computing breach of the algorithms used to implement confidentiality.\n>\n> I believe this is also achievable with a non-extension-block approach by implementing a globally-verified publicly-visible counter of the total amount in all confidential transaction outputs.\n> Then it becomes impossible to move from confidential to public transactions with a value more than this counter, thus preventing inflation even if a future QC breach allows confidential transaction value commitments to be opened to any value.\n>\n> (do note that a non-extension-block approach is a definite hardfork)\n>\n> > - Capacity increase: the CT signature is stored in the extension block, so CT transactions increase the maximum number of transactions per block\n>\n> This is not an unalloyed positive: block size increase, even via extension block, translates to greater network capacity usage globally on all fullnodes.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Hampus Sj\u00f6berg",
                "date": "2019-02-14T22:32:17",
                "message_text_only": "Hi ZmnSCPxj.\n\n> There is a position that fullnodes must be able to get a view of the UTXO\nset, and extension blocks (which are invisible to pre-extension-block\nfullnodes) means that fullnodes no longer have an accurate view of the UTXO\nset.\n> SegWit still provides pre-SegWit fullnodes with a view of the UTXO set,\nalthough pre-SegWit fullnodes could be convinced that a particular UTXO is\nanyone-can-spend even though they are no longer anyone-can-spend.\n> Under this point-of-view, then, extension block is \"not\" soft fork.\n\nThere's a way to do CT without an extension block and while still\nmaintaining a correct UTXO set for old nodes. Perhaps it is similar what\nyou meant with this comment (I believe you don't need to do a hardfork\nthough)\n\n>  Then it becomes impossible to move from confidential to public\ntransactions with a value more than this counter, thus preventing inflation\neven if a future QC breach allows confidential transaction value\ncommitments to be opened to any value.\n> (do note that a non-extension-block approach is a definite hardfork)\n\nAnyway, the method goes like this:\n\nFunds that go in to CT-mode are placed in a consensus/miner controlled\nreserve pool. To go out from CT back to normal, funds are then transferred\nback to the user from this pool.\nCT transactions seen from a non-upgraded node will be a transaction with 0\nsat outputs. The actual rangeproof commitment could be placed in the script\noutput or perhaps somewhere else.\n\nTo enter CT-mode, you'll need to make a commitment. The transaction\ncontains two outputs, one to the reserve pool containing the funds that can\nonly be reclaimed when you go back to normal and one CT-output that you can\nstart doing CT transactions from.\nI believe this could be made seamlessly with just a new bech32 address\nspecifically for CT. Sending to a CT address could be done as easily as\nsending to a P2SH. In other words, it doesn't have to be two steps to send\nto someone over at CT space.\n\n> It is \"evil\" soft fork since older nodes are forced to upgrade as their\nintended functionality becomes impossible.\n> In this point-of-view, it is no better than a hard fork, which at least\nis very noisy about how older fullnode versions will simply stop working.\n\nRegarding normal extension blocks, I think it is definitely better than a\nhardfork since there's no way to be derailed from the network, even though\nyou do not understand the rules fully.\n\nSidenote, I think Trey Del Bonis is right regarding the terminology here,\nevil softforks/soft hardforks usually mean that you abandon the old chain\nto force all nodes to upgrade (https://petertodd.org/2016/forced-soft-forks\n).\n\nBest\nHampus\n\n\nDen tis 12 feb. 2019 kl 13:49 skrev ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n> Good morning Kenshiro,\n>\n> > - Soft fork: old nodes see CT transactions as \"sendtoany\" transactions\n>\n> There is a position that fullnodes must be able to get a view of the UTXO\n> set, and extension blocks (which are invisible to pre-extension-block\n> fullnodes) means that fullnodes no longer have an accurate view of the UTXO\n> set.\n> SegWit still provides pre-SegWit fullnodes with a view of the UTXO set,\n> although pre-SegWit fullnodes could be convinced that a particular UTXO is\n> anyone-can-spend even though they are no longer anyone-can-spend.\n>\n> Under this point-of-view, then, extension block is \"not\" soft fork.\n> It is \"evil\" soft fork since older nodes are forced to upgrade as their\n> intended functionality becomes impossible.\n> In this point-of-view, it is no better than a hard fork, which at least is\n> very noisy about how older fullnode versions will simply stop working.\n>\n> > - Safe: if there is a software bug in CT it's impossible to create new\n> coins because the coins move from normal block to normal block as public\n> transactions\n>\n> I think more relevant here is the issue of a future quantum computing\n> breach of the algorithms used to implement confidentiality.\n>\n> I believe this is also achievable with a non-extension-block approach by\n> implementing a globally-verified publicly-visible counter of the total\n> amount in all confidential transaction outputs.\n> Then it becomes impossible to move from confidential to public\n> transactions with a value more than this counter, thus preventing inflation\n> even if a future QC breach allows confidential transaction value\n> commitments to be opened to any value.\n>\n> (do note that a non-extension-block approach is a definite hardfork)\n>\n> > - Capacity increase: the CT signature is stored in the extension block,\n> so CT transactions increase the maximum number of transactions per block\n>\n> This is not an unalloyed positive: block size increase, even via extension\n> block, translates to greater network capacity usage globally on all\n> fullnodes.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190214/a82b2025/attachment.html>"
            },
            {
                "author": "Kenshiro []",
                "date": "2019-02-14T21:14:03",
                "message_text_only": "Greetings,\n\nI think extension blocks could be optional, and it could be many different extension blocks with different functionalities like Confidential Transactions or smart contracts. Only the interested nodes would enable this extension blocks, the rest would see only the classic blockchain without extension blocks. So it's not a matter of \"old\" and \"new\" nodes, all are updated nodes with extension blocks enabled or not. The only ones that need to understand the protocols of all existing extension blocks are the miners, which must verify that transactions from \"anyone-can-spend\" to a \"classic\" address are legal.\n\nSo this is what a node with all extension blocks disabled would see in the blockchain:\n\n  *   Classic address to classic address: as always\n  *   Classic address to extension block address: transaction to \"anyone-can-spend\"\n  *   Extension block address to classic address: transaction from \"anyone-can-spend\"\n  *   Extension block address to extension block address: it doesn't see it because it doesn't download the extension blocks, only the main blocks.\n\nAll coins that are in extension blocks are also in the \"anyone-can-spend\" address of the main blocks, so basic nodes are aware of the total number of coins. It's totally safe.\n\nSo for the particular case of Confidential Transactions, it would work as explained. The CT transaction details would be in the extension block which could have the same size as the main block so the total size of the blockchain (main blocks + extension blocks) would be double.\n\nWith this method bitcoin could add new features without losing the \"store of value\" property, as the base protocol never changes. Again, maybe I'm missing some technical detail here, I'm still learning \ud83d\ude0a\n\nRegards\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190214/7c4436b2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Implementing Confidential Transactions in extension blocks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Trey Del Bonis",
                "Hampus Sj\u00f6berg",
                "Kenshiro []"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 16978
        }
    },
    {
        "title": "[bitcoin-dev] Safer sighashes and more granular SIGHASH_NOINPUT",
        "thread_messages": [
            {
                "author": "Pieter Wuille",
                "date": "2019-02-09T00:39:54",
                "message_text_only": "On Wed, 19 Dec 2018 at 18:06, Rusty Russell via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Meanwhile, both SIGHASH_NOINPUT and OP_MASK have the reuse-is-dangerous\n> property; with OP_MASK the danger is limited to reuse-on-the-same-script\n> (ie. if you use the same key for a non-lightning output and a lightning\n> output, you're safe with OP_MASK.  However, this is far less likely in\n> practice).\n\nHaving had some more time to consider this and seeing discussions\nabout alternatives, I agree. It doesn't seem that OP_MASK protects\nagainst any likely failure modes. I do think that there are realistic\nrisks around NOINPUT, but output tagging (as suggested in another ML\nthread) seems to match those much better than masking does.\n\nCheers,\n\n-- \nPieter"
            }
        ],
        "thread_summary": {
            "title": "Safer sighashes and more granular SIGHASH_NOINPUT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Pieter Wuille"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 774
        }
    },
    {
        "title": "[bitcoin-dev] [Lightning-dev] CPFP Carve-Out for Fee-Prediction Issues in Contracting Applications (eg Lightning)",
        "thread_messages": [
            {
                "author": "Rusty Russell",
                "date": "2019-02-13T04:22:39",
                "message_text_only": "Matt Corallo <lf-lists at mattcorallo.com> writes:\n>>> Thus, even if you imagine a steady-state mempool growth, unless the \n>>> \"near the top of the mempool\" criteria is \"near the top of the next \n>>> block\" (which is obviously *not* incentive-compatible)\n>> \n>> I was defining \"top of mempool\" as \"in the first 4 MSipa\", ie. next\n>> block, and assumed you'd only allow RBF if the old package wasn't in the\n>> top and the replacement would be.  That seems incentive compatible; more\n>> than the current scheme?\n>\n> My point was, because of block time variance, even that criteria doesn't hold up. If you assume a steady flow of new transactions and one or two blocks come in \"late\", suddenly \"top 4MWeight\" isn't likely to get confirmed until a few blocks come in \"early\". Given block variance within a 12 block window, this is a relatively likely scenario.\n\n[ Digging through old mail. ]\n\nDoesn't really matter.  Lightning close algorithm would be:\n\n1.  Give bitcoind unileratal close.\n2.  Ask bitcoind what current expidited fee is (or survey your mempool).\n3.  Give bitcoind child \"push\" tx at that total feerate.\n4.  If next block doesn't contain unilateral close tx, goto 2.\n\nIn this case, if you allow a simpified RBF where 'you can replace if\n1. feerate is higher, 2. new tx is in first 4Msipa of mempool, 3. old tx isnt',\nit works.\n\nIt allows someone 100k of free tx spam, sure.  But it's simple.\n\nWe could further restrict it by marking the unilateral close somehow to\nsay \"gonna be pushed\" and further limiting the child tx weight (say,\n5kSipa?) in that case.\n\nCheers,\nRusty."
            }
        ],
        "thread_summary": {
            "title": "CPFP Carve-Out for Fee-Prediction Issues in Contracting Applications (eg Lightning)",
            "categories": [
                "bitcoin-dev",
                "Lightning-dev"
            ],
            "authors": [
                "Rusty Russell"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1585
        }
    },
    {
        "title": "[bitcoin-dev] [BIP Proposal] Simple Proof-of-Reserves Transactions",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2019-02-15T15:18:18",
                "message_text_only": "On Tuesday 29 January 2019 22:03:04 Steven Roose via bitcoin-dev wrote:\n> The existence of the first input (which is just a commitment hash) ensures\n> that this transaction is invalid and can never be confirmed.\n\nBut nodes can never prove the transaction is invalid, thus if sent it, they \nwill likely cache the \"transaction\", taking up memory. I'm not sure if this \nis an actual problem, as an attacker can fabricate such transactions anyway.\n\n> #:Not all systems that will be used for verification have access to a full\n> index of all transactions.  However, proofs should be easily verifiable\n> even after some of the UTXOs used in the proof are no longer unspent.\n> Metadata present in the proof allows for relatively efficient verification\n> of proofs even if no transaction index is available.\n\nI don't see anything in the format that would prove unspentness...\n\n> The proposed proof-file format provides a standard way of combining\n> multiple proofs and associated metadata.  The specification of the format\n> is in the Protocol\n> Buffers<ref>https://github.com/protocolbuffers/protobuf/</ref> format.\n\nIIRC, this has been contentious for its use in BIP70 and may hinder adoption.\n\n> message OutputMeta {\n> // Identify the outpoint.\n> bytes txid = 1;\n> uint32 vout = 2;\n>\n> // The block hash of the block where this output was created.\n> bytes block_hash = 3;\n\nThis isn't really sufficient. There should probably be a merkle proof.\n\nLuke"
            }
        ],
        "thread_summary": {
            "title": "Simple Proof-of-Reserves Transactions",
            "categories": [
                "bitcoin-dev",
                "BIP Proposal"
            ],
            "authors": [
                "Luke Dashjr"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1444
        }
    },
    {
        "title": "[bitcoin-dev] NIH warning (was Re: [BIP Proposal] Simple Proof-of-Reserves Transactions)",
        "thread_messages": [
            {
                "author": "Pavol Rusnak",
                "date": "2019-02-16T16:49:06",
                "message_text_only": "On 15/02/2019 16:18, Luke Dashjr via bitcoin-dev wrote:\n>> The proposed proof-file format provides a standard way of combining\n>> multiple proofs and associated metadata.  The specification of the format\n>> is in the Protocol\n>> Buffers<ref>https://github.com/protocolbuffers/protobuf/</ref> format.\n> \n> IIRC, this has been contentious for its use in BIP70 and may hinder adoption.\n\nOff-topic to main discussion of this thread. But I need to voice my opinion.\n\nWe've been using Protocol buffers in Trezor since the beginning and so\nfar it has proven to be as a great choice.\n\nWhile I agree it is always risky to add an exotic dependency to a\nsoftware project, this one has lots of interoperable implementations in\nall possible languages you can name and it's very easy to work with.\n\nIn the past, the Bitcoin dev community used the same arguments with\nregards to PSBT and we ended up with something that is almost as complex\nas protobuf, but it's de-facto proprietary to Bitcoin.\n\nCherry on top is that PSBT format can be easily translated back and\nforth to PB making it even more obvious that PB should have been used in\nthe first place.\n\nNow everyone ELSE needs to implement this proprietary format and this\nactually hinders adoption, not using Protocol Buffers. If these were\nused since the beginning, there would be much more PSBT usage already.\n\n-- \nBest Regards / S pozdravom,\n\nPavol \"stick\" Rusnak\nCTO, SatoshiLabs"
            },
            {
                "author": "William Casarin",
                "date": "2019-02-17T18:00:32",
                "message_text_only": "Pavol Rusnak via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org>\nwrites:\n\n> We've been using Protocol buffers in Trezor since the beginning and so\n> far it has proven to be as a great choice.\n>\n> While I agree it is always risky to add an exotic dependency to a\n> software project, this one has lots of interoperable implementations in\n> all possible languages you can name and it's very easy to work with.\n>\n> In the past, the Bitcoin dev community used the same arguments with\n> regards to PSBT and we ended up with something that is almost as complex\n> as protobuf, but it's de-facto proprietary to Bitcoin.\n>\n> Cherry on top is that PSBT format can be easily translated back and\n> forth to PB making it even more obvious that PB should have been used in\n> the first place.\n\nOne argument against Protobuf is that people are already moving away\nfrom it in favor of FlatBuffers, Google's successor to Protobuf that\ndoesn't require serialization/deserialization of structures.\n\nDo we really want to be chasing the latest serialization library fad\neach time a new one comes out? I do think there is value in having\naccessible serialization formats, which is why I think it's a good idea\nto provide custom format to protobuf conversion tools.\n\nThis way users who prefer not to include large dependencies don't have\nto, and protobuf users can just do an extra step to convert it into\ntheir preferred format.\n\nCheers,\nWill\n\n-- \nhttps://jb55.com"
            }
        ],
        "thread_summary": {
            "title": "NIH warning (was Re: Simple Proof-of-Reserves Transactions)",
            "categories": [
                "bitcoin-dev",
                "BIP Proposal"
            ],
            "authors": [
                "Pavol Rusnak",
                "William Casarin"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 2869
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal - Signatures of Messages using Bitcoin Private Keys",
        "thread_messages": [
            {
                "author": "Christopher Gilliard",
                "date": "2019-02-17T14:14:37",
                "message_text_only": "I have written up a proposed BIP. It has to do with Signature formats when\nusing Bitcoin Private keys. It is here:\nhttps://github.com/cgilliard/BIP/blob/master/README.md\n\nThis BIP was written up as suggested in this github issue:\nhttps://github.com/bitcoin/bitcoin/issues/10542\n\nNote that the proposal is inline with the implementation that Trezor\nimplemented in the above issue.\n\nAny feedback would be appreciated. Please let me know what the steps are\nwith regards to getting a BIP number assigned or any other process steps\nrequired.\n\nRegards,\nChris\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190217/c451f5ef/attachment.html>"
            },
            {
                "author": "Adam Ficsor",
                "date": "2019-02-17T19:42:05",
                "message_text_only": "In Wasabi wallet we are in the finishing line of an encryption manager.\nI'll ask the OP to review your BIP and probably I'll do it myself, too\nbefore I'd merge. Feel free to review/test our PR, too:\nhttps://github.com/zkSNACKs/WalletWasabi/pull/1127\n\nOn Sun, Feb 17, 2019 at 6:01 PM Christopher Gilliard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I have written up a proposed BIP. It has to do with Signature formats when\n> using Bitcoin Private keys. It is here:\n> https://github.com/cgilliard/BIP/blob/master/README.md\n>\n> This BIP was written up as suggested in this github issue:\n> https://github.com/bitcoin/bitcoin/issues/10542\n>\n> Note that the proposal is inline with the implementation that Trezor\n> implemented in the above issue.\n>\n> Any feedback would be appreciated. Please let me know what the steps are\n> with regards to getting a BIP number assigned or any other process steps\n> required.\n>\n> Regards,\n> Chris\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nBest,\n\u00c1d\u00e1m\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190217/00affeb6/attachment.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2019-02-18T22:59:25",
                "message_text_only": "Then, since you wrote this proposal, maybe you should add the very\nprecise description of the signing/verification process since it is\ndocumented nowhere\n\nI don't get the use of the speech regarding keys while it should focus\non signatures which are summarized in a vague sentence inspired by your\nref [2] with a not very logical link to the next paragraph stating that\nr,s should be 32B and the whole thing 65B with a header of 1B, you did\nnot invent it, that's probably the rule, not sure where it is specified\nagain and for what purpose, the header seems completely of no use\nespecially when you extend to segwit/bech32 since you just have to check\nthat related compressed key matches\n\nLe 17/02/2019 \u00e0 15:14, Christopher Gilliard via bitcoin-dev a \u00e9crit\u00a0:\n> I have written up a proposed BIP. It has to do with Signature formats\n> when using Bitcoin Private keys. It is\n> here:\u00a0https://github.com/cgilliard/BIP/blob/master/README.md\n>\n> This BIP was written up as suggested in this github\n> issue:\u00a0https://github.com/bitcoin/bitcoin/issues/10542\n>\n> Note that the proposal is inline with the implementation that Trezor\n> implemented in the above issue.\n>\n> Any feedback would be\u00a0appreciated. Please let me know what the steps\n> are with regards to getting a BIP number assigned or any other process\n> steps required.\n>\n> Regards,\n> Chris\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-- \nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190218/971a925a/attachment-0001.html>"
            },
            {
                "author": "Christopher Gilliard",
                "date": "2019-02-18T23:24:27",
                "message_text_only": "The proposal includes actual code that does verification, but I didn't\ninclude code for signing. I thought it could be inferred, but I could at\nleast include a description of how to sign. I am not sure exactly what part\nyou are referring to by \"keys speech\", but the signatures are done by ECDSA\nkeys so it's hard to not include anything about keys even though that's not\nthe main topic. The \"Background on ECDSA keys\" section was mainly meant to\ngive background about what kind of keys Bitcoin uses, for people who\nalready know that they can easily skip this section so I would probably\nthink it's best just to leave in.  Maybe it should be at the end as an\naddendum though. Yes, I did not invent any of this, I'm just documenting\nwhat people actually seem to do because I had to verify signatures as part\nof a project I'm working on. I would have liked to have had this document\nwhen I started the project so I thought it might be useful to others since\nas far as I can tell this was not specified anywhere. The reason for\nincluding this data in the header is the same that compressed/uncompressed\nis included in the header so that you know which type of key the signature\nis from and you don't have to try all options to see if any matches. This\nis why Trezor did that way and why I documented it. I'm sure there are\nother ways to do this, but since this is out there in the field being used\nand is a reasonable solution, I thought I'd write it up.\n\nOn Mon, Feb 18, 2019 at 2:59 PM Aymeric Vitte <vitteaymeric at gmail.com>\nwrote:\n\n> Then, since you wrote this proposal, maybe you should add the very precise\n> description of the signing/verification process since it is documented\n> nowhere\n>\n> I don't get the use of the speech regarding keys while it should focus on\n> signatures which are summarized in a vague sentence inspired by your ref\n> [2] with a not very logical link to the next paragraph stating that r,s\n> should be 32B and the whole thing 65B with a header of 1B, you did not\n> invent it, that's probably the rule, not sure where it is specified again\n> and for what purpose, the header seems completely of no use especially when\n> you extend to segwit/bech32 since you just have to check that related\n> compressed key matches\n> Le 17/02/2019 \u00e0 15:14, Christopher Gilliard via bitcoin-dev a \u00e9crit :\n>\n> I have written up a proposed BIP. It has to do with Signature formats when\n> using Bitcoin Private keys. It is here:\n> https://github.com/cgilliard/BIP/blob/master/README.md\n>\n> This BIP was written up as suggested in this github issue:\n> https://github.com/bitcoin/bitcoin/issues/10542\n>\n> Note that the proposal is inline with the implementation that Trezor\n> implemented in the above issue.\n>\n> Any feedback would be appreciated. Please let me know what the steps are\n> with regards to getting a BIP number assigned or any other process steps\n> required.\n>\n> Regards,\n> Chris\n>\n> _______________________________________________\n> bitcoin-dev mailing listbitcoin-dev at lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> --\n> Move your coins by yourself (browser version): https://peersm.com/wallet\n> Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\n> Zcash wallets made simple: https://github.com/Ayms/zcash-wallets\n> Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\n> Get the torrent dynamic blocklist: http://peersm.com/getblocklist\n> Check the 10 M passwords list: http://peersm.com/findmyass\n> Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org\n> Peersm : http://www.peersm.com\n> torrent-live: https://github.com/Ayms/torrent-live\n> node-Tor : https://www.github.com/Ayms/node-Tor\n> GitHub : https://www.github.com/Ayms\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190218/c16867ac/attachment-0001.html>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2019-02-18T23:50:30",
                "message_text_only": "Ah, OK, that's of course a good thing to document this undocumented (and\nstrange) stuff, as a matter of fact I implemented it after reading your\npost (because this was on my todo list since some time) and got annoyed\nquickly, mainly by what is doing formatMessageForSigning (which is quite\ntrivial when you know it but would be good to document precisely)\n\nSo, yes, it's a good idea to write this, regarding the header I still\ndon't see the use, testing the different possibilities is not a big\ndeal, why the signature format is not the same as transactions one is\nmysterious too\n\nLe 19/02/2019 \u00e0 00:24, Christopher Gilliard a \u00e9crit\u00a0:\n> The proposal includes actual code that does verification, but I didn't\n> include code for signing. I thought it could be inferred, but I could\n> at least include a description of how to sign. I am not sure exactly\n> what part you are referring to by \"keys speech\", but the signatures\n> are done by ECDSA keys so it's hard to not include anything about keys\n> even though that's not the main topic. The \"Background on ECDSA keys\"\n> section was mainly meant to give background about what kind of keys\n> Bitcoin uses, for people who already know that they can easily skip\n> this section so I would probably think it's best just to leave in.\u00a0\n> Maybe it should be at the end as an addendum though. Yes, I did not\n> invent any of this, I'm just documenting what people actually seem to\n> do because I had to verify signatures as part of a project I'm working\n> on. I would have liked to have had this document when I started the\n> project so I thought it might be useful to others since as far as I\n> can tell this was not specified anywhere. The reason for including\n> this data in the header is the same that compressed/uncompressed is\n> included in the header so that you know which type of key the\n> signature is from and you don't have to try all options to see if any\n> matches. This is why Trezor did that way and why I documented it. I'm\n> sure there are other ways to do this, but since this is out there in\n> the field being used and is a reasonable solution, I thought I'd write\n> it up.\n>\n> On Mon, Feb 18, 2019 at 2:59 PM Aymeric Vitte <vitteaymeric at gmail.com\n> <mailto:vitteaymeric at gmail.com>> wrote:\n>\n>     Then, since you wrote this proposal, maybe you should add the very\n>     precise description of the signing/verification process since it\n>     is documented nowhere\n>\n>     I don't get the use of the speech regarding keys while it should\n>     focus on signatures which are summarized in a vague sentence\n>     inspired by your ref [2] with a not very logical link to the next\n>     paragraph stating that r,s should be 32B and the whole thing 65B\n>     with a header of 1B, you did not invent it, that's probably the\n>     rule, not sure where it is specified again and for what purpose,\n>     the header seems completely of no use especially when you extend\n>     to segwit/bech32 since you just have to check that related\n>     compressed key matches\n>\n>     Le 17/02/2019 \u00e0 15:14, Christopher Gilliard via bitcoin-dev a \u00e9crit\u00a0:\n>>     I have written up a proposed BIP. It has to do with Signature\n>>     formats when using Bitcoin Private keys. It is\n>>     here:\u00a0https://github.com/cgilliard/BIP/blob/master/README.md\n>>\n>>     This BIP was written up as suggested in this github\n>>     issue:\u00a0https://github.com/bitcoin/bitcoin/issues/10542\n>>\n>>     Note that the proposal is inline with the implementation that\n>>     Trezor implemented in the above issue.\n>>\n>>     Any feedback would be\u00a0appreciated. Please let me know what the\n>>     steps are with regards to getting a BIP number assigned or any\n>>     other process steps required.\n>>\n>>     Regards,\n>>     Chris\n>>\n>>     _______________________________________________\n>>     bitcoin-dev mailing list\n>>     bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>\n>>     https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>     -- \n>     Move your coins by yourself (browser version): https://peersm.com/wallet\n>     Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\n>     Zcash wallets made simple: https://github.com/Ayms/zcash-wallets\n>     Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\n>     Get the torrent dynamic blocklist: http://peersm.com/getblocklist\n>     Check the 10 M passwords list: http://peersm.com/findmyass\n>     Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org\n>     Peersm : http://www.peersm.com\n>     torrent-live: https://github.com/Ayms/torrent-live\n>     node-Tor : https://www.github.com/Ayms/node-Tor\n>     GitHub : https://www.github.com/Ayms\n>\n-- \nMove your coins by yourself (browser version): https://peersm.com/wallet\nBitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190218/c0f13f92/attachment.html>"
            },
            {
                "author": "Christopher Gilliard",
                "date": "2019-02-19T00:29:34",
                "message_text_only": "Trying the four possible options (p2pkh compressed, p2pkh uncompressed,\nseg3, and bech32) is certainly a possibility and in fact, that's what I\nended up doing because not every wallet implements something like this, but\nif there is a header field currently in use, it seemed reasonable to me to\nuse it specify which type of key is being used. If the header includes\nwhether the key is compressed or not compressed it seems logical to include\nall data about what type of key it is and not just this one type of\ninformation. That's why I thought the solution made sense and I wrote it up.\n\nOn Mon, Feb 18, 2019 at 3:50 PM Aymeric Vitte <vitteaymeric at gmail.com>\nwrote:\n\n> Ah, OK, that's of course a good thing to document this undocumented (and\n> strange) stuff, as a matter of fact I implemented it after reading your\n> post (because this was on my todo list since some time) and got annoyed\n> quickly, mainly by what is doing formatMessageForSigning (which is quite\n> trivial when you know it but would be good to document precisely)\n>\n> So, yes, it's a good idea to write this, regarding the header I still\n> don't see the use, testing the different possibilities is not a big deal,\n> why the signature format is not the same as transactions one is mysterious\n> too\n> Le 19/02/2019 \u00e0 00:24, Christopher Gilliard a \u00e9crit :\n>\n> The proposal includes actual code that does verification, but I didn't\n> include code for signing. I thought it could be inferred, but I could at\n> least include a description of how to sign. I am not sure exactly what part\n> you are referring to by \"keys speech\", but the signatures are done by ECDSA\n> keys so it's hard to not include anything about keys even though that's not\n> the main topic. The \"Background on ECDSA keys\" section was mainly meant to\n> give background about what kind of keys Bitcoin uses, for people who\n> already know that they can easily skip this section so I would probably\n> think it's best just to leave in.  Maybe it should be at the end as an\n> addendum though. Yes, I did not invent any of this, I'm just documenting\n> what people actually seem to do because I had to verify signatures as part\n> of a project I'm working on. I would have liked to have had this document\n> when I started the project so I thought it might be useful to others since\n> as far as I can tell this was not specified anywhere. The reason for\n> including this data in the header is the same that compressed/uncompressed\n> is included in the header so that you know which type of key the signature\n> is from and you don't have to try all options to see if any matches. This\n> is why Trezor did that way and why I documented it. I'm sure there are\n> other ways to do this, but since this is out there in the field being used\n> and is a reasonable solution, I thought I'd write it up.\n>\n> On Mon, Feb 18, 2019 at 2:59 PM Aymeric Vitte <vitteaymeric at gmail.com>\n> wrote:\n>\n>> Then, since you wrote this proposal, maybe you should add the very\n>> precise description of the signing/verification process since it is\n>> documented nowhere\n>>\n>> I don't get the use of the speech regarding keys while it should focus on\n>> signatures which are summarized in a vague sentence inspired by your ref\n>> [2] with a not very logical link to the next paragraph stating that r,s\n>> should be 32B and the whole thing 65B with a header of 1B, you did not\n>> invent it, that's probably the rule, not sure where it is specified again\n>> and for what purpose, the header seems completely of no use especially when\n>> you extend to segwit/bech32 since you just have to check that related\n>> compressed key matches\n>> Le 17/02/2019 \u00e0 15:14, Christopher Gilliard via bitcoin-dev a \u00e9crit :\n>>\n>> I have written up a proposed BIP. It has to do with Signature formats\n>> when using Bitcoin Private keys. It is here:\n>> https://github.com/cgilliard/BIP/blob/master/README.md\n>>\n>> This BIP was written up as suggested in this github issue:\n>> https://github.com/bitcoin/bitcoin/issues/10542\n>>\n>> Note that the proposal is inline with the implementation that Trezor\n>> implemented in the above issue.\n>>\n>> Any feedback would be appreciated. Please let me know what the steps are\n>> with regards to getting a BIP number assigned or any other process steps\n>> required.\n>>\n>> Regards,\n>> Chris\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing listbitcoin-dev at lists.linuxfoundation.orghttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>> --\n>> Move your coins by yourself (browser version): https://peersm.com/wallet\n>> Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\n>> Zcash wallets made simple: https://github.com/Ayms/zcash-wallets\n>> Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\n>> Get the torrent dynamic blocklist: http://peersm.com/getblocklist\n>> Check the 10 M passwords list: http://peersm.com/findmyass\n>> Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org\n>> Peersm : http://www.peersm.com\n>> torrent-live: https://github.com/Ayms/torrent-live\n>> node-Tor : https://www.github.com/Ayms/node-Tor\n>> GitHub : https://www.github.com/Ayms\n>>\n>> --\n> Move your coins by yourself (browser version): https://peersm.com/wallet\n> Bitcoin transactions made simple: https://github.com/Ayms/bitcoin-transactions\n> Zcash wallets made simple: https://github.com/Ayms/zcash-wallets\n> Bitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\n> Get the torrent dynamic blocklist: http://peersm.com/getblocklist\n> Check the 10 M passwords list: http://peersm.com/findmyass\n> Anti-spies and private torrents, dynamic blocklist: http://torrent-live.org\n> Peersm : http://www.peersm.com\n> torrent-live: https://github.com/Ayms/torrent-live\n> node-Tor : https://www.github.com/Ayms/node-Tor\n> GitHub : https://www.github.com/Ayms\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190218/f2be0366/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal - Signatures of Messages using Bitcoin Private Keys",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Christopher Gilliard",
                "Aymeric Vitte",
                "Adam Ficsor"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 20036
        }
    },
    {
        "title": "[bitcoin-dev] BIP proposal - addrv2 message",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2019-02-18T07:56:38",
                "message_text_only": "See https://gist.github.com/laanwj/4fe8470881d7b9499eedc48dc9ef1ad1 for formatted version,\n\nLook under \"Considerations\" for topics that might still need to be discussed.\n\n<pre>\n  BIP: ???\n  Layer: Peer Services\n  Title: addrv2 message\n  Author: Wladimir J. van der Laan <laanwj at gmail.com>\n  Comments-Summary: No comments yet.\n  Comments-URI: \n  Status: Draft\n  Type: Standards Track\n  Created: 2018-06-01\n  License: BSD-2-Clause\n</pre>\n\n==Introduction==\n\n===Abstract===\n\nThis document proposes a new P2P message to gossip longer node addresses over the P2P network.\nThis is required to support new-generation Onion addresses, I2P, and potentially other networks\nthat have longer endpoint addresses than fit in the 128 bits of the current <code>addr</code> message.\n\n===Copyright===\n\nThis BIP is licensed under the 2-clause BSD license.\n\n===Motivation===\n\nTor v3 hidden services are part of the stable release of Tor since version 0.3.2.9. They have\nvarious advantages compared to the old hidden services, among which better encryption and privacy\n<ref>[https://gitweb.torproject.org/torspec.git/tree/rend-spec-v3.txt Tor Rendezvous Specification - Version 3]</ref>.\nThese services have 256 bit addresses and thus do not fit in the existing <code>addr</code> message, which encapsulates onion addresses in OnionCat IPv6 addresses.\n\nOther transport-layer protocols such as I2P have always used longer\naddresses. This change would make it possible to gossip such addresses over the\nP2P network, so that other peers can connect to them.\n\n==Specification==\n\n<blockquote>\nThe key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\",\n\"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in this document are to be\ninterpreted as described in RFC 2119<ref>[https://tools.ietf.org/html/rfc2119 RFC 2119]</ref>.\n</blockquote>\n\nThe <code>addrv2</code> message is defined as a message where <code>pchCommand == \"addrv2\"</code>.\nIt is serialized in the standard encoding for P2P messages.\nIts format is similar to the current <code>addr</code> message format\n<ref>[https://bitcoin.org/en/developer-reference#addr Bitcoin Developer Reference: addr message]</ref>, with the difference that the \nfixed 16-byte IP address is replaced by a network ID and a variable-length address, and the time and services format has been changed to VARINT.\n\nThis means that the message contains a serialized <code>std::vector</code> of the following structure:\n\n{| class=\"wikitable\" style=\"width: auto; text-align: center; font-size: smaller; table-layout: fixed;\"\n!Type\n!Name\n!Description\n|-\n| <code>VARINT</code> (unsigned)\n| <code>time</code>\n| Time that this node was last seen as connected to the network. A time in Unix epoch time format, up to 64 bits wide.\n|-\n| <code>VARINT</code> (unsigned)\n| <code>services</code>\n| Service bits. A 64-wide bit field.\n|-\n| <code>uint8_t</code>\n| <code>networkID</code>\n| Network identifier. An 8-bit value that specifies which network is addressed.\n|-\n| <code>std::vector<uint8_t></code>\n| <code>addr</code>\n| Network address. The interpretation depends on networkID.\n|-\n| <code>uint16_t</code>\n| <code>port</code>\n| Network port. If not relevant for the network this MUST be 0.\n|}\n\nOne message can contain up to 1,000 addresses. Clients SHOULD reject messages with more addresses.\n\nField <code>addr</code> has a variable length, with a maximum of 32 bytes (256 bits). Clients SHOULD reject\nlonger addresses.\n\nThe list of reserved network IDs is as follows:\n\n{| class=\"wikitable\" style=\"width: auto; text-align: center; font-size: smaller; table-layout: fixed;\"\n!Network ID\n!Enumeration\n!Address length (bytes)\n!Description\n|-\n| <code>0x01</code>\n| <code>IPV4</code>\n| 4\n| IPv4 address (globally routed internet)\n|-\n| <code>0x02</code>\n| <code>IPV6</code>\n| 16\n| IPv6 address (globally routed internet)\n|-\n| <code>0x03</code>\n| <code>TORV2</code>\n| 10\n| Tor v2 hidden service address\n|-\n| <code>0x04</code>\n| <code>TORV3</code>\n| 32\n| Tor v3 hidden service address\n|-\n| <code>0x05</code>\n| <code>I2P</code>\n| 32\n| I2P overlay network address\n|-\n| <code>0x06</code>\n| <code>CJDNS</code>\n| 16\n| Cjdns overlay network address\n|}\n\nTo allow for future extensibility, clients MUST ignore address types that they do not know about.\nClient MAY store and gossip address formats that they do not know about. Further network ID numbers MUST be reserved in a new BIP document.\n\nClients SHOULD reject addresses that have a different length than specified in this table for a specific address ID, as these are meaningless.\n\nSee the appendices for the address encodings to be used for the various networks.\n\n==Compatibility==\n\nSend <code>addrv2</code> messages only, and exclusively, when the peer has a certain protocol version (or higher):\n<source lang=\"c++\">\n//! gossiping using `addrv2` messages starts with this version\nstatic const int GOSSIP_ADDRV2_VERSION = 70016;\n</source>\nFor older peers keep sending the legacy <code>addr</code> message, ignoring addresses with the newly introduced address types.\n\n==Reference implementation==\n\nThe reference implementation is available at (to be done)\n\n==Considerations==\n\n(to be discussed)\n\n* ''Client MAY store and gossip address formats that they do not know about'': does it ever make sense to gossip addresses outside a certain overlay network? Say, I2P addresses to Tor? I'm not sure. Especially for networks that have no exit nodes as there is no overlap with the globally routed internet at all.\n\n* Lower precision of <code>time</code> field? seconds precision seems overkill, and can even be harmful, there have been attacks that exploited high precision timestamps for mapping the current network topology.\n\n** (gmaxwell) If you care about space time field could be reduced to 16 bits easily.  Turn it into a \"time ago seen\" quantized to 1 hour precision. (IIRC we quantize times to 2hrs regardless).\n\n* Rolling <code>port</code> into <code>addr</code>, or making the port optional, would make it possible to shave off two bytes for address types that don't have ports (however, all of the currently listed formats have a concept of port.). It could also be an optional data item (see below).\n\n* (gmaxwell) Optional (per-service) data could be useful for various things:\n\n** Node-flavors for striping (signalling which slice of the blocks the node has in selective pruning)\n\n** Payload for is alternative ports for other transports (e.g. UDP ports)\n\n** If we want optional flags. I guess the best thing would just be a byte to include the count of them, then a byte \"type\" for each one where the type also encodes if the payload is 0/8/16/32 bits. (using the two MSB of the type to encode the length).  And then bound the count of them so that the total is  still reasonably sized.\n\n==Acknowledgements==\n\n- Jonas Schnelli: change <code>services</code> field to VARINT, to make the message more compact in the likely case instead of always using 8 bytes.\n\n- Luke-Jr: change <code>time</code> field to VARINT, for post-2038 compatibility.\n\n- Gregory Maxwell: various suggestions regarding extensibility\n\n==Appendix A: Tor v2 address encoding==\n\nThe new message introduces a separate network ID for <code>TORV2</code>. \n\nClients MUST send Tor hidden service addresses with this network ID, with the 80-bit hidden service ID in the address field. This is the same as the representation in the legacy <code>addr</code> message, minus the 6 byte prefix of the OnionCat wrapping.\n\nClients SHOULD ignore OnionCat (<code>fd87:d87e:eb43::/48</code>) addresses on receive if they come with the <code>IPV6</code> network ID.\n\n==Appendix B: Tor v3 address encoding==\n\nAccording to the spec <ref>[https://gitweb.torproject.org/torspec.git/tree/rend-spec-v3.txt Tor Rendezvous Specification - Version 3: Encoding onion addresses]</ref>, next-gen <code>.onion</code> addresses are encoded as follows:\n<pre>\nonion_address = base32(PUBKEY | CHECKSUM | VERSION) + \".onion\"\n CHECKSUM = H(\".onion checksum\" | PUBKEY | VERSION)[:2]\n\n where:\n   - PUBKEY is the 32 bytes ed25519 master pubkey of the hidden service.\n   - VERSION is an one byte version field (default value '\\x03')\n   - \".onion checksum\" is a constant string\n   - CHECKSUM is truncated to two bytes before inserting it in onion_address\n</pre>\n\nTor v3 addresses MUST be sent with the <code>TORV3</code> network ID, with the 32-byte PUBKEY part in the address field. As VERSION will always be '\\x03' in the case of v3 addresses, this is enough to reconstruct the onion address.\n\n==Appendix C: I2P address encoding==\n\nLike Tor, I2P naming uses a base32-encoded address format<ref>[https://geti2p.net/en/docs/naming#base32 I2P: Naming and address book]</ref>.\n\nI2P uses 52 characters (256 bits) to represent the full SHA-256 hash, followed by <code>.b32.i2p</code>.\n\nI2P addresses MUST be sent with the <code>I2P</code> network ID, with the decoded SHA-256 hash as address field.\n\n==Appendix D: Cjdns address encoding==\n\nCjdns addresses are simply IPv6 addresses in the <code>fc00::/8</code> range<ref>[https://github.com/cjdelisle/cjdns/blob/6e46fa41f5647d6b414612d9d63626b0b952746b/doc/Whitepaper.md#pulling-it-all-together Cjdns whitepaper: Pulling It All Together]</ref>. They MUST be sent with the <code>CJDNS</code> network ID.\n\n==References==\n\n<references/>"
            }
        ],
        "thread_summary": {
            "title": "BIP proposal - addrv2 message",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9275
        }
    },
    {
        "title": "[bitcoin-dev] Privacy literature review",
        "thread_messages": [
            {
                "author": "Chris Belcher",
                "date": "2019-02-23T20:17:17",
                "message_text_only": "Hello list,\n\nFor the last few weeks I've been working on a literature review for\nbitcoin privacy:\n\nhttps://en.bitcoin.it/wiki/Privacy\n\nIt aims to cover about all privacy issues in bitcoin, including\nLightning network, and has a bunch of examples to help demonstrate how\nthe concepts work in practice.\n\nThere is also a new wiki category with smaller related articles:\n\nhttps://en.bitcoin.it/wiki/Category:Privacy\n\nRegards\nCB"
            }
        ],
        "thread_summary": {
            "title": "Privacy literature review",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Belcher"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 423
        }
    },
    {
        "title": "[bitcoin-dev] BIP - Symbol for satoshi",
        "thread_messages": [
            {
                "author": "Amine Chakak",
                "date": "2019-02-23T22:10:28",
                "message_text_only": "Hi,\n\nI don't know if this is the right place to do so, but it says on the\nwebsite to first propose ideas for BIPS to the mailing list.\n\nI would like to propose @bitficus idea for a satoshi symbol (monetary).\nThe idea has been floated around to switch to satoshi as a base unit.\nThe lightning network uses satoshis as a base unit.\n\nHere is the proposal :\nhttps://twitter.com/bitficus/status/1097979724515557377\n\nPleas let me know if it would be appropriate to write a BIP for it.\n\nThank you for your consideration.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190223/250f912b/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP - Symbol for satoshi",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Amine Chakak"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 701
        }
    },
    {
        "title": "[bitcoin-dev] Vulnerability relating to 64-byte transactions in Bitcoin Core 0.13 branch",
        "thread_messages": [
            {
                "author": "Suhas Daftuar",
                "date": "2019-02-25T19:29:21",
                "message_text_only": "Hi,\n\nI'm writing to report a consensus vulnerability affecting Bitcoin Core\nversions\n0.13.0, 0.13.1, and 0.13.2.  These software versions reached end-of-life on\n2018-08-01.\n\nThe issue surrounds a fundamental design flaw in Bitcoin's Merkle tree\nconstruction.  Last year, the vulnerability (CVE-2017-12842) around 64-byte\ntransactions being used to trick light clients into thinking that a\ntransaction\nwas committed in a block was discussed on this mailing list\n(\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2018-June/016091.html\n).\nThere is a related attack resulting from the ambiguity around 64-byte\ntransactions which could be used to cause a vulnerable full-node\nimplementation\nto fall out of consensus.\n\nThe attack on light clients discussed previously was centered on the idea of\nclaiming the Merkle tree has one more level in it than it actually has, to\nprove that a candidate transaction is in the chain by having its hash match\none\nside of a 64-byte transaction. The vulnerability I am describing here\ninvolves\ngoing the other direction: find a row of interior nodes in the Merkle tree\nthat successfully deserialize as transactions, in order to make a block\nappear\nto be invalid. (This is of a similar character to the attack described by\nSergio Demian Lerner on\nhttps://bitslog.wordpress.com/2018/06/09/leaf-node-weakness-in-bitcoin-merkle-tree-design/\n,\nin the section titled \"An (expensive) attack to partition Bitcoin\".)\n\nIt has long been recognized that malleating a block's transactions in a way\nthat produces the same Merkle root could be used to cause a node to fall\nout of\nconsensus, because of the logic in Bitcoin Core to cache the invalidity of\nblocks (ie to avoid re-validation of known-invalid ones, which would\notherwise\nmake the software vulnerable to DoS). Malleation by \"going up\" the Merkle\ntree,\nand claiming that some interior row is in fact the set of (64-byte)\ntransactions in a block, could be used to cause the Bitcoin Core 0.13\nbranch to\nincorrectly mark as invalid a block that in fact has a valid set of\ntransactions. Moreover, this requires very little work to accomplish -- less\nthan 22 bits of work in all.\n\nI have attached a writeup that I put together for my own memory and notes\nwhich\ngoes into more detail (along with a summary of other Merkle tree issues,\nincluding the duplicate transactions issue from CVE-2012-2459 and the SPV\nissue); please see sections 3.1 and 4.1 for a discussion.  The bug in 0.13\nwas\nintroduced as an unintended side-effect of a change I authored\n(https://github.com/bitcoin/bitcoin/pull/7225).  Once I learned of this\ncategory of Merkle malleation issues, I realized that the change\ninadvertently\nintroduced a vulnerability to this issue that did not previously exist (in\nany\nprior version of the software, as far as I can tell).  A bug fix that\neffectively reverted the change (\nhttps://github.com/bitcoin/bitcoin/pull/9765)\nwas made just before the 0.14 version of Bitcoin Core was released, and no\nlater versions of the software are affected.\n\nAlso, I have scanned the blockchain looking for instances where the first\ntwo\nhashes in any row of the Merkle tree would deserialize validly as a 64-byte\ntransaction, and I have found zero such instances.  So in particular there\nare\nno blocks on Bitcoin's main chain (as of this writing) that could be used to\nattack an 0.13 node.\n\nI thought it best to withhold disclosure of this vulnerability before a\nmitigation was in place for the related SPV-issue (which I assumed would\nbecome\nobvious with this disclosure); once that became public last summer and a\nmitigation deployed (by making 64-byte transactions nonstandard), that\nconcern\nwas eliminated.\n\nThanks to Johnson Lau and Greg Maxwell for originally alerting me to this\nissue.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190225/a27d8837/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: BitcoinMerkle.pdf\nType: application/pdf\nSize: 200880 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20190225/a27d8837/attachment-0001.pdf>"
            }
        ],
        "thread_summary": {
            "title": "Vulnerability relating to 64-byte transactions in Bitcoin Core 0.13 branch",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Suhas Daftuar"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4226
        }
    },
    {
        "title": "[bitcoin-dev] Fortune Cookies to Bitcoin Seed",
        "thread_messages": [
            {
                "author": "Trey Del Bonis",
                "date": "2019-02-28T03:48:57",
                "message_text_only": "Hello all,\n\nThis might be another proto-BIP similar to the post about using a card\nshuffle as a wallet seed that was posted here a few weeks back:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2019-February/016645.html\n\nThis is an idea I had to deriving a wallet seed from the lucky numbers\non a fortunes from fortune cookies [1].\n\nOn one side is some silly fortune, which we don't really care about\nhere.  But depending on the brand, on the other side there's 2 parts:\n\n* \"Learn Chinese\", with a word in English and its translation into\nChinese characters and the (probably) pinyin.\n\n* \"Lucky Numbers\", followed by usually 6 or 7 numbers, presumably in\nthe range of 1 to 99.  Someone can correct me on this if I'm wrong.\n\nSo each number should have around ~6.6 bits of entropy, which means\nyou could generate a \"very secure\" wallet seed with about 7 fortunes.\nWe can remember the order of the numbers on these fortunes based on\nthe English words, which we can commit to memory.\n\nIt's considered a rule of thumb that you can remember \"7 things\" at\nonce, which is pretty convenient for this.  Sometimes the numbers are\nsorted, which decreases the entropy a bit, but that can be remedied\nwith just more fortunes.  This also splits up the information required\nto reconstruct the seed into both something physical and something\nremembered, and there isn't any particular ordering that someone can\nmess up by, say, shuffling the card deck.  Although someone is\narguably more likely to throw away random fortunes than they are to\nthrow away a deck of cards which is a weakness of this scheme.\n\nIt also arguably has better deniability.  If you keep a pile of 20\nfortunes (with different \"Learn Chinese\" words) and remember which 7\nof them are for your key, but pick another 7 you can use to make a\ndecoy wallet to use if being forced to reveal a wallet.  Keeping 20\naround is a little excessive but it gives 390700800 possible wallets.\nSo security can be trivially parameterized based on how secure you\nwant your wallet to be if someone finds your stash.\n\nI wrote a little Python script to generate a key with this, it's not\nvery clean and could be much improved but it works pretty well as a\nproof of concept: https://gitlab.com/delbonis/chinese-wallet\n\n-Trey Del Bonis\n\n[1] https://en.wikipedia.org/wiki/Fortune_cookie"
            }
        ],
        "thread_summary": {
            "title": "Fortune Cookies to Bitcoin Seed",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Trey Del Bonis"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2323
        }
    }
]