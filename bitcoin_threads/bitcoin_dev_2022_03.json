[
    {
        "title": "[bitcoin-dev] Recursive covenant opposition, or the absence thereof, was Re: TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2022-03-01T05:39:36",
                "message_text_only": "@Paul\n> I believe that money has very strong network effects. ... users will\n\"clump up\" and get \"stuck\".\n\nI'm of the same opinion.\n\n> This entire issue is avoided completely, if all the chains\n--decentralized and centralized-- and in the same monetary unit. Then, the\nmonetary network effects never interfere, and the decentralized chain is\nalways guaranteed to exist.\n\nIt sounds like what you're saying is that without side chains, everyone\nmight switch entirely to some altcoin and bitcoin will basically die. And\nat that point, the insecurity of that coin people switched to can be\nheavily exploited by some attacker(s). Is that right? Its an interesting\nthought experiment. However, it leads me to wonder: if a sidechain gets so\npopular that it dominates the main chain, why would people keep that main\nchain around at all? A sidechain could eject the main chain and all its\nbaggage if it got so big. So I don't think it can really be said that the\nproblem can be avoided \"completely\". But in any case, I see your line of\nthinking.\n\n> someone is actually in the wrong, if they proactively censor an\nexperiment of any type. If a creator is willing to stand behind something,\nthen it should be tried.\n> it makes no difference if users have their funds stolen from a\ncentralized Solana contract or from a bip300 centralized bit-Solana\nsidechain. I don't see why the tears shed would be any different.\n\nI agree with you. My point was not that we should stop anyone from doing\nthis. My point was only that we shouldn't advocate for ideas we think\naren't good. You were advocating for a \"largeblock sidechain\", and unless\nyou have good reasons to think that is an idea likely to succeed and want\nto share them with us, then you shouldn't be advocating for that. But\ncertainly if someone *does* think so and has their own reasons, I wouldn't\nwant to censor or stop them. But I wouldn't advocate for them to do it\nunless their ideas were convincing to me, because I know enough to know the\ndangers of large block blockchains.\n\n\n\nOn Mon, Feb 28, 2022 at 4:55 PM Paul Sztorc via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 2/28/2022 1:49 AM, ZmnSCPxj wrote:\n>\n> ...\n>\n> ...\n>\n> Perhaps, someone will invent a way, to LN-onboard WITHOUT needing new layer1 bytes.\n>\n> If so, a \"rich man\" could open a LN channel, and gradually transfer it to new people.\n>\n> Such a technique would need to meet two requirements (or, so it seems to me):\n> #1: The layer1 UTXO (that defines the channel) can never change (ie, the 32-bytes which define the p2sh/tapscript/covenant/whatever, must stay what-they-were when the channel was opened).\n> #2: The new part-owners (who are getting coins from the rich man), will have new pubkeys which are NOT known, until AFTER the channel is opened and confirmed on the blockchain.\n>\n> Not sure how you would get both #1 and #2 at the same time. But I am not up to date on the latest LN research.\n>\n> Yes, using channel factories.\n>\n> I think you may be wrong about this.\n> ...\n>\n> I am not wrong about this.\n>\n> Well, let's take a closer look then.\n>\n> The topic was: \"a way, to LN-onboard [a new pubkey] WITHOUT needing new layer1 bytes\".\n>\n> By which I meant, that I could generate a new pubkey right now, and add it to the LN, without any onchain action.\n>\n> I can shorten and restate the two requirements (and reorder them) as:\n> #2: Can later add a new public key to the membership set.\n> #1: Without an onchain action.\n>\n> And yet you yourself say, very clearly:\n>\n>\n> ... That is why I said changing the membership set requires onchain action.\n>\n> Which would seem to directly contradict what you say about channel\n> factories. Unless you can show me how to add my new pubkey_4, to a 3-of-3\n> channel factory opened last year. Without using an onchain action. You seem\n> to want to instead change the subject. (To something like: 'we can do\n> better the rate (32 bytes per 5 onboards), from your footnote'.) Which is\n> fine. But it is not what I bought up.\n>\n> ***\n>\n> In general, you seem to have a future in mind, where new users onboard via factory.\n> For example, 50,000 new users want to onboard in the next block. These strangers, spontaneously organize into 1000 factories of 55 people each, (50 newbies with zero coins + 5 wealthier BTCs who have lots of coins). They then broadcast into the block and join Bitcoin.\n> And this one factory provides them with many channels, so it can meet most/all of their needs.\n>\n> I am not here to critique factories. I was simply observing that your logic \"sidechains don't scale, because you have to share your messages\" is not quite airtight, because in the case of onboarding the situation is reversed and so supports the exact opposite conclusion.\n> I believe I have made my point by now. It should be easy for people to see what each of us has in mind, and the strengths and weaknesses.\n>\n> I am curious about something, though. Maybe you can help me.\n> Presumably there are risks to large factories. Perhaps an attacker could join each new factory with just $1 of BTC, spend this $1, and then refuse to cooperate with the factory any further. Thus they can disable the factory at a cost of $1 rented dollar.\n> If 1000 factories are opened per block, this would be 52.5 M factories per year, $52.5 million USD per year to disable all the factories out of spite. (All of which they would eventually get back.) I can think of a few people who might try it.\n>\n>\n> I mean, like, LN ... has a lot more onboarding activity than half-hearted sidechains like Liquid or Rootstock.\n>\n> I don't see the relevance of this. We are talking about the future\n> (theoretical), not the past (empirical). For example, someone could say\n> \"Ethereum has a lot more onboarding activity than LN ...\" but this would\n> also make no difference to anything.\n>\n> ...The onboarding rate only needs to be as fast as the rate at which people want to join Bitcoin.\n> ...\n>\n> As I pointed out in the other thread:\n>\n> * LN:\n>   * Funds can be stolen IF:\n>     * There is a 51% miner, AND\n>     * The 51% miner is a member of a channel/channel factory you are in.\n> * Drivechains:\n>   * Funds can be stolen IF:\n>     * There is a 51% miner.\n> ...\n> So there is a real degradation of security in Drivechains, and if you compute the numbers, I am reasonably sure that 33% of the world is unlikely to want to use Bitcoin within one month.\n> I mean we already had a pandemic and everyone going online and so on, and yet Bitcoin blockchain feerates are *still* small, I had to fix a bug in CLBOSS that came up only due to hitting the minimum feerate, so no --- people are not joining Bitcoin at a rate faster than Bitcoin + LN can handle it, even with a pretty good reason to move payments online.\n>\n> Worse, once 100% of the world is onboarded, the extra onboarding capacity is useless since the onboarding rate can only match the birth rate (including birth of legal persons such as corporations), which we expect is much lower than 33% increase per ***month***.\n>\n> You are buying too much capacity at a real degradation in security, and I am not convinced the extra capacity is worth the loss of security.\n>\n> Separating the onboarding rate from the payment rate is a *good thing*, because we can then design their structures differently.\n> Make onboarding slow but secure (so that their money is very secure), but make payment rate faster and less secure (because in-flight payments are likely to be much smaller than the total owned funds).\n>\n> Obviously I don't agree with any of these sentences (most are irrelevant, some false). But I would only be repeating myself.\n>\n> Paul\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220228/388b93e2/attachment.html>"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-03-02T00:00:00",
                "message_text_only": "On 3/1/2022 12:39 AM, Billy Tetrud wrote:\n\n>> This entire issue is avoided completely, if all the chains \n>> --decentralized and centralized-- and in the same monetary unit. \n>> Then, the monetary network effects never interfere, and the \n>> decentralized chain is always guaranteed to exist.\n> It sounds like what you're saying is that without side chains, \n> everyone might switch entirely to some altcoin and bitcoin will \n> basically die. And at that point, the insecurity of that coin people \n> switched to can be heavily exploited by some attacker(s). Is that right?\n\nYes, precisely.\n\n\n> Its an interesting thought experiment. However, it leads me to wonder: \n> if a sidechain gets so popular that it dominates the main chain, why \n> would people keep that main chain around at all?\n\nFor some reason, this is a very popular question. I suppose if you believe in \"one size fits all\" chain philosophy (see comment below), it makes sense to say \"these sidechains are terrible\" on Monday and then \"these sidechains are so good they will replace the mainchain\" on Tuesday.\n\nIn any event, sidechains cannot exist without their mainchain (as I see it). For example, imagine that you are on a zcash sidechain, and someone claims they deposited 1000 BTC, from Bitcoin Core into this sidechain? Do you give them 1000 z-BTC, or not? Without the mainchain,\nyou can't tell.\n\nIf you run the Bip300 DriveNet demo software (drivechain.info/releases), you will see for yourself: the test-sidechains are absolutely inert, UNTIL they have rpc access to the mainchain. (Exactly the same way that a LN node needs a Bitcoin Core node.)\n\n\n> > someone is actually in the wrong, if they proactively censor an \n> experiment of any type. If a creator is willing to stand behind \n> something, then it should be tried.\n> > it makes no difference if users have their funds stolen from a \n> centralized Solana contract or from a bip300 centralized bit-Solana \n> sidechain. I don't see why the tears shed would be any different.\n> I agree with you. My point was not that we should stop anyone from \n> doing this. My point was only that we shouldn't advocate for ideas we \n> think aren't good. You were advocating for a \"largeblock sidechain\", \n> and unless you have good reasons to think that is an idea likely to \n> succeed and want to share them with us, then you shouldn't be \n> advocating for that. But certainly if someone *does* think so and has \n> their own reasons, I wouldn't want to censor or stop them. But I \n> wouldn't advocate for them to do it unless their ideas were convincing \n> to me, because I know enough to know the dangers of large block \n> blockchains.\n\nYes, I strongly agree, that we should only advocate for ideas we believe in.\n\nI do not believe in naive layer1 largeblockerism. But I do believe in sidechain largeblockism.\n\nSomething funny once happened to me when I was on a Bitcoin conference panel*. There were three people: myself, a Blockstream person, and an (ex)BitPay person. The first two of us, were valiantly defending the small block position. I gave my usual speech: that node costs must remain low, so that people can run full nodes. The largeblocker mentioned that they ran many nodes (including BCH nodes etc) and didn't mind the cost, so I disclosed --in a good-natured way-- that I do not even run a BTC full node myself (out of choice). Thus, I was yammering about software I wasn't even running, I had no skin in the game! Lo and behold -- my Blockstream smallblocker ally-on-the-panel, immediately admitted to everyone that he did not run a full node either. The only node-runner was the largeblocker. The audience found this very amusing (as did I).\n\nWe smallblockers, justified our sinful nodeless behavior, as follows (paraphrasing): we receive BTC mainly from people that we know (and have a long-term relationship with); our receipts are not time sensitive; we are not paid in BTC that often; if payments turned out to be forged we would have enormous recourse against our counterparties; etc.\n\nWe did not run full nodes, because we did not need to draw on the blockchain's powers, **for those transactions**.\n\nWhich is my point: people are different, and transactions are different. I make many transactions today, with VISA or Venmo. These are not censorship-resistant, but somehow I survive the month, without bursting into flames.\n\nWouldn't life be better, if we Bitcoiners could easily sweep those fiat transactions into *some* part of the BTC universe? (For example, a family of largeblock sidechains). To me the answer is clearly yes.\n\nUnlike layer1-largeblockism, no one running Bitcoin Core ever needs to see these 'btc' transactions (the same as we don't see them today, on account of them not existing at all); they do not burden Bitcoin Core full nodes. Hence why it seems like a good idea to me.\n\nAn SPV-wallet-of-a-largeblock-sidechain, is of course, a *disgrace* compared to a full-node-of-smallblock-mainchain-Bitcoin-Core. But, it is emphatically superior to Venmo / VISA or even \"custodial LN\". And certainly superior to nothing.\n\nPaul\n\n*https://www.youtube.com/watch?v=V3cvH2eWqfU\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220301/30a4d2aa/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-04T12:35:27",
                "message_text_only": "> \"these sidechains are terrible\" on Monday and then \"these sidechains are\nso good they will replace the mainchain\" on Tuesday\n\nYour premise is that a sidechain might come to dominate bitcoin, and that\nthis would be better than an altcoin dominating bitcoin. Did I\nmisunderstand you? Not quite sure why you're balking at me simply\nconfirming your premise.\n\n> sidechains cannot exist without their mainchain .. imagine .. a zcash\nsidechain, and someone claims they deposited 1000 BTC\n\nA sidechain could stop supporting deposits from or withdrawals to bitcoin\nand completely break any relationship with the main chain. I agree this is\nnot as sure of a thing as starting with an altcoin (which of course never\nhas that kind of relationship with bitcoin). So I do think there are some\nmerits to sidechains in your scenario. However, I don't think its quite\naccurate to say it completely solves the problem (of a less-secure altcoin\nbecoming dominant).\n\nYour anecdote about not running a full node is amusing, and I've often\nfound myself in that position. I certainly agree different people are\ndifferent and so different trade offs can be better for different\npeople. However,\nthe question is: what tradeoffs does a largeblock sidechain do better than\nboth eg Visa and lightning?\n\n>Wouldn't life be better, if we Bitcoiners could easily sweep those fiat transactions into *some* part of the BTC universe? (For example, a family of largeblock sidechains). To me the answer is clearly yes.\n\nI guess its not as clear to me. We agree it wouldn't significantly burden\nBitcoin-only nodes, but not being a burden is not a sufficient reason to do\nsomething, only reason to not prevent it. But what are the benefits to a\nuser of that chain? Slightly lower fees than main bitcoin? More\ndecentralization than Visa or Venmo? Doesn't lightning already do better on\nboth accounts?\n\n\n\nOn Tue, Mar 1, 2022 at 6:00 PM Paul Sztorc <truthcoin at gmail.com> wrote:\n\n> On 3/1/2022 12:39 AM, Billy Tetrud wrote:\n>\n> This entire issue is avoided completely, if all the chains --decentralized and centralized-- and in the same monetary unit. Then, the monetary network effects never interfere, and the decentralized chain is always guaranteed to exist.\n>\n> It sounds like what you're saying is that without side chains, everyone might switch entirely to some altcoin and bitcoin will basically die. And at that point, the insecurity of that coin people switched to can be heavily exploited by some attacker(s). Is that right?\n>\n> Yes, precisely.\n>\n> Its an interesting thought experiment. However, it leads me to wonder: if a sidechain gets so popular that it dominates the main chain, why would people keep that main chain around at all?\n>\n> For some reason, this is a very popular question. I suppose if you believe in \"one size fits all\" chain philosophy (see comment below), it makes sense to say \"these sidechains are terrible\" on Monday and then \"these sidechains are so good they will replace the mainchain\" on Tuesday.\n>\n> In any event, sidechains cannot exist without their mainchain (as I see it). For example, imagine that you are on a zcash sidechain, and someone claims they deposited 1000 BTC, from Bitcoin Core into this sidechain? Do you give them 1000 z-BTC, or not? Without the mainchain,\n> you can't tell.\n>\n> If you run the Bip300 DriveNet demo software (drivechain.info/releases), you will see for yourself: the test-sidechains are absolutely inert, UNTIL they have rpc access to the mainchain. (Exactly the same way that a LN node needs a Bitcoin Core node.)\n>\n>\n>\n> > someone is actually in the wrong, if they proactively censor an experiment of any type. If a creator is willing to stand behind something, then it should be tried.\n>\n> > it makes no difference if users have their funds stolen from a centralized Solana contract or from a bip300 centralized bit-Solana sidechain. I don't see why the tears shed would be any different.\n>\n> I agree with you. My point was not that we should stop anyone from doing this. My point was only that we shouldn't advocate for ideas we think aren't good. You were advocating for a \"largeblock sidechain\", and unless you have good reasons to think that is an idea likely to succeed and want to share them with us, then you shouldn't be advocating for that. But certainly if someone *does* think so and has their own reasons, I wouldn't want to censor or stop them. But I wouldn't advocate for them to do it unless their ideas were convincing to me, because I know enough to know the dangers of large block blockchains.\n>\n> Yes, I strongly agree, that we should only advocate for ideas we believe in.\n>\n> I do not believe in naive layer1 largeblockerism. But I do believe in sidechain largeblockism.\n>\n> Something funny once happened to me when I was on a Bitcoin conference panel*. There were three people: myself, a Blockstream person, and an (ex)BitPay person. The first two of us, were valiantly defending the small block position. I gave my usual speech: that node costs must remain low, so that people can run full nodes. The largeblocker mentioned that they ran many nodes (including BCH nodes etc) and didn't mind the cost, so I disclosed --in a good-natured way-- that I do not even run a BTC full node myself (out of choice). Thus, I was yammering about software I wasn't even running, I had no skin in the game! Lo and behold -- my Blockstream smallblocker ally-on-the-panel, immediately admitted to everyone that he did not run a full node either. The only node-runner was the largeblocker. The audience found this very amusing (as did I).\n>\n> We smallblockers, justified our sinful nodeless behavior, as follows (paraphrasing): we receive BTC mainly from people that we know (and have a long-term relationship with); our receipts are not time sensitive; we are not paid in BTC that often; if payments turned out to be forged we would have enormous recourse against our counterparties; etc.\n>\n> We did not run full nodes, because we did not need to draw on the blockchain's powers, **for those transactions**.\n>\n> Which is my point: people are different, and transactions are different. I make many transactions today, with VISA or Venmo. These are not censorship-resistant, but somehow I survive the month, without bursting into flames.\n>\n> Wouldn't life be better, if we Bitcoiners could easily sweep those fiat transactions into *some* part of the BTC universe? (For example, a family of largeblock sidechains). To me the answer is clearly yes.\n>\n> Unlike layer1-largeblockism, no one running Bitcoin Core ever needs to see these 'btc' transactions (the same as we don't see them today, on account of them not existing at all); they do not burden Bitcoin Core full nodes. Hence why it seems like a good idea to me.\n>\n> An SPV-wallet-of-a-largeblock-sidechain, is of course, a *disgrace* compared to a full-node-of-smallblock-mainchain-Bitcoin-Core. But, it is emphatically superior to Venmo / VISA or even \"custodial LN\". And certainly superior to nothing.\n>\n> Paul\n>\n> * https://www.youtube.com/watch?v=V3cvH2eWqfU\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220304/40846e4a/attachment-0001.html>"
            },
            {
                "author": "Paul Sztorc",
                "date": "2022-03-04T20:06:50",
                "message_text_only": "On 3/4/2022 7:35 AM, Billy Tetrud wrote:\n>> sidechains cannot exist without their mainchain ...\n> \n> A sidechain could stop supporting deposits from or withdrawals to \n> bitcoin and completely break any relationship with the main chain.\n> I agree this is not as sure of a thing as starting with an altcoin\n> (which of course never has that kind of relationship with bitcoin).\n> So I do think there are some merits to sidechains in your scenario.\n> However, I don't think its quite accurate to say it completely\n> solves the problem (of a less-secure altcoin becoming dominant).\n\n\nIt is hard to see how this \"sidechain cuts off the mainchain\" scenario \ncould plausibly be in enough people's interest:\n\n* Miners would lose the block subsidy (ie, the 6.25 BTC, or whatever of \nit that still remains), and txn fees from the mainchain and all other \nmerged mined chains.\n* Developers would lose the ability to create a dissenting new piece of \nsoftware (and would instead be forced into a permanent USSR-style \"one \nparty system\" intellectual monoculture).\n* Users would lose --permanently-- the ability to take their coins to \nnew blockchains, removing almost all of their leverage.\n\nFurthermore, because sidechains cannot exist without their parent (but \nnot vice-versa), we can expect a large permanent interest in keeping \nmainchain node costs low. Aka: very small mainchain blocks forever. So, \nthe shut-it-down mainchain-haters, would have to meet the question \"why \nnot just leave things the way they are?\". And the cheaper the \nmainchain-nodes are, the harder that question is to answer.\n\nHowever, if a sidechain really were so overwhelmingly popular as to \nclear all of these hurdles, then I would first want to understand why it \nis so popular. Maybe it is a good thing and we should cheer it on.\n\n\n> Your anecdote about not running a full node is amusing, and I've often \n> found myself in that position. I certainly agree different people are \n> different and so different trade offs can be better for different \n> people. However, the question is: what tradeoffs does a largeblock \n> sidechain do better than both eg Visa and lightning?\n\nYes, that's true. There are very many tradeoffs in general:\n\n1. Onboarding\n2. Route Capacity / Payment Limits\n3. Failed Payments\n4. Speed of Payment\n5. Receive while offline / need for interaction/monitoring/watchtowers\n6. Micropayments\n7. Types of fees charged, and for what\n8. Contribution to layer1 security budget\n9. Auditability (re: large organizations) / general complexity\n\nLN is certainly better for 4 and 6. But everything else is probably up \nfor grabs. And this is not intended to be an exhaustive list. I just \nmade it up now.\n\n(And, if the layer2 is harmless, then its existence can be justified via \none single net benefit, for some users, somewhere on the tradeoff-list.)\n\nPaul"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-04T08:42:12",
                "message_text_only": "Good morning vjudeu,\n\n> > Continuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n>\n> The sidechain can push zero bytes on-chain, just by placing a sidechain hash in OP_RETURN inside TapScript. Then, every sidechain node can check that \"this sidechain hash is connected with this Taproot address\", without pushing 32 bytes on-chain.\n\nThe Taproot address itself has to take up 32 bytes onchain, so this saves nothing.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-04T13:43:40",
                "message_text_only": "> The Taproot address itself has to take up 32 bytes onchain, so this saves nothing.\n\nThere is always at least one address, because you have a coinbase transaction and a solo miner or mining pool that is getting the whole reward. So, instead of using separate OP_RETURN's for each sidechain, for each federation, and for every \"commitment to the blockchain\", all we need is just tweaking that miner's key and placing everything inside unused TapScript. Then, we don't need separate 32 bytes for this and separate 32 bytes for that, we only need a commitment and a MAST-based path that can link such commitment to the address of this miner.\n\nSo, instead of having:\n\n<coinbasePubkey>\n<opReturn1>\n<opReturn2>\n...\n<opReturnN>\n\nWe could have:\n\n<tweakedCoinbasePubkey>\n\nOn 2022-03-04 09:42:23 user ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> Good morning vjudeu,\n\n> > Continuous operation of the sidechain then implies a constant stream of 32-byte commitments, whereas continuous operation of a channel factory, in the absence of membership set changes, has 0 bytes per block being published.\n>\n> The sidechain can push zero bytes on-chain, just by placing a sidechain hash in OP_RETURN inside TapScript. Then, every sidechain node can check that \"this sidechain hash is connected with this Taproot address\", without pushing 32 bytes on-chain.\n\nThe Taproot address itself has to take up 32 bytes onchain, so this saves nothing.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Recursive covenant opposition, or the absence thereof, was Re: TXHASH + CHECKSIGFROMSTACKVERIFY in lieu of CTV and ANYPREVOUT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "vjudeu at gazeta.pl",
                "Billy Tetrud",
                "Paul Sztorc"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 25471
        }
    },
    {
        "title": "[bitcoin-dev] Decentralized BIP 47 payment code directory",
        "thread_messages": [
            {
                "author": "Peter",
                "date": "2022-03-01T17:31:53",
                "message_text_only": "Hi,\n\nRegarding to BIP47 there's a newer version (v3 and v4) proposed here:\nhttps://github.com/OpenBitcoinPrivacyProject/rfc/blob/master/obpp-05.mediawiki\n\nThis newer version addresses some issues from v1.\n\nNow the notification from Alice to Bob is a transaction from Alice to Alice as a bare 1 of 3 multisig. The other 2 pubkeys represent Alice's payment code and Bob's payment identifier. Eliminating the toxic change issue.\n\nThe overhead is a one time 64 byte for the two pubkeys. This overhead would be amortized over the lifetime of the Alice / Bob relationship.\n\nAdditionally the first economic payment from Alice to Bob can be included along with the notification transaction.\n\nPayment codes are recoverable from the bip32 seed. No extra backups required.\n\nThis new version is in production with Samourai wallet.\n\nThis BIP47 v3 allows Alice to receive Bob's address without exposing her IP/identity to Charlie who can watch Alice receive the payment code material from Bob without knowing if Alice acted by sending a payment to Bob.\n\nAn xpub doesn't accomplish this because if you have your xpub in a crowdfunding platform the platform or anyone who hacks it can identify your payments. With the payment code you can associate yourself publicly with your payment code and no one (who is not the sender) will know if you received funds as your payment code is not visible in the blockchain.\n\nRegards\n\nPeter\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220301/9983095f/attachment-0001.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-03-02T04:45:58",
                "message_text_only": "Hi Peter,\n\n> Regarding to BIP47 there's a newer version (v3 and v4) proposed here: https://github.com/OpenBitcoinPrivacyProject/rfc/blob/master/obpp-05.mediawiki\n\n> Now the notification from Alice to Bob is a transaction from Alice to Alice as a bare 1 of 3 multisig. The other 2 pubkeys represent Alice's payment code and Bob's payment identifier. Eliminating the toxic change issue.\n\nThanks for sharing the link. Removing toxic change sounds good and certainly an improvement.\n\n> An xpub doesn't accomplish this because if you have your xpub in a crowdfunding platform the platform or anyone who hacks it can identify your payments. With the payment code you can associate yourself publicly with your payment code and no one (who is not the sender) will know if you received funds as your payment code is not visible in the blockchain.\n\nCrowdfunding platform can also be self hosted like BTCPay server. In this case XPUB is not shared with anyone as long as machine running BTCPay is secure. Problem with this setup is users need to setup BTCPay, running all the time and manage gap limit. No such thing is required in using BIP 47 payment codes.\n\nThere is also a [rust library][1] to use BIP 47 and I found the link on twitter yesterday. I think this can be helpful for application developers to implement BIP 47 payment codes in different bitcoin projects.\n\n[1]: https://github.com/rust-bitcoin/rust-bip47\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220302/ac2fb7e7/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Decentralized BIP 47 payment code directory",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "Peter"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3226
        }
    },
    {
        "title": "[bitcoin-dev] Wasabi Wallet 2.0 Testnet Release",
        "thread_messages": [
            {
                "author": "Max Hillebrand",
                "date": "2022-03-01T20:48:20",
                "message_text_only": "Hello list,\n\ntl;dr: we have been working on a little something, and Wasabi 2.0 is now \nready for your review and feedback.\n\nWasabi Wallet 2.0 is a Bitcoin wallet providing effortless privacy for \nits users. Just like Wasabi 1.0, this is achieved by default on the \nnetwork layer with a deep Tor integration, and on the synchronization \nlayer with BIP158 block filters or the packaged Bitcoin full node. \nHowever, 2.0 upgrades the privacy on the blockchain layer with a new \nWabisabi coinjoin implementation, running by default in the background.\n\nWabisabi is a drop-in replacement for the ZeroLink coinjoin coordination \nprotocol. Instead of Chaumian [or Schnorr] blind signatures, it uses \nkeyed verified anonymous credentials and Pedersen commitments. This \nenables anonymous DoS protection for centrally coordinated coinjoins \nwithout relying on equal amount outputs. This flexibility in the \ncoordination enables a more sophisticated amount decomposition, \nspecifically with standard denominations of low Hamming weight, in our \ncase powers of two, powers of three, and the preferred value series [1, \n2, 5]. In our simulations, this results often in \"changeless\" coinjoins \n[all outputs at least two anonymity set, aka count of equal value \noutputs] for transactions with more than 50 inputs. Whereas in Wasabi \n1.0 each user had to participate in the smallest standard denomination \nof 0.1 btc, now there is no mandatory output decomposition, and the \nminimum amount is 5000 sats. This is **substantial** block space \nsavings, reducing the amount of mining fees paid, and the time until the \nuser's utxo set is private.\n\nThanks to these efficiency improvements, we are now comformaking \ncoinjoin transactions the default in Wasabi's UX. As soon as bitcoin is \nreceived in the wallet, the client will register the confirmed coin as \ninput for the PSBT with the backend coordinator. Within a couple hours, \nthe user has numerous utxos which can be spent privately without \nrevealing their pre-mix transaction history. The resulting UX is simple: \nreceive, wait, spend. Privately. Effortless. For everyone.\n\nWhenever the user wants to spend bitcoin to an address, the wallet \nautomatically selects those private coins with sufficient sats, coin \ncontrol is displayed to the user. However, when the private balance is \ninsufficient to make the payment, the user has the option to adjust the \ncoin selection with the help of the previously provided contact labels. \nSince labeling is mandatory in Wasabi, we can abstract away the utxo \nconcept and display only the contact labels for the users to choose \nfrom. Wasabi also suggests the user to slightly adjust the payment \namount so as to avoid the creation of a change utxo, decreasing fees and \nimproving future privacy.\n\nToday, we are proud to finally reveal our work in progress in a public \npreview release with coinjoin on testnet. We kindly ask for your help \ntesting the completely new UI/UX, reviewing the cryptography and \ncoordination protocol, and especially coinjoining to analyze the \nresulting transaction graph in the wild.\n\nThank you to all contributors past and present!\n\nSkol\nMax Hillebrand\n\nDownload the testnet release: \nhttps://github.com/zkSNACKs/WalletWasabi/releases/tag/v1.98.0.0\n\nWebsite: https://wasabiwallet.io\nOnion: http://wasabiukrxmkdgve5kynjztuovbg43uxcbcxn6y2okcrsg7gb6jdmbad.onion\nTestnet coordinator: \nhttp://testwnp3fugjln6vh5vpj7mvq3lkqqwjj3c2aafyu7laxz42kgwh2rad.onion"
            },
            {
                "author": "nopara73",
                "date": "2022-03-01T22:50:24",
                "message_text_only": "The first Wasabi Wallet 2.0 testnet coinjoin with real users:\nhttps://blockstream.info/testnet/tx/68849dc71e6eb860b4b8aa3f57b9bc8178a002b54f85a46305bfaaad28b40444\n\nOn Tue, Mar 1, 2022 at 11:27 PM Max Hillebrand via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello list,\n>\n> tl;dr: we have been working on a little something, and Wasabi 2.0 is now\n> ready for your review and feedback.\n>\n> Wasabi Wallet 2.0 is a Bitcoin wallet providing effortless privacy for\n> its users. Just like Wasabi 1.0, this is achieved by default on the\n> network layer with a deep Tor integration, and on the synchronization\n> layer with BIP158 block filters or the packaged Bitcoin full node.\n> However, 2.0 upgrades the privacy on the blockchain layer with a new\n> Wabisabi coinjoin implementation, running by default in the background.\n>\n> Wabisabi is a drop-in replacement for the ZeroLink coinjoin coordination\n> protocol. Instead of Chaumian [or Schnorr] blind signatures, it uses\n> keyed verified anonymous credentials and Pedersen commitments. This\n> enables anonymous DoS protection for centrally coordinated coinjoins\n> without relying on equal amount outputs. This flexibility in the\n> coordination enables a more sophisticated amount decomposition,\n> specifically with standard denominations of low Hamming weight, in our\n> case powers of two, powers of three, and the preferred value series [1,\n> 2, 5]. In our simulations, this results often in \"changeless\" coinjoins\n> [all outputs at least two anonymity set, aka count of equal value\n> outputs] for transactions with more than 50 inputs. Whereas in Wasabi\n> 1.0 each user had to participate in the smallest standard denomination\n> of 0.1 btc, now there is no mandatory output decomposition, and the\n> minimum amount is 5000 sats. This is **substantial** block space\n> savings, reducing the amount of mining fees paid, and the time until the\n> user's utxo set is private.\n>\n> Thanks to these efficiency improvements, we are now comformaking\n> coinjoin transactions the default in Wasabi's UX. As soon as bitcoin is\n> received in the wallet, the client will register the confirmed coin as\n> input for the PSBT with the backend coordinator. Within a couple hours,\n> the user has numerous utxos which can be spent privately without\n> revealing their pre-mix transaction history. The resulting UX is simple:\n> receive, wait, spend. Privately. Effortless. For everyone.\n>\n> Whenever the user wants to spend bitcoin to an address, the wallet\n> automatically selects those private coins with sufficient sats, coin\n> control is displayed to the user. However, when the private balance is\n> insufficient to make the payment, the user has the option to adjust the\n> coin selection with the help of the previously provided contact labels.\n> Since labeling is mandatory in Wasabi, we can abstract away the utxo\n> concept and display only the contact labels for the users to choose\n> from. Wasabi also suggests the user to slightly adjust the payment\n> amount so as to avoid the creation of a change utxo, decreasing fees and\n> improving future privacy.\n>\n> Today, we are proud to finally reveal our work in progress in a public\n> preview release with coinjoin on testnet. We kindly ask for your help\n> testing the completely new UI/UX, reviewing the cryptography and\n> coordination protocol, and especially coinjoining to analyze the\n> resulting transaction graph in the wild.\n>\n> Thank you to all contributors past and present!\n>\n> Skol\n> Max Hillebrand\n>\n> Download the testnet release:\n> https://github.com/zkSNACKs/WalletWasabi/releases/tag/v1.98.0.0\n>\n> Website: https://wasabiwallet.io\n> Onion:\n> http://wasabiukrxmkdgve5kynjztuovbg43uxcbcxn6y2okcrsg7gb6jdmbad.onion\n> Testnet coordinator:\n> http://testwnp3fugjln6vh5vpj7mvq3lkqqwjj3c2aafyu7laxz42kgwh2rad.onion\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nBest,\n\u00c1d\u00e1m\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220301/4af073ec/attachment.html>"
            },
            {
                "author": "Prayank",
                "date": "2022-03-02T04:24:00",
                "message_text_only": "Hi Max,\n\n> Whenever the user wants to spend bitcoin to an address, the wallet automatically selects those private coins with sufficient sats, coin control is displayed to the user.\n\n1.There are no 'private' coins. Every coin is public in Bitcoin.\n\n2.Since, the wallet assumes some coins as 'private' based on certain things it can be misleading for the user. Privacy depends on the things users want to share with others.\n\n3.There is no coin control in Wasabi Wallet 2. \n\n> However, when the private balance is insufficient to make the payment, the user has the option to adjust the coin selection with the help of the previously provided contact labels.\n\nUser does not select coins because they are never shared with the user in the first place.\n\n[Selecting some labels][1] with misleading text 'who can see this transaction' does not look helpful.\n\n> Wasabi also suggests the user to slightly adjust the payment amount so as to avoid the creation of a change utxo, decreasing fees and improving future privacy.\n\nPrivacy involved in using a change or not using it is debatable. Not using a change address makes it easier to understand who might be the recipient in a transaction whereas using a change address same as other outputs would be difficult to analyze for possible recipients.\n\nWasabi wallet does not have different types of addresses to use for a change however [Bitcoin Core][2] recently made some related improvement which would improve privacy.\n\n> We kindly ask for your help testing the completely new UI/UX\n\nAs WW2 is not developed for power users (mentioned by developers working on Wasabi), I am not sure if bitcoin dev mailing list would be the best place to look for newbies. As far as issues are concerned, there are several things not fixed and shared in different GitHub issues or discussions. These include privacy, security and other things.\n\n\n[1]: https://i.imgur.com/Gxjmhau.png\n[2]: https://github.com/bitcoin/bitcoin/pull/23789\n\n\n-- \nPrayank\n\nA3B1 E430 2298 178F\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220302/3a93c6ec/attachment.html>"
            },
            {
                "author": "nopara73",
                "date": "2022-03-10T12:33:21",
                "message_text_only": ">  There is no coin control in Wasabi Wallet 2.\n\nThis is correct, but in and of itself can be misleading for those who know\nthat privacy in Bitcoin is near impossible without coin control, because\nthe conclusion would be then that Wasabi 2.0 ruined privacy for no reason,\nwhich is obviously not the case, in fact it improves it in many ways.\n\nThe idea is that you don't need coin control when you can make your\ntransaction with coinjoined coins. These coins are indistinguishable, so\nyou don't really have a use for coin control in that case. I think this is\nnon-controversial, but what about the case when you cannot make the tx from\ncoinjoined coins?\n\nIn that case there still is a mandatory privacy control, which is an\nimproved version of coin control. The insight here is that, in coin control\nsettings, users are differentiating between coins based on their labels.\nSince Wasabi creates label clusters, it is ok to select the clusters the\nuser wants to make the transaction from instead of individual coins. I know\nyou liked the never released cluster selection page before it got further\nimproved to be a privacy control page, but note the privacy control still\nuses the same insight, it just further removed unnecessary friction. That\nbeing said, coins can also be seen with this super secret developer key\ncombination: CTRL + D + C\n\n> User does not select coins because they are never shared with the user in\nthe first place.\n\nAs explained above it is selecting coins indirectly rather than directly.\nIt is selecting clusters of coins that are assumed to belong to the same\nwallet from an outside observer's point of view instead of individually\nselecting coins one by one.\n\n>  There are no 'private' coins. Every coin is public in Bitcoin.\n\nNot sure I'd like to engage in bikeshedding on terminology, but in my\nopinion this terminology is not only true, but also good and useful:\nOwnership of equalized coinjoin UTXOs is only known by the owner and not by\nexternal observers. The owner has control over who it reveals the ownership\nof these UTXOs. Privacy is your ability to selectively reveal yourself to\nthe world, therefore the terminology of \"private coins\" naturally makes\nsense and it's a useful differentiator from non-coinjoined coins.\n\n>  Since, the wallet assumes some coins as 'private' based on certain\nthings it can be misleading for the user. Privacy depends on the things\nusers want to share with others.\n\nThe wallet does not assume. The user assumes when selecting the anonymity\nlevels. The wallet works with the user's assumption of its threat model. If\na misleading claim can be made here then it's that the user misleads the\nwallet (and her/himself) rather than the other way around.\n\n>  Privacy involved in using a change or not using it is debatable. Not\nusing a change address makes it easier to understand who might be the\nrecipient in a transaction whereas using a change address same as other\noutputs would be difficult to analyze for possible recipients.\n\nAlthough I agree it's debatable, but for different reasons. I'd rather take\nan issue of its usefulness instead. About the assumption that it's easier\nto understand who might be the recipient, that's incorrect as the\ntransaction can easily be considered a self spend. In comparison to change\ngenerating transactions, there the change and the recipient can most of the\ntimes be established.\n\n>  Wasabi wallet does not have different types of addresses to use for a\nchange however [Bitcoin Core][2] recently made some related improvement\nwhich would improve privacy.\n\nYup. Unfortunately this is a hack to make the wallet feel like a light\nwallet as it greatly reduces the size of the client side filters we have.\nAlthough, as the blockchain grows further optimizations are needed. So it's\nnot very helpful if Bitcoin Core gives us 10 GB of filters so we can use\nall the types of addresses. We had a pull request to Core about creating\ncustom filters, but it was NACK-ed. In order to do this correctly and get\nmerged into Core we'd have to have a more comprehensive modification than\nour initial PR and that we have no resources to allocate to yet.\n\n>  As far as issues are concerned, there are several things not fixed and\nshared in different GitHub issues or discussions. These include privacy,\nsecurity and other things.\n\nI greatly disagree with this assessment, in fact, quite the opposite. Take\nfor example the tremendous activity your pull request about an empty catch\nblock received: https://github.com/zkSNACKs/WalletWasabi/pull/6791\nNo sane project would allow their best developers to spend more than 5\nminutes on this issue, yet 7 developers were discussing if leaving a single\nempty catch block in the code could be a potential security risk in the\nfuture and our resolution was actually contributing to NBitcoin to make\nsure we aren't getting an exception for incorrect password, but rather a\nboolean signal.\n\n>  As WW2 is not developed for power users (mentioned by developers working\non Wasabi), I am not sure if bitcoin dev mailing list would be the best\nplace to look for newbies.\n\nI do agree that the bitcoin-dev mailing list is not where the target users\nof Wasabi 2.0 are to be found, however Wasabi 2.0 is a great forward step\nof Bitcoin development and developers could certainly benefit from knowing\nabout great innovations it comes with.\n\nOn Wed, Mar 9, 2022 at 5:27 PM Prayank via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Max,\n>\n> > Whenever the user wants to spend bitcoin to an address, the wallet\n> automatically selects those private coins with sufficient sats, coin\n> control is displayed to the user.\n>\n> 1.There are no 'private' coins. Every coin is public in Bitcoin.\n>\n> 2.Since, the wallet assumes some coins as 'private' based on certain\n> things it can be misleading for the user. Privacy depends on the things\n> users want to share with others.\n>\n> 3.There is no coin control in Wasabi Wallet 2.\n>\n> > However, when the private balance is insufficient to make the payment,\n> the user has the option to adjust the coin selection with the help of the\n> previously provided contact labels.\n>\n> User does not select coins because they are never shared with the user in\n> the first place.\n>\n> [Selecting some labels][1] with misleading text 'who can see this\n> transaction' does not look helpful.\n>\n> > Wasabi also suggests the user to slightly adjust the payment amount so\n> as to avoid the creation of a change utxo, decreasing fees and improving\n> future privacy.\n>\n> Privacy involved in using a change or not using it is debatable. Not using\n> a change address makes it easier to understand who might be the recipient\n> in a transaction whereas using a change address same as other outputs would\n> be difficult to analyze for possible recipients.\n>\n> Wasabi wallet does not have different types of addresses to use for a\n> change however [Bitcoin Core][2] recently made some related improvement\n> which would improve privacy.\n>\n> > We kindly ask for your help testing the completely new UI/UX\n>\n> As WW2 is not developed for power users (mentioned by developers working\n> on Wasabi), I am not sure if bitcoin dev mailing list would be the best\n> place to look for newbies. As far as issues are concerned, there are\n> several things not fixed and shared in different GitHub issues or\n> discussions. These include privacy, security and other things.\n>\n>\n> [1]: https://i.imgur.com/Gxjmhau.png\n> [2]: https://github.com/bitcoin/bitcoin/pull/23789\n>\n>\n> --\n> Prayank\n>\n> A3B1 E430 2298 178F\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n-- \nBest,\n\u00c1d\u00e1m\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220310/15b3dc90/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Wasabi Wallet 2.0 Testnet Release",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Prayank",
                "nopara73",
                "Max Hillebrand"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 17771
        }
    },
    {
        "title": "[bitcoin-dev] Recurring bitcoin/LN payments using DLCs",
        "thread_messages": [
            {
                "author": "Chris Stewart",
                "date": "2022-03-03T12:58:55",
                "message_text_only": "DLCs are typically thought to be used for betting. Alice & Bob want to\nspeculate on an event, and have bitcoin payouts rewarded to them if they\nbet correctly. The oracle determines what event occurred and produces\nattestations representing that outcome.\n\nRecently I had a conversation with a friend about implementing recurring\nsubscriptions with Discreet Log Contracts. At a high level, you should\nthink about this working like ACH. If you are purchasing a subscription\nfrom Netflix, they will deduct $20 from your bank account every month. To\ndo this, you give them your credit card information.\n\nYou can do this with Discreet Log Contracts. It requires a slightly\nmodified DLC setup. Netflix would create an oracle representing a monthly\nsubscription. They require that users setup DLCs to them that will be\nexecuted at the end of the month. Alice, a subscriber to Netflix, creates a\nunilaterally funded DLC to Netflix. She creates adaptor signatures for her\npayment and sends them to Netflix.\n\nNo bitcoin transaction is required to create this subscription since the\nDLC is unilaterally funded. Alice can \u201ccancel\u201d the subscription at any time\nby spending from the utxo she is using to fund the DLC.\n\nAt the end of the month, Netflix attests that it is time to charge Alice\nfor her subscription. Netflix takes its own attestation and decrypts\nAlice\u2019s adaptor signature to get her signature to send funds to Netflix.\nNetflix publishes the settlement transaction for the DLC which pays Netflix\nit\u2019s subscription fee for the next month. Netflix also publishes a new\nannouncement for next month so that Alice can create a new DLC subscription.\n\nNetflix needs to give Alice a bitcoin address to pay to.\n\nThe information Alice is required to send Netflix is\n\n\n   1.\n\n   Her utxo used to fund the DLC\n   2.\n\n   Her adaptor signature representing her monthly subscription to netflix.\n\n\nNetflix must verify the adaptor signatures are correct and the utxo exists.\n\nWhy is this useful?\n\nIt's very convenient for a user to give access to withdraw a certain amount\nof money from a bank account at a given time in the future. This is how\nrecurring payments work in tradfi. This brings the same principle to\nbitcoin payments.\n\nDLCs also give you the power to specify how much the service can withdraw.\nFor instance, with Netflix, they shouldn\u2019t have the ability to withdraw\nthousands of dollars worth of bitcoin. The monthly service fee is $20. With\nDLCs, you can cryptographically enforce that they will only receive $20.\nThey cannot withdraw more or less money than they are authorized to.\n\nThere may be concerns about Netflix being both the oracle and the entity\nreceiving a monthly payment. I would argue this is mitigated by the fact\nthat the service provider could steal at most one months worth of service\nfees for users of the subscription. After users get scammed once, they will\ncancel their future subscription and distrust the service. The key feature\nis the amount of money in the subscription is predetermined, thus the\noracle cannot withdraw excess funds if they are evil.\n\n### QA\n\nDoes the DLC use a 2 of 2 multisig between Netflix and Alice?\n\nNo, the DLC is unilaterally funded by Alice. This allows her to create the\nsubscription without an onchain transaction, and also allows her to cancel\nthe subscription at any time. She cancels the subscription by double\nspending the utxo.\n\nCan Netflix steal all the money in the funding output?\n\nNo, Alice\u2019s adaptor signatures allow Netflix to withdraw a specific amount\nof bitcoin. The change is sent back to an address Alice controls. Both of\nthese outputs are protected by the adaptor signature.\n\nIs there a perverse incentive for Netflix to be the oracle and receive the\nsubscription?\n\nThe most Netflix can steal in this setup is one months worth of\nsubscription fees across the entire customer base. In this setup, Alice is\naccepting that risk for the convenience of auto withdrawals from her\nbitcoin wallet. Remember, Alice can cancel the subscription at any time she\nwants by spending from the funding utxo.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220303/874d520f/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-04T08:22:12",
                "message_text_only": "Good morning Chris,\n\nQuick question.\n\nHow does this improve over just handing over `nLockTime`d transactions?\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Chris Stewart",
                "date": "2022-03-05T14:45:56",
                "message_text_only": "Hey ZmnSCPxj,\n\nI thought about this for a few days and I think you are right. In the case\nof recurring payments this is identical to nLocktime. When doing recurring\npayments with this scheme, you probably want to rate limit subsequent UTXOs\n_with_ nlocktimes to make sure a malicious Netflix can't withdraw 12 month\nso of subscriptions by attesting with their oracle 12 times.\n\nI think this proposal describes arbitrary lines of pre-approved credit from\na bitcoin wallet. The line can be drawn down with oracle attestations. You\ncan mix in locktimes on these pre-approved lines of credit if you would\nlike to rate limit, or ignore rate limiting and allow the full utxo to be\nspent by the borrower. It really is contextual to the use case IMO.\n\n-Chris\n\nOn Fri, Mar 4, 2022 at 2:22 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n>\n> Good morning Chris,\n>\n> Quick question.\n>\n> How does this improve over just handing over `nLockTime`d transactions?\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/94c8f310/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-05T22:57:39",
                "message_text_only": "Good morning Chris,\n\n> I think this proposal describes arbitrary lines of pre-approved credit from a bitcoin wallet. The line can be drawn down with oracle attestations. You can mix in locktimes on these pre-approved lines of credit if you would like to rate limit, or ignore rate limiting and allow the full utxo to be spent by the borrower. It really is contextual to the use case IMO.\n\nAh, that seems more useful.\n\nHere is an example application that might benefit from this scheme:\n\nI am commissioning some work from some unbranded workperson.\nI do not know how long the work will take, and I do not trust the workperson to accurately tell me how complete the work is.\nHowever, both I and the workperson trust a branded third party (the oracle) who can judge the work for itself and determine if it is complete or not.\nSo I create a transaction whose signature can be completed only if the oracle releases a proper scalar and hand it over to the workperson.\nThen the workperson performs the work, then asks the oracle to judge if the work has been completed, and if so, the work can be compensated.\n\nOn the other hand, the above, where the oracle determines *when* the fund can be spent, can also be implemented by a simple 2-of-3, and called an \"escrow\".\nAfter all, the oracle attestation can be a partial signature as well, not just a scalar.\nIs there a better application for this scheme?\n\nI suppose if the oracle attestation is intended to be shared among multiple such transactions?\nThere may be multiple PTLCs, that are triggered by a single oracle?\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-06T00:14:51",
                "message_text_only": "This may be of interest:\n\nhttps://github.com/sapio-lang/sapio/blob/01830132bbbe39c3225e173e099f6e1a0611461c/sapio/examples/subscription.py\n\nBasically, a (old, python) sapio contract whereby you can make cancellable\nsubscriptions that are essentially a time based autopay scheme whereby\ncancellation gives time for the receiver to claim the correct amount of\nmoney.\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\nOn Sat, Mar 5, 2022 at 10:58 PM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Chris,\n>\n> > I think this proposal describes arbitrary lines of pre-approved credit\n> from a bitcoin wallet. The line can be drawn down with oracle attestations.\n> You can mix in locktimes on these pre-approved lines of credit if you would\n> like to rate limit, or ignore rate limiting and allow the full utxo to be\n> spent by the borrower. It really is contextual to the use case IMO.\n>\n> Ah, that seems more useful.\n>\n> Here is an example application that might benefit from this scheme:\n>\n> I am commissioning some work from some unbranded workperson.\n> I do not know how long the work will take, and I do not trust the\n> workperson to accurately tell me how complete the work is.\n> However, both I and the workperson trust a branded third party (the\n> oracle) who can judge the work for itself and determine if it is complete\n> or not.\n> So I create a transaction whose signature can be completed only if the\n> oracle releases a proper scalar and hand it over to the workperson.\n> Then the workperson performs the work, then asks the oracle to judge if\n> the work has been completed, and if so, the work can be compensated.\n>\n> On the other hand, the above, where the oracle determines *when* the fund\n> can be spent, can also be implemented by a simple 2-of-3, and called an\n> \"escrow\".\n> After all, the oracle attestation can be a partial signature as well, not\n> just a scalar.\n> Is there a better application for this scheme?\n>\n> I suppose if the oracle attestation is intended to be shared among\n> multiple such transactions?\n> There may be multiple PTLCs, that are triggered by a single oracle?\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/df48d359/attachment-0001.html>"
            },
            {
                "author": "Chris Stewart",
                "date": "2022-03-06T14:05:25",
                "message_text_only": ">On the other hand, the above, where the oracle determines *when* the fund\ncan be spent, can also be implemented by a simple 2-of-3, and called an\n\"escrow\".\n\nI think something that is underappreciated by protocol developers is the\nfact that multisig requires interactiveness at settlement time. The\nmultisig escrow provider needs to know the exact details about the bitcoin\ntransaction and needs to issue a signature (gotta sign the outpoints, the\nfee, the payout addresses etc).\n\nWith PTLCs that isn't the case, and thus gives a UX improvement for Alice &\nBob that are using the escrow provider. The oracle (or escrow) just issues\nattestations. Bob or Alice take those attestations and complete the adaptor\nsignature. Instead of a bi-directional communication requirement (the\noracle working with Bob or Alice to build the bitcoin tx) at settlement\ntime there is only unidirectional communication required. Non-interactive\nsettlement is one of the big selling points of DLC style applications IMO.\n\nOne of the unfortunate things about LN is the interactiveness requirements\nare very high, which makes developing applications hard (especially mobile\napplications). I don't think this solves lightning's problems, but it is a\nworthy goal to reduce interactiveness requirements with new bitcoin\napplications to give better UX.\n\n-Chris\n\nOn Sat, Mar 5, 2022 at 4:57 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Chris,\n>\n> > I think this proposal describes arbitrary lines of pre-approved credit\n> from a bitcoin wallet. The line can be drawn down with oracle attestations.\n> You can mix in locktimes on these pre-approved lines of credit if you would\n> like to rate limit, or ignore rate limiting and allow the full utxo to be\n> spent by the borrower. It really is contextual to the use case IMO.\n>\n> Ah, that seems more useful.\n>\n> Here is an example application that might benefit from this scheme:\n>\n> I am commissioning some work from some unbranded workperson.\n> I do not know how long the work will take, and I do not trust the\n> workperson to accurately tell me how complete the work is.\n> However, both I and the workperson trust a branded third party (the\n> oracle) who can judge the work for itself and determine if it is complete\n> or not.\n> So I create a transaction whose signature can be completed only if the\n> oracle releases a proper scalar and hand it over to the workperson.\n> Then the workperson performs the work, then asks the oracle to judge if\n> the work has been completed, and if so, the work can be compensated.\n>\n> On the other hand, the above, where the oracle determines *when* the fund\n> can be spent, can also be implemented by a simple 2-of-3, and called an\n> \"escrow\".\n> After all, the oracle attestation can be a partial signature as well, not\n> just a scalar.\n> Is there a better application for this scheme?\n>\n> I suppose if the oracle attestation is intended to be shared among\n> multiple such transactions?\n> There may be multiple PTLCs, that are triggered by a single oracle?\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/1ebd4af5/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-06T20:11:12",
                "message_text_only": "Good morning Chris,\n\n> >On the other hand, the above, where the oracle determines *when* the fund can be spent, can also be implemented by a simple 2-of-3, and called an \"escrow\".\n>\n> I think something that is underappreciated by protocol developers is the fact that multisig requires interactiveness at settlement time. The multisig escrow provider needs to know the exact details about the bitcoin transaction and needs to issue a signature (gotta sign the outpoints, the fee, the payout addresses etc).\n>\n> With PTLCs that isn't the case, and thus gives a UX improvement for Alice & Bob that are using the escrow provider. The oracle (or escrow) just issues attestations. Bob or Alice take those attestations and complete the adaptor signature. Instead of a bi-directional communication requirement (the oracle working with Bob or Alice to build the bitcoin tx) at settlement time there is only unidirectional communication required. Non-interactive settlement is one of the big selling points of DLC style applications IMO.\n>\n> One of the unfortunate things about LN is the interactiveness requirements are very high, which makes developing applications hard (especially mobile applications). I don't think this solves lightning's problems, but it is a worthy goal to reduce interactiveness requirements with new bitcoin applications to give better UX.\n\nGood point.\n\nI should note that 2-of-3 contracts are *not* transportable over LN, but PTLCs *are* transportable.\nSo the idea still has merit for LN, as a replacement for 2-fo-3 escrows.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Chris Stewart",
                "date": "2022-03-06T20:53:55",
                "message_text_only": "FWIW, the initial use case that I hinted at in the OP is for lightning.\n\nThe problem this company has is they offer an inbound liquidity service,\nbut it is common after a user purchases liquidity, the channel goes unused.\n\nThis is bad for the company as their liquidity is tied up in unproductive\nchannels. The idea was to implement a monthly service fee that requires the\nuser to pay a fixed amount if the channel isn\u2019t being used. This\ncompensates the company for the case where their liquidity is NOT being\nused. With standard lightning fees, you only get paid when liquidity is\nused. You don\u2019t get paid when it is NOT being used. If you are offering\nliquidity as a service this is bad.\n\nThe user purchasing liquidity can make the choice to pay the liquidity fee,\nor not to pay it. In the case where a user does not pay the fee, the\ncompany can take this as a signal that they are no longer interested in the\nservice. That way they can put their liquidity to use somewhere else that\nis more productive for the rest of the network.\n\nSo it\u2019s sort of a recurring payment for liquidity as a service, at least\nthat is how I\u2019m thinking about it currently.\n\nOn Sun, Mar 6, 2022 at 2:11 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Chris,\n>\n> > >On the other hand, the above, where the oracle determines *when* the\n> fund can be spent, can also be implemented by a simple 2-of-3, and called\n> an \"escrow\".\n> >\n> > I think something that is underappreciated by protocol developers is the\n> fact that multisig requires interactiveness at settlement time. The\n> multisig escrow provider needs to know the exact details about the bitcoin\n> transaction and needs to issue a signature (gotta sign the outpoints, the\n> fee, the payout addresses etc).\n> >\n> > With PTLCs that isn't the case, and thus gives a UX improvement for\n> Alice & Bob that are using the escrow provider. The oracle (or escrow) just\n> issues attestations. Bob or Alice take those attestations and complete the\n> adaptor signature. Instead of a bi-directional communication requirement\n> (the oracle working with Bob or Alice to build the bitcoin tx) at\n> settlement time there is only unidirectional communication required.\n> Non-interactive settlement is one of the big selling points of DLC style\n> applications IMO.\n> >\n> > One of the unfortunate things about LN is the interactiveness\n> requirements are very high, which makes developing applications hard\n> (especially mobile applications). I don't think this solves lightning's\n> problems, but it is a worthy goal to reduce interactiveness requirements\n> with new bitcoin applications to give better UX.\n>\n> Good point.\n>\n> I should note that 2-of-3 contracts are *not* transportable over LN, but\n> PTLCs *are* transportable.\n> So the idea still has merit for LN, as a replacement for 2-fo-3 escrows.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/f4d6b51c/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Recurring bitcoin/LN payments using DLCs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Chris Stewart",
                "Jeremy Rubin"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 17499
        }
    },
    {
        "title": "[bitcoin-dev] bitcoin scripting and lisp",
        "thread_messages": [
            {
                "author": "Anthony Towns",
                "date": "2022-03-04T01:04:42",
                "message_text_only": "On Sun, Feb 27, 2022 at 04:34:31PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> In reaction to this, AJ Towns mailed me privately about some of his\n> thoughts on this insane `OP_EVICT` proposal.\n> He observed that we could generalize the `OP_EVICT` opcode by\n> decomposing it into smaller parts, including an operation congruent\n> to the Scheme/Haskell/Scala `map` operation.\n\nAt much the same time Zman was thinking about OP_FOLD and in exactly the\nsame context, I was wondering what the simplest possible language that\nhad some sort of map construction was -- I mean simplest in a \"practical\nengineering\" sense; I think Simplicity already has the Euclidean/Peano\n\"least axioms\" sense covered.\n\nThe thing that's most appealing to me about bitcoin script as it stands\n(beyond \"it works\") is that it's really pretty simple in an engineering\nsense: it's just a \"forth\" like system, where you put byte strings on a\nstack and have a few operators to manipulate them.  The alt-stack, and\nsupporting \"IF\" and \"CODESEPARATOR\" add a little additional complexity,\nbut really not very much.\n\nTo level-up from that, instead of putting byte strings on a stack, you\ncould have some other data structure than a stack -- eg one that allows\nnesting. Simple ones that come to mind are lists of (lists of) byte\nstrings, or a binary tree of byte strings [0]. Both those essentially\ngive you a lisp-like language -- lisp is obviously all about lists,\nand a binary tree is just made of things or pairs of things, and pairs\nof things are just another way of saying \"car\" and \"cdr\".\n\nA particular advantage of lisp-like approaches is that they treat code\nand data exactly the same -- so if we're trying to leave the option open\nfor a transaction to supply some unexpected code on the witness stack,\nthen lisp handles that really naturally: you were going to include data\non the stack anyway, and code and data are the same, so you don't have\nto do anything special at all. And while I've never really coded in\nlisp at all, my understanding is that its biggest problems are all about\ndoing things efficiently at large scales -- but script's problem space\nis for very small scale things, so there's at least reason to hope that\nany problems lisp might have won't actually show up for this use case.\n\nSo, to me, that seemed like something worth looking into...\n\n\n\nAfter looking into it, I actually think chia lisp [1] gets pretty much all\nthe major design decisions pretty much right. There are obviously a few\nchanges needed given the differences in design between chia and bitcoin:\n\n - having secp256k1 signatures (and curve operations), instead of\n   BLS12-381 ones\n\n - adding tx introspection instead of having bundle-oriented CREATE_COIN,\n   and CREATE/ASSERT results [10]\n\nand there are a couple of other things that could maybe be improved\nupon:\n\n - serialization seems to be a bit verbose -- 100kB of serialized clvm\n   code from a random block gzips to 60kB; optimising the serialization\n   for small lists, and perhaps also for small literal numbers might be\n   a feasible improvement; though it's not clear to me how frequently\n   serialization size would be the limiting factor for cost versus\n   execution time or memory usage.\n\n - I don't think execution costing takes into account how much memory\n   is used at any one time, just how much was allocated in total; so\n   the equivalent of (OP_DUP OP_DROP OP_DUP OP_DROP ..) only has the\n   allocations accounted for, with no discount given for the immediate\n   freeing, so it gets treated as having the same cost as (OP_DUP\n   OP_DUP ..  OP_DROP OP_DROP ..). Doing it that way would be a worse\n   than how bitcoin script is currently costed, but doing better might\n   mean locking in an evaluation method at the consensus level. Seems\n   worth looking into, at least.\n\nBut otherwise, it seems a pretty good match.\n\n\n\nI think you'd need about 40 opcodes to match bitcoin script and (roughly)\nchia lisp, something like:\n\n   q                - quote\n   a                - apply\n   x                - exception / immediately fail (OP_RETURN style)\n   i                - if/then/else\n   softfork         - upgradability\n   not, all, any    - boolean logic\n   bitand, bitor, bitxor, bitnot, shift - bitwise logic\n   =                - bitwise equality\n   > - + * / divmod - (signed, bignum) arithmetic\n   ashift           - arithmetic shift (sign extended)\n   >s               - string comparison\n   strlen, substr, concat - string ops\n   f, r, c, l       - list ops (head, tail, make a list, is this a list?)\n   sha256           - hashing\n\n   numequal         - arithmetic equal, equivalent to (= (+ a 0) (+ b 0))\n   ripemd160, hash160, hash256 - more hashing\n   bip342-txmsg     - given a sighash byte, construct the bip342 message\n   bip340-verify    - given a pubkey, message, and signature bip340 verify it\n   tx               - get various information about the tx\n   taproot          - get merkle path/internalpubkey/program/annex information\n   ecdsa            - same as bip340-verify, except for traditional ecdsa?\n   secp256k1-muladd - given (a B C) where B,C are points, calculate a*B+C?\n\nThat compares to about 60 (non-disabled) opcodes in current script.\nPretty much all the opcodes in the first section are directly from chia\nlisp, while all the rest are to complete the \"bitcoin\" functionality.\nThe last two are extensions that are more food for thought than a real\nproposal.\n\n\n\nUsing a lisp-style approach seems an improvement in general to me.\nFor example, rather than the streaming-sha256 approach in Elements,\nwhere you could write:\n\n  \"a\" SHA256INITIALIZE\n  \"b\" SHA256UPDATE\n  \"c\" SHA256UPDATE\n  \"d\" SHA256FINALIZE\n\nto get the sha256 of \"abcd\" without having to CAT them first (important\nif they'd potentially overflow the 520B stack item limit), in chia lisp\nyou write:\n\n  (sha256 \"a\" \"b\" \"c\" \"d\")\n\nwhich still has the benefit of streaming the inputs into the function,\nbut only adds a single opcode, doesn't involve representing the internal\nsha256 midstate on the stack, and generally seems easier to understand,\nat least to me.\n\nAs another example, following the traditional functional \"tail recursion\ninstead of for-loops\" approach, doing CHECKMULTISIG might become\nsomething like:\n\n   (defun checksig (sig key)\n          bip340-verify (f sig) (bip342-txmsg (r sig)) key)\n\n   (defun checkmultisig (sigs keys k)\n          if (= k 0) \n\t     1\n\t     (if (l sigs)\n\t         (if (checksig (f sigs) (f keys))\n\t             (checkmultisig (r sigs) (r keys) (- k 1))\n\t\t     (checkmultisig sigs (r keys) k)\n                 )\n\t\t 0\n             )\n   )\n\nHere each \"sig\" is a pair of a 64B bip340 signature and a 1B sighash;\ninstead of a 65B string combining both, and sigs, keys are lists, and k\nis the number of successful signature checks you're requiring for\nsuccess.\n\nOf course, \"defun\" and \"if\" aren't listed as opcodes above; instead you\nhave a compiler that gives you nice macros like defun and translates them\ninto correct uses of the \"a\" opcode, etc. As I understand it, those sort\nof macros and translations are pretty well understood across lisp-like\nlanguages, and, of course, they're already implemented for chia lisp.\n\n\n\nI think with the \"tx\" opcode defined similarly to how Rusty suggested it\n[2] you could implement OP_CTV-like behaviours in similar way, and also\nreplace \"bip342-txmsg\" with your own code to generate SIGHASH_ANYPREVOUT\nor SIGHASH_GROUP style messages to sign. (This would mean also being able\nto pull information about the utxo being spent to obtain its amount and\nscriptpubkey, which are committed to wit ANYPREVOUT. If it was also able\nto obtain the \"is_coinbase\" flag, that might allow you a more accurate\ncovenant-based implementation of drivechains...)\n\nLikewise, with the \"taproot\" opcode defined in a way that lets you extract\nout the internal public key and merkle path, I think you could implement\nOP_TLUV and OP_EVICT with a similar recursive approach.\n\n\n\nThere's two ways to think about upgradability here; if someday we want\nto add new opcodes to the language -- perhaps something to validate zero\nknowledge proofs or calculate sha3 or use a different ECC curve, or some\nway to support cross-input signature aggregation, or perhaps it's just\nthat some snippets are very widely used and we'd like to code them in\nC++ directly so they validate quicker and don't use up as much block\nweight. One approach is to just define a new version of the language\nvia the tapleaf version, defining new opcodes however we like.\n\nThe other is to use the \"softfork\" opcode -- chia defines it as:\n\n  (softfork cost code)\n\nthough I think it would probably be better if it were \n\n  (softfork cost version code)\n\nwhere the idea is that \"code\" will use the \"x\" opcode if there's a\nproblem, and anyone supporting the \"version\" softfork can verify that\nthere aren't any problems at a cost of \"cost\". However, whether you\ndo or don't support that softfork, as far as the rest of the script is\nconcerned, the expression will either fail entirely or evaluate as zero;\nso anyone who doesn't support the softfork can just replace it with zero\nand continue on, treating it as if it had costed \"cost\" units.\n\nOne thing worth noting: \"softfork\" behaves more like OP_NOP than\ntapscript's OP_SUCCESS -- I think it's just not possible in general to\nhave OP_SUCCESS-like behaviour if you're trying to allow accepting code\nfrom the witness data -- otherwise as soon as you reveal that your script\ndoes accept arbitrary code supplied by the spender, someone could stick\nin an OP_SUCCESS code, and remove all the restrictions on spending and\nsteal your funds.\n\n\n\nTo me, it seems like chia lisp is a better answer to the problem here\nthan the Simplicity language. Simplicity is complicated in a few ways:\n\n - it defines over 100 jets (plus another 6 for sha3, and another 45 for\n   individual libsecp256k1 functions) that need to be implemented\n   natively to efficiently track consensus [3]\n\n - as far as I know, how to soft-fork in new jets isn't yet well\n   established. I think the ideal is that you just write everything in\n   raw simplicity, and either the interpreter has a jet and does things\n   quickly, or doesn't, and gets the same result much more slowly [4]. But\n   that approach doesn't seem compatible with maintaining consensus,\n   when \"slowly\" can be slower by more than 6 orders of magnitude [5].\n\n - to understand what's going on with a smart contract, you need to\n   understand both simplicity (to define the program) and the bit machine\n   (to follow how it's computed), both of which are fairly novel --\n   and if nobody's directly coding in simplicity which seems likely,\n   you're adding a third layer on top of that: you want to understand\n   what the programmer asked for (the source), what is included in the\n   transaction (the simplicity code) and how that's executed by consensus\n   code (via the bit machine).\n\nThere's a branch for enabling simplicity on elements/liquid [6]\nto see what enabling simplicity might involve in concrete terms; it\njust... doesn't seem at all simple in practice to me. I can't see how\nyou'd reasonably translate that simplicity code into a BIP series that\nanyone could understand, or how you'd do a thorough review of all the\nchanges...\n\nBy contrast, chia lisp has fewer opcodes than Simplicity's jets, has\nfeasible approaches to low-impact soft forks to increase functionality,\ncan be used with only two levels of abstraction (lisp with macros and\nthe opcodes-only vm level) that seem not too bad to understand, and\n(in my opinion) doesn't seem too hard to implement/maintain reasonably.\n\nOn the other hand, Simplicity's big advantage over *everything* else\nis in formal verification. But I'm not really seeing why you couldn't\npreserve that advantage by writing simplicity definitions for the \"lisp\"\nopcodes [7], so that you can \"compile\" the lisp programs to simplicity,\nand then verify them however you like.\n\n\n\nOne of the things people sometimes claim about bitcoin as an asset,\nis that it's got both the advantage of having been first to market,\nbut also that if some altcoin comes along with great new ideas, then\nthose ideas can just be incorporated into bitcoin too, so bitcoin can\npreserve it's lead even from innovators. Granted, I've only really been\nlooking at chia lisp for a bit over a week, but it really seems to me\nlike a case where it might be worth putting that philosophy into practice.\n\n\n\nIf we were to adopt this, obviously we shouldn't call it \"chia lisp\"\nanymore, since it wouldn't work the same in important ways. But since\nthe program would be encoded as a binary-tree of car/cdr pairs, maybe\nwe could call it \"binary-tree coded script\", or \"btc-script\" for short...\n\nPS: related tweets: [8]\n\nCheers,\naj\n\n[0] You could also allow things to be pushed onto the stack that\n    (recursively) can push things onto the stack -- the language \"Joy\"\n    takes this approach. It seems to end up equivalent to doing things\n    in a list oriented way to me.\n\n[1] https://chialisp.com/docs/ref/clvm\n\n[2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019871.html\n\n[3] https://raw.githubusercontent.com/ElementsProject/simplicity/pdf/Simplicity-TR.pdf\n\n[4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-November/015244.html\n    https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-October/015227.html\n    https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-October/015228.html\n\n[5] https://medium.com/blockstream/simplicity-jets-release-803db10fd589\n\n[6] https://github.com/ElementsProject/elements/compare/simplicity\n\n[7] At least I think so? chia lisp does multibyte math, that is \"+\"\n    can accept \"arbitrary\" length bytestrings, which it interprets as\n    numbers and adds together; whereas Simplicity requires finite types.\n    I think you could decide \"a program that only costs X can't have any\n    bytestrings greater than length k*X\" and construct a finite type up to\n    that length, maybe? So long as you're only using it for verification,\n    maybe that stays feasible? Or perhaps you could arbitrarily limit\n    the strings to a max of 520 bytes at a consensus level, and the\n    corresponding Simplicity types to 4160 bits and go from there?\n\n[8] https://twitter.com/brian_trollz/status/1499048316956549123\n    https://twitter.com/jb55/status/1499045998315724801\n\n[9] Oops, out of order footnotes. Anyway...\n\n[10] [9] The CREATE/ASSERT bundling stuff is interesting; and could be\n    used to achieve functionality like the \"transaction sponsorship\"\n    stuff. It doesn't magically solve the issues with maintaining the\n    mempool and using that to speed up block acceptance, though, and\n    the chia chain has apparently suffered from mempool-flooding attacks\n    recently [11] so I don't think they've solved the broader problem,\n    and thus I think it still makes more sense to stick with bitcoin's\n    current model here.\n\n[11] https://thechiaplot.net/2021/11/03/interview-with-the-chia-dust-stormer/\n     https://github.com/Chia-Network/post-mortem/blob/main/post-mortem.md"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-04T23:10:48",
                "message_text_only": "Good morning aj,\n\n> On Sun, Feb 27, 2022 at 04:34:31PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > In reaction to this, AJ Towns mailed me privately about some of his\n> > thoughts on this insane `OP_EVICT` proposal.\n> > He observed that we could generalize the `OP_EVICT` opcode by\n> > decomposing it into smaller parts, including an operation congruent\n> > to the Scheme/Haskell/Scala `map` operation.\n>\n> At much the same time Zman was thinking about OP_FOLD and in exactly the\n> same context, I was wondering what the simplest possible language that\n> had some sort of map construction was -- I mean simplest in a \"practical\n> engineering\" sense; I think Simplicity already has the Euclidean/Peano\n> \"least axioms\" sense covered.\n>\n> The thing that's most appealing to me about bitcoin script as it stands\n> (beyond \"it works\") is that it's really pretty simple in an engineering\n> sense: it's just a \"forth\" like system, where you put byte strings on a\n> stack and have a few operators to manipulate them. The alt-stack, and\n> supporting \"IF\" and \"CODESEPARATOR\" add a little additional complexity,\n> but really not very much.\n>\n> To level-up from that, instead of putting byte strings on a stack, you\n> could have some other data structure than a stack -- eg one that allows\n> nesting. Simple ones that come to mind are lists of (lists of) byte\n> strings, or a binary tree of byte strings [0]. Both those essentially\n> give you a lisp-like language -- lisp is obviously all about lists,\n> and a binary tree is just made of things or pairs of things, and pairs\n> of things are just another way of saying \"car\" and \"cdr\".\n>\n> A particular advantage of lisp-like approaches is that they treat code\n> and data exactly the same -- so if we're trying to leave the option open\n> for a transaction to supply some unexpected code on the witness stack,\n> then lisp handles that really naturally: you were going to include data\n> on the stack anyway, and code and data are the same, so you don't have\n> to do anything special at all. And while I've never really coded in\n> lisp at all, my understanding is that its biggest problems are all about\n> doing things efficiently at large scales -- but script's problem space\n> is for very small scale things, so there's at least reason to hope that\n> any problems lisp might have won't actually show up for this use case.\n\nI heartily endorse LISP --- it has a trivial implementation of `eval` that is easily implementable once you have defined a proper data type in preferred-language-here to represent LISP datums.\nCombine it with your idea of committing to a max-number-of-operations (which increases the weight of the transaction) and you may very well have something viable.\n(In particular, even though `eval` is traditionally (re-)implemented in LISP itself, the limit on max-number-of-operations means any `eval` implementation within the same language is also forcibly made total.)\n\nOf note is that the supposed \"problem at scale\" of LISP is, as I understand it, due precisely to its code and data being homoiconic to each other.\nThis homoiconicity greatly tempts LISP programmers to use macros, i.e. programs that generate other programs from some input syntax.\nHomoiconicity means that one can manipulate code just as easily as the data, and thus LISP macros are a trivial extension on the language.\nThis allows each LISP programmer to just code up a macro to expand common patterns.\nHowever, each LISP programmer then ends up implementing *different*, but *similar* macros from each other.\nUnfortunately, programming at scale requires multiple programmers speaking the same language.\nThen programming at scale is hampered because each LISP programmer has their own private dialect of LISP (formed from the common LISP language and from their own extensive set of private macros) and intercommunication between them is hindered by the fact that each one speaks their own private dialect.\nSome LISP-like languages (e.g. Scheme) have classically targeted a \"small\" subset of absolutely-necessary operations, and each implementation of the language immediately becomes a new dialect due to having slightly different forms for roughly the same convenience function or macro, and *then* individual programmers build their own private dialect on top.\nFor Scheme specifically, R7RS has targeted providing a \"large\" standard as well, as did R6RS (which only *had* a \"large\" standard), but individual Scheme implementations have not always liked to implement *all* the \"large\" standard.\n\nOtherwise, every big C program contains a half-assed implementation of half of Common LISP, so ----\n\n\n> -   I don't think execution costing takes into account how much memory\n>     is used at any one time, just how much was allocated in total; so\n>     the equivalent of (OP_DUP OP_DROP OP_DUP OP_DROP ..) only has the\n>     allocations accounted for, with no discount given for the immediate\n>     freeing, so it gets treated as having the same cost as (OP_DUP\n>     OP_DUP .. OP_DROP OP_DROP ..). Doing it that way would be a worse\n>     than how bitcoin script is currently costed, but doing better might\n>     mean locking in an evaluation method at the consensus level. Seems\n>     worth looking into, at least.\n\nThis may depend on the language that the interpreter is written in.\n\nFor example, on a typical GC language, both the N * `OP_DUP OP_DROP` and the N * `OP_DUP` + N * `OP_DROP` will have similar behavior when allocated at the nursery.\nSince the GC nursery acts as a large buffer of potential allocations, the amount of work done in both cases would be the same, at least until the number of allocs exceeds the nursery size.\n\nAlternately, the implementation may use immutable byte vectors, in which case `OP_DUP` is just a pointer copy.\nOr alternately the implementation may use copy-on-write byte vectors, in which case `OP_DUP` is just a pointer copy plus refcount increment, and `OP_DROP` is just a refcount decrement, and the amount of memory used remains small.\n\n\n\n\n>     There's two ways to think about upgradability here; if someday we want\n>     to add new opcodes to the language -- perhaps something to validate zero\n>     knowledge proofs or calculate sha3 or use a different ECC curve, or some\n>     way to support cross-input signature aggregation, or perhaps it's just\n>     that some snippets are very widely used and we'd like to code them in\n>     C++ directly so they validate quicker and don't use up as much block\n>     weight. One approach is to just define a new version of the language\n>     via the tapleaf version, defining new opcodes however we like.\n>\n>     The other is to use the \"softfork\" opcode -- chia defines it as:\n>\n>     (softfork cost code)\n>\n>     though I think it would probably be better if it were\n>\n>     (softfork cost version code)\n>\n>     where the idea is that \"code\" will use the \"x\" opcode if there's a\n>     problem, and anyone supporting the \"version\" softfork can verify that\n>     there aren't any problems at a cost of \"cost\". However, whether you\n>     do or don't support that softfork, as far as the rest of the script is\n>     concerned, the expression will either fail entirely or evaluate as zero;\n>     so anyone who doesn't support the softfork can just replace it with zero\n>     and continue on, treating it as if it had costed \"cost\" units.\n>\n>     One thing worth noting: \"softfork\" behaves more like OP_NOP than\n>     tapscript's OP_SUCCESS -- I think it's just not possible in general to\n>     have OP_SUCCESS-like behaviour if you're trying to allow accepting code\n>     from the witness data -- otherwise as soon as you reveal that your script\n>     does accept arbitrary code supplied by the spender, someone could stick\n>     in an OP_SUCCESS code, and remove all the restrictions on spending and\n>     steal your funds.\n\nOh no `1 RETURN`!\n\nWell, the advantage of chialisp here is that it enables the opcode for a *block* of code, so the opcode *itself* could return arbitrary data, it is just the entire block of code that is restricted to returning `0`.\nSo it would be something like an `OP_BEGINSOFTFORK .... OP_ENDSOFTFORK` where any reserved opcodes in the middle have arbitrary behavior, the entire block gets a *copy* of the current stack and alt stack, with any changes to the stack / alt stack disappear after the `OP_ENDSOFTFORK`.\nThus, the entire block as a whole would act as an `OP_NOP`.\n(OG Bitcoin SCRIPT FTW!)\n\nI think the `softfork` form would have to be a syntax though and not a procedure, as I think you want `cost` to be statically determined, and very likely also `version`.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-05T13:41:24",
                "message_text_only": "It seems like a decent concept for exploration.\n\nAJ, I'd be interested to know what you've been able to build with Chia Lisp\nand what your experience has been... e.g. what does the Lightning Network\nlook like on Chia?\n\n\nOne question that I have had is that it seems like to me that neither\nsimplicity nor chia lisp would be particularly suited to a ZK prover...\n\nWere that the explicit goal, it would seem that we could pretty easily\nadapt something like Cairo for Bitcoin transactions, and then we'd get a\nbig privacy benefit as well as enabling whatever programming paradigm you\nfind convenient (as it is compiled to a circuit verifier of some kind)...\n\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/60845e5d/attachment.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-03-05T20:10:11",
                "message_text_only": "On Sat, Mar 5, 2022 at 8:41 AM Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> It seems like a decent concept for exploration.\n>\n> AJ, I'd be interested to know what you've been able to build with Chia\n> Lisp and what your experience has been... e.g. what does the Lightning\n> Network look like on Chia?\n>\n>\n> One question that I have had is that it seems like to me that neither\n> simplicity nor chia lisp would be particularly suited to a ZK prover...\n>\n\nNot that I necessarily disagree with this statement, but I can say that I\nhave experimented with compiling Simplicity to Boolean circuits.  It was a\nwhile ago, but I think the result of compiling my SHA256 program was within\nan order of magnitude of the hand made SHA256 circuit for bulletproofs.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/d223d490/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-05T23:20:20",
                "message_text_only": "Good morning Russell,\n\n> On Sat, Mar 5, 2022 at 8:41 AM Jeremy Rubin via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> > It seems like a decent concept for exploration.\n> >\n> > AJ, I'd be interested to know what you've been able to build with Chia Lisp and what your experience has been... e.g. what does the Lightning Network look like on Chia?\n> >\n> > One question that I have had is that it seems like to me that neither simplicity nor chia lisp would be particularly suited to a ZK prover...\n>\n> Not that I necessarily disagree with this statement, but I can say that I have experimented with compiling Simplicity to Boolean circuits.\u00a0 It was a while ago, but I think the result of compiling my SHA256 program was within an order of magnitude of the hand made SHA256 circuit for bulletproofs.\n\n\"Within\" can mean \"larger\" or \"smaller\" in this context, which was it?\n>From what I understand, compilers for ZK-provable circuits are still not as effective as humans, so I would assume \"larger\", but I would be much interested if it is \"smaller\"!\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-03-06T02:09:45",
                "message_text_only": "The circuit generated from Simplicity was larger than the hand made one.\n\nOn Sat, Mar 5, 2022 at 6:20 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Russell,\n>\n> > On Sat, Mar 5, 2022 at 8:41 AM Jeremy Rubin via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > > It seems like a decent concept for exploration.\n> > >\n> > > AJ, I'd be interested to know what you've been able to build with Chia\n> Lisp and what your experience has been... e.g. what does the Lightning\n> Network look like on Chia?\n> > >\n> > > One question that I have had is that it seems like to me that neither\n> simplicity nor chia lisp would be particularly suited to a ZK prover...\n> >\n> > Not that I necessarily disagree with this statement, but I can say that\n> I have experimented with compiling Simplicity to Boolean circuits.  It was\n> a while ago, but I think the result of compiling my SHA256 program was\n> within an order of magnitude of the hand made SHA256 circuit for\n> bulletproofs.\n>\n> \"Within\" can mean \"larger\" or \"smaller\" in this context, which was it?\n> From what I understand, compilers for ZK-provable circuits are still not\n> as effective as humans, so I would assume \"larger\", but I would be much\n> interested if it is \"smaller\"!\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/b803a6b5/attachment.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-07T06:26:47",
                "message_text_only": ">\n> After looking into it, I actually think chia lisp [1] gets pretty much all\n> the major design decisions pretty much right. There are obviously a few\n> changes needed given the differences in design between chia and bitcoin:\n>\n>  - having secp256k1 signatures (and curve operations), instead of\n>    BLS12-381 ones\n>\n>  - adding tx introspection instead of having bundle-oriented CREATE_COIN,\n>    and CREATE/ASSERT results [10]\n>\n\nBitcoin uses the UTXO model as opposed to Chia's Coin Set model. While\nthese are close enough that it's often explained as Chia uses the UTXO\nmodel but that isn't technically true. Relevant to the above comment is\nthat in the UTXO model transactions get passed to a scriptpubkey and it\neither assert fails or it doesn't, while in the coin set model each puzzle\n(scriptpubkey) gets run and either assert fails or returns a list of extra\nconditions it has, possibly including timelocks and creating new coins,\npaying fees, and other things.\n\nIf you're doing everything from scratch it's cleaner to go with the coin\nset model, but retrofitting onto existing Bitcoin it may be best to leave\nthe UTXO model intact and compensate by adding a bunch more opcodes which\nare special to parsing Bitcoin transactions. The transaction format itself\ncan be mostly left alone but to enable some of the extra tricks (mostly\nimplementing capabilities) it's probably a good idea to make new\nconventions for how a transaction can have advisory information which\nspecifies which of the inputs to a transaction is the parent of a specific\noutput and also info which is used for communication between the UTXOs in a\ntransaction.\n\nBut one could also make lisp-generated UTXOs be based off transactions\nwhich look completely trivial and have all their important information be\nstored separately in a new vbytes area. That works but results in a bit of\na dual identity where some coins have both an old style id and a new style\nid which gunks up what\n\n\n>\n>  - serialization seems to be a bit verbose -- 100kB of serialized clvm\n>    code from a random block gzips to 60kB; optimising the serialization\n>    for small lists, and perhaps also for small literal numbers might be\n>    a feasible improvement; though it's not clear to me how frequently\n>    serialization size would be the limiting factor for cost versus\n>    execution time or memory usage.\n>\n\nA lot of this is because there's a hook for doing compression at the\nconsensus layer which isn't being used aggressively yet. That one has the\ndownside that the combined cost of transactions can add up very\nnonlinearly, but when you have constantly repeated bits of large\nboilerplate it gets close and there isn't much of an alternative. That said\neven with that form of compression maxxed out it's likely that gzip could\nstill do some compression but that would be better done in the database and\nin wire protocol formats rather than changing the format which is hashed at\nthe consensus layer.\n\n\n> Pretty much all the opcodes in the first section are directly from chia\n> lisp, while all the rest are to complete the \"bitcoin\" functionality.\n> The last two are extensions that are more food for thought than a real\n> proposal.\n>\n\nAre you thinking of this as a completely alternative script format or an\nextension to bitcoin script? They're radically different approaches and\nit's hard to see how they mix. Everything in lisp is completely sandboxed,\nand that functionality is important to a lot of things, and it's really\nnormal to be given a reveal of a scriptpubkey and be able to rely on your\nparsing of it.\n\n\n> There's two ways to think about upgradability here; if someday we want\n> to add new opcodes to the language -- perhaps something to validate zero\n> knowledge proofs or calculate sha3 or use a different ECC curve, or some\n> way to support cross-input signature aggregation, or perhaps it's just\n> that some snippets are very widely used and we'd like to code them in\n> C++ directly so they validate quicker and don't use up as much block\n> weight. One approach is to just define a new version of the language\n> via the tapleaf version, defining new opcodes however we like.\n>\n\nA nice side benefit of sticking with the UTXO model is that the soft fork\nhook can be that all unknown opcodes make the entire thing automatically\npass.\n\n\n>\n> The other is to use the \"softfork\" opcode -- chia defines it as:\n>\n>   (softfork cost code)\n>\n> though I think it would probably be better if it were\n>\n>   (softfork cost version code)\n>\n\nSince softfork has to date never been used that second parameter is\ntechnically completely ignored and could be anything at all. Most likely a\nconvention including some kind of version information will be created the\nfirst time it's used. Also Chia shoves total cost into blocks at the\nconsensus layer out of an abundance of caution although that isn't\ntechnically necessary.\n\n[10] [9] The CREATE/ASSERT bundling stuff is interesting; and could be\n>     used to achieve functionality like the \"transaction sponsorship\"\n>     stuff. It doesn't magically solve the issues with maintaining the\n>     mempool and using that to speed up block acceptance, though, and\n>     the chia chain has apparently suffered from mempool-flooding attacks\n>     recently [11] so I don't think they've solved the broader problem,\n>\n\nChia's approach to transaction fees is essentially identical to Bitcoin's\nalthough a lot fewer things in the ecosystem support fees due to a lack of\nhaving needed it yet. I don't think mempool issues have much to do with\nchoice of scriptpubkey language. which is mostly about adding in covenants\nand capabilities.\n\nThat said, Ethereum does have trivial aggregation of unrelated\ntransactions, and the expense of almost everything else. There are a bunch\nof ways automatic aggregation functionality could be added to coin set\nmempools by giving them some understanding of the semantics of some\ntransactions, but that hasn't been implemented yet.\n\nI previously posted some thoughts about this here:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019722.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/4ed797aa/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-07T22:56:38",
                "message_text_only": "Good morning Bram,\n\n> while in the coin set model each puzzle (scriptpubkey) gets run and either assert fails or returns a list of extra conditions it has, possibly including timelocks and creating new coins, paying fees, and other things.\n\nDoes this mean it basically gets recursive covenants?\nOr is a condition in this list of conditions written a more restrictive language which itself cannot return a list of conditions?\n\n\n> > \u00a0- serialization seems to be a bit verbose -- 100kB of serialized clvm\n> > \u00a0 \u00a0code from a random block gzips to 60kB; optimising the serialization\n> > \u00a0 \u00a0for small lists, and perhaps also for small literal numbers might be\n> > \u00a0 \u00a0a feasible improvement; though it's not clear to me how frequently\n> > \u00a0 \u00a0serialization size would be the limiting factor for cost versus\n> > \u00a0 \u00a0execution time or memory usage.\n>\n> A lot of this is because there's a hook for doing compression at the consensus layer which isn't being used aggressively yet. That one has the downside that the combined cost of transactions can add up very nonlinearly, but when you have constantly repeated bits of large boilerplate it gets close and there isn't much of an alternative. That said even with that form of compression maxxed out it's likely that gzip could still do some compression but that would be better done in the database and in wire protocol formats rather than changing the format which is hashed at the consensus layer.\n\nHow different is this from \"jets\" as proposed in Simplicity?\n\n> > Pretty much all the opcodes in the first section are directly from chia\n> > lisp, while all the rest are to complete the \"bitcoin\" functionality.\n> > The last two are extensions that are more food for thought than a real\n> > proposal.\n>\n> Are you thinking of this as a completely alternative script format or an extension to bitcoin script? They're radically different approaches and it's hard to see how they mix. Everything in lisp is completely sandboxed, and that functionality is important to a lot of things, and it's really normal to be given a reveal of a scriptpubkey and be able to rely on your parsing of it.\n\nI believe AJ is proposing a completely alternative format to OG Bitcoin SCRIPT.\nBasically, as I understand it, nothing in the design of Tapscript versions prevents us from completely changing the interpretation of Tapscript bytes, and use a completely different language.\nThat is, we could designate a new Tapscript version as completely different from OG Bitcoin SCRIPT.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-08T01:27:19",
                "message_text_only": "On Sun, Mar 06, 2022 at 10:26:47PM -0800, Bram Cohen via bitcoin-dev wrote:\n> > After looking into it, I actually think chia lisp [1] gets pretty much all\n> > the major design decisions pretty much right. There are obviously a few\n> > changes needed given the differences in design between chia and bitcoin:\n> Bitcoin uses the UTXO model as opposed to Chia's Coin Set model. While\n> these are close enough that it's often explained as Chia uses the UTXO\n> model but that isn't technically true. Relevant to the above comment is\n> that in the UTXO model transactions get passed to a scriptpubkey and it\n> either assert fails or it doesn't, while in the coin set model each puzzle\n> (scriptpubkey) gets run and either assert fails or returns a list of extra\n> conditions it has, possibly including timelocks and creating new coins,\n> paying fees, and other things.\n\nOne way to match the way bitcoin do things, you could have the \"list of\nextra conditions\" encoded explicitly in the transaction via the annex,\nand then check the extra conditions when the script is executed.\n\n> If you're doing everything from scratch it's cleaner to go with the coin\n> set model, but retrofitting onto existing Bitcoin it may be best to leave\n> the UTXO model intact and compensate by adding a bunch more opcodes which\n> are special to parsing Bitcoin transactions. The transaction format itself\n> can be mostly left alone but to enable some of the extra tricks (mostly\n> implementing capabilities) it's probably a good idea to make new\n> conventions for how a transaction can have advisory information which\n> specifies which of the inputs to a transaction is the parent of a specific\n> output and also info which is used for communication between the UTXOs in a\n> transaction.\n\nI think the parent/child coin relationship is only interesting when\n\"unrelated\" spends can assert that the child coin is being created -- ie\nthings along the lines of the \"transaction sponsorship\" proposal. My\nfeeling is that complicates the mempool a bit much, so is best left for\nlater, if done at all.\n\n(I think the hard part of managing the extra conditions is mostly\nin keeping it efficient to manage the mempool and construct the most\nprofitable blocks/bundles, rather than where the data goes)\n\n> But one could also make lisp-generated UTXOs be based off transactions\n> which look completely trivial and have all their important information be\n> stored separately in a new vbytes area. That works but results in a bit of\n> a dual identity where some coins have both an old style id and a new style\n> id which gunks up what\n\nWe've already got a txid and a wtxid, adding more ids seems best avoided\nif possible...\n\n> > Pretty much all the opcodes in the first section are directly from chia\n> > lisp, while all the rest are to complete the \"bitcoin\" functionality.\n> > The last two are extensions that are more food for thought than a real\n> > proposal.\n> Are you thinking of this as a completely alternative script format or an\n> extension to bitcoin script?\n\nAs an alternative to tapscript, so when constructing the merkle tree of\nscripts for a taproot address, you could have some of those scripts be\nin tapscript as it exists today with OP_CHECKSIG etc, and others could\nbe in lisp. (You could then have an entirely lisp-based sub-merkle-tree\nof lisp fragments via sha256tree or similar of course)\n\n> They're radically different approaches and\n> it's hard to see how they mix. Everything in lisp is completely sandboxed,\n> and that functionality is important to a lot of things, and it's really\n> normal to be given a reveal of a scriptpubkey and be able to rely on your\n> parsing of it.\n\nThe above prevents combining puzzles/solutions from multiple coin spends,\nbut I don't think that's very attractive in bitcoin's context, the way\nit is for chia. I don't think it loses much else?\n\n> > There's two ways to think about upgradability here; if someday we want\n> > to add new opcodes to the language -- perhaps something to validate zero\n> > knowledge proofs or calculate sha3 or use a different ECC curve, or some\n> > way to support cross-input signature aggregation, or perhaps it's just\n> > that some snippets are very widely used and we'd like to code them in\n> > C++ directly so they validate quicker and don't use up as much block\n> > weight. One approach is to just define a new version of the language\n> > via the tapleaf version, defining new opcodes however we like.\n> A nice side benefit of sticking with the UTXO model is that the soft fork\n> hook can be that all unknown opcodes make the entire thing automatically\n> pass.\n\nI don't think that works well if you want to allow the spender (the\npuzzle solution) to be able to use opcodes introduced in a soft-fork\n(eg, for graftroot-like behaviour)?\n\n> Chia's approach to transaction fees is essentially identical to Bitcoin's\n> although a lot fewer things in the ecosystem support fees due to a lack of\n> having needed it yet. I don't think mempool issues have much to do with\n> choice of scriptpubkey language. which is mostly about adding in covenants\n> and capabilities.\n\nHaving third parties be able to link their spends to yours complicates\nmempool behaviour a fair bit (see the discussions on pinning wrt\nlightning txs -- and that's only with direct participants being able to\nlink transactions). But it's very much a second-order effect compared\nto having fees being a meaningful thing at all. It took, what, six or\nseven years for people to start actually using dynamic fees in bitcoin?\n\n> I previously posted some thoughts about this here:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019722.html\n\nI'm pretty skeptical about having a database of large script snippets\nthat will hopefully be reused in the future.\n\nOn Mon, Mar 07, 2022 at 10:56:38PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> > while in the coin set model each puzzle (scriptpubkey) gets run and either assert fails or returns a list of extra conditions it has, possibly including timelocks and creating new coins, paying fees, and other things.\n> Does this mean it basically gets recursive covenants?\n\nIn chia the \"scriptPubKey\" is the hash of a lisp program, and when you\ncreate a new coin, the \"scriptPubKey\" of the newly generated coin is\nalso the output of a lisp program. So writing a quine gets you general\nrecursive covenants in a pretty straight forward way, as I understand it.\n\n> > > \u00a0- serialization seems to be a bit verbose -- 100kB of serialized clvm\n> > > \u00a0 \u00a0code from a random block gzips to 60kB; optimising the serialization\n> > > \u00a0 \u00a0for small lists, and perhaps also for small literal numbers might be\n> > > \u00a0 \u00a0a feasible improvement; though it's not clear to me how frequently\n> > > \u00a0 \u00a0serialization size would be the limiting factor for cost versus\n> > > \u00a0 \u00a0execution time or memory usage.\n> > A lot of this is because there's a hook for doing compression at the consensus layer which isn't being used aggressively yet. That one has the downside that the combined cost of transactions can add up very nonlinearly, but when you have constantly repeated bits of large boilerplate it gets close and there isn't much of an alternative. That said even with that form of compression maxxed out it's likely that gzip could still do some compression but that would be better done in the database and in wire protocol formats rather than changing the format which is hashed at the consensus layer.\n> How different is this from \"jets\" as proposed in Simplicity?\n\nRather than a \"transaction\" containing \"inputs/outputs\", chia has spend\nbundles that spend and create coins; and spend bundles can be merged\ntogether, so that a block only has a single spend bundle. That spend\nbundle joins all the puzzles (the programs that, when hashed match\nthe scriptPubKey) and solutions (scriptSigs) for the coins being spent\ntogether.\n\nI /think/ the compression hook would be to allow you to have the puzzles\nbe (re)generated via another lisp program if that was more efficient\nthan just listing them out. But I assume it would be turtles, err,\nlisp all the way down, no special C functions like with jets.\n\n> > > Pretty much all the opcodes in the first section are directly from chia\n> > > lisp, while all the rest are to complete the \"bitcoin\" functionality.\n> > > The last two are extensions that are more food for thought than a real\n> > > proposal.\n> >\n> > Are you thinking of this as a completely alternative script format or an extension to bitcoin script? They're radically different approaches and it's hard to see how they mix. Everything in lisp is completely sandboxed, and that functionality is important to a lot of things, and it's really normal to be given a reveal of a scriptpubkey and be able to rely on your parsing of it.\n> \n> I believe AJ is proposing a completely alternative format to OG Bitcoin SCRIPT.\n> Basically, as I understand it, nothing in the design of Tapscript versions prevents us from completely changing the interpretation of Tapscript bytes, and use a completely different language.\n> That is, we could designate a new Tapscript version as completely different from OG Bitcoin SCRIPT.\n\nBIP342 defines tapscript, and it's selected by the taproot leaf version\n0xc0; this hypothetical lispy \"btcscript\" might be selected via taproot\nleaf version 0xc2 or similar (elements' tapscript variant is selected\nvia version 0xc4).\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-08T03:06:43",
                "message_text_only": "Good morning aj et al.,\n\n\n> > They're radically different approaches and\n> > it's hard to see how they mix. Everything in lisp is completely sandboxed,\n> > and that functionality is important to a lot of things, and it's really\n> > normal to be given a reveal of a scriptpubkey and be able to rely on your\n> > parsing of it.\n>\n> The above prevents combining puzzles/solutions from multiple coin spends,\n> but I don't think that's very attractive in bitcoin's context, the way\n> it is for chia. I don't think it loses much else?\n\nBut cross-input signature aggregation is a nice-to-have we want for Bitcoin, and, to me, cross-input sigagg is not much different from cross-input puzzle/solution compression.\n\nFor example you might have multiple HTLCs, with mostly the same code except for details like who the acceptor and offerrer are, exact hash, and timelock, and you could claim multiple HTLCs in a single tx and feed the details separately but the code for the HTLC is common to all of the HTLCs.\nYou do not even need to come from the same protocol if multiple protocols use the same code for implementing HTLC.\n\n> > > There's two ways to think about upgradability here; if someday we want\n> > > to add new opcodes to the language -- perhaps something to validate zero\n> > > knowledge proofs or calculate sha3 or use a different ECC curve, or some\n> > > way to support cross-input signature aggregation, or perhaps it's just\n> > > that some snippets are very widely used and we'd like to code them in\n> > > C++ directly so they validate quicker and don't use up as much block\n> > > weight. One approach is to just define a new version of the language\n> > > via the tapleaf version, defining new opcodes however we like.\n> > > A nice side benefit of sticking with the UTXO model is that the soft fork\n> > > hook can be that all unknown opcodes make the entire thing automatically\n> > > pass.\n>\n> I don't think that works well if you want to allow the spender (the\n> puzzle solution) to be able to use opcodes introduced in a soft-fork\n> (eg, for graftroot-like behaviour)?\n\nThis does not apply to current Bitcoin since we no longer accept a SCRIPT from the spender, we now have a witness stack.\nHowever, once we introduce opcodes that allow recursive covenants, it seems this is now a potential footgun if the spender can tell the puzzle SCRIPT to load some code that will then be used in the *next* UTXO created, and *then* the spender can claim it.\n\nHmmm.... Or maybe not?\nIf the spender can already tell the puzzle SCRIPT to send the funds to a SCRIPT that is controlled by the spender, the spender can already tell the puzzle SCRIPT to forward the funds to a pubkey the spender controls.\nSo this seems to be more like \"do not write broken SCRIPTs\"?\n\n> > > > - serialization seems to be a bit verbose -- 100kB of serialized clvm\n> > > > \u00a0 \u00a0code from a random block gzips to 60kB; optimising the serialization\n> > > > \u00a0 \u00a0for small lists, and perhaps also for small literal numbers might be\n> > > > \u00a0 \u00a0a feasible improvement; though it's not clear to me how frequently\n> > > > \u00a0 \u00a0serialization size would be the limiting factor for cost versus\n> > > > \u00a0 \u00a0execution time or memory usage.\n> > > > A lot of this is because there's a hook for doing compression at the consensus layer which isn't being used aggressively yet. That one has the downside that the combined cost of transactions can add up very nonlinearly, but when you have constantly repeated bits of large boilerplate it gets close and there isn't much of an alternative. That said even with that form of compression maxxed out it's likely that gzip could still do some compression but that would be better done in the database and in wire protocol formats rather than changing the format which is hashed at the consensus layer.\n> > > > How different is this from \"jets\" as proposed in Simplicity?\n>\n> Rather than a \"transaction\" containing \"inputs/outputs\", chia has spend\n> bundles that spend and create coins; and spend bundles can be merged\n> together, so that a block only has a single spend bundle. That spend\n> bundle joins all the puzzles (the programs that, when hashed match\n> the scriptPubKey) and solutions (scriptSigs) for the coins being spent\n> together.\n>\n> I /think/ the compression hook would be to allow you to have the puzzles\n> be (re)generated via another lisp program if that was more efficient\n> than just listing them out. But I assume it would be turtles, err,\n> lisp all the way down, no special C functions like with jets.\n\nEh, you could use Common LISP or a recent-enough RnRS Scheme to write a cryptocurrency node software, so \"special C function\" seems to overprivilege C...\nI suppose the more proper way to think of this is that jets are *equivalent to* some code in the hosted language, and have an *efficient implementation* in the hosting language.\nIn this view, the current OG Bitcoin SCRIPT is the hosted language, and the C++ Bitcoin Core interpreter for it is in the hosting language C++.\n\nLISP can be both the hosted and hosting language because it is easy to implement `eval` in LISP and you can write macros which transform small code snippets into larger code.\nLISP code generating *more* LISP code is pretty much what macros are.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-09T03:07:51",
                "message_text_only": "On Mon, Mar 7, 2022 at 7:06 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n>\n> But cross-input signature aggregation is a nice-to-have we want for\n> Bitcoin, and, to me, cross-input sigagg is not much different from\n> cross-input puzzle/solution compression.\n>\n\nCross-input signature aggregation has a lot of headaches unless you're\nusing BLS signatures, in which case you always aggregate everything all the\ntime because it can be done after the fact noninteractively. In that case\nit makes sense to have a special aggregated signature which always comes\nwith a transaction or block. But it might be a bit much to bundle both lisp\nand BLS support into one big glop.\n\n\n\n>\n> For example you might have multiple HTLCs, with mostly the same code\n> except for details like who the acceptor and offerrer are, exact hash, and\n> timelock, and you could claim multiple HTLCs in a single tx and feed the\n> details separately but the code for the HTLC is common to all of the HTLCs.\n> You do not even need to come from the same protocol if multiple protocols\n> use the same code for implementing HTLC.\n>\n\nHTLCs, at least in Chia, have embarrassingly little code in them. Like, so\nlittle that there's almost nothing to compress.\n\n\n> This does not apply to current Bitcoin since we no longer accept a SCRIPT\n> from the spender, we now have a witness stack.\n>\n\nMy mental model of Bitcoin is to pretend that segwit was always there and\nthe separation of different sections of data is a semantic quibble.\n\n\n> So this seems to be more like \"do not write broken SCRIPTs\"?\n>\n\nIn general if people footgun that's their own fault. The resistance to\ncovenants and capabilities in the past has largely been around what would\nhappen if you had opt-out covenants which acted as riders and could monkey\naround in later spends which were none of their business. But if they're\nfully baked into the scriptpubkey then they're opted into by the recipient\nand there aren't any weird surprises.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220308/5137151d/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-09T14:30:34",
                "message_text_only": "Good morning Bram,\n\n> On Mon, Mar 7, 2022 at 7:06 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>\n> > But cross-input signature aggregation is a nice-to-have we want for Bitcoin, and, to me, cross-input sigagg is not much different from cross-input puzzle/solution compression.\n>\n> Cross-input signature aggregation has a lot of headaches unless you're using BLS signatures, in which case you always aggregate everything all the time because it can be done after the fact noninteractively. In that case it makes sense to have a special aggregated signature which always comes with a transaction or block. But it might be a bit much to bundle both lisp and BLS support into one big glop.\n\nYou misunderstand my point.\n\nI am not saying \"we should add sigagg and lisp together!\"\n\nI am pointing out that:\n\n* We want to save bytes by having multiple inputs of a transaction use the same single signature (i.e. sigagg).\n\nis not much different from:\n\n* We want to save bytes by having multiple inputs of a transaction use the same `scriptPubKey` template.\n\n> > For example you might have multiple HTLCs, with mostly the same code except for details like who the acceptor and offerrer are, exact hash, and timelock, and you could claim multiple HTLCs in a single tx and feed the details separately but the code for the HTLC is common to all of the HTLCs.\n> > You do not even need to come from the same protocol if multiple protocols use the same code for implementing HTLC.\n>\n> HTLCs, at least in Chia, have embarrassingly\u00a0little code in them. Like, so little that there's almost nothing to compress.\n\nIn Bitcoin at least an HTLC has, if you remove the `OP_PUSH`es, by my count, 13 bytes.\nIf you have a bunch of HTLCs you want to claim, you can reduce your witness data by 13 bytes minus whatever number of bytes you need to indicate this.\nThat amounts to about 3 vbytes per HTLC, which can be significant enough to be worth it (consider that Taproot moving away from encoded signatures saves only 9 weight units per signature, i.e. about 2 vbytes).\n\nDo note that PTLCs remain more space-efficient though, so forget about HTLCs and just use PTLCs.\n\n>\n> > This does not apply to current Bitcoin since we no longer accept a SCRIPT from the spender, we now have a witness stack.\n>\n> My mental model of Bitcoin is to pretend that segwit was always there and the separation of different sections of data is a semantic quibble.\n\nThis is not a semantic quibble --- `witness` contains only the equivalent of `OP_PUSH`es, while `scriptSig` can in theory contain non-`OP_PUSH` opcodes.\nxref. `1 RETURN`.\n\nAs-is, with SegWit the spender no longer is able to provide any SCRIPT at all, but new opcodes may allow the spender to effectively inject any SCRIPT they want, once again, because `witness` data may now become code.\n\n> But if they're fully baked into the scriptpubkey then they're opted into by the recipient and there aren't any weird surprises.\n\nThis is really what I kinda object to.\nYes, \"buyer beware\", but consider that as the covenant complexity increases, the probability of bugs, intentional or not, sneaking in, increases as well.\nAnd a bug is really \"a weird surprise\" --- xref TheDAO incident.\n\nThis makes me kinda wary of using such covenant features at all, and if stuff like `SIGHASH_ANYPREVOUT` or `OP_CHECKTEMPLATEVERIFY` are not added but must be reimplemented via a covenant feature, I would be saddened, as I now have to contend with the complexity of covenant features and carefully check that `SIGHASH_ANYPREVOUT`/`OP_CHECKTEMPLATEVERIFY` were implemented correctly.\nTrue I also still have to check the C++ source code if they are implemented directly as opcodes, but I can read C++ better than frikkin Bitcoin SCRIPT.\nNot to mention that I now have to review both the (more complicated due to more general) covenant feature implementation, *and* the implementation of `SIGHASH_ANYPREVOUT`/`OP_CHECKTEMPLATEVERIFY` in terms of the covenant feature.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-11T04:46:45",
                "message_text_only": "On Tue, Mar 08, 2022 at 03:06:43AM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> > > They're radically different approaches and\n> > > it's hard to see how they mix. Everything in lisp is completely sandboxed,\n> > > and that functionality is important to a lot of things, and it's really\n> > > normal to be given a reveal of a scriptpubkey and be able to rely on your\n> > > parsing of it.\n> > The above prevents combining puzzles/solutions from multiple coin spends,\n> > but I don't think that's very attractive in bitcoin's context, the way\n> > it is for chia. I don't think it loses much else?\n> But cross-input signature aggregation is a nice-to-have we want for Bitcoin, and, to me, cross-input sigagg is not much different from cross-input puzzle/solution compression.\n\nSignature aggregation has a lot more maths and crypto involved than\nreversible compression of puzzles/solutions. I was more meaning\ncross-transaction relationships rather than cross-input ones though.\n\n> > I /think/ the compression hook would be to allow you to have the puzzles\n> > be (re)generated via another lisp program if that was more efficient\n> > than just listing them out. But I assume it would be turtles, err,\n> > lisp all the way down, no special C functions like with jets.\n> Eh, you could use Common LISP or a recent-enough RnRS Scheme to write a cryptocurrency node software, so \"special C function\" seems to overprivilege C...\n\nJets are \"special\" in so far as they are costed differently at the\nconsensus level than the equivalent pure/jetless simplicity code that\nthey replace.  Whether they're written in C or something else isn't the\nimportant part.\n\nBy comparison, generating lisp code with lisp code in chia doesn't get\nspecial treatment.\n\n(You *could* also use jets in a way that doesn't impact consensus just\nto make your node software more efficient in the normal case -- perhaps\nvia a JIT compiler that sees common expressions in the blockchain and\noptimises them eg)\n\nOn Wed, Mar 09, 2022 at 02:30:34PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> Do note that PTLCs remain more space-efficient though, so forget about HTLCs and just use PTLCs.\n\nNote that PTLCs aren't really Chia-friendly, both because chia doesn't\nhave secp256k1 operations in the first place, but also because you can't\ndo a scriptless-script because the information you need to extract\nis lost when signatures are non-interactively aggregated via BLS --\nso that adds an expensive extra ECC operation rather than reusing an\nop you're already paying for (scriptless script PTLCs) or just adding\na cheap hash operation (HTLCs).\n\n(Pretty sure Chia could do (= PTLC (pubkey_for_exp PREIMAGE)) for\npreimage reveal of BLS PTLCs, but that wouldn't be compatible with\nbitcoin secp256k1 PTLCs. You could sha256 the PTLC to save a few bytes,\nbut I think given how much a sha256 opcode costs in Chia, that that\nwould actually be more expensive?)\n\nNone of that applies to a bitcoin implementation that doesn't switch to\nBLS signatures though.\n\n> > But if they're fully baked into the scriptpubkey then they're opted into by the recipient and there aren't any weird surprises.\n> This is really what I kinda object to.\n> Yes, \"buyer beware\", but consider that as the covenant complexity increases, the probability of bugs, intentional or not, sneaking in, increases as well.\n> And a bug is really \"a weird surprise\" --- xref TheDAO incident.\n\nWhich is better: a bug in the complicated script code specified for\nimplementing eltoo in a BOLT; or a bug in the BIP/implementation of a\nnew sighash feature designed to make it easy to implement eltoo, that's\nbeen soft-forked into consensus?\n\nSeems to me, that it's always better to have the bug be at the wallet\nlevel, since that can be fixed by upgrading individual wallet software.\n\n> This makes me kinda wary of using such covenant features at all, and if stuff like `SIGHASH_ANYPREVOUT` or `OP_CHECKTEMPLATEVERIFY` are not added but must be reimplemented via a covenant feature, I would be saddened, as I now have to contend with the complexity of covenant features and carefully check that `SIGHASH_ANYPREVOUT`/`OP_CHECKTEMPLATEVERIFY` were implemented correctly.\n> True I also still have to check the C++ source code if they are implemented directly as opcodes, but I can read C++ better than frikkin Bitcoin SCRIPT.\n\nIf OP_CHECKTEMPLATEVERIFY (etc) is implemented as a consensus update, you\nprobably want to review the C++ code even if you're not going to use it,\njust to make sure consensus doesn't end up broken as a result. Whereas if\nit's only used by other people's wallets, you might be able to ignore it\nentirely (at least until it becomes so common that any bugs might allow\na significant fraction of BTC to be stolen/lost and indirectly cause a\nsystemic risk).\n\n> Not to mention that I now have to review both the (more complicated due to more general) covenant feature implementation, *and* the implementation of `SIGHASH_ANYPREVOUT`/`OP_CHECKTEMPLATEVERIFY` in terms of the covenant feature.\n\nI'm not sure that a \"covenant language implementation\" would necessarily\nbe \"that\" complicated. And if so, having a DSL for covenants could,\nat least in theory, make for a much simpler implementation of\nANYPREVOUT/CTV/TLUV/EVICT/etc than doing it directly in C++, which\nmight mean those things are less likely to have \"weird surprises\" rather\nthan more.\n\nCheers,\naj"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-16T06:52:09",
                "message_text_only": "On Thu, Mar 10, 2022 at 8:46 PM Anthony Towns <aj at erisian.com.au> wrote:\n\n> Note that PTLCs aren't really Chia-friendly, both because chia doesn't\n> have secp256k1 operations in the first place, but also because you can't\n> do a scriptless-script because the information you need to extract\n> is lost when signatures are non-interactively aggregated via BLS --\n> so that adds an expensive extra ECC operation rather than reusing an\n> op you're already paying for (scriptless script PTLCs) or just adding\n> a cheap hash operation (HTLCs).\n>\n\nThe CLVM currently supports BLS12-381 group 1 point operations which it\nuses to support taproot which I think is enough to support PTLCs but\nobviously isn't compatible with secp. In the future there will likely be a\nsoft fork to include a complete set of BLS12-381 operations mostly to\nsupport ZK implementation.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220315/497a7250/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-16T14:54:05",
                "message_text_only": "Good morning aj et al.,\n\n> On Tue, Mar 08, 2022 at 03:06:43AM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > > > They're radically different approaches and\n> > > > it's hard to see how they mix. Everything in lisp is completely sandboxed,\n> > > > and that functionality is important to a lot of things, and it's really\n> > > > normal to be given a reveal of a scriptpubkey and be able to rely on your\n> > > > parsing of it.\n> > > > The above prevents combining puzzles/solutions from multiple coin spends,\n> > > > but I don't think that's very attractive in bitcoin's context, the way\n> > > > it is for chia. I don't think it loses much else?\n> > > > But cross-input signature aggregation is a nice-to-have we want for Bitcoin, and, to me, cross-input sigagg is not much different from cross-input puzzle/solution compression.\n>\n> Signature aggregation has a lot more maths and crypto involved than\n> reversible compression of puzzles/solutions. I was more meaning\n> cross-transaction relationships rather than cross-input ones though.\n\nMy point is that in the past we were willing to discuss the complicated crypto math around cross-input sigagg in order to save bytes, so it seems to me that cross-input compression of puzzles/solutions at least merits a discussion, since it would require a lot less heavy crypto math, and *also* save bytes.\n\n> > > I /think/ the compression hook would be to allow you to have the puzzles\n> > > be (re)generated via another lisp program if that was more efficient\n> > > than just listing them out. But I assume it would be turtles, err,\n> > > lisp all the way down, no special C functions like with jets.\n> > > Eh, you could use Common LISP or a recent-enough RnRS Scheme to write a cryptocurrency node software, so \"special C function\" seems to overprivilege C...\n>\n> Jets are \"special\" in so far as they are costed differently at the\n> consensus level than the equivalent pure/jetless simplicity code that\n> they replace. Whether they're written in C or something else isn't the\n> important part.\n>\n> By comparison, generating lisp code with lisp code in chia doesn't get\n> special treatment.\n\nHmm, what exactly do you mean here?\n\nIf I have a shorter piece of code that expands to a larger piece of code because metaprogramming, is it considered the same cost as the larger piece of code (even if not all parts of the larger piece of code are executed, e.g. branches)?\n\nOr is the cost simply proportional to the number of operations actually executed?\n\nI think there are two costs here:\n\n* Cost of bytes to transmit over the network.\n* Cost of CPU load.\n\nOver here in Bitcoin we have been mostly conflating the two, to the point that Taproot even eliminates unexecuted branches from being transmitted over the network so that bytes transmitted is approximately equal to opcodes executed.\n\nIt seems to me that lisp-generating-lisp compression would reduce the cost of bytes transmitted, but increase the CPU load (first the metaprogram runs, and *then* the produced program runs).\n\n> (You could also use jets in a way that doesn't impact consensus just\n> to make your node software more efficient in the normal case -- perhaps\n> via a JIT compiler that sees common expressions in the blockchain and\n> optimises them eg)\n\nI believe that is relevant in the other thread about Jets that I and Billy forked off from `OP_FOLD`?\n\n\nOver in that thread, we seem to have largely split jets into two types:\n\n* Consensus-critical jets which need a softfork but reduce the weight of the jetted code (and which are invisible to pre-softfork nodes).\n* Non-consensus-critical jets which only need relay change and reduces bytes sent, but keeps the weight of the jetted code.\n\nIt seems to me that lisp-generating-lisp compression would roughly fall into the \"non-consensus-critical jets\", roughly.\n\n\n> On Wed, Mar 09, 2022 at 02:30:34PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > Do note that PTLCs remain more space-efficient though, so forget about HTLCs and just use PTLCs.\n>\n> Note that PTLCs aren't really Chia-friendly, both because chia doesn't\n> have secp256k1 operations in the first place, but also because you can't\n> do a scriptless-script because the information you need to extract\n> is lost when signatures are non-interactively aggregated via BLS --\n> so that adds an expensive extra ECC operation rather than reusing an\n> op you're already paying for (scriptless script PTLCs) or just adding\n> a cheap hash operation (HTLCs).\n>\n> (Pretty sure Chia could do (= PTLC (pubkey_for_exp PREIMAGE)) for\n> preimage reveal of BLS PTLCs, but that wouldn't be compatible with\n> bitcoin secp256k1 PTLCs. You could sha256 the PTLC to save a few bytes,\n> but I think given how much a sha256 opcode costs in Chia, that that\n> would actually be more expensive?)\n>\n> None of that applies to a bitcoin implementation that doesn't switch to\n> BLS signatures though.\n\nNot being a mathist, I have absolutely no idea, but: at least as I understood from the original mimblewimble.txt from Voldemort, BLS signatures had an additional assumption, which I *think* means \"theoretically less secure than SECP256K1 Schnorr / ECDSA\".\nIs my understanding correct?\nAnd if so, how theoretical would that be?\n\nPTLC signatures have the very nice property of being indistinguishable from non-PTLC signatures to anyone without the adaptor, and I think privacy-by-default should be what we encourage.\n\n> > > But if they're fully baked into the scriptpubkey then they're opted into by the recipient and there aren't any weird surprises.\n> > > This is really what I kinda object to.\n> > > Yes, \"buyer beware\", but consider that as the covenant complexity increases, the probability of bugs, intentional or not, sneaking in, increases as well.\n> > > And a bug is really \"a weird surprise\" --- xref TheDAO incident.\n>\n> Which is better: a bug in the complicated script code specified for\n> implementing eltoo in a BOLT; or a bug in the BIP/implementation of a\n> new sighash feature designed to make it easy to implement eltoo, that's\n> been soft-forked into consensus?\n>\n> Seems to me, that it's always better to have the bug be at the wallet\n> level, since that can be fixed by upgrading individual wallet software.\n\nGood point.\n\nThough I should note that BIP-118 was originally proposed with a 5-line patch, so ---\n\n> I'm not sure that a \"covenant language implementation\" would necessarily\n> be \"that\" complicated. And if so, having a DSL for covenants could,\n> at least in theory, make for a much simpler implementation of\n> ANYPREVOUT/CTV/TLUV/EVICT/etc than doing it directly in C++, which\n> might mean those things are less likely to have \"weird surprises\" rather\n> than more.\n\n<rant>\nDSLs?\nDomain-specific languages?\n\nDo you know how many people hate autoconf?\nThat is because autoconf is secretly an embedded DSL in a really obscure language called `m4`.\nSome of the `autoconf` weirdnesses are due precisely to having to hack `m4` to make it look nicer, like that weird rule to use double `[[` and `]]` quotes around sections of program source code.\nYes, it means we can have a nice `autoconf-archive`, but the actual code inside that archive?\nPeople *making*`autoconf` macros have to learn both `m4` and the existing `autoconf` macro ecosystem.\n\nThen there is BluespecSV.\nBluespec used to be an embedded DSL inside Haskell.\nNobody wanted it because they had to learn *two* languages, Haskell, and Bluespec.\nEventually they created BluespecSV, which was a language with completely separate grammar and tokens from Haskell, instead of embedded in it, and Bluespec was finally *actually* used in production.\nBut the damage was done: people who do digital hardware design tend to *bristle* when they hear the word \"Haskell\", because of all the horrible embedded DSLs in Haskell (Bluespec was just one, but I have heard of a few others which never managed to jump from being more than a lab toy, including a cute one where the on-FPGA layout of the circuit was part of the construction of the circuit description).\n\nEmbedded DSLs are cute, but they require learning two languages, not a single new one.\nJust say no to embedded DSLs!\n</rant>\n\nAh, much better.\n\nThis seems to me to be not much different from adding a separate compiler, which translates from the surface language to the underlying opcode/lisp language, with similar risks: now you have *another* bit of indirection to audit.\nIt feels like building a perpetual-motion machine, where we keep adding more stuff in the hope of reducing the complexity.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-19T17:34:34",
                "message_text_only": "On Wed, Mar 16, 2022 at 7:54 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> My point is that in the past we were willing to discuss the complicated\n> crypto math around cross-input sigagg in order to save bytes, so it seems\n> to me that cross-input compression of puzzles/solutions at least merits a\n> discussion, since it would require a lot less heavy crypto math, and *also*\n> save bytes.\n>\n\nWhen using BLS signatures all that math is much simpler. You use a single\naggregated signature and always aggregate everything all the time.\n\nI think there are two costs here:\n>\n> * Cost of bytes to transmit over the network.\n> * Cost of CPU load.\n>\n\nThere are three potential costs: CPU, bytes, and making outputs. In Chia\nit's balanced so that the costs to a standard transaction in all three\nbuckets are roughly the same. In Bitcoin the three are implicitly tied to\neach other by design which makes vbytes work okayish for Bitcoin Script as\nit exists today.\n\n\n> It seems to me that lisp-generating-lisp compression would reduce the cost\n> of bytes transmitted, but increase the CPU load (first the metaprogram\n> runs, and *then* the produced program runs).\n>\n\nNah, CPU costs are dominated by signatures. Simple operations like applying\nsome parameters to a template don't add much.\n\n\n> Not being a mathist, I have absolutely no idea, but: at least as I\n> understood from the original mimblewimble.txt from Voldemort, BLS\n> signatures had an additional assumption, which I *think* means\n> \"theoretically less secure than SECP256K1 Schnorr / ECDSA\".\n> Is my understanding correct?\n> And if so, how theoretical would that be?\n>\n\nIt includes some an extra cryptographic assumption but it's extremely\ntheoretical, having more to do with guessing what size of group provides\ncomparable security in number of bits than whether the whole approach is in\nquestion. BLS12-381 is fairly conservative.\n\n\n>\n> PTLC signatures have the very nice property of being indistinguishable\n> from non-PTLC signatures to anyone without the adaptor, and I think\n> privacy-by-default should be what we encourage.\n>\n\nYou do lose out on that when you aggregate.\n\n\n> > I'm not sure that a \"covenant language implementation\" would necessarily\n> > be \"that\" complicated. And if so, having a DSL for covenants could,\n> > at least in theory, make for a much simpler implementation of\n> > ANYPREVOUT/CTV/TLUV/EVICT/etc than doing it directly in C++, which\n> > might mean those things are less likely to have \"weird surprises\" rather\n> > than more.\n>\n> <rant>\n> DSLs?\n> Domain-specific languages?\n>\n\nBitcoin Script is already a domain specific language, and the point of\nadding in a lisp-family language would be to make it so that covenants and\ncapabilities can be implemented in the same language as is used for regular\ncoin scripting. The idea is to get off the treadmill of soft forking in\nlanguage features every time new functionality is wanted and make it\npossible to implement all that on chain.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220319/9f946703/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-22T23:37:23",
                "message_text_only": "On Wed, Mar 16, 2022 at 02:54:05PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> My point is that in the past we were willing to discuss the complicated crypto math around cross-input sigagg in order to save bytes, so it seems to me that cross-input compression of puzzles/solutions at least merits a discussion, since it would require a lot less heavy crypto math, and *also* save bytes.\n\nMaybe it would be; but it's not something I was intending to bring up in\nthis thread.\n\nChia allows any coin spend to reference any output created in the\nsame block, and potentially any other input in the same block, and\nautomatically aggregates all signatures in a block; that's all pretty\nneat, but trying to do all that in bitcoin in step one doesn't seem smart.\n\n> > > > I /think/ the compression hook would be to allow you to have the puzzles\n> > > > be (re)generated via another lisp program if that was more efficient\n> > > > than just listing them out. But I assume it would be turtles, err,\n> > > > lisp all the way down, no special C functions like with jets.\n> > > > Eh, you could use Common LISP or a recent-enough RnRS Scheme to write a cryptocurrency node software, so \"special C function\" seems to overprivilege C...\n> > Jets are \"special\" in so far as they are costed differently at the\n> > consensus level than the equivalent pure/jetless simplicity code that\n> > they replace. Whether they're written in C or something else isn't the\n> > important part.\n> > By comparison, generating lisp code with lisp code in chia doesn't get\n> > special treatment.\n> Hmm, what exactly do you mean here?\n\nThis is going a bit into the weeds...\n\n> If I have a shorter piece of code that expands to a larger piece of code because metaprogramming, is it considered the same cost as the larger piece of code (even if not all parts of the larger piece of code are executed, e.g. branches)?\n\nChia looks at the problem differently to bitcoin. In bitcoin each\ntransaction includes a set of inputs, and each of those inputs contains\nboth a reference to a utxo which has a scriptPubKey, and a solution for\nthe scriptPubKey called the scriptSig. In chia, each block contains a\nlist of coins (~utxos) that are being spent, each of which has a hash\nof its puzzle (~scriptPubKey) which must be solved; each block then\ncontains a lisp program that will produce all the transaction info,\nnamely coin (~utxo id), puzzle reveal (~witness program) and solution\n(~witness stack); then to verify the block, you need to check the coins\nexist, the puzzle reveals all match the corresponding coin's puzzle,\nthat the puzzle+solution executes successfully, and that the assertions\nthat get returned by all the puzzle+solutions are all consistent.\n\n> Or is the cost simply proportional to the number of operations actually executed?\n\nAIUI, the cost is the sum of the size of the program, as well as how\nmuch compute and memory is used to run the program.\n\nIn comparison, the cost for an input with tapscript is the size of that\ninput; memory usage has a fixed maximum (1000 elements in the\nstack/altstack, and 520 bytes per element); and compute resources are\nlimited according to the size of the input.\n\n> It seems to me that lisp-generating-lisp compression would reduce the cost of bytes transmitted, but increase the CPU load (first the metaprogram runs, and *then* the produced program runs).\n\nIn chia, you're always running the metaprogram, it may just be that that\nprogram is the equivalent of:\n\n   stuff = lambda: [(\"hello\", \"world\"), (\"hello\", \"Z-man\")]\n\nwhich doesn't seem much better than just saying:\n\n   stuff = [(\"hello\", \"world\"), (\"hello\", \"Z-man\")]\n\nThe advantage is that you could construct a block template optimiser\nthat rewrites the program to:\n\n   def stuff():\n       h = \"hello\"\n       return [(h, \"world\"), (h, \"Z-man\")]\n\nwhich for large values of \"hello\" may be worthwhile (and the standard\npuzzle in chia is large enough at that that might well be worthwhile at\n~227 bytes, since it implements taproot/graftroot logic from scratch).\n\n> Over in that thread, we seem to have largely split jets into two types:\n> * Consensus-critical jets which need a softfork but reduce the weight of the jetted code (and which are invisible to pre-softfork nodes).\n> * Non-consensus-critical jets which only need relay change and reduces bytes sent, but keeps the weight of the jetted code.\n> It seems to me that lisp-generating-lisp compression would roughly fall into the \"non-consensus-critical jets\", roughly.\n\nIt could do; but the way it's used in chia is consensus-critical. \n\nI'm not 100% sure how costing works in chia, but I believe a block\ntemplate optimiser as above might allow miners to fit more transactions\nin their block and therefore collect more transaction fees. That makes\nthe block packing problem harder though, since it means your transaction\nis \"cheaper\" if it's more similar to other transactions in the block. I\ndon't think it's relevant today since fees seem to mostly be less than 1%\nof the block reward...\n\nThe ability to reference prior blocks might mitigate that; but that\ndepends on how those back references are costed, which is all way beyond\nmy knowledge.\n\n> > On Wed, Mar 09, 2022 at 02:30:34PM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> Not being a mathist, I have absolutely no idea, but: at least as I understood from the original mimblewimble.txt from Voldemort, BLS signatures had an additional assumption, which I *think* means \"theoretically less secure than SECP256K1 Schnorr / ECDSA\".\n> Is my understanding correct?\n> And if so, how theoretical would that be?\n\nLike everything else in crypto, it's completely theoretical until it\nstarts becoming practical?\n\n> PTLC signatures have the very nice property of being indistinguishable from non-PTLC signatures to anyone without the adaptor, and I think privacy-by-default should be what we encourage.\n\nIn bitcoin, you have a ~64B signature in every input, and hiding\na 32B secret in each of those is quite feasible if they're schnorr\nsignatures. When the block is published, 1000 different people can look\nat 1000 different signatures, and discover the 1000 different secrets\nthey wanted to know.\n\nIn chia, every signature in the block is aggregated, so there is only a\nsingle ~96B signature in each block, and there's no way to hide 32kB\nworth of secret information in there. I'm not sure of the maths, but I\nthink your options in chia and their costs would be roughly:\n\n  * normal tx with just agg signature, no lightning secrets = 1,200,000\n\n  * aggregated signature + hash preimage = 1,200,300 (HTLC)\n  * aggregated signature + point d.log = 2,526,946 (PTLC visible)\n  * manual disaggregated signature = 2,772,020 (PTLC hidden)\n\nBut your lightning preimage reveal doesn't look like a normal chia\ntransaction in any case.\n\n(Because chia's BLS12-381 curve differs from bitcoin's secp256k1,\nit's not even possible to reveal a secp256k1 PTLC preimage on chia, so\nyou couldn't share a single PTLC-based lightning networks even if you\nsolved the exchange rate problem. Well, I guess you could theoretically\nimplement secp256k1 maths from scratch in chia lisp...)\n\n> <rant>\n> DSLs?\n> Domain-specific languages?\n> Do you know how many people hate autoconf?\n\nUh, that seems like the sort of thing you type up then delete before\nsending...\n\n> This seems to me to be not much different from adding a separate\n> compiler, which translates from the surface language to the underlying\n> opcode/lisp language,\n\nNo, what I meant was the lisp/opcode language is the DSL.\n\nThough that said, there is a difference between chia lisp with macros\nand clvm code; p2_delegated_puzzle with macros looks like:\n\n(mod\n  (public_key delegated_puzzle delegated_puzzle_solution)\n  (include condition_codes.clvm)\n  ;; hash a tree\n  ;; This is used to calculate a puzzle hash given a puzzle program.\n  (defun sha256tree1\n         (TREE)\n         (if (l TREE)\n             (sha256 2 (sha256tree1 (f TREE)) (sha256tree1 (r TREE)))\n             (sha256 1 TREE)\n         )\n  )\n  (c (list AGG_SIG_ME public_key (sha256tree1 delegated_puzzle))\n    (a delegated_puzzle delegated_puzzle_solution))\n)\n\nbut as clvm code, it looks like:\n\n(a (q 4 (c 4 (c 5 (c (a 6 (c 2 (c 11 ()))) ()))) (a 11 23)) (c (q 50 2 (i (l 5) (q 11 (q . 2) (a 6 (c 2 (c 9 ()))) (a 6 (c 2 (c 13 ())))) (q 11 (q . 1) 5)) 1) 1))\n\nI don't think you want to include code comments in the blockchain though,\nso at some level I guess \"compiling\" is unavoidable.\n\nCheers,\naj"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-16T06:40:51",
                "message_text_only": "On Wed, Mar 9, 2022 at 6:30 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> I am pointing out that:\n>\n> * We want to save bytes by having multiple inputs of a transaction use the\n> same single signature (i.e. sigagg).\n>\n> is not much different from:\n>\n> * We want to save bytes by having multiple inputs of a transaction use the\n> same `scriptPubKey` template.\n>\n\nFair point. In the past Bitcoin has been resistant to such things because\nfor example reusing pubkeys can save you from having to separately pay for\nthe reveals of all of them but letting people get credit for that\nincentivizes key reuse which isn't such a great thing.\n\n\n>\n> > > For example you might have multiple HTLCs, with mostly the same code\n> except for details like who the acceptor and offerrer are, exact hash, and\n> timelock, and you could claim multiple HTLCs in a single tx and feed the\n> details separately but the code for the HTLC is common to all of the HTLCs.\n> > > You do not even need to come from the same protocol if multiple\n> protocols use the same code for implementing HTLC.\n> >\n> > HTLCs, at least in Chia, have embarrassingly little code in them. Like,\n> so little that there's almost nothing to compress.\n>\n> In Bitcoin at least an HTLC has, if you remove the `OP_PUSH`es, by my\n> count, 13 bytes.\n> If you have a bunch of HTLCs you want to claim, you can reduce your\n> witness data by 13 bytes minus whatever number of bytes you need to\n> indicate this.\n> That amounts to about 3 vbytes per HTLC, which can be significant enough\n> to be worth it (consider that Taproot moving away from encoded signatures\n> saves only 9 weight units per signature, i.e. about 2 vbytes).\n>\n\nOh I see. That's already extremely small overhead. When you start\noptimizing at that level you wind up doing things like pulling all the\nHTLCs into the same block to take the overhead of pulling in the template\nonly once.\n\n\n>\n> Do note that PTLCs remain more space-efficient though, so forget about\n> HTLCs and just use PTLCs.\n>\n\nIt makes a lot of sense to make a payment channel system using PTLCs and\neltoo right off the bat but then you wind up rewriting everything from\nscratch.\n\n\n> > > This does not apply to current Bitcoin since we no longer accept a\n> SCRIPT from the spender, we now have a witness stack.\n> >\n> > My mental model of Bitcoin is to pretend that segwit was always there\n> and the separation of different sections of data is a semantic quibble.\n>\n> This is not a semantic quibble --- `witness` contains only the equivalent\n> of `OP_PUSH`es, while `scriptSig` can in theory contain non-`OP_PUSH`\n> opcodes.\n> xref. `1 RETURN`.\n>\n\nIt's very normal when you're using lisp for snippets of code to be passed\nin as data and then verified and executed. That's enabled by the extreme\nadherence to no side effects.\n\n\n> This makes me kinda wary of using such covenant features at all, and if\n> stuff like `SIGHASH_ANYPREVOUT` or `OP_CHECKTEMPLATEVERIFY` are not added\n> but must be reimplemented via a covenant feature, I would be saddened, as I\n> now have to contend with the complexity of covenant features and carefully\n> check that `SIGHASH_ANYPREVOUT`/`OP_CHECKTEMPLATEVERIFY` were implemented\n> correctly.\n>\n\nEven the 'standard format' transaction which supports taproot and graftroot\nis implemented in CLVM. The benefit of this approach is that new\nfunctionality can be implemented and deployed immediately rather than\nhaving to painstakingly go through a soft fork deployment for each thing.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220315/eeccf5c8/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-16T15:09:15",
                "message_text_only": "Good morning Bram,\n\n> On Wed, Mar 9, 2022 at 6:30 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n>\n> > I am pointing out that:\n> >\n> > * We want to save bytes by having multiple inputs of a transaction use the same single signature (i.e. sigagg).\n> >\n> > is not much different from:\n> >\n> > * We want to save bytes by having multiple inputs of a transaction use the same `scriptPubKey` template.\n>\n> Fair point. In the past Bitcoin has been resistant to such things because for example reusing pubkeys can save you from having to separately pay for the reveals of all of them but letting people get credit for that incentivizes key reuse which isn't such a great thing.\n\nSee paragraph below:\n\n> > > > For example you might have multiple HTLCs, with mostly the same code except for details like who the acceptor and offerrer are, exact hash, and timelock, and you could claim multiple HTLCs in a single tx and feed the details separately but the code for the HTLC is common to all of the HTLCs.\n> > > > You do not even need to come from the same protocol if multiple protocols use the same code for implementing HTLC.\n\nNote that the acceptor and offerrer are represented by pubkeys here.\nSo we do not want to encourage key reuse, we want to encourage reuse of *how* the pubkeys are used (but rotate the pubkeys).\n\nIn the other thread on Jets in bitcoin-dev I proposed moving data like pubkeys into a separate part of the SCRIPT in order to (1) not encourage key reuse and (2) make it easier to compress the code.\nIn LISP terms, it would be like requiring that top-level code have a `(let ...)` form around it where the assigned data *must* be constants or `quote`, and disallowing constants and `quote` elsewhere, then any generated LISP code has to execute in the same top-level environment defined by this top-level `let`.\n\nSo you can compress the code by using some metaprogramming where LISP generates LISP code but you still need to work within the confines of the available constants.\n\n> > > HTLCs, at least in Chia, have embarrassingly\u00a0little code in them. Like, so little that there's almost nothing to compress.\n> >\n> > In Bitcoin at least an HTLC has, if you remove the `OP_PUSH`es, by my count, 13 bytes.\n> > If you have a bunch of HTLCs you want to claim, you can reduce your witness data by 13 bytes minus whatever number of bytes you need to indicate this.\n> > That amounts to about 3 vbytes per HTLC, which can be significant enough to be worth it (consider that Taproot moving away from encoded signatures saves only 9 weight units per signature, i.e. about 2 vbytes).\n>\n> Oh I see. That's already extremely small overhead. When you start optimizing at that level you wind up doing things like pulling all the HTLCs into the same block to take the overhead of pulling in the template only once.\n> \u00a0\n>\n> > Do note that PTLCs remain more space-efficient though, so forget about HTLCs and just use PTLCs.\n>\n> It makes a lot of sense to make a payment channel system using PTLCs and eltoo right off the bat but then you wind up rewriting everything from scratch.\n\nBunch of #reckless devs implemented Lightning with just HTLCs so that is that, *shrug*, gotta wonder what those people were thinking, not waiting for PTLCs.\n\n> \u00a0\n>\n> > > > This does not apply to current Bitcoin since we no longer accept a SCRIPT from the spender, we now have a witness stack.\n> > >\n> > > My mental model of Bitcoin is to pretend that segwit was always there and the separation of different sections of data is a semantic quibble.\n> >\n> > This is not a semantic quibble --- `witness` contains only the equivalent of `OP_PUSH`es, while `scriptSig` can in theory contain non-`OP_PUSH` opcodes.\n> > xref. `1 RETURN`.\n>\n> It's very normal when you're using lisp for snippets of code to be passed in as data and then verified and executed. That's enabled by the extreme adherence to no side effects.\n\nQuining still allows Turing-completeness and infinite loops, which *is* still a side effect, though as I understand it ChiaLISP uses the \"Turing-complete but with a max number of ops\" kind of totality.\n\n> > This makes me kinda wary of using such covenant features at all, and if stuff like `SIGHASH_ANYPREVOUT` or `OP_CHECKTEMPLATEVERIFY` are not added but must be reimplemented via a covenant feature, I would be saddened, as I now have to contend with the complexity of covenant features and carefully check that `SIGHASH_ANYPREVOUT`/`OP_CHECKTEMPLATEVERIFY` were implemented correctly.\n>\n> Even the 'standard format' transaction which supports taproot and graftroot is implemented in CLVM. The benefit of this approach is that new functionality can be implemented and deployed immediately rather than having to painstakingly go through a soft fork deployment for each thing.\n\nWow, just wow.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-09T02:54:56",
                "message_text_only": "On Mon, Mar 7, 2022 at 5:27 PM Anthony Towns <aj at erisian.com.au> wrote:\n\n> One way to match the way bitcoin do things, you could have the \"list of\n> extra conditions\" encoded explicitly in the transaction via the annex,\n> and then check the extra conditions when the script is executed.\n>\n\nThe conditions are already basically what's in transactions. I think the\nonly thing missing is the assertion about one's own id, which could be\nadded in by, in addition to passing the scriptpubkey the transaction it's\npart of, also passing in the index of inputs which it itself is.\n\n\n>\n> > If you're doing everything from scratch it's cleaner to go with the coin\n> > set model, but retrofitting onto existing Bitcoin it may be best to leave\n> > the UTXO model intact and compensate by adding a bunch more opcodes which\n> > are special to parsing Bitcoin transactions. The transaction format\n> itself\n> > can be mostly left alone but to enable some of the extra tricks (mostly\n> > implementing capabilities) it's probably a good idea to make new\n> > conventions for how a transaction can have advisory information which\n> > specifies which of the inputs to a transaction is the parent of a\n> specific\n> > output and also info which is used for communication between the UTXOs\n> in a\n> > transaction.\n>\n> I think the parent/child coin relationship is only interesting when\n> \"unrelated\" spends can assert that the child coin is being created -- ie\n> things along the lines of the \"transaction sponsorship\" proposal. My\n> feeling is that complicates the mempool a bit much, so is best left for\n> later, if done at all.\n>\n\nThe parent/child relationship is mostly about implementing capabilities.\nThere's this fundamental trick, sort of the ollie of UTXO programming,\nwhere to make a coin have a capability you have a wrapper around an inner\npuzzle for it where the wrapper asserts 'my parent must either be the\nunique originator of this capability or also have this same wrapper' and it\nenforces that by being given a reveal of its parent and told/asserting its\nown id which it can derive from that parent. The main benefit of the coin\nset approach over UTXO is that it reduces the amount of stuff to be\nrevealed and string mangling involved in the parentage check.\n\n\n>\n> (I think the hard part of managing the extra conditions is mostly\n> in keeping it efficient to manage the mempool and construct the most\n> profitable blocks/bundles, rather than where the data goes)\n>\n\nNot sure what you mean by this. Conditions map fairly closely with what's\nin Bitcoin transactions and are designed so to be monotonic and so the\ncosts and fees are known up front. The only way two transactions can\nconflict with each other is if they both try to spend the same coin.\n\n\n> > They're radically different approaches and\n> > it's hard to see how they mix. Everything in lisp is completely\n> sandboxed,\n> > and that functionality is important to a lot of things, and it's really\n> > normal to be given a reveal of a scriptpubkey and be able to rely on your\n> > parsing of it.\n>\n> The above prevents combining puzzles/solutions from multiple coin spends,\n> but I don't think that's very attractive in bitcoin's context, the way\n> it is for chia. I don't think it loses much else?\n>\n\nMaking something lisp-based be a completely alternative script type would\nalso be my preferred approach.\n\n\n> > A nice side benefit of sticking with the UTXO model is that the soft fork\n> > hook can be that all unknown opcodes make the entire thing automatically\n> > pass.\n>\n> I don't think that works well if you want to allow the spender (the\n> puzzle solution) to be able to use opcodes introduced in a soft-fork\n> (eg, for graftroot-like behaviour)?\n>\n\nThis is already the approach to soft forking in Bitcoin script and I don't\nsee anything wrong with it. You shouldn't write scripts using previously\nunrecognized opcodes until after they've been soft forked into having real\nfunctionality, and if you do that and accidentally write an anyonecanspend\nthat's your own fault.\n\n\n> Having third parties be able to link their spends to yours complicates\n> mempool behaviour a fair bit (see the discussions on pinning wrt\n> lightning txs -- and that's only with direct participants being able to\n> link transactions).\n\n\nBitcoin already has that with spending of transaction outputs, and Chia's\nmempool doesn't currently let transactions depend on other transactions in\nthe mempool. If you do have that sort of dependency, you should have to\nsmush both transactions together to make a single larger transaction and\nmake it have enough of a fee to replace the smaller one.\n\nI'm pretty skeptical about having a database of large script snippets\n> that will hopefully be reused in the future.\n>\n\nThat database is just the list of old blocks which can be dredged up to\nhave code pulled out of them.\n\n\n> In chia the \"scriptPubKey\" is the hash of a lisp program, and when you\n> create a new coin, the \"scriptPubKey\" of the newly generated coin is\n> also the output of a lisp program. So writing a quine gets you general\n> recursive covenants in a pretty straight forward way, as I understand it.\n>\n\nUsually you don't quite write a quine because you can be passed in your own\ncode and assert your own id derived from it, but that's the basic idea. You\nneed to validate your own id anyway when you have a capability.\n\n\n> Rather than a \"transaction\" containing \"inputs/outputs\", chia has spend\n> bundles that spend and create coins; and spend bundles can be merged\n> together, so that a block only has a single spend bundle. That spend\n> bundle joins all the puzzles (the programs that, when hashed match\n> the scriptPubKey) and solutions (scriptSigs) for the coins being spent\n> together.\n>\n\nI often refer to spend bundles as 'transactions'. Hope that isn't too\nconfusing. They serve the same function in the mempool.\n\n\n>\n> I /think/ the compression hook would be to allow you to have the puzzles\n> be (re)generated via another lisp program if that was more efficient\n> than just listing them out.\n\n\nIt literally has a lisp program called the generator which returns the list\nof puzzle reveals and transactions. The simplest version of that program is\nto return a quoted list.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220308/14bf1b2c/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-10T06:47:17",
                "message_text_only": "On Tue, Mar 08, 2022 at 06:54:56PM -0800, Bram Cohen via bitcoin-dev wrote:\n> On Mon, Mar 7, 2022 at 5:27 PM Anthony Towns <aj at erisian.com.au> wrote:\n> > One way to match the way bitcoin do things, you could have the \"list of\n> > extra conditions\" encoded explicitly in the transaction via the annex,\n> > and then check the extra conditions when the script is executed.\n> The conditions are already basically what's in transactions. I think the\n> only thing missing is the assertion about one's own id, which could be\n> added in by, in addition to passing the scriptpubkey the transaction it's\n> part of, also passing in the index of inputs which it itself is.\n\nTo redo the singleton pattern in bitcoin's context, I think you'd have\nto pass in both the full tx you're spending (to be able to get the\ntxid of its parent) and the full tx of its parent (to be able to get\nthe scriptPubKey that your utxo spent) which seems klunky but at least\npossible (you'd be able to drop the witness data at least; without that\nevery tx would be including the entire history of the singleton).\n\n> > > A nice side benefit of sticking with the UTXO model is that the soft fork\n> > > hook can be that all unknown opcodes make the entire thing automatically\n> > > pass.\n> > I don't think that works well if you want to allow the spender (the\n> > puzzle solution) to be able to use opcodes introduced in a soft-fork\n> > (eg, for graftroot-like behaviour)?\n> This is already the approach to soft forking in Bitcoin script and I don't\n> see anything wrong with it.\n\nIt's fine in Bitcoin script, because the scriptPubKey already commits to\nall the opcodes that can possibly be used for any particular output. With\na lisp approach, however, you could pass in additional code fragments\nto execute. For example, where you currently say:\n\n  script: [pubkey] CHECKSIG\n  witness: [64B signature][0x83]\n\n(where 0x83 is SINGLE|ANYONECANPAY) you might translate that to:\n\n  script: (checksig pubkey (bip342-txmsg 3) 2)\n  witness: signature 0x83\n\nwhere \"3\" grabs the sighash byte, and \"2\" grabs the signature. But you\ncould also translate it to:\n\n  script: (checksig pubkey (sha256 3 (a 3)) 2)\n  witness: signature (bip342-txmsg 0x83)\n\nwhere \"a 3\" takes \"(bip342-txmsg 0x83)\" then evaluates it, and (sha256\n3 (a 3)) makes sure you've signed off on both how the message was\nconstructed as well as what the message was. The advantage there is that\nthe spender can then create their own signature hashes however they like;\neven ones that hadn't been thought of when the output was created.\n\nBut what if we later softfork in a bip118-txmsg for quick and easy\nANYPREVOUT style-signatures, and want to use that instead of custom\nlisp code? You can't just stick (softfork C (bip118-txmsg 0xc3)) into\nthe witness, because it will evaluate to nil and you won't be signing\nanything. But you *could* change the script to something like:\n\n  script: (softfork C (q checksigverify pubkey (a 3) 2))\n  witness: signature (bip118-txmsg 0xc3)\n\nBut what happens if the witness instead has:\n\n  script: (softfork C (q checksigverify pubkey (a 3) 2))\n  witness: fake-signature (fakeopcode 0xff)\n\nIf softfork is just doing a best effort for whatever opcodes it knows\nabout, and otherwise succeeding, then it has to succeed, and your\nscript/output has become anyone-can-spend.\n\nOn the other hand, if you could tell the softfork op that you only wanted\nops up-to-and-including the 118 softfork, then it could reject fakeopcode\nand fail the script, which I think gives the desirable behaviour.\n\nCheers,\naj"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-16T06:45:48",
                "message_text_only": "On Wed, Mar 9, 2022 at 10:47 PM Anthony Towns <aj at erisian.com.au> wrote:\n\n>\n> To redo the singleton pattern in bitcoin's context, I think you'd have\n> to pass in both the full tx you're spending (to be able to get the\n> txid of its parent) and the full tx of its parent (to be able to get\n> the scriptPubKey that your utxo spent) which seems klunky but at least\n> possible (you'd be able to drop the witness data at least; without that\n> every tx would be including the entire history of the singleton).\n>\n\nYes that's the idea. Since the parent transaction is in the blockchain it\ncould be pulled in automatically without having to charge vbytes for it.\n\n\n> If softfork is just doing a best effort for whatever opcodes it knows\n> about, and otherwise succeeding, then it has to succeed, and your\n> script/output has become anyone-can-spend.\n>\n\nThat can be alleviated by when things call untrusted code they can wrap it\nin a guard which can be the soft fork opcode itself.\n\n\n>\n> On the other hand, if you could tell the softfork op that you only wanted\n> ops up-to-and-including the 118 softfork, then it could reject fakeopcode\n> and fail the script, which I think gives the desirable behaviour.\n>\n\nA simple approach to versioning like that may be more expedient. Soft\nforking in CLVM isn't implemented yet.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220315/ccdf755f/attachment.html>"
            },
            {
                "author": "Bram Cohen",
                "date": "2022-03-09T02:24:15",
                "message_text_only": "On Mon, Mar 7, 2022 at 2:56 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n>\n> > while in the coin set model each puzzle (scriptpubkey) gets run and\n> either assert fails or returns a list of extra conditions it has, possibly\n> including timelocks and creating new coins, paying fees, and other things.\n>\n> Does this mean it basically gets recursive covenants?\n> Or is a condition in this list of conditions written a more restrictive\n> language which itself cannot return a list of conditions?\n>\n\nThe conditions language is extremely restrictive but does allow for\nrecursive covenants through the route of specifying output\nscriptpubkeys/puzzles, which Bitcoin already sort of in principle supports\nexcept that Bitcoin script's ability to generate and parse transactions\nisn't quite up to the task.\n\n\n> > A lot of this is because there's a hook for doing compression at the\n> consensus layer which isn't being used aggressively yet. That one has the\n> downside that the combined cost of transactions can add up very\n> nonlinearly, but when you have constantly repeated bits of large\n> boilerplate it gets close and there isn't much of an alternative. That said\n> even with that form of compression maxxed out it's likely that gzip could\n> still do some compression but that would be better done in the database and\n> in wire protocol formats rather than changing the format which is hashed at\n> the consensus layer.\n>\n> How different is this from \"jets\" as proposed in Simplicity?\n>\n\nMy vague impression is that Simplicity jets are meant to be for things like\nSha3 rather than shared library code. It might be that the way to use it\nwould be to implement CLVM opcodes be a bunch of Simplicity jets. Whether\nthat would be making the CLVM irrelevant or going through a pointless bit\nof theatre to be based on Simplicity under the hood I don't know.\n\nThe compression hook is that in each block instead of there being a list of\npuzzle reveals and solutions there's a generator written in CLVM which\noutputs that list, and it can be passed the generators of old blocks from\nwhich it can pull out code snippits.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220308/a6fdcea8/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "bitcoin scripting and lisp",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Anthony Towns",
                "ZmnSCPxj",
                "Russell O'Connor",
                "Bram Cohen",
                "Jeremy Rubin"
            ],
            "messages_count": 23,
            "total_messages_chars_count": 106418
        }
    },
    {
        "title": "[bitcoin-dev] BIP Draft Submission",
        "thread_messages": [
            {
                "author": "Asher Hopp",
                "date": "2022-03-04T02:07:59",
                "message_text_only": "This is my first time submitting anything to this mailer list, so I am here\nwith humility and I would appreciate any feedback about any aspect of my\nBIP draft submission below. If you want to reach out to me directly you can\nemail me at asher at seent.com.\n\nAbstract\nRather than having a maximum supply of 21 million Bitcoin, there should be\na maximum supply of 21 trillion Bitcoin. This can be accomplished by moving\nthe decimal place 6 places to the right of where it is today, while\nreserving two degrees of accuracy after the decimal point.\n\nCopyright\nThis BIP is under the Creative Commons Zero (CC0) license.\n\nBackground\nOn February 6th, 2010 Satoshi Nakamoto responded to a bitcointalk forum\ndiscussion about the divisibility and economics of bitcoin as a global\ncurrency. Satoshi chimed in to the conversation when two ideas formed:\n1. Bitcoin is so scarce that a perception may exist that there is not\nenough to go around \u2013 there is not even 1 Bitcoin available per person on\nEarth.\n2. If Bitcoin\u2019s value continues to deflate against inflating fiat\ncurrencies, Bitcoin transactions may become smaller and smaller, requiring\nthe potentially tedious use of many leading 0\u2019s after the decimal point.\n\nSatoshi\u2019s suggested response to these issues was a software update to\nchange where the decimal place and commas are displayed when software\ninterprets a Bitcoin wallet\u2019s balance: \u201cIf it gets tiresome working with\nsmall numbers, we could change where the display shows the decimal point.\nSame amount of money, just different convention for where the \",\"'s and\n\".\"'s go.  e.g. moving the decimal place 3 places would mean if you had\n1.00000 before, now it shows it as 1,000.00.\u201d (\nhttps://bitcointalk.org/index.php?topic=44.msg267#msg267)\n\nSince 2010, when Satoshi wrote that post Bitcoin has indeed become a\nglobally adopted currency, the dollar has inflated significantly, and\nBitcoin has deflated. There are many debates in the Bitcoin community\nconcerning the nomenclature of Bitcoin\u2019s atomic unit (satoshis, sats, bits,\nbitcents, mbits, etc). The debate has somewhat spiraled out of control, and\nthere is no clearly emerging community consensus. Additionally this issue\nimpacts the technology world outside of Bitcoin because there are several\nproposals for various Unicode characters which factions of the Bitcoin\ncommunity have started using to represent the atomic Bitcoin unit despite\nno formalized consensus.  Therefore The conditions are right to move\nforward with Satoshi's vision and move the decimal place.\n\nDetails\nThere are several benefits to moving the decimal 6 places to the right in\nBitcoin wallet balance notation:\n1. Unit bias. It is a widely held belief that Bitcoin\u2019s adoption may be\nhindered because would-be participants have a negative perception of\nBitcoin\u2019s unit size. One Bitcoin so expensive, and some people may be\nturned off by the idea of only owning a fraction of a unit.\n2. Community cohesion. The Bitcoin community is deeply divided by various\nproposed atomic unit names, but if this BIP is adopted there is no need to\ndebate nomenclature for the Bitcoin atomic unit. Bitcoin software providers\ncan simply continue using the Bitcoin Unicode character (\u20bf, U+20BF), and\nthere are no additional unicode characters required.\n3. Simplicity and standardization. Bitcoin has no borders and is used by\npeople in just about every corner of the world. Other than the name Bitcoin\nand the Unicode character we have, there is no consensus around other\nnotations for Bitcoin as a currency. Rather than introducing new concepts\nfor people to learn, this BIP allows Bitcoin to grow under a single\nstandardized unit specification, with a single standard unit name, unit\nsize, and unit Unicode character.\n\nThere is only one drawback I can identify with this BIP, and it is purely\npsychological. Moving the decimal place may produce bad optics in the\nshort-term, and Bitcoin\u2019s detractors will likely seize the opportunity to\nspread misinformation that moving the decimal place changes the monetary\nvalue of anyone\u2019s Bitcoin. It is important to note that if this BIP were to\ngain consensus approval, the community would need to prepare talking points\nand coordinate educational outreach efforts to explain to Bitcoin users and\nwallet developers that this change does not change the proportion of the\ntotal value of Bitcoin any particular wallet holds, and is simply a\nnotational change. There are no \u201cwinners\u201d and no \u201closers\u201d in this BIP \u2013 all\nBitcoin participants would be impacted in an equal and proportionate manner\non pari passu terms, and there is no change to Bitcoin\u2019s monetary policy.\n\nImplementation\nThe software updates needed to implement this BIP are restricted to the\nwallet's CLI/GUI configuration, and only involve changing the location of\nthe decimal point and commas when viewing balances or reviewing transaction\ndata. Each wallet provider including Bitcoin Core would simply need to\nupdate the display of a wallet\u2019s balance by moving the decimal place 6\nplaces to the right.\n\nCompatibility\nBecause this BIP is a consensus change around the display of Bitcoin wallet\nbalances and transaction amounts, everything will be backwards compatible\nwith previous versions of Bitcoin. There would be no interruption in\nservices for Bitcoin wallets which do not implement this BIP, however there\ncould conceivably be human error problems with miscommunication between\ncounterparties after this BIP is implemented. I believe this risk is\nextremely minimal because an error of 6 decimal places is so significant\nthat it should be immediately noticed by any two parties conducting a\ntransaction.\n\nCheers,\nAsher\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220303/6291497c/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-05T19:13:27",
                "message_text_only": "If you're serious about this, you should write up considerations around\nusing the satoshi as a unit. That unit has none of the problems you\ndescribe. Satoshis is already a well accepted unit, and is likely to be a\nvery practical one that might match within an order of magnitude of (the\ncurrent buying power of) US cents.\n\n> this BIP is a consensus change around the display of Bitcoin wallet\nbalances\n\nFyi, this is not something that's considered a \"consensus change\", which is\nsomething that affects the validity of a block.\n\nOn Fri, Mar 4, 2022, 09:19 Asher Hopp via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> This is my first time submitting anything to this mailer list, so I am\n> here with humility and I would appreciate any feedback about any aspect of\n> my BIP draft submission below. If you want to reach out to me directly you\n> can email me at asher at seent.com.\n>\n> Abstract\n> Rather than having a maximum supply of 21 million Bitcoin, there should be\n> a maximum supply of 21 trillion Bitcoin. This can be accomplished by moving\n> the decimal place 6 places to the right of where it is today, while\n> reserving two degrees of accuracy after the decimal point.\n>\n> Copyright\n> This BIP is under the Creative Commons Zero (CC0) license.\n>\n> Background\n> On February 6th, 2010 Satoshi Nakamoto responded to a bitcointalk forum\n> discussion about the divisibility and economics of bitcoin as a global\n> currency. Satoshi chimed in to the conversation when two ideas formed:\n> 1. Bitcoin is so scarce that a perception may exist that there is not\n> enough to go around \u2013 there is not even 1 Bitcoin available per person on\n> Earth.\n> 2. If Bitcoin\u2019s value continues to deflate against inflating fiat\n> currencies, Bitcoin transactions may become smaller and smaller, requiring\n> the potentially tedious use of many leading 0\u2019s after the decimal point.\n>\n> Satoshi\u2019s suggested response to these issues was a software update to\n> change where the decimal place and commas are displayed when software\n> interprets a Bitcoin wallet\u2019s balance: \u201cIf it gets tiresome working with\n> small numbers, we could change where the display shows the decimal point.\n> Same amount of money, just different convention for where the \",\"'s and\n> \".\"'s go.  e.g. moving the decimal place 3 places would mean if you had\n> 1.00000 before, now it shows it as 1,000.00.\u201d (\n> https://bitcointalk.org/index.php?topic=44.msg267#msg267)\n>\n> Since 2010, when Satoshi wrote that post Bitcoin has indeed become a\n> globally adopted currency, the dollar has inflated significantly, and\n> Bitcoin has deflated. There are many debates in the Bitcoin community\n> concerning the nomenclature of Bitcoin\u2019s atomic unit (satoshis, sats, bits,\n> bitcents, mbits, etc). The debate has somewhat spiraled out of control, and\n> there is no clearly emerging community consensus. Additionally this issue\n> impacts the technology world outside of Bitcoin because there are several\n> proposals for various Unicode characters which factions of the Bitcoin\n> community have started using to represent the atomic Bitcoin unit despite\n> no formalized consensus.  Therefore The conditions are right to move\n> forward with Satoshi's vision and move the decimal place.\n>\n> Details\n> There are several benefits to moving the decimal 6 places to the right in\n> Bitcoin wallet balance notation:\n> 1. Unit bias. It is a widely held belief that Bitcoin\u2019s adoption may be\n> hindered because would-be participants have a negative perception of\n> Bitcoin\u2019s unit size. One Bitcoin so expensive, and some people may be\n> turned off by the idea of only owning a fraction of a unit.\n> 2. Community cohesion. The Bitcoin community is deeply divided by various\n> proposed atomic unit names, but if this BIP is adopted there is no need to\n> debate nomenclature for the Bitcoin atomic unit. Bitcoin software providers\n> can simply continue using the Bitcoin Unicode character (\u20bf, U+20BF), and\n> there are no additional unicode characters required.\n> 3. Simplicity and standardization. Bitcoin has no borders and is used by\n> people in just about every corner of the world. Other than the name Bitcoin\n> and the Unicode character we have, there is no consensus around other\n> notations for Bitcoin as a currency. Rather than introducing new concepts\n> for people to learn, this BIP allows Bitcoin to grow under a single\n> standardized unit specification, with a single standard unit name, unit\n> size, and unit Unicode character.\n>\n> There is only one drawback I can identify with this BIP, and it is purely\n> psychological. Moving the decimal place may produce bad optics in the\n> short-term, and Bitcoin\u2019s detractors will likely seize the opportunity to\n> spread misinformation that moving the decimal place changes the monetary\n> value of anyone\u2019s Bitcoin. It is important to note that if this BIP were to\n> gain consensus approval, the community would need to prepare talking points\n> and coordinate educational outreach efforts to explain to Bitcoin users and\n> wallet developers that this change does not change the proportion of the\n> total value of Bitcoin any particular wallet holds, and is simply a\n> notational change. There are no \u201cwinners\u201d and no \u201closers\u201d in this BIP \u2013 all\n> Bitcoin participants would be impacted in an equal and proportionate manner\n> on pari passu terms, and there is no change to Bitcoin\u2019s monetary policy.\n>\n> Implementation\n> The software updates needed to implement this BIP are restricted to the\n> wallet's CLI/GUI configuration, and only involve changing the location of\n> the decimal point and commas when viewing balances or reviewing transaction\n> data. Each wallet provider including Bitcoin Core would simply need to\n> update the display of a wallet\u2019s balance by moving the decimal place 6\n> places to the right.\n>\n> Compatibility\n> Because this BIP is a consensus change around the display of Bitcoin\n> wallet balances and transaction amounts, everything will be backwards\n> compatible with previous versions of Bitcoin. There would be no\n> interruption in services for Bitcoin wallets which do not implement this\n> BIP, however there could conceivably be human error problems with\n> miscommunication between counterparties after this BIP is implemented. I\n> believe this risk is extremely minimal because an error of 6 decimal places\n> is so significant that it should be immediately noticed by any two parties\n> conducting a transaction.\n>\n> Cheers,\n> Asher\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/76d7fd2c/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP Draft Submission",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Billy Tetrud",
                "Asher Hopp"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 12657
        }
    },
    {
        "title": "[bitcoin-dev] LN/mercury integrations",
        "thread_messages": [
            {
                "author": "Tom Trevethan",
                "date": "2022-03-04T16:49:55",
                "message_text_only": "A couple of features we are considering for the mercury statechain\nwallet/service and would be good to get comments/feedback on.\n\n1.\nIn the current setup (https://github.com/commerceblock/mercury), deposits\nare free and permissionless (i.e. no authentication required to generate a\nshared key deposit addresses) and the mercury server fee (as a fixed\npercentage of the coin value) is collected in the withdrawal transaction as\na UTXO paid to a fixed, specified bitcoin address. This has the advantage\nof making the deposit process low friction and user friendly, but has some\ndisadvantages:\n\nThe withdrawal transaction fee output is typically a small fraction of the\ncoin value and for the smallest coin values is close to the dust limit\n(i.e. these outputs may not be spendable in a high tx fee environment).\nThe on-chain mercury fee explicitly labels all withdrawn coins as mercury\nstatechain withdrawals, which is a privacy concern for many users.\n\nThe alternative that mitigates these issues is to charge the fee up-front,\nvia a LN invoice, before the shared key deposit address is generated. In\nthis approach, a user would specify in the wallet that they wanted to\ndeposit a specific amount into a statecoin, and instead of performing a\nshared  key generation with the server, would request a LN invoice for the\nwithdrawal fee from the server, which would be returned to the wallet and\ndisplayed to the user.\n\nThe user would then copy this invoice (by C&P or QR code) into a third\nparty LN wallet and pay the fee. A LN node running on the mercury server\nback end would then verify that the payment had been made, and enable the\nwallet to continue with the deposit keygen and deposit process. This coin\nwould be labeled as \u2018fee paid\u2019 by the wallet and server, and not be subject\nto an on-chain fee payment on withdrawal.\n\n\n2.\nWithdrawal directly into LN channel.\n\nCurrently the wallet can send the coin to any type of bitcoin address\n(except Taproot - P2TR - which is a pending straightforward upgrade).\nTo create a dual funded channel (i.e. a channel where the counterparty\nprovides BTC in addition to the mercury user) the withdrawal transaction\nprocess and co-signing with the server must support the handling of PSBTs.\nIn this case, the withdrawal step would involve the mercury wallet\nco-signing (with the mercury server), a PSBT created by a LN wallet.\n\nTo enable this, the mercury wallet should be able to both create a PSBT on\nthe withdrawal page, and then co-sign it with the server, and then send it\nto the channel counterparty out of band (or via the third party LN\nwallet/node), and import a PSBT created by the channel counterparty and\nsign it, and export and/or broadcast the fully signed PSBT.\n\nThis seems to be possible (i.e. paying directly to a dual funded channel\nopening tx from a third party wallet) with c-lightning and lnd via RPC, but\nI\u2019m not aware of any LN wallet that would support this kind of thing. It\nhas the potential to eliminate an on-chain tx, which could be valuable in a\nhigh-fee environment.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220304/cc2f89e8/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "LN/mercury integrations",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tom Trevethan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3216
        }
    },
    {
        "title": "[bitcoin-dev] One testnet to rule them all",
        "thread_messages": [
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-04T20:46:04",
                "message_text_only": "In testnet3, anyone can become a miner, it is possible to even mine a block on some CPU, because the difficulty can drop to one. In signet, we create some challenge, for example 1-of-2 multisig, that can restrict who can mine, so that chain can be \"unreliably reliable\". Then, my question is: why signets are introducing new coins out of thin air, instead of forming two-way peg-in between testnet3 and signet?\n\nThe lack of coins is not a bug, it is a feature. We have more halvings in testnet3 than in mainnet or signets, but it can be good, we can use this to see, what can happen with a chain after many halvings. Also, in testnet3 there is no need to have any coins if we are mining. Miners can create, move and destroy zero satoshis. They can also extend the precision of the coins, so a single coin in testnet3 can be represented as a thousand of coins in some signet sidechain.\n\nRecently, there are some discussions regarding sidechains. Before they will become a real thing, running on mainnet, they should be tested. Nowadays, a popular way of testing new features is creating a new signet with new rules. But the question still remains: why we need new coins, created out of thin air? And even when some signet wants to do that, then why it is not pegged into testnet3? Then it would have as much chainwork protection as testnet3!\n\nIt seems that testnet3 is good enough to represent the main chain during sidechain testing. It is permissionless and open, anyone can start mining sidechain blocks, anyone with a CPU can be lucky and find a block with the minimal difficulty. Also, because of blockstorms and regular chain reorgs, some extreme scenarios, like stealing all coins from some sidechain, can be tested in a public way, because that \"unfriendly and unstable\" environment can be used to test stronger attacks than in a typical chain.\n\nPutting that proposal into practice can be simple and require just creating one Taproot address per signet in testnet3. Then, it is possible to create one testnet transaction (every three months) that would move coins to and from testnet3, so the same coins could travel between many signets. New signets can be pegged in with 1:1 ratio, existing signets can be transformed into signet sidechains (the signet miners rule that chains, so they can enforce any transition rules they need)."
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-05T16:19:26",
                "message_text_only": "There's no point to pegging coins that are worthless into a system of also\nworthless coins, unless you want to test the mechanism of testing pegging.\n\nAs is, it's hard enough to get people set up on a signet, if they have to\nrun two nodes and then scramble to find testnet coins and then peg them\nwere just raising the barriers to entry for starting to use a signet for\ntesting.\n\n\nIf anything I think we should permanently shutter testnet now that signet\nis available.\n\nOn Sat, Mar 5, 2022, 3:53 PM vjudeu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> In testnet3, anyone can become a miner, it is possible to even mine a\n> block on some CPU, because the difficulty can drop to one. In signet, we\n> create some challenge, for example 1-of-2 multisig, that can restrict who\n> can mine, so that chain can be \"unreliably reliable\". Then, my question is:\n> why signets are introducing new coins out of thin air, instead of forming\n> two-way peg-in between testnet3 and signet?\n>\n> The lack of coins is not a bug, it is a feature. We have more halvings in\n> testnet3 than in mainnet or signets, but it can be good, we can use this to\n> see, what can happen with a chain after many halvings. Also, in testnet3\n> there is no need to have any coins if we are mining. Miners can create,\n> move and destroy zero satoshis. They can also extend the precision of the\n> coins, so a single coin in testnet3 can be represented as a thousand of\n> coins in some signet sidechain.\n>\n> Recently, there are some discussions regarding sidechains. Before they\n> will become a real thing, running on mainnet, they should be tested.\n> Nowadays, a popular way of testing new features is creating a new signet\n> with new rules. But the question still remains: why we need new coins,\n> created out of thin air? And even when some signet wants to do that, then\n> why it is not pegged into testnet3? Then it would have as much chainwork\n> protection as testnet3!\n>\n> It seems that testnet3 is good enough to represent the main chain during\n> sidechain testing. It is permissionless and open, anyone can start mining\n> sidechain blocks, anyone with a CPU can be lucky and find a block with the\n> minimal difficulty. Also, because of blockstorms and regular chain reorgs,\n> some extreme scenarios, like stealing all coins from some sidechain, can be\n> tested in a public way, because that \"unfriendly and unstable\" environment\n> can be used to test stronger attacks than in a typical chain.\n>\n> Putting that proposal into practice can be simple and require just\n> creating one Taproot address per signet in testnet3. Then, it is possible\n> to create one testnet transaction (every three months) that would move\n> coins to and from testnet3, so the same coins could travel between many\n> signets. New signets can be pegged in with 1:1 ratio, existing signets can\n> be transformed into signet sidechains (the signet miners rule that chains,\n> so they can enforce any transition rules they need).\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/b5cb634a/attachment-0001.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-05T18:17:07",
                "message_text_only": "> There's no point to pegging coins that are worthless into a system of also worthless coins, unless you want to test the mechanism of testing pegging.\n\nBut testing pegging is what is needed if we ever want to introduce sidechains. On the other hand, even if we don't want sidechains, then the question still remains: why we need more than 21 million coins for testing, if we don't need more than 21 million coins for real transactions?\n\n> If anything I think we should permanently shutter testnet now that signet is available.\n\nThen, in that case, the \"mainchain\" can be our official signet and other signets can be pegged into that. Also, testnet3 is permissionless, so how signet can replace that? Because if you want to test mining and you cannot mine any blocks in signet, then it is another problem.\n\nOn 2022-03-05 17:19:40 user Jeremy Rubin <jeremy.l.rubin at gmail.com> wrote:\nThere's no point to pegging coins that are worthless into a system of also worthless coins, unless you want to test the mechanism of testing pegging.\n\n\nAs is, it's hard enough to get people set up on a signet, if they have to run two nodes and then scramble to find testnet coins and then peg them were just raising the barriers to entry for starting to use a signet for testing.\n\n\n\n\nIf anything I think we should permanently shutter testnet now that signet is available.\n\n\nOn Sat, Mar 5, 2022, 3:53 PM vjudeu via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nIn testnet3, anyone can become a miner, it is possible to even mine a block on some CPU, because the difficulty can drop to one. In signet, we create some challenge, for example 1-of-2 multisig, that can restrict who can mine, so that chain can be \"unreliably reliable\". Then, my question is: why signets are introducing new coins out of thin air, instead of forming two-way peg-in between testnet3 and signet?\n\nThe lack of coins is not a bug, it is a feature. We have more halvings in testnet3 than in mainnet or signets, but it can be good, we can use this to see, what can happen with a chain after many halvings. Also, in testnet3 there is no need to have any coins if we are mining. Miners can create, move and destroy zero satoshis. They can also extend the precision of the coins, so a single coin in testnet3 can be represented as a thousand of coins in some signet sidechain.\n\nRecently, there are some discussions regarding sidechains. Before they will become a real thing, running on mainnet, they should be tested. Nowadays, a popular way of testing new features is creating a new signet with new rules. But the question still remains: why we need new coins, created out of thin air? And even when some signet wants to do that, then why it is not pegged into testnet3? Then it would have as much chainwork protection as testnet3!\n\nIt seems that testnet3 is good enough to represent the main chain during sidechain testing. It is permissionless and open, anyone can start mining sidechain blocks, anyone with a CPU can be lucky and find a block with the minimal difficulty. Also, because of blockstorms and regular chain reorgs, some extreme scenarios, like stealing all coins from some sidechain, can be tested in a public way, because that \"unfriendly and unstable\" environment can be used to test stronger attacks than in a typical chain.\n\nPutting that proposal into practice can be simple and require just creating one Taproot address per signet in testnet3. Then, it is possible to create one testnet transaction (every three months) that would move coins to and from testnet3, so the same coins could travel between many signets. New signets can be pegged in with 1:1 ratio, existing signets can be transformed into signet sidechains (the signet miners rule that chains, so they can enforce any transition rules they need).\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-05T23:40:11",
                "message_text_only": "Signet degrades to a testnet if you make your key OP_TRUE.\n\n\nIt's not about needing 21M coins it's about easily getting access to said\ncoins for testing, where it's kinda tricky to get testnet coins.\n\nOn Sat, Mar 5, 2022, 6:17 PM <vjudeu at gazeta.pl> wrote:\n\n> > There's no point to pegging coins that are worthless into a system of\n> also worthless coins, unless you want to test the mechanism of testing\n> pegging.\n>\n> But testing pegging is what is needed if we ever want to introduce\n> sidechains. On the other hand, even if we don't want sidechains, then the\n> question still remains: why we need more than 21 million coins for testing,\n> if we don't need more than 21 million coins for real transactions?\n>\n> > If anything I think we should permanently shutter testnet now that\n> signet is available.\n>\n> Then, in that case, the \"mainchain\" can be our official signet and other\n> signets can be pegged into that. Also, testnet3 is permissionless, so how\n> signet can replace that? Because if you want to test mining and you cannot\n> mine any blocks in signet, then it is another problem.\n>\n> On 2022-03-05 17:19:40 user Jeremy Rubin <jeremy.l.rubin at gmail.com> wrote:\n> There's no point to pegging coins that are worthless into a system of also\n> worthless coins, unless you want to test the mechanism of testing pegging.\n>\n>\n> As is, it's hard enough to get people set up on a signet, if they have to\n> run two nodes and then scramble to find testnet coins and then peg them\n> were just raising the barriers to entry for starting to use a signet for\n> testing.\n>\n>\n>\n>\n> If anything I think we should permanently shutter testnet now that signet\n> is available.\n>\n>\n> On Sat, Mar 5, 2022, 3:53 PM vjudeu via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> In testnet3, anyone can become a miner, it is possible to even mine a\n> block on some CPU, because the difficulty can drop to one. In signet, we\n> create some challenge, for example 1-of-2 multisig, that can restrict who\n> can mine, so that chain can be \"unreliably reliable\". Then, my question is:\n> why signets are introducing new coins out of thin air, instead of forming\n> two-way peg-in between testnet3 and signet?\n>\n> The lack of coins is not a bug, it is a feature. We have more halvings in\n> testnet3 than in mainnet or signets, but it can be good, we can use this to\n> see, what can happen with a chain after many halvings. Also, in testnet3\n> there is no need to have any coins if we are mining. Miners can create,\n> move and destroy zero satoshis. They can also extend the precision of the\n> coins, so a single coin in testnet3 can be represented as a thousand of\n> coins in some signet sidechain.\n>\n> Recently, there are some discussions regarding sidechains. Before they\n> will become a real thing, running on mainnet, they should be tested.\n> Nowadays, a popular way of testing new features is creating a new signet\n> with new rules. But the question still remains: why we need new coins,\n> created out of thin air? And even when some signet wants to do that, then\n> why it is not pegged into testnet3? Then it would have as much chainwork\n> protection as testnet3!\n>\n> It seems that testnet3 is good enough to represent the main chain during\n> sidechain testing. It is permissionless and open, anyone can start mining\n> sidechain blocks, anyone with a CPU can be lucky and find a block with the\n> minimal difficulty. Also, because of blockstorms and regular chain reorgs,\n> some extreme scenarios, like stealing all coins from some sidechain, can be\n> tested in a public way, because that \"unfriendly and unstable\" environment\n> can be used to test stronger attacks than in a typical chain.\n>\n> Putting that proposal into practice can be simple and require just\n> creating one Taproot address per signet in testnet3. Then, it is possible\n> to create one testnet transaction (every three months) that would move\n> coins to and from testnet3, so the same coins could travel between many\n> signets. New signets can be pegged in with 1:1 ratio, existing signets can\n> be transformed into signet sidechains (the signet miners rule that chains,\n> so they can enforce any transition rules they need).\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/35f61e46/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "One testnet to rule them all",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "vjudeu at gazeta.pl",
                "Jeremy Rubin"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 14241
        }
    },
    {
        "title": "[bitcoin-dev] Annex Purpose Discussion: OP_ANNEX, Turing Completeness, and other considerations",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-04T23:21:41",
                "message_text_only": "I've seen some discussion of what the Annex can be used for in Bitcoin. For\nexample, some people have discussed using the annex as a data field for\nsomething like CHECKSIGFROMSTACK type stuff (additional authenticated data)\nor for something like delegation (the delegation is to the annex). I think\nbefore devs get too excited, we should have an open discussion about what\nthis is actually for, and figure out if there are any constraints to using\nit however we may please.\n\nThe BIP is tight lipped about it's purpose, saying mostly only:\n\n*What is the purpose of the annex? The annex is a reserved space for future\nextensions, such as indicating the validation costs of computationally\nexpensive new opcodes in a way that is recognizable without knowing the\nscriptPubKey of the output being spent. Until the meaning of this field is\ndefined by another softfork, users SHOULD NOT include annex in\ntransactions, or it may lead to PERMANENT FUND LOSS.*\n\n*The annex (or the lack of thereof) is always covered by the signature and\ncontributes to transaction weight, but is otherwise ignored during taproot\nvalidation.*\n\n*Execute the script, according to the applicable script rules[11], using\nthe witness stack elements excluding the script s, the control block c, and\nthe annex a if present, as initial stack.*\n\nEssentially, I read this as saying: The annex is the ability to pad a\ntransaction with an additional string of 0's that contribute to the virtual\nweight of a transaction, but has no validation cost itself. Therefore,\nsomehow, if you needed to validate more signatures than 1 per 50 virtual\nweight units, you could add padding to buy extra gas. Or, we might somehow\nmake the witness a small language (e.g., run length encoded zeros) such\nthat we can very quickly compute an equivalent number of zeros to 'charge'\nwithout actually consuming the space but still consuming a linearizable\nresource... or something like that. We might also e.g. want to use the\nannex to reserve something else, like the amount of memory. In general, we\nare using the annex to express a resource constraint efficiently. This\nmight be useful for e.g. simplicity one day.\n\nGenerating an Annex: One should write a tracing executor for a script, run\nit, measure the resource costs, and then generate an annex that captures\nany externalized costs.\n\n-------------------\n\nIntroducing OP_ANNEX: Suppose there were some sort of annex pushing opcode,\nOP_ANNEX which puts the annex on the stack as well as a 0 or 1 (to\ndifferentiate annex is 0 from no annex, e.g. 0 1 means annex was 0 and 0 0\nmeans no annex). This would be equivalent to something based on <annex\nflag> OP_TXHASH <has annex flag> OP_TXHASH.\n\nNow suppose that I have a computation that I am running in a script as\nfollows:\n\nOP_ANNEX\nOP_IF\n    `some operation that requires annex to be <1>`\nOP_ELSE\n    OP_SIZE\n    `some operation that requires annex to be len(annex) + 1 or does a\nchecksig`\nOP_ENDIF\n\nNow every time you run this, it requires one more resource unit than the\nlast time you ran it, which makes your satisfier use the annex as some sort\nof \"scratch space\" for a looping construct, where you compute a new annex,\nloop with that value, and see if that annex is now accepted by the program.\n\nIn short, it kinda seems like being able to read the annex off of the stack\nmakes witness construction somehow turing complete, because we can use it\nas a register/tape for some sort of computational model.\n\n-------------------\n\nThis seems at odds with using the annex as something that just helps you\nheuristically guess  computation costs, now it's somehow something that\nacts to make script satisfiers recursive.\n\nBecause the Annex is signed, and must be the same, this can also be\ninconvenient:\n\nSuppose that you have a Miniscript that is something like: and(or(PK(A),\nPK(A')), X, or(PK(B), PK(B'))).\n\nA or A' should sign with B or B'. X is some sort of fragment that might\nrequire a value that is unknown (and maybe recursively defined?) so\ntherefore if we send the PSBT to A first, which commits to the annex, and\nthen X reads the annex and say it must be something else, A must sign\nagain. So you might say, run X first, and then sign with A and C or B.\nHowever, what if the script somehow detects the bitstring WHICH_A WHICH_B\nand has a different Annex per selection (e.g., interpret the bitstring as a\nint and annex must == that int). Now, given and(or(K1, K1'),... or(Kn,\nKn')) we end up with needing to pre-sign 2**n annex values somehow... this\nseems problematic theoretically.\n\nOf course this wouldn't be miniscript then. Because miniscript is just for\nthe well behaved subset of script, and this seems ill behaved. So maybe\nwe're OK?\n\nBut I think the issue still arises where suppose I have a simple thing\nlike: and(COLD_LOGIC, HOT_LOGIC) where both contains a signature, if\nCOLD_LOGIC and HOT_LOGIC can both have different costs, I need to decide\nwhat logic each satisfier for the branch is going to use in advance, or\nsign all possible sums of both our annex costs? This could come up if\ncold/hot e.g. use different numbers of signatures / use checksigCISAadd\nwhich maybe requires an annex argument.\n\n\n\n------------\n\nIt seems like one good option is if we just go on and banish the OP_ANNEX.\nMaybe that solves some of this? I sort of think so. It definitely seems\nlike we're not supposed to access it via script, given the quote from above:\n\n*Execute the script, according to the applicable script rules[11], using\nthe witness stack elements excluding the script s, the control block c, and\nthe annex a if present, as initial stack.*\n\nIf we were meant to have it, we would have not nixed it from the stack, no?\nOr would have made the opcode for it as a part of taproot...\n\nBut recall that the annex is committed to by the signature.\n\nSo it's only a matter of time till we see some sort of Cat and Schnorr\nTricks III the Annex Edition that lets you use G cleverly to get the annex\nonto the stack again, and then it's like we had OP_ANNEX all along, or\nwithout CAT, at least something that we can detect that the value has\nchanged and cause this satisfier looping issue somehow.\n\nNot to mention if we just got OP_TXHASH\n\n\n\n-----------\n\nIs the annex bad? After writing this I sort of think so?\n\nOne solution would be to... just soft-fork it out. Always must be 0. When\nwe come up with a use case for something like an annex, we can find a way\nto add it back.  Maybe this means somehow pushing multiple annexes and\nhaving an annex stack, where only sub-segments are signed for the last\nexecuted signature? That would solve looping... but would it break some\naggregation thing? Maybe.\n\n\nAnother solution would be to make it so the annex is never committed to and\nunobservable from the script, but that the annex is always something that\nyou can run get_annex(stack) to generate the annex. Thus it is a hint for\nvalidation rules, but not directly readable, and if it is modified you\nfigure out the txn was cheaper sometime after you execute the scripts and\ncan decrease the value when you relay. But this sounds like something that\nneeds to be a p2p only annex, because consensus we may not care (unless\nit's something like preallocating memory for validation?).\n\n-----------------------\n\nOverall my preference is -- perhaps sadly -- looking like we should\nsoft-fork it out of our current Checksig (making the policy that it must 0\na consensus rule) and redesign the annex technique later when we actually\nknow what it is for with a new checksig or other mechanism. But It's not a\nhard opinion! It just seems like you can't practically use the annex for\nthis worklimit type thing *and* observe it from the stack meaningfully.\n\n\n\nThanks for coming to my ted-talk,\n\nJeremy\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220304/361dd848/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-04T23:33:10",
                "message_text_only": "Good morning Jeremy,\n\nUmm `OP_ANNEX` seems boring ....\n\n\n> It seems like one good option is if we just go on and banish the OP_ANNEX. Maybe that solves some of this? I sort of think so. It definitely seems like we're not supposed to access it via script, given the quote from above:\n>\n> Execute the script, according to the applicable script rules[11], using the witness stack elements excluding the script s, the control block c, and the annex a if present, as initial stack.\n> If we were meant to have it, we would have not nixed it from the stack, no? Or would have made the opcode for it as a part of taproot...\n>\n> But recall that the annex is committed\u00a0to by\u00a0the signature.\n>\n> So it's only a matter of time till we see some sort of Cat and Schnorr Tricks III the Annex Edition that lets you use G cleverly to get the annex onto the stack again, and then it's like we had OP_ANNEX all along, or without CAT, at least something that we can detect that the value has changed and cause this satisfier looping issue somehow.\n\n... Never mind I take that back.\n\nHmmm.\n\nActually if the Annex is supposed to be ***just*** for adding weight to the transaction so that we can do something like increase limits on SCRIPT execution, then it does *not* have to be covered by any signature.\nIt would then be third-party malleable, but suppose we have a \"valid\" transaction on the mempool where the Annex weight is the minimum necessary:\n\n* If a malleated transaction has a too-low Annex, then the malleated transaction fails validation and the current transaction stays in the mempool.\n* If a malleated transaction has a higher Annex, then the malleated transaction has lower feerate than the current transaction and cannot evict it from the mempool.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Christian Decker",
                "date": "2022-03-06T12:55:53",
                "message_text_only": "We'd have to be very carefully with this kind of third-party malleability,\nsince it'd make transaction pinning trivial without even requiring the\nability to spend one of the outputs (which current CPFP based pinning\nattacks require).\n\nCheers,\nChristian\n\nOn Sat, 5 Mar 2022, 00:33 ZmnSCPxj via bitcoin-dev, <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning Jeremy,\n>\n> Umm `OP_ANNEX` seems boring ....\n>\n>\n> > It seems like one good option is if we just go on and banish the\n> OP_ANNEX. Maybe that solves some of this? I sort of think so. It definitely\n> seems like we're not supposed to access it via script, given the quote from\n> above:\n> >\n> > Execute the script, according to the applicable script rules[11], using\n> the witness stack elements excluding the script s, the control block c, and\n> the annex a if present, as initial stack.\n> > If we were meant to have it, we would have not nixed it from the stack,\n> no? Or would have made the opcode for it as a part of taproot...\n> >\n> > But recall that the annex is committed to by the signature.\n> >\n> > So it's only a matter of time till we see some sort of Cat and Schnorr\n> Tricks III the Annex Edition that lets you use G cleverly to get the annex\n> onto the stack again, and then it's like we had OP_ANNEX all along, or\n> without CAT, at least something that we can detect that the value has\n> changed and cause this satisfier looping issue somehow.\n>\n> ... Never mind I take that back.\n>\n> Hmmm.\n>\n> Actually if the Annex is supposed to be ***just*** for adding weight to\n> the transaction so that we can do something like increase limits on SCRIPT\n> execution, then it does *not* have to be covered by any signature.\n> It would then be third-party malleable, but suppose we have a \"valid\"\n> transaction on the mempool where the Annex weight is the minimum necessary:\n>\n> * If a malleated transaction has a too-low Annex, then the malleated\n> transaction fails validation and the current transaction stays in the\n> mempool.\n> * If a malleated transaction has a higher Annex, then the malleated\n> transaction has lower feerate than the current transaction and cannot evict\n> it from the mempool.\n>\n> Regards,\n> ZmnSCPxj\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/87be9dd4/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-05T05:59:24",
                "message_text_only": "On Fri, Mar 04, 2022 at 11:21:41PM +0000, Jeremy Rubin via bitcoin-dev wrote:\n> I've seen some discussion of what the Annex can be used for in Bitcoin. \n\nhttps://www.erisian.com.au/meetbot/taproot-bip-review/2019/taproot-bip-review.2019-11-12-19.00.log.html\n\nincludes some discussion on that topic from the taproot review meetings.\n\nThe difference between information in the annex and information in\neither a script (or the input data for the script that is the rest of\nthe witness) is (in theory) that the annex can be analysed immediately\nand unconditionally, without necessarily even knowing anything about\nthe utxo being spent.\n\nThe idea is that we would define some simple way of encoding (multiple)\nentries into the annex -- perhaps a tag/length/value scheme like\nlightning uses; maybe if we add a lisp scripting language to consensus,\nwe just reuse the list encoding from that? -- at which point we might\nuse one tag to specify that a transaction uses advanced computation, and\nneeds to be treated as having a heavier weight than its serialized size\nimplies; but we could use another tag for per-input absolute locktimes;\nor another tag to commit to a past block height having a particular hash.\n\nIt seems like a good place for optimising SIGHASH_GROUP (allowing a group\nof inputs to claim a group of outputs for signing, but not allowing inputs\nfrom different groups to ever claim the same output; so that each output\nis hashed at most once for this purpose) -- since each input's validity\ndepends on the other inputs' state, it's better to be able to get at\nthat state as easily as possible rather than having to actually execute\nother scripts before your can tell if your script is going to be valid.\n\n> The BIP is tight lipped about it's purpose\n\nBIP341 only reserves an area to put the annex; it doesn't define how\nit's used or why it should be used.\n\n> Essentially, I read this as saying: The annex is the ability to pad a\n> transaction with an additional string of 0's \n\nIf you wanted to pad it directly, you can do that in script already\nwith a PUSH/DROP combo.\n\nThe point of doing it in the annex is you could have a short byte\nstring, perhaps something like \"0x010201a4\" saying \"tag 1, data length 2\nbytes, value 420\" and have the consensus intepretation of that be \"this\ntransaction should be treated as if it's 420 weight units more expensive\nthan its serialized size\", while only increasing its witness size by\n6 bytes (annex length, annex flag, and the four bytes above). Adding 6\nbytes for a 426 weight unit increase seems much better than adding 426\nwitness bytes.\n\nThe example scenario is that if there was an opcode to verify a\nzero-knowledge proof, eg I think bulletproof range proofs are something\nlike 10x longer than a signature, but require something like 400x the\nvalidation time. Since checksig has a validation weight of 50 units,\na bulletproof verify might have a 400x greater validation weight, ie\n20,000 units, while your witness data is only 650 bytes serialized. In\nthat case, we'd need to artificially bump the weight of you transaction\nup by the missing 19,350 units, or else an attacker could fill a block\nwith perhaps 6000 bulletproofs costing the equivalent of 120M signature\noperations, rather than the 80k sigops we currently expect as the maximum\nin a block. Seems better to just have \"0x01024b96\" stuck in the annex,\nthan 19kB of zeroes.\n\n> Introducing OP_ANNEX: Suppose there were some sort of annex pushing opcode,\n> OP_ANNEX which puts the annex on the stack\n\nI think you'd want to have a way of accessing individual entries from\nthe annex, rather than the annex as a single unit.\n\n> Now suppose that I have a computation that I am running in a script as\n> follows:\n> \n> OP_ANNEX\n> OP_IF\n>     `some operation that requires annex to be <1>`\n> OP_ELSE\n>     OP_SIZE\n>     `some operation that requires annex to be len(annex) + 1 or does a\n> checksig`\n> OP_ENDIF\n> \n> Now every time you run this,\n\nYou only run a script from a transaction once at which point its\nannex is known (a different annex gives a different wtxid and breaks\nany signatures), and can't reference previous or future transactions'\nannexes...\n\n> Because the Annex is signed, and must be the same, this can also be\n> inconvenient:\n\nThe annex is committed to by signatures in the same way nVersion,\nnLockTime and nSequence are committed to by signatures; I think it helps\nto think about it in a similar way.\n\n> Suppose that you have a Miniscript that is something like: and(or(PK(A),\n> PK(A')), X, or(PK(B), PK(B'))).\n> \n> A or A' should sign with B or B'. X is some sort of fragment that might\n> require a value that is unknown (and maybe recursively defined?) so\n> therefore if we send the PSBT to A first, which commits to the annex, and\n> then X reads the annex and say it must be something else, A must sign\n> again. So you might say, run X first, and then sign with A and C or B.\n> However, what if the script somehow detects the bitstring WHICH_A WHICH_B\n> and has a different Annex per selection (e.g., interpret the bitstring as a\n> int and annex must == that int). Now, given and(or(K1, K1'),... or(Kn,\n> Kn')) we end up with needing to pre-sign 2**n annex values somehow... this\n> seems problematic theoretically.\n\nNote that you need to know what the annex will contain before you sign,\nsince the annex is committed to via the signature. If \"X\" will need\nentries in the annex that aren't able to be calculated by the other\nparties, then they need to be the first to contribute to the PSBT, not A.\n\nI think the analogy to locktimes would be \"I need the locktime to be at\nleast block 900k, should I just sign that now, or check that nobody else\nis going to want it to be block 950k or something? Or should I just sign\nwith nLockTime at 900k, 910k, 920k, 930k, etc and let someone else pick\nthe right one?\" The obvious solution is just to work out what the\nnLockTime should be first, then run signing rounds. Likewise, work out\nwhat the annex should be first, then run the signing rounds.\n\nCLTV also has the problem that if you have one script fragment with\nCLTV by time, and another with CLTV by height, you can't come up with\nan nLockTime that will ever satisfy both. If you somehow have script\nfragments that require incompatible interpretations of the annex, you're\nlikewise going to be out of luck.\n\nHaving a way of specifying locktimes in the annex can solve that\nparticular problem with CLTV (different inputs can sign different\nlocktimes, and you could have different tags for by-time/by-height so\nthat even the same input can have different clauses requiring both),\nbut the general problem still exists.\n\n(eg, you might have per-input by-height absolute locktimes as annex\nentry 3, and per-input by-time absolute locktimes as annex entry 4,\nso you might convert:\n\n \"900e3 CLTV DROP\" -> \"900e3 3 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n\n \"500e6 CLTV DROP\" -> \"500e6 4 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n\nfor height/time locktime checks respectively)\n\n> Of course this wouldn't be miniscript then. Because miniscript is just for\n> the well behaved subset of script, and this seems ill behaved. So maybe\n> we're OK?\n\nThe CLTV issue hit miniscript:\n\nhttps://medium.com/blockstream/dont-mix-your-timelocks-d9939b665094\n\n> But I think the issue still arises where suppose I have a simple thing\n> like: and(COLD_LOGIC, HOT_LOGIC) where both contains a signature, if\n> COLD_LOGIC and HOT_LOGIC can both have different costs, I need to decide\n> what logic each satisfier for the branch is going to use in advance, or\n> sign all possible sums of both our annex costs? This could come up if\n> cold/hot e.g. use different numbers of signatures / use checksigCISAadd\n> which maybe requires an annex argument.\n\nSignatures pay for themselves -- every signature is 64 or 65 bytes,\nbut only has 50 units of validation weight. (That is, a signature check\nis about 50x the cost of hashing 520 bytes of data, which is the next\nhighest cost operation we have, and is treated as costing 1 unit, and\nimmediately paid for by the 1 byte that writing OP_HASH256 takes up)\n\nThat's why the \"add cost\" use of the annex is only talked about in\nhypotheticals, not specified -- for reasonable scripts with today's\nopcodes, it's not needed.\n\nIf you're doing cross-input signature aggregation, everybody needs to\nagree on the message they're signing in the first place, so you definitely\ncan't delay figuring out some bits of some annex until after signing.\n\n> It seems like one good option is if we just go on and banish the OP_ANNEX.\n> Maybe that solves some of this? I sort of think so. It definitely seems\n> like we're not supposed to access it via script, given the quote from above:\n\nHow the annex works isn't defined, so it doesn't make any sense to\naccess it from script. When how it works is defined, I expect it might\nwell make sense to access it from script -- in a similar way that the\nCLTV and CSV opcodes allow accessing nLockTime and nSequence from script.\n\nTo expand on that: the logic to prevent a transaction confirming too\nearly occurs by looking at nLockTime and nSequence, but script can\nensure that an attempt to use \"bad\" values for those can never be a\nvalid transaction; likewise, consensus may look at the annex to enforce\nnew conditions as to when a transaction might be valid (and can do so\nwithout needing to evaluate any scripts), but the individual scripts can\nmake sure that the annex has been set to what the utxo owner considered\nto be reasonable values.\n\n> One solution would be to... just soft-fork it out. Always must be 0. When\n> we come up with a use case for something like an annex, we can find a way\n> to add it back.\n\nThe point of reserving the annex the way it has been is exactly this --\nit should not be used now, but when we agree on how it should be used,\nwe have an area that's immediately ready to be used.\n\n(For the cases where you don't need script to enforce reasonable values,\nreserving it now means those new consensus rules can be used immediately\nwith utxos that predate the new consensus rules -- so you could update\noffchain contracts from per-tx to per-input locktimes immediately without\nhaving to update the utxo on-chain first)\n\nCheers,\naj"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-05T12:20:02",
                "message_text_only": "On Sat, Mar 5, 2022 at 5:59 AM Anthony Towns <aj at erisian.com.au> wrote:\n\n> On Fri, Mar 04, 2022 at 11:21:41PM +0000, Jeremy Rubin via bitcoin-dev\n> wrote:\n> > I've seen some discussion of what the Annex can be used for in Bitcoin.\n>\n>\n> https://www.erisian.com.au/meetbot/taproot-bip-review/2019/taproot-bip-review.2019-11-12-19.00.log.html\n>\n> includes some discussion on that topic from the taproot review meetings.\n>\n> The difference between information in the annex and information in\n> either a script (or the input data for the script that is the rest of\n> the witness) is (in theory) that the annex can be analysed immediately\n> and unconditionally, without necessarily even knowing anything about\n> the utxo being spent.\n>\n\nI agree that should happen, but there are cases where this would not work.\nE.g., imagine OP_LISP_EVAL + OP_ANNEX... and then you do delegation via the\nthing in the annex.\n\nNow the annex can be executed as a script.\n\n\n\n>\n> The idea is that we would define some simple way of encoding (multiple)\n> entries into the annex -- perhaps a tag/length/value scheme like\n> lightning uses; maybe if we add a lisp scripting language to consensus,\n> we just reuse the list encoding from that? -- at which point we might\n> use one tag to specify that a transaction uses advanced computation, and\n> needs to be treated as having a heavier weight than its serialized size\n> implies; but we could use another tag for per-input absolute locktimes;\n> or another tag to commit to a past block height having a particular hash.\n>\n\nYes, this seems tough to do without redefining checksig to allow partial\nannexes. Hence thinking we should make our current checksig behavior\nrequire it be 0, future operations should be engineered with specific\nstructured annex in mind.\n\n\n\n>\n> It seems like a good place for optimising SIGHASH_GROUP (allowing a group\n> of inputs to claim a group of outputs for signing, but not allowing inputs\n> from different groups to ever claim the same output; so that each output\n> is hashed at most once for this purpose) -- since each input's validity\n> depends on the other inputs' state, it's better to be able to get at\n> that state as easily as possible rather than having to actually execute\n> other scripts before your can tell if your script is going to be valid.\n>\n\nI think SIGHASH_GROUP could be some sort of mutable stack value, not ANNEX.\nyou want to be able to compute what range you should sign, and then the\nsignature should cover the actual range not the argument itself.\n\nWhy sign the annex literally?\n\nWhy require that all signatures in one output sign the exact same digest?\nWhat if one wants to sign for value and another for value + change?\n\n\n\n>\n> > The BIP is tight lipped about it's purpose\n>\n> BIP341 only reserves an area to put the annex; it doesn't define how\n> it's used or why it should be used.\n>\n>\nIt does define how it's used, Checksig must commit to it. Were there no\nopcodes dependent on it I would agree, and that would be preferable.\n\n\n\n\n> > Essentially, I read this as saying: The annex is the ability to pad a\n> > transaction with an additional string of 0's\n>\n> If you wanted to pad it directly, you can do that in script already\n> with a PUSH/DROP combo.\n>\n\nYou cannot, because the push/drop would not be signed and would be\nmalleable.\n\nThe annex is not malleable, so it can be used to this as authenticated\npadding.\n\n\n\n>\n> The point of doing it in the annex is you could have a short byte\n> string, perhaps something like \"0x010201a4\" saying \"tag 1, data length 2\n> bytes, value 420\" and have the consensus intepretation of that be \"this\n> transaction should be treated as if it's 420 weight units more expensive\n> than its serialized size\", while only increasing its witness size by\n> 6 bytes (annex length, annex flag, and the four bytes above). Adding 6\n> bytes for a 426 weight unit increase seems much better than adding 426\n> witness bytes.\n>\n>\nYes, that's what I say in the next sentence,\n\n*> Or, we might somehow make the witness a small language (e.g., run length\nencoded zeros) such that we can very quickly compute an equivalent number\nof zeros to 'charge' without actually consuming the space but still\nconsuming a linearizable resource... or something like that.*\n\nso I think we concur on that.\n\n\n\n> > Introducing OP_ANNEX: Suppose there were some sort of annex pushing\n> opcode,\n> > OP_ANNEX which puts the annex on the stack\n>\n> I think you'd want to have a way of accessing individual entries from\n> the annex, rather than the annex as a single unit.\n>\n\nOr OP_ANNEX + OP_SUBSTR + OP_POVARINTSTR? Then you can just do 2 pops for\nthe length and the tag and then get the data.\n\n\n>\n> > Now suppose that I have a computation that I am running in a script as\n> > follows:\n> >\n> > OP_ANNEX\n> > OP_IF\n> >     `some operation that requires annex to be <1>`\n> > OP_ELSE\n> >     OP_SIZE\n> >     `some operation that requires annex to be len(annex) + 1 or does a\n> > checksig`\n> > OP_ENDIF\n> >\n> > Now every time you run this,\n>\n> You only run a script from a transaction once at which point its\n> annex is known (a different annex gives a different wtxid and breaks\n> any signatures), and can't reference previous or future transactions'\n> annexes...\n>\n>\nIn a transaction validator, yes. But in a satisfier, no.\n\nAnd it doesn't break the signatures if we add the ability to only sign over\na part of an annex either/multiple annexes, since the annex could be\nmutable partially.\n\n\nNot true about accessing previous TXNs annexes. All coins spend from\nCoinbase transactions. If you can get the COutpoint you're spending, you\ncan get the parent of the COutpoint... and iterate backwards so on and so\nforth. Then you have the CB txn, which commits to the tree of wtxids. So\nyou get previous transactions annexes comitted there.\n\n\nFor future transactions, you can, as a miner with decent hashrate you could\npromise what your Coinbase transaction would be for a future block and what\nthe Outputs would be, and then you can pop open that as well... but you\ncan't show valid PoW for that one so I'm not sure that's different than\nauthenticated data. But where it does have a use is that you could, if you\nhad OP_COUTPOINTVERIFY, say that this coin is only spendable if a miner\nmines the specific block that you want at a certain height (e.g., with only\nyour txn in it?) and then they can claim the outpoint in the future... so\nmaybe there is something there bizzare that can happen with that\ncapability....\n\n\n\n> > Because the Annex is signed, and must be the same, this can also be\n> > inconvenient:\n>\n> The annex is committed to by signatures in the same way nVersion,\n> nLockTime and nSequence are committed to by signatures; I think it helps\n> to think about it in a similar way.\n>\n\nnSequence, yes, nLockTime is per-tx.\n\nBTW i think we now consider nSeq/nLock to be misdesigned given desire to\nvary these per-input/per-tx....\\\n\nso if the annex is like these perhaps it's also misdesigned.\n\n>\n> > Suppose that you have a Miniscript that is something like: and(or(PK(A),\n> > PK(A')), X, or(PK(B), PK(B'))).\n> >\n> > A or A' should sign with B or B'. X is some sort of fragment that might\n> > require a value that is unknown (and maybe recursively defined?) so\n> > therefore if we send the PSBT to A first, which commits to the annex, and\n> > then X reads the annex and say it must be something else, A must sign\n> > again. So you might say, run X first, and then sign with A and C or B.\n> > However, what if the script somehow detects the bitstring WHICH_A WHICH_B\n> > and has a different Annex per selection (e.g., interpret the bitstring\n> as a\n> > int and annex must == that int). Now, given and(or(K1, K1'),... or(Kn,\n> > Kn')) we end up with needing to pre-sign 2**n annex values somehow...\n> this\n> > seems problematic theoretically.\n>\n> Note that you need to know what the annex will contain before you sign,\n> since the annex is committed to via the signature. If \"X\" will need\n> entries in the annex that aren't able to be calculated by the other\n> parties, then they need to be the first to contribute to the PSBT, not A.\n>\n> I think the analogy to locktimes would be \"I need the locktime to be at\n> least block 900k, should I just sign that now, or check that nobody else\n> is going to want it to be block 950k or something? Or should I just sign\n> with nLockTime at 900k, 910k, 920k, 930k, etc and let someone else pick\n> the right one?\" The obvious solution is just to work out what the\n> nLockTime should be first, then run signing rounds. Likewise, work out\n> what the annex should be first, then run the signing rounds.\n>\n\n\nYes, my point is this is computationally hard to do sometimes.\n\n>\n> CLTV also has the problem that if you have one script fragment with\n> CLTV by time, and another with CLTV by height, you can't come up with\n> an nLockTime that will ever satisfy both. If you somehow have script\n> fragments that require incompatible interpretations of the annex, you're\n> likewise going to be out of luck.\n>\n>\nYes, see above. If we don't know how the annex will be structured or used,\nthis is the point of this thread....\n\nWe need to drill down how to not introduce these problems.\n\n\n\n> Having a way of specifying locktimes in the annex can solve that\n> particular problem with CLTV (different inputs can sign different\n> locktimes, and you could have different tags for by-time/by-height so\n> that even the same input can have different clauses requiring both),\n> but the general problem still exists.\n>\n> (eg, you might have per-input by-height absolute locktimes as annex\n> entry 3, and per-input by-time absolute locktimes as annex entry 4,\n> so you might convert:\n>\n>  \"900e3 CLTV DROP\" -> \"900e3 3 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n>\n>  \"500e6 CLTV DROP\" -> \"500e6 4 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n>\n> for height/time locktime checks respectively)\n>\n> > Of course this wouldn't be miniscript then. Because miniscript is just\n> for\n> > the well behaved subset of script, and this seems ill behaved. So maybe\n> > we're OK?\n>\n> The CLTV issue hit miniscript:\n>\n> https://medium.com/blockstream/dont-mix-your-timelocks-d9939b665094\n\n\nMaybe the humour didn't hit -- we can only define well behaved as best we\nknow, and the solution was to re-define miniscript to only be the well\ndefined subset of miniscript once the bug in the spec was found.\n\n\n\n>\n> > It seems like one good option is if we just go on and banish the\n> OP_ANNEX.\n> > Maybe that solves some of this? I sort of think so. It definitely seems\n> > like we're not supposed to access it via script, given the quote from\n> above:\n>\n> How the annex works isn't defined, so it doesn't make any sense to\n> access it from script. When how it works is defined, I expect it might\n> well make sense to access it from script -- in a similar way that the\n> CLTV and CSV opcodes allow accessing nLockTime and nSequence from script.\n>\n\nThat's false: CLTV and CSV expressly do not allow accessing it from script,\nonly lower bounding it (and transitively proving that it was not of the\nother flavour).\n\nSo you can't actually get the exact nLockTime / Sequence on the stack\n(exception: if you use the maximum allowable value, then there are no other\nvalues...)\n\n\nGiven that it's not defined at all, that's why I'm skeptical about signing\nit at all presently.\n\nIf theres a future upgrade, it would be compatible as we can add new\nsighash flags to cover that.\n\n\n> > One solution would be to... just soft-fork it out. Always must be 0. When\n> > we come up with a use case for something like an annex, we can find a way\n> > to add it back.\n>\n> The point of reserving the annex the way it has been is exactly this --\n> it should not be used now, but when we agree on how it should be used,\n> we have an area that's immediately ready to be used.\n\n\n> (For the cases where you don't need script to enforce reasonable values,\n> reserving it now means those new consensus rules can be used immediately\n> with utxos that predate the new consensus rules -- so you could update\n> offchain contracts from per-tx to per-input locktimes immediately without\n> having to update the utxo on-chain first)\n>\n\nI highly doubt that we will not need new sighash flags once it is ready to\nallow partial covers of the annex, e.g. like the structured ones described\nabove.\n\nWe're already doing a soft fork for the new annex rules, so this isn't a\nbig deal...\n\nLegacy outputs can use these new sighash flags as well, in theory (maybe\nI'll do a post on why we shouldn't...)\n\n\n\nCheers,\n\nJeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/8fbd9e23/attachment-0001.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-07T08:08:03",
                "message_text_only": "On Sat, Mar 05, 2022 at 12:20:02PM +0000, Jeremy Rubin via bitcoin-dev wrote:\n> On Sat, Mar 5, 2022 at 5:59 AM Anthony Towns <aj at erisian.com.au> wrote:\n> > The difference between information in the annex and information in\n> > either a script (or the input data for the script that is the rest of\n> > the witness) is (in theory) that the annex can be analysed immediately\n> > and unconditionally, without necessarily even knowing anything about\n> > the utxo being spent.\n> I agree that should happen, but there are cases where this would not work.\n> E.g., imagine OP_LISP_EVAL + OP_ANNEX... and then you do delegation via the\n> thing in the annex.\n> Now the annex can be executed as a script.\n\nYou've got the implication backwards: the benefit isn't that the annex\n*can't* be used as/in a script; it's that it *can* be used *without*\nhaving to execute/analyse a script (and without even having to load the\nutxo being spent).\n\nHow big a benefit that is might be debatable -- it's only a different\nordering of the work you have to do to be sure the transaction is valid;\nit doesn't reduce the total work. And I think you can easily design\ninvalid transactions that will maximise the work required to establish\nthe tx is invalid, no matter what order you validate things.\n\n> Yes, this seems tough to do without redefining checksig to allow partial\n> annexes.\n\n\"Redefining checksig to allow X\" in taproot means \"defining a new pubkey\nformat that allows a new sighash that allows X\", which, if it turns out\nto be necessary/useful, is entirely possible.  It's not sensible to do\nwhat you suggest *now* though, because we don't have a spec of how a\npartial annex might look.\n\n> Hence thinking we should make our current checksig behavior\n> require it be 0,\n\nSignatures already require the annex to not be present. If you personally\nwant to do that for every future transaction you sign off on, you\nalready can.\n\n> > It seems like a good place for optimising SIGHASH_GROUP (allowing a group\n> > of inputs to claim a group of outputs for signing, but not allowing inputs\n> > from different groups to ever claim the same output; so that each output\n> > is hashed at most once for this purpose) -- since each input's validity\n> > depends on the other inputs' state, it's better to be able to get at\n> > that state as easily as possible rather than having to actually execute\n> > other scripts before your can tell if your script is going to be valid.\n> I think SIGHASH_GROUP could be some sort of mutable stack value, not ANNEX.\n\nThe annex is already a stack value, and the SIGHASH_GROUP parameter\ncannot be mutable since it will break the corresponding signature, and\n(in order to ensure validating SIGHASH_GROUP signatures don't require\nhashing the same output multiple times) also impacts SIGHASH_GROUP\nsignatures from other inputs.\n\n> you want to be able to compute what range you should sign, and then the\n> signature should cover the actual range not the argument itself.\n\nThe value that SIGHASH_GROUP proposes putting in the annex is just an\nindication of whether (a) this input is using the same output group as\nthe previous input; or else (b) how many outputs are in this input's\noutput group. The signature naturally commits to that value because it's\nsigning all the outputs in the group anyway.\n\n> Why sign the annex literally?\n\nTo prevent it from being third-party malleable.\n\nWhen there is some meaning assigned to the annex then perhaps it will\nmake sense to add some more granular way of accessing it via script, but\nuntil then, committing to the whole thing is the best option possible,\nsince it still allows some potential uses of the annex without having\nto define a new sighash.\n\nNote that signing only part of the annex means that you probably\nreintroduce the quadratic hashing problem -- that is, with a script of\nlength X and an annex of length Y, you may have to hash O(X*Y) bytes\ninstead of O(X+Y) bytes (because every X/k bytes of the script selects\na different Y/j subset of the annex to sign).\n\n> Why require that all signatures in one output sign the exact same digest?\n> What if one wants to sign for value and another for value + change?\n\nYou can already have one signature for value and one for value+change:\nuse SIGHASH_SINGLE for the former, and SIGHASH_ALL for the latter.\nSIGHASH_GROUP is designed for the case where the \"value\" goes to\nmultiple places.\n\n> > > Essentially, I read this as saying: The annex is the ability to pad a\n> > > transaction with an additional string of 0's\n> > If you wanted to pad it directly, you can do that in script already\n> > with a PUSH/DROP combo.\n> You cannot, because the push/drop would not be signed and would be\n> malleable.\n\nIf it's a PUSH, then it's in the tapscript and committed to by the\nscriptPubKey, and not malleable.\n\nThere's currently no reason to have padding specifiable at spend time --\nyou know when you're writing the script whether the spender can reuse\nthe same signature for multiple CHECKSIG ops, because the only way to\ndo that is to add DUP/etc opcodes -- so if you're doing that, you can\nadd any necessary padding at the same time.\n\n> The annex is not malleable, so it can be used to this as authenticated\n> padding.\n\nThe reason that the annex is not third-party malleable is that its\ncontent is committed to by signatures.\n\n> > The point of doing it in the annex is you could have a short byte\n> > string, perhaps something like \"0x010201a4\" saying \"tag 1, data length 2\n> > bytes, value 420\" and have the consensus intepretation of that be \"this\n> > transaction should be treated as if it's 420 weight units more expensive\n> > than its serialized size\", while only increasing its witness size by\n> > 6 bytes (annex length, annex flag, and the four bytes above). Adding 6\n> > bytes for a 426 weight unit increase seems much better than adding 426\n> > witness bytes.\n> Yes, that's what I say in the next sentence,\n> *> Or, we might somehow make the witness a small language (e.g., run length\n> encoded zeros)\n\nIf you're doing run-length encoding, you might as well just use gzip at\nthe p2p and storage layers; you don't need to touch consensus at all.\nThat's not an extensible or particularly interesting idea.\n\n> > > Introducing OP_ANNEX: Suppose there were some sort of annex pushing\n> > opcode,\n> > > OP_ANNEX which puts the annex on the stack\n> > I think you'd want to have a way of accessing individual entries from\n> > the annex, rather than the annex as a single unit.\n> Or OP_ANNEX + OP_SUBSTR + OP_POVARINTSTR? Then you can just do 2 pops for\n> the length and the tag and then get the data.\n\nIf you want to make things as inconvenient as possible, sure, I guess?\n\n> > > Now every time you run this,\n> > You only run a script from a transaction once at which point its\n> > annex is known (a different annex gives a different wtxid and breaks\n> > any signatures), and can't reference previous or future transactions'\n> > annexes...\n> In a transaction validator, yes. But in a satisfier, no.\n\nIn a satisfier you don't \"run\" a script, you provide a solution to\nthe script...\n\nYou can certainly create scripts where it's not possible to provide\nvalid solutions, eg:\n\n    DUP EQUAL NOT VERIFY\n\nor where it's theoretically possible but in practice extremely difficult\nto provide solutions, eg:\n\n    DUP 2 <P> <Q> 2 CHECKMULTISIG\n    2DUP EQUAL NOT VERIFY SHA256 SWAP SHA256 EQUAL\n\nor where the difficulty is known and there really isn't an easier way\nof coming up with a solution than doing multiple guesses and validating\nthe result:\n\n    SIZE 80 EQUAL NOT VERIFY HASH256 0 18 SUBSTR 0 NUMEQUAL\n\nBut if you don't want to make life difficult for yourself, the answer's\npretty easy: just don't do those things. Or, at a higher level, don't\ndesign new opcodes where you have to do those sorts of things.\n\n> Not true about accessing previous TXNs annexes. All coins spend from\n> Coinbase transactions. If you can get the COutpoint you're spending, you\n> can get the parent of the COutpoint... and iterate backwards so on and so\n> forth. Then you have the CB txn, which commits to the tree of wtxids. So\n> you get previous transactions annexes comitted there.\n\nNone of that information is stored in the utxo database or accessible at\nvalidation time. Adding that information would make the utxo database\nmuch larger, increasing the costs of running a node, and increasing\nvalidation time for each transaction/block.\n\n> For future transactions,\n\n(For future transactions, if you had generic recursive covenants and\na opcode to examine the annex, you could prevent spending without a\nparticular value appearing in the annex; that doesn't let you \"inspect\"\na future annex, though)\n\n> > > Because the Annex is signed, and must be the same, this can also be\n> > > inconvenient:\n> > The annex is committed to by signatures in the same way nVersion,\n> > nLockTime and nSequence are committed to by signatures; I think it helps\n> > to think about it in a similar way.\n> nSequence, yes, nLockTime is per-tx.\n\nnVersion is also per-tx not per-input. You still need to establish all\nthree of them before you start signing things.\n\n> BTW i think we now consider nSeq/nLock to be misdesigned given desire to\n> vary these per-input/per-tx....\\\n\nSince nSequence is per-input, you can obviously vary that per-input; and\nyou can vary all three per-tx.\n\n> > > Suppose that you have a Miniscript that is something like: and(or(PK(A),\n> > > PK(A')), X, or(PK(B), PK(B'))).\n> Yes, my point is this is computationally hard to do sometimes.\n\nSometimes, what makes things computationally hard is that you've got\nthe wrong approach to looking at the problem.\n\n> > CLTV also has the problem that if you have one script fragment with\n> > CLTV by time, and another with CLTV by height, you can't come up with\n> > an nLockTime that will ever satisfy both. If you somehow have script\n> > fragments that require incompatible interpretations of the annex, you're\n> > likewise going to be out of luck.\n> Yes, see above. If we don't know how the annex will be structured or used,\n\nIf you don't know how the annex will be structured or used, don't use\nit. That's exactly how things are today, because no one knows how it\nwill be structured or used.\n\n> this is the point of this thread....\n> We need to drill down how to not introduce these problems.\n\n>From where I sit, it looks like you're drawing hasty conclusions based\non a lot of misconceptions. That's not the way you avoid introducing\nproblems...\n\nI mean, having the misconceptions is perfectly reasonable; if anyone\nknew exactly how annex things should work, we'd have a spec already. It's\nleaping straight to \"this is the only way it can work, it's a dumb way,\nand therefore we should throw this out immediately\" that I don't really\nsee the humour in.\n\n> > > It seems like one good option is if we just go on and banish the\n> > OP_ANNEX.\n> > > Maybe that solves some of this? I sort of think so. It definitely seems\n> > > like we're not supposed to access it via script, given the quote from\n> > above:\n> > How the annex works isn't defined, so it doesn't make any sense to\n> > access it from script. When how it works is defined, I expect it might\n> > well make sense to access it from script -- in a similar way that the\n> > CLTV and CSV opcodes allow accessing nLockTime and nSequence from script.\n> That's false: CLTV and CSV expressly do not allow accessing it from script,\n> only lower bounding it\n\nLower bounding something requires accessing it.\n\nThat CLTV/CSV only allows lower-bounding it rather than more arbitrary\nmanipulation is mostly due to having to be implemented via upgrading an\nOP_NOP opcode, rather than any other reason, IMHO.\n\n> Legacy outputs can use these new sighash flags as well, in theory (maybe\n> I'll do a post on why we shouldn't...)\n\nExisting outputs can't use new sighash flags introduced by a soft fork --\nif they could, then those outputs would have been anyone-can-spend prior\nto the soft fork activating, because node software that doesn't support\nthe soft fork isn't able to calculate the message that the signature\napplies to, so can't reject invalid signatures.\n\nPerhaps you mean \"we could replace OP_NOPx by OP_CHECKSIGv2 and allow\ncreating new p2wsh or p2sh addresses that can be spent using the new\nflags\", but I can't really think why anyone would bring that up at\nthis point, except as a way of deliberately wasting people's time and\nattention...\n\nCheers,\naj"
            },
            {
                "author": "Christian Decker",
                "date": "2022-03-06T13:12:52",
                "message_text_only": "One thing that we recently stumbled over was that we use CLTV in eltoo not\nfor timelock but to have a comparison between two committed numbers coming\nfrom the spent and the spending transaction (ordering requirement of\nstates). We couldn't use a number on the stack of the scriptSig as the\nsignature doesn't commit to it, which is why we commandeered nLocktime\nvalues that are already in the past.\n\nWith the annex we could have a way to get a committed to number we can pull\nonto the stack, and free the nLocktime for other uses again. It'd also be\nless roundabout to explain in classes :-)\n\nAn added benefit would be that update transactions, being singlesig, can be\ncombined into larger transactions by third parties or watchtowers to\namortize some of the fixed cost of getting them confirmed, allowing\non-path-aggregation basically (each node can group and aggregate\ntransactions as they forward them). This is currently not possible since\nall the transactions that we'd like to batch would have to have the same\nnLocktime at the moment.\n\nSo I think it makes sense to partition the annex into a global annex shared\nby the entire transaction, and one for each input. Not sure if one for\ninputs would also make sense as it'd bloat the utxo set and could be\nemulated by using the input that is spending it.\n\nCheers,\nChristian\n\nOn Sat, 5 Mar 2022, 07:33 Anthony Towns via bitcoin-dev, <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Mar 04, 2022 at 11:21:41PM +0000, Jeremy Rubin via bitcoin-dev\n> wrote:\n> > I've seen some discussion of what the Annex can be used for in Bitcoin.\n>\n>\n> https://www.erisian.com.au/meetbot/taproot-bip-review/2019/taproot-bip-review.2019-11-12-19.00.log.html\n>\n> includes some discussion on that topic from the taproot review meetings.\n>\n> The difference between information in the annex and information in\n> either a script (or the input data for the script that is the rest of\n> the witness) is (in theory) that the annex can be analysed immediately\n> and unconditionally, without necessarily even knowing anything about\n> the utxo being spent.\n>\n> The idea is that we would define some simple way of encoding (multiple)\n> entries into the annex -- perhaps a tag/length/value scheme like\n> lightning uses; maybe if we add a lisp scripting language to consensus,\n> we just reuse the list encoding from that? -- at which point we might\n> use one tag to specify that a transaction uses advanced computation, and\n> needs to be treated as having a heavier weight than its serialized size\n> implies; but we could use another tag for per-input absolute locktimes;\n> or another tag to commit to a past block height having a particular hash.\n>\n> It seems like a good place for optimising SIGHASH_GROUP (allowing a group\n> of inputs to claim a group of outputs for signing, but not allowing inputs\n> from different groups to ever claim the same output; so that each output\n> is hashed at most once for this purpose) -- since each input's validity\n> depends on the other inputs' state, it's better to be able to get at\n> that state as easily as possible rather than having to actually execute\n> other scripts before your can tell if your script is going to be valid.\n>\n> > The BIP is tight lipped about it's purpose\n>\n> BIP341 only reserves an area to put the annex; it doesn't define how\n> it's used or why it should be used.\n>\n> > Essentially, I read this as saying: The annex is the ability to pad a\n> > transaction with an additional string of 0's\n>\n> If you wanted to pad it directly, you can do that in script already\n> with a PUSH/DROP combo.\n>\n> The point of doing it in the annex is you could have a short byte\n> string, perhaps something like \"0x010201a4\" saying \"tag 1, data length 2\n> bytes, value 420\" and have the consensus intepretation of that be \"this\n> transaction should be treated as if it's 420 weight units more expensive\n> than its serialized size\", while only increasing its witness size by\n> 6 bytes (annex length, annex flag, and the four bytes above). Adding 6\n> bytes for a 426 weight unit increase seems much better than adding 426\n> witness bytes.\n>\n> The example scenario is that if there was an opcode to verify a\n> zero-knowledge proof, eg I think bulletproof range proofs are something\n> like 10x longer than a signature, but require something like 400x the\n> validation time. Since checksig has a validation weight of 50 units,\n> a bulletproof verify might have a 400x greater validation weight, ie\n> 20,000 units, while your witness data is only 650 bytes serialized. In\n> that case, we'd need to artificially bump the weight of you transaction\n> up by the missing 19,350 units, or else an attacker could fill a block\n> with perhaps 6000 bulletproofs costing the equivalent of 120M signature\n> operations, rather than the 80k sigops we currently expect as the maximum\n> in a block. Seems better to just have \"0x01024b96\" stuck in the annex,\n> than 19kB of zeroes.\n>\n> > Introducing OP_ANNEX: Suppose there were some sort of annex pushing\n> opcode,\n> > OP_ANNEX which puts the annex on the stack\n>\n> I think you'd want to have a way of accessing individual entries from\n> the annex, rather than the annex as a single unit.\n>\n> > Now suppose that I have a computation that I am running in a script as\n> > follows:\n> >\n> > OP_ANNEX\n> > OP_IF\n> >     `some operation that requires annex to be <1>`\n> > OP_ELSE\n> >     OP_SIZE\n> >     `some operation that requires annex to be len(annex) + 1 or does a\n> > checksig`\n> > OP_ENDIF\n> >\n> > Now every time you run this,\n>\n> You only run a script from a transaction once at which point its\n> annex is known (a different annex gives a different wtxid and breaks\n> any signatures), and can't reference previous or future transactions'\n> annexes...\n>\n> > Because the Annex is signed, and must be the same, this can also be\n> > inconvenient:\n>\n> The annex is committed to by signatures in the same way nVersion,\n> nLockTime and nSequence are committed to by signatures; I think it helps\n> to think about it in a similar way.\n>\n> > Suppose that you have a Miniscript that is something like: and(or(PK(A),\n> > PK(A')), X, or(PK(B), PK(B'))).\n> >\n> > A or A' should sign with B or B'. X is some sort of fragment that might\n> > require a value that is unknown (and maybe recursively defined?) so\n> > therefore if we send the PSBT to A first, which commits to the annex, and\n> > then X reads the annex and say it must be something else, A must sign\n> > again. So you might say, run X first, and then sign with A and C or B.\n> > However, what if the script somehow detects the bitstring WHICH_A WHICH_B\n> > and has a different Annex per selection (e.g., interpret the bitstring\n> as a\n> > int and annex must == that int). Now, given and(or(K1, K1'),... or(Kn,\n> > Kn')) we end up with needing to pre-sign 2**n annex values somehow...\n> this\n> > seems problematic theoretically.\n>\n> Note that you need to know what the annex will contain before you sign,\n> since the annex is committed to via the signature. If \"X\" will need\n> entries in the annex that aren't able to be calculated by the other\n> parties, then they need to be the first to contribute to the PSBT, not A.\n>\n> I think the analogy to locktimes would be \"I need the locktime to be at\n> least block 900k, should I just sign that now, or check that nobody else\n> is going to want it to be block 950k or something? Or should I just sign\n> with nLockTime at 900k, 910k, 920k, 930k, etc and let someone else pick\n> the right one?\" The obvious solution is just to work out what the\n> nLockTime should be first, then run signing rounds. Likewise, work out\n> what the annex should be first, then run the signing rounds.\n>\n> CLTV also has the problem that if you have one script fragment with\n> CLTV by time, and another with CLTV by height, you can't come up with\n> an nLockTime that will ever satisfy both. If you somehow have script\n> fragments that require incompatible interpretations of the annex, you're\n> likewise going to be out of luck.\n>\n> Having a way of specifying locktimes in the annex can solve that\n> particular problem with CLTV (different inputs can sign different\n> locktimes, and you could have different tags for by-time/by-height so\n> that even the same input can have different clauses requiring both),\n> but the general problem still exists.\n>\n> (eg, you might have per-input by-height absolute locktimes as annex\n> entry 3, and per-input by-time absolute locktimes as annex entry 4,\n> so you might convert:\n>\n>  \"900e3 CLTV DROP\" -> \"900e3 3 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n>\n>  \"500e6 CLTV DROP\" -> \"500e6 4 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n>\n> for height/time locktime checks respectively)\n>\n> > Of course this wouldn't be miniscript then. Because miniscript is just\n> for\n> > the well behaved subset of script, and this seems ill behaved. So maybe\n> > we're OK?\n>\n> The CLTV issue hit miniscript:\n>\n> https://medium.com/blockstream/dont-mix-your-timelocks-d9939b665094\n>\n> > But I think the issue still arises where suppose I have a simple thing\n> > like: and(COLD_LOGIC, HOT_LOGIC) where both contains a signature, if\n> > COLD_LOGIC and HOT_LOGIC can both have different costs, I need to decide\n> > what logic each satisfier for the branch is going to use in advance, or\n> > sign all possible sums of both our annex costs? This could come up if\n> > cold/hot e.g. use different numbers of signatures / use checksigCISAadd\n> > which maybe requires an annex argument.\n>\n> Signatures pay for themselves -- every signature is 64 or 65 bytes,\n> but only has 50 units of validation weight. (That is, a signature check\n> is about 50x the cost of hashing 520 bytes of data, which is the next\n> highest cost operation we have, and is treated as costing 1 unit, and\n> immediately paid for by the 1 byte that writing OP_HASH256 takes up)\n>\n> That's why the \"add cost\" use of the annex is only talked about in\n> hypotheticals, not specified -- for reasonable scripts with today's\n> opcodes, it's not needed.\n>\n> If you're doing cross-input signature aggregation, everybody needs to\n> agree on the message they're signing in the first place, so you definitely\n> can't delay figuring out some bits of some annex until after signing.\n>\n> > It seems like one good option is if we just go on and banish the\n> OP_ANNEX.\n> > Maybe that solves some of this? I sort of think so. It definitely seems\n> > like we're not supposed to access it via script, given the quote from\n> above:\n>\n> How the annex works isn't defined, so it doesn't make any sense to\n> access it from script. When how it works is defined, I expect it might\n> well make sense to access it from script -- in a similar way that the\n> CLTV and CSV opcodes allow accessing nLockTime and nSequence from script.\n>\n> To expand on that: the logic to prevent a transaction confirming too\n> early occurs by looking at nLockTime and nSequence, but script can\n> ensure that an attempt to use \"bad\" values for those can never be a\n> valid transaction; likewise, consensus may look at the annex to enforce\n> new conditions as to when a transaction might be valid (and can do so\n> without needing to evaluate any scripts), but the individual scripts can\n> make sure that the annex has been set to what the utxo owner considered\n> to be reasonable values.\n>\n> > One solution would be to... just soft-fork it out. Always must be 0. When\n> > we come up with a use case for something like an annex, we can find a way\n> > to add it back.\n>\n> The point of reserving the annex the way it has been is exactly this --\n> it should not be used now, but when we agree on how it should be used,\n> we have an area that's immediately ready to be used.\n>\n> (For the cases where you don't need script to enforce reasonable values,\n> reserving it now means those new consensus rules can be used immediately\n> with utxos that predate the new consensus rules -- so you could update\n> offchain contracts from per-tx to per-input locktimes immediately without\n> having to update the utxo on-chain first)\n>\n> Cheers,\n> aj\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/1fc4e790/attachment-0001.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-06T13:21:57",
                "message_text_only": "Hi Christian,\n\nFor that purpose I'd recommend having a checksig extra that is\n\n<data> <n> <sig> <pk> checksigextra that allows N extra data items on the\nstack in addition to the txn hash. This would allow signers to sign some\naddtl arguments, but would not be an annex since the values would not have\nany consensus meaning (whereas annex is designed to have one)\n\n\nI've previously discussed this for eltoo with giving signatures an explicit\nextra seqnum, but it can be generalized as above.\n\n\n\nW.r.t. pinning, if the annex is a pure function of the script execution,\nthen there's no issue with letting it be mutable (e.g. for a validation\ncost hint). But permitting both validation cost commitments and stack\nreadability is asking too much of the annex IMO.\n\nOn Sun, Mar 6, 2022, 1:13 PM Christian Decker <decker.christian at gmail.com>\nwrote:\n\n> One thing that we recently stumbled over was that we use CLTV in eltoo not\n> for timelock but to have a comparison between two committed numbers coming\n> from the spent and the spending transaction (ordering requirement of\n> states). We couldn't use a number on the stack of the scriptSig as the\n> signature doesn't commit to it, which is why we commandeered nLocktime\n> values that are already in the past.\n>\n> With the annex we could have a way to get a committed to number we can\n> pull onto the stack, and free the nLocktime for other uses again. It'd also\n> be less roundabout to explain in classes :-)\n>\n> An added benefit would be that update transactions, being singlesig, can\n> be combined into larger transactions by third parties or watchtowers to\n> amortize some of the fixed cost of getting them confirmed, allowing\n> on-path-aggregation basically (each node can group and aggregate\n> transactions as they forward them). This is currently not possible since\n> all the transactions that we'd like to batch would have to have the same\n> nLocktime at the moment.\n>\n> So I think it makes sense to partition the annex into a global annex\n> shared by the entire transaction, and one for each input. Not sure if one\n> for inputs would also make sense as it'd bloat the utxo set and could be\n> emulated by using the input that is spending it.\n>\n> Cheers,\n> Christian\n>\n> On Sat, 5 Mar 2022, 07:33 Anthony Towns via bitcoin-dev, <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On Fri, Mar 04, 2022 at 11:21:41PM +0000, Jeremy Rubin via bitcoin-dev\n>> wrote:\n>> > I've seen some discussion of what the Annex can be used for in Bitcoin.\n>>\n>>\n>> https://www.erisian.com.au/meetbot/taproot-bip-review/2019/taproot-bip-review.2019-11-12-19.00.log.html\n>>\n>> includes some discussion on that topic from the taproot review meetings.\n>>\n>> The difference between information in the annex and information in\n>> either a script (or the input data for the script that is the rest of\n>> the witness) is (in theory) that the annex can be analysed immediately\n>> and unconditionally, without necessarily even knowing anything about\n>> the utxo being spent.\n>>\n>> The idea is that we would define some simple way of encoding (multiple)\n>> entries into the annex -- perhaps a tag/length/value scheme like\n>> lightning uses; maybe if we add a lisp scripting language to consensus,\n>> we just reuse the list encoding from that? -- at which point we might\n>> use one tag to specify that a transaction uses advanced computation, and\n>> needs to be treated as having a heavier weight than its serialized size\n>> implies; but we could use another tag for per-input absolute locktimes;\n>> or another tag to commit to a past block height having a particular hash.\n>>\n>> It seems like a good place for optimising SIGHASH_GROUP (allowing a group\n>> of inputs to claim a group of outputs for signing, but not allowing inputs\n>> from different groups to ever claim the same output; so that each output\n>> is hashed at most once for this purpose) -- since each input's validity\n>> depends on the other inputs' state, it's better to be able to get at\n>> that state as easily as possible rather than having to actually execute\n>> other scripts before your can tell if your script is going to be valid.\n>>\n>> > The BIP is tight lipped about it's purpose\n>>\n>> BIP341 only reserves an area to put the annex; it doesn't define how\n>> it's used or why it should be used.\n>>\n>> > Essentially, I read this as saying: The annex is the ability to pad a\n>> > transaction with an additional string of 0's\n>>\n>> If you wanted to pad it directly, you can do that in script already\n>> with a PUSH/DROP combo.\n>>\n>> The point of doing it in the annex is you could have a short byte\n>> string, perhaps something like \"0x010201a4\" saying \"tag 1, data length 2\n>> bytes, value 420\" and have the consensus intepretation of that be \"this\n>> transaction should be treated as if it's 420 weight units more expensive\n>> than its serialized size\", while only increasing its witness size by\n>> 6 bytes (annex length, annex flag, and the four bytes above). Adding 6\n>> bytes for a 426 weight unit increase seems much better than adding 426\n>> witness bytes.\n>>\n>> The example scenario is that if there was an opcode to verify a\n>> zero-knowledge proof, eg I think bulletproof range proofs are something\n>> like 10x longer than a signature, but require something like 400x the\n>> validation time. Since checksig has a validation weight of 50 units,\n>> a bulletproof verify might have a 400x greater validation weight, ie\n>> 20,000 units, while your witness data is only 650 bytes serialized. In\n>> that case, we'd need to artificially bump the weight of you transaction\n>> up by the missing 19,350 units, or else an attacker could fill a block\n>> with perhaps 6000 bulletproofs costing the equivalent of 120M signature\n>> operations, rather than the 80k sigops we currently expect as the maximum\n>> in a block. Seems better to just have \"0x01024b96\" stuck in the annex,\n>> than 19kB of zeroes.\n>>\n>> > Introducing OP_ANNEX: Suppose there were some sort of annex pushing\n>> opcode,\n>> > OP_ANNEX which puts the annex on the stack\n>>\n>> I think you'd want to have a way of accessing individual entries from\n>> the annex, rather than the annex as a single unit.\n>>\n>> > Now suppose that I have a computation that I am running in a script as\n>> > follows:\n>> >\n>> > OP_ANNEX\n>> > OP_IF\n>> >     `some operation that requires annex to be <1>`\n>> > OP_ELSE\n>> >     OP_SIZE\n>> >     `some operation that requires annex to be len(annex) + 1 or does a\n>> > checksig`\n>> > OP_ENDIF\n>> >\n>> > Now every time you run this,\n>>\n>> You only run a script from a transaction once at which point its\n>> annex is known (a different annex gives a different wtxid and breaks\n>> any signatures), and can't reference previous or future transactions'\n>> annexes...\n>>\n>> > Because the Annex is signed, and must be the same, this can also be\n>> > inconvenient:\n>>\n>> The annex is committed to by signatures in the same way nVersion,\n>> nLockTime and nSequence are committed to by signatures; I think it helps\n>> to think about it in a similar way.\n>>\n>> > Suppose that you have a Miniscript that is something like: and(or(PK(A),\n>> > PK(A')), X, or(PK(B), PK(B'))).\n>> >\n>> > A or A' should sign with B or B'. X is some sort of fragment that might\n>> > require a value that is unknown (and maybe recursively defined?) so\n>> > therefore if we send the PSBT to A first, which commits to the annex,\n>> and\n>> > then X reads the annex and say it must be something else, A must sign\n>> > again. So you might say, run X first, and then sign with A and C or B.\n>> > However, what if the script somehow detects the bitstring WHICH_A\n>> WHICH_B\n>> > and has a different Annex per selection (e.g., interpret the bitstring\n>> as a\n>> > int and annex must == that int). Now, given and(or(K1, K1'),... or(Kn,\n>> > Kn')) we end up with needing to pre-sign 2**n annex values somehow...\n>> this\n>> > seems problematic theoretically.\n>>\n>> Note that you need to know what the annex will contain before you sign,\n>> since the annex is committed to via the signature. If \"X\" will need\n>> entries in the annex that aren't able to be calculated by the other\n>> parties, then they need to be the first to contribute to the PSBT, not A.\n>>\n>> I think the analogy to locktimes would be \"I need the locktime to be at\n>> least block 900k, should I just sign that now, or check that nobody else\n>> is going to want it to be block 950k or something? Or should I just sign\n>> with nLockTime at 900k, 910k, 920k, 930k, etc and let someone else pick\n>> the right one?\" The obvious solution is just to work out what the\n>> nLockTime should be first, then run signing rounds. Likewise, work out\n>> what the annex should be first, then run the signing rounds.\n>>\n>> CLTV also has the problem that if you have one script fragment with\n>> CLTV by time, and another with CLTV by height, you can't come up with\n>> an nLockTime that will ever satisfy both. If you somehow have script\n>> fragments that require incompatible interpretations of the annex, you're\n>> likewise going to be out of luck.\n>>\n>> Having a way of specifying locktimes in the annex can solve that\n>> particular problem with CLTV (different inputs can sign different\n>> locktimes, and you could have different tags for by-time/by-height so\n>> that even the same input can have different clauses requiring both),\n>> but the general problem still exists.\n>>\n>> (eg, you might have per-input by-height absolute locktimes as annex\n>> entry 3, and per-input by-time absolute locktimes as annex entry 4,\n>> so you might convert:\n>>\n>>  \"900e3 CLTV DROP\" -> \"900e3 3 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n>>\n>>  \"500e6 CLTV DROP\" -> \"500e6 4 PUSH_ANNEX_ENTRY GREATERTHANOREQUAL VERIFY\"\n>>\n>> for height/time locktime checks respectively)\n>>\n>> > Of course this wouldn't be miniscript then. Because miniscript is just\n>> for\n>> > the well behaved subset of script, and this seems ill behaved. So maybe\n>> > we're OK?\n>>\n>> The CLTV issue hit miniscript:\n>>\n>> https://medium.com/blockstream/dont-mix-your-timelocks-d9939b665094\n>>\n>> > But I think the issue still arises where suppose I have a simple thing\n>> > like: and(COLD_LOGIC, HOT_LOGIC) where both contains a signature, if\n>> > COLD_LOGIC and HOT_LOGIC can both have different costs, I need to decide\n>> > what logic each satisfier for the branch is going to use in advance, or\n>> > sign all possible sums of both our annex costs? This could come up if\n>> > cold/hot e.g. use different numbers of signatures / use checksigCISAadd\n>> > which maybe requires an annex argument.\n>>\n>> Signatures pay for themselves -- every signature is 64 or 65 bytes,\n>> but only has 50 units of validation weight. (That is, a signature check\n>> is about 50x the cost of hashing 520 bytes of data, which is the next\n>> highest cost operation we have, and is treated as costing 1 unit, and\n>> immediately paid for by the 1 byte that writing OP_HASH256 takes up)\n>>\n>> That's why the \"add cost\" use of the annex is only talked about in\n>> hypotheticals, not specified -- for reasonable scripts with today's\n>> opcodes, it's not needed.\n>>\n>> If you're doing cross-input signature aggregation, everybody needs to\n>> agree on the message they're signing in the first place, so you definitely\n>> can't delay figuring out some bits of some annex until after signing.\n>>\n>> > It seems like one good option is if we just go on and banish the\n>> OP_ANNEX.\n>> > Maybe that solves some of this? I sort of think so. It definitely seems\n>> > like we're not supposed to access it via script, given the quote from\n>> above:\n>>\n>> How the annex works isn't defined, so it doesn't make any sense to\n>> access it from script. When how it works is defined, I expect it might\n>> well make sense to access it from script -- in a similar way that the\n>> CLTV and CSV opcodes allow accessing nLockTime and nSequence from script.\n>>\n>> To expand on that: the logic to prevent a transaction confirming too\n>> early occurs by looking at nLockTime and nSequence, but script can\n>> ensure that an attempt to use \"bad\" values for those can never be a\n>> valid transaction; likewise, consensus may look at the annex to enforce\n>> new conditions as to when a transaction might be valid (and can do so\n>> without needing to evaluate any scripts), but the individual scripts can\n>> make sure that the annex has been set to what the utxo owner considered\n>> to be reasonable values.\n>>\n>> > One solution would be to... just soft-fork it out. Always must be 0.\n>> When\n>> > we come up with a use case for something like an annex, we can find a\n>> way\n>> > to add it back.\n>>\n>> The point of reserving the annex the way it has been is exactly this --\n>> it should not be used now, but when we agree on how it should be used,\n>> we have an area that's immediately ready to be used.\n>>\n>> (For the cases where you don't need script to enforce reasonable values,\n>> reserving it now means those new consensus rules can be used immediately\n>> with utxos that predate the new consensus rules -- so you could update\n>> offchain contracts from per-tx to per-input locktimes immediately without\n>> having to update the utxo on-chain first)\n>>\n>> Cheers,\n>> aj\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/b83e5d18/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-03-07T00:59:33",
                "message_text_only": "Hi Jeremy,\n\n> I've seen some discussion of what the Annex can be used for in Bitcoin.\nFor\n> example, some people have discussed using the annex as a data field for\n> something like CHECKSIGFROMSTACK type stuff (additional authenticated\ndata)\n> or for something like delegation (the delegation is to the annex). I think\n> before devs get too excited, we should have an open discussion about what\n> this is actually for, and figure out if there are any constraints to using\n> it however we may please.\n\nI think one interesting purpose of the annex is to serve as a transaction\nfield extension, where we assign new consensus validity rules to the annex\npayloads.\n\nOne could think about new types of locks, e.g where a transaction inclusion\nis constrained before the annex payload value is superior to the chain's\n`ChainWork`. This could be useful in case of contentious forks, where you\nwant your transaction to confirm only when enough work is accumulated, and\nheight isn't a reliable indicator anymore.\n\nOr a relative-timelock where the endpoint is the presence of a state number\nencumbering the spent transaction. This could be useful in the context of\npayment pools, where the user withdraw transactions are all encumbered by a\nbip68 relative-timelock, as you don't know which one is going to confirm\nfirst, but where you don't care about enforcement of the timelocks once the\ncontestation delay has played once  and no higher-state update transaction\nhas confirmed.\n\nOf course, we could reuse the nSequence field for some of those new types\nof locks, though we would lose the flexibility of combining multiple locks\nencumbering the same input.\n\nAnother use for the annex is locating there the SIGHASH_GROUP group count\nvalue. One advantage over placing the value as a script stack item could be\nto have annex payloads interdependency validity, where other annex payloads\nare reusing the group count value as part of their own semantics.\n\nAntoine\n\nLe ven. 4 mars 2022 \u00e0 18:22, Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> I've seen some discussion of what the Annex can be used for in Bitcoin.\n> For example, some people have discussed using the annex as a data field for\n> something like CHECKSIGFROMSTACK type stuff (additional authenticated data)\n> or for something like delegation (the delegation is to the annex). I think\n> before devs get too excited, we should have an open discussion about what\n> this is actually for, and figure out if there are any constraints to using\n> it however we may please.\n>\n> The BIP is tight lipped about it's purpose, saying mostly only:\n>\n> *What is the purpose of the annex? The annex is a reserved space for\n> future extensions, such as indicating the validation costs of\n> computationally expensive new opcodes in a way that is recognizable without\n> knowing the scriptPubKey of the output being spent. Until the meaning of\n> this field is defined by another softfork, users SHOULD NOT include annex\n> in transactions, or it may lead to PERMANENT FUND LOSS.*\n>\n> *The annex (or the lack of thereof) is always covered by the signature and\n> contributes to transaction weight, but is otherwise ignored during taproot\n> validation.*\n>\n> *Execute the script, according to the applicable script rules[11], using\n> the witness stack elements excluding the script s, the control block c, and\n> the annex a if present, as initial stack.*\n>\n> Essentially, I read this as saying: The annex is the ability to pad a\n> transaction with an additional string of 0's that contribute to the virtual\n> weight of a transaction, but has no validation cost itself. Therefore,\n> somehow, if you needed to validate more signatures than 1 per 50 virtual\n> weight units, you could add padding to buy extra gas. Or, we might somehow\n> make the witness a small language (e.g., run length encoded zeros) such\n> that we can very quickly compute an equivalent number of zeros to 'charge'\n> without actually consuming the space but still consuming a linearizable\n> resource... or something like that. We might also e.g. want to use the\n> annex to reserve something else, like the amount of memory. In general, we\n> are using the annex to express a resource constraint efficiently. This\n> might be useful for e.g. simplicity one day.\n>\n> Generating an Annex: One should write a tracing executor for a script, run\n> it, measure the resource costs, and then generate an annex that captures\n> any externalized costs.\n>\n> -------------------\n>\n> Introducing OP_ANNEX: Suppose there were some sort of annex pushing\n> opcode, OP_ANNEX which puts the annex on the stack as well as a 0 or 1 (to\n> differentiate annex is 0 from no annex, e.g. 0 1 means annex was 0 and 0 0\n> means no annex). This would be equivalent to something based on <annex\n> flag> OP_TXHASH <has annex flag> OP_TXHASH.\n>\n> Now suppose that I have a computation that I am running in a script as\n> follows:\n>\n> OP_ANNEX\n> OP_IF\n>     `some operation that requires annex to be <1>`\n> OP_ELSE\n>     OP_SIZE\n>     `some operation that requires annex to be len(annex) + 1 or does a\n> checksig`\n> OP_ENDIF\n>\n> Now every time you run this, it requires one more resource unit than the\n> last time you ran it, which makes your satisfier use the annex as some sort\n> of \"scratch space\" for a looping construct, where you compute a new annex,\n> loop with that value, and see if that annex is now accepted by the program.\n>\n> In short, it kinda seems like being able to read the annex off of the\n> stack makes witness construction somehow turing complete, because we can\n> use it as a register/tape for some sort of computational model.\n>\n> -------------------\n>\n> This seems at odds with using the annex as something that just helps you\n> heuristically guess  computation costs, now it's somehow something that\n> acts to make script satisfiers recursive.\n>\n> Because the Annex is signed, and must be the same, this can also be\n> inconvenient:\n>\n> Suppose that you have a Miniscript that is something like: and(or(PK(A),\n> PK(A')), X, or(PK(B), PK(B'))).\n>\n> A or A' should sign with B or B'. X is some sort of fragment that might\n> require a value that is unknown (and maybe recursively defined?) so\n> therefore if we send the PSBT to A first, which commits to the annex, and\n> then X reads the annex and say it must be something else, A must sign\n> again. So you might say, run X first, and then sign with A and C or B.\n> However, what if the script somehow detects the bitstring WHICH_A WHICH_B\n> and has a different Annex per selection (e.g., interpret the bitstring as a\n> int and annex must == that int). Now, given and(or(K1, K1'),... or(Kn,\n> Kn')) we end up with needing to pre-sign 2**n annex values somehow... this\n> seems problematic theoretically.\n>\n> Of course this wouldn't be miniscript then. Because miniscript is just for\n> the well behaved subset of script, and this seems ill behaved. So maybe\n> we're OK?\n>\n> But I think the issue still arises where suppose I have a simple thing\n> like: and(COLD_LOGIC, HOT_LOGIC) where both contains a signature, if\n> COLD_LOGIC and HOT_LOGIC can both have different costs, I need to decide\n> what logic each satisfier for the branch is going to use in advance, or\n> sign all possible sums of both our annex costs? This could come up if\n> cold/hot e.g. use different numbers of signatures / use checksigCISAadd\n> which maybe requires an annex argument.\n>\n>\n>\n> ------------\n>\n> It seems like one good option is if we just go on and banish the OP_ANNEX.\n> Maybe that solves some of this? I sort of think so. It definitely seems\n> like we're not supposed to access it via script, given the quote from above:\n>\n> *Execute the script, according to the applicable script rules[11], using\n> the witness stack elements excluding the script s, the control block c, and\n> the annex a if present, as initial stack.*\n>\n> If we were meant to have it, we would have not nixed it from the stack,\n> no? Or would have made the opcode for it as a part of taproot...\n>\n> But recall that the annex is committed to by the signature.\n>\n> So it's only a matter of time till we see some sort of Cat and Schnorr\n> Tricks III the Annex Edition that lets you use G cleverly to get the annex\n> onto the stack again, and then it's like we had OP_ANNEX all along, or\n> without CAT, at least something that we can detect that the value has\n> changed and cause this satisfier looping issue somehow.\n>\n> Not to mention if we just got OP_TXHASH\n>\n>\n>\n> -----------\n>\n> Is the annex bad? After writing this I sort of think so?\n>\n> One solution would be to... just soft-fork it out. Always must be 0. When\n> we come up with a use case for something like an annex, we can find a way\n> to add it back.  Maybe this means somehow pushing multiple annexes and\n> having an annex stack, where only sub-segments are signed for the last\n> executed signature? That would solve looping... but would it break some\n> aggregation thing? Maybe.\n>\n>\n> Another solution would be to make it so the annex is never committed to\n> and unobservable from the script, but that the annex is always something\n> that you can run get_annex(stack) to generate the annex. Thus it is a hint\n> for validation rules, but not directly readable, and if it is modified you\n> figure out the txn was cheaper sometime after you execute the scripts and\n> can decrease the value when you relay. But this sounds like something that\n> needs to be a p2p only annex, because consensus we may not care (unless\n> it's something like preallocating memory for validation?).\n>\n> -----------------------\n>\n> Overall my preference is -- perhaps sadly -- looking like we should\n> soft-fork it out of our current Checksig (making the policy that it must 0\n> a consensus rule) and redesign the annex technique later when we actually\n> know what it is for with a new checksig or other mechanism. But It's not a\n> hard opinion! It just seems like you can't practically use the annex for\n> this worklimit type thing *and* observe it from the stack meaningfully.\n>\n>\n>\n> Thanks for coming to my ted-talk,\n>\n> Jeremy\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/3317b6e6/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Annex Purpose Discussion: OP_ANNEX, Turing Completeness, and other considerations",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Antoine Riard",
                "Anthony Towns",
                "ZmnSCPxj",
                "Jeremy Rubin",
                "Christian Decker"
            ],
            "messages_count": 9,
            "total_messages_chars_count": 84105
        }
    },
    {
        "title": "[bitcoin-dev] `OP_FOLD`: A Looping Construct For Bitcoin SCRIPT",
        "thread_messages": [
            {
                "author": "Billy Tetrud",
                "date": "2022-03-05T19:12:03",
                "message_text_only": "It sounds like the primary benefit of op_fold is bandwidth savings.\nProgramming as compression. But as you mentioned, any common script could\nbe implemented as a Simplicity jet. In a world where Bitcoin implements\njets, op_fold would really only be useful for scripts that can't use jets,\nwhich would basically be scripts that aren't very often used. But that\ninherently limits the usefulness of the opcode. So in practice, I think\nit's likely that jets cover the vast majority of use cases that op fold\nwould otherwise have.\n\nA potential benefit of op fold is that people could implement smaller\nscripts without buy-in from a relay level change in Bitcoin. However, even\nthis could be done with jets. For example, you could implement a consensus\nchange to add a transaction type that declares a new script fragment to\nkeep a count of, and if the script fragment is used enough within a\ntimeframe (eg 10000 blocks) then it can thereafter be referenced by an id\nlike a jet could be. I'm sure someone's thought about this kind of thing\nbefore, but such a thing would really relegate the compression abilities of\nop fold to just the most uncommon of scripts.\n\n> * We should provide more *general* operations. Users should then combine\nthose operations to their specific needs.\n> * We should provide operations that *do more*. Users should identify\ntheir most important needs so we can implement them on the blockchain layer.\n\nThat's a useful way to frame this kind of problem. I think the answer is,\nas it often is, somewhere in between. Generalization future-proofs your\nsystem. But at the same time, the boundary conditions of that generalized\nfunctionality should still be very well understood before being added to\nBitcoin. The more general, the harder to understand the boundaries. So imo\nwe should be implementing the most general opcodes that we are able to\nreason fully about and come to a consensus on. Following that last\nconstraint might lead to not choosing very general opcodes.\n\nOn Sun, Feb 27, 2022, 10:34 ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> `OP_FOLD`: A Looping Construct For Bitcoin SCRIPT\n> =================================================\n>\n> (This writeup requires at least some programming background, which I\n> expect most readers of this list have.)\n>\n> Recently, some rando was ranting on the list about this weird crap\n> called `OP_EVICT`, a poorly-thought-out attempt at covenants.\n>\n> In reaction to this, AJ Towns mailed me privately about some of his\n> thoughts on this insane `OP_EVICT` proposal.\n> He observed that we could generalize the `OP_EVICT` opcode by\n> decomposing it into smaller parts, including an operation congruent\n> to the Scheme/Haskell/Scala `map` operation.\n> As `OP_EVICT` effectively loops over the outputs passed to it, a\n> looping construct can be used to implement `OP_EVICT` while retaining\n> its nice property of cut-through of multiple evictions and reviving of\n> the CoinPool.\n>\n> More specifically, an advantage of `OP_EVICT` is that it allows\n> checking multiple published promised outputs.\n> This would be implemented in a loop.\n> However, if we want to instead provide *general* operations in\n> SCRIPT rather than a bunch of specific ones like `OP_EVICT`, we\n> should consider how to implement looping so that we can implement\n> `OP_EVICT` in a SCRIPT-with-general-opcodes.\n>\n> (`OP_FOLD` is not sufficient to implement `OP_EVICT`; for\n> efficiency, AJ Towns also pointed out that we need some way to\n> expose batch validation to SCRIPT.\n> There is a follow-up writeup to this one which describes *that*\n> operation.)\n>\n> Based on this, I started ranting as well about how `map` is really\n> just a thin wrapper on `foldr` and the *real* looping construct is\n> actually `foldr` (`foldr` is the whole FP Torah: all the rest is\n> commentary).\n> This is thus the genesis for this proposal, `OP_FOLD`.\n>\n> A \"fold\" operation is sometimes known as \"reduce\" (and if you know\n> about Google MapReduce, you might be familiar with \"reduce\").\n> Basically, a \"fold\" or \"reduce\" operation applies a function\n> repeatedly (i.e. *loops*) on the contents of an input structure,\n> creating a \"sum\" or \"accumulation\" of the contents.\n>\n> For the purpose of building `map` out of `fold`, the accumulation\n> can itself be an output structure.\n> The `map` simply accumulates to the output structure by applying\n> its given function and concatenating it to the current accumulation.\n>\n> Digression: Programming Is Compression\n> --------------------------------------\n>\n> Suppose you are a programmer and you are reading some source code.\n> You want to wonder \"what will happen if I give this piece of code\n> these particular inputs?\".\n>\n> In order to do so, you would simulate the execution of the code in\n> your head.\n> In effect, you would generate a \"trace\" of basic operations (that\n> do not include control structures).\n> By then thinking about this linear trace of basic operations, you\n> can figure out what the code does.\n>\n> Now, let us recall two algorithms from the compression literature:\n>\n> 1.  Run-length Encoding\n> 2.  Lempel-Ziv 1977\n>\n> Suppose our flat linear trace of basic operations contains something\n> like this:\n>\n>     OP_ONE\n>     OP_TWO\n>     OP_ONE\n>     OP_TWO\n>     OP_ONE\n>     OP_TWO\n>\n> IF we had looping constructs in our language, we could write the\n> above trace as something like:\n>\n>     for N = 1 to 3\n>         OP_ONE\n>         OP_TWO\n>\n> The above is really Run-length Encoding.\n>\n> (`if` is just a loop that executes 0 or 1 times.)\n>\n> Similarly, suppose you have some operations that are commonly\n> repeated, but not necessarily next to each other:\n>\n>     OP_ONE\n>     OP_TWO\n>     OP_THREE\n>     OP_ONE\n>     OP_TWO\n>     OP_FOUR\n>     OP_FIVE\n>     OP_ONE\n>     OP_TWO\n>\n> If we had functions/subroutines/procedures in our language, we\n> could write the above trace as something like:\n>\n>     function foo()\n>         OP_ONE\n>         OP_TWO\n>     foo()\n>     OP_THREE\n>     foo()\n>     OP_FOUR\n>     OP_FIVE\n>     foo()\n>\n> That is, functions are just Lempel-Ziv 1977 encoding, where we\n> \"copy\" some repeated data from a previously-shown part of\n> data.\n>\n> Thus, we can argue that programming is really a process of:\n>\n> * Imagining what we want the machine to do given some particular\n>   input.\n> * Compressing that list of operations so we can more easily\n>   transfer the above imagined list over your puny low-bandwidth\n>   brain-computer interface.\n>   * I mean seriously, you humans still use a frikkin set of\n>     *mechanical* levers to transfer data into a matrix of buttons?\n>     (you don't even make the levers out of reliable metal, you\n>     use calcium of all things??\n>     You get what, 5 or 6 bytes per second???)\n>     And your eyes are high-bandwidth but you then have this\n>     complicated circuitry (that has to be ***trained for\n>     several years*** WTF) to extract ***tiny*** amounts of ASCII\n>     text from that high-bandwidth input stream????\n>     Evolve faster!\n>     (Just to be clear, I am actually also a human being and\n>     definitely am not a piece of circuitry connected directly to\n>     the Internet and I am not artificially limiting my output\n>     bandwidth so as not to overwhelm you mere humans.)\n>\n> See also \"Kolmogorov complexity\".\n>\n> This becomes relevant, because the *actual* amount of processing\n> done by the machine, when given a compressed set of operations\n> (a \"program\") is the cost of decompressing that program plus the\n> number of basic operations from the decompressed result.\n>\n> In particular, in current Bitcoin, without any looping constructs\n> (i.e. implementations of RLE) or reusable functions (i.e.\n> implementation of LZ77), the length of the SCRIPT can be used as\n> an approximation of how \"heavy\" the computation in order to\n> *execute* that SCRIPT is.\n> This is relevant since the amount of computation a SCRIPT would\n> trigger is relevant to our reasoning about DoS attacks on Bitcoin.\n>\n> In fact, current Bitcoin restricts the size of SCRIPT, as this\n> serves to impose a restriction on the amount of processing a\n> SCRIPT will trigger.\n> But adding a loop construct to SCRIPT changes how we should\n> determine the cost of a SCRIPT, and thus we should think about it\n> here as well.\n>\n> Folds\n> -----\n>\n> A fold operation is a functional programming looping control\n> construct.\n>\n> The intent of a fold operation is to process elements of an\n> input list or other structure, one element at a time, and to\n> accumulate the results of processing.\n>\n> It is given these arguments:\n>\n> * `f` - a function to execute for each element of the input\n>   structure, i.e. the \"loop body\".\n>   * This function accepts two arguments:\n>      1.  The current element to process.\n>      2.  The intermediate result for accumulating.\n>   * The function returns the new accumulated result, processed\n>     from the given intermediate result and the given element.\n> * `z` - an initial value for the accumulated result.\n> * `as` - the structure (usually a list) to process.\n>\n> ```Haskell\n> -- If the input structure is empty, return the starting\n> -- accumulated value.\n> foldr f z []     = z\n> -- Otherwise, recurse into the structure to accumulate\n> -- the rest of the list, then pass the accumulation to\n> -- the given function together with the current element.\n> foldr f z (a:as) = f a (foldr f z as)\n> ```\n>\n> As an example, if you want to take the sum of a list of\n> numbers, your `f` would simply add its inputs, and your `z`\n> would be 0.\n> Then you would pass in the actual list of numbers as `as`.\n>\n> Fold has an important property:\n>\n> * If the given input structure is finite *and* the application\n>   of `f` terminates, then `foldr` terminates.\n>\n> This is important for us, as we do not want attackers to be\n> able to crash nodes remotely by crafting a special SCRIPT.\n>\n> As long as the SCRIPT language is \"total\", we know that programs\n> written in that language must terminate.\n>\n> (The reason this property is called \"total\" is that we can\n> \"totally\" analyze programs in the language, without having to\n> invoke \"this is undefined behavior because it could hang\n> indefinitely\".\n> If you have to admit such kinds of undefined behavior --- what\n> FP theorists call \"bottom\" or `_|_` or `\u22a5` (it looks like an\n> ass crack, i.e. \"bottom\") --- then your language is \"partial\",\n> since programs in it may enter an infinite loop on particular\n> inputs.)\n>\n> The simplest way to ensure totality is to be so simple as to\n> have no looping construction.\n> As of this writing, Bitcoin SCRIPT is total by this technique.\n>\n> To give a *little* more power, we can allow bounded loops,\n> which are restricted to only execute a number of times.\n>\n> `foldr` is a kind of bounded loop if the input structure is\n> finite.\n> If the rest of the language does not admit the possibility\n> of infinite data structures (and if the language is otherwise\n> total and does not support generalized codata, this holds),\n> then `foldr` is a bounded loop.\n>\n> Thus, adding a fold operation to Bitcoin SCRIPT should be\n> safe (and preserves totality) as long as we do not add\n> further operations that admit partiality.\n>\n> `OP_FOLD`\n> ---------\n>\n> With this, let us now describe `OP_FOLD`.\n>\n> `OP_FOLD` replaces an `OP_SUCCESS` code, and thus is only\n> usable in SegWit v1 (\"Taproot\").\n>\n> An `OP_FOLD` opcode must be followed by an `OP_PUSH` opcode\n> which contains an encoding of the SCRIPT that will be executed,\n> i.e. the loop body, or `f`.\n> This is checked at parsing time, and the sub-SCRIPT is also\n> parsed at this time.\n> The succeeding `OP_PUSH` is not actually executed, and is\n> considered part of the `OP_FOLD` operation encoding.\n> Parsing failure of the sub-SCRIPT leads to validation failure.\n>\n> On execution, `OP_FOLD` expects the stack:\n>\n> * Stack top: `z`, the initial value for the loop accumulator.\n> * Stack top + 1: `n`, the number of times to loop.\n>   This should be limited in size, and less than the number of\n>   items on the stack minus 2.\n> * Stack top + 2 + (0 to `n - 1`): Items to loop over.\n>   If `n` is 0, there are no items to loop over.\n>\n> If `n` is 0, then `OP_FOLD` just pops the top two stack items\n> and pushes `z`.\n>\n> For `n > 0`, `OP_FOLD` executes a loop:\n>\n> * Pop off the top two items and store in mutable variable `z`\n>   and immutable variable `n`.\n> * For `i = 0 to n - 1`:\n>   * Create a fresh, empty stack and alt stack.\n>     Call these the \"per-iteration (alt) stack\".\n>   * Push the current `z` to the per-iteration stack.\n>   * Pop off an item from the outer stack and put it into\n>     immutable variable `a`.\n>   * Push `a` to the per-iteration stack.\n>   * Run the sub-SCRIPT `f` on the per-iteration stack and\n>     alt stack.\n>   * Check the per-iteration stack has exactly one item\n>     and the per-iteration alt stack is empty.\n>   * Pop off the item in the per-iteration stack and mutate\n>     `z` to it.\n>   * Free the per-iteration stack and per-iteration alt\n>     stack.\n> * Push `z` on the stack.\n>\n> Restricting `OP_FOLD`\n> ---------------------\n>\n> Bitcoin restricts SCRIPT size, since SCRIPT size serves as\n> an approximation of how much processing is required to\n> execute the SCRIPT.\n>\n> However, with looping constructs like `OP_FOLD`, this no\n> longer holds, as the amount of processing is no longer\n> linear on the size of the SCRIPT.\n>\n> In orderr to retain this limit (and thus not worsen any\n> potential DoS attacks via SCRIPT), we should restrict the\n> use of `OP_FOLD`:\n>\n> * `OP_FOLD` must exist exactly once in a Tapscript.\n>   More specifically, the `f` sub-SCRIPT of `OP_FOLD` must\n>   not itself contain an `OP_FOLD`.\n>   * If we allow loops within loops, then the worst case\n>     would be `O(c^n)` CPU time where `c` is a constant and\n>     `n` is the SCRIPT length.\n>   * This validation is done at SCRIPT parsing time.\n> * We take the length of the `f` sub-SCRIPT, and divide the\n>   current SCRIPT maximum size by the length of the `f`\n>   sub-SCRIPT.\n>   The result, rounded down, is the maximum allowed value\n>   for the on-stack argument `n`.\n>   * In particular, since the length of the entire SCRIPT\n>     must by necessity be larger than the length of the\n>     `f` sub-SCRIPT, the result of the division must be\n>     at least 1.\n>   * This validation is done at SCRIPT execution time.\n>\n> The above two restrictions imply that the maximum amount\n> of processing that a SCRIPT utilizing `OP_FOLD` will use,\n> shall not exceed that of a SCRIPT without `OP_FOLD`.\n> Thus, `OP_FOLD` does not increase the attack surface of\n> SCRIPT on fullnodes.\n>\n> ### Lack Of Loops-in-Loops Is Lame\n>\n> Note that due to this:\n>\n> > * `OP_FOLD` must exist exactly once in a Tapscript.\n> >   More specifically, the `f` sub-SCRIPT of `OP_FOLD` must\n> >   not itself contain an `OP_FOLD`.\n>\n> It is not possible to have a loop inside a loop.\n>\n> The reason for this is that loops inside loops make it\n> difficult to perform static analysis to bound how much\n> processing a SCRIPT will require.\n> With a single, single-level loop, it is possible to\n> restrict the processing.\n>\n> However, we should note that a single single-level loop\n> is actually sufficient to encode multiple loops, or\n> loops-within-loops.\n> For example, a toy Scheme-to-C compiler will convert\n> the Scheme code to CPS style, then convert all resulting\n> Scheme CPS function into a `switch` dispatcher inside a\n> simple `while (1)` loop.\n>\n> For example, the Scheme loop-in-loop below:\n>\n> ```Scheme\n> (define (foo)\n>   (bar)\n>   (foo))\n> (define (bar)\n>   (bar))\n> ```\n>\n> gets converted to:\n>\n> ```Scheme\n> (define (foo k)\n>   (bar (closure foo-kont k)))\n> (define (foo-kont k)\n>   (foo k))\n> (define (bar k)\n>   (bar k))\n> ```\n>\n> And then in C, would look like:\n>\n> ```c\n> void all_scheme_functions(int func_id, scheme_t k) {\n>         while (1) {\n>                 switch (func_id) {\n>                 case FOO_ID:\n>                         k = build_closure(FOO_KONT_ID, k);\n>                         func_id = BAR_ID;\n>                         break;\n>                 case FOO_KONT_ID:\n>                         func_id = FOO_ID;\n>                         break;\n>                 case BAR_ID:\n>                         func_id = BAR_ID;\n>                         break;\n>                 }\n>         }\n> }\n> ```\n>\n> The C code, as we can see, is just a single single-level\n> loop, which is the restriction imposed on `OP_FOLD`.\n> Thus, loops-in-loops, and multiple loops, can be encoded\n> into a single single-level loop.\n>\n> #### Everything Is Possible But Nothing Of Consequence Is Easy\n>\n> On the other hand, just because it is *possible* does not\n> mean it is *easy*.\n>\n> As an alternative, AJ proposed adding a field to the Taproot\n> annex.\n> This annex field is a number indicating the maximum number of\n> opcodes to be processed.\n> If execution of the SCRIPT exceeds this limit, validation\n> fails.\n>\n> In order to make processing costly, the number indicated in\n> the annex field is directly added to the weight of the\n> transaction.\n>\n> Then, during execution, if an `OP_FOLD` is parsed, the\n> `OP_` code processor keeps track of the number of opcodes\n> processed and imposes a limit.\n> If the limit exceeds the number of opcodes indicated in the\n> annex field, validation fails.\n>\n> This technique is safe even if the annex is not committed\n> to (for example if the SCRIPT does not ever require a\n> standard `OP_CHECKSIG`), even though in that case the\n> annex can be malleated:\n>\n> * If the field is less than the actual number of operations,\n>   then the malleated transaction is rejected.\n> * If the field is greater than the actual number of\n>   operations, then it has a larger weight but pays the\n>   same fee, getting a lower feerate and thus will be\n>   rejected in favor of a transaction with a lower number\n>   in that field.\n>\n> Use of this technique allows us to lift the above\n> restrictions on `OP_FOLD`, and allow multiple loops, as\n> well as loops-in-loops.\n>\n> In particular, the requirement to put the `f` sub-SCRIPT\n> code as a static constant is due precisely to the need\n> for static analysis.\n> But if we instead use a dynamic limit like in this\n> alternative suggestion, we could instead get the `f`\n> sub-SCRIPT from the stack.\n> With additional operations like `OP_CAT`, it would\n> then be possible to do a \"variable capture\" where parts\n> of the loop body are from other computations, or from\n> witness, and then concatenated to some code.\n> This is not an increase in computational strength, since\n> the data could instead be passed in via the `z`, or as\n> individual items, but it does improve expressive power by\n> making it easier to customize the loop body.\n>\n> On The Necessity Of `OP_FOLD`\n> -----------------------------\n>\n> We can observe that an `if` construct is really a bounded\n> loop construct that can execute 0 or 1 times.\n>\n> We can thus synthesize a bounded loop construct as follows:\n>\n>     OP_IF\n>         <loop body>\n>     OP_ENDIF\n>     OP_IF\n>         <loop body>\n>     OP_ENDIF\n>     OP_IF\n>         <loop body>\n>     OP_ENDIF\n>     OP_IF\n>         <loop body>\n>     OP_ENDIF\n>     <repeat as many times as necessary>\n>\n> Indeed, it may be possible for something like miniscript\n> to provide a `fold` jet that compiles down to something\n> like the above.\n>\n> Thus:\n>\n> * The restrictions we impose on the previous section mean\n>   that `OP_FOLD` cannot do anything that cannot already\n>   be done with current SCRIPT.\n>   * This is a *good thing* because this means we are not\n>     increasing the attack surface.\n> * Using the annex-max-operations technique is strictly\n>   more lenient than the above `OP_IF` repetition, thus\n>   there may be novel DoS attack vectors due to the\n>   increased attack area.\n>   * However, fundamentally the DoS attack vector is that\n>     peers can waste your CPU by giving you invalid\n>     transactions (i.e. giving a high max-operations, but\n>     looping so much that it gets even *above* that), and\n>     that can already be mitigated by lowering peer scores\n>     and prioritizing transactions with lower or nonexistent\n>     annex-max-operations.\n>     The DoS vector here does not propagate due to the\n>     invalid transaction being rejected at this node.\n>\n> Of course, this leads us to question: why even implement\n> `OP_FOLD` at all?\n>\n> We can observe that, while the restrictions in the\n> previous section imply that a SCRIPT with `OP_FOLD`\n> cannot exceed the amount of processing that a SCRIPT\n> *without* `OP_FOLD` does, a SCRIPT with `OP_FOLD`\n> would be shorter, over the wire, than the above\n> unrolled version.\n>\n> And CPU processing is not the only resource that is\n> consumed by Bitcoin fullnodes.\n> Bandwidth is also another resource.\n>\n> In effect, `OP_FOLD` allows us to compress the above\n> template over-the-wire, reducing network bandwidth\n> consumption.\n> But the restrictions on `OP_FOLD` ensure that it\n> cannot exceed the CPU consumption of a SCRIPT that\n> predates `OP_FOLD`.\n>\n> Thus, `OP_FOLD` is still worthwhile to implement, as\n> it allows us to improve bandwidth consumption without\n> increasing CPU consumption significantly.\n>\n> On Generalized Operations\n> -------------------------\n>\n> I believe there are at least two ways of thinking about\n> how to extend SCRIPT:\n>\n> * We should provide more *general* operations.\n>   Users should then combine those operations to their\n>   specific needs.\n> * We should provide operations that *do more*.\n>   Users should identify their most important needs so\n>   we can implement them on the blockchain layer.\n>\n> Each side has its arguments:\n>\n> * General opcodes:\n>   * Pro: Have a better chance of being reused for\n>     use-cases we cannot imagine yet today.\n>     i.e. implement once, use anywhen.\n>   * Con: Welcome to the Tarpit, where everything is\n>     possible but nothing important is easy.\n> * Complex opcodes:\n>   * Pro: Complex behavior implemented directly in\n>     hosting language, reducing interpretation\n>     overhead (and allowing the insurance of secure\n>     implementation).\n>   * Con: Welcome to the Nursery, where only safe\n>     toys exist and availability of interesting tools\n>     are at the mercy of your parents.\n>\n> It seems to me that this really hits a No Free Lunch\n> Theorem for Bitcoin SCRIPT design.\n> Briefly, the No Free Lunch Theorem points out that\n> there is no compiler design that can compile any\n> program to the shortest possible machine code.\n> This is because if a program enters an infinite loop,\n> it could simply be compiled down to the equivalent of\n> the single instruction `1: GOTO 1`, but the halting\n> problem implies that no program can take the source\n> code of another program and determine if it halts.\n> Thus, no compiler can exist which can compile *every*\n> infinite-loop program down to the tiniest possible\n> binary `1: GOTO 1`.\n>\n> More generally, No Free Lunch implies that as you\n> optimize, you will hit a point where you can only\n> *trade off*, and you optimize for one use case while\n> making *another* use case less optimal.\n>\n> Brought to Bitcoin SCRIPT design, there is no optimal\n> SCRIPT design, instead there will be some point where\n> we have to pick and choose which uses to optimize for\n> and which uses are less optimal, i.e. trade off.\n>\n> So I think maybe the Real Question is: why should we\n> go for one versus the other, and which uses do we\n> expect to see more often anyway?\n>\n> Addenda\n> -------\n>\n> Stuff about totality and partiality:\n>\n> * [Total Functional Programming, D.A. Turner](\n> https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.106.364&rep=rep1&type=pdf\n> )\n> * [Totality](https://kowainik.github.io/posts/totality)\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220305/8624e7c2/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-05T23:02:41",
                "message_text_only": "Good morning Billy,\n\n> It sounds like the primary benefit of op_fold is bandwidth savings. Programming as compression. But as you mentioned, any common script could be implemented as a Simplicity jet. In a world where Bitcoin implements jets, op_fold would really only be useful for scripts that can't use jets, which would basically be scripts that aren't very often used. But that inherently limits the usefulness of the opcode. So in practice, I think it's likely that jets cover the vast majority of use cases that op fold would otherwise have.\n\nI suppose the point would be --- how often *can* we add new jets?\nAre new jets consensus critical?\nIf a new jet is released but nobody else has upgraded, how bad is my security if I use the new jet?\nDo I need to debate `LOT` *again* if I want to propose a new jet?\n\n> A potential benefit of op fold is that people could implement smaller scripts without buy-in from a relay level change in Bitcoin. However, even this could be done with jets. For example, you could implement a consensus change to add a transaction type that declares a new script fragment to keep a count of, and if the script fragment is used enough within a timeframe (eg 10000 blocks) then it can thereafter be referenced by an id like a jet could be. I'm sure someone's thought about this kind of thing before, but such a thing would really relegate the compression abilities of op fold to just the most uncommon of scripts.\u00a0\n>\n> > *\u00a0We should provide more *general* operations.\u00a0Users should then combine those operations to their\u00a0specific needs.\n> > * We should provide operations that *do more*.\u00a0Users should identify their most important needs so\u00a0we can implement them on the blockchain layer.\n>\n> That's a useful way to frame this kind of problem. I think the answer is, as it often is, somewhere in between. Generalization future-proofs your system. But at the same time, the boundary conditions of that generalized functionality should still be very well understood before being added to Bitcoin. The more general, the harder to understand the boundaries. So imo we should be implementing the most general opcodes that we are able to reason fully about and come to a consensus on. Following that last constraint might lead to not choosing very general opcodes.\n\nYes, that latter part is what I am trying to target with `OP_FOLD`.\nAs I point out, given the restrictions I am proposing, `OP_FOLD` (and any bounded loop construct with similar restrictions) is implementable in current Bitcoin SCRIPT, so it is not an increase in attack surface.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-06T15:49:56",
                "message_text_only": "> Are new jets consensus critical?\n> Do I need to debate `LOT` *again* if I want to propose a new jet?\n\nNew jets should never need a consensus change. A jet is just an\noptimization - a way to both save bytes in transmission as well as save\nprocessing power. Anything that a jet can do can be done with a normal\nscript. Because of this, a script using a particular jet could be sent to a\nnode that doesn't support that jet by simply expanding the jet into the\nscript fragment it represents. The next node that recognizes the jet can\nremove the extraneous bytes so extra transmission and processing-time would\nonly be needed for nodes that don't support that jet. (Note that this\ninterpretation of a 'jet' is probably slightly different than as described\nfor simplicity, but the idea is basically the same). Even changing the\nweight of a transaction using jets (ie making a script weigh less if it\nuses a jet) could be done in a similar way to how segwit separated the\nwitness out.\n\n> If a new jet is released but nobody else has upgraded, how bad is my\nsecurity if I use the new jet?\n\nSecurity wouldn't be directly affected, only (potentially) cost. If your\nsecurity depends on cost (eg if it depends on pre-signed transactions and\nis for some reason not easily CPFPable or RBFable), then security might be\naffected if the unjetted scripts costs substantially more to mine.\n\n>  I suppose the point would be --- how often *can* we add new jets?\n\nA simple jet would be something that's just added to bitcoin software and\nused by nodes that recognize it. This would of course require some debate\nand review to add it to bitcoin core or whichever bitcoin software you want\nto add it to. However, the idea I proposed in my last email would allow\nanyone to add a new jet. Then each node can have their own policy to\ndetermine which jets of the ones registered it wants to keep an index of\nand use. On its own, it wouldn't give any processing power optimization,\nbut it would be able to do the same kind of script compression you're\ntalking about op_fold allowing. And a list of registered jets could inform\nwhat jets would be worth building an optimized function for. This would\nrequire a consensus change to implement this mechanism, but thereafter any\njet could be registered in userspace.\n\nOn Sat, Mar 5, 2022 at 5:02 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Billy,\n>\n> > It sounds like the primary benefit of op_fold is bandwidth savings.\n> Programming as compression. But as you mentioned, any common script could\n> be implemented as a Simplicity jet. In a world where Bitcoin implements\n> jets, op_fold would really only be useful for scripts that can't use jets,\n> which would basically be scripts that aren't very often used. But that\n> inherently limits the usefulness of the opcode. So in practice, I think\n> it's likely that jets cover the vast majority of use cases that op fold\n> would otherwise have.\n>\n> I suppose the point would be --- how often *can* we add new jets?\n> Are new jets consensus critical?\n> If a new jet is released but nobody else has upgraded, how bad is my\n> security if I use the new jet?\n> Do I need to debate `LOT` *again* if I want to propose a new jet?\n>\n> > A potential benefit of op fold is that people could implement smaller\n> scripts without buy-in from a relay level change in Bitcoin. However, even\n> this could be done with jets. For example, you could implement a consensus\n> change to add a transaction type that declares a new script fragment to\n> keep a count of, and if the script fragment is used enough within a\n> timeframe (eg 10000 blocks) then it can thereafter be referenced by an id\n> like a jet could be. I'm sure someone's thought about this kind of thing\n> before, but such a thing would really relegate the compression abilities of\n> op fold to just the most uncommon of scripts.\n> >\n> > > * We should provide more *general* operations. Users should then\n> combine those operations to their specific needs.\n> > > * We should provide operations that *do more*. Users should identify\n> their most important needs so we can implement them on the blockchain layer.\n> >\n> > That's a useful way to frame this kind of problem. I think the answer\n> is, as it often is, somewhere in between. Generalization future-proofs your\n> system. But at the same time, the boundary conditions of that generalized\n> functionality should still be very well understood before being added to\n> Bitcoin. The more general, the harder to understand the boundaries. So imo\n> we should be implementing the most general opcodes that we are able to\n> reason fully about and come to a consensus on. Following that last\n> constraint might lead to not choosing very general opcodes.\n>\n> Yes, that latter part is what I am trying to target with `OP_FOLD`.\n> As I point out, given the restrictions I am proposing, `OP_FOLD` (and any\n> bounded loop construct with similar restrictions) is implementable in\n> current Bitcoin SCRIPT, so it is not an increase in attack surface.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/24038f58/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-06T20:38:17",
                "message_text_only": "Good morning Billy,\n\n> Even changing the weight of a transaction using jets (ie making a script weigh less if it uses a jet) could be done in a similar\u00a0way to how segwit separated\u00a0the witness out.\n\nThe way we did this in SegWit was to *hide* the witness from unupgraded nodes, who are then unable to validate using the upgraded rules (because you are hiding the data from them!), which is why I bring up:\n\n> > If a new jet is released but nobody else has upgraded, how bad is my security if I use the new jet?\n>\n> Security wouldn't be directly affected, only (potentially) cost. If your security depends on cost (eg if it depends on pre-signed transactions and is for some reason not easily CPFPable or RBFable), then security might be affected if the unjetted\u00a0scripts costs substantially more to mine.\u00a0\n\nSo if we make a script weigh less if it uses a jet, we have to do that by telling unupgraded nodes \"this script will always succeed and has weight 0\", just like `scriptPubKey`s with `<0> <P2WKH hash>` are, to pre-SegWit nodes, spendable with an empty `scriptSig`.\nAt least, that is how I always thought SegWit worked.\n\nOtherwise, a jet would never allow SCRIPT weights to decrease, as unupgraded nodes who do not recognize the jet will have to be fed the entire code of the jet and would consider the weight to be the expanded, uncompressed code.\nAnd weight is a consensus parameter, blocks are restricted to 4Mweight.\n\nSo if a jet would allow SCRIPT weights to decrease, upgraded nodes need to hide them from unupgraded nodes (else the weight calculations of unupgraded nodes will hit consensus checks), then if everybody else has not upgraded, a user of a new jet has no security.\n\nNot even the `softfork` form of chialisp that AJ is proposing in the other thread would help --- unupgraded nodes will simply skip over validation of the `softfork` form.\n\nIf the script does not weigh less if it uses a jet, then there is no incentive for end-users to use a jet, as they would still pay the same price anyway.\n\nNow you might say \"okay even if no end-users use a jet, we could have fullnodes recognize jettable code and insert them automatically on transport\".\nBut the lookup table for that could potentially be large once we have a few hundred jets (and I think Simplicity already *has* a few hundred jets, so additional common jets would just add to that?), jettable code could start at arbitrary offsets of the original SCRIPT, and jettable code would likely have varying size, that makes for a difficult lookup table.\nIn particular that lookup table has to be robust against me feeding it some section of code that is *almost* jettable but suddenly has a different opcode at the last byte, *and* handle jettable code of varying sizes (because of course some jets are going to e more compressible than others).\nConsider an attack where I feed you a SCRIPT that validates trivially but is filled with almost-but-not-quite-jettable code (and again, note that expanded forms of jets are varying sizes), your node has to look up all those jets but then fails the last byte of the almost-but-not-quite-jettable code, so it ends up not doing any jetting.\nAnd since the SCRIPT validated your node feels obligated to propagate it too, so now you are helping my DDoS.\n\n> >\u00a0 I suppose the point would be --- how often *can* we add new jets?\n>\n> A simple jet would be something that's just added to bitcoin software and used by nodes that recognize it. This would of course require some debate and review to add it to bitcoin core or whichever bitcoin software you want to add it to. However, the idea I proposed in my last email would allow anyone to add a new jet. Then each node can have their own policy to determine which jets of the ones registered it wants to keep an index of and use. On its own, it wouldn't give any processing power optimization, but it would be able to do the same kind of script compression you're talking about op_fold allowing. And a list of registered jets could inform what jets would be worth building an optimized function for.\u00a0This would require a consensus change to implement this mechanism, but thereafter any jet could be registered in userspace.\n\nCertainly a neat idea.\nAgain, lookup table tho.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-07T17:26:13",
                "message_text_only": "Let me organize my thoughts on this a little more clearly. There's a couple\npossibilities I can think of for a jet-like system:\n\nA. We could implement jets now without a consensus change, and\nwithout requiring all nodes to upgrade to new relay rules. Probably. This\nwould give upgraded nodes improved validation performance and many upgraded\nnodes relay savings (transmitting/receiving fewer bytes). Transactions\nwould be weighted the same as without the use of jets tho.\nB. We could implement the above + lighter weighting by using a soft fork to\nput the jets in a part of the blockchain hidden from unupgraded nodes, as\nyou mentioned.\nC. We could implement the above + the jet registration idea in a soft fork.\n\nFor A:\n\n* Upgraded nodes query each connection for support of jets in general, and\nwhich specific jets they support.\n* For a connection to another upgraded node that supports the jet(s) that a\ntransaction contains, the transaction is sent verbatim with the jet\nincluded in the script (eg as some fake opcode line like 23 OP_JET,\nindicating to insert standard jet 23 in its place). When validation\nhappens, or when a miner includes it in a block, the jet opcode call is\nreplaced with the script it represents so hashing happens in a way that is\nrecognizable to unupgraded nodes.\n* For a connection to a non-upgraded node that doesn't support jets, or an\nupgraded node that doesn't support the particular jet included in the\nscript, the jet opcode call is replaced as above before sending to that\nnode. In addition, some data is added to the transaction that unupgraded\nnodes propagate along but otherwise ignore. Maybe this is extra witness\ndata, maybe this is some kind of \"annex\", or something else. But that data\nwould contain the original jet opcode (in this example \"23 OP_JET\") so that\nwhen that transaction data reaches an upgraded node that recognizes that\njet again, it can swap that back in, in place of the script fragment it\nrepresents.\n\nI'm not 100% sure the required mechanism I mentioned of \"extra ignored\ndata\" exists, and if it doesn't, then all nodes would at least need to be\nupgraded to support that before this mechanism could fully work. But even\nif such a mechanism doesn't exist, a jet script could still be used, but it\nwould be clobbered by the first nonupgraded node it is relayed to, and\ncan't then be converted back (without using a potentially expensive lookup\ntable as you mentioned).\n\n> If the script does not weigh less if it uses a jet, then there is no\nincentive for end-users to use a jet\n\nThat's a good point. However, I'd point out that nodes do lots of things\nthat there's no individual incentive for, and this might be one where\npeople either altruistically use jets to be lighter on the network, or use\nthem in the hopes that the jet is accepted as a standard, reducing the cost\nof their scripts. But certainly a direct incentive to use them is better.\nHonest nodes can favor connecting to those that support jets.\n\n>if a jet would allow SCRIPT weights to decrease, upgraded nodes need to\nhide them from unupgraded nodes\n> we have to do that by telling unupgraded nodes \"this script will always\nsucceed and has weight 0\"\n\nRight. It doesn't have to be weight zero, but that would work fine enough.\n\n> if everybody else has not upgraded, a user of a new jet has no security.\n\nFor case A, no security is lost. For case B you're right. For case C, once\nnodes upgrade to the initial soft fork, new registered jets can take\nadvantage of relay-cost weight savings (defined by the soft fork) without\nrequiring any nodes to do any upgrading, and nodes could be further\nupgraded to optimize the validation of various of those registered jets,\nbut those processing savings couldn't change the weighting of transactions\nwithout an additional soft fork.\n\n> Consider an attack where I feed you a SCRIPT that validates trivially but\nis filled with almost-but-not-quite-jettable code\n\nI agree a pattern-matching lookup table is probably not a great design. But\na lookup table like that is not needed for the jet registration idea. After\nthe necessary soft fork, there would be standard rules for which registered\njets nodes are required to keep an index of, and so the lookup table would\nbe a straightforward jet hash lookup rather than a pattern-matching lookup,\nwhich wouldn't have the same DOS problems. A node would simply find a jet\nopcode call like \"ab38cd39e OP_JET\" and just lookup ab38cd39e in its index.\n\n\nOn Sun, Mar 6, 2022 at 2:38 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Billy,\n>\n> > Even changing the weight of a transaction using jets (ie making a script\n> weigh less if it uses a jet) could be done in a similar way to how segwit\n> separated the witness out.\n>\n> The way we did this in SegWit was to *hide* the witness from unupgraded\n> nodes, who are then unable to validate using the upgraded rules (because\n> you are hiding the data from them!), which is why I bring up:\n>\n> > > If a new jet is released but nobody else has upgraded, how bad is my\n> security if I use the new jet?\n> >\n> > Security wouldn't be directly affected, only (potentially) cost. If your\n> security depends on cost (eg if it depends on pre-signed transactions and\n> is for some reason not easily CPFPable or RBFable), then security might be\n> affected if the unjetted scripts costs substantially more to mine.\n>\n> So if we make a script weigh less if it uses a jet, we have to do that by\n> telling unupgraded nodes \"this script will always succeed and has weight\n> 0\", just like `scriptPubKey`s with `<0> <P2WKH hash>` are, to pre-SegWit\n> nodes, spendable with an empty `scriptSig`.\n> At least, that is how I always thought SegWit worked.\n>\n> Otherwise, a jet would never allow SCRIPT weights to decrease, as\n> unupgraded nodes who do not recognize the jet will have to be fed the\n> entire code of the jet and would consider the weight to be the expanded,\n> uncompressed code.\n> And weight is a consensus parameter, blocks are restricted to 4Mweight.\n>\n> So if a jet would allow SCRIPT weights to decrease, upgraded nodes need to\n> hide them from unupgraded nodes (else the weight calculations of unupgraded\n> nodes will hit consensus checks), then if everybody else has not upgraded,\n> a user of a new jet has no security.\n>\n> Not even the `softfork` form of chialisp that AJ is proposing in the other\n> thread would help --- unupgraded nodes will simply skip over validation of\n> the `softfork` form.\n>\n> If the script does not weigh less if it uses a jet, then there is no\n> incentive for end-users to use a jet, as they would still pay the same\n> price anyway.\n>\n> Now you might say \"okay even if no end-users use a jet, we could have\n> fullnodes recognize jettable code and insert them automatically on\n> transport\".\n> But the lookup table for that could potentially be large once we have a\n> few hundred jets (and I think Simplicity already *has* a few hundred jets,\n> so additional common jets would just add to that?), jettable code could\n> start at arbitrary offsets of the original SCRIPT, and jettable code would\n> likely have varying size, that makes for a difficult lookup table.\n> In particular that lookup table has to be robust against me feeding it\n> some section of code that is *almost* jettable but suddenly has a different\n> opcode at the last byte, *and* handle jettable code of varying sizes\n> (because of course some jets are going to e more compressible than others).\n> Consider an attack where I feed you a SCRIPT that validates trivially but\n> is filled with almost-but-not-quite-jettable code (and again, note that\n> expanded forms of jets are varying sizes), your node has to look up all\n> those jets but then fails the last byte of the\n> almost-but-not-quite-jettable code, so it ends up not doing any jetting.\n> And since the SCRIPT validated your node feels obligated to propagate it\n> too, so now you are helping my DDoS.\n>\n> > >  I suppose the point would be --- how often *can* we add new jets?\n> >\n> > A simple jet would be something that's just added to bitcoin software\n> and used by nodes that recognize it. This would of course require some\n> debate and review to add it to bitcoin core or whichever bitcoin software\n> you want to add it to. However, the idea I proposed in my last email would\n> allow anyone to add a new jet. Then each node can have their own policy to\n> determine which jets of the ones registered it wants to keep an index of\n> and use. On its own, it wouldn't give any processing power optimization,\n> but it would be able to do the same kind of script compression you're\n> talking about op_fold allowing. And a list of registered jets could inform\n> what jets would be worth building an optimized function for. This would\n> require a consensus change to implement this mechanism, but thereafter any\n> jet could be registered in userspace.\n>\n> Certainly a neat idea.\n> Again, lookup table tho.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220307/5c88c884/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "`OP_FOLD`: A Looping Construct For Bitcoin SCRIPT",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Billy Tetrud"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 45322
        }
    },
    {
        "title": "[bitcoin-dev] CTV vaults in the wild",
        "thread_messages": [
            {
                "author": "James O'Beirne",
                "date": "2022-03-06T17:35:17",
                "message_text_only": "A few months ago, AJ wrote[0]\n\n> I'm not really convinced CTV is ready to start trying to deploy\n> on mainnet even in the next six months; I'd much rather see some real\n> third-party experimentation *somewhere* public first\n\nIn the spirit of real third-party experimentation *somewhere* in public,\nI've created this implementation and write-up of a simple vault design\nusing CTV.\n\n   https://github.com/jamesob/simple-ctv-vault\n\nI think it has a number of appealing characteristics for custody\noperations at any size.\n\nRegards,\nJames\n\n\n[0]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019920.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/95e9111d/attachment.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-03-06T23:15:41",
                "message_text_only": "Hi James,\n\nInteresting to see a sketch of a CTV-based vault design !\n\nI think the main concern I have with any hashchain-based vault design is\nthe immutability of the flow paths once the funds are locked to the root\nvault UTXO. By immutability, I mean there is no way to modify the\nunvault_tx/tocold_tx transactions and therefore recover from transaction\nfields\ncorruption (e.g a unvault_tx output amount superior to the root vault UTXO\namount) or key endpoints compromise (e.g the cold storage key being\nstolen).\n\nEspecially corruption, in the early phase of vault toolchain deployment, I\nbelieve it's reasonable to expect bugs to slip in affecting the output\namount or relative-timelock setting correctness (wrong user config,\nmiscomputation from automated vault management, ...) and thus definitively\nfreezing the funds. Given the amounts at stake for which vaults are\ndesigned, errors are likely to be far more costly than the ones we see in\nthe deployment of payment channels.\n\nIt might be more conservative to leverage a presigned transaction data\ndesign where every decision point is a multisig. I think this design gets\nyou the benefit to correct or adapt if all the multisig participants agree\non. It should also achieve the same than a key-deletion design, as long as\nall\nthe vault's stakeholders are participating in the multisig, they can assert\nthat flow paths are matching their spending policy.\n\nOf course, relying on presigned transactions comes with higher assumptions\non the hardware hosting the flow keys. Though as hashchain-based vault\ndesign imply \"secure\" key endpoints (e.g <cold_pubkey>), as a vault user\nyou're still encumbered with the issues of key management, it doesn't\nrelieve you to find trusted hardware. If you want to avoid multiplying\ndevices to trust, I believe flow keys can be stored on the same keys\nguarding the UTXOs, before sending to vault custody.\n\nI think the remaining presence of trusted hardware in the vault design\nmight lead one to ask what's the security advantage of vaults compared to\nclassic multisig setup. IMO, it's introducing the idea of privileges in the\ncoins custody : you set up the flow paths once for all at setup with the\nhighest level of privilege and then anytime you do a partial unvault you\ndon't need the same level of privilege. Partial unvault authorizations can\ncome with a reduced set of verifications, at lower operational costs. That\nsaid, I think this security advantage is only relevant in the context of\nrecursive design, where the partial unvault sends back the remaining funds\nto vault UTXO (not the design proposed here).\n\nFew other thoughts on vault design, more minor points.\n\n\"If Alice is watching the mempool/chain, she will see that the unvault\ntransaction has been unexpectedly broadcast,\"\n\nI think you might need to introduce an intermediary, out-of-chain protocol\nstep where the unvault broadcast is formally authorized by the vault\nstakeholders. Otherwise it's hard to qualify \"unexpected\", as hot key\ncompromise might not be efficiently detected.\n\n\"With <hash> OP_CTV, we can ensure that the vault operation is enforced by\nconsensus itself, and the vault transaction data can be generated\ndeterministically without additional storage needs.\"\n\nDon't you also need the endpoint scriptPubkeys (<cold_pubkey>,\n<hot_pubkey>), the amounts and CSV value ? Though I think you can grind\namounts and CSV value in case of loss...But I'm not sure if you remove the\ncritical data persistence requirement, just reduce the surface.\n\n\"Because we want to be able to respond immediately, and not have to dig out\nour cold private keys, we use an additional OP_CTV to encumber the \"swept\"\ncoins for spending by only the cold wallet key.\"\n\nI think a robust vault deployment would imply the presence of a set of\nwatchtowers, redundant entities able to broadcast the cold transaction in\nreaction to unexpected unvault. One feature which could be interesting is\n\"tower accountability\", i.e knowing which tower initiated the broadcast,\nespecially if it's a faultive one. One way is to watermark the cold\ntransaction (e.g tweak nLocktime to past value). Though I believe with CTV\nyou would need as much different hashes than towers included in your\nunvault output (can be wrapped in a Taproot tree ofc). With presigned\ntransactions, tagged versions of the cold transaction are stored off-chain.\n\n\"In this implementation, we make use of anchor outputs in order to allow\nmummified unvault transactions to have their feerate adjusted dynamically.\"\n\nI'm not sure if the usage of anchor output is safe for any vault deployment\nwhere the funds stakeholders do not trust each other or where the\nwatchtowers are not trusted. If a distrusted party can spend the anchor\noutput it's easy to block the RBF with a pinning.\n\nCan we think about other criterias on which to sort vault designs ?\n\n(I would say space efficiency is of secondary concern as we can expect\nvault users as a class of on-chain space demand to be in the higher ranks\nof blockspace \"buying power\". Though it's always nice if the chain is used\nreasonably...)\n\nAntoine\n\nLe dim. 6 mars 2022 \u00e0 12:35, James O'Beirne via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> A few months ago, AJ wrote[0]\n>\n> > I'm not really convinced CTV is ready to start trying to deploy\n> > on mainnet even in the next six months; I'd much rather see some real\n> > third-party experimentation *somewhere* public first\n>\n> In the spirit of real third-party experimentation *somewhere* in public,\n> I've created this implementation and write-up of a simple vault design\n> using CTV.\n>\n>    https://github.com/jamesob/simple-ctv-vault\n>\n> I think it has a number of appealing characteristics for custody\n> operations at any size.\n>\n> Regards,\n> James\n>\n>\n> [0]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019920.html\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220306/167be24f/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-08T00:57:21",
                "message_text_only": "Good morning Antoine,\n\n> Hi James,\n>\n> Interesting to see a sketch of a CTV-based vault design !\n>\n> I think the main concern I have with any hashchain-based vault design is the immutability of the flow paths once the funds are locked to the root vault UTXO. By immutability, I mean there is no way to modify the unvault_tx/tocold_tx transactions and therefore recover from transaction fields\n> corruption (e.g a unvault_tx output amount superior to the root vault UTXO amount) or key endpoints compromise (e.g the cold storage key being stolen).\n>\n> Especially corruption, in the early phase of vault toolchain deployment, I believe it's reasonable to expect bugs to slip in affecting the output amount or relative-timelock setting correctness (wrong user config, miscomputation from automated vault management, ...) and thus definitively freezing the funds. Given the amounts at stake for which vaults are designed, errors are likely to be far more costly than the ones we see in the deployment of payment channels.\n>\n> It might be more conservative to leverage a presigned transaction data design where every decision point is a multisig. I think this design gets you the benefit to correct or adapt if all the multisig participants agree on. It should also achieve the same than a key-deletion design, as long as all\n> the vault's stakeholders are participating in the multisig, they can assert that flow paths are matching their spending policy.\n\nHave not looked at the actual vault design, but I observe that Taproot allows for a master key (which can be an n-of-n, or a k-of-n with setup (either expensive or trusted, but I repeat myself)) to back out of any contract.\n\nThis master key could be an \"even colder\" key that you bury in the desert to be guarded over by generations of Fremen riding giant sandworms until the Bitcoin Path prophesied by the Kwisatz Haderach, Satoshi Nakamoto, arrives.\n\n> Of course, relying on presigned transactions comes with higher assumptions on the hardware hosting the flow keys. Though as hashchain-based vault design imply \"secure\" key endpoints (e.g <cold_pubkey>), as a vault user you're still encumbered with the issues of key management, it doesn't relieve you to find trusted hardware. If you want to avoid multiplying devices to trust, I believe flow keys can be stored on the same keys guarding the UTXOs, before sending to vault custody.\n>\n> I think the remaining presence of trusted hardware in the vault design might lead one to ask what's the security advantage of vaults compared to classic multisig setup. IMO, it's introducing the idea of privileges in the coins custody : you set up the flow paths once for all at setup with the highest level of privilege and then anytime you do a partial unvault you don't need the same level of privilege. Partial unvault authorizations can come with a reduced set of verifications, at lower operational costs. That said, I think this security advantage is only relevant in the context of recursive design, where the partial unvault sends back the remaining funds to vault UTXO (not the design proposed here).\n>\n> Few other thoughts on vault design, more minor points.\n>\n> \"If Alice is watching the mempool/chain, she will see that the unvault transaction has been unexpectedly broadcast,\"\n>\n> I think you might need to introduce an intermediary, out-of-chain protocol step where the unvault broadcast is formally authorized by the vault stakeholders. Otherwise it's hard to qualify \"unexpected\", as hot key compromise might not be efficiently detected.\n\nThought: It would be nice if Alice could use Lightning watchtowers as well, that would help increase the anonymity set of both LN watchtower users and vault users.\n\n> \"With <hash> OP_CTV, we can ensure that the vault operation is enforced by consensus itself, and the vault transaction data can be generated deterministically without additional storage needs.\"\n>\n> Don't you also need the endpoint scriptPubkeys (<cold_pubkey>, <hot_pubkey>), the amounts and CSV value ? Though I think you can grind amounts and CSV value in case of loss...But I'm not sure if you remove the critical data persistence requirement, just reduce the surface.\n>\n> \"Because we want to be able to respond immediately, and not have to dig out our cold private keys, we use an additional OP_CTV to encumber the \"swept\" coins for spending by only the cold wallet key.\"\n>\n> I think a robust vault deployment would imply the presence of a set of watchtowers, redundant entities able to broadcast the cold transaction in reaction to unexpected unvault. One feature which could be interesting is \"tower accountability\", i.e knowing which tower initiated the broadcast, especially if it's a faultive one. One way is to watermark the cold transaction (e.g tweak nLocktime to past value). Though I believe with CTV you would need as much different hashes than towers included in your unvault output (can be wrapped in a Taproot tree ofc). With presigned transactions, tagged versions of the cold transaction are stored off-chain.\n\nWith Taproot trees the versions of the cold transaction are also stored off-chain, and each tower gets its own transaction revealing only one of the tapleaf branches.\nIt does have the disadvantage that you have O(log N) x 32 Merkle tree path references, whereas a presigned Taproot transaction just needs a single 64-byte signature for possibly millions of towers.\n\n> \"In this implementation, we make use of anchor outputs in order to allow mummified unvault transactions to have their feerate adjusted dynamically.\"\n>\n> I'm not sure if the usage of anchor output is safe for any vault deployment where the funds stakeholders do not trust each other or where the watchtowers are not trusted. If a distrusted party can spend the anchor output it's easy to block the RBF with a pinning.\n\nI agree.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-03-10T21:12:44",
                "message_text_only": "Hi Zeeman,\n\n> Have not looked at the actual vault design, but I observe that Taproot\nallows for a master key (which can be an n-of-n, or a k-of-n with setup\n(either expensive or trusted, but I repeat myself)) to back out of any\ncontract.\n>\n> This master key could be an \"even colder\" key that you bury in the desert\nto be guarded over by generations of Fremen riding giant sandworms until\nthe Bitcoin Path prophesied by the Kwisatz Haderach, Satoshi Nakamoto,\narrives.\n\nYes I agree you can always bless your hashchain-based off-chain contract\nwith an upgrade path thanks to Taproot. Though now this master key become\nthe point-of-failure to compromise, compared to hashchain.\n\nI think you can even go fancier than a human desert to hide a master key\nwith \"vaults\" geostationary satellites [0] !\n\n[0] https://github.com/oleganza/bitcoin-papers/blob/master/SatelliteVault.md\n\n> Thought: It would be nice if Alice could use Lightning watchtowers as\nwell, that would help increase the anonymity set of both LN watchtower\nusers and vault users.\n\nWell, I'm not sure if it's really binding toward the watchtowers.\nA LN channel is likely to have a high-frequency of updates (in both\nLN-penalty/Eltoo design I think)\nA vault is likely to have low-frequency of updates (e.g an once a day\nspending)\n\nI think that point is addressable by generating noise traffic from the\nvault entity to adopt a classic LN channel pattern. However, as a vault\n\"high-stake\" user, you might not be eager to leak your watchtower IP\naddress or even Tor onion service to \"low-stake\" LN channel swarms of\nusers. So it might end up on different tower deployments because off-chain\ncontracts' level of safety requirements are not the same, I don't know..\n\n> With Taproot trees the versions of the cold transaction are also stored\noff-chain, and each tower gets its own transaction revealing only one of\nthe tapleaf branches.\n> It does have the disadvantage that you have O(log N) x 32 Merkle tree\npath references, whereas a presigned Taproot transaction just needs a\nsingle 64-byte signature for possibly millions of towers.\n\nI agree here though note vaults users might be interested to pay the fee\nwitness premium just to get the tower accountability feature.\n\nAntoine\n\nLe lun. 7 mars 2022 \u00e0 19:57, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning Antoine,\n>\n> > Hi James,\n> >\n> > Interesting to see a sketch of a CTV-based vault design !\n> >\n> > I think the main concern I have with any hashchain-based vault design is\n> the immutability of the flow paths once the funds are locked to the root\n> vault UTXO. By immutability, I mean there is no way to modify the\n> unvault_tx/tocold_tx transactions and therefore recover from transaction\n> fields\n> > corruption (e.g a unvault_tx output amount superior to the root vault\n> UTXO amount) or key endpoints compromise (e.g the cold storage key being\n> stolen).\n> >\n> > Especially corruption, in the early phase of vault toolchain deployment,\n> I believe it's reasonable to expect bugs to slip in affecting the output\n> amount or relative-timelock setting correctness (wrong user config,\n> miscomputation from automated vault management, ...) and thus definitively\n> freezing the funds. Given the amounts at stake for which vaults are\n> designed, errors are likely to be far more costly than the ones we see in\n> the deployment of payment channels.\n> >\n> > It might be more conservative to leverage a presigned transaction data\n> design where every decision point is a multisig. I think this design gets\n> you the benefit to correct or adapt if all the multisig participants agree\n> on. It should also achieve the same than a key-deletion design, as long as\n> all\n> > the vault's stakeholders are participating in the multisig, they can\n> assert that flow paths are matching their spending policy.\n>\n> Have not looked at the actual vault design, but I observe that Taproot\n> allows for a master key (which can be an n-of-n, or a k-of-n with setup\n> (either expensive or trusted, but I repeat myself)) to back out of any\n> contract.\n>\n> This master key could be an \"even colder\" key that you bury in the desert\n> to be guarded over by generations of Fremen riding giant sandworms until\n> the Bitcoin Path prophesied by the Kwisatz Haderach, Satoshi Nakamoto,\n> arrives.\n>\n> > Of course, relying on presigned transactions comes with higher\n> assumptions on the hardware hosting the flow keys. Though as\n> hashchain-based vault design imply \"secure\" key endpoints (e.g\n> <cold_pubkey>), as a vault user you're still encumbered with the issues of\n> key management, it doesn't relieve you to find trusted hardware. If you\n> want to avoid multiplying devices to trust, I believe flow keys can be\n> stored on the same keys guarding the UTXOs, before sending to vault custody.\n> >\n> > I think the remaining presence of trusted hardware in the vault design\n> might lead one to ask what's the security advantage of vaults compared to\n> classic multisig setup. IMO, it's introducing the idea of privileges in the\n> coins custody : you set up the flow paths once for all at setup with the\n> highest level of privilege and then anytime you do a partial unvault you\n> don't need the same level of privilege. Partial unvault authorizations can\n> come with a reduced set of verifications, at lower operational costs. That\n> said, I think this security advantage is only relevant in the context of\n> recursive design, where the partial unvault sends back the remaining funds\n> to vault UTXO (not the design proposed here).\n> >\n> > Few other thoughts on vault design, more minor points.\n> >\n> > \"If Alice is watching the mempool/chain, she will see that the unvault\n> transaction has been unexpectedly broadcast,\"\n> >\n> > I think you might need to introduce an intermediary, out-of-chain\n> protocol step where the unvault broadcast is formally authorized by the\n> vault stakeholders. Otherwise it's hard to qualify \"unexpected\", as hot key\n> compromise might not be efficiently detected.\n>\n> Thought: It would be nice if Alice could use Lightning watchtowers as\n> well, that would help increase the anonymity set of both LN watchtower\n> users and vault users.\n>\n> > \"With <hash> OP_CTV, we can ensure that the vault operation is enforced\n> by consensus itself, and the vault transaction data can be generated\n> deterministically without additional storage needs.\"\n> >\n> > Don't you also need the endpoint scriptPubkeys (<cold_pubkey>,\n> <hot_pubkey>), the amounts and CSV value ? Though I think you can grind\n> amounts and CSV value in case of loss...But I'm not sure if you remove the\n> critical data persistence requirement, just reduce the surface.\n> >\n> > \"Because we want to be able to respond immediately, and not have to dig\n> out our cold private keys, we use an additional OP_CTV to encumber the\n> \"swept\" coins for spending by only the cold wallet key.\"\n> >\n> > I think a robust vault deployment would imply the presence of a set of\n> watchtowers, redundant entities able to broadcast the cold transaction in\n> reaction to unexpected unvault. One feature which could be interesting is\n> \"tower accountability\", i.e knowing which tower initiated the broadcast,\n> especially if it's a faultive one. One way is to watermark the cold\n> transaction (e.g tweak nLocktime to past value). Though I believe with CTV\n> you would need as much different hashes than towers included in your\n> unvault output (can be wrapped in a Taproot tree ofc). With presigned\n> transactions, tagged versions of the cold transaction are stored off-chain.\n>\n> With Taproot trees the versions of the cold transaction are also stored\n> off-chain, and each tower gets its own transaction revealing only one of\n> the tapleaf branches.\n> It does have the disadvantage that you have O(log N) x 32 Merkle tree path\n> references, whereas a presigned Taproot transaction just needs a single\n> 64-byte signature for possibly millions of towers.\n>\n> > \"In this implementation, we make use of anchor outputs in order to allow\n> mummified unvault transactions to have their feerate adjusted dynamically.\"\n> >\n> > I'm not sure if the usage of anchor output is safe for any vault\n> deployment where the funds stakeholders do not trust each other or where\n> the watchtowers are not trusted. If a distrusted party can spend the anchor\n> output it's easy to block the RBF with a pinning.\n>\n> I agree.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220310/850b9636/attachment.html>"
            },
            {
                "author": "James O'Beirne",
                "date": "2022-03-08T19:46:03",
                "message_text_only": "Hey Antoine,\n\nThanks for taking a look at the repo.\n\n> I believe it's reasonable to expect bugs to slip in affecting the\n> output amount or relative-timelock setting correctness\n\nI don't really see the vaults case as any different from other\nsufficiently involved uses of bitcoin script - I don't remember anyone\nraising these concerns for lightning scripts or DLCs or tapscript use,\nany of which could be catastrophic if wallet implementations are not\ntested properly.\n\nBy comparison, decreasing amount per vault step and one CSV use\nseems pretty simple. It's certainly easy to test (as the repo shows),\nand really the only parameter the user has is how many blocks to delay\nto the `tohot_tx` and perhaps fee-rate. Not too hard to test\ncomprehensively as far as I can tell.\n\n\n> I think the main concern I have with any hashchain-based vault design\n> is the immutability of the flow paths once the funds are locked to the\n> root vault UTXO.\n\nIsn't this kind of inherent to the idea of covenants? You're\nprecommitting to a spend path. You can put in as many \"escape-hatch\"\nconditions as you want (e.g. Jeremy makes the good point I should\ninclude an immediate-to-cold step that is sibling to the unvaulting),\nbut fundamentally if you're doing covenants, you're precommitting to a\nflow of funds. Otherwise what's the point?\n\n\n> I think the remaining presence of trusted hardware in the vault design\n> might lead one to ask what's the security advantage of vaults compared\n> to classic multisig setup.\n\nWho's saying to trust hardware? Your cold key in the vault structure\ncould have been generated by performing SHA rounds with the\npebbles in your neighbor's zen garden.\n\nKeeping an actively used multi-sig setup secure certainly isn't free or\neasy. Multi-sig ceremonies (which of course can be used in this scheme)\ncan be cumbersome to coordinate.\n\nIf there's a known scheme that doesn't require covenants, but has\nsimilar usage and security characteristics, I'd love\nto know it! But being able to lock coins up for an arbitrary amount of\ntime and then have advance notice of an attempted spend only seems\npossible with some kind of covenant technique.\n\n> That said, I think this security advantage is only relevant in the\n> context of recursive design, where the partial unvault sends back the\n> remaining funds to vault UTXO (not the design proposed here).\n\nI'm not really sure why this would be. Yeah, it would be cool to be able\nto partially unvault arbitrary amounts or something, but that seems like\nanother order of complexity. Personally, I'd be happy to \"tranche up\"\nfunds I'd like to store into a collection of single-hop vaults vs.\nthe techniques available to us today.\n\n\n> I think you might need to introduce an intermediary, out-of-chain\n> protocol step where the unvault broadcast is formally authorized by\n> the vault stakeholders. Otherwise it's hard to qualify \"unexpected\",\n> as hot key compromise might not be efficiently detected.\n\nSure; if you're using vaults I think it's safe to assume you're a fairly\nsophisticated user of bitcoin, so running a process that monitors the\nchain and responds immediately with keyless to-cold broadcasts\ndoesn't seem totally out of the question, especially with conservative\nblock delays.\n\nPretty straightforward to send such a process (whether it's a program or\na collection of humans) an authenticated signal that says \"hey, expect a\nwithdrawal.\" This kind of alert allows for cross-referencing the\nactivity and seems a lot better than nothing!\n\n> Don't you also need the endpoint scriptPubkeys (<cold_pubkey>,\n> <hot_pubkey>), the amounts and CSV value ? Though I think you can\n> grind amounts and CSV value in case of loss...But I'm not sure if you\n> remove the critical data persistence requirement, just reduce the\n> surface.\n\nWith any use of bitcoin you're going to have critical data that needs to\nbe maintained (your privkeys at a minimum), so the game is always\nreducing surface area. If the presigned-txn vault design\nappealed to you as a user, this seems like a strict improvement.\n\n> I'm not sure if the usage of anchor output is safe for any vault\n> deployment where the funds stakeholders do not trust each other or\n> where the watchtowers are not trusted.\n\nI'm not sure who's proposing that counterparties who don't trust each\nother make a vault together. I'm thinking of individual users and\ncustodians, each of which functions as a single trusted entity.\n\nPerhaps your point here is that if I'm a custodian operating a vault and\nsomeone unexpectedly hacks the fee keys that encumber all of my anchor\noutputs, they can possibly pin my attempted response to the unvault\ntransaction - and that's true. But that doesn't seem like a fault unique\nto this scheme, and points to the need for better fee-bumping needs a la\nSIGHASH_GROUP or transaction sponsors.[0]\n\n> I would say space efficiency is of secondary concern\n\nIf every major custodian ends up implementing some type of vault scheme\n(not out of the question), this might be a lot of space! However I'm all\nfor facilitating the flow of bitcoin from major custodians to miners...\nbut it seems like we could do that more cleanly with a block size\nreduction ;). (JUST KIDDING!)\n\n---\n\nI think your idea about having watchtowers serve double-duty for\nlightning channels and vault schemes like this is a very good one!\n\n\nJames\n\n\n[0]:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220308/78ab90ef/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-03-10T22:31:32",
                "message_text_only": "Hi James,\n\n> I don't really see the vaults case as any different from other\n> sufficiently involved uses of bitcoin script - I don't remember anyone\n> raising these concerns for lightning scripts or DLCs or tapscript use,\n> any of which could be catastrophic if wallet implementations are not\n> tested properly.\n\nI think on the lightning side there were enough concerns w.r.t bugs\naffecting the toolchains in their infancy phases to motivate developers to\nbound max channel value to 2^24 for a while [0]\n\n[0] https://github.com/lightning/bolts/pull/590\n\n> By comparison, decreasing amount per vault step and one CSV use\n> seems pretty simple. It's certainly easy to test (as the repo shows),\n> and really the only parameter the user has is how many blocks to delay\n> to the `tohot_tx` and perhaps fee-rate. Not too hard to test\n> comprehensively as far as I can tell.\n\nAs of today you won't be able to test against bitcoin core that a CSV'ed\ntransaction is valid for propagation across the network because your\nmempool is going to reject it as non-final [1]\n\n[1] https://github.com/bitcoin/bitcoin/pull/21413\n\nVerifying that your whole set of off-chain covenanted transactions is\npropagating well at different feerate levels, and there is no surface\noffered to a malicious vault co-owner to pin them can turn quickly as a\nreal challenge, I believe.\n\n> I think the main concern I have with any hashchain-based vault design\n> is the immutability of the flow paths once the funds are locked to the\n> root vault UTXO.\n\n> Isn't this kind of inherent to the idea of covenants? You're\n> precommitting to a spend path. You can put in as many \"escape-hatch\"\n> conditions as you want (e.g. Jeremy makes the good point I should\n> include an immediate-to-cold step that is sibling to the unvaulting),\n> but fundamentally if you're doing covenants, you're precommitting to a\n> flow of funds. Otherwise what's the point?\n\nYeah, I agree here that's the idea of covenants to commit to a flow of\nfunds. However, I think leveraging hashchain covenants in terms of  vault\ndesign comes at the price to make transaction generation errors or key\nendpoint compromises hardly irrevocable.\n\nI would say you can achieve the same end goal of precommiting to a flow of\nfunds with \"pre-signed\" transactions (and actually that's what we do for\nlightning) though while still keeping the upgrade emergency option open. Of\ncourse, you re-introduce more assumptions on the devices where the upgrade\nkeys are laying.\n\nI believe both designs are viable, it's more a matter of explaining\nsecurity and reliability trade-offs to the vaults users. They might be even\ncomplimentary as answering different classes of self-custody needs. I'm\njust worried as protocol devs, we have a good understanding of those\ntrade-offs to convey them well to the vaults users and have them make a\nwell-informed decision.\n\n> Who's saying to trust hardware? Your cold key in the vault structure\n> could have been generated by performing SHA rounds with the\n> pebbles in your neighbor's zen garden.\n>\n> Keeping an actively used multi-sig setup secure certainly isn't free or\n> easy. Multi-sig ceremonies (which of course can be used in this scheme)\n> can be cumbersome to coordinate.\n>\n> If there's a known scheme that doesn't require covenants, but has\n> similar usage and security characteristics, I'd love\n> to know it! But being able to lock coins up for an arbitrary amount of\n> time and then have advance notice of an attempted spend only seems\n> possible with some kind of covenant technique.\n\nWell, if by covenants you include pre-signed transactions vaults designs,\nno sadly I don't know schemes offering the same usage and security\ncharacteristics...\n\n> That said, I think this security advantage is only relevant in the\n> context of recursive design, where the partial unvault sends back the\n> remaining funds to vault UTXO (not the design proposed here).\n\n> I'm not really sure why this would be. Yeah, it would be cool to be able\n> to partially unvault arbitrary amounts or something, but that seems like\n> another order of complexity. Personally, I'd be happy to \"tranche up\"\n> funds I'd like to store into a collection of single-hop vaults vs.\n> the techniques available to us today.\n\nHmmm if you would like to be able to partially unvault arbitrary amounts,\nwhile still precommitting to the flow of funds, you might need a sighash\nflag extension like SIGHASH_ANYAMOUNT ? (my 2 sats, I don't have a design)\n\nYes, \"tranche up\" funds where the remainder is sent back to a vault UTXO\nsounds to me belonging to the recursive class of design, and yeah I agree\nthat might be one of the most interesting features of vaults.\n\n> Pretty straightforward to send such a process (whether it's a program or\n> a collection of humans) an authenticated signal that says \"hey, expect a\n> withdrawal.\" This kind of alert allows for cross-referencing the\n> activity and seems a lot better than nothing!\n\nYep, a nice improvement. And now you enter into a new wormhole of providing\nyour process with keys to authenticate the signals and how those\nnon-necessarily bitcoin locking-UTXO keys can be compromised or even how\nthe alert mechanism can be abused. We know, security is a never over game :D\n\n> With any use of bitcoin you're going to have critical data that needs to\n> be maintained (your privkeys at a minimum), so the game is always\n> reducing surface area. If the presigned-txn vault design\n> appealed to you as a user, this seems like a strict improvement.\n\nI agree here, the critical data surface sounds to be better with\nhashchain-based vaults designs.\n\n> I'm not sure who's proposing that counterparties who don't trust each\n> other make a vault together. I'm thinking of individual users and\n> custodians, each of which functions as a single trusted entity.\n\nIf you have a set of custodians, even if they belong to the same\nadministrative entity, one of them might be compromised or bribed and leak\nthe anchor output key to an attacker, therefore preventing the remaining\ncustodians from using the `tocold_tx` ability as expected.\n\nI'm not sure if thinking of the custodians as a single trusted entity is a\nhard enough assumption for high-stake vaults designs..\n\n> Perhaps your point here is that if I'm a custodian operating a vault and\n> someone unexpectedly hacks the fee keys that encumber all of my anchor\n> outputs, they can possibly pin my attempted response to the unvault\n> transaction - and that's true. But that doesn't seem like a fault unique\n> to this scheme, and points to the need for better fee-bumping needs a la\n> SIGHASH_GROUP or transaction sponsors.[0]\n\nYes agree here.\n\n> I would say space efficiency is of secondary concern\n\n> If every major custodian ends up implementing some type of vault scheme\n> (not out of the question), this might be a lot of space! However I'm all\n> for facilitating the flow of bitcoin from major custodians to miners...\n> but it seems like we could do that more cleanly with a block size\n> reduction ;). (JUST KIDDING!)\n\nHaha, designing _inefficient_ off-chain contracts for high buying power\nusers to have them pay better the miners in a post-block subsidy world.\nSounds smart, well done :)\n\nAntoine\n\nLe mar. 8 mars 2022 \u00e0 14:46, James O'Beirne <james.obeirne at gmail.com> a\n\u00e9crit :\n\n> Hey Antoine,\n>\n> Thanks for taking a look at the repo.\n>\n> > I believe it's reasonable to expect bugs to slip in affecting the\n> > output amount or relative-timelock setting correctness\n>\n> I don't really see the vaults case as any different from other\n> sufficiently involved uses of bitcoin script - I don't remember anyone\n> raising these concerns for lightning scripts or DLCs or tapscript use,\n> any of which could be catastrophic if wallet implementations are not\n> tested properly.\n>\n> By comparison, decreasing amount per vault step and one CSV use\n> seems pretty simple. It's certainly easy to test (as the repo shows),\n> and really the only parameter the user has is how many blocks to delay\n> to the `tohot_tx` and perhaps fee-rate. Not too hard to test\n> comprehensively as far as I can tell.\n>\n>\n> > I think the main concern I have with any hashchain-based vault design\n> > is the immutability of the flow paths once the funds are locked to the\n> > root vault UTXO.\n>\n> Isn't this kind of inherent to the idea of covenants? You're\n> precommitting to a spend path. You can put in as many \"escape-hatch\"\n> conditions as you want (e.g. Jeremy makes the good point I should\n> include an immediate-to-cold step that is sibling to the unvaulting),\n> but fundamentally if you're doing covenants, you're precommitting to a\n> flow of funds. Otherwise what's the point?\n>\n>\n> > I think the remaining presence of trusted hardware in the vault design\n> > might lead one to ask what's the security advantage of vaults compared\n> > to classic multisig setup.\n>\n> Who's saying to trust hardware? Your cold key in the vault structure\n> could have been generated by performing SHA rounds with the\n> pebbles in your neighbor's zen garden.\n>\n> Keeping an actively used multi-sig setup secure certainly isn't free or\n> easy. Multi-sig ceremonies (which of course can be used in this scheme)\n> can be cumbersome to coordinate.\n>\n> If there's a known scheme that doesn't require covenants, but has\n> similar usage and security characteristics, I'd love\n> to know it! But being able to lock coins up for an arbitrary amount of\n> time and then have advance notice of an attempted spend only seems\n> possible with some kind of covenant technique.\n>\n> > That said, I think this security advantage is only relevant in the\n> > context of recursive design, where the partial unvault sends back the\n> > remaining funds to vault UTXO (not the design proposed here).\n>\n> I'm not really sure why this would be. Yeah, it would be cool to be able\n> to partially unvault arbitrary amounts or something, but that seems like\n> another order of complexity. Personally, I'd be happy to \"tranche up\"\n> funds I'd like to store into a collection of single-hop vaults vs.\n> the techniques available to us today.\n>\n>\n> > I think you might need to introduce an intermediary, out-of-chain\n> > protocol step where the unvault broadcast is formally authorized by\n> > the vault stakeholders. Otherwise it's hard to qualify \"unexpected\",\n> > as hot key compromise might not be efficiently detected.\n>\n> Sure; if you're using vaults I think it's safe to assume you're a fairly\n> sophisticated user of bitcoin, so running a process that monitors the\n> chain and responds immediately with keyless to-cold broadcasts\n> doesn't seem totally out of the question, especially with conservative\n> block delays.\n>\n> Pretty straightforward to send such a process (whether it's a program or\n> a collection of humans) an authenticated signal that says \"hey, expect a\n> withdrawal.\" This kind of alert allows for cross-referencing the\n> activity and seems a lot better than nothing!\n>\n> > Don't you also need the endpoint scriptPubkeys (<cold_pubkey>,\n> > <hot_pubkey>), the amounts and CSV value ? Though I think you can\n> > grind amounts and CSV value in case of loss...But I'm not sure if you\n> > remove the critical data persistence requirement, just reduce the\n> > surface.\n>\n> With any use of bitcoin you're going to have critical data that needs to\n> be maintained (your privkeys at a minimum), so the game is always\n> reducing surface area. If the presigned-txn vault design\n> appealed to you as a user, this seems like a strict improvement.\n>\n> > I'm not sure if the usage of anchor output is safe for any vault\n> > deployment where the funds stakeholders do not trust each other or\n> > where the watchtowers are not trusted.\n>\n> I'm not sure who's proposing that counterparties who don't trust each\n> other make a vault together. I'm thinking of individual users and\n> custodians, each of which functions as a single trusted entity.\n>\n> Perhaps your point here is that if I'm a custodian operating a vault and\n> someone unexpectedly hacks the fee keys that encumber all of my anchor\n> outputs, they can possibly pin my attempted response to the unvault\n> transaction - and that's true. But that doesn't seem like a fault unique\n> to this scheme, and points to the need for better fee-bumping needs a la\n> SIGHASH_GROUP or transaction sponsors.[0]\n>\n> > I would say space efficiency is of secondary concern\n>\n> If every major custodian ends up implementing some type of vault scheme\n> (not out of the question), this might be a lot of space! However I'm all\n> for facilitating the flow of bitcoin from major custodians to miners...\n> but it seems like we could do that more cleanly with a block size\n> reduction ;). (JUST KIDDING!)\n>\n> ---\n>\n> I think your idea about having watchtowers serve double-duty for\n> lightning channels and vault schemes like this is a very good one!\n>\n>\n> James\n>\n>\n> [0]:\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220310/cb328ca3/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV vaults in the wild",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Antoine Riard",
                "James O'Beirne"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 40224
        }
    },
    {
        "title": "[bitcoin-dev] Jets (Was: `OP_FOLD`: A Looping Construct For Bitcoin SCRIPT)",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-07T23:35:04",
                "message_text_only": "Good morning Billy,\n\nChanged subject since this is only tangentially related to `OP_FOLD`.\n\n> Let me organize my thoughts on this a little more clearly. There's a couple possibilities I can think of for a jet-like system:\n>\n> A. We could implement jets now without a consensus change, and without\u00a0requiring all nodes to upgrade to new relay rules. Probably. This would give upgraded nodes improved\u00a0validation performance and many upgraded nodes relay savings (transmitting/receiving fewer bytes). Transactions would be weighted the same as without the use of jets tho.\n> B. We could implement the above\u00a0+ lighter weighting by using a soft fork to put the jets in a part of the blockchain hidden from unupgraded nodes, as you mentioned.\u00a0\n> C. We could implement the above\u00a0+ the jet registration idea in a soft fork.\u00a0\n>\n> For A:\n>\n> * Upgraded nodes query each connection for support of jets in general, and which specific jets they support.\n> * For a connection to another upgraded node that supports the jet(s) that a transaction contains, the transaction is sent verbatim\u00a0with the jet included in the script (eg as some fake opcode line like 23 OP_JET, indicating to insert standard jet 23 in its place). When validation happens, or when a miner includes it in a block, the jet opcode call is replaced with the script it represents so hashing happens in a way that is recognizable to unupgraded nodes.\n> * For a connection to a non-upgraded node that doesn't support jets, or an upgraded node that doesn't support the particular jet included in the script, the jet opcode call is replaced as above before sending to that node. In addition, some data is added to the transaction that unupgraded nodes propagate along but otherwise ignore. Maybe this is extra witness data, maybe this is some kind of \"annex\", or something else. But that data would contain the original jet opcode (in this example \"23 OP_JET\") so that when that transaction data reaches an upgraded node that recognizes that jet again, it can swap that back in, in place of the script fragment it represents.\u00a0\n>\n> I'm not 100% sure the required mechanism I mentioned of \"extra ignored data\" exists, and if it doesn't, then all nodes would at least need to be upgraded to support that before this mechanism could fully work.\n\nI am not sure that can even be *made* to exist.\nIt seems to me a trivial way to launch a DDoS: Just ask a bunch of fullnodes to add this 1Mb of extra ignored data in this tiny 1-input-1-output transaction so I pay only a small fee if it confirms but the bandwidth of all fullnodes is wasted transmitting and then ignoring this block of data.\n\n> But even if such a mechanism doesn't exist, a jet script could still be used, but it would be clobbered by the first nonupgraded node it is relayed to, and can't then be converted back (without using a potentially expensive lookup table as you mentioned).\u00a0\n\nYes, and people still run Bitcoin Core 0.8.x.....\n\n> > If the script does not weigh less if it uses a jet, then there is no incentive for end-users to use a jet\n>\n> That's a good point. However, I'd point out that nodes do lots of things that there's no individual incentive for, and this might be one where people either altruistically use jets to be lighter on the network, or use them in the hopes that the jet is accepted as a standard, reducing the cost of their scripts. But certainly a direct incentive to use them is better. Honest nodes can favor connecting to those that support jets.\n\nSince you do not want a dynamic lookup table (because of the cost of lookup), how do new jets get introduced?\nIf a new jet requires coordinated deployment over the network, then you might as well just softfork and be done with it.\nIf a new jet can just be entered into some configuration file, how do you coordinate those between multiple users so that there *is* some benefit for relay?\n\n> >if a jet would allow SCRIPT weights to decrease, upgraded nodes need to hide them from unupgraded nodes\n> > we have to do that by telling unupgraded nodes \"this script will always succeed and has weight 0\"\n>\n> Right. It doesn't have to be weight zero, but that would work fine enough.\u00a0\n>\n> > if everybody else has not upgraded, a user of a new jet has no security.\n>\n> For case A, no security is lost. For case B you're right. For case C, once nodes upgrade to the initial soft fork, new registered jets can take advantage of relay-cost weight savings (defined by the soft fork) without requiring any nodes to do any upgrading, and nodes could be further upgraded to optimize the validation of various of those registered jets, but those processing savings couldn't change the weighting of transactions without an additional soft fork.\n>\n> > Consider an attack where I feed you a SCRIPT that validates trivially but is filled with almost-but-not-quite-jettable code\n>\n> I agree a pattern-matching lookup table is probably not a great design. But a lookup table like that is not needed for the jet registration idea. After the necessary soft fork, there would be standard rules for which registered jets nodes are required to keep an index of, and so the lookup table would be a straightforward jet hash lookup rather than a pattern-matching lookup, which wouldn't have the same DOS problems. A node would simply find a jet opcode call like \"ab38cd39e OP_JET\" and just lookup ab38cd39e in its index.\u00a0\n\nHow does the unupgraded-to-upgraded boundary work?\nHaving a static lookup table is better since you can pattern-match on strings of specific, static length, and we can take a page from `rsync` and use its \"rolling checksum\" idea which works with identifying strings of a certain specific length at arbitrary offsets.\n\nSay you have jetted sequences where the original code is 42 bytes, and another jetted sequence where the original code is 54 bytes, you would keep a 42-byte rolling checksum and a separate 54-byte rolling checksum, and then when it matches, you check if the last 42 or 54 bytes matched the jetted sequences.\n\nIt does imply having a bunch of rolling checksums around, though.\nSigh.\n\n---\n\nTo make jets more useful, we should redesign the language so that `OP_PUSH` is not in the opcode stream, but instead, we have a separate table of constants that is attached / concatenated to the actual SCRIPT.\n\nSo for example instead of an HTLC having embedded `OP_PUSH`es like this:\n\n   OP_IF\n       OP_HASH160 <hash> OP_EQUALVERIFY OP_DUP OP_HASH160 <acceptor pkh>\n   OP_ELSE\n       <timeout> OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160 <offerrer pkh>\n   OP_ENDIF\n   OP_EQUALVERIFY\n   OP_CHECKSIG\n\nWe would have:\n\n   constants:\n       h = <hash>\n       a = <acceptor pkh>\n       t = <timeout>\n       o = <offerer pkh>\n   script:\n       OP_IF\n           OP_HASH160 h OP_EQUALVERIFY OP_DUP OP_HASH160 a\n       OP_ELSE\n           t OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160 o\n       OP_ENDIF\n       OP_EQUALVERIFY\n       OP_CHECKSIG\n\nThe above allows for more compressibility, as the entire `script` portion can be recognized as a jet outright.\nMove the incompressible hashes out of the main SCRIPT body.\n\nWe should note as well that this makes it *easier* to create recursive covenants (for good or ill) out of `OP_CAT` and whatever opcode you want that allows recursive covenants in combination with `OP_CAT`.\nGenerally, recursive covenants are *much* more interesting if they can change some variables at each iteration, and having a separate table-of-constants greatly facilitates that.\n\nIndeed, the exercise of `OP_TLUV` in [drivechains-over-recursive-convenants][] puts the loop variables into the front of the SCRIPT to make it easier to work with the SCRIPT manipulation.\n\n[drivechains-over-recursive-covenants]: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019976.html\n\n---\n\nPerhaps we can consider the general vs specific tension in information-theoretic terms.\n\nA language which supports more computational power --- i.e. more general --- must, by necessity, have longer symbols, as a basic law of information theory.\nAfter all, a general language can express more things.\n\nHowever, we do recognize that certain sequences of things-to-say are much more likely than others.\nThat is, we expect that certain sequences \"make sense\" to do.\nThat is why \"jets\" are even proposed, they are shortcuts towards those.\n\nAssuming a general language is already deployed for Bitcoin, then a new opcode is a jet as it simply makes the SCRIPT shorter.\n\nInstead of starting with a verbose (by necessity) general language, we could instead start with a terse but restricted language, and slowly loosen up its restrictions by adding new capabilities in softforks.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-10T05:05:35",
                "message_text_only": "Hi ZmnSCPxj,\n\n>  Just ask a bunch of fullnodes to add this 1Mb of extra ignored data in\nthis tiny 1-input-1-output transaction so I pay only a small fee\n\nI'm not suggesting that you wouldn't have to pay a fee for it. You'd pay a\nfee for it as normal, so there's no DOS vector. Doesn't adding\nextra witness data do what would be needed here? Eg simply adding extra\ndata onto the witness script that will remain unconsumed after successful\nexecution of the script?\n\n> how do new jets get introduced?\n\nIn scenario A, new jets get introduced by being added to bitcoin software\nas basically relay rules.\n\n> If a new jet requires coordinated deployment over the network, then you\nmight as well just softfork and be done with it.\n\nIt would not need a coordinated deployment. However, the more nodes that\nsupported that jet, the more efficient using it would be for the network.\n\n> If a new jet can just be entered into some configuration file, how do you\ncoordinate those between multiple users so that there *is* some benefit for\nrelay?\n\nWhen a new version of bitcoin comes out, people generally upgrade to it\neventually. No coordination is needed. 100% of the network need not support\na jet. Just some critical mass to get some benefit.\n\n> Having a static lookup table is better since you can pattern-match on\nstrings of specific, static length\n\nSorry, better than what exactly?\n\n> How does the unupgraded-to-upgraded boundary work?\n\nThis is what I'm thinking. Imagine a simple script:\n\nOP_DUP\nOP_ADD\n\nwith witness\n\n1\n\nThis would execute as 1+1 = 2 -> success. Let's say the script is\njettified so we can instead write it as:\n\nOP_JET\n1b5f03cf # adler32 hash of the replaced script\n\nwith a witness:\n\nOP_JET   # Some number that represents OP_JET\n1b5f03cf\n0\n1\n\nA jet-aware node transmitting to another jet-aware node can transmit it as\nis (tho it would need to do a swap out to validate). For a jet-aware node\nto transmit this to a non-jet aware node, it would need to swap the OP_JET\ncall with the script it represents. So the transaction sent to the non-jet\naware node would have:\n\nScript:\n\nOP_DUP\nOP_ADD\n\nWitness:\n\nOP_JET\n1b5f03cf\n0\n1\n\nAnd you can see that this would execute and succeed by adding 1+1 and\nending up with the stack:\n\n2\n0\n1b5f03cf\nOP_JET\n\nWhich would succeed because of the non-zero top of stack.\n\nWhen the non-jet aware node sends this to a jet-aware node, that node would\nsee the extra items on the stack after script execution, and would\ninterpret them as an OP_JET call specifying that OP_JET should replace the\nwitness items starting at index 0 with `1b5f03cf  OP_JET`. It does this and\nthen sends that along to the next hop.\n\nIn order to support this without a soft fork, this extra otherwise\nunnecessary data would be needed, but for jets that represent long scripts,\nthe extra witness data could be well worth it (for the network).\n\nHowever, this extra data would be a disincentive to do transactions this\nway, even when its better for the network. So it might not be worth doing\nit this way without a soft fork. But with a soft fork to upgrade nodes to\nsupport an OP_JET opcode, the extra witness data can be removed (replaced\nwith out-of-band script fragment transmission for nodes that don't support\na particular jet).\n\nOne interesting additional thing that could be done with this mechanism is\nto add higher-order function ability to jets, which could allow nodes to\nadd OP_FOLD or similar functions as a jet without requiring additional soft\nforks.  Hypothetically, you could imagine a jet script that uses an OP_LOOP\njet be written as follows:\n\n5             # Loop 5 times\n1             # Loop the next 1 operation\n3c1g14ad\nOP_JET\nOP_ADD  # The 1 operation to loop\n\nThe above would sum up 5 numbers from the stack. And while this summation\njet can't be represented in bitcoin script on its own (since bitcoin script\ncan't manipulate opcode calls), the jet *call* can still be represented as:\n\nOP_ADD\nOP_ADD\nOP_ADD\nOP_ADD\nOP_ADD\n\nwhich means all of the above replacement functionality would work just as\nwell.\n\nSo my point here is that jets implemented in a way similar to this would\ngive a much wider range of \"code as compression\" possibilities than\nimplementing a single opcode like op_fold.\n\n> To make jets more useful, we should redesign the language so that\n`OP_PUSH` is not in the opcode stream, but instead, we have a separate\ntable of constants that is attached / concatenated to the actual SCRIPT.\n\nThis can already be done, right? You just have to redesign the script to\nconsume and swap/rot around the data in the right way to separate them out\nfrom the main script body.\n\n\nOn Mon, Mar 7, 2022 at 5:35 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Billy,\n>\n> Changed subject since this is only tangentially related to `OP_FOLD`.\n>\n> > Let me organize my thoughts on this a little more clearly. There's a\n> couple possibilities I can think of for a jet-like system:\n> >\n> > A. We could implement jets now without a consensus change, and\n> without requiring all nodes to upgrade to new relay rules. Probably. This\n> would give upgraded nodes improved validation performance and many upgraded\n> nodes relay savings (transmitting/receiving fewer bytes). Transactions\n> would be weighted the same as without the use of jets tho.\n> > B. We could implement the above + lighter weighting by using a soft fork\n> to put the jets in a part of the blockchain hidden from unupgraded nodes,\n> as you mentioned.\n> > C. We could implement the above + the jet registration idea in a soft\n> fork.\n> >\n> > For A:\n> >\n> > * Upgraded nodes query each connection for support of jets in general,\n> and which specific jets they support.\n> > * For a connection to another upgraded node that supports the jet(s)\n> that a transaction contains, the transaction is sent verbatim with the jet\n> included in the script (eg as some fake opcode line like 23 OP_JET,\n> indicating to insert standard jet 23 in its place). When validation\n> happens, or when a miner includes it in a block, the jet opcode call is\n> replaced with the script it represents so hashing happens in a way that is\n> recognizable to unupgraded nodes.\n> > * For a connection to a non-upgraded node that doesn't support jets, or\n> an upgraded node that doesn't support the particular jet included in the\n> script, the jet opcode call is replaced as above before sending to that\n> node. In addition, some data is added to the transaction that unupgraded\n> nodes propagate along but otherwise ignore. Maybe this is extra witness\n> data, maybe this is some kind of \"annex\", or something else. But that data\n> would contain the original jet opcode (in this example \"23 OP_JET\") so that\n> when that transaction data reaches an upgraded node that recognizes that\n> jet again, it can swap that back in, in place of the script fragment it\n> represents.\n> >\n> > I'm not 100% sure the required mechanism I mentioned of \"extra ignored\n> data\" exists, and if it doesn't, then all nodes would at least need to be\n> upgraded to support that before this mechanism could fully work.\n>\n> I am not sure that can even be *made* to exist.\n> It seems to me a trivial way to launch a DDoS: Just ask a bunch of\n> fullnodes to add this 1Mb of extra ignored data in this tiny\n> 1-input-1-output transaction so I pay only a small fee if it confirms but\n> the bandwidth of all fullnodes is wasted transmitting and then ignoring\n> this block of data.\n>\n> > But even if such a mechanism doesn't exist, a jet script could still be\n> used, but it would be clobbered by the first nonupgraded node it is relayed\n> to, and can't then be converted back (without using a potentially expensive\n> lookup table as you mentioned).\n>\n> Yes, and people still run Bitcoin Core 0.8.x.....\n>\n> > > If the script does not weigh less if it uses a jet, then there is no\n> incentive for end-users to use a jet\n> >\n> > That's a good point. However, I'd point out that nodes do lots of things\n> that there's no individual incentive for, and this might be one where\n> people either altruistically use jets to be lighter on the network, or use\n> them in the hopes that the jet is accepted as a standard, reducing the cost\n> of their scripts. But certainly a direct incentive to use them is better.\n> Honest nodes can favor connecting to those that support jets.\n>\n> Since you do not want a dynamic lookup table (because of the cost of\n> lookup), how do new jets get introduced?\n> If a new jet requires coordinated deployment over the network, then you\n> might as well just softfork and be done with it.\n> If a new jet can just be entered into some configuration file, how do you\n> coordinate those between multiple users so that there *is* some benefit for\n> relay?\n>\n> > >if a jet would allow SCRIPT weights to decrease, upgraded nodes need to\n> hide them from unupgraded nodes\n> > > we have to do that by telling unupgraded nodes \"this script will\n> always succeed and has weight 0\"\n> >\n> > Right. It doesn't have to be weight zero, but that would work fine\n> enough.\n> >\n> > > if everybody else has not upgraded, a user of a new jet has no\n> security.\n> >\n> > For case A, no security is lost. For case B you're right. For case C,\n> once nodes upgrade to the initial soft fork, new registered jets can take\n> advantage of relay-cost weight savings (defined by the soft fork) without\n> requiring any nodes to do any upgrading, and nodes could be further\n> upgraded to optimize the validation of various of those registered jets,\n> but those processing savings couldn't change the weighting of transactions\n> without an additional soft fork.\n> >\n> > > Consider an attack where I feed you a SCRIPT that validates trivially\n> but is filled with almost-but-not-quite-jettable code\n> >\n> > I agree a pattern-matching lookup table is probably not a great design.\n> But a lookup table like that is not needed for the jet registration idea.\n> After the necessary soft fork, there would be standard rules for which\n> registered jets nodes are required to keep an index of, and so the lookup\n> table would be a straightforward jet hash lookup rather than a\n> pattern-matching lookup, which wouldn't have the same DOS problems. A node\n> would simply find a jet opcode call like \"ab38cd39e OP_JET\" and just lookup\n> ab38cd39e in its index.\n>\n> How does the unupgraded-to-upgraded boundary work?\n> Having a static lookup table is better since you can pattern-match on\n> strings of specific, static length, and we can take a page from `rsync` and\n> use its \"rolling checksum\" idea which works with identifying strings of a\n> certain specific length at arbitrary offsets.\n>\n> Say you have jetted sequences where the original code is 42 bytes, and\n> another jetted sequence where the original code is 54 bytes, you would keep\n> a 42-byte rolling checksum and a separate 54-byte rolling checksum, and\n> then when it matches, you check if the last 42 or 54 bytes matched the\n> jetted sequences.\n>\n> It does imply having a bunch of rolling checksums around, though.\n> Sigh.\n>\n> ---\n>\n> To make jets more useful, we should redesign the language so that\n> `OP_PUSH` is not in the opcode stream, but instead, we have a separate\n> table of constants that is attached / concatenated to the actual SCRIPT.\n>\n> So for example instead of an HTLC having embedded `OP_PUSH`es like this:\n>\n>    OP_IF\n>        OP_HASH160 <hash> OP_EQUALVERIFY OP_DUP OP_HASH160 <acceptor pkh>\n>    OP_ELSE\n>        <timeout> OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160\n> <offerrer pkh>\n>    OP_ENDIF\n>    OP_EQUALVERIFY\n>    OP_CHECKSIG\n>\n> We would have:\n>\n>    constants:\n>        h = <hash>\n>        a = <acceptor pkh>\n>        t = <timeout>\n>        o = <offerer pkh>\n>    script:\n>        OP_IF\n>            OP_HASH160 h OP_EQUALVERIFY OP_DUP OP_HASH160 a\n>        OP_ELSE\n>            t OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160 o\n>        OP_ENDIF\n>        OP_EQUALVERIFY\n>        OP_CHECKSIG\n>\n> The above allows for more compressibility, as the entire `script` portion\n> can be recognized as a jet outright.\n> Move the incompressible hashes out of the main SCRIPT body.\n>\n> We should note as well that this makes it *easier* to create recursive\n> covenants (for good or ill) out of `OP_CAT` and whatever opcode you want\n> that allows recursive covenants in combination with `OP_CAT`.\n> Generally, recursive covenants are *much* more interesting if they can\n> change some variables at each iteration, and having a separate\n> table-of-constants greatly facilitates that.\n>\n> Indeed, the exercise of `OP_TLUV` in\n> [drivechains-over-recursive-convenants][] puts the loop variables into the\n> front of the SCRIPT to make it easier to work with the SCRIPT manipulation.\n>\n> [drivechains-over-recursive-covenants]:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019976.html\n>\n> ---\n>\n> Perhaps we can consider the general vs specific tension in\n> information-theoretic terms.\n>\n> A language which supports more computational power --- i.e. more general\n> --- must, by necessity, have longer symbols, as a basic law of information\n> theory.\n> After all, a general language can express more things.\n>\n> However, we do recognize that certain sequences of things-to-say are much\n> more likely than others.\n> That is, we expect that certain sequences \"make sense\" to do.\n> That is why \"jets\" are even proposed, they are shortcuts towards those.\n>\n> Assuming a general language is already deployed for Bitcoin, then a new\n> opcode is a jet as it simply makes the SCRIPT shorter.\n>\n> Instead of starting with a verbose (by necessity) general language, we\n> could instead start with a terse but restricted language, and slowly loosen\n> up its restrictions by adding new capabilities in softforks.\n>\n> Regards,\n> ZmnSCPxj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220309/08cd72e2/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-10T06:43:56",
                "message_text_only": "Good morning Billy,\n\n> Hi\u00a0ZmnSCPxj,\n>\n> >\u00a0 Just ask a bunch of fullnodes to add this 1Mb of extra ignored data in this tiny 1-input-1-output transaction so I pay only a small fee\n>\n> I'm not suggesting that you wouldn't have to pay a fee for it. You'd pay a fee for it as normal, so there's no DOS vector. Doesn't adding extra\u00a0witness data do what would be needed here? Eg simply adding extra data onto the witness script that will remain unconsumed after successful execution of the script?\n\nI think we would want to have a cleanstack rule at some point (do not remember out-of-hand if Taproot already enforces one).\n\nSo now being nice to the network is *more* costly?\nThat just *dis*incentivizes jet usage.\n\n> > how do new jets get introduced?\n>\n> In scenario A, new jets get introduced by being added to bitcoin software as basically relay rules.\u00a0\n>\n> > If a new jet requires coordinated deployment over the network, then you might as well just softfork and be done with it.\n>\n> It would not need a coordinated deployment. However, the more nodes that supported that jet, the more efficient using it would be for the network.\u00a0\n>\n> > If a new jet can just be entered into some configuration file, how do you coordinate those between multiple users so that there *is* some benefit for relay?\n>\n> When a new version of bitcoin comes out, people generally upgrade to it eventually. No coordination is needed. 100% of the network need not support a jet. Just some critical mass to\u00a0get some benefit.\u00a0\n\nHow large is the critical mass needed?\n\nIf you use witness to transport jet information across non-upgraded nodes, then that disincentivizes use of jets and you can only incentivize jets by softfork, so you might as well just get a softfork.\n\nIf you have no way to transport jet information from an upgraded through a non-upgraded back to an upgraded node, then I think you need a fairly large buy-in from users before non-upgraded nodes are rare enough that relay is not much affected, and if the required buy-in is large enough, you might as well softfork.\n\n> > Having a static lookup table is better since you can pattern-match on strings of specific, static length\n>\n> Sorry, better than what exactly?\u00a0\n\nThan using a dynamic lookup table, which is how I understood your previous email about \"scripts in the 1000 past blocks\".\n\n> > How does the unupgraded-to-upgraded boundary work?\n> <snip>\n> When the non-jet aware node sends this to a jet-aware node, that node would see the extra items on the stack after script execution, and would interpret them as an OP_JET call specifying that OP_JET should replace the witness items starting at index 0 with `1b5f03cf\u00a0\u00a0OP_JET`. It does this and then sends that along to the next hop.\n\nIt would have to validate as well that the SCRIPT sub-section matches the jet, else I could pretend to be a non-jet-aware node and give you a SCRIPT sub-section that does not match the jet and would cause your validation to diverge from other nodes.\n\nAdler32 seems a bit short though, it seems to me that it may lead to two different SCRIPT subsections hashing to the same hash.\n\nSuppose I have two different node softwares.\nOne uses a particular interpretation for a particular Adler32 hash.\nThe other uses a different interpretation.\nIf we are not careful, if these two jet-aware software talk to each other, they will ban each other from the network and cause a chainsplit.\nSince the Bitcoin software is open source, nothing prevents anyone from using a different SCRIPT subsection for a particular Adler32 hash if they find a collision and can somehow convince people to run their modified software.\n\n> In order to support this without a soft fork, this extra otherwise unnecessary\u00a0data would be needed, but for jets that represent long scripts, the extra witness data could be well worth it (for the network).\u00a0\n>\n> However, this extra data would be a disincentive to do transactions this way, even when its\u00a0better for the network. So it might not be worth doing it this way without a soft fork. But with a soft fork to upgrade nodes to support an OP_JET opcode, the extra witness data can be removed (replaced with out-of-band script fragment transmission for nodes that don't support a particular jet).\u00a0\n\nWhich is why I pointed out that each individual jet may very well require a softfork, or enough buy-in that you might as well just softfork.\n\n> One interesting additional thing that could be done with this mechanism is to add higher-order function ability to jets, which could allow nodes to add OP_FOLD or similar functions as a jet without requiring additional soft forks.\u00a0 Hypothetically, you could imagine a jet script that uses an OP_LOOP jet be written as follows:\n>\n> 5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Loop 5 times\n> 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Loop the next 1 operation\n> 3c1g14ad\u00a0\n> OP_JET\n> OP_ADD\u00a0 # The 1 operation to loop\n>\n> The above would sum up 5 numbers from the stack. And while this summation jet can't be represented in bitcoin script on its own (since bitcoin script can't manipulate opcode calls), the jet *call* can still be represented as:\n>\n> OP_ADD\u00a0\u00a0\n> OP_ADD\u00a0\u00a0\n> OP_ADD\u00a0\u00a0\n> OP_ADD\u00a0\u00a0\n> OP_ADD\u00a0\u00a0\n>\n> which means all of the above replacement functionality would work just as well.\u00a0\n>\n> So my point here is that jets implemented in a way similar to this would give a much wider range of \"code as compression\" possibilities than implementing a single opcode like op_fold.\u00a0\n\nYes, that is certainly the case, and nothing really prevents us bringing \"programming as compression\" to its logical conclusion.\n\n> > To make jets more useful, we should redesign the language so that `OP_PUSH` is not in the opcode stream, but instead, we have a separate table of constants that is attached / concatenated to the actual SCRIPT.\n>\n> This can already be done, right? You just have to redesign the script to consume and swap/rot around the data in the right way to separate them out from the main script body.\u00a0\n\nYes, but that implies additional operations (and execution overhead), increasing the costs to use jets, which makes it even less palatable to use jets, *in addition to* the witness hack disincentivizing jets.\n\nSo I would suggest that, if we were to seriously pursue jets, we should really replace most of the `OP_PUSH` opcodes with variants that look up in a static table at the start, before the executable script body.\nI.e. opcodes 0x01 to 0x4e instead mean \"push contents of `c1` to `c78` from the constants table\", and have aliases `a` through `z` for `c1` to `c26`, etc.\nThat way, replacing the `OP_PUSH` is shorter in the actual SCRIPT (instead of a bunch of stack manipulations) and hopefully the overhead of the constants table can be kept low.\n\nIn particular, this helps jets compose more easily; if we want a SCRIPT that incorporates an existing jet, we do not have to manipulate the stack in a way that the existing jet expects, we just load the proper data into the constants table.\n\nOr something, anyway.\nThis seems a fair amount of complexity here.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-11T14:11:33",
                "message_text_only": "> I think we would want to have a cleanstack rule at some point\n\nAh is this a rule where a script shouldn't validate if more than just a\ntrue is left on the stack? I can see how that would prevent the\nnon-soft-fork version of what I'm proposing.\n\n> How large is the critical mass needed?\n\nWell it seems we've agreed that were we going to do this, we would want to\nat least do a soft-fork to make known jet scripts lighter weight (and\nunknown jet scripts not-heavier) than their non-jet counterparts. So given\na situation where this soft fork happens, and someone wants to implement a\nnew jet, how much critical mass would be needed for the network to get some\nbenefit from the jet? Well, the absolute minimum for some benefit to happen\nis that two nodes that support that jet are connected. In such a case, one\nnode can send that jet scripted transaction along without sending the data\nof what the jet stands for. The jet itself is pretty small, like 2 or so\nbytes. So that does impose a small additional cost on nodes that don't\nsupport a jet. For 100,000 nodes, that means 200,000 bytes of transmission\nwould need to be saved for a jet to break even. So if the jet stands for a\n22 byte script, it would break even when 10% of the network supported it.\nIf the jet stood for a 102 byte script, it would break even when 2% of the\nnetwork supported it. So how much critical mass is necessary for it to be\nworth it depends on what the script is.\n\n>  Than using a dynamic lookup table, which is how I understood your\nprevious email about \"scripts in the 1000 past blocks\".\n\nAh, I didn't mean using a dynamic lookup. This was about the idea of jet\nregistration, where registered jets would be kept a count of for some\nnumber of blocks (eg 1000) and dropped if they don't reach a threshold rate\nof usage. A static lookup table would work for this, I agree.\n\n> It would have to validate as well that the SCRIPT sub-section matches the\njet\n\nSeems like a good idea.\n\n> Adler32 seems a bit short though\n\nYou might be right. Certainly some more care would need to be taken in an\nactual implementation than I've taken writing my back of the napkin idea\nout ; )\n\n> nothing prevents anyone from using a different SCRIPT subsection for a\nparticular Adler32 hash if they find a collision and can somehow convince\npeople to run their modified software.\n\nSomeone that can convince people to run their modified software can\n*always* cause those people to chainsplit from the main chain, so I don't\nthink the above ideas are special in this regard.\n\n>> it might not be worth doing it this way without a soft fork\n> Which is why I pointed out that each individual jet may very well require\na softfork, or enough buy-in that you might as well just softfork.\n\nI'm saying something rather different actually. I don't think each\nindividual jet requires a softfork to be quite useful. What I meant by \"it\nmight not be worth doing it this way without a soft fork\" is that we\nprobably want to implement a soft fork to allow all jet scripts to have\nreduced blockweight. However, once most nodes support that soft fork, new\nindividual jets do not need a softfork for the network to take advantage of\nthem. As I mused about above, even 10% of the network supporting a jet\nstandin for a medium length script could result in significant network\nbandwidth savings. Different sections of the network could decide\nindividually what jets they want to support without needing the usual chaos\nof a soft fork for each one, but of course the more the better for a\npopular jet. There would be benefits for eventually soft forking such jets\nin (to make them weigh even less based on implementation of optimized\nvalidation functions), and real life usage of those jets could inform the\ndecisions around them. They could already be well tested in the wild before\nbeing upgraded in a softfork.\n\n> Yes, but that implies additional operations (and execution overhead),\nincreasing the costs to use jets, which makes it even less palatable to use\njets, *in addition to* the witness hack disincentivizing jets.\n\nFor the use of a single jet, this can be completely solved by that jet. All\nthe additional operations you're talking about only need to happen in a\ngeneral bitcoin script evaluator. But a jet evaluator can be hand optimized\nfor that jet, which could operate in exactly the the function-like way you\nsuggested, because it would actually be a function, under the hood.\n\n> this helps jets compose more easily; if we want a SCRIPT that\nincorporates an existing jet, we do not have to manipulate the stack in a\nway that the existing jet expects, we just load the proper data into the\nconstants table.\n\nI think I see what you're saying about multiple jets composing together\neasily. I think your idea about loading constants from their initial\npositions has merit - basically function arguments that you can reference\nby position rather than needing to arrange them in the right order on the\nstack. Such is the existence of a stack-based language. I like the idea of\neg `c1` to `c26` paralleling the pushdata opcodes. The question I have is:\nwhere would the constants table come from? Would it reference the original\npositions of items on the witness stack?\n\n\n\n\n\n\n\nOn Thu, Mar 10, 2022 at 12:44 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Billy,\n>\n> > Hi ZmnSCPxj,\n> >\n> > >  Just ask a bunch of fullnodes to add this 1Mb of extra ignored data\n> in this tiny 1-input-1-output transaction so I pay only a small fee\n> >\n> > I'm not suggesting that you wouldn't have to pay a fee for it. You'd pay\n> a fee for it as normal, so there's no DOS vector. Doesn't adding\n> extra witness data do what would be needed here? Eg simply adding extra\n> data onto the witness script that will remain unconsumed after successful\n> execution of the script?\n>\n> I think we would want to have a cleanstack rule at some point (do not\n> remember out-of-hand if Taproot already enforces one).\n>\n> So now being nice to the network is *more* costly?\n> That just *dis*incentivizes jet usage.\n>\n> > > how do new jets get introduced?\n> >\n> > In scenario A, new jets get introduced by being added to bitcoin\n> software as basically relay rules.\n> >\n> > > If a new jet requires coordinated deployment over the network, then\n> you might as well just softfork and be done with it.\n> >\n> > It would not need a coordinated deployment. However, the more nodes that\n> supported that jet, the more efficient using it would be for the network.\n> >\n> > > If a new jet can just be entered into some configuration file, how do\n> you coordinate those between multiple users so that there *is* some benefit\n> for relay?\n> >\n> > When a new version of bitcoin comes out, people generally upgrade to it\n> eventually. No coordination is needed. 100% of the network need not support\n> a jet. Just some critical mass to get some benefit.\n>\n> How large is the critical mass needed?\n>\n> If you use witness to transport jet information across non-upgraded nodes,\n> then that disincentivizes use of jets and you can only incentivize jets by\n> softfork, so you might as well just get a softfork.\n>\n> If you have no way to transport jet information from an upgraded through a\n> non-upgraded back to an upgraded node, then I think you need a fairly large\n> buy-in from users before non-upgraded nodes are rare enough that relay is\n> not much affected, and if the required buy-in is large enough, you might as\n> well softfork.\n>\n> > > Having a static lookup table is better since you can pattern-match on\n> strings of specific, static length\n> >\n> > Sorry, better than what exactly?\n>\n> Than using a dynamic lookup table, which is how I understood your previous\n> email about \"scripts in the 1000 past blocks\".\n>\n> > > How does the unupgraded-to-upgraded boundary work?\n> > <snip>\n> > When the non-jet aware node sends this to a jet-aware node, that node\n> would see the extra items on the stack after script execution, and would\n> interpret them as an OP_JET call specifying that OP_JET should replace the\n> witness items starting at index 0 with `1b5f03cf  OP_JET`. It does this and\n> then sends that along to the next hop.\n>\n> It would have to validate as well that the SCRIPT sub-section matches the\n> jet, else I could pretend to be a non-jet-aware node and give you a SCRIPT\n> sub-section that does not match the jet and would cause your validation to\n> diverge from other nodes.\n>\n> Adler32 seems a bit short though, it seems to me that it may lead to two\n> different SCRIPT subsections hashing to the same hash.\n>\n> Suppose I have two different node softwares.\n> One uses a particular interpretation for a particular Adler32 hash.\n> The other uses a different interpretation.\n> If we are not careful, if these two jet-aware software talk to each other,\n> they will ban each other from the network and cause a chainsplit.\n> Since the Bitcoin software is open source, nothing prevents anyone from\n> using a different SCRIPT subsection for a particular Adler32 hash if they\n> find a collision and can somehow convince people to run their modified\n> software.\n>\n> > In order to support this without a soft fork, this extra otherwise\n> unnecessary data would be needed, but for jets that represent long scripts,\n> the extra witness data could be well worth it (for the network).\n> >\n> > However, this extra data would be a disincentive to do transactions this\n> way, even when its better for the network. So it might not be worth doing\n> it this way without a soft fork. But with a soft fork to upgrade nodes to\n> support an OP_JET opcode, the extra witness data can be removed (replaced\n> with out-of-band script fragment transmission for nodes that don't support\n> a particular jet).\n>\n> Which is why I pointed out that each individual jet may very well require\n> a softfork, or enough buy-in that you might as well just softfork.\n>\n> > One interesting additional thing that could be done with this mechanism\n> is to add higher-order function ability to jets, which could allow nodes to\n> add OP_FOLD or similar functions as a jet without requiring additional soft\n> forks.  Hypothetically, you could imagine a jet script that uses an OP_LOOP\n> jet be written as follows:\n> >\n> > 5             # Loop 5 times\n> > 1             # Loop the next 1 operation\n> > 3c1g14ad\n> > OP_JET\n> > OP_ADD  # The 1 operation to loop\n> >\n> > The above would sum up 5 numbers from the stack. And while this\n> summation jet can't be represented in bitcoin script on its own (since\n> bitcoin script can't manipulate opcode calls), the jet *call* can still be\n> represented as:\n> >\n> > OP_ADD\n> > OP_ADD\n> > OP_ADD\n> > OP_ADD\n> > OP_ADD\n> >\n> > which means all of the above replacement functionality would work just\n> as well.\n> >\n> > So my point here is that jets implemented in a way similar to this would\n> give a much wider range of \"code as compression\" possibilities than\n> implementing a single opcode like op_fold.\n>\n> Yes, that is certainly the case, and nothing really prevents us bringing\n> \"programming as compression\" to its logical conclusion.\n>\n> > > To make jets more useful, we should redesign the language so that\n> `OP_PUSH` is not in the opcode stream, but instead, we have a separate\n> table of constants that is attached / concatenated to the actual SCRIPT.\n> >\n> > This can already be done, right? You just have to redesign the script to\n> consume and swap/rot around the data in the right way to separate them out\n> from the main script body.\n>\n> Yes, but that implies additional operations (and execution overhead),\n> increasing the costs to use jets, which makes it even less palatable to use\n> jets, *in addition to* the witness hack disincentivizing jets.\n>\n> So I would suggest that, if we were to seriously pursue jets, we should\n> really replace most of the `OP_PUSH` opcodes with variants that look up in\n> a static table at the start, before the executable script body.\n> I.e. opcodes 0x01 to 0x4e instead mean \"push contents of `c1` to `c78`\n> from the constants table\", and have aliases `a` through `z` for `c1` to\n> `c26`, etc.\n> That way, replacing the `OP_PUSH` is shorter in the actual SCRIPT (instead\n> of a bunch of stack manipulations) and hopefully the overhead of the\n> constants table can be kept low.\n>\n> In particular, this helps jets compose more easily; if we want a SCRIPT\n> that incorporates an existing jet, we do not have to manipulate the stack\n> in a way that the existing jet expects, we just load the proper data into\n> the constants table.\n>\n> Or something, anyway.\n> This seems a fair amount of complexity here.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220311/d0fc1fb3/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-16T15:38:57",
                "message_text_only": "Good morning Billy,\n\n> > I think we would want to have a cleanstack rule at some point\n>\n> Ah is this a rule where a script shouldn't validate if more than just a true is left on the stack? I can see how that would prevent the non-soft-fork version of what I'm proposing.\u00a0\n\nYes.\nThere was also an even stronger cleanstack rule where the stack and alt stack are totally empty.\nThis is because a SCRIPT really just returns \"valid\" or \"invalid\", and `OP_VERIFY` can be trivially appended to a SCRIPT that leaves a single stack item to convert to a SCRIPT that leaves no stack items and retains the same behavior.\n\n>\n> > How large is the critical mass needed?\n>\n> Well it seems we've agreed that were we going to do this, we would want to at least do a soft-fork to make known jet scripts lighter weight (and unknown jet scripts not-heavier) than their\u00a0non-jet counterparts. So given a situation where this soft fork happens, and someone wants to implement a new jet, how much critical mass would be needed for the network to get some benefit from the jet? Well, the absolute minimum for some benefit to happen is that two nodes that support that jet are connected. In such a case, one node can send that jet scripted transaction along without sending the data of what the jet stands for. The jet itself is pretty small, like 2 or so bytes. So that does impose a small additional cost on nodes that don't support a jet. For 100,000 nodes, that means 200,000 bytes of transmission would need to be saved for a jet to break even. So if the jet stands for a 22 byte script, it would break even when 10% of the network supported it. If the jet stood for a 102 byte script, it would break even when 2% of the network supported it. So how much critical mass is necessary for it to be worth it depends on what the script is.\u00a0\n\nThe math seems reasonable.\n\n\n> The question I have is: where would the constants table come from? Would it reference the original positions of items on the witness stack?\u00a0\n\nThe constants table would be part of the SCRIPT puzzle, and thus not in the witness solution.\nI imagine the SCRIPT would be divided into two parts: (1) a table of constants and (2) the actual opcodes to execute.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-16T15:59:00",
                "message_text_only": ">  The constants table would be part of the SCRIPT puzzle\n\nAh I see what you're saying now. You're not talking about referencing\ninputs from the spender, but rather constants for the script writer to\nparameterize a jet with. TBH I think both would be useful, and both could\npotentially be done in the same way (ie reference their position in the\nscript before any evaluation starts). I think your idea is a good one.\n\nCheers\n\nOn Wed, Mar 16, 2022 at 10:39 AM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Billy,\n>\n> > > I think we would want to have a cleanstack rule at some point\n> >\n> > Ah is this a rule where a script shouldn't validate if more than just a\n> true is left on the stack? I can see how that would prevent the\n> non-soft-fork version of what I'm proposing.\n>\n> Yes.\n> There was also an even stronger cleanstack rule where the stack and alt\n> stack are totally empty.\n> This is because a SCRIPT really just returns \"valid\" or \"invalid\", and\n> `OP_VERIFY` can be trivially appended to a SCRIPT that leaves a single\n> stack item to convert to a SCRIPT that leaves no stack items and retains\n> the same behavior.\n>\n> >\n> > > How large is the critical mass needed?\n> >\n> > Well it seems we've agreed that were we going to do this, we would want\n> to at least do a soft-fork to make known jet scripts lighter weight (and\n> unknown jet scripts not-heavier) than their non-jet counterparts. So given\n> a situation where this soft fork happens, and someone wants to implement a\n> new jet, how much critical mass would be needed for the network to get some\n> benefit from the jet? Well, the absolute minimum for some benefit to happen\n> is that two nodes that support that jet are connected. In such a case, one\n> node can send that jet scripted transaction along without sending the data\n> of what the jet stands for. The jet itself is pretty small, like 2 or so\n> bytes. So that does impose a small additional cost on nodes that don't\n> support a jet. For 100,000 nodes, that means 200,000 bytes of transmission\n> would need to be saved for a jet to break even. So if the jet stands for a\n> 22 byte script, it would break even when 10% of the network supported it.\n> If the jet stood for a 102 byte script, it would break even when 2% of the\n> network supported it. So how much critical mass is necessary for it to be\n> worth it depends on what the script is.\n>\n> The math seems reasonable.\n>\n>\n> > The question I have is: where would the constants table come from? Would\n> it reference the original positions of items on the witness stack?\n>\n> The constants table would be part of the SCRIPT puzzle, and thus not in\n> the witness solution.\n> I imagine the SCRIPT would be divided into two parts: (1) a table of\n> constants and (2) the actual opcodes to execute.\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220316/5f4b78e0/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Jets (Was: `OP_FOLD`: A Looping Construct For Bitcoin SCRIPT)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Billy Tetrud"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 47652
        }
    },
    {
        "title": "[bitcoin-dev] CTV Meeting #5 Agenda (Tuesday, March 7th, 12:00 PT)",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-08T02:50:39",
                "message_text_only": "Hi all,\n\nThere will be a CTV meeting tomorrow at noon PT. Agenda below:\n\n1) Sapio Taproot Support Update / Request for Review (20 Minutes)\n    - Experimental support for Taproot merged on master\nhttps://github.com/sapio-lang/sapio\n2) Transaction Sponsoring v.s CPFP/RBF (20 Minutes)\n    -\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n    -\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n3) Jamesob's Non-Recursive Vaults Post (20 minutes)\n    -\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020067.html\n4) What the heck is everyone talking about on the mailing list all of the\nsudden (30 minutes)\n    - EVICT, TLUV, FOLD, Lisp, OP_ANNEX, Drivechain Covenants, Jets, Etc\n5) Q&A (30 mins)\n\nBest,\n\nJeremy\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220307/b455f5e5/attachment.html>"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-08T03:32:13",
                "message_text_only": "* Tuesday, March 8th.\n\nI think Noon PT == 8pm UTC?\n\nbut dont trust me i cant even tell what day is what.\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\n\nOn Mon, Mar 7, 2022 at 6:50 PM Jeremy Rubin <jeremy.l.rubin at gmail.com>\nwrote:\n\n> Hi all,\n>\n> There will be a CTV meeting tomorrow at noon PT. Agenda below:\n>\n> 1) Sapio Taproot Support Update / Request for Review (20 Minutes)\n>     - Experimental support for Taproot merged on master\n> https://github.com/sapio-lang/sapio\n> 2) Transaction Sponsoring v.s CPFP/RBF (20 Minutes)\n>     -\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n>     -\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n> 3) Jamesob's Non-Recursive Vaults Post (20 minutes)\n>     -\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020067.html\n> 4) What the heck is everyone talking about on the mailing list all of the\n> sudden (30 minutes)\n>     - EVICT, TLUV, FOLD, Lisp, OP_ANNEX, Drivechain Covenants, Jets, Etc\n> 5) Q&A (30 mins)\n>\n> Best,\n>\n> Jeremy\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220307/9bf27d21/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-09T11:02:35",
                "message_text_only": "Since this has meetings like taproot, it seems it's going to end up being\nadded in bitcoin core no matter what.\n\nShould we start the conversation on how to resist it when that happens?\nWe should talk more about activation mechanisms and how users should be\nable to actively resist them more.\n\n\nOn Tue, Mar 8, 2022, 03:32 Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> * Tuesday, March 8th.\n>\n> I think Noon PT == 8pm UTC?\n>\n> but dont trust me i cant even tell what day is what.\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n>\n>\n> On Mon, Mar 7, 2022 at 6:50 PM Jeremy Rubin <jeremy.l.rubin at gmail.com>\n> wrote:\n>\n>> Hi all,\n>>\n>> There will be a CTV meeting tomorrow at noon PT. Agenda below:\n>>\n>> 1) Sapio Taproot Support Update / Request for Review (20 Minutes)\n>>     - Experimental support for Taproot merged on master\n>> https://github.com/sapio-lang/sapio\n>> 2) Transaction Sponsoring v.s CPFP/RBF (20 Minutes)\n>>     -\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n>>     -\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n>> 3) Jamesob's Non-Recursive Vaults Post (20 minutes)\n>>     -\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020067.html\n>> 4) What the heck is everyone talking about on the mailing list all of the\n>> sudden (30 minutes)\n>>     - EVICT, TLUV, FOLD, Lisp, OP_ANNEX, Drivechain Covenants, Jets, Etc\n>> 5) Q&A (30 mins)\n>>\n>> Best,\n>>\n>> Jeremy\n>>\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220309/f707b2f4/attachment-0001.html>"
            },
            {
                "author": "Michael Folkson",
                "date": "2022-03-09T14:14:40",
                "message_text_only": "Hi Jorge\n\n> Since this has meetings like taproot, it seems it's going to end up being added in bitcoin core no matter what.\n\nAnyone can set up a IRC channel, anyone can organize a IRC meeting, anyone can announce meetings on the mailing list. Just because an individual is enthusiastic for a soft fork proposal does not imply it has community consensus or that it is likely to be merged into Core in the short term or long term. It is true that other soft fork proposal authors/contributors are not taking the approach Jeremy is taking and are instead working quietly in the background. I prefer the latter approach so soon after Taproot activation but I look forward to hearing about the progress made on other proposals in due course.\n\n> Should we start the conversation on how to resist it when that happens?\nWe should talk more about activation mechanisms and how users should be able to actively resist them more.\n\nI can only speak for myself but if activation was being pursued for a soft fork that didn't have community consensus I would seek to join you in an effort to resist that activation. Taproot (pre-activation discussion) set a strong precedent in terms of community outreach and patiently building community consensus over many years. If that precedent was thrown out I think we are in danger of creating the chaos that most of us would seek to avoid. You are free to start whatever conversation you want but personally until Jeremy or whoever else embarks on an activation attempt I'd rather forget about activation discussions for a while.\n\n> What is ST? If it may be a reason to oppose CTV, why not talk about it more explicitly so that others can understand the criticisms?\n\nST is short for Speedy Trial, the activation mechanism used for Taproot. I have implored people on many occasions now to not mix discussion of a soft fork proposal with discussion of an activation mechanism. Those discussions can happen in parallel but they are entirely independent topics of discussion. Mixing them is misleading at best and manipulative at worst.\n\n> It seems that criticism isn't really that welcomed and is just explained away.\nPerhaps it is just my subjective perception. Sometimes it feels we're going from \"don't trust, verify\" to \"just trust jeremy rubin\", i hope this is really just my subjective perception. Because I think it would be really bad that we started to blindly trust people like that, and specially jeremy.\n\nI think we should generally avoid getting personal on this mailing list. However, although I agree that Jeremy has done some things in the past that have been over-exuberant to put it mildly, as long as he listens to community feedback and doesn't try to force through a contentious soft fork earlier than the community is comfortable with I think his work can add material value to the future soft fork discussion. I entirely agree that we can't get into a situation where any one individual can push through a soft fork without getting community consensus and deep technical review from as many qualified people as possible. That can take a long time (the demands on long term contributors' time are vast) and hence anyone without serious levels of patience should probably exclusively work on sidechains, altcoins etc (or non-consensus changes in Bitcoin) rather than Bitcoin consensus changes.\n\nThanks\nMichael\n\n--\nMichael Folkson\nEmail: michaelfolkson at [protonmail.com](http://protonmail.com/)\nKeybase: michaelfolkson\nPGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n\n------- Original Message -------\nOn Wednesday, March 9th, 2022 at 11:02 AM, Jorge Tim\u00f3n via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Since this has meetings like taproot, it seems it's going to end up being added in bitcoin core no matter what.\n>\n> Should we start the conversation on how to resist it when that happens?\n> We should talk more about activation mechanisms and how users should be able to actively resist them more.\n>\n> On Tue, Mar 8, 2022, 03:32 Jeremy Rubin via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> * Tuesday, March 8th.\n>>\n>> I think Noon PT == 8pm UTC?\n>>\n>> but dont trust me i cant even tell what day is what.\n>> --\n>> [@JeremyRubin](https://twitter.com/JeremyRubin)\n>>\n>> On Mon, Mar 7, 2022 at 6:50 PM Jeremy Rubin <jeremy.l.rubin at gmail.com> wrote:\n>>\n>>> Hi all,\n>>>\n>>> There will be a CTV meeting tomorrow at noon PT. Agenda below:\n>>>\n>>> 1) Sapio Taproot Support Update / Request for Review (20 Minutes)\n>>> - Experimental support for Taproot merged on master https://github.com/sapio-lang/sapio\n>>> 2) Transaction Sponsoring v.s CPFP/RBF (20 Minutes)\n>>> - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n>>> - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n>>> 3) Jamesob's Non-Recursive Vaults Post (20 minutes)\n>>> - https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020067.html\n>>> 4) What the heck is everyone talking about on the mailing list all of the sudden (30 minutes)\n>>> - EVICT, TLUV, FOLD, Lisp, OP_ANNEX, Drivechain Covenants, Jets, Etc\n>>> 5) Q&A (30 mins)\n>>>\n>>> Best,\n>>>\n>>> Jeremy\n>>>\n>>> --\n>>> [@JeremyRubin](https://twitter.com/JeremyRubin)\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220309/750d12a9/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-10T11:41:52",
                "message_text_only": "On Wed, Mar 9, 2022, 14:14 Michael Folkson <michaelfolkson at protonmail.com>\nwrote:\n\n> Hi Jorge\n>\n> > Since this has meetings like taproot, it seems it's going to end up\n> being added in bitcoin core no matter what.\n>\n> Anyone can set up a IRC channel, anyone can organize a IRC meeting, anyone\n> can announce meetings on the mailing list. Just because an individual is\n> enthusiastic for a soft fork proposal does not imply it has community\n> consensus or that it is likely to be merged into Core in the short term or\n> long term. It is true that other soft fork proposal authors/contributors\n> are not taking the approach Jeremy is taking and are instead working\n> quietly in the background. I prefer the latter approach so soon after\n> Taproot activation but I look forward to hearing about the progress made on\n> other proposals in due course.\n>\n\nI hope you're right and not every proposal that gets to have a meeting gets\ndeployed.\n\n> Should we start the conversation on how to resist it when that happens?\n> We should talk more about activation mechanisms and how users should be\n> able to actively resist them more.\n>\n> I can only speak for myself but if activation was being pursued for a soft\n> fork that didn't have community consensus I would seek to join you in an\n> effort to resist that activation. Taproot (pre-activation discussion) set a\n> strong precedent in terms of community outreach and patiently building\n> community consensus over many years. If that precedent was thrown out I\n> think we are in danger of creating the chaos that most of us would seek to\n> avoid. You are free to start whatever conversation you want but personally\n> until Jeremy or whoever else embarks on an activation attempt I'd rather\n> forget about activation discussions for a while.\n>\n\nI strongly disagree taproot set a strong precedent in terms of listening to\ncriticism and looking for consensus. Lots of legitimate criticisms seemed\nto be simply ignored.\nI really think it set a bad preference, even if taproot as deployed is\ngood, which I'm not sure about.\n\n> What is ST? If it may be a reason to oppose CTV, why not talk about it\n> more explicitly so that others can understand the criticisms?\n>\n> ST is short for Speedy Trial, the activation mechanism used for Taproot. I\n> have implored people on many occasions now to not mix discussion of a soft\n> fork proposal with discussion of an activation mechanism. Those discussions\n> can happen in parallel but they are entirely independent topics of\n> discussion. Mixing them is misleading at best and manipulative at worst.\n>\n\nThanks. Yes, those topics were ignored before \"let's focus on the proposal\nfirst\" and afterwards \"let's just deploy this and we can discuss this in\nmore detail for the next proposal\".\nAnd I thonk lots of valid criticism was ignored and disregarded.\n\n\n> It seems that criticism isn't really that welcomed and is just explained\n> away.\n> Perhaps it is just my subjective perception. Sometimes it feels we're\n> going from \"don't trust, verify\" to \"just trust jeremy rubin\", i hope this\n> is really just my subjective perception. Because I think it would be really\n> bad that we started to blindly trust people like that, and specially jeremy.\n>\n> I think we should generally avoid getting personal on this mailing list.\n> However, although I agree that Jeremy has done some things in the past that\n> have been over-exuberant to put it mildly, as long as he listens to\n> community feedback and doesn't try to force through a contentious soft fork\n> earlier than the community is comfortable with I think his work can add\n> material value to the future soft fork discussion. I entirely agree that we\n> can't get into a situation where any one individual can push through a soft\n> fork without getting community consensus and deep technical review from as\n> many qualified people as possible. That can take a long time (the demands\n> on long term contributors' time are vast) and hence anyone without serious\n> levels of patience should probably exclusively work on sidechains, altcoins\n> etc (or non-consensus changes in Bitcoin) rather than Bitcoin consensus\n> changes.\n>\n\nYou're right, we shouldn't get personal. We shouldn't ignore feedback from\nme, mark friedenbach or luke just because of who it comes from.\nI don't think jeremy listens to feedback, judging from taproot activation\ndiscussions, I felt very much ignores by him and others. Luke was usually\nignored. Mark criticisms on taproot, not the activation itself, seemed to\nbe ignored as well. I mean, if somebody refuted his concerns somewhere, I\nmissed it.\nBut even if I believe jremey has malicious intentions and doesn't listen to\nthe community, you're still right, we shouldn't get personal. I shoud\nassume the same malevolent intentions I assume jeremy has from everyone\nelse.\n\nThanks\n> Michael\n>\n> --\n> Michael Folkson\n> Email: michaelfolkson at protonmail.com\n> Keybase: michaelfolkson\n> PGP: 43ED C999 9F85 1D40 EAF4 9835 92D6 0159 214C FEE3\n>\n> ------- Original Message -------\n> On Wednesday, March 9th, 2022 at 11:02 AM, Jorge Tim\u00f3n via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Since this has meetings like taproot, it seems it's going to end up being\n> added in bitcoin core no matter what.\n>\n> Should we start the conversation on how to resist it when that happens?\n> We should talk more about activation mechanisms and how users should be\n> able to actively resist them more.\n>\n>\n> On Tue, Mar 8, 2022, 03:32 Jeremy Rubin via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> * Tuesday, March 8th.\n>>\n>> I think Noon PT == 8pm UTC?\n>>\n>> but dont trust me i cant even tell what day is what.\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>>\n>>\n>> On Mon, Mar 7, 2022 at 6:50 PM Jeremy Rubin <jeremy.l.rubin at gmail.com>\n>> wrote:\n>>\n>>> Hi all,\n>>>\n>>> There will be a CTV meeting tomorrow at noon PT. Agenda below:\n>>>\n>>> 1) Sapio Taproot Support Update / Request for Review (20 Minutes)\n>>> - Experimental support for Taproot merged on master\n>>> https://github.com/sapio-lang/sapio\n>>> 2) Transaction Sponsoring v.s CPFP/RBF (20 Minutes)\n>>> -\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n>>> -\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html\n>>> 3) Jamesob's Non-Recursive Vaults Post (20 minutes)\n>>> -\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020067.html\n>>> 4) What the heck is everyone talking about on the mailing list all of\n>>> the sudden (30 minutes)\n>>> - EVICT, TLUV, FOLD, Lisp, OP_ANNEX, Drivechain Covenants, Jets, Etc\n>>> 5) Q&A (30 mins)\n>>>\n>>> Best,\n>>>\n>>> Jeremy\n>>>\n>>>\n>>> --\n>>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220310/6cf93b3b/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV Meeting #5 Agenda (Tuesday, March 7th, 12:00 PT)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jorge Tim\u00f3n",
                "Jeremy Rubin",
                "Michael Folkson"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 17108
        }
    },
    {
        "title": "[bitcoin-dev] OP_AMOUNT Discussion",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-08T17:09:58",
                "message_text_only": "Hi Devs,\n\nRecently, I've been getting a lot of questions about OP_AMOUNT. It's also\ncome up in the context of \"CTV is unsafe because it can't handle differing\namounts\". Just sharing some preliminary thinking on it:\n\nIt could come in many variants:\n\nOP_PUSHAMOUNT\nOP_AMOUNTVERIFY\nOP_PUSHAMOUNTSPLIT\nOP_SPLITAMOUNTVERIFY\n\nIf we want to do a NOP upgrade, we may prefer the *VERIFY formats. If we\nwant to do a SUCCESSX upgrade, we could do the PUSH format.\n\nThe SplitAmount format is required because amounts are > 5 bytes (51 bits\nneeded max), unless we also do some sort of OP_UPGRADEMATH semantic whereby\npresence of an Amount opcode enables 64 bit (or 256 bit?) math opcodes.\n\nAnd could be applied to the cross product of:\n\nThe Transaction\nAn Input\nAn Output\nThe fees total\nThe fees this input - this output\nThis Input\n\"This\" Output\n\nA lot of choices! The simplest version of it just being just this input,\nand no other (all that is required for many useful cases, e.g. single sig\nif less than 1 btc).\n\n\nA while back I designed some logic for a split amount verifying opcode\nhere: (I don't love it, so hadn't shared it widely).\nhttps://gist.github.com/JeremyRubin/d9f146475f53673cd03c26ab46492504\n\nThere are some decent use cases for amount checking.\n\nFor instance, one could create a non-recursive covenant that there must be\nan output which exactly matches the sats in the input at the same index.\nThis could be used for colored coins, statechains, TLUV/EVICT based payment\npools, etc.\n\nAnother use case could be to make a static address / descriptor that\ndisables low security spends if more coins are the input.\n\nYet another could be to enable pay-what-you-want options, where depending\non how much gets paid into an address different behaviors are permitted.\n\nLastly, as noted in BIP-119, you can make a belt-and-suspenders value check\nin CTV contracts to enable a backup withdrawal should you send the wrong\namount to a vault.\n\nOverall, I think the most straightforward path would be to work on this\nonly for tapscript, no legacy, and then spec out upgraded math operations,\nand then OP_PUSHAMOUNT is pretty straightforward & low technical risk.\nUnfortunately, the upgraded math and exact semantics are highly\nbikesheddable... If anyone is interested in working on this, I'd be happy\nto review/advise on it. Otherwise, I would likely start working on this\nsometime after I'm spending less effort on CTV.\n\nBlockstream liquid has some work in this regard that may be copyable for\nthe math part, but likely not the amount opcode:\nhttps://github.com/ElementsProject/elements/blob/master/doc/tapscript_opcodes.md\n However, they chose to do only 64 bit arithmetic and I personally think\nthat the community might prefer wider operations, the difficulty being in\nnot incidentally enabling OP_CAT as a size, bitshift, and add fragment (or\nproving that OP_CAT is OK?).\n\nsee also: https://rubin.io/bitcoin/2021/12/05/advent-8/#OP_AMOUNT\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220308/2b76f84a/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "OP_AMOUNT Discussion",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 3182
        }
    },
    {
        "title": "[bitcoin-dev] Meeting Summary & Logs for CTV Meeting #5",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-09T00:36:46",
                "message_text_only": "Logs here: https://gnusha.org/ctv-bip-review/2022-03-08.log\n\nNotes:\n\n1) Sapio Updates\n\nSapio has Experimental Taproot Support now.\nSee logs for how to help.\nRust-bitcoin can also use your help reviewing, e.g.\nhttps://github.com/rust-bitcoin/rust-miniscript/pull/305\nAdding MuSig support for the oracle servers would be really cool, if\nsomeone wants a challenge.\n\n2) Transaction Sponsors\n\nWhat sponsors are vs. RBF/CPFP.\nWhy there's not a BIP # assigned (despite it being written up as a BIP+impl\nin\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html,\nshould only get a number if it seems like people agree).\n\n3) James' Vaults Post\n\nJames' vaults are similar to prior art on recursive CTV vaults (Kanzure's /\nJeremy's), where the number of steps = 1.\nActually ends up being a very good design for many custody purposes, might\nbe a good \"80% of the benefit 20% of the work\" type of thing.\nPeople maybe want different things out of vaults... how customizable must\nit be?\n\n4) Mailing list be poppin'\n\nZmn shared a prepared remark which spurred a nice conversation.\nGeneral sentiment that we should be careful adding crazy amounts of power,\nwith great power comes great responsibility...\nMaybe we shouldn't care though -- don't send to scripts you don't like?\nMath is scary -- you can do all sorts of bizarre stuff with more power\n(e.g., what if you made an EVM inside a bitcoin output).\nThings like OP_EVICT should be bounded by design.\nProblem X: Infrastructure issue for all more flexible covenants:\n   1) generate a transition function you would like\n   2) compile it into a script covenant\n   3) request the transition/txn you want to have happen\n    4) produce a satisifaction of the script covenant for that transaction\n   5) prove the transition function *is* what you wanted/secure\nQuantifying how hard X is for a given proposal is a good idea.\nYou can prototype covenants with federations in Sapio pretty easily... more\npeople should try this!\n\n5) General discuss\nPeople suck at naming things... give things more unique names for protocols!\nJeremy will name something the Hot Tub Coin Machine\nSome discussion on forking, if theres any kind of consensus forming, doing\nthings like\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018833.html\nHow much does a shot-on-goal cost / unforced errors of not making an\nactivating client available precluding being able to activate\nluke-jr: never ST; ST is a reason enough to oppose CTV\njamesob: <javascript> OP_DOTHETHING\n\nbest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220308/1b56b098/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-09T11:08:06",
                "message_text_only": "What is ST? If it may be a reason to oppose CTV, why not talk about it more\nexplicitly so that others can understand the criticisms?\nIt seems that criticism isn't really that welcomed and is just explained\naway.\nPerhaps it is just my subjective perception.\nSometimes it feels we're going from \"don't trust, verify\" to \"just trust\njeremy rubin\", i hope this is really just my subjective perception. Because\nI think it would be really bad that we started to blindly trust people like\nthat, and specially jeremy.\n\n\nOn Wed, Mar 9, 2022, 00:37 Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Logs here: https://gnusha.org/ctv-bip-review/2022-03-08.log\n>\n> Notes:\n>\n> 1) Sapio Updates\n>\n> Sapio has Experimental Taproot Support now.\n> See logs for how to help.\n> Rust-bitcoin can also use your help reviewing, e.g.\n> https://github.com/rust-bitcoin/rust-miniscript/pull/305\n> Adding MuSig support for the oracle servers would be really cool, if\n> someone wants a challenge.\n>\n> 2) Transaction Sponsors\n>\n> What sponsors are vs. RBF/CPFP.\n> Why there's not a BIP # assigned (despite it being written up as a\n> BIP+impl in\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2020-September/018168.html,\n> should only get a number if it seems like people agree).\n>\n> 3) James' Vaults Post\n>\n> James' vaults are similar to prior art on recursive CTV vaults (Kanzure's\n> / Jeremy's), where the number of steps = 1.\n> Actually ends up being a very good design for many custody purposes, might\n> be a good \"80% of the benefit 20% of the work\" type of thing.\n> People maybe want different things out of vaults... how customizable must\n> it be?\n>\n> 4) Mailing list be poppin'\n>\n> Zmn shared a prepared remark which spurred a nice conversation.\n> General sentiment that we should be careful adding crazy amounts of power,\n> with great power comes great responsibility...\n> Maybe we shouldn't care though -- don't send to scripts you don't like?\n> Math is scary -- you can do all sorts of bizarre stuff with more power\n> (e.g., what if you made an EVM inside a bitcoin output).\n> Things like OP_EVICT should be bounded by design.\n> Problem X: Infrastructure issue for all more flexible covenants:\n>    1) generate a transition function you would like\n>    2) compile it into a script covenant\n>    3) request the transition/txn you want to have happen\n>     4) produce a satisifaction of the script covenant for that transaction\n>    5) prove the transition function *is* what you wanted/secure\n> Quantifying how hard X is for a given proposal is a good idea.\n> You can prototype covenants with federations in Sapio pretty easily...\n> more people should try this!\n>\n> 5) General discuss\n> People suck at naming things... give things more unique names for\n> protocols!\n> Jeremy will name something the Hot Tub Coin Machine\n> Some discussion on forking, if theres any kind of consensus forming,\n> doing things like\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-April/018833.html\n> How much does a shot-on-goal cost / unforced errors of not making an\n> activating client available precluding being able to activate\n> luke-jr: never ST; ST is a reason enough to oppose CTV\n> jamesob: <javascript> OP_DOTHETHING\n>\n> best,\n>\n> Jeremy\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220309/aebc98d4/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-09T14:42:36",
                "message_text_only": "Good morning Jorge,\n\n> What is ST? If it may be a reason to oppose CTV, why not talk about it more explicitly so that others can understand the criticisms?\n\nST is Speedy Trial.\nBasically, a short softfork attempt with `lockinontimeout=false` is first done.\nIf this fails, then developers stop and think and decide whether to offer a UASF `lockinontimeout=true` version or not.\n\nJeremy showed a state diagram of Speedy Trial on the IRC, which was complicated enough that I ***joked*** that it would be better to not implement `OP_CTV` and just use One OPCODE To Rule Them All, a.k.a. `OP_RING`.\n\nIf you had actually read the IRC logs you would have understood it, I even explicitly asked \"ST ?=\" so that the IRC logs have it explicitly listed as \"Speedy Trial\".\n\n\n> It seems that criticism isn't really that welcomed and is just explained away.\n\nIt seems that you are trying to grasp at any criticism and thus fell victim to a joke.\n\n> Perhaps it is just my subjective perception.\n> Sometimes it feels we're going from \"don't trust, verify\" to \"just trust jeremy rubin\", i hope this is really just my subjective perception. Because I think it would be really bad that we started to blindly trust people like that, and specially jeremy.\n\nWhy \"specially jeremy\"?\nAny particular information you think is relevant?\n\nThe IRC logs were linked, you know, you could have seen what was discussed.\n\nIn particular, on the other thread you mention:\n\n> We should talk more about activation mechanisms and how users should be able to actively resist them more.\n\nSpeedy Trial means that users with mining hashpower can block the initial Speedy Trial, and the failure to lock in ***should*** cause the developers to stop-and-listen.\nIf the developers fail to stop-and-listen, then a counter-UASF can be written which *rejects* blocks signalling *for* the upgrade, which will chainsplit from a pro-UASF `lockinontimeout=true`, but clients using the initial Speedy Trial code will follow which one has better hashpower.\n\nIf we assume that hashpower follows price, then users who want for / against a particular softfork will be able to resist the Speedy Trial, and if developers release a UASF `lockinontimeout=true` later, will have the choice to reject running the UASF and even running a counter-UASF.\n\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-10T11:28:55",
                "message_text_only": "Thank you for explaining. I agree with luke then, I'm against speedy trial.\nI explained why already, I think.\nIn summary: speedy trial kind of means is miners and not users who decide\nthe rules.\nIt gives users less opportunities to react and oppose a malevolent change\nin case miners want to impose such change on them.\n\n\nWhy specially jeremy?\n\nI personally distrust him more from experience, but that's subjective, and\nkind of offtopic. Sorry, I should try to distrust all the other devs as\nmuch as I distrust him in particular.\n\"Don't trust, verify\", right?\n\n\nOn Wed, Mar 9, 2022, 14:42 ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Jorge,\n>\n> > What is ST? If it may be a reason to oppose CTV, why not talk about it\n> more explicitly so that others can understand the criticisms?\n>\n> ST is Speedy Trial.\n> Basically, a short softfork attempt with `lockinontimeout=false` is first\n> done.\n> If this fails, then developers stop and think and decide whether to offer\n> a UASF `lockinontimeout=true` version or not.\n>\n> Jeremy showed a state diagram of Speedy Trial on the IRC, which was\n> complicated enough that I ***joked*** that it would be better to not\n> implement `OP_CTV` and just use One OPCODE To Rule Them All, a.k.a.\n> `OP_RING`.\n>\n> If you had actually read the IRC logs you would have understood it, I even\n> explicitly asked \"ST ?=\" so that the IRC logs have it explicitly listed as\n> \"Speedy Trial\".\n>\n>\n> > It seems that criticism isn't really that welcomed and is just explained\n> away.\n>\n> It seems that you are trying to grasp at any criticism and thus fell\n> victim to a joke.\n>\n> > Perhaps it is just my subjective perception.\n> > Sometimes it feels we're going from \"don't trust, verify\" to \"just trust\n> jeremy rubin\", i hope this is really just my subjective perception. Because\n> I think it would be really bad that we started to blindly trust people like\n> that, and specially jeremy.\n>\n> Why \"specially jeremy\"?\n> Any particular information you think is relevant?\n>\n> The IRC logs were linked, you know, you could have seen what was discussed.\n>\n> In particular, on the other thread you mention:\n>\n> > We should talk more about activation mechanisms and how users should be\n> able to actively resist them more.\n>\n> Speedy Trial means that users with mining hashpower can block the initial\n> Speedy Trial, and the failure to lock in ***should*** cause the developers\n> to stop-and-listen.\n> If the developers fail to stop-and-listen, then a counter-UASF can be\n> written which *rejects* blocks signalling *for* the upgrade, which will\n> chainsplit from a pro-UASF `lockinontimeout=true`, but clients using the\n> initial Speedy Trial code will follow which one has better hashpower.\n>\n> If we assume that hashpower follows price, then users who want for /\n> against a particular softfork will be able to resist the Speedy Trial, and\n> if developers release a UASF `lockinontimeout=true` later, will have the\n> choice to reject running the UASF and even running a counter-UASF.\n>\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220310/5a3a82c6/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Meeting Summary & Logs for CTV Meeting #5",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Jorge Tim\u00f3n",
                "Jeremy Rubin"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 12002
        }
    },
    {
        "title": "[bitcoin-dev] Improving RBF Policy",
        "thread_messages": [
            {
                "author": "Gloria Zhao",
                "date": "2022-03-09T15:09:55",
                "message_text_only": "Hi RBF friends,\n\nPosting a summary of RBF discussions at coredev (mostly on transaction\nrelay rate-limiting), user-elected descendant limit as a short term\nsolution to unblock package RBF, and mining score, all open for feedback:\n\nOne big concept discussed was baking DoS protection into the p2p level\nrather than policy level. TLDR: The fees are not paid to the node operator,\nbut to the miner. While we can use fees to reason about the cost of an\nattack, if we're ultimately interested in preventing resource exhaustion,\nmaybe we want to \"stop the bleeding\" when it happens and bound the amount\nof resources used in general. There were two main ideas:\n\n1. Transaction relay rate limiting (i.e. the one you proposed above or some\nvariation) with a feerate-based priority queue\n2. Staggered broadcast of replacement transactions: within some time\ninterval, maybe accept multiple replacements for the same prevout, but only\nrelay the original transaction.\n\nLooking to solicit feedback on these ideas and the concept in general. Is\nit a good idea (separate from RBF) to add rate-limiting in transaction\nrelay? And is it the right direction to think about RBF DoS protection this\nway?\n\nA lingering concern that I have about this idea is it would then be\npossible to impact the propagation of another person\u2019s transaction, i.e.,\nan attacker can censor somebody\u2019s transaction from ever being announced by\na node if they send enough transactions to fill up the rate limit.\nObviously this would be expensive since they're spending a lot on fees, but\nI imagine it could be profitable in some situations to spend a few thousand\ndollars to prevent anyone from hearing about a transaction for a few hours.\nThis might be a non-issue in practice if the rate limit is generous and\ntraffic isn\u2019t horrendous, but is this a problem?\n\nAnd if we don't require an increase in (i.e. addition of \"new\") absolute\nfees, users are essentially allowed to \u201crecycle\u201d fees. In the scenario\nwhere we prioritize relay based on feerate, users could potentially be\nplaced higher in the queue, ahead of other users\u2019 transactions, multiple\ntimes, without ever adding more fees to the transaction. Again, maybe this\nisn\u2019t a huge deal in practice if we set the parameters right, but it seems\u2026\nnot great, in principle.\n\n---------\n\nIt's probably also a good idea to point out that there's been some\ndiscussion happening on the gist containing my original post on this thread\n(https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff).\n\nSuhas and Matt [proposed][0] adding a policy rule allowing users to specify\ndescendant limits on their transactions. For example, some nth bit of\nnSequence with nVersion 3 means \"this transaction won't have more than X\nvbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\nsolves the pinning problem with package RBF where the attacker's package\ncontains a very large and high-fee descendant.\n\nWe could add this policy and deploy it with package RBF/package relay so\nthat LN can use it by setting the user-elected descendant limit flag on\ncommitment transactions. (Otherwise package RBF is blocked until we find a\nmore comprehensive solution to the pinning attack).\n\nIt's simple to [implement][1] as a mempool policy, but adds some complexity\nfor wallets that use it, since it limits their use of UTXOs from\ntransactions with this bit set.\n\n---------\n\nAlso, coming back to the idea of \"we can't just use {individual, ancestor}\nfeerate,\" I'm interested in soliciting feedback on adding a \u201cmining score\u201d\ncalculator. I've implemented one [here][2] which takes the transaction in\nquestion, grabs all of the connected mempool transactions (including\nsiblings, coparents, etc., as they wouldn\u2019t be in the ancestor nor\ndescendant sets), and builds a \u201cblock template\u201d using our current mining\nalgorithm. The mining score of a transaction is the ancestor feerate at\nwhich it is included.\n\nThis would be helpful for something like ancestor-aware funding and\nfee-bumping in the wallet: [3], [4]. I think if we did the rate-limited\npriority queue for transaction relay, we'd want to use something like this\nas the priority value. And for RBF, we probably want to require that a\nreplacement have a higher mining score than the original transactions. This\ncould be computationally expensive to do all the time; it could be good to\ncache it but that could make mempool bookkeeping more complicated. Also, if\nwe end up trying to switch to a candidate set-based algorithm for mining,\nwe'd of course need a new calculator.\n\n[0]:\nhttps://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4058140#gistcomment-4058140\n[1]: https://github.com/glozow/bitcoin/tree/2022-02-user-desclimit\n[2] https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n[3]: https://github.com/bitcoin/bitcoin/issues/9645\n[4]: https://github.com/bitcoin/bitcoin/issues/15553\n\nBest,\nGloria\n\nOn Tue, Feb 8, 2022 at 4:58 AM Anthony Towns <aj at erisian.com.au> wrote:\n\n> On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n> > @aj:\n> > > I wonder sometimes if it could be sufficient to just have a relay rate\n> > > limit and prioritise by ancestor feerate though. Maybe something like:\n> > > - instead of adding txs to each peers setInventoryTxToSend immediately,\n> > >   set a mempool flag \"relayed=false\"\n> > > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n> > >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n> > >   calculate how much kB those txs were, and do this again after\n> > >   SIZE/RATELIMIT seconds\n>\n> > > - don't include \"relayed=false\" txs when building blocks?\n>\n> The \"?\" was me not being sure that point is a good suggestion...\n>\n> Miners might reasonably decide to have no rate limit, and always relay,\n> and never exclude txs -- but the question then becomes is whether they\n> hear about the tx at all, so rate limiting behaviour could still be a\n> potential problem for whoever made the tx.\n>\n> > Wow cool! I think outbound tx relay size-based rate-limiting and\n> > prioritizing tx relay by feerate are great ideas for preventing spammers\n> > from wasting bandwidth network-wide. I agree, this would slow the low\n> > feerate spam down, preventing a huge network-wide bandwidth spike. And it\n> > would allow high feerate transactions to propagate as they should,\n> > regardless of how busy traffic is. Combined with inbound tx request\n> > rate-limiting, might this be sufficient to prevent DoS regardless of the\n> > fee-based replacement policies?\n>\n> I think you only want to do outbound rate limits, ie, how often you send\n> INV, GETDATA and TX messages? Once you receive any of those, I think\n> you have to immediately process / ignore it, you can't really sensibly\n> defer it (beyond the existing queues we have that just build up while\n> we're busy processing other things first)?\n>\n> > One point that I'm not 100% clear on: is it ok to prioritize the\n> > transactions by ancestor feerate in this scheme? As I described in the\n> > original post, this can be quite different from the actual feerate we\n> would\n> > consider a transaction in a block for. The transaction could have a high\n> > feerate sibling bumping its ancestor.\n> > For example, A (1sat/vB) has 2 children: B (49sat/vB) and C (5sat/vB). If\n> > we just received C, it would be incorrect to give it a priority equal to\n> > its ancestor feerate (3sat/vB) because if we constructed a block template\n> > now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n> > Then, if we imagine that top N is >5sat/vB, we're not relaying C. If we\n> > also exclude C when building blocks, we're missing out on good fees.\n>\n> I think you're right that this would be ugly. It's something of a\n> special case:\n>\n>  a) you really care about C getting into the next block; but\n>  b) you're trusting B not being replaced by a higher fee tx that\n>     doesn't have A as a parent; and\n>  c) there's a lot of txs bidding the floor of the next block up to a\n>     level in-between the ancestor fee rate of 3sat/vB and the tx fee\n>     rate of 5sat/vB\n>\n> Without (a), maybe you don't care about it getting to a miner quickly.\n> If your trust in (b) was misplaced, then your tx's effective fee rate\n> will drop and (because of (c)), you'll lose anyway. And if the spam ends\n> up outside of (c)'s range, either the rate limiting won't take effect\n> (spam's too cheap) and you'll be fine, or you'll miss out on the block\n> anyway (spam's paying more than your tx rate) and you never had any hope\n> of making it in.\n>\n> Note that we already rate limit via INVENTORY_BROADCAST_MAX /\n> *_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\n> per 10 minutes for outbound connections. This would be a weight based\n> rate limit instead-of/in-addition-to that, I guess.\n>\n> As far as a non-ugly approach goes, I think you'd have to be smarter about\n> tracking the \"effective fee rate\" than the ancestor fee rate manages;\n> maybe that's something that could fall out of Murch and Clara's candidate\n> set blockbuilding ideas [0] ?\n>\n> Perhaps that same work would also make it possible to come up with\n> a better answer to \"do I care that this replacement would invalidate\n> these descendents?\"\n>\n> [0] https://github.com/Xekyo/blockbuilding\n>\n> > > - keep high-feerate evicted txs around for a while in case they get\n> > >   mined by someone else to improve compact block relay, a la the\n> > >   orphan pool?\n> > Replaced transactions are already added to vExtraTxnForCompact :D\n>\n> I guess I was thinking that it's just a 100 tx LRU cache, which might\n> not be good enough?\n>\n> Maybe it would be more on point to have a rate limit apply only to\n> replacement transactions?\n>\n> > For wallets, AJ's \"All you need is for there to be *a* path that follows\n> > the new relay rules and gets from your node/wallet to perhaps 10% of\n> > hashpower\" makes sense to me (which would be the former).\n>\n> Perhaps a corollarly of that is that it's *better* to have the mempool\n> acceptance rule only consider economic incentives, and have the spam\n> prevention only be about \"shall I tell my peers about this?\"\n>\n> If you don't have that split; then the anti-spam rules can prevent you\n> from getting the tx in the mempool at all; whereas if you do have the\n> split, then even if the bitcoind anti-spam rules are blocking you at\n> every turn, you can still send your tx to miners by some other route,\n> and then they can add it to their mempool directly without any hassle.\n>\n> Cheers,\n> aj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220309/097a4111/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-11T16:22:07",
                "message_text_only": "Hi Gloria,\n\n>  1. Transaction relay rate limiting\n\nI have a similar concern as yours, that this could prevent higher fee-rate\ntransactions from being broadcast.\n\n> 2. Staggered broadcast of replacement transactions: within some time\ninterval, maybe accept multiple replacements for the same prevout, but only\nrelay the original transaction.\n\nBy this do you mean basically having a batching window where, on receiving\na replacement transaction, a node will wait for a period of time,\npotentially receiving many replacements for the same transaction (or many\nseparate conflicting transactions), and only broadcasting the \"best\" one(s)\nat the end of that time window?\n\nIts an interesting idea, but it would produce a problem. Every hop that\nreplacement transaction takes would be delayed by this staggered window. If\nthe window were 3 minutes and transactions generally take 20 hops to get to\nthe majority of miners, a \"best-case average\" delay might be 3.75 minutes\n(noting that among your 8 nodes, its quite likely one of them would have a\nwindow ending much sooner than 3 minutes). Some (maybe 3% of) nodes would\nexperience delays of more than 20 minutes. That kind of delay isn't great.\n\nHowever it made me think of another idea: a transaction replacement\nbroadcast cooldown. What if nodes kept track of the time they broadcasted\nthe last replacement for a package and had a relay cooldown after the last\nreplacement was broadcasted? A node receiving a replacement would relay the\nreplacement immediately if the package its replacing was broadcasted more\nthan X seconds ago, and otherwise it would wait until the time when that\npackage was broadcasted at least X seconds ago to broadcast it. Any\nreplacements it receives during that waiting period would replace as\nnormal, meaning the unrebroadcasted replacement would never be\nbroadcasted, and only the highest value replacement would be broadcasted at\nthe end of the cooldown.\n\nThis wouldn't prevent a higher-fee-rate transaction from being broadcasted\n(like rate limiting could), but would still be effective at limiting\nunnecessary data transmission. Another benefit is that in the\nnon-adversarial case, replacement transactions wouldn't be subject to any\ndelay at all (while in the staggered broadcast idea, most replacements\nwould experience some delay). And in the adversarial case, where a\nmalicious actor broadcasts a low-as-possible-value replacement just before\nyours, the worst case delay is just whatever the cooldown period is. I\nwould imagine that maybe 1 minute would be a reasonable worst-case delay.\nThis would limit spam for a transaction that makes it into a block to ~10x\n(9 to 1). I don't see much of a downside to doing this beyond just the\nslight additional complexity of relay rules (and considering it could save\nsubstantial additional code complexity, even that is a benefit).\n\nAll a node would need to do is keep a timestamp on each transaction they\nreceive for when it was broadcasted and check it when a replacement comes\nin. If now-broadcastDate < cooldown, set a timer for cooldown -\n(now-broadcastDate) to broadcast it. If another replacement comes in, clear\nthat timer and repeat using the original broadcast date (since the\nunbroadcast transaction doesn't have a broadcast date yet).\n\nI think it might also be useful to note that eliminating \"extra data\"\ncaused by careless or malicious actors (spam or whatever you want to call\nit) should not be the goal. It is impossible to prevent all spam. What we\nshould be aiming for is more specific: we should attempt to design a system\nwhere spam is manageable. Eg if our goal is to ensure that a bitcoin node\nuses no more than 10% of the bandwidth of a \"normal\" user, if current\nnon-spam traffic only requires 1% of a \"normal\" users's bandwidth, then the\nnetwork can bear a 9 to 1 ratio of spam. When a node spins up, there is a\nlot more data to download and process. So we know that all full nodes can\nhandle at least as much traffic as they handle during IBD. What's the\ndifference between those amounts? I'm not sure, but I would guess that IBD\nis at least a couple times more demanding than a fully synced node. So I\nmight suggest that as long as spam can be kept below a ratio of maybe 2 to\n1, we should consider the design acceptable (and therefore more complexity\nunnecessary).\n\nThe 1 minute broadcast cooldown I mentioned before wouldn't be quite\nsufficient to achieve that ratio. But a 3.33 minute cooldown would be.\nWhether this is \"too much\" is something that would have to be discussed, I\nsuspect a worst-case adversarial 3.33 minute delay would not be \"too much\".\nDoing this could basically eliminate any risk of actual service denial via\nreplacement transactions.\n\nHowever, I do think that these DOS concerns are quite overblown. I wrote up a\ncomment on your rbf-improvements.md\n<https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#gistcomment-4093100>\ndetailing\nmy thought process on that. The summary is that as long as the fee-rate\nrelay rule is maintained, any \"spam\" is actually paid for, either by the\n\"first\" transaction in the spam chain, or by the \"spam\" itself. Even\nwithout something like a minimum RBF relay delay limiting how much spam\ncould be created, the economics of the fee-rate rule already sufficiently\nmitigate the issue of spam.\nOn Wed, Mar 9, 2022 at 9:37 AM Gloria Zhao via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi RBF friends,\n>\n> Posting a summary of RBF discussions at coredev (mostly on transaction\n> relay rate-limiting), user-elected descendant limit as a short term\n> solution to unblock package RBF, and mining score, all open for feedback:\n>\n> One big concept discussed was baking DoS protection into the p2p level\n> rather than policy level. TLDR: The fees are not paid to the node operator,\n> but to the miner. While we can use fees to reason about the cost of an\n> attack, if we're ultimately interested in preventing resource exhaustion,\n> maybe we want to \"stop the bleeding\" when it happens and bound the amount\n> of resources used in general. There were two main ideas:\n>\n> 1. Transaction relay rate limiting (i.e. the one you proposed above or\n> some variation) with a feerate-based priority queue\n> 2. Staggered broadcast of replacement transactions: within some time\n> interval, maybe accept multiple replacements for the same prevout, but only\n> relay the original transaction.\n>\n> Looking to solicit feedback on these ideas and the concept in general. Is\n> it a good idea (separate from RBF) to add rate-limiting in transaction\n> relay? And is it the right direction to think about RBF DoS protection this\n> way?\n>\n> A lingering concern that I have about this idea is it would then be\n> possible to impact the propagation of another person\u2019s transaction, i.e.,\n> an attacker can censor somebody\u2019s transaction from ever being announced by\n> a node if they send enough transactions to fill up the rate limit.\n> Obviously this would be expensive since they're spending a lot on fees, but\n> I imagine it could be profitable in some situations to spend a few thousand\n> dollars to prevent anyone from hearing about a transaction for a few hours.\n> This might be a non-issue in practice if the rate limit is generous and\n> traffic isn\u2019t horrendous, but is this a problem?\n>\n> And if we don't require an increase in (i.e. addition of \"new\") absolute\n> fees, users are essentially allowed to \u201crecycle\u201d fees. In the scenario\n> where we prioritize relay based on feerate, users could potentially be\n> placed higher in the queue, ahead of other users\u2019 transactions, multiple\n> times, without ever adding more fees to the transaction. Again, maybe this\n> isn\u2019t a huge deal in practice if we set the parameters right, but it seems\u2026\n> not great, in principle.\n>\n> ---------\n>\n> It's probably also a good idea to point out that there's been some\n> discussion happening on the gist containing my original post on this thread\n> (https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff).\n>\n> Suhas and Matt [proposed][0] adding a policy rule allowing users to\n> specify descendant limits on their transactions. For example, some nth bit\n> of nSequence with nVersion 3 means \"this transaction won't have more than X\n> vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n> solves the pinning problem with package RBF where the attacker's package\n> contains a very large and high-fee descendant.\n>\n> We could add this policy and deploy it with package RBF/package relay so\n> that LN can use it by setting the user-elected descendant limit flag on\n> commitment transactions. (Otherwise package RBF is blocked until we find a\n> more comprehensive solution to the pinning attack).\n>\n> It's simple to [implement][1] as a mempool policy, but adds some\n> complexity for wallets that use it, since it limits their use of UTXOs from\n> transactions with this bit set.\n>\n> ---------\n>\n> Also, coming back to the idea of \"we can't just use {individual, ancestor}\n> feerate,\" I'm interested in soliciting feedback on adding a \u201cmining score\u201d\n> calculator. I've implemented one [here][2] which takes the transaction in\n> question, grabs all of the connected mempool transactions (including\n> siblings, coparents, etc., as they wouldn\u2019t be in the ancestor nor\n> descendant sets), and builds a \u201cblock template\u201d using our current mining\n> algorithm. The mining score of a transaction is the ancestor feerate at\n> which it is included.\n>\n> This would be helpful for something like ancestor-aware funding and\n> fee-bumping in the wallet: [3], [4]. I think if we did the rate-limited\n> priority queue for transaction relay, we'd want to use something like this\n> as the priority value. And for RBF, we probably want to require that a\n> replacement have a higher mining score than the original transactions. This\n> could be computationally expensive to do all the time; it could be good to\n> cache it but that could make mempool bookkeeping more complicated. Also, if\n> we end up trying to switch to a candidate set-based algorithm for mining,\n> we'd of course need a new calculator.\n>\n> [0]:\n> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4058140#gistcomment-4058140\n> [1]: https://github.com/glozow/bitcoin/tree/2022-02-user-desclimit\n> [2] https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n> [3]: https://github.com/bitcoin/bitcoin/issues/9645\n> [4]: https://github.com/bitcoin/bitcoin/issues/15553\n>\n> Best,\n> Gloria\n>\n> On Tue, Feb 8, 2022 at 4:58 AM Anthony Towns <aj at erisian.com.au> wrote:\n>\n>> On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n>> > @aj:\n>> > > I wonder sometimes if it could be sufficient to just have a relay rate\n>> > > limit and prioritise by ancestor feerate though. Maybe something like:\n>> > > - instead of adding txs to each peers setInventoryTxToSend\n>> immediately,\n>> > >   set a mempool flag \"relayed=false\"\n>> > > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n>> > >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n>> > >   calculate how much kB those txs were, and do this again after\n>> > >   SIZE/RATELIMIT seconds\n>>\n>> > > - don't include \"relayed=false\" txs when building blocks?\n>>\n>> The \"?\" was me not being sure that point is a good suggestion...\n>>\n>> Miners might reasonably decide to have no rate limit, and always relay,\n>> and never exclude txs -- but the question then becomes is whether they\n>> hear about the tx at all, so rate limiting behaviour could still be a\n>> potential problem for whoever made the tx.\n>>\n>> > Wow cool! I think outbound tx relay size-based rate-limiting and\n>> > prioritizing tx relay by feerate are great ideas for preventing spammers\n>> > from wasting bandwidth network-wide. I agree, this would slow the low\n>> > feerate spam down, preventing a huge network-wide bandwidth spike. And\n>> it\n>> > would allow high feerate transactions to propagate as they should,\n>> > regardless of how busy traffic is. Combined with inbound tx request\n>> > rate-limiting, might this be sufficient to prevent DoS regardless of the\n>> > fee-based replacement policies?\n>>\n>> I think you only want to do outbound rate limits, ie, how often you send\n>> INV, GETDATA and TX messages? Once you receive any of those, I think\n>> you have to immediately process / ignore it, you can't really sensibly\n>> defer it (beyond the existing queues we have that just build up while\n>> we're busy processing other things first)?\n>>\n>> > One point that I'm not 100% clear on: is it ok to prioritize the\n>> > transactions by ancestor feerate in this scheme? As I described in the\n>> > original post, this can be quite different from the actual feerate we\n>> would\n>> > consider a transaction in a block for. The transaction could have a high\n>> > feerate sibling bumping its ancestor.\n>> > For example, A (1sat/vB) has 2 children: B (49sat/vB) and C (5sat/vB).\n>> If\n>> > we just received C, it would be incorrect to give it a priority equal to\n>> > its ancestor feerate (3sat/vB) because if we constructed a block\n>> template\n>> > now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n>> > Then, if we imagine that top N is >5sat/vB, we're not relaying C. If we\n>> > also exclude C when building blocks, we're missing out on good fees.\n>>\n>> I think you're right that this would be ugly. It's something of a\n>> special case:\n>>\n>>  a) you really care about C getting into the next block; but\n>>  b) you're trusting B not being replaced by a higher fee tx that\n>>     doesn't have A as a parent; and\n>>  c) there's a lot of txs bidding the floor of the next block up to a\n>>     level in-between the ancestor fee rate of 3sat/vB and the tx fee\n>>     rate of 5sat/vB\n>>\n>> Without (a), maybe you don't care about it getting to a miner quickly.\n>> If your trust in (b) was misplaced, then your tx's effective fee rate\n>> will drop and (because of (c)), you'll lose anyway. And if the spam ends\n>> up outside of (c)'s range, either the rate limiting won't take effect\n>> (spam's too cheap) and you'll be fine, or you'll miss out on the block\n>> anyway (spam's paying more than your tx rate) and you never had any hope\n>> of making it in.\n>>\n>> Note that we already rate limit via INVENTORY_BROADCAST_MAX /\n>> *_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\n>> per 10 minutes for outbound connections. This would be a weight based\n>> rate limit instead-of/in-addition-to that, I guess.\n>>\n>> As far as a non-ugly approach goes, I think you'd have to be smarter about\n>> tracking the \"effective fee rate\" than the ancestor fee rate manages;\n>> maybe that's something that could fall out of Murch and Clara's candidate\n>> set blockbuilding ideas [0] ?\n>>\n>> Perhaps that same work would also make it possible to come up with\n>> a better answer to \"do I care that this replacement would invalidate\n>> these descendents?\"\n>>\n>> [0] https://github.com/Xekyo/blockbuilding\n>>\n>> > > - keep high-feerate evicted txs around for a while in case they get\n>> > >   mined by someone else to improve compact block relay, a la the\n>> > >   orphan pool?\n>> > Replaced transactions are already added to vExtraTxnForCompact :D\n>>\n>> I guess I was thinking that it's just a 100 tx LRU cache, which might\n>> not be good enough?\n>>\n>> Maybe it would be more on point to have a rate limit apply only to\n>> replacement transactions?\n>>\n>> > For wallets, AJ's \"All you need is for there to be *a* path that follows\n>> > the new relay rules and gets from your node/wallet to perhaps 10% of\n>> > hashpower\" makes sense to me (which would be the former).\n>>\n>> Perhaps a corollarly of that is that it's *better* to have the mempool\n>> acceptance rule only consider economic incentives, and have the spam\n>> prevention only be about \"shall I tell my peers about this?\"\n>>\n>> If you don't have that split; then the anti-spam rules can prevent you\n>> from getting the tx in the mempool at all; whereas if you do have the\n>> split, then even if the bitcoind anti-spam rules are blocking you at\n>> every turn, you can still send your tx to miners by some other route,\n>> and then they can add it to their mempool directly without any hassle.\n>>\n>> Cheers,\n>> aj\n>>\n>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220311/694c49aa/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-12T08:18:39",
                "message_text_only": "In reading through more of the discussion, it seems the idea I presented\nabove might basically be a reformulation of t-bast's rate-limiting idea\npresented in this comment\n<https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4081349#gistcomment-4081349>.\nPerhaps he could comment on whether that characterization is accurate.\n\nOn Fri, Mar 11, 2022 at 10:22 AM Billy Tetrud <billy.tetrud at gmail.com>\nwrote:\n\n> Hi Gloria,\n>\n> >  1. Transaction relay rate limiting\n>\n> I have a similar concern as yours, that this could prevent higher fee-rate\n> transactions from being broadcast.\n>\n> > 2. Staggered broadcast of replacement transactions: within some time\n> interval, maybe accept multiple replacements for the same prevout, but only\n> relay the original transaction.\n>\n> By this do you mean basically having a batching window where, on receiving\n> a replacement transaction, a node will wait for a period of time,\n> potentially receiving many replacements for the same transaction (or many\n> separate conflicting transactions), and only broadcasting the \"best\" one(s)\n> at the end of that time window?\n>\n> Its an interesting idea, but it would produce a problem. Every hop that\n> replacement transaction takes would be delayed by this staggered window. If\n> the window were 3 minutes and transactions generally take 20 hops to get to\n> the majority of miners, a \"best-case average\" delay might be 3.75 minutes\n> (noting that among your 8 nodes, its quite likely one of them would have a\n> window ending much sooner than 3 minutes). Some (maybe 3% of) nodes would\n> experience delays of more than 20 minutes. That kind of delay isn't great.\n>\n> However it made me think of another idea: a transaction replacement\n> broadcast cooldown. What if nodes kept track of the time they broadcasted\n> the last replacement for a package and had a relay cooldown after the last\n> replacement was broadcasted? A node receiving a replacement would relay the\n> replacement immediately if the package its replacing was broadcasted more\n> than X seconds ago, and otherwise it would wait until the time when that\n> package was broadcasted at least X seconds ago to broadcast it. Any\n> replacements it receives during that waiting period would replace as\n> normal, meaning the unrebroadcasted replacement would never be\n> broadcasted, and only the highest value replacement would be broadcasted at\n> the end of the cooldown.\n>\n> This wouldn't prevent a higher-fee-rate transaction from being broadcasted\n> (like rate limiting could), but would still be effective at limiting\n> unnecessary data transmission. Another benefit is that in the\n> non-adversarial case, replacement transactions wouldn't be subject to any\n> delay at all (while in the staggered broadcast idea, most replacements\n> would experience some delay). And in the adversarial case, where a\n> malicious actor broadcasts a low-as-possible-value replacement just before\n> yours, the worst case delay is just whatever the cooldown period is. I\n> would imagine that maybe 1 minute would be a reasonable worst-case delay.\n> This would limit spam for a transaction that makes it into a block to ~10x\n> (9 to 1). I don't see much of a downside to doing this beyond just the\n> slight additional complexity of relay rules (and considering it could save\n> substantial additional code complexity, even that is a benefit).\n>\n> All a node would need to do is keep a timestamp on each transaction they\n> receive for when it was broadcasted and check it when a replacement comes\n> in. If now-broadcastDate < cooldown, set a timer for cooldown -\n> (now-broadcastDate) to broadcast it. If another replacement comes in, clear\n> that timer and repeat using the original broadcast date (since the\n> unbroadcast transaction doesn't have a broadcast date yet).\n>\n> I think it might also be useful to note that eliminating \"extra data\"\n> caused by careless or malicious actors (spam or whatever you want to call\n> it) should not be the goal. It is impossible to prevent all spam. What we\n> should be aiming for is more specific: we should attempt to design a system\n> where spam is manageable. Eg if our goal is to ensure that a bitcoin node\n> uses no more than 10% of the bandwidth of a \"normal\" user, if current\n> non-spam traffic only requires 1% of a \"normal\" users's bandwidth, then the\n> network can bear a 9 to 1 ratio of spam. When a node spins up, there is a\n> lot more data to download and process. So we know that all full nodes can\n> handle at least as much traffic as they handle during IBD. What's the\n> difference between those amounts? I'm not sure, but I would guess that IBD\n> is at least a couple times more demanding than a fully synced node. So I\n> might suggest that as long as spam can be kept below a ratio of maybe 2 to\n> 1, we should consider the design acceptable (and therefore more complexity\n> unnecessary).\n>\n> The 1 minute broadcast cooldown I mentioned before wouldn't be quite\n> sufficient to achieve that ratio. But a 3.33 minute cooldown would be.\n> Whether this is \"too much\" is something that would have to be discussed, I\n> suspect a worst-case adversarial 3.33 minute delay would not be \"too much\".\n> Doing this could basically eliminate any risk of actual service denial via\n> replacement transactions.\n>\n> However, I do think that these DOS concerns are quite overblown. I wrote\n> up a comment on your rbf-improvements.md\n> <https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#gistcomment-4093100> detailing\n> my thought process on that. The summary is that as long as the fee-rate\n> relay rule is maintained, any \"spam\" is actually paid for, either by the\n> \"first\" transaction in the spam chain, or by the \"spam\" itself. Even\n> without something like a minimum RBF relay delay limiting how much spam\n> could be created, the economics of the fee-rate rule already sufficiently\n> mitigate the issue of spam.\n> On Wed, Mar 9, 2022 at 9:37 AM Gloria Zhao via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi RBF friends,\n>>\n>> Posting a summary of RBF discussions at coredev (mostly on transaction\n>> relay rate-limiting), user-elected descendant limit as a short term\n>> solution to unblock package RBF, and mining score, all open for feedback:\n>>\n>> One big concept discussed was baking DoS protection into the p2p level\n>> rather than policy level. TLDR: The fees are not paid to the node operator,\n>> but to the miner. While we can use fees to reason about the cost of an\n>> attack, if we're ultimately interested in preventing resource exhaustion,\n>> maybe we want to \"stop the bleeding\" when it happens and bound the amount\n>> of resources used in general. There were two main ideas:\n>>\n>> 1. Transaction relay rate limiting (i.e. the one you proposed above or\n>> some variation) with a feerate-based priority queue\n>> 2. Staggered broadcast of replacement transactions: within some time\n>> interval, maybe accept multiple replacements for the same prevout, but only\n>> relay the original transaction.\n>>\n>> Looking to solicit feedback on these ideas and the concept in general. Is\n>> it a good idea (separate from RBF) to add rate-limiting in transaction\n>> relay? And is it the right direction to think about RBF DoS protection this\n>> way?\n>>\n>> A lingering concern that I have about this idea is it would then be\n>> possible to impact the propagation of another person\u2019s transaction, i.e.,\n>> an attacker can censor somebody\u2019s transaction from ever being announced by\n>> a node if they send enough transactions to fill up the rate limit.\n>> Obviously this would be expensive since they're spending a lot on fees, but\n>> I imagine it could be profitable in some situations to spend a few thousand\n>> dollars to prevent anyone from hearing about a transaction for a few hours.\n>> This might be a non-issue in practice if the rate limit is generous and\n>> traffic isn\u2019t horrendous, but is this a problem?\n>>\n>> And if we don't require an increase in (i.e. addition of \"new\") absolute\n>> fees, users are essentially allowed to \u201crecycle\u201d fees. In the scenario\n>> where we prioritize relay based on feerate, users could potentially be\n>> placed higher in the queue, ahead of other users\u2019 transactions, multiple\n>> times, without ever adding more fees to the transaction. Again, maybe this\n>> isn\u2019t a huge deal in practice if we set the parameters right, but it seems\u2026\n>> not great, in principle.\n>>\n>> ---------\n>>\n>> It's probably also a good idea to point out that there's been some\n>> discussion happening on the gist containing my original post on this thread\n>> (https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff).\n>>\n>> Suhas and Matt [proposed][0] adding a policy rule allowing users to\n>> specify descendant limits on their transactions. For example, some nth bit\n>> of nSequence with nVersion 3 means \"this transaction won't have more than X\n>> vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n>> solves the pinning problem with package RBF where the attacker's package\n>> contains a very large and high-fee descendant.\n>>\n>> We could add this policy and deploy it with package RBF/package relay so\n>> that LN can use it by setting the user-elected descendant limit flag on\n>> commitment transactions. (Otherwise package RBF is blocked until we find a\n>> more comprehensive solution to the pinning attack).\n>>\n>> It's simple to [implement][1] as a mempool policy, but adds some\n>> complexity for wallets that use it, since it limits their use of UTXOs from\n>> transactions with this bit set.\n>>\n>> ---------\n>>\n>> Also, coming back to the idea of \"we can't just use {individual,\n>> ancestor} feerate,\" I'm interested in soliciting feedback on adding a\n>> \u201cmining score\u201d calculator. I've implemented one [here][2] which takes the\n>> transaction in question, grabs all of the connected mempool transactions\n>> (including siblings, coparents, etc., as they wouldn\u2019t be in the ancestor\n>> nor descendant sets), and builds a \u201cblock template\u201d using our current\n>> mining algorithm. The mining score of a transaction is the ancestor feerate\n>> at which it is included.\n>>\n>> This would be helpful for something like ancestor-aware funding and\n>> fee-bumping in the wallet: [3], [4]. I think if we did the rate-limited\n>> priority queue for transaction relay, we'd want to use something like this\n>> as the priority value. And for RBF, we probably want to require that a\n>> replacement have a higher mining score than the original transactions. This\n>> could be computationally expensive to do all the time; it could be good to\n>> cache it but that could make mempool bookkeeping more complicated. Also, if\n>> we end up trying to switch to a candidate set-based algorithm for mining,\n>> we'd of course need a new calculator.\n>>\n>> [0]:\n>> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4058140#gistcomment-4058140\n>> [1]: https://github.com/glozow/bitcoin/tree/2022-02-user-desclimit\n>> [2] https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n>> [3]: https://github.com/bitcoin/bitcoin/issues/9645\n>> [4]: https://github.com/bitcoin/bitcoin/issues/15553\n>>\n>> Best,\n>> Gloria\n>>\n>> On Tue, Feb 8, 2022 at 4:58 AM Anthony Towns <aj at erisian.com.au> wrote:\n>>\n>>> On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n>>> > @aj:\n>>> > > I wonder sometimes if it could be sufficient to just have a relay\n>>> rate\n>>> > > limit and prioritise by ancestor feerate though. Maybe something\n>>> like:\n>>> > > - instead of adding txs to each peers setInventoryTxToSend\n>>> immediately,\n>>> > >   set a mempool flag \"relayed=false\"\n>>> > > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n>>> > >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n>>> > >   calculate how much kB those txs were, and do this again after\n>>> > >   SIZE/RATELIMIT seconds\n>>>\n>>> > > - don't include \"relayed=false\" txs when building blocks?\n>>>\n>>> The \"?\" was me not being sure that point is a good suggestion...\n>>>\n>>> Miners might reasonably decide to have no rate limit, and always relay,\n>>> and never exclude txs -- but the question then becomes is whether they\n>>> hear about the tx at all, so rate limiting behaviour could still be a\n>>> potential problem for whoever made the tx.\n>>>\n>>> > Wow cool! I think outbound tx relay size-based rate-limiting and\n>>> > prioritizing tx relay by feerate are great ideas for preventing\n>>> spammers\n>>> > from wasting bandwidth network-wide. I agree, this would slow the low\n>>> > feerate spam down, preventing a huge network-wide bandwidth spike. And\n>>> it\n>>> > would allow high feerate transactions to propagate as they should,\n>>> > regardless of how busy traffic is. Combined with inbound tx request\n>>> > rate-limiting, might this be sufficient to prevent DoS regardless of\n>>> the\n>>> > fee-based replacement policies?\n>>>\n>>> I think you only want to do outbound rate limits, ie, how often you send\n>>> INV, GETDATA and TX messages? Once you receive any of those, I think\n>>> you have to immediately process / ignore it, you can't really sensibly\n>>> defer it (beyond the existing queues we have that just build up while\n>>> we're busy processing other things first)?\n>>>\n>>> > One point that I'm not 100% clear on: is it ok to prioritize the\n>>> > transactions by ancestor feerate in this scheme? As I described in the\n>>> > original post, this can be quite different from the actual feerate we\n>>> would\n>>> > consider a transaction in a block for. The transaction could have a\n>>> high\n>>> > feerate sibling bumping its ancestor.\n>>> > For example, A (1sat/vB) has 2 children: B (49sat/vB) and C (5sat/vB).\n>>> If\n>>> > we just received C, it would be incorrect to give it a priority equal\n>>> to\n>>> > its ancestor feerate (3sat/vB) because if we constructed a block\n>>> template\n>>> > now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n>>> > Then, if we imagine that top N is >5sat/vB, we're not relaying C. If we\n>>> > also exclude C when building blocks, we're missing out on good fees.\n>>>\n>>> I think you're right that this would be ugly. It's something of a\n>>> special case:\n>>>\n>>>  a) you really care about C getting into the next block; but\n>>>  b) you're trusting B not being replaced by a higher fee tx that\n>>>     doesn't have A as a parent; and\n>>>  c) there's a lot of txs bidding the floor of the next block up to a\n>>>     level in-between the ancestor fee rate of 3sat/vB and the tx fee\n>>>     rate of 5sat/vB\n>>>\n>>> Without (a), maybe you don't care about it getting to a miner quickly.\n>>> If your trust in (b) was misplaced, then your tx's effective fee rate\n>>> will drop and (because of (c)), you'll lose anyway. And if the spam ends\n>>> up outside of (c)'s range, either the rate limiting won't take effect\n>>> (spam's too cheap) and you'll be fine, or you'll miss out on the block\n>>> anyway (spam's paying more than your tx rate) and you never had any hope\n>>> of making it in.\n>>>\n>>> Note that we already rate limit via INVENTORY_BROADCAST_MAX /\n>>> *_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\n>>> per 10 minutes for outbound connections. This would be a weight based\n>>> rate limit instead-of/in-addition-to that, I guess.\n>>>\n>>> As far as a non-ugly approach goes, I think you'd have to be smarter\n>>> about\n>>> tracking the \"effective fee rate\" than the ancestor fee rate manages;\n>>> maybe that's something that could fall out of Murch and Clara's candidate\n>>> set blockbuilding ideas [0] ?\n>>>\n>>> Perhaps that same work would also make it possible to come up with\n>>> a better answer to \"do I care that this replacement would invalidate\n>>> these descendents?\"\n>>>\n>>> [0] https://github.com/Xekyo/blockbuilding\n>>>\n>>> > > - keep high-feerate evicted txs around for a while in case they get\n>>> > >   mined by someone else to improve compact block relay, a la the\n>>> > >   orphan pool?\n>>> > Replaced transactions are already added to vExtraTxnForCompact :D\n>>>\n>>> I guess I was thinking that it's just a 100 tx LRU cache, which might\n>>> not be good enough?\n>>>\n>>> Maybe it would be more on point to have a rate limit apply only to\n>>> replacement transactions?\n>>>\n>>> > For wallets, AJ's \"All you need is for there to be *a* path that\n>>> follows\n>>> > the new relay rules and gets from your node/wallet to perhaps 10% of\n>>> > hashpower\" makes sense to me (which would be the former).\n>>>\n>>> Perhaps a corollarly of that is that it's *better* to have the mempool\n>>> acceptance rule only consider economic incentives, and have the spam\n>>> prevention only be about \"shall I tell my peers about this?\"\n>>>\n>>> If you don't have that split; then the anti-spam rules can prevent you\n>>> from getting the tx in the mempool at all; whereas if you do have the\n>>> split, then even if the bitcoind anti-spam rules are blocking you at\n>>> every turn, you can still send your tx to miners by some other route,\n>>> and then they can add it to their mempool directly without any hassle.\n>>>\n>>> Cheers,\n>>> aj\n>>>\n>>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220312/f3ede5ed/attachment-0001.html>"
            },
            {
                "author": "Gloria Zhao",
                "date": "2022-03-14T10:29:16",
                "message_text_only": "Hi Billy,\n\n> We should expect miners will be using a more complex, more optimal way of\ndetermining what blocks they're working on [...] we should instead run with\nthe assumption that miners keep all potentially relevant transactions in\ntheir mempools, including potentially many conflicting transctions, in\norder to create the most profitable blocks. And therefore we shouldn't put\nthe constraint on normal non-mining full nodes to do that same more-complex\nmempool behavior or add any complexity for the purpose of denying\ntransaction replacements.\n\n> I think a lot of the complexity in these ideas is because of the attempt\nto match relay rules with miner\ninclusion rules.\n\nI think the assumption that miners are using a completely different\nimplementation of mempool and block template building is false. IIUC, most\nminers use Bitcoin Core and perhaps configure their node differently (e.g.\nlarger mempool and different minfeerates), but also use `getblocktemplate`\nwhich means the same ancestor package-based mining algorithm.\n\nOf course, I'm not a miner, so if anybody is a miner or has seen miners'\nsetups, please correct me if I'm wrong.\n\nIn either case, we would want our mining algorithm to result in block\ntemplates that are as close as possible to perfectly incentive compatibile.\n\nFundamentally, I believe default mempool policy (which perhaps naturally\ncreates a network-wide transaction relay policy) should be as close to the\nmining code as possible. Imagine node A only keeps 1 block's worth of\ntransactions, and node B keeps a (default) 300MB mempool. The contents of\nnode A's mempool should be as close as possible to a block template\ngenerated from node B's mempool. Otherwise, node A's mempool is not very\nuseful - their fee estimation is flawed and compact block relay won't do\nthem much good if they need to re-request a lot of block transactions.\nNext, imagine that node B is a miner. It would be very suboptimal if the\nmining code was ancestor package-based (i.e. supports CPFP), but the\nmempool policy only cared about individual feerate, and evicted low-fee\nparents despite their high-fee children. It's easy to see why this would be\nsuboptimal.\nAttempting to match mempool policy with the mining algorithm is also\narguably the point of package relay. Our mining code uses ancestor packages\nwhich is good, but we only submit transactions one at a time to the\nmempool, so a transaction's high-fee children can't be considered until\nthey are all already in the mempool. Package relay allows us to start\nthinking about ancestor packages immediately when evaluating transactions\nfor submission to the mempool.\n\nThe attempt to match policy with miner inclusion rules is deliberate and\nnecessary.\n\n> I want to echo James O'Beirne's opinion on this that this may be the\nwrong path to go down (a path of more complexity without much gain). He\nsaid: \"Special consideration for \"what should be in the next block\" and/or\nthe caching of block templates seems like an imposing dependency, dragging\nin a bunch of state and infrastructure to a question that should be solely\nlimited to mempool feerate aggregates and the feerate of the particular txn\npackage a wallet is concerned with.\"\n\nIt seems that I under-explained the purpose of building/caching block\ntemplates in my original post, since both you and James have the same\nmisunderstanding. Since RBF's introduction, we have improved to an ancestor\npackage-based mining algorithm. This supports CPFP (incentive compatible)\nand it is now common to see more complex \"families\" of transactions as\nusers fee-bump transactions (market is working, yay). On the other hand, we\nno longer have an accurate way of determining a transaction's \"mining\nscore,\" i.e., the feerate of this transaction's ancestor package when it is\nincluded in a block template using our current mining algorithm.\n\nThis limitation is a big blocker in proposing new fee/feerate RBF rules.\nFor example, if we say \"the transaction needs a better feerate,\" this is\nobviously flawed, since the original transactions may have very\nhigh-feerate children, and the replacement transaction may have low feerate\nparents. So what we really want is \"the transaction needs to be more\nincentive compatible to mine based on our mining algorithm,\" but we have no\nway of getting that information right now.\n\nIn my original post, I [described 4 heuristics to get transaction's \"mining\nscore\"][1] using the current data cached in the mempool (e.g. max ancestor\nfeerate of descendant set), as well as why they don't work. As such, the\nbest way to calculate a transaction's mining score AFAICT is to grab all of\nthe related transactions and build a mini \"block template\" with them. The\n[implementation][2] I sent last week also cuts out some of the fluff, so\nthe pseudocode looks like this:\n\n// Get ALL connected entries (ancestors, descendants, siblings, cousins,\ncoparents, etc.)\nvector<TxMempoolEntry> cluster = mempool.GetAllTransactionsRelatedTo(txid);\nsort(cluster, ancestorfeerate);\n\n// For deducting ancestors when they are included separately\nvector<ModifiedTxMempoolEntry> modified_entries;\n\nwhile (!cluster.empty() and !modified_entries.empty()) {\n    iter = BetterAncestorFeerateOf(cluster, modified_entries);\n    best_ancestor_package = GetAncestorSet(iter);\n    mining_score = Feerate(best_ancestor_package);\n    for (entry : best_ancestor_package) {\n       mining_scores[entry] = mining_score;\n       for (descendant : GetAllDescendants(entry) {\n           modified_entries[descendant].DeductAncestorFromAccounting(entry);\n       }\n    }\n}\n\nPerhaps somebody will come up with a better way to do this, but this is my\nsolution, and I hope I've now sufficiently described why I've made an\neffort to think about this. It's not because I want to make things more\nfancy or complicated, but because we currently have no way of accurately\ndetermining a transaction's mining score. The reason I proposed a\npersistent block template is so we can efficiently query the mining score\nof all transactions in the mempool.\n\n> However, I do think that these DOS concerns are quite overblown. I wrote\nup a comment on your rbf-improvements.md\n<https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#gistcomment-4093100>\ndetailing\nmy thought process on that. The summary is that as long as the fee-rate\nrelay rule is maintained, any \"spam\" is actually paid for, either by the\n\"first\" transaction in the spam chain, or by the \"spam\" itself.\n\nThe DoS concern is not overblown. I recommend you re-read the [current RBF\npolicy][3]; Rule 3 and 4 *are* the feerate relay rules. Removing Rule 3\nmeans allowing recycled fees, so new transactions are not necessarily\n\"paying\" anything.\n\n[1]:\nhttps://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#mining-score-of-a-mempool-transaction\n[2]: https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n[3]:\nhttps://github.com/bitcoin/bitcoin/blob/master/doc/policy/mempool-replacements.md\n\nBest,\nGloria\n\nOn Sat, Mar 12, 2022 at 8:18 AM Billy Tetrud <billy.tetrud at gmail.com> wrote:\n\n> In reading through more of the discussion, it seems the idea I presented\n> above might basically be a reformulation of t-bast's rate-limiting idea\n> presented in this comment\n> <https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4081349#gistcomment-4081349>.\n> Perhaps he could comment on whether that characterization is accurate.\n>\n> On Fri, Mar 11, 2022 at 10:22 AM Billy Tetrud <billy.tetrud at gmail.com>\n> wrote:\n>\n>> Hi Gloria,\n>>\n>> >  1. Transaction relay rate limiting\n>>\n>> I have a similar concern as yours, that this could prevent higher\n>> fee-rate transactions from being broadcast.\n>>\n>> > 2. Staggered broadcast of replacement transactions: within some time\n>> interval, maybe accept multiple replacements for the same prevout, but only\n>> relay the original transaction.\n>>\n>> By this do you mean basically having a batching window where, on\n>> receiving a replacement transaction, a node will wait for a period of time,\n>> potentially receiving many replacements for the same transaction (or many\n>> separate conflicting transactions), and only broadcasting the \"best\" one(s)\n>> at the end of that time window?\n>>\n>> Its an interesting idea, but it would produce a problem. Every hop that\n>> replacement transaction takes would be delayed by this staggered window. If\n>> the window were 3 minutes and transactions generally take 20 hops to get to\n>> the majority of miners, a \"best-case average\" delay might be 3.75 minutes\n>> (noting that among your 8 nodes, its quite likely one of them would have a\n>> window ending much sooner than 3 minutes). Some (maybe 3% of) nodes would\n>> experience delays of more than 20 minutes. That kind of delay isn't great.\n>>\n>> However it made me think of another idea: a transaction replacement\n>> broadcast cooldown. What if nodes kept track of the time they broadcasted\n>> the last replacement for a package and had a relay cooldown after the last\n>> replacement was broadcasted? A node receiving a replacement would relay the\n>> replacement immediately if the package its replacing was broadcasted more\n>> than X seconds ago, and otherwise it would wait until the time when that\n>> package was broadcasted at least X seconds ago to broadcast it. Any\n>> replacements it receives during that waiting period would replace as\n>> normal, meaning the unrebroadcasted replacement would never be\n>> broadcasted, and only the highest value replacement would be broadcasted at\n>> the end of the cooldown.\n>>\n>> This wouldn't prevent a higher-fee-rate transaction from being\n>> broadcasted (like rate limiting could), but would still be effective at\n>> limiting unnecessary data transmission. Another benefit is that in the\n>> non-adversarial case, replacement transactions wouldn't be subject to any\n>> delay at all (while in the staggered broadcast idea, most replacements\n>> would experience some delay). And in the adversarial case, where a\n>> malicious actor broadcasts a low-as-possible-value replacement just before\n>> yours, the worst case delay is just whatever the cooldown period is. I\n>> would imagine that maybe 1 minute would be a reasonable worst-case delay.\n>> This would limit spam for a transaction that makes it into a block to ~10x\n>> (9 to 1). I don't see much of a downside to doing this beyond just the\n>> slight additional complexity of relay rules (and considering it could save\n>> substantial additional code complexity, even that is a benefit).\n>>\n>> All a node would need to do is keep a timestamp on each transaction they\n>> receive for when it was broadcasted and check it when a replacement comes\n>> in. If now-broadcastDate < cooldown, set a timer for cooldown -\n>> (now-broadcastDate) to broadcast it. If another replacement comes in, clear\n>> that timer and repeat using the original broadcast date (since the\n>> unbroadcast transaction doesn't have a broadcast date yet).\n>>\n>> I think it might also be useful to note that eliminating \"extra data\"\n>> caused by careless or malicious actors (spam or whatever you want to call\n>> it) should not be the goal. It is impossible to prevent all spam. What we\n>> should be aiming for is more specific: we should attempt to design a system\n>> where spam is manageable. Eg if our goal is to ensure that a bitcoin node\n>> uses no more than 10% of the bandwidth of a \"normal\" user, if current\n>> non-spam traffic only requires 1% of a \"normal\" users's bandwidth, then the\n>> network can bear a 9 to 1 ratio of spam. When a node spins up, there is a\n>> lot more data to download and process. So we know that all full nodes can\n>> handle at least as much traffic as they handle during IBD. What's the\n>> difference between those amounts? I'm not sure, but I would guess that IBD\n>> is at least a couple times more demanding than a fully synced node. So I\n>> might suggest that as long as spam can be kept below a ratio of maybe 2 to\n>> 1, we should consider the design acceptable (and therefore more complexity\n>> unnecessary).\n>>\n>> The 1 minute broadcast cooldown I mentioned before wouldn't be quite\n>> sufficient to achieve that ratio. But a 3.33 minute cooldown would be.\n>> Whether this is \"too much\" is something that would have to be discussed, I\n>> suspect a worst-case adversarial 3.33 minute delay would not be \"too much\".\n>> Doing this could basically eliminate any risk of actual service denial via\n>> replacement transactions.\n>>\n>> However, I do think that these DOS concerns are quite overblown. I wrote\n>> up a comment on your rbf-improvements.md\n>> <https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#gistcomment-4093100> detailing\n>> my thought process on that. The summary is that as long as the fee-rate\n>> relay rule is maintained, any \"spam\" is actually paid for, either by the\n>> \"first\" transaction in the spam chain, or by the \"spam\" itself. Even\n>> without something like a minimum RBF relay delay limiting how much spam\n>> could be created, the economics of the fee-rate rule already sufficiently\n>> mitigate the issue of spam.\n>> On Wed, Mar 9, 2022 at 9:37 AM Gloria Zhao via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Hi RBF friends,\n>>>\n>>> Posting a summary of RBF discussions at coredev (mostly on transaction\n>>> relay rate-limiting), user-elected descendant limit as a short term\n>>> solution to unblock package RBF, and mining score, all open for feedback:\n>>>\n>>> One big concept discussed was baking DoS protection into the p2p level\n>>> rather than policy level. TLDR: The fees are not paid to the node operator,\n>>> but to the miner. While we can use fees to reason about the cost of an\n>>> attack, if we're ultimately interested in preventing resource exhaustion,\n>>> maybe we want to \"stop the bleeding\" when it happens and bound the amount\n>>> of resources used in general. There were two main ideas:\n>>>\n>>> 1. Transaction relay rate limiting (i.e. the one you proposed above or\n>>> some variation) with a feerate-based priority queue\n>>> 2. Staggered broadcast of replacement transactions: within some time\n>>> interval, maybe accept multiple replacements for the same prevout, but only\n>>> relay the original transaction.\n>>>\n>>> Looking to solicit feedback on these ideas and the concept in general.\n>>> Is it a good idea (separate from RBF) to add rate-limiting in transaction\n>>> relay? And is it the right direction to think about RBF DoS protection this\n>>> way?\n>>>\n>>> A lingering concern that I have about this idea is it would then be\n>>> possible to impact the propagation of another person\u2019s transaction, i.e.,\n>>> an attacker can censor somebody\u2019s transaction from ever being announced by\n>>> a node if they send enough transactions to fill up the rate limit.\n>>> Obviously this would be expensive since they're spending a lot on fees, but\n>>> I imagine it could be profitable in some situations to spend a few thousand\n>>> dollars to prevent anyone from hearing about a transaction for a few hours.\n>>> This might be a non-issue in practice if the rate limit is generous and\n>>> traffic isn\u2019t horrendous, but is this a problem?\n>>>\n>>> And if we don't require an increase in (i.e. addition of \"new\") absolute\n>>> fees, users are essentially allowed to \u201crecycle\u201d fees. In the scenario\n>>> where we prioritize relay based on feerate, users could potentially be\n>>> placed higher in the queue, ahead of other users\u2019 transactions, multiple\n>>> times, without ever adding more fees to the transaction. Again, maybe this\n>>> isn\u2019t a huge deal in practice if we set the parameters right, but it seems\u2026\n>>> not great, in principle.\n>>>\n>>> ---------\n>>>\n>>> It's probably also a good idea to point out that there's been some\n>>> discussion happening on the gist containing my original post on this thread\n>>> (https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff).\n>>>\n>>> Suhas and Matt [proposed][0] adding a policy rule allowing users to\n>>> specify descendant limits on their transactions. For example, some nth bit\n>>> of nSequence with nVersion 3 means \"this transaction won't have more than X\n>>> vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n>>> solves the pinning problem with package RBF where the attacker's package\n>>> contains a very large and high-fee descendant.\n>>>\n>>> We could add this policy and deploy it with package RBF/package relay so\n>>> that LN can use it by setting the user-elected descendant limit flag on\n>>> commitment transactions. (Otherwise package RBF is blocked until we find a\n>>> more comprehensive solution to the pinning attack).\n>>>\n>>> It's simple to [implement][1] as a mempool policy, but adds some\n>>> complexity for wallets that use it, since it limits their use of UTXOs from\n>>> transactions with this bit set.\n>>>\n>>> ---------\n>>>\n>>> Also, coming back to the idea of \"we can't just use {individual,\n>>> ancestor} feerate,\" I'm interested in soliciting feedback on adding a\n>>> \u201cmining score\u201d calculator. I've implemented one [here][2] which takes the\n>>> transaction in question, grabs all of the connected mempool transactions\n>>> (including siblings, coparents, etc., as they wouldn\u2019t be in the ancestor\n>>> nor descendant sets), and builds a \u201cblock template\u201d using our current\n>>> mining algorithm. The mining score of a transaction is the ancestor feerate\n>>> at which it is included.\n>>>\n>>> This would be helpful for something like ancestor-aware funding and\n>>> fee-bumping in the wallet: [3], [4]. I think if we did the rate-limited\n>>> priority queue for transaction relay, we'd want to use something like this\n>>> as the priority value. And for RBF, we probably want to require that a\n>>> replacement have a higher mining score than the original transactions. This\n>>> could be computationally expensive to do all the time; it could be good to\n>>> cache it but that could make mempool bookkeeping more complicated. Also, if\n>>> we end up trying to switch to a candidate set-based algorithm for mining,\n>>> we'd of course need a new calculator.\n>>>\n>>> [0]:\n>>> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4058140#gistcomment-4058140\n>>> [1]: https://github.com/glozow/bitcoin/tree/2022-02-user-desclimit\n>>> [2] https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n>>> [3]: https://github.com/bitcoin/bitcoin/issues/9645\n>>> [4]: https://github.com/bitcoin/bitcoin/issues/15553\n>>>\n>>> Best,\n>>> Gloria\n>>>\n>>> On Tue, Feb 8, 2022 at 4:58 AM Anthony Towns <aj at erisian.com.au> wrote:\n>>>\n>>>> On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n>>>> > @aj:\n>>>> > > I wonder sometimes if it could be sufficient to just have a relay\n>>>> rate\n>>>> > > limit and prioritise by ancestor feerate though. Maybe something\n>>>> like:\n>>>> > > - instead of adding txs to each peers setInventoryTxToSend\n>>>> immediately,\n>>>> > >   set a mempool flag \"relayed=false\"\n>>>> > > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs\n>>>> to\n>>>> > >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n>>>> > >   calculate how much kB those txs were, and do this again after\n>>>> > >   SIZE/RATELIMIT seconds\n>>>>\n>>>> > > - don't include \"relayed=false\" txs when building blocks?\n>>>>\n>>>> The \"?\" was me not being sure that point is a good suggestion...\n>>>>\n>>>> Miners might reasonably decide to have no rate limit, and always relay,\n>>>> and never exclude txs -- but the question then becomes is whether they\n>>>> hear about the tx at all, so rate limiting behaviour could still be a\n>>>> potential problem for whoever made the tx.\n>>>>\n>>>> > Wow cool! I think outbound tx relay size-based rate-limiting and\n>>>> > prioritizing tx relay by feerate are great ideas for preventing\n>>>> spammers\n>>>> > from wasting bandwidth network-wide. I agree, this would slow the low\n>>>> > feerate spam down, preventing a huge network-wide bandwidth spike.\n>>>> And it\n>>>> > would allow high feerate transactions to propagate as they should,\n>>>> > regardless of how busy traffic is. Combined with inbound tx request\n>>>> > rate-limiting, might this be sufficient to prevent DoS regardless of\n>>>> the\n>>>> > fee-based replacement policies?\n>>>>\n>>>> I think you only want to do outbound rate limits, ie, how often you send\n>>>> INV, GETDATA and TX messages? Once you receive any of those, I think\n>>>> you have to immediately process / ignore it, you can't really sensibly\n>>>> defer it (beyond the existing queues we have that just build up while\n>>>> we're busy processing other things first)?\n>>>>\n>>>> > One point that I'm not 100% clear on: is it ok to prioritize the\n>>>> > transactions by ancestor feerate in this scheme? As I described in the\n>>>> > original post, this can be quite different from the actual feerate we\n>>>> would\n>>>> > consider a transaction in a block for. The transaction could have a\n>>>> high\n>>>> > feerate sibling bumping its ancestor.\n>>>> > For example, A (1sat/vB) has 2 children: B (49sat/vB) and C\n>>>> (5sat/vB). If\n>>>> > we just received C, it would be incorrect to give it a priority equal\n>>>> to\n>>>> > its ancestor feerate (3sat/vB) because if we constructed a block\n>>>> template\n>>>> > now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n>>>> > Then, if we imagine that top N is >5sat/vB, we're not relaying C. If\n>>>> we\n>>>> > also exclude C when building blocks, we're missing out on good fees.\n>>>>\n>>>> I think you're right that this would be ugly. It's something of a\n>>>> special case:\n>>>>\n>>>>  a) you really care about C getting into the next block; but\n>>>>  b) you're trusting B not being replaced by a higher fee tx that\n>>>>     doesn't have A as a parent; and\n>>>>  c) there's a lot of txs bidding the floor of the next block up to a\n>>>>     level in-between the ancestor fee rate of 3sat/vB and the tx fee\n>>>>     rate of 5sat/vB\n>>>>\n>>>> Without (a), maybe you don't care about it getting to a miner quickly.\n>>>> If your trust in (b) was misplaced, then your tx's effective fee rate\n>>>> will drop and (because of (c)), you'll lose anyway. And if the spam ends\n>>>> up outside of (c)'s range, either the rate limiting won't take effect\n>>>> (spam's too cheap) and you'll be fine, or you'll miss out on the block\n>>>> anyway (spam's paying more than your tx rate) and you never had any hope\n>>>> of making it in.\n>>>>\n>>>> Note that we already rate limit via INVENTORY_BROADCAST_MAX /\n>>>> *_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\n>>>> per 10 minutes for outbound connections. This would be a weight based\n>>>> rate limit instead-of/in-addition-to that, I guess.\n>>>>\n>>>> As far as a non-ugly approach goes, I think you'd have to be smarter\n>>>> about\n>>>> tracking the \"effective fee rate\" than the ancestor fee rate manages;\n>>>> maybe that's something that could fall out of Murch and Clara's\n>>>> candidate\n>>>> set blockbuilding ideas [0] ?\n>>>>\n>>>> Perhaps that same work would also make it possible to come up with\n>>>> a better answer to \"do I care that this replacement would invalidate\n>>>> these descendents?\"\n>>>>\n>>>> [0] https://github.com/Xekyo/blockbuilding\n>>>>\n>>>> > > - keep high-feerate evicted txs around for a while in case they get\n>>>> > >   mined by someone else to improve compact block relay, a la the\n>>>> > >   orphan pool?\n>>>> > Replaced transactions are already added to vExtraTxnForCompact :D\n>>>>\n>>>> I guess I was thinking that it's just a 100 tx LRU cache, which might\n>>>> not be good enough?\n>>>>\n>>>> Maybe it would be more on point to have a rate limit apply only to\n>>>> replacement transactions?\n>>>>\n>>>> > For wallets, AJ's \"All you need is for there to be *a* path that\n>>>> follows\n>>>> > the new relay rules and gets from your node/wallet to perhaps 10% of\n>>>> > hashpower\" makes sense to me (which would be the former).\n>>>>\n>>>> Perhaps a corollarly of that is that it's *better* to have the mempool\n>>>> acceptance rule only consider economic incentives, and have the spam\n>>>> prevention only be about \"shall I tell my peers about this?\"\n>>>>\n>>>> If you don't have that split; then the anti-spam rules can prevent you\n>>>> from getting the tx in the mempool at all; whereas if you do have the\n>>>> split, then even if the bitcoind anti-spam rules are blocking you at\n>>>> every turn, you can still send your tx to miners by some other route,\n>>>> and then they can add it to their mempool directly without any hassle.\n>>>>\n>>>> Cheers,\n>>>> aj\n>>>>\n>>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220314/a6e228e5/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-15T01:43:31",
                "message_text_only": "Hi Gloria,\n\nIt seems you're responding to what I wrote on github. Happy to respond, but\nperhaps we should keep it there so the conversation is kept linearly\ntogether?\n\nI'm curious what you think of my thoughts on what you brought up most\nrecently in this thread about rate limiting / staggered window dos\nprotection stuff.\n\nCheers,\nBT\n\n\nOn Mon, Mar 14, 2022 at 5:29 AM Gloria Zhao <gloriajzhao at gmail.com> wrote:\n\n> Hi Billy,\n>\n> > We should expect miners will be using a more complex, more optimal way\n> of determining what blocks they're working on [...] we should instead run\n> with the assumption that miners keep all potentially relevant transactions\n> in their mempools, including potentially many conflicting transctions, in\n> order to create the most profitable blocks. And therefore we shouldn't put\n> the constraint on normal non-mining full nodes to do that same more-complex\n> mempool behavior or add any complexity for the purpose of denying\n> transaction replacements.\n>\n> > I think a lot of the complexity in these ideas is because of the attempt\n> to match relay rules with miner\n> inclusion rules.\n>\n> I think the assumption that miners are using a completely different\n> implementation of mempool and block template building is false. IIUC, most\n> miners use Bitcoin Core and perhaps configure their node differently (e.g.\n> larger mempool and different minfeerates), but also use `getblocktemplate`\n> which means the same ancestor package-based mining algorithm.\n>\n> Of course, I'm not a miner, so if anybody is a miner or has seen miners'\n> setups, please correct me if I'm wrong.\n>\n> In either case, we would want our mining algorithm to result in block\n> templates that are as close as possible to perfectly incentive compatibile.\n>\n> Fundamentally, I believe default mempool policy (which perhaps naturally\n> creates a network-wide transaction relay policy) should be as close to the\n> mining code as possible. Imagine node A only keeps 1 block's worth of\n> transactions, and node B keeps a (default) 300MB mempool. The contents of\n> node A's mempool should be as close as possible to a block template\n> generated from node B's mempool. Otherwise, node A's mempool is not very\n> useful - their fee estimation is flawed and compact block relay won't do\n> them much good if they need to re-request a lot of block transactions.\n> Next, imagine that node B is a miner. It would be very suboptimal if the\n> mining code was ancestor package-based (i.e. supports CPFP), but the\n> mempool policy only cared about individual feerate, and evicted low-fee\n> parents despite their high-fee children. It's easy to see why this would be\n> suboptimal.\n> Attempting to match mempool policy with the mining algorithm is also\n> arguably the point of package relay. Our mining code uses ancestor packages\n> which is good, but we only submit transactions one at a time to the\n> mempool, so a transaction's high-fee children can't be considered until\n> they are all already in the mempool. Package relay allows us to start\n> thinking about ancestor packages immediately when evaluating transactions\n> for submission to the mempool.\n>\n> The attempt to match policy with miner inclusion rules is deliberate and\n> necessary.\n>\n> > I want to echo James O'Beirne's opinion on this that this may be the\n> wrong path to go down (a path of more complexity without much gain). He\n> said: \"Special consideration for \"what should be in the next block\" and/or\n> the caching of block templates seems like an imposing dependency, dragging\n> in a bunch of state and infrastructure to a question that should be solely\n> limited to mempool feerate aggregates and the feerate of the particular txn\n> package a wallet is concerned with.\"\n>\n> It seems that I under-explained the purpose of building/caching block\n> templates in my original post, since both you and James have the same\n> misunderstanding. Since RBF's introduction, we have improved to an ancestor\n> package-based mining algorithm. This supports CPFP (incentive compatible)\n> and it is now common to see more complex \"families\" of transactions as\n> users fee-bump transactions (market is working, yay). On the other hand, we\n> no longer have an accurate way of determining a transaction's \"mining\n> score,\" i.e., the feerate of this transaction's ancestor package when it is\n> included in a block template using our current mining algorithm.\n>\n> This limitation is a big blocker in proposing new fee/feerate RBF rules.\n> For example, if we say \"the transaction needs a better feerate,\" this is\n> obviously flawed, since the original transactions may have very\n> high-feerate children, and the replacement transaction may have low feerate\n> parents. So what we really want is \"the transaction needs to be more\n> incentive compatible to mine based on our mining algorithm,\" but we have no\n> way of getting that information right now.\n>\n> In my original post, I [described 4 heuristics to get transaction's\n> \"mining score\"][1] using the current data cached in the mempool (e.g. max\n> ancestor feerate of descendant set), as well as why they don't work. As\n> such, the best way to calculate a transaction's mining score AFAICT is to\n> grab all of the related transactions and build a mini \"block template\" with\n> them. The [implementation][2] I sent last week also cuts out some of the\n> fluff, so the pseudocode looks like this:\n>\n> // Get ALL connected entries (ancestors, descendants, siblings, cousins,\n> coparents, etc.)\n> vector<TxMempoolEntry> cluster = mempool.GetAllTransactionsRelatedTo(txid);\n> sort(cluster, ancestorfeerate);\n>\n> // For deducting ancestors when they are included separately\n> vector<ModifiedTxMempoolEntry> modified_entries;\n>\n> while (!cluster.empty() and !modified_entries.empty()) {\n>     iter = BetterAncestorFeerateOf(cluster, modified_entries);\n>     best_ancestor_package = GetAncestorSet(iter);\n>     mining_score = Feerate(best_ancestor_package);\n>     for (entry : best_ancestor_package) {\n>        mining_scores[entry] = mining_score;\n>        for (descendant : GetAllDescendants(entry) {\n>\n> modified_entries[descendant].DeductAncestorFromAccounting(entry);\n>        }\n>     }\n> }\n>\n> Perhaps somebody will come up with a better way to do this, but this is my\n> solution, and I hope I've now sufficiently described why I've made an\n> effort to think about this. It's not because I want to make things more\n> fancy or complicated, but because we currently have no way of accurately\n> determining a transaction's mining score. The reason I proposed a\n> persistent block template is so we can efficiently query the mining score\n> of all transactions in the mempool.\n>\n> > However, I do think that these DOS concerns are quite overblown. I wrote\n> up a comment on your rbf-improvements.md\n> <https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#gistcomment-4093100> detailing\n> my thought process on that. The summary is that as long as the fee-rate\n> relay rule is maintained, any \"spam\" is actually paid for, either by the\n> \"first\" transaction in the spam chain, or by the \"spam\" itself.\n>\n> The DoS concern is not overblown. I recommend you re-read the [current RBF\n> policy][3]; Rule 3 and 4 *are* the feerate relay rules. Removing Rule 3\n> means allowing recycled fees, so new transactions are not necessarily\n> \"paying\" anything.\n>\n> [1]:\n> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#mining-score-of-a-mempool-transaction\n> [2]: https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n> [3]:\n> https://github.com/bitcoin/bitcoin/blob/master/doc/policy/mempool-replacements.md\n>\n> Best,\n> Gloria\n>\n> On Sat, Mar 12, 2022 at 8:18 AM Billy Tetrud <billy.tetrud at gmail.com>\n> wrote:\n>\n>> In reading through more of the discussion, it seems the idea I presented\n>> above might basically be a reformulation of t-bast's rate-limiting idea\n>> presented in this comment\n>> <https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4081349#gistcomment-4081349>.\n>> Perhaps he could comment on whether that characterization is accurate.\n>>\n>> On Fri, Mar 11, 2022 at 10:22 AM Billy Tetrud <billy.tetrud at gmail.com>\n>> wrote:\n>>\n>>> Hi Gloria,\n>>>\n>>> >  1. Transaction relay rate limiting\n>>>\n>>> I have a similar concern as yours, that this could prevent higher\n>>> fee-rate transactions from being broadcast.\n>>>\n>>> > 2. Staggered broadcast of replacement transactions: within some time\n>>> interval, maybe accept multiple replacements for the same prevout, but only\n>>> relay the original transaction.\n>>>\n>>> By this do you mean basically having a batching window where, on\n>>> receiving a replacement transaction, a node will wait for a period of time,\n>>> potentially receiving many replacements for the same transaction (or many\n>>> separate conflicting transactions), and only broadcasting the \"best\" one(s)\n>>> at the end of that time window?\n>>>\n>>> Its an interesting idea, but it would produce a problem. Every hop that\n>>> replacement transaction takes would be delayed by this staggered window. If\n>>> the window were 3 minutes and transactions generally take 20 hops to get to\n>>> the majority of miners, a \"best-case average\" delay might be 3.75 minutes\n>>> (noting that among your 8 nodes, its quite likely one of them would have a\n>>> window ending much sooner than 3 minutes). Some (maybe 3% of) nodes would\n>>> experience delays of more than 20 minutes. That kind of delay isn't great.\n>>>\n>>> However it made me think of another idea: a transaction replacement\n>>> broadcast cooldown. What if nodes kept track of the time they broadcasted\n>>> the last replacement for a package and had a relay cooldown after the last\n>>> replacement was broadcasted? A node receiving a replacement would relay the\n>>> replacement immediately if the package its replacing was broadcasted more\n>>> than X seconds ago, and otherwise it would wait until the time when that\n>>> package was broadcasted at least X seconds ago to broadcast it. Any\n>>> replacements it receives during that waiting period would replace as\n>>> normal, meaning the unrebroadcasted replacement would never be\n>>> broadcasted, and only the highest value replacement would be broadcasted at\n>>> the end of the cooldown.\n>>>\n>>> This wouldn't prevent a higher-fee-rate transaction from being\n>>> broadcasted (like rate limiting could), but would still be effective at\n>>> limiting unnecessary data transmission. Another benefit is that in the\n>>> non-adversarial case, replacement transactions wouldn't be subject to any\n>>> delay at all (while in the staggered broadcast idea, most replacements\n>>> would experience some delay). And in the adversarial case, where a\n>>> malicious actor broadcasts a low-as-possible-value replacement just before\n>>> yours, the worst case delay is just whatever the cooldown period is. I\n>>> would imagine that maybe 1 minute would be a reasonable worst-case delay.\n>>> This would limit spam for a transaction that makes it into a block to ~10x\n>>> (9 to 1). I don't see much of a downside to doing this beyond just the\n>>> slight additional complexity of relay rules (and considering it could save\n>>> substantial additional code complexity, even that is a benefit).\n>>>\n>>> All a node would need to do is keep a timestamp on each transaction they\n>>> receive for when it was broadcasted and check it when a replacement comes\n>>> in. If now-broadcastDate < cooldown, set a timer for cooldown -\n>>> (now-broadcastDate) to broadcast it. If another replacement comes in, clear\n>>> that timer and repeat using the original broadcast date (since the\n>>> unbroadcast transaction doesn't have a broadcast date yet).\n>>>\n>>> I think it might also be useful to note that eliminating \"extra data\"\n>>> caused by careless or malicious actors (spam or whatever you want to call\n>>> it) should not be the goal. It is impossible to prevent all spam. What we\n>>> should be aiming for is more specific: we should attempt to design a system\n>>> where spam is manageable. Eg if our goal is to ensure that a bitcoin node\n>>> uses no more than 10% of the bandwidth of a \"normal\" user, if current\n>>> non-spam traffic only requires 1% of a \"normal\" users's bandwidth, then the\n>>> network can bear a 9 to 1 ratio of spam. When a node spins up, there is a\n>>> lot more data to download and process. So we know that all full nodes can\n>>> handle at least as much traffic as they handle during IBD. What's the\n>>> difference between those amounts? I'm not sure, but I would guess that IBD\n>>> is at least a couple times more demanding than a fully synced node. So I\n>>> might suggest that as long as spam can be kept below a ratio of maybe 2 to\n>>> 1, we should consider the design acceptable (and therefore more complexity\n>>> unnecessary).\n>>>\n>>> The 1 minute broadcast cooldown I mentioned before wouldn't be quite\n>>> sufficient to achieve that ratio. But a 3.33 minute cooldown would be.\n>>> Whether this is \"too much\" is something that would have to be discussed, I\n>>> suspect a worst-case adversarial 3.33 minute delay would not be \"too much\".\n>>> Doing this could basically eliminate any risk of actual service denial via\n>>> replacement transactions.\n>>>\n>>> However, I do think that these DOS concerns are quite overblown. I wrote\n>>> up a comment on your rbf-improvements.md\n>>> <https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4093100#gistcomment-4093100> detailing\n>>> my thought process on that. The summary is that as long as the fee-rate\n>>> relay rule is maintained, any \"spam\" is actually paid for, either by the\n>>> \"first\" transaction in the spam chain, or by the \"spam\" itself. Even\n>>> without something like a minimum RBF relay delay limiting how much spam\n>>> could be created, the economics of the fee-rate rule already sufficiently\n>>> mitigate the issue of spam.\n>>> On Wed, Mar 9, 2022 at 9:37 AM Gloria Zhao via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Hi RBF friends,\n>>>>\n>>>> Posting a summary of RBF discussions at coredev (mostly on transaction\n>>>> relay rate-limiting), user-elected descendant limit as a short term\n>>>> solution to unblock package RBF, and mining score, all open for feedback:\n>>>>\n>>>> One big concept discussed was baking DoS protection into the p2p level\n>>>> rather than policy level. TLDR: The fees are not paid to the node operator,\n>>>> but to the miner. While we can use fees to reason about the cost of an\n>>>> attack, if we're ultimately interested in preventing resource exhaustion,\n>>>> maybe we want to \"stop the bleeding\" when it happens and bound the amount\n>>>> of resources used in general. There were two main ideas:\n>>>>\n>>>> 1. Transaction relay rate limiting (i.e. the one you proposed above or\n>>>> some variation) with a feerate-based priority queue\n>>>> 2. Staggered broadcast of replacement transactions: within some time\n>>>> interval, maybe accept multiple replacements for the same prevout, but only\n>>>> relay the original transaction.\n>>>>\n>>>> Looking to solicit feedback on these ideas and the concept in general.\n>>>> Is it a good idea (separate from RBF) to add rate-limiting in transaction\n>>>> relay? And is it the right direction to think about RBF DoS protection this\n>>>> way?\n>>>>\n>>>> A lingering concern that I have about this idea is it would then be\n>>>> possible to impact the propagation of another person\u2019s transaction, i.e.,\n>>>> an attacker can censor somebody\u2019s transaction from ever being announced by\n>>>> a node if they send enough transactions to fill up the rate limit.\n>>>> Obviously this would be expensive since they're spending a lot on fees, but\n>>>> I imagine it could be profitable in some situations to spend a few thousand\n>>>> dollars to prevent anyone from hearing about a transaction for a few hours.\n>>>> This might be a non-issue in practice if the rate limit is generous and\n>>>> traffic isn\u2019t horrendous, but is this a problem?\n>>>>\n>>>> And if we don't require an increase in (i.e. addition of \"new\")\n>>>> absolute fees, users are essentially allowed to \u201crecycle\u201d fees. In the\n>>>> scenario where we prioritize relay based on feerate, users could\n>>>> potentially be placed higher in the queue, ahead of other users\u2019\n>>>> transactions, multiple times, without ever adding more fees to the\n>>>> transaction. Again, maybe this isn\u2019t a huge deal in practice if we set the\n>>>> parameters right, but it seems\u2026 not great, in principle.\n>>>>\n>>>> ---------\n>>>>\n>>>> It's probably also a good idea to point out that there's been some\n>>>> discussion happening on the gist containing my original post on this thread\n>>>> (https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff).\n>>>>\n>>>> Suhas and Matt [proposed][0] adding a policy rule allowing users to\n>>>> specify descendant limits on their transactions. For example, some nth bit\n>>>> of nSequence with nVersion 3 means \"this transaction won't have more than X\n>>>> vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n>>>> solves the pinning problem with package RBF where the attacker's package\n>>>> contains a very large and high-fee descendant.\n>>>>\n>>>> We could add this policy and deploy it with package RBF/package relay\n>>>> so that LN can use it by setting the user-elected descendant limit flag on\n>>>> commitment transactions. (Otherwise package RBF is blocked until we find a\n>>>> more comprehensive solution to the pinning attack).\n>>>>\n>>>> It's simple to [implement][1] as a mempool policy, but adds some\n>>>> complexity for wallets that use it, since it limits their use of UTXOs from\n>>>> transactions with this bit set.\n>>>>\n>>>> ---------\n>>>>\n>>>> Also, coming back to the idea of \"we can't just use {individual,\n>>>> ancestor} feerate,\" I'm interested in soliciting feedback on adding a\n>>>> \u201cmining score\u201d calculator. I've implemented one [here][2] which takes the\n>>>> transaction in question, grabs all of the connected mempool transactions\n>>>> (including siblings, coparents, etc., as they wouldn\u2019t be in the ancestor\n>>>> nor descendant sets), and builds a \u201cblock template\u201d using our current\n>>>> mining algorithm. The mining score of a transaction is the ancestor feerate\n>>>> at which it is included.\n>>>>\n>>>> This would be helpful for something like ancestor-aware funding and\n>>>> fee-bumping in the wallet: [3], [4]. I think if we did the rate-limited\n>>>> priority queue for transaction relay, we'd want to use something like this\n>>>> as the priority value. And for RBF, we probably want to require that a\n>>>> replacement have a higher mining score than the original transactions. This\n>>>> could be computationally expensive to do all the time; it could be good to\n>>>> cache it but that could make mempool bookkeeping more complicated. Also, if\n>>>> we end up trying to switch to a candidate set-based algorithm for mining,\n>>>> we'd of course need a new calculator.\n>>>>\n>>>> [0]:\n>>>> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4058140#gistcomment-4058140\n>>>> [1]: https://github.com/glozow/bitcoin/tree/2022-02-user-desclimit\n>>>> [2] https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n>>>> [3]: https://github.com/bitcoin/bitcoin/issues/9645\n>>>> [4]: https://github.com/bitcoin/bitcoin/issues/15553\n>>>>\n>>>> Best,\n>>>> Gloria\n>>>>\n>>>> On Tue, Feb 8, 2022 at 4:58 AM Anthony Towns <aj at erisian.com.au> wrote:\n>>>>\n>>>>> On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n>>>>> > @aj:\n>>>>> > > I wonder sometimes if it could be sufficient to just have a relay\n>>>>> rate\n>>>>> > > limit and prioritise by ancestor feerate though. Maybe something\n>>>>> like:\n>>>>> > > - instead of adding txs to each peers setInventoryTxToSend\n>>>>> immediately,\n>>>>> > >   set a mempool flag \"relayed=false\"\n>>>>> > > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs\n>>>>> to\n>>>>> > >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n>>>>> > >   calculate how much kB those txs were, and do this again after\n>>>>> > >   SIZE/RATELIMIT seconds\n>>>>>\n>>>>> > > - don't include \"relayed=false\" txs when building blocks?\n>>>>>\n>>>>> The \"?\" was me not being sure that point is a good suggestion...\n>>>>>\n>>>>> Miners might reasonably decide to have no rate limit, and always relay,\n>>>>> and never exclude txs -- but the question then becomes is whether they\n>>>>> hear about the tx at all, so rate limiting behaviour could still be a\n>>>>> potential problem for whoever made the tx.\n>>>>>\n>>>>> > Wow cool! I think outbound tx relay size-based rate-limiting and\n>>>>> > prioritizing tx relay by feerate are great ideas for preventing\n>>>>> spammers\n>>>>> > from wasting bandwidth network-wide. I agree, this would slow the low\n>>>>> > feerate spam down, preventing a huge network-wide bandwidth spike.\n>>>>> And it\n>>>>> > would allow high feerate transactions to propagate as they should,\n>>>>> > regardless of how busy traffic is. Combined with inbound tx request\n>>>>> > rate-limiting, might this be sufficient to prevent DoS regardless of\n>>>>> the\n>>>>> > fee-based replacement policies?\n>>>>>\n>>>>> I think you only want to do outbound rate limits, ie, how often you\n>>>>> send\n>>>>> INV, GETDATA and TX messages? Once you receive any of those, I think\n>>>>> you have to immediately process / ignore it, you can't really sensibly\n>>>>> defer it (beyond the existing queues we have that just build up while\n>>>>> we're busy processing other things first)?\n>>>>>\n>>>>> > One point that I'm not 100% clear on: is it ok to prioritize the\n>>>>> > transactions by ancestor feerate in this scheme? As I described in\n>>>>> the\n>>>>> > original post, this can be quite different from the actual feerate\n>>>>> we would\n>>>>> > consider a transaction in a block for. The transaction could have a\n>>>>> high\n>>>>> > feerate sibling bumping its ancestor.\n>>>>> > For example, A (1sat/vB) has 2 children: B (49sat/vB) and C\n>>>>> (5sat/vB). If\n>>>>> > we just received C, it would be incorrect to give it a priority\n>>>>> equal to\n>>>>> > its ancestor feerate (3sat/vB) because if we constructed a block\n>>>>> template\n>>>>> > now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n>>>>> > Then, if we imagine that top N is >5sat/vB, we're not relaying C. If\n>>>>> we\n>>>>> > also exclude C when building blocks, we're missing out on good fees.\n>>>>>\n>>>>> I think you're right that this would be ugly. It's something of a\n>>>>> special case:\n>>>>>\n>>>>>  a) you really care about C getting into the next block; but\n>>>>>  b) you're trusting B not being replaced by a higher fee tx that\n>>>>>     doesn't have A as a parent; and\n>>>>>  c) there's a lot of txs bidding the floor of the next block up to a\n>>>>>     level in-between the ancestor fee rate of 3sat/vB and the tx fee\n>>>>>     rate of 5sat/vB\n>>>>>\n>>>>> Without (a), maybe you don't care about it getting to a miner quickly.\n>>>>> If your trust in (b) was misplaced, then your tx's effective fee rate\n>>>>> will drop and (because of (c)), you'll lose anyway. And if the spam\n>>>>> ends\n>>>>> up outside of (c)'s range, either the rate limiting won't take effect\n>>>>> (spam's too cheap) and you'll be fine, or you'll miss out on the block\n>>>>> anyway (spam's paying more than your tx rate) and you never had any\n>>>>> hope\n>>>>> of making it in.\n>>>>>\n>>>>> Note that we already rate limit via INVENTORY_BROADCAST_MAX /\n>>>>> *_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\n>>>>> per 10 minutes for outbound connections. This would be a weight based\n>>>>> rate limit instead-of/in-addition-to that, I guess.\n>>>>>\n>>>>> As far as a non-ugly approach goes, I think you'd have to be smarter\n>>>>> about\n>>>>> tracking the \"effective fee rate\" than the ancestor fee rate manages;\n>>>>> maybe that's something that could fall out of Murch and Clara's\n>>>>> candidate\n>>>>> set blockbuilding ideas [0] ?\n>>>>>\n>>>>> Perhaps that same work would also make it possible to come up with\n>>>>> a better answer to \"do I care that this replacement would invalidate\n>>>>> these descendents?\"\n>>>>>\n>>>>> [0] https://github.com/Xekyo/blockbuilding\n>>>>>\n>>>>> > > - keep high-feerate evicted txs around for a while in case they get\n>>>>> > >   mined by someone else to improve compact block relay, a la the\n>>>>> > >   orphan pool?\n>>>>> > Replaced transactions are already added to vExtraTxnForCompact :D\n>>>>>\n>>>>> I guess I was thinking that it's just a 100 tx LRU cache, which might\n>>>>> not be good enough?\n>>>>>\n>>>>> Maybe it would be more on point to have a rate limit apply only to\n>>>>> replacement transactions?\n>>>>>\n>>>>> > For wallets, AJ's \"All you need is for there to be *a* path that\n>>>>> follows\n>>>>> > the new relay rules and gets from your node/wallet to perhaps 10% of\n>>>>> > hashpower\" makes sense to me (which would be the former).\n>>>>>\n>>>>> Perhaps a corollarly of that is that it's *better* to have the mempool\n>>>>> acceptance rule only consider economic incentives, and have the spam\n>>>>> prevention only be about \"shall I tell my peers about this?\"\n>>>>>\n>>>>> If you don't have that split; then the anti-spam rules can prevent you\n>>>>> from getting the tx in the mempool at all; whereas if you do have the\n>>>>> split, then even if the bitcoind anti-spam rules are blocking you at\n>>>>> every turn, you can still send your tx to miners by some other route,\n>>>>> and then they can add it to their mempool directly without any hassle.\n>>>>>\n>>>>> Cheers,\n>>>>> aj\n>>>>>\n>>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220314/a77ad21d/attachment-0001.html>"
            },
            {
                "author": "Antoine Riard",
                "date": "2022-03-17T02:02:30",
                "message_text_only": "Hi Mempoololic Anonymous fellow,\n\n> 2. Staggered broadcast of replacement transactions: within some time\n> interval, maybe accept multiple replacements for the same prevout, but\nonly\n> relay the original transaction.\n\nIf the goal of replacement staggering is to save on bandwidth, I'm not sure\nit's going to be effective if you consider replacement done from a\nshared-utxo. E.g, Alice broadcasts a package to confirm her commitment,\nrelay is staggered until T. At the same time, Bob broadcasts a package to\nconfirm his version of the commitment at a slightly better feerate, relay\nis staggered until T.\n\nAt T, package A gradually floods from Alice's peers and package B does the\nsame from Bob's peers. When there is an intersection. B overrides A and\nstarts to replace package A in the network mempools nearest to Alice. I\nthink those peers won't have bandwidth saving from adopting a replacement\nstaggering strategy.\n\nOr maybe that's something completely different if you have in mind ? I\nthink it's worth more staggering detail to guess if it's robust against all\nthe replacement propagations patterns.\n\nThough if we aim to save on replacement bandwidth I wonder if a \"diff-only\"\nstrategy, assuming some new p2p mechanism, would be more interesting (as\ndiscussed in the recent \"Thoughts on fee bumping thread\").\n\n> A lingering concern that I have about this idea is it would then be\n> possible to impact the propagation of another person\u2019s transaction, i.e.,\n> an attacker can censor somebody\u2019s transaction from ever being announced by\n> a node if they send enough transactions to fill up the rate limit.\n> Obviously this would be expensive since they're spending a lot on fees,\nbut\n> I imagine it could be profitable in some situations to spend a few\nthousand\n> dollars to prevent anyone from hearing about a transaction for a few\nhours.\n> This might be a non-issue in practice if the rate limit is generous and\n> traffic isn\u2019t horrendous, but is this a problem?\n\nI think I share the concern too about an attacker exhausting a node\ntransaction relay ressources to prevent another person's transaction to\npropagate, especially if the transaction targeted is a L2's time-sensitive\none. In that latter context, an attacker would aim to delay the relay of a\ntime-sensitive transaction (e.g a HTLC-success) to the miners, until the\ntimelock expires. The malicious delay period should swallow the go-to-chain\nHTLC deadline (\"the deadline for received HTLCs this node fulfilled\" in\nbolt 2 parlance), in that current example 18 blocks.\n\nLet's say we allocate 10 MB of bandwidth per-block period. Once the 10 MB\nare exhausted, there is no more bandwidth allocated until the next block is\nissued. If the top mempool feerate is 1 sat/vb, such naive design would\nallow an attacker to buy all the p2p network bandwidth period for 0.1 BTC.\nIf an attacker aims to jam a HTLC transaction for the 18 blocks period, the\ncost is of 1,8 BTC. If the attacker is a LN counterparty to a HTLC worth\nmore than 1.8 BTC, the attack sounds economically profitable.\n\nWorst, the p2p network bandwidth is a public resource while a HTLC is a\nprivate, off-chain contract. An attacker could be counterparty to many\nHTLCs, where each HTLC individual value is far inferior to the global p2p\nbandwidth cost but the sum, only known to the attacker, is superior to.\nTherefore, it sounds to me that p2p network bandwidth might be attractive\nif the stealing are batched.\n\nIs the attacker scenario described credible ? Are the numbers sketched out\nrealistic ?\n\nIf yes, I think one design insight for eventual transaction relay rate\nlimiting would be to make them \"dynamic\", and not naively fixed for a\nperiod. By making them dynamic, an attacker would have to compete with the\neffective feerate proposed by the victim transaction. E.g, if the\nHTLC-success feerate is of 10 sat/vb, an attacker would have to propose a\nstream of malicious transaction of more than 10 sat/vb during the whole\nHTLC deadline period for the transaction-relay jamming to be effective.\n\nFurther, the attack might be invisible from the victim standpoint, the\nmalicious flow of feerate competitive transactions can be hard to\ndissociate from an honest one. Thus, you can expect the\nHTLC transaction issuer to only slowly increase the feerate at each block,\nand those moves to be anticipated by the attacker. Even if the transaction\nissuer adopts a scorched-earth approach for the latest blocks of the\ndeadline, the absolute value of the HTLC burnt in fees might still be less\nthan the transaction relay bandwidth exhaustion paid by the attacker\nbecause the attack is batched by the attacker.\n\nI'm not sure if this reasoning is correct. Though if yes, the issue sounds\nreally similar to \"flood&loot\" attack affecting LN previously researched on\n[0]. What worries me more with this \"exhaust&loot\" is that if we introduce\nbounded transaction relay rate limiting, it sounds a cheaper public\nressource to buy than the mempool..\n\n[0] https://arxiv.org/pdf/2006.08513.pdf\n\nAnyway, I would say it's worthy to investigate more transaction relay rate\nlimiting designs and especially carefully weigh the implications for L2s.\nThose ones might have to adjust their fee-bumping and transaction\nrebroadcast strategies in consequence.\n\n> Suhas and Matt [proposed][0] adding a policy rule allowing users to\nspecify\n> descendant limits on their transactions. For example, some nth bit of\n> nSequence with nVersion 3 means \"this transaction won't have more than X\n> vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n> solves the pinning problem with package RBF where the attacker's package\n> contains a very large and high-fee descendant.\n\nHey, what if the pinning transaction has a parent with a junk feerate ?\n\nLet's say you have commitment tx for a HTLC of value 500000 sats, with top\nmempool feerate of 50 sat/vbyte. The commitment tx is pinned by a malicious\ntx of size 1000 vbytes, matching top mempool feerate. This malicious tx has\na second unconfirmed parent (in addition to the commitment) of size\nMAX_STANDARD_TX_WEIGHT offering a 1 sat/vb. I think the pinning transaction\nancestor score would be less than 2 sat/vb and thus considered irrelevant\nfor block template inclusion ? At the same time, as the pinning transaction\nis attached with a top mempool feerate, the honest user wouldn't be able to\nreplace it with a better-feerate proposal ? Unless adopting a\nscorched-earth approach,  although economically I don't think this\nfee-bumping strategy is safe in case of batch-pinning.\n\nIt might be fixable if we make one additional requirement \"The child\ntransaction subject to the user-elected descendant limit must have only one\nunconfirmed parent\" (here the commitment\ntransaction) ? Though I'm not even sure of the robustness of this fix. The\ncommitment transaction itself could be used as a junk parent to downgrade\nthe pinning transaction ancestor score. E.g, using a revoked commitment\ntransaction with `max_accepted_htlcs` on both sides, pre-signed with a\nfeerate of 1 sat/vb. We might restrict the maximum number of pending HTLCs\nnetwork-wise to make the worst commitment transaction size reasonable,\nthough not sure if my LN colleagues are going to like the idea..\n\nIs that reasoning correct and conform to our Ancestor Set Based algorithm\napproach ? Maybe more details are needed.\n\n> Also, coming back to the idea of \"we can't just use {individual, ancestor}\n> feerate,\" I'm interested in soliciting feedback on adding a \u201cmining score\u201d\n> calculator. I've implemented one [here][2] which takes the transaction in\n> question, grabs all of the connected mempool transactions (including\n> siblings, coparents, etc., as they wouldn\u2019t be in the ancestor nor\n> descendant sets), and builds a \u201cblock template\u201d using our current mining\n> algorithm. The mining score of a transaction is the ancestor feerate at\n> which it is included.\n\nI don't have a strong opinion there yet, though if we make this \"block\ntemplate\" construction the default one, I would be really conservative to\navoid malicious child attachment on multi-party transactions downgrading\nthe block inclusion efficiency.\n\nAntoine\n\nLe mer. 9 mars 2022 \u00e0 10:37, Gloria Zhao via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Hi RBF friends,\n>\n> Posting a summary of RBF discussions at coredev (mostly on transaction\n> relay rate-limiting), user-elected descendant limit as a short term\n> solution to unblock package RBF, and mining score, all open for feedback:\n>\n> One big concept discussed was baking DoS protection into the p2p level\n> rather than policy level. TLDR: The fees are not paid to the node operator,\n> but to the miner. While we can use fees to reason about the cost of an\n> attack, if we're ultimately interested in preventing resource exhaustion,\n> maybe we want to \"stop the bleeding\" when it happens and bound the amount\n> of resources used in general. There were two main ideas:\n>\n> 1. Transaction relay rate limiting (i.e. the one you proposed above or\n> some variation) with a feerate-based priority queue\n> 2. Staggered broadcast of replacement transactions: within some time\n> interval, maybe accept multiple replacements for the same prevout, but only\n> relay the original transaction.\n>\n> Looking to solicit feedback on these ideas and the concept in general. Is\n> it a good idea (separate from RBF) to add rate-limiting in transaction\n> relay? And is it the right direction to think about RBF DoS protection this\n> way?\n>\n> A lingering concern that I have about this idea is it would then be\n> possible to impact the propagation of another person\u2019s transaction, i.e.,\n> an attacker can censor somebody\u2019s transaction from ever being announced by\n> a node if they send enough transactions to fill up the rate limit.\n> Obviously this would be expensive since they're spending a lot on fees, but\n> I imagine it could be profitable in some situations to spend a few thousand\n> dollars to prevent anyone from hearing about a transaction for a few hours.\n> This might be a non-issue in practice if the rate limit is generous and\n> traffic isn\u2019t horrendous, but is this a problem?\n>\n> And if we don't require an increase in (i.e. addition of \"new\") absolute\n> fees, users are essentially allowed to \u201crecycle\u201d fees. In the scenario\n> where we prioritize relay based on feerate, users could potentially be\n> placed higher in the queue, ahead of other users\u2019 transactions, multiple\n> times, without ever adding more fees to the transaction. Again, maybe this\n> isn\u2019t a huge deal in practice if we set the parameters right, but it seems\u2026\n> not great, in principle.\n>\n> ---------\n>\n> It's probably also a good idea to point out that there's been some\n> discussion happening on the gist containing my original post on this thread\n> (https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff).\n>\n> Suhas and Matt [proposed][0] adding a policy rule allowing users to\n> specify descendant limits on their transactions. For example, some nth bit\n> of nSequence with nVersion 3 means \"this transaction won't have more than X\n> vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n> solves the pinning problem with package RBF where the attacker's package\n> contains a very large and high-fee descendant.\n>\n> We could add this policy and deploy it with package RBF/package relay so\n> that LN can use it by setting the user-elected descendant limit flag on\n> commitment transactions. (Otherwise package RBF is blocked until we find a\n> more comprehensive solution to the pinning attack).\n>\n> It's simple to [implement][1] as a mempool policy, but adds some\n> complexity for wallets that use it, since it limits their use of UTXOs from\n> transactions with this bit set.\n>\n> ---------\n>\n> Also, coming back to the idea of \"we can't just use {individual, ancestor}\n> feerate,\" I'm interested in soliciting feedback on adding a \u201cmining score\u201d\n> calculator. I've implemented one [here][2] which takes the transaction in\n> question, grabs all of the connected mempool transactions (including\n> siblings, coparents, etc., as they wouldn\u2019t be in the ancestor nor\n> descendant sets), and builds a \u201cblock template\u201d using our current mining\n> algorithm. The mining score of a transaction is the ancestor feerate at\n> which it is included.\n>\n> This would be helpful for something like ancestor-aware funding and\n> fee-bumping in the wallet: [3], [4]. I think if we did the rate-limited\n> priority queue for transaction relay, we'd want to use something like this\n> as the priority value. And for RBF, we probably want to require that a\n> replacement have a higher mining score than the original transactions. This\n> could be computationally expensive to do all the time; it could be good to\n> cache it but that could make mempool bookkeeping more complicated. Also, if\n> we end up trying to switch to a candidate set-based algorithm for mining,\n> we'd of course need a new calculator.\n>\n> [0]:\n> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4058140#gistcomment-4058140\n> [1]: https://github.com/glozow/bitcoin/tree/2022-02-user-desclimit\n> [2] https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n> [3]: https://github.com/bitcoin/bitcoin/issues/9645\n> [4]: https://github.com/bitcoin/bitcoin/issues/15553\n>\n> Best,\n> Gloria\n>\n> On Tue, Feb 8, 2022 at 4:58 AM Anthony Towns <aj at erisian.com.au> wrote:\n>\n>> On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n>> > @aj:\n>> > > I wonder sometimes if it could be sufficient to just have a relay rate\n>> > > limit and prioritise by ancestor feerate though. Maybe something like:\n>> > > - instead of adding txs to each peers setInventoryTxToSend\n>> immediately,\n>> > >   set a mempool flag \"relayed=false\"\n>> > > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n>> > >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n>> > >   calculate how much kB those txs were, and do this again after\n>> > >   SIZE/RATELIMIT seconds\n>>\n>> > > - don't include \"relayed=false\" txs when building blocks?\n>>\n>> The \"?\" was me not being sure that point is a good suggestion...\n>>\n>> Miners might reasonably decide to have no rate limit, and always relay,\n>> and never exclude txs -- but the question then becomes is whether they\n>> hear about the tx at all, so rate limiting behaviour could still be a\n>> potential problem for whoever made the tx.\n>>\n>> > Wow cool! I think outbound tx relay size-based rate-limiting and\n>> > prioritizing tx relay by feerate are great ideas for preventing spammers\n>> > from wasting bandwidth network-wide. I agree, this would slow the low\n>> > feerate spam down, preventing a huge network-wide bandwidth spike. And\n>> it\n>> > would allow high feerate transactions to propagate as they should,\n>> > regardless of how busy traffic is. Combined with inbound tx request\n>> > rate-limiting, might this be sufficient to prevent DoS regardless of the\n>> > fee-based replacement policies?\n>>\n>> I think you only want to do outbound rate limits, ie, how often you send\n>> INV, GETDATA and TX messages? Once you receive any of those, I think\n>> you have to immediately process / ignore it, you can't really sensibly\n>> defer it (beyond the existing queues we have that just build up while\n>> we're busy processing other things first)?\n>>\n>> > One point that I'm not 100% clear on: is it ok to prioritize the\n>> > transactions by ancestor feerate in this scheme? As I described in the\n>> > original post, this can be quite different from the actual feerate we\n>> would\n>> > consider a transaction in a block for. The transaction could have a high\n>> > feerate sibling bumping its ancestor.\n>> > For example, A (1sat/vB) has 2 children: B (49sat/vB) and C (5sat/vB).\n>> If\n>> > we just received C, it would be incorrect to give it a priority equal to\n>> > its ancestor feerate (3sat/vB) because if we constructed a block\n>> template\n>> > now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n>> > Then, if we imagine that top N is >5sat/vB, we're not relaying C. If we\n>> > also exclude C when building blocks, we're missing out on good fees.\n>>\n>> I think you're right that this would be ugly. It's something of a\n>> special case:\n>>\n>>  a) you really care about C getting into the next block; but\n>>  b) you're trusting B not being replaced by a higher fee tx that\n>>     doesn't have A as a parent; and\n>>  c) there's a lot of txs bidding the floor of the next block up to a\n>>     level in-between the ancestor fee rate of 3sat/vB and the tx fee\n>>     rate of 5sat/vB\n>>\n>> Without (a), maybe you don't care about it getting to a miner quickly.\n>> If your trust in (b) was misplaced, then your tx's effective fee rate\n>> will drop and (because of (c)), you'll lose anyway. And if the spam ends\n>> up outside of (c)'s range, either the rate limiting won't take effect\n>> (spam's too cheap) and you'll be fine, or you'll miss out on the block\n>> anyway (spam's paying more than your tx rate) and you never had any hope\n>> of making it in.\n>>\n>> Note that we already rate limit via INVENTORY_BROADCAST_MAX /\n>> *_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\n>> per 10 minutes for outbound connections. This would be a weight based\n>> rate limit instead-of/in-addition-to that, I guess.\n>>\n>> As far as a non-ugly approach goes, I think you'd have to be smarter about\n>> tracking the \"effective fee rate\" than the ancestor fee rate manages;\n>> maybe that's something that could fall out of Murch and Clara's candidate\n>> set blockbuilding ideas [0] ?\n>>\n>> Perhaps that same work would also make it possible to come up with\n>> a better answer to \"do I care that this replacement would invalidate\n>> these descendents?\"\n>>\n>> [0] https://github.com/Xekyo/blockbuilding\n>>\n>> > > - keep high-feerate evicted txs around for a while in case they get\n>> > >   mined by someone else to improve compact block relay, a la the\n>> > >   orphan pool?\n>> > Replaced transactions are already added to vExtraTxnForCompact :D\n>>\n>> I guess I was thinking that it's just a 100 tx LRU cache, which might\n>> not be good enough?\n>>\n>> Maybe it would be more on point to have a rate limit apply only to\n>> replacement transactions?\n>>\n>> > For wallets, AJ's \"All you need is for there to be *a* path that follows\n>> > the new relay rules and gets from your node/wallet to perhaps 10% of\n>> > hashpower\" makes sense to me (which would be the former).\n>>\n>> Perhaps a corollarly of that is that it's *better* to have the mempool\n>> acceptance rule only consider economic incentives, and have the spam\n>> prevention only be about \"shall I tell my peers about this?\"\n>>\n>> If you don't have that split; then the anti-spam rules can prevent you\n>> from getting the tx in the mempool at all; whereas if you do have the\n>> split, then even if the bitcoind anti-spam rules are blocking you at\n>> every turn, you can still send your tx to miners by some other route,\n>> and then they can add it to their mempool directly without any hassle.\n>>\n>> Cheers,\n>> aj\n>>\n>> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220316/85385745/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-17T15:59:11",
                "message_text_only": "@Antoine\n>  B overrides A and starts to replace package A in the network mempools\nnearest to Alice. I think those peers won't have bandwidth saving from\nadopting a replacement staggering strategy.\n\nThat's an interesting point, but even with that fact, the method would be\neffective at limiting spam. While yes, considering just a single unit of\npotential spam, only the nodes that didn't relay the spam in the first\nplace would save bandwidth. However, the point is not to prevent a single\nunit, but to prevent thousands of units of spam. Even if in the situation\nyou describe Bob and Alice sent 100 replacement transaction per seconds, it\nwould lead to only 1 transaction sent by Bob's peers and only 2\ntransactions sent by Alice's peers (within a given stagger/cooldown\nwindow). That seems pretty effective to me.\n\n> I wonder if a \"diff-only\" strategy.. would be more interesting\n\nI think a diff-only relay strategy is definitely interesting. But its\ncompletely orthogonal. A diff only strategy isn't really a spam reduction\nmechanism, but rather a relay optimization that reduces bandwidth on all\nrelay. Both can be done - seems like it could be argued that both should be\ndone.\n\n>> For example, some nth bit of nSequence with nVersion 3 means \"this\ntransaction won't have more than X vbytes of descendants\"\n>what if the pinning transaction has a parent with a junk feerate ?\n\nI think you're right that this scheme would also be susceptible to pinning.\n\nOne thing I've identified as pretty much always suboptimal in any kind of\npolicy is cliffs. Hard cut offs very often cause problems. You see this in\nwelfare cliffs where the cliff disincentivizes people from earning more\nmoney for example. Its almost always better to build in a smooth gradient.\n\nRate limiting where a node would relay replacement transaction data up to a\ncertain point and then stop is a cliff like this. The descendant byte limit\nis a cliff like this. If such things were to be actually done, I'd\nrecommend building in some kind of gradient where, say, every X vbytes of\ndescendents requires Y additional feerate, or something to that effect.\nThat way you can always add more descendents as long as you're willing to\npay a higher and higher feerate for them. However, I think simply removing\nthe absolute feerate rule is a better solution to that kind of RBF pinning.\n\n\n\nOn Thu, Mar 17, 2022 at 4:32 AM Antoine Riard via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi Mempoololic Anonymous fellow,\n>\n> > 2. Staggered broadcast of replacement transactions: within some time\n> > interval, maybe accept multiple replacements for the same prevout, but\n> only\n> > relay the original transaction.\n>\n> If the goal of replacement staggering is to save on bandwidth, I'm not\n> sure it's going to be effective if you consider replacement done from a\n> shared-utxo. E.g, Alice broadcasts a package to confirm her commitment,\n> relay is staggered until T. At the same time, Bob broadcasts a package to\n> confirm his version of the commitment at a slightly better feerate, relay\n> is staggered until T.\n>\n> At T, package A gradually floods from Alice's peers and package B does the\n> same from Bob's peers. When there is an intersection. B overrides A and\n> starts to replace package A in the network mempools nearest to Alice. I\n> think those peers won't have bandwidth saving from adopting a replacement\n> staggering strategy.\n>\n> Or maybe that's something completely different if you have in mind ? I\n> think it's worth more staggering detail to guess if it's robust against all\n> the replacement propagations patterns.\n>\n> Though if we aim to save on replacement bandwidth I wonder if a\n> \"diff-only\" strategy, assuming some new p2p mechanism, would be more\n> interesting (as discussed in the recent \"Thoughts on fee bumping thread\").\n>\n> > A lingering concern that I have about this idea is it would then be\n> > possible to impact the propagation of another person\u2019s transaction, i.e.,\n> > an attacker can censor somebody\u2019s transaction from ever being announced\n> by\n> > a node if they send enough transactions to fill up the rate limit.\n> > Obviously this would be expensive since they're spending a lot on fees,\n> but\n> > I imagine it could be profitable in some situations to spend a few\n> thousand\n> > dollars to prevent anyone from hearing about a transaction for a few\n> hours.\n> > This might be a non-issue in practice if the rate limit is generous and\n> > traffic isn\u2019t horrendous, but is this a problem?\n>\n> I think I share the concern too about an attacker exhausting a node\n> transaction relay ressources to prevent another person's transaction to\n> propagate, especially if the transaction targeted is a L2's time-sensitive\n> one. In that latter context, an attacker would aim to delay the relay of a\n> time-sensitive transaction (e.g a HTLC-success) to the miners, until the\n> timelock expires. The malicious delay period should swallow the go-to-chain\n> HTLC deadline (\"the deadline for received HTLCs this node fulfilled\" in\n> bolt 2 parlance), in that current example 18 blocks.\n>\n> Let's say we allocate 10 MB of bandwidth per-block period. Once the 10 MB\n> are exhausted, there is no more bandwidth allocated until the next block is\n> issued. If the top mempool feerate is 1 sat/vb, such naive design would\n> allow an attacker to buy all the p2p network bandwidth period for 0.1 BTC.\n> If an attacker aims to jam a HTLC transaction for the 18 blocks period, the\n> cost is of 1,8 BTC. If the attacker is a LN counterparty to a HTLC worth\n> more than 1.8 BTC, the attack sounds economically profitable.\n>\n> Worst, the p2p network bandwidth is a public resource while a HTLC is a\n> private, off-chain contract. An attacker could be counterparty to many\n> HTLCs, where each HTLC individual value is far inferior to the global p2p\n> bandwidth cost but the sum, only known to the attacker, is superior to.\n> Therefore, it sounds to me that p2p network bandwidth might be attractive\n> if the stealing are batched.\n>\n> Is the attacker scenario described credible ? Are the numbers sketched out\n> realistic ?\n>\n> If yes, I think one design insight for eventual transaction relay rate\n> limiting would be to make them \"dynamic\", and not naively fixed for a\n> period. By making them dynamic, an attacker would have to compete with the\n> effective feerate proposed by the victim transaction. E.g, if the\n> HTLC-success feerate is of 10 sat/vb, an attacker would have to propose a\n> stream of malicious transaction of more than 10 sat/vb during the whole\n> HTLC deadline period for the transaction-relay jamming to be effective.\n>\n> Further, the attack might be invisible from the victim standpoint, the\n> malicious flow of feerate competitive transactions can be hard to\n> dissociate from an honest one. Thus, you can expect the\n> HTLC transaction issuer to only slowly increase the feerate at each block,\n> and those moves to be anticipated by the attacker. Even if the transaction\n> issuer adopts a scorched-earth approach for the latest blocks of the\n> deadline, the absolute value of the HTLC burnt in fees might still be less\n> than the transaction relay bandwidth exhaustion paid by the attacker\n> because the attack is batched by the attacker.\n>\n> I'm not sure if this reasoning is correct. Though if yes, the issue sounds\n> really similar to \"flood&loot\" attack affecting LN previously researched on\n> [0]. What worries me more with this \"exhaust&loot\" is that if we introduce\n> bounded transaction relay rate limiting, it sounds a cheaper public\n> ressource to buy than the mempool..\n>\n> [0] https://arxiv.org/pdf/2006.08513.pdf\n>\n> Anyway, I would say it's worthy to investigate more transaction relay rate\n> limiting designs and especially carefully weigh the implications for L2s.\n> Those ones might have to adjust their fee-bumping and transaction\n> rebroadcast strategies in consequence.\n>\n> > Suhas and Matt [proposed][0] adding a policy rule allowing users to\n> specify\n> > descendant limits on their transactions. For example, some nth bit of\n> > nSequence with nVersion 3 means \"this transaction won't have more than X\n> > vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n> > solves the pinning problem with package RBF where the attacker's package\n> > contains a very large and high-fee descendant.\n>\n> Hey, what if the pinning transaction has a parent with a junk feerate ?\n>\n> Let's say you have commitment tx for a HTLC of value 500000 sats, with top\n> mempool feerate of 50 sat/vbyte. The commitment tx is pinned by a malicious\n> tx of size 1000 vbytes, matching top mempool feerate. This malicious tx has\n> a second unconfirmed parent (in addition to the commitment) of size\n> MAX_STANDARD_TX_WEIGHT offering a 1 sat/vb. I think the pinning transaction\n> ancestor score would be less than 2 sat/vb and thus considered irrelevant\n> for block template inclusion ? At the same time, as the pinning transaction\n> is attached with a top mempool feerate, the honest user wouldn't be able to\n> replace it with a better-feerate proposal ? Unless adopting a\n> scorched-earth approach,  although economically I don't think this\n> fee-bumping strategy is safe in case of batch-pinning.\n>\n> It might be fixable if we make one additional requirement \"The child\n> transaction subject to the user-elected descendant limit must have only one\n> unconfirmed parent\" (here the commitment\n> transaction) ? Though I'm not even sure of the robustness of this fix. The\n> commitment transaction itself could be used as a junk parent to downgrade\n> the pinning transaction ancestor score. E.g, using a revoked commitment\n> transaction with `max_accepted_htlcs` on both sides, pre-signed with a\n> feerate of 1 sat/vb. We might restrict the maximum number of pending HTLCs\n> network-wise to make the worst commitment transaction size reasonable,\n> though not sure if my LN colleagues are going to like the idea..\n>\n> Is that reasoning correct and conform to our Ancestor Set Based algorithm\n> approach ? Maybe more details are needed.\n>\n> > Also, coming back to the idea of \"we can't just use {individual,\n> ancestor}\n> > feerate,\" I'm interested in soliciting feedback on adding a \u201cmining\n> score\u201d\n> > calculator. I've implemented one [here][2] which takes the transaction in\n> > question, grabs all of the connected mempool transactions (including\n> > siblings, coparents, etc., as they wouldn\u2019t be in the ancestor nor\n> > descendant sets), and builds a \u201cblock template\u201d using our current mining\n> > algorithm. The mining score of a transaction is the ancestor feerate at\n> > which it is included.\n>\n> I don't have a strong opinion there yet, though if we make this \"block\n> template\" construction the default one, I would be really conservative to\n> avoid malicious child attachment on multi-party transactions downgrading\n> the block inclusion efficiency.\n>\n> Antoine\n>\n> Le mer. 9 mars 2022 \u00e0 10:37, Gloria Zhao via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n>\n>> Hi RBF friends,\n>>\n>> Posting a summary of RBF discussions at coredev (mostly on transaction\n>> relay rate-limiting), user-elected descendant limit as a short term\n>> solution to unblock package RBF, and mining score, all open for feedback:\n>>\n>> One big concept discussed was baking DoS protection into the p2p level\n>> rather than policy level. TLDR: The fees are not paid to the node operator,\n>> but to the miner. While we can use fees to reason about the cost of an\n>> attack, if we're ultimately interested in preventing resource exhaustion,\n>> maybe we want to \"stop the bleeding\" when it happens and bound the amount\n>> of resources used in general. There were two main ideas:\n>>\n>> 1. Transaction relay rate limiting (i.e. the one you proposed above or\n>> some variation) with a feerate-based priority queue\n>> 2. Staggered broadcast of replacement transactions: within some time\n>> interval, maybe accept multiple replacements for the same prevout, but only\n>> relay the original transaction.\n>>\n>> Looking to solicit feedback on these ideas and the concept in general. Is\n>> it a good idea (separate from RBF) to add rate-limiting in transaction\n>> relay? And is it the right direction to think about RBF DoS protection this\n>> way?\n>>\n>> A lingering concern that I have about this idea is it would then be\n>> possible to impact the propagation of another person\u2019s transaction, i.e.,\n>> an attacker can censor somebody\u2019s transaction from ever being announced by\n>> a node if they send enough transactions to fill up the rate limit.\n>> Obviously this would be expensive since they're spending a lot on fees, but\n>> I imagine it could be profitable in some situations to spend a few thousand\n>> dollars to prevent anyone from hearing about a transaction for a few hours.\n>> This might be a non-issue in practice if the rate limit is generous and\n>> traffic isn\u2019t horrendous, but is this a problem?\n>>\n>> And if we don't require an increase in (i.e. addition of \"new\") absolute\n>> fees, users are essentially allowed to \u201crecycle\u201d fees. In the scenario\n>> where we prioritize relay based on feerate, users could potentially be\n>> placed higher in the queue, ahead of other users\u2019 transactions, multiple\n>> times, without ever adding more fees to the transaction. Again, maybe this\n>> isn\u2019t a huge deal in practice if we set the parameters right, but it seems\u2026\n>> not great, in principle.\n>>\n>> ---------\n>>\n>> It's probably also a good idea to point out that there's been some\n>> discussion happening on the gist containing my original post on this thread\n>> (https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff).\n>>\n>> Suhas and Matt [proposed][0] adding a policy rule allowing users to\n>> specify descendant limits on their transactions. For example, some nth bit\n>> of nSequence with nVersion 3 means \"this transaction won't have more than X\n>> vbytes of descendants\" where X = max(1000, vsizeof(tx)) or something. It\n>> solves the pinning problem with package RBF where the attacker's package\n>> contains a very large and high-fee descendant.\n>>\n>> We could add this policy and deploy it with package RBF/package relay so\n>> that LN can use it by setting the user-elected descendant limit flag on\n>> commitment transactions. (Otherwise package RBF is blocked until we find a\n>> more comprehensive solution to the pinning attack).\n>>\n>> It's simple to [implement][1] as a mempool policy, but adds some\n>> complexity for wallets that use it, since it limits their use of UTXOs from\n>> transactions with this bit set.\n>>\n>> ---------\n>>\n>> Also, coming back to the idea of \"we can't just use {individual,\n>> ancestor} feerate,\" I'm interested in soliciting feedback on adding a\n>> \u201cmining score\u201d calculator. I've implemented one [here][2] which takes the\n>> transaction in question, grabs all of the connected mempool transactions\n>> (including siblings, coparents, etc., as they wouldn\u2019t be in the ancestor\n>> nor descendant sets), and builds a \u201cblock template\u201d using our current\n>> mining algorithm. The mining score of a transaction is the ancestor feerate\n>> at which it is included.\n>>\n>> This would be helpful for something like ancestor-aware funding and\n>> fee-bumping in the wallet: [3], [4]. I think if we did the rate-limited\n>> priority queue for transaction relay, we'd want to use something like this\n>> as the priority value. And for RBF, we probably want to require that a\n>> replacement have a higher mining score than the original transactions. This\n>> could be computationally expensive to do all the time; it could be good to\n>> cache it but that could make mempool bookkeeping more complicated. Also, if\n>> we end up trying to switch to a candidate set-based algorithm for mining,\n>> we'd of course need a new calculator.\n>>\n>> [0]:\n>> https://gist.github.com/glozow/25d9662c52453bd08b4b4b1d3783b9ff?permalink_comment_id=4058140#gistcomment-4058140\n>> [1]: https://github.com/glozow/bitcoin/tree/2022-02-user-desclimit\n>> [2] https://github.com/glozow/bitcoin/tree/2022-02-mining-score\n>> [3]: https://github.com/bitcoin/bitcoin/issues/9645\n>> [4]: https://github.com/bitcoin/bitcoin/issues/15553\n>>\n>> Best,\n>> Gloria\n>>\n>> On Tue, Feb 8, 2022 at 4:58 AM Anthony Towns <aj at erisian.com.au> wrote:\n>>\n>>> On Mon, Feb 07, 2022 at 11:16:26AM +0000, Gloria Zhao wrote:\n>>> > @aj:\n>>> > > I wonder sometimes if it could be sufficient to just have a relay\n>>> rate\n>>> > > limit and prioritise by ancestor feerate though. Maybe something\n>>> like:\n>>> > > - instead of adding txs to each peers setInventoryTxToSend\n>>> immediately,\n>>> > >   set a mempool flag \"relayed=false\"\n>>> > > - on a time delay, add the top N (by fee rate) \"relayed=false\" txs to\n>>> > >   each peer's setInventoryTxToSend and mark them as \"relayed=true\";\n>>> > >   calculate how much kB those txs were, and do this again after\n>>> > >   SIZE/RATELIMIT seconds\n>>>\n>>> > > - don't include \"relayed=false\" txs when building blocks?\n>>>\n>>> The \"?\" was me not being sure that point is a good suggestion...\n>>>\n>>> Miners might reasonably decide to have no rate limit, and always relay,\n>>> and never exclude txs -- but the question then becomes is whether they\n>>> hear about the tx at all, so rate limiting behaviour could still be a\n>>> potential problem for whoever made the tx.\n>>>\n>>> > Wow cool! I think outbound tx relay size-based rate-limiting and\n>>> > prioritizing tx relay by feerate are great ideas for preventing\n>>> spammers\n>>> > from wasting bandwidth network-wide. I agree, this would slow the low\n>>> > feerate spam down, preventing a huge network-wide bandwidth spike. And\n>>> it\n>>> > would allow high feerate transactions to propagate as they should,\n>>> > regardless of how busy traffic is. Combined with inbound tx request\n>>> > rate-limiting, might this be sufficient to prevent DoS regardless of\n>>> the\n>>> > fee-based replacement policies?\n>>>\n>>> I think you only want to do outbound rate limits, ie, how often you send\n>>> INV, GETDATA and TX messages? Once you receive any of those, I think\n>>> you have to immediately process / ignore it, you can't really sensibly\n>>> defer it (beyond the existing queues we have that just build up while\n>>> we're busy processing other things first)?\n>>>\n>>> > One point that I'm not 100% clear on: is it ok to prioritize the\n>>> > transactions by ancestor feerate in this scheme? As I described in the\n>>> > original post, this can be quite different from the actual feerate we\n>>> would\n>>> > consider a transaction in a block for. The transaction could have a\n>>> high\n>>> > feerate sibling bumping its ancestor.\n>>> > For example, A (1sat/vB) has 2 children: B (49sat/vB) and C (5sat/vB).\n>>> If\n>>> > we just received C, it would be incorrect to give it a priority equal\n>>> to\n>>> > its ancestor feerate (3sat/vB) because if we constructed a block\n>>> template\n>>> > now, B would bump A, and C's new ancestor feerate is 5sat/vB.\n>>> > Then, if we imagine that top N is >5sat/vB, we're not relaying C. If we\n>>> > also exclude C when building blocks, we're missing out on good fees.\n>>>\n>>> I think you're right that this would be ugly. It's something of a\n>>> special case:\n>>>\n>>>  a) you really care about C getting into the next block; but\n>>>  b) you're trusting B not being replaced by a higher fee tx that\n>>>     doesn't have A as a parent; and\n>>>  c) there's a lot of txs bidding the floor of the next block up to a\n>>>     level in-between the ancestor fee rate of 3sat/vB and the tx fee\n>>>     rate of 5sat/vB\n>>>\n>>> Without (a), maybe you don't care about it getting to a miner quickly.\n>>> If your trust in (b) was misplaced, then your tx's effective fee rate\n>>> will drop and (because of (c)), you'll lose anyway. And if the spam ends\n>>> up outside of (c)'s range, either the rate limiting won't take effect\n>>> (spam's too cheap) and you'll be fine, or you'll miss out on the block\n>>> anyway (spam's paying more than your tx rate) and you never had any hope\n>>> of making it in.\n>>>\n>>> Note that we already rate limit via INVENTORY_BROADCAST_MAX /\n>>> *_INVENTORY_BROADCAST_INTERVAL; which gets to something like 10,500 txs\n>>> per 10 minutes for outbound connections. This would be a weight based\n>>> rate limit instead-of/in-addition-to that, I guess.\n>>>\n>>> As far as a non-ugly approach goes, I think you'd have to be smarter\n>>> about\n>>> tracking the \"effective fee rate\" than the ancestor fee rate manages;\n>>> maybe that's something that could fall out of Murch and Clara's candidate\n>>> set blockbuilding ideas [0] ?\n>>>\n>>> Perhaps that same work would also make it possible to come up with\n>>> a better answer to \"do I care that this replacement would invalidate\n>>> these descendents?\"\n>>>\n>>> [0] https://github.com/Xekyo/blockbuilding\n>>>\n>>> > > - keep high-feerate evicted txs around for a while in case they get\n>>> > >   mined by someone else to improve compact block relay, a la the\n>>> > >   orphan pool?\n>>> > Replaced transactions are already added to vExtraTxnForCompact :D\n>>>\n>>> I guess I was thinking that it's just a 100 tx LRU cache, which might\n>>> not be good enough?\n>>>\n>>> Maybe it would be more on point to have a rate limit apply only to\n>>> replacement transactions?\n>>>\n>>> > For wallets, AJ's \"All you need is for there to be *a* path that\n>>> follows\n>>> > the new relay rules and gets from your node/wallet to perhaps 10% of\n>>> > hashpower\" makes sense to me (which would be the former).\n>>>\n>>> Perhaps a corollarly of that is that it's *better* to have the mempool\n>>> acceptance rule only consider economic incentives, and have the spam\n>>> prevention only be about \"shall I tell my peers about this?\"\n>>>\n>>> If you don't have that split; then the anti-spam rules can prevent you\n>>> from getting the tx in the mempool at all; whereas if you do have the\n>>> split, then even if the bitcoind anti-spam rules are blocking you at\n>>> every turn, you can still send your tx to miners by some other route,\n>>> and then they can add it to their mempool directly without any hassle.\n>>>\n>>> Cheers,\n>>> aj\n>>>\n>>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220317/4fcd061a/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Improving RBF Policy",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Billy Tetrud",
                "Gloria Zhao",
                "Antoine Riard"
            ],
            "messages_count": 7,
            "total_messages_chars_count": 137856
        }
    },
    {
        "title": "[bitcoin-dev] Speedy Trial",
        "thread_messages": [
            {
                "author": "Russell O'Connor",
                "date": "2022-03-11T00:12:19",
                "message_text_only": "On Thu., Mar. 10, 2022, 08:04 Jorge Tim\u00f3n via bitcoin-dev, <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> You're right, we shouldn't get personal. We shouldn't ignore feedback from\n> me, mark friedenbach or luke just because of who it comes from.\n>\n\nFor goodness sake Jorge, enough with the persecution complex.\n\nAs the person who initially proposed the Speedy Trial deployment design, I\ncan say it was designed to take in account those concerns raised by luke-jr\nand the \"no-miner-veto\" faction.  I also listened to the\n\"devs-do-not-decide\" faction and the \"no-divegent-consensus-rules\" faction\nand their concerns.\n\nThe \"no-miner-veto\" concerns are, to an extent, addressed by the short\ntimeline of Speedy Trial.  No more waiting 2 years on the miners dragging\ntheir feet.  If ST fails to active then we are back where we started with\nat most a few weeks lost.  And those weeks aren't really lost if they would\nhave been wasted away anyways trying to find broad consensus on another\ndeployment mechanism.\n\nI get that you don't like the design of Speedy Trial.  You may even object\nthat it fails to really address your concerns by leaving open how to follow\nup a failed Speedy Trial deployment.  But regardless of how you feel, I\nbelieve I did meaningfully address the those miner-veto concerns and other\npeople agree with me.\n\nIf you are so concerned about listening to legitimate criticism, maybe you\ncan design a new deployment mechanism that addresses the concerns of the\n\"devs-do-not-decide\" faction and the \"no-divegent-consensus-rules\"\nfaction.  Or do you feel that their concerns are illegitimate?  Maybe, by\nsheer coincidence, all people you disagree with have illegitimate concerns\nwhile only your concerns are legitimate.\n\nA major contender to the Speedy Trial design at the time was to mandate\neventual forced signalling, championed by luke-jr.  It turns out that, at\nthe time of that proposal, a large amount of hash power simply did not have\nthe firmware required to support signalling.  That activation proposal\nnever got broad consensus, and rightly so, because in retrospect we see\nthat the design might have risked knocking a significant fraction of mining\npower offline if it had been deployed.  Imagine if the firmware couldn't be\nquickly updated or imagine if the problem had been hardware related.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220310/3b9bd8b8/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2022-03-11T00:28:08",
                "message_text_only": "On Friday 11 March 2022 00:12:19 Russell O'Connor via bitcoin-dev wrote:\n> The \"no-miner-veto\" concerns are, to an extent, addressed by the short\n> timeline of Speedy Trial.  No more waiting 2 years on the miners dragging\n> their feet.\n\nIt's still a miner veto. The only way this works is if the full deployment \n(with UASF fallback) is released in parallel.\n\n> If you are so concerned about listening to legitimate criticism, maybe you\n> can design a new deployment mechanism that addresses the concerns of the\n> \"devs-do-not-decide\" faction and the \"no-divegent-consensus-rules\"\n> faction.\n\nBIP8 already does that.\n\n> A major contender to the Speedy Trial design at the time was to mandate\n> eventual forced signalling, championed by luke-jr.  It turns out that, at\n> the time of that proposal, a large amount of hash power simply did not have\n> the firmware required to support signalling.  That activation proposal\n> never got broad consensus,\n\nBIP 8 did in fact have broad consensus before some devs decided to ignore the \ncommunity and do their own thing. Why are you trying to rewrite history?\n\n> and rightly so, because in retrospect we see \n> that the design might have risked knocking a significant fraction of mining\n> power offline if it had been deployed.  Imagine if the firmware couldn't be\n> quickly updated or imagine if the problem had been hardware related.\n\nThey had 18 months to fix their broken firmware. That's plenty of time.\n\nLuke"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-11T05:41:58",
                "message_text_only": ">  BIP 8 did in fact have broad consensus\n\nI hear you claim this often Luke, but claiming its so does not make it so.\nDo you think BIP8 still has broad consensus? If that's the case, maybe all\nthat's needed is to gather some evidence\n<https://www.youtube.com/watch?v=U7rXOgL4oFQ> and present it.\n\nOn Thu, Mar 10, 2022 at 6:38 PM Luke Dashjr via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Friday 11 March 2022 00:12:19 Russell O'Connor via bitcoin-dev wrote:\n> > The \"no-miner-veto\" concerns are, to an extent, addressed by the short\n> > timeline of Speedy Trial.  No more waiting 2 years on the miners dragging\n> > their feet.\n>\n> It's still a miner veto. The only way this works is if the full deployment\n> (with UASF fallback) is released in parallel.\n>\n> > If you are so concerned about listening to legitimate criticism, maybe\n> you\n> > can design a new deployment mechanism that addresses the concerns of the\n> > \"devs-do-not-decide\" faction and the \"no-divegent-consensus-rules\"\n> > faction.\n>\n> BIP8 already does that.\n>\n> > A major contender to the Speedy Trial design at the time was to mandate\n> > eventual forced signalling, championed by luke-jr.  It turns out that, at\n> > the time of that proposal, a large amount of hash power simply did not\n> have\n> > the firmware required to support signalling.  That activation proposal\n> > never got broad consensus,\n>\n> BIP 8 did in fact have broad consensus before some devs decided to ignore\n> the\n> community and do their own thing. Why are you trying to rewrite history?\n>\n> > and rightly so, because in retrospect we see\n> > that the design might have risked knocking a significant fraction of\n> mining\n> > power offline if it had been deployed.  Imagine if the firmware couldn't\n> be\n> > quickly updated or imagine if the problem had been hardware related.\n>\n> They had 18 months to fix their broken firmware. That's plenty of time.\n>\n> Luke\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220310/c8344427/attachment-0001.html>"
            },
            {
                "author": "pushd",
                "date": "2022-03-11T11:14:33",
                "message_text_only": "> Do you think BIP8 still has broad consensus? If that's the case, maybe all\nthat's needed is to gather some evidence<https://www.youtube.com/watch?v=U7rXOgL4oFQ>; and present it.\n\nThis pull request had some support and a few disagreements: https://archive.fo/uw1cO\n\npushd\n---parallel lines meet at infinity?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220311/1206460f/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-11T12:19:36",
                "message_text_only": "On Fri, Mar 11, 2022, 00:12 Russell O'Connor via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n> On Thu., Mar. 10, 2022, 08:04 Jorge Tim\u00f3n via bitcoin-dev, <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>\n>>\n>> You're right, we shouldn't get personal. We shouldn't ignore feedback\n>> from me, mark friedenbach or luke just because of who it comes from.\n>>\n>\n> For goodness sake Jorge, enough with the persecution complex.\n>\n\nThanks for answering.\n\nAs the person who initially proposed the Speedy Trial deployment design, I\n> can say it was designed to take in account those concerns raised by luke-jr\n> and the \"no-miner-veto\" faction.  I also listened to the\n> \"devs-do-not-decide\" faction and the \"no-divegent-consensus-rules\" faction\n> and their concerns.\n>\n\nThat's great, but it still doesn't take into account my concerns. I'm not\npart of any of those \"factions\". I guess I'm part of the \"yes-user-veto\"\nfaction. I know, I know, we don't matter because the \"no-divergent-rules\"\n\"faction\" matters too much for us to be listened.\n\n\n\nThe \"no-miner-veto\" concerns are, to an extent, addressed by the short\n> timeline of Speedy Trial.  No more waiting 2 years on the miners dragging\n> their feet.  If ST fails to active then we are back where we started with\n> at most a few weeks lost.  And those weeks aren't really lost if they would\n> have been wasted away anyways trying to find broad consensus on another\n> deployment mechanism.\n>\n> I get that you don't like the design of Speedy Trial.  You may even object\n> that it fails to really address your concerns by leaving open how to follow\n> up a failed Speedy Trial deployment.  But regardless of how you feel, I\n> believe I did meaningfully address the those miner-veto concerns and other\n> people agree with me.\n>\n> If you are so concerned about listening to legitimate criticism, maybe you\n> can design a new deployment mechanism that addresses the concerns of the\n> \"devs-do-not-decide\" faction and the \"no-divegent-consensus-rules\"\n> faction.  Or do you feel that their concerns are illegitimate?  Maybe, by\n> sheer coincidence, all people you disagree with have illegitimate concerns\n> while only your concerns are legitimate.\n>\n\nI talked about this. But the \"no-divergent-rules\" faction doesn't like it,\nso we can pretend we have listened to this \"faction\" and addressed all its\nconcerns, I guess.\nOr perhaps it's just \"prosectution complex\", but, hey, what do I know about\npsychology?\n\nA major contender to the Speedy Trial design at the time was to mandate\n> eventual forced signalling, championed by luke-jr.  It turns out that, at\n> the time of that proposal, a large amount of hash power simply did not have\n> the firmware required to support signalling.  That activation proposal\n> never got broad consensus, and rightly so, because in retrospect we see\n> that the design might have risked knocking a significant fraction of mining\n> power offline if it had been deployed.  Imagine if the firmware couldn't be\n> quickly updated or imagine if the problem had been hardware related.\n>\n\nYes, I like this solution too, with a little caveat: an easy mechanism for\nusers to actively oppose a proposal.\nLuke alao talked about this.\nIf users oppose, they should use activation as a trigger to fork out of the\nnetwork by invalidating the block that produces activation.\nThe bad scenario here is that miners want to deploy something but users\ndon't want to.\n\"But that may lead to a fork\". Yeah, I know.\nI hope imagining a scenario in which developers propose something that most\nminers accept but some users reject is not taboo.\n\nSome of these discussions started at the time of segwit activation. Yes,\nsegwit, not taproot.\n\nAs for mark, he wasn't talking about activation, but quantum computing\nconcerns. Perhaps those have been \"addressed\"?\nI just don't know where.\n\n_______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220311/98eb3c55/attachment.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-03-11T13:47:14",
                "message_text_only": "On Fri, Mar 11, 2022 at 7:18 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> I talked about this. But the \"no-divergent-rules\" faction doesn't like it,\n> so we can pretend we have listened to this \"faction\" and addressed all its\n> concerns, I guess.\n> Or perhaps it's just \"prosectution complex\", but, hey, what do I know\n> about psychology?\n>\n\nYour accusations of bad faith on the part of myself and pretty much\neveryone else makes me disinclined to continue this discussion with you.\nI'll reply, but if you want me to continue beyond this, then you need to\nknock it off with the accusations.\n\n\n> A major contender to the Speedy Trial design at the time was to mandate\n>> eventual forced signalling, championed by luke-jr.  It turns out that, at\n>> the time of that proposal, a large amount of hash power simply did not have\n>> the firmware required to support signalling.  That activation proposal\n>> never got broad consensus, and rightly so, because in retrospect we see\n>> that the design might have risked knocking a significant fraction of mining\n>> power offline if it had been deployed.  Imagine if the firmware couldn't be\n>> quickly updated or imagine if the problem had been hardware related.\n>>\n>\n> Yes, I like this solution too, with a little caveat: an easy mechanism for\n> users to actively oppose a proposal.\n> Luke alao talked about this.\n> If users oppose, they should use activation as a trigger to fork out of\n> the network by invalidating the block that produces activation.\n> The bad scenario here is that miners want to deploy something but users\n> don't want to.\n> \"But that may lead to a fork\". Yeah, I know.\n> I hope imagining a scenario in which developers propose something that\n> most miners accept but some users reject is not taboo.\n>\n\nThis topic is not taboo.\n\nThere are a couple of ways of opting out of taproot.  Firstly, users can\njust not use taproot.  Secondly, users can choose to not enforce taproot\neither by running an older version of Bitcoin Core or otherwise forking the\nsource code.  Thirdly, if some users insist on a chain where taproot is\n\"not activated\", they can always softk-fork in their own rule that\ndisallows the version bits that complete the Speedy Trial activation\nsequence, or alternatively soft-fork in a rule to make spending from (or\nto) taproot addresses illegal.\n\nAs for mark, he wasn't talking about activation, but quantum computing\n> concerns. Perhaps those have been \"addressed\"?\n> I just don't know where.\n>\n\nQuantum concerns were discussed.  Working from memory, the arguments were\n(1) If you are worried about your funds not being secured by taproot, then\ndon't use taproot addresses, and (2) If you are worried about everyone\nelse's funds not being quantum secure by other people choosing to use\ntaproot, well it is already too late because over 5M BTC is currently\nquantum insecure due to pubkey reuse <\nhttps://nitter.net/pwuille/status/1108091924404027392>.  I think it is\nunlikely that a quantum breakthrough will sneak up on us without time to\naddress the issue and, at the very least, warn people to move their funds\noff of taproot and other reused addresses, if not forking in some quantum\nsecure alternative.  A recent paper <\nhttps://www.sussex.ac.uk/physics/iqt/wp-content/uploads/2022/01/Webber-2022.pdf>\nsuggest that millions physical qubits will be needed to break EC in a day\nwith current error correction technology.  But even if taproot were to be\nvery suddenly banned, there is still a small possibility for recovery\nbecause one can prove ownership of HD pubkeys by providing a zero-knowledge\nproof of the chaincode used to derive them.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220311/40b6c9ad/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-11T14:04:29",
                "message_text_only": "On Fri, Mar 11, 2022, 13:47 Russell O'Connor via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Mar 11, 2022 at 7:18 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>> I talked about this. But the \"no-divergent-rules\" faction doesn't like\n>> it, so we can pretend we have listened to this \"faction\" and addressed all\n>> its concerns, I guess.\n>> Or perhaps it's just \"prosectution complex\", but, hey, what do I know\n>> about psychology?\n>>\n>\n> Your accusations of bad faith on the part of myself and pretty much\n> everyone else makes me disinclined to continue this discussion with you.\n> I'll reply, but if you want me to continue beyond this, then you need to\n> knock it off with the accusations.\n>\n\nWhat accusations of bad faith?\nYou're accusing me of having prosecution complex.\nI'm accusing you of ignoring the \"yes-users-veto\" faction. But that doesn't\nrequire bad faith, you may simply not understand the \"faction\".\n\nA major contender to the Speedy Trial design at the time was to mandate\n>>> eventual forced signalling, championed by luke-jr.  It turns out that, at\n>>> the time of that proposal, a large amount of hash power simply did not have\n>>> the firmware required to support signalling.  That activation proposal\n>>> never got broad consensus, and rightly so, because in retrospect we see\n>>> that the design might have risked knocking a significant fraction of mining\n>>> power offline if it had been deployed.  Imagine if the firmware couldn't be\n>>> quickly updated or imagine if the problem had been hardware related.\n>>>\n>>\n>> Yes, I like this solution too, with a little caveat: an easy mechanism\n>> for users to actively oppose a proposal.\n>> Luke alao talked about this.\n>> If users oppose, they should use activation as a trigger to fork out of\n>> the network by invalidating the block that produces activation.\n>> The bad scenario here is that miners want to deploy something but users\n>> don't want to.\n>> \"But that may lead to a fork\". Yeah, I know.\n>> I hope imagining a scenario in which developers propose something that\n>> most miners accept but some users reject is not taboo.\n>>\n>\n> This topic is not taboo.\n>\n> There are a couple of ways of opting out of taproot.  Firstly, users can\n> just not use taproot.  Secondly, users can choose to not enforce taproot\n> either by running an older version of Bitcoin Core or otherwise forking the\n> source code.  Thirdly, if some users insist on a chain where taproot is\n> \"not activated\", they can always softk-fork in their own rule that\n> disallows the version bits that complete the Speedy Trial activation\n> sequence, or alternatively soft-fork in a rule to make spending from (or\n> to) taproot addresses illegal.\n>\n\nSince it's about activation in general and not about taproot specifically,\nyour third point is the one that applies.\nUsers could have coordinated to have \"activation x\" never activated in\ntheir chains if they simply make a rule that activating a given proposal\n(with bip8) is forbidden in their chain.\nBut coordination requires time.\nPlease, try to imagine an example for an activation that you wouldn't like\nyourself. Imagine it gets proposed and you, as a user, want to resist it.\n\n\nAs for mark, he wasn't talking about activation, but quantum computing\n>> concerns. Perhaps those have been \"addressed\"?\n>> I just don't know where.\n>>\n>\n> Quantum concerns were discussed.  Working from memory, the arguments were\n> (1) If you are worried about your funds not being secured by taproot, then\n> don't use taproot addresses, and (2) If you are worried about everyone\n> else's funds not being quantum secure by other people choosing to use\n> taproot, well it is already too late because over 5M BTC is currently\n> quantum insecure due to pubkey reuse <\n> https://nitter.net/pwuille/status/1108091924404027392>.  I think it is\n> unlikely that a quantum breakthrough will sneak up on us without time to\n> address the issue and, at the very least, warn people to move their funds\n> off of taproot and other reused addresses, if not forking in some quantum\n> secure alternative.  A recent paper <\n> https://www.sussex.ac.uk/physics/iqt/wp-content/uploads/2022/01/Webber-2022.pdf>\n> suggest that millions physical qubits will be needed to break EC in a day\n> with current error correction technology.  But even if taproot were to be\n> very suddenly banned, there is still a small possibility for recovery\n> because one can prove ownership of HD pubkeys by providing a zero-knowledge\n> proof of the chaincode used to derive them.\n>\n\nThank you, perhaps I'm wrong about this and all his concerns were addressed\nand all his suggestions heard. I guess I shouldn't have brought that up,\nsince I cannot talk for Mark.\n\n_______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220311/0b51b48f/attachment-0001.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-03-12T13:34:59",
                "message_text_only": "On Fri, Mar 11, 2022 at 9:03 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n>\n> A major contender to the Speedy Trial design at the time was to mandate\n>> eventual forced signalling, championed by luke-jr.  It turns out that, at\n>> the time of that proposal, a large amount of hash power simply did not have\n>> the firmware required to support signalling.  That activation proposal\n>> never got broad consensus, and rightly so, because in retrospect we see\n>> that the design might have risked knocking a significant fraction of mining\n>> power offline if it had been deployed.  Imagine if the firmware couldn't be\n>> quickly updated or imagine if the problem had been hardware related.\n>>\n>\n>>> Yes, I like this solution too, with a little caveat: an easy mechanism\n>>> for users to actively oppose a proposal.\n>>> Luke alao talked about this.\n>>> If users oppose, they should use activation as a trigger to fork out of\n>>> the network by invalidating the block that produces activation.\n>>> The bad scenario here is that miners want to deploy something but users\n>>> don't want to.\n>>> \"But that may lead to a fork\". Yeah, I know.\n>>> I hope imagining a scenario in which developers propose something that\n>>> most miners accept but some users reject is not taboo.\n>>>\n>>\n>> This topic is not taboo.\n>>\n>> There are a couple of ways of opting out of taproot.  Firstly, users can\n>> just not use taproot.  Secondly, users can choose to not enforce taproot\n>> either by running an older version of Bitcoin Core or otherwise forking the\n>> source code.  Thirdly, if some users insist on a chain where taproot is\n>> \"not activated\", they can always softk-fork in their own rule that\n>> disallows the version bits that complete the Speedy Trial activation\n>> sequence, or alternatively soft-fork in a rule to make spending from (or\n>> to) taproot addresses illegal.\n>>\n>\n> Since it's about activation in general and not about taproot specifically,\n> your third point is the one that applies.\n> Users could have coordinated to have \"activation x\" never activated in\n> their chains if they simply make a rule that activating a given proposal\n> (with bip8) is forbidden in their chain.\n> But coordination requires time.\n>\n\nA mechanism of soft-forking against activation exists.  What more do you\nwant? Are we supposed to write the code on behalf of this hypothetical\ngroup of users who may or may not exist for them just so that they can have\na node that remains stalled on Speedy Trial lockin?  That simply isn't\nreasonable, but if you think it is, I invite you to create such a fork.\n\n\n> Please, try to imagine an example for an activation that you wouldn't like\n> yourself. Imagine it gets proposed and you, as a user, want to resist it.\n>\n\nIf I believe I'm in the economic majority then I'll just refuse to upgrade\nmy node, which was option 2. I don't know why you dismissed it.\n\nNot much can prevent a miner cartel from enforcing rules that users don't\nwant other than hard forking a replacement POW.  There is no effective\ndifference between some developers releasing a malicious soft-fork of\nBitcoin and the miners releasing a malicious version themselves.  And when\nthe miner cartel forms, they aren't necessarily going to be polite enough\nto give a transparent signal of their new rules.  However, without the\neconomic majority enforcing their set of rules, the cartel continuously\nrisks falling apart from the temptation of transaction fees of the censored\ntransactions.\n\nOn the other hand, If I find out I'm in the economic minority then I have\nlittle choice but to either accept the existence of the new rules or sell\nmy Bitcoin.  Look, you cannot have the perfect system of money all by your\nlonesome self.  Money doesn't have economic value if no one else wants to\ntrade you for it.  Just ask that poor user who YOLO'd his own taproot\nactivation in advance all by themselves.  I'm sure they think they've got\njust the perfect money system, with taproot early and everything.  But now\ntheir node is stuck at block 692261\n<https://b10c.me/blog/007-spending-p2tr-pre-activation/> and hasn't made\nprogress since.  No doubt they are hunkered down for the long term,\nabsolutely committed to their fork and just waiting for the rest of the\nworld to come around to how much better their version of Bitcoin is than\nthe rest of us.\n\nEven though you've dismissed it, one of the considerations of taproot was\nthat it is opt-in for users to use the functionality.  Future soft-forks\nought to have the same considerations to the extent possible.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220312/11e3c7a0/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-12T17:52:59",
                "message_text_only": ">  If I find out I'm in the economic minority then I have little choice but\nto either accept the existence of the new rules or sell my Bitcoin\n\nI do worry about what I have called a \"dumb majority soft fork\". This is\nwhere, say, mainstream adoption has happened, some crisis of some magnitude\nhappens that convinces a lot of people something needs to change now. Let's\nsay it's another congestion period where fees spike for months. Getting\ninto and out of lighting is hard and maybe even the security of lightning's\nsecurity model is called into question because it would either take too\nlong to get a transaction on chain or be too expensive. Panicy people might\nonce again think something like \"let's increase the block size to 1GB, then\nwe'll never have this problem again\". This could happen in a segwit-like\nsoft fork.\n\nIn a future where Bitcoin is the dominant world currency, it might not be\nunrealistic to imagine that an economic majority might not understand why\nsuch a thing would be so dangerous, or think the risk is low enough to be\nworth it. At that point, we in the economic minority would need a plan to\nhard fork away. One wouldn't necessarily need to sell all their majority\nfork Bitcoin, but they could.\n\nThat minority fork would of course need some mining power. How much? I\ndon't know, but we should think about how small of a minority chain we\ncould imagine might be worth saving. Is 5% enough? 1%? How long would the\nchain stall if hash power dropped to 1%?\n\nTBH I give the world a ~50% chance that something like this happens in the\nnext 100 years. Maybe Bitcoin will ossify and we'll lose all the people\nthat had deep knowledge on these kinds of things because almost no one's\nactively working on it. Maybe the crisis will be too much for people to\nremain rational and think long term. Who knows? But I think that at some\npoint it will become dangerous if there isn't a well discussed well vetted\nplan for what to do in such a scenario. Maybe we can think about that 10\nyears from now, but we probably shouldn't wait much longer than that. And\nmaybe it's as simple as: tweak the difficulty recalculation and then just\nrelease a soft fork aware Bitcoin version that rejects the new rules or\nrejects a specific existing post-soft-fork block. Would it be that simple?\n\nOn Sat, Mar 12, 2022, 07:35 Russell O'Connor via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Mar 11, 2022 at 9:03 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>>\n>> A major contender to the Speedy Trial design at the time was to mandate\n>>> eventual forced signalling, championed by luke-jr.  It turns out that, at\n>>> the time of that proposal, a large amount of hash power simply did not have\n>>> the firmware required to support signalling.  That activation proposal\n>>> never got broad consensus, and rightly so, because in retrospect we see\n>>> that the design might have risked knocking a significant fraction of mining\n>>> power offline if it had been deployed.  Imagine if the firmware couldn't be\n>>> quickly updated or imagine if the problem had been hardware related.\n>>>\n>>\n>>>> Yes, I like this solution too, with a little caveat: an easy mechanism\n>>>> for users to actively oppose a proposal.\n>>>> Luke alao talked about this.\n>>>> If users oppose, they should use activation as a trigger to fork out of\n>>>> the network by invalidating the block that produces activation.\n>>>> The bad scenario here is that miners want to deploy something but users\n>>>> don't want to.\n>>>> \"But that may lead to a fork\". Yeah, I know.\n>>>> I hope imagining a scenario in which developers propose something that\n>>>> most miners accept but some users reject is not taboo.\n>>>>\n>>>\n>>> This topic is not taboo.\n>>>\n>>> There are a couple of ways of opting out of taproot.  Firstly, users can\n>>> just not use taproot.  Secondly, users can choose to not enforce taproot\n>>> either by running an older version of Bitcoin Core or otherwise forking the\n>>> source code.  Thirdly, if some users insist on a chain where taproot is\n>>> \"not activated\", they can always softk-fork in their own rule that\n>>> disallows the version bits that complete the Speedy Trial activation\n>>> sequence, or alternatively soft-fork in a rule to make spending from (or\n>>> to) taproot addresses illegal.\n>>>\n>>\n>> Since it's about activation in general and not about taproot\n>> specifically, your third point is the one that applies.\n>> Users could have coordinated to have \"activation x\" never activated in\n>> their chains if they simply make a rule that activating a given proposal\n>> (with bip8) is forbidden in their chain.\n>> But coordination requires time.\n>>\n>\n> A mechanism of soft-forking against activation exists.  What more do you\n> want? Are we supposed to write the code on behalf of this hypothetical\n> group of users who may or may not exist for them just so that they can have\n> a node that remains stalled on Speedy Trial lockin?  That simply isn't\n> reasonable, but if you think it is, I invite you to create such a fork.\n>\n>\n>> Please, try to imagine an example for an activation that you wouldn't\n>> like yourself. Imagine it gets proposed and you, as a user, want to resist\n>> it.\n>>\n>\n> If I believe I'm in the economic majority then I'll just refuse to upgrade\n> my node, which was option 2. I don't know why you dismissed it.\n>\n> Not much can prevent a miner cartel from enforcing rules that users don't\n> want other than hard forking a replacement POW.  There is no effective\n> difference between some developers releasing a malicious soft-fork of\n> Bitcoin and the miners releasing a malicious version themselves.  And when\n> the miner cartel forms, they aren't necessarily going to be polite enough\n> to give a transparent signal of their new rules.  However, without the\n> economic majority enforcing their set of rules, the cartel continuously\n> risks falling apart from the temptation of transaction fees of the censored\n> transactions.\n>\n> On the other hand, If I find out I'm in the economic minority then I have\n> little choice but to either accept the existence of the new rules or sell\n> my Bitcoin.  Look, you cannot have the perfect system of money all by your\n> lonesome self.  Money doesn't have economic value if no one else wants to\n> trade you for it.  Just ask that poor user who YOLO'd his own taproot\n> activation in advance all by themselves.  I'm sure they think they've got\n> just the perfect money system, with taproot early and everything.  But now\n> their node is stuck at block 692261\n> <https://b10c.me/blog/007-spending-p2tr-pre-activation/> and hasn't made\n> progress since.  No doubt they are hunkered down for the long term,\n> absolutely committed to their fork and just waiting for the rest of the\n> world to come around to how much better their version of Bitcoin is than\n> the rest of us.\n>\n> Even though you've dismissed it, one of the considerations of taproot was\n> that it is opt-in for users to use the functionality.  Future soft-forks\n> ought to have the same considerations to the extent possible.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220312/95ceba55/attachment-0001.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-17T12:18:11",
                "message_text_only": "On Sat, Mar 12, 2022 at 7:34 PM Billy Tetrud via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> >  If I find out I'm in the economic minority then I have little choice but to either accept the existence of the new rules or sell my Bitcoin\n>\n> I do worry about what I have called a \"dumb majority soft fork\". This is where, say, mainstream adoption has happened, some crisis of some magnitude happens that convinces a lot of people something needs to change now. Let's say it's another congestion period where fees spike for months. Getting into and out of lighting is hard and maybe even the security of lightning's security model is called into question because it would either take too long to get a transaction on chain or be too expensive. Panicy people might once again think something like \"let's increase the block size to 1GB, then we'll never have this problem again\". This could happen in a segwit-like soft fork.\n\nI guess this is a better explained example for a hypothetical \"evil\nfork\" that may sound more concrete and plausible to some people than\nmy own, which isn't that different. Thanks.\n\n> In a future where Bitcoin is the dominant world currency, it might not be unrealistic to imagine that an economic majority might not understand why such a thing would be so dangerous, or think the risk is low enough to be worth it. At that point, we in the economic minority would need a plan to hard fork away. One wouldn't necessarily need to sell all their majority fork Bitcoin, but they could.\n>\n> That minority fork would of course need some mining power. How much? I don't know, but we should think about how small of a minority chain we could imagine might be worth saving. Is 5% enough? 1%? How long would the chain stall if hash power dropped to 1%?\n\nIn perfect competition the mining power costs per chain tends to equal\nthe rewards offered by that chain, both in subsidy and transaction\nfees.\nFor example, if chain A gets a reward 10 times as valuable as chain\nB's reward, then one should expect it to get 10 times more hashrate\ntoo.\nOf course, perfect competition is just a theoretical concept though."
            },
            {
                "author": "Kate Salazar",
                "date": "2022-03-23T22:34:21",
                "message_text_only": "Hey\n\nOn Sat, Mar 12, 2022 at 7:34 PM Billy Tetrud via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> >  If I find out I'm in the economic minority then I have little choice but to either accept the existence of the new rules or sell my Bitcoin\n>\n> I do worry about what I have called a \"dumb majority soft fork\". This is where, say, mainstream adoption has happened, some crisis of some magnitude happens that convinces a lot of people something needs to change now. Let's say it's another congestion period where fees spike for months. Getting into and out of lighting is hard and maybe even the security of lightning's security model is called into question because it would either take too long to get a transaction on chain or be too expensive. Panicy people might once again think something like \"let's increase the block size to 1GB, then we'll never have this problem again\". This could happen in a segwit-like soft fork.\n\nBitcoin has never been mainstream, and yet somehow you have known\nwhere you needed to be, all the time. The same will apply then. This\nis a non-issue.\n\n>\n> In a future where Bitcoin is the dominant world currency, it might not be unrealistic to imagine that an economic majority might not understand why such a thing would be so dangerous, or think the risk is low enough to be worth it. At that point, we in the economic minority would need a plan to hard fork away. One wouldn't necessarily need to sell all their majority fork Bitcoin, but they could.\n\nAgain, Bitcoin _is_ not an economic majority. Has never been. But\nsmart money always wins. This is a non-issue.\n\nIf one doesn't know where to be, there's the option to defer choices.\nI was a big blocker myself, and yet I'm fairly OK even after being so\nwrong. Even if forced to choose because of evil deadlines (which is\nreally unlikely), a divide strategy should be helpful enough to cut\nlosses in those cases.\n\n>\n> That minority fork would of course need some mining power. How much? I don't know, but we should think about how small of a minority chain we could imagine might be worth saving. Is 5% enough? 1%? How long would the chain stall if hash power dropped to 1%?\n>\n> TBH I give the world a ~50% chance that something like this happens in the next 100 years. Maybe Bitcoin will ossify and we'll lose all the people that had deep knowledge on these kinds of things because almost no one's actively working on it. Maybe the crisis will be too much for people to remain rational and think long term. Who knows? But I think that at some point it will become dangerous if there isn't a well discussed well vetted plan for what to do in such a scenario. Maybe we can think about that 10 years from now, but we probably shouldn't wait much longer than that. And maybe it's as simple as: tweak the difficulty recalculation and then just release a soft fork aware Bitcoin version that rejects the new rules or rejects a specific existing post-soft-fork block. Would it be that simple?\n\nMaybe this is worth thinking about, but really, there'll always be\nsmart enough people around. However\ndumb people sometimes are not as dangerous as we think, and\nsmart people sometimes are not as flawless as we desire to take for granted.\n\n>\n> On Sat, Mar 12, 2022, 07:35 Russell O'Connor via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> On Fri, Mar 11, 2022 at 9:03 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>>>\n>>>\n>>>> A major contender to the Speedy Trial design at the time was to mandate eventual forced signalling, championed by luke-jr.  It turns out that, at the time of that proposal, a large amount of hash power simply did not have the firmware required to support signalling.  That activation proposal never got broad consensus, and rightly so, because in retrospect we see that the design might have risked knocking a significant fraction of mining power offline if it had been deployed.  Imagine if the firmware couldn't be quickly updated or imagine if the problem had been hardware related.\n>>>>>\n>>>>>\n>>>>> Yes, I like this solution too, with a little caveat: an easy mechanism for users to actively oppose a proposal.\n>>>>> Luke alao talked about this.\n>>>>> If users oppose, they should use activation as a trigger to fork out of the network by invalidating the block that produces activation.\n>>>>> The bad scenario here is that miners want to deploy something but users don't want to.\n>>>>> \"But that may lead to a fork\". Yeah, I know.\n>>>>> I hope imagining a scenario in which developers propose something that most miners accept but some users reject is not taboo.\n>>>>\n>>>>\n>>>> This topic is not taboo.\n>>>>\n>>>> There are a couple of ways of opting out of taproot.  Firstly, users can just not use taproot.  Secondly, users can choose to not enforce taproot either by running an older version of Bitcoin Core or otherwise forking the source code.  Thirdly, if some users insist on a chain where taproot is \"not activated\", they can always softk-fork in their own rule that disallows the version bits that complete the Speedy Trial activation sequence, or alternatively soft-fork in a rule to make spending from (or to) taproot addresses illegal.\n>>>\n>>>\n>>> Since it's about activation in general and not about taproot specifically, your third point is the one that applies.\n>>> Users could have coordinated to have \"activation x\" never activated in their chains if they simply make a rule that activating a given proposal (with bip8) is forbidden in their chain.\n>>> But coordination requires time.\n>>\n>>\n>> A mechanism of soft-forking against activation exists.  What more do you want? Are we supposed to write the code on behalf of this hypothetical group of users who may or may not exist for them just so that they can have a node that remains stalled on Speedy Trial lockin?  That simply isn't reasonable, but if you think it is, I invite you to create such a fork.\n>>\n>>>\n>>> Please, try to imagine an example for an activation that you wouldn't like yourself. Imagine it gets proposed and you, as a user, want to resist it.\n>>\n>>\n>> If I believe I'm in the economic majority then I'll just refuse to upgrade my node, which was option 2. I don't know why you dismissed it.\n>>\n>> Not much can prevent a miner cartel from enforcing rules that users don't want other than hard forking a replacement POW.  There is no effective difference between some developers releasing a malicious soft-fork of Bitcoin and the miners releasing a malicious version themselves.  And when the miner cartel forms, they aren't necessarily going to be polite enough to give a transparent signal of their new rules.  However, without the economic majority enforcing their set of rules, the cartel continuously risks falling apart from the temptation of transaction fees of the censored transactions.\n>>\n>> On the other hand, If I find out I'm in the economic minority then I have little choice but to either accept the existence of the new rules or sell my Bitcoin.  Look, you cannot have the perfect system of money all by your lonesome self.  Money doesn't have economic value if no one else wants to trade you for it.  Just ask that poor user who YOLO'd his own taproot activation in advance all by themselves.  I'm sure they think they've got just the perfect money system, with taproot early and everything.  But now their node is stuck at block 692261 and hasn't made progress since.  No doubt they are hunkered down for the long term, absolutely committed to their fork and just waiting for the rest of the world to come around to how much better their version of Bitcoin is than the rest of us.\n>>\n>> Even though you've dismissed it, one of the considerations of taproot was that it is opt-in for users to use the functionality.  Future soft-forks ought to have the same considerations to the extent possible.\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-15T17:21:29",
                "message_text_only": "Boker tov bitcoin devs,\n\nA mechanism of soft-forking against activation exists.  What more do you\n> want?\n>\n\nAgreed -- that should be enough.\n\n\n\n> Are we supposed to write the code on behalf of this hypothetical group of\n> users who may or may not exist for them just so that they can have a node\n> that remains stalled on Speedy Trial lockin?\n>\nThat simply isn't reasonable, but if you think it is, I invite you to\n> create such a fork.\n>\n\nDisagree.\n\nIt is a reasonable ask.\n\nI've done it in about 40 lines of python:\nhttps://github.com/jeremyrubin/forkd\n\nMerry Christmas Jorge, please vet the code carefully before running.\n\nPeace,\n\nJeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220315/07cb6ea8/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-17T04:17:20",
                "message_text_only": "@Aj Your steps seem reasonable. I definitely agree step one (talking to\neach other) is obviously the ideal solution, when it works.\n\nStep 2 (futures market) is the option I would say I understand the least.\nIn any case, a futures market seems like it only incorporates the\nopinions/predictions of the group of people willing to bet money on things\nlike this. This is likely to be a rather small group of particular types of\npeople. I find it a bit difficult to reconcile the theories that betting\nrings like this are good at predicting against the inherent selection bias\nof the group of betting individuals. Going just by number of individuals\n(or probably even by amount of currency risked) this seems like a futures\nmarket would inherently be a small and biased group. Potentially useful,\nbut I wouldn't assume that it could be taken stand-alone as a proxy for\nconsensus.\n\nI'm curious what you think about a coin-weighted poll like I suggested here\n<https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/019022.html>\nbeing\nadded to that list of steps? Surely this would be a broader group of people\nthan a futures market, tho still obviously a group subject to selection\nbias.\n\n@jeremy drops the bomb. I'm sure Jorge will be running this within the\nyear.\n\n\n\nOn Tue, Mar 15, 2022 at 12:25 PM Jeremy Rubin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Boker tov bitcoin devs,\n>\n> A mechanism of soft-forking against activation exists.  What more do you\n>> want?\n>>\n>\n> Agreed -- that should be enough.\n>\n>\n>\n>> Are we supposed to write the code on behalf of this hypothetical group of\n>> users who may or may not exist for them just so that they can have a node\n>> that remains stalled on Speedy Trial lockin?\n>>\n> That simply isn't reasonable, but if you think it is, I invite you to\n>> create such a fork.\n>>\n>\n> Disagree.\n>\n> It is a reasonable ask.\n>\n> I've done it in about 40 lines of python:\n> https://github.com/jeremyrubin/forkd\n>\n> Merry Christmas Jorge, please vet the code carefully before running.\n>\n> Peace,\n>\n> Jeremy\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220316/0bf6deff/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-18T18:36:03",
                "message_text_only": "On Tue, Mar 15, 2022 at 6:25 PM Jeremy Rubin via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Boker tov bitcoin devs,\n\nI don't undesrtand what that means, sorry\n\n>> A mechanism of soft-forking against activation exists.  What more do you want?\n>\n>\n> Agreed -- that should be enough.\n\nNo, resistance should ideally be a priori, not a posteriori.\n\n>\n>>\n>> Are we supposed to write the code on behalf of this hypothetical group of users who may or may not exist for them just so that they can have a node that remains stalled on Speedy Trial lockin?\n>>\n>> That simply isn't reasonable, but if you think it is, I invite you to create such a fork.\n>\n>\n> Disagree.\n>\n> It is a reasonable ask.\n>\n> I've done it in about 40 lines of python: https://github.com/jeremyrubin/forkd\n>\n> Merry Christmas Jorge, please vet the code carefully before running.\n\n40 lines of python code should be easy to bet even if the author was\nvery bad at writing readable code and obfuscated his code on purpose.\nI don't know if it's the case, because, sorry, I'm not reviewing your\ncode at the moment.\n\"Vet the code carefully before running\" strikes me as arrogant and\ncondescending. as if you were implying my engineering capacity was\nvery limited. If you say these things to me publicly, I can only only\nimagine what you have told other devs behind my back about my capacity\n(or even my ideology) if they ever asked (or perhaps without them\nasking). I really hope you haven't lied to anyone about my ideology,\nJ.\nPerhaps I do have \"prosectution complex\" with you indeed. Not with\nRussel, but with you.\nAfter all, I've publicly say I don't trust you, haven't I?\nBut, again, what do I know about psychology?\n\nGoing back on topic, the reason I won't review your code is because\nyou have rushed a design before understanding the analysis.\nNo, I'm not asking for a stalled mechanism for speedy trial, I don't\nwant speedy trial.\nWe disagree on the analysis of the problem to solve, that's why we\ndisagree on the design for the solution.\n\nhttps://en.wikipedia.org/wiki/Systems_development_life_cycle#Analysis\n\nAnyway, perhaps I look at the code in the future if your proposal\nconsensus change seems to prosper.\n\nRegarding \"merry christmas\"...what the f are you talking about? it's\nnot Christmas time and neither you or me are christians, are you?\nIf this is some sort of riddle or joke, we really must have very\ndifferent senses of humor, because I don't get it.\nCome on, J, let's both try to stay on topic or people will start to\ncorrectly point out that we both negatively discriminate each other\nfor offtopic reasons, be them reasonably justified or not.\n\n> Peace,\n\nAma, y ensancha el alma.\n\n> Jeremy\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-17T12:08:25",
                "message_text_only": "On Sat, Mar 12, 2022 at 2:35 PM Russell O'Connor via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> On Fri, Mar 11, 2022 at 9:03 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> A mechanism of soft-forking against activation exists.  What more do you want? Are we supposed to write the code on behalf of this hypothetical group of users who may or may not exist for them just so that they can have a node that remains stalled on Speedy Trial lockin?  That simply isn't reasonable, but if you think it is, I invite you to create such a fork.\n\nI want BIP+LOT=true to be used. I want speedy trial not to be used.\nLuke wrote the code to resist BIP8+LOT=true, and if he didn't, I could\nwrite it myself, yes.\nIf you think that's not reasonable code to ever run, I don't think\nyou're really getting the \"softfork THAT YOU OPPOSE\" part of the\nhypothetical right. Let me try to help with an example, although I\nhope we don't get derailed in the implementation details of the\nhypothetical evil proposal.\n\nSuppose someone proposes a weight size limit increase by a extension\nblock softfork.\nOr instead of that, just imagine the final version of the covenants\nproposal has a backdoor in it or something.\n\n\nWould you rather that proposal be deployed with speedy trial\nactivation or with BIP8+LOT=true activation?\n\n>>\n>> Please, try to imagine an example for an activation that you wouldn't like yourself. Imagine it gets proposed and you, as a user, want to resist it.\n>\n>\n> If I believe I'm in the economic majority then I'll just refuse to upgrade my node, which was option 2. I don't know why you dismissed it.\n\nNot upgrading your node doesn't prevent the softfork from being\nactivated in your chain.\nA softfork may affect you indirectly even if you don't use the new\nfeatures yourself directly.\nYou may chose to stay in the old chain even if you don't consider\nyou're \"in the economic majority\" at that moment.\n\n> Not much can prevent a miner cartel from enforcing rules that users don't want other than hard forking a replacement POW.  There is no effective difference between some developers releasing a malicious soft-fork of Bitcoin and the miners releasing a malicious version themselves.  And when the miner cartel forms, they aren't necessarily going to be polite enough to give a transparent signal of their new rules.  However, without the economic majority enforcing their set of rules, the cartel continuously risks falling apart from the temptation of transaction fees of the censored transactions.\n\nIt is true that a mining cartel doesn't need to use speedy trial or\nBIP8+LOT=true to apply rule changes they want just because we do.\nBut they would do if they wanted to maintain the appearance of benevolence.\n\n> On the other hand, If I find out I'm in the economic minority then I have little choice but to either accept the existence of the new rules or sell my Bitcoin.  Look, you cannot have the perfect system of money all by your lonesome self.  Money doesn't have economic value if no one else wants to trade you for it.  Just ask that poor user who YOLO'd his own taproot activation in advance all by themselves.  I'm sure they think they've got just the perfect money system, with taproot early and everything.  But now their node is stuck at block 692261 and hasn't made progress since.  No doubt they are hunkered down for the long term, absolutely committed to their fork and just waiting for the rest of the world to come around to how much better their version of Bitcoin is than the rest of us.\n\nWell, you could also have the option to stay in the old chain with the\neconomic minority, it doesn't have to be you alone.\nWe agree that one person alone can't use a currency.\n\n> Even though you've dismissed it, one of the considerations of taproot was that it is opt-in for users to use the functionality.  Future soft-forks ought to have the same considerations to the extent possible.\n\nWell, the same could be said about segwit. And yet all the\nconsequences of the change are not opt in.\nFor example, segwit contained a block size limit increase.\nSure, you can just not validate the witnesses, but then you're no\nlonger a full node."
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-17T15:38:53",
                "message_text_only": "@Jorge\n> Any user polling system is going to be vulnerable to sybil attacks.\n\nNot the one I'll propose right here. What I propose specifically is\na coin-weighted signature-based poll with the following components:\nA. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90%\nsupport:10%}> for each UTXO they want to respond to the poll with.\nB. A signed message like that is valid only while that UTXO has not been\nspent.\nC. Poll results are considered only at each particular block height, where\nthe support and opposition responses are weighted by the UTXO amount (and\nthe support/oppose fraction in the message). This means you'd basically see\na rolling poll through the blockchain as new signed poll messages come in\nand as their UTXOs are spent.\n\nThis is not vulnerable to sybil attacks because it requires access to UTXOs\nand response-weight is directly tied to UTXO amount. If someone signs a\npoll message with a key that can unlock (or is in some other designated way\nassociated with) a UTXO, and then spends that UTXO, their poll response\nstops being counted for all block heights after the UTXO was spent.\n\nWhy put support and oppose fractions in the message? Who would want to both\nsupport and oppose something? Any multiple participant UTXO would. Eg\nlightning channels would, where each participant disagrees with the other.\nThey need to sign together, so they can have an agreement to sign for the\nfractions that match their respective channel balances (using a force\nchannel close as a last resort against an uncooperative partner as usual).\n\nThis does have the potential issue of public key exposure prior to spending\nfor current addresses. But that could be fixed with a new address type that\nhas two public keys / spend paths: one for spending and one for signing.\n\n> In perfect competition the mining power costs per chain tends to equal\nthe rewards offered by that chain, both in subsidy and transaction fees.\n\nAgreed, but it takes time for an economic shock to reach its new\nequilibrium. That period of time, which might be rather precarious, should\nbe considered in a plan to preserve a minority fork.\n\n> Would you rather that proposal be deployed with speedy trial activation\nor with BIP8+LOT=true activation?\n\nFor a proposal I don't want to succeed, I absolutely would prefer speedy\ntrial over BIP8+LOT=true. Speedy trial at 90% signaling threshold can\nquickly determine that the proposal (hopefully) does not have enough\nconsensus among miners. By contrast, BIP8+LOT=true could polarize the\ndebate, worsening the community's ability to communicate and talk through\nissues. It would also basically guarantee that a fork happens, which in the\nbest case (in my hypothetical point of view where I don't like the\nproposal) would mean some small minority forks off the network, which\nreduces the main chain's value somewhat (at least temporarily). Worst case\na small majority forces the issue at near 50% which would cause all sorts\nof blockchain issues and would have a high probability of leading to a\nhardfork by the minority.\n\nAll this sounds rather more tenable with speedy trial. Any proposal has\nless chance of causing an actual fork (soft or otherwise) with speedy trial\nvs LOT=true. LOT=true guarantees a fork if even a single person is running\nit. LOT=true could certainly come in handy to initiate a UASF, but IMO\nthat's better left as a plan B or C.\n\n> segwit... all the consequences of the change are not opt in.\n\nI definitely agree there. The consequences of a soft fork are not always\nopt in. That's basically what my example of a \"dumb majority soft fork\" is,\nand sounds like what your \"evil fork\" basically is.\n\nOn Thu, Mar 17, 2022 at 7:19 AM Jorge Tim\u00f3n via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Sat, Mar 12, 2022 at 2:35 PM Russell O'Connor via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> > On Fri, Mar 11, 2022 at 9:03 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> > A mechanism of soft-forking against activation exists.  What more do you\n> want? Are we supposed to write the code on behalf of this hypothetical\n> group of users who may or may not exist for them just so that they can have\n> a node that remains stalled on Speedy Trial lockin?  That simply isn't\n> reasonable, but if you think it is, I invite you to create such a fork.\n>\n> I want BIP+LOT=true to be used. I want speedy trial not to be used.\n> Luke wrote the code to resist BIP8+LOT=true, and if he didn't, I could\n> write it myself, yes.\n> If you think that's not reasonable code to ever run, I don't think\n> you're really getting the \"softfork THAT YOU OPPOSE\" part of the\n> hypothetical right. Let me try to help with an example, although I\n> hope we don't get derailed in the implementation details of the\n> hypothetical evil proposal.\n>\n> Suppose someone proposes a weight size limit increase by a extension\n> block softfork.\n> Or instead of that, just imagine the final version of the covenants\n> proposal has a backdoor in it or something.\n>\n>\n> Would you rather that proposal be deployed with speedy trial\n> activation or with BIP8+LOT=true activation?\n>\n> >>\n> >> Please, try to imagine an example for an activation that you wouldn't\n> like yourself. Imagine it gets proposed and you, as a user, want to resist\n> it.\n> >\n> >\n> > If I believe I'm in the economic majority then I'll just refuse to\n> upgrade my node, which was option 2. I don't know why you dismissed it.\n>\n> Not upgrading your node doesn't prevent the softfork from being\n> activated in your chain.\n> A softfork may affect you indirectly even if you don't use the new\n> features yourself directly.\n> You may chose to stay in the old chain even if you don't consider\n> you're \"in the economic majority\" at that moment.\n>\n> > Not much can prevent a miner cartel from enforcing rules that users\n> don't want other than hard forking a replacement POW.  There is no\n> effective difference between some developers releasing a malicious\n> soft-fork of Bitcoin and the miners releasing a malicious version\n> themselves.  And when the miner cartel forms, they aren't necessarily going\n> to be polite enough to give a transparent signal of their new rules.\n> However, without the economic majority enforcing their set of rules, the\n> cartel continuously risks falling apart from the temptation of transaction\n> fees of the censored transactions.\n>\n> It is true that a mining cartel doesn't need to use speedy trial or\n> BIP8+LOT=true to apply rule changes they want just because we do.\n> But they would do if they wanted to maintain the appearance of benevolence.\n>\n> > On the other hand, If I find out I'm in the economic minority then I\n> have little choice but to either accept the existence of the new rules or\n> sell my Bitcoin.  Look, you cannot have the perfect system of money all by\n> your lonesome self.  Money doesn't have economic value if no one else wants\n> to trade you for it.  Just ask that poor user who YOLO'd his own taproot\n> activation in advance all by themselves.  I'm sure they think they've got\n> just the perfect money system, with taproot early and everything.  But now\n> their node is stuck at block 692261 and hasn't made progress since.  No\n> doubt they are hunkered down for the long term, absolutely committed to\n> their fork and just waiting for the rest of the world to come around to how\n> much better their version of Bitcoin is than the rest of us.\n>\n> Well, you could also have the option to stay in the old chain with the\n> economic minority, it doesn't have to be you alone.\n> We agree that one person alone can't use a currency.\n>\n> > Even though you've dismissed it, one of the considerations of taproot\n> was that it is opt-in for users to use the functionality.  Future\n> soft-forks ought to have the same considerations to the extent possible.\n>\n> Well, the same could be said about segwit. And yet all the\n> consequences of the change are not opt in.\n> For example, segwit contained a block size limit increase.\n> Sure, you can just not validate the witnesses, but then you're no\n> longer a full node.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220317/082e92f9/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-18T23:01:43",
                "message_text_only": "Good morning Billy,\n\n> @Jorge\n> > Any user polling system is going to be vulnerable to sybil attacks.\n>\n> Not the one I'll propose right here. What I propose specifically is a\u00a0coin-weighted signature-based poll with the following components:\n> A. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90% support:10%}> for each UTXO they want to respond to the poll with.\n> B. A signed message like that is valid only while that UTXO has not been spent.\n> C. Poll results are considered only at each particular block height, where the support and opposition responses are weighted by the UTXO amount (and the support/oppose fraction in the message). This means you'd basically see a rolling poll through the blockchain as new signed poll messages come in and as their UTXOs are spent.\u00a0\n>\n> This is not vulnerable to sybil attacks because it requires access to UTXOs and response-weight is directly tied to UTXO amount. If someone signs a poll message with a key that can unlock (or is in some other designated way associated with) a UTXO, and then spends that UTXO, their poll response stops being counted for all block heights after the UTXO was spent.\u00a0\n>\n> Why put support and oppose fractions in the message? Who would want to both support and oppose something? Any multiple participant UTXO would. Eg lightning channels would, where each participant disagrees with the other. They need to sign together, so they can have an agreement to sign for the fractions that match their respective channel balances (using a force channel close as a last resort against an uncooperative partner as usual).\u00a0\n\nThis does not quite work, as lightning channel balances can be changed at any time.\nI might agree that you have 90% of the channel and I have 10% of the channel right now, but if you then send a request to forward your funds out, I need to be able to invalidate the previous signal, one that is tied to the fulfillment of the forwarding request.\nThis begins to add complexity.\n\nMore pointedly, if the signaling is done onchain, then a forward on the LN requires that I put up invalidations of previous signals, also onchain, otherwise you could cheaty cheat your effective balance by moving your funds around.\nBut the point of LN is to avoid putting typical everyday forwards onchain.\n\n> This does have the potential issue of public key exposure prior to spending for current addresses. But that could be fixed with a new address type that has two public keys / spend paths: one for spending and one for signing.\u00a0\n\nThis issue is particularly relevant to vault constructions.\nTypically a vault has a \"cold\" key that is the master owner of the fund, with \"hot\" keys having partial access.\nSemantically, we would consider the \"cold\" key to be the \"true\" owner of the fund, with \"hot\" key being delegates who are semi-trusted, but not as trusted as the \"cold\" key.\n\nSo, we should consider a vote from the \"cold\" key only.\nHowever, the point is that the \"cold\" key wants to be kept offline as much as possible for security.\n\nI suppose the \"cold\" key could be put online just once to create the signal message, but vault owners might not want to vote because of the risk, and their weight might be enough to be important in your voting scheme (consider that the point of vaults is to protect large funds).\n\n\nA sub-issue here with the spend/signal pubkey idea is that if I need to be able to somehow indicate that a long-term-cold-storage UTXO has a signaling pubkey, I imagine this mechanism of indioating might itself require a softfork, so you have a chicken-and-egg problem...\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-21T03:41:42",
                "message_text_only": "Good Evening ZmnSCPxj,\n\n>  I need to be able to invalidate the previous signal, one that is tied to\nthe fulfillment of the forwarding request.\n\nYou're right that there's some nuance there. You could add a block hash\ninto the poll message and define things so any subsequent poll message sent\nwith a newer block hash overrides the old poll message at the block with\nthat hash and later blocks. That way if a channel balance changes\nsignificantly, a new poll message can be sent out.\n\nOr you could remove the ability to specify fractional support/opposition\nand exclude multiparty UTXOs from participation. I tend to like the idea of\nthe possibility of full participation tho, even in a world that mainly uses\nlightning.\n\n> if the signaling is done onchain\n\nI don't think any of this signaling needs to be done on-chain. Anyone who\nwants to keep a count of the poll can simply collect together all these\npoll messages and count up the weighted preferences. Yes, it would be\npossible for one person to send out many conflicting poll messages, but\nthis could be handled without any commitment to the blockchain. A simple\nthing to do would be to simply invalidate poll messages that conflict (ie\ninclude them both in your list of counted messages, but ignore them in your\nweighted-sums of poll preferences). As long as these polls are simply used\nto inform action rather than to trigger action, it should be ok that\nsomeone can produce biased incomplete counts, since anyone can show a\nprovably more complete set (a superset) of poll messages. Also, since this\nwould generally be a time-bound thing, where this poll information would\nfor example be used to gauge support for a soft fork, there isn't much of a\nneed to keep the poll messages on an immutable ledger. Old poll data is\ninherently not very practically useful compared to recent poll data. So we\ncan kind of side step things like history attacks by simply ignoring polls\nthat aren't recent.\n\n> Semantically, we would consider the \"cold\" key to be the \"true\" owner of\nthe fund, with \"hot\" key being delegates who are semi-trusted, but not as\ntrusted as the \"cold\" key.\n\nI'm not sure I agree with those semantics as a hard rule. I don't consider\na \"key\" to be an owner of anything. A person owns a key, which gives them\naccess to funds. A key is a tool, and the owner of a key or wallet vault\ncan define whatever semantics they want. If they want to designate a hot\nkey as their poll-signing key, that's their prerogative. If they want to\nrequire a cold-key as their message-signing key or even require multisig\nsigning, that's up to them as well. You could even mirror wallet-vault\nconstructs by overriding a poll message signed with fewer key using one\nsigned with more keys. The trade offs you bring up are reasonable\nconsiderations, and I think which trade offs to choose may vary by the\nindividual in question and their individual situation. However, I think the\ntime-bound and non-binding nature of a poll makes the risks here pretty\nsmall for most situations you would want to use this in (eg in a soft-fork\npoll). It should be reasonable to consider any signed poll message valid,\nregardless of possibilities of theft or key renting shinanigans. Nacho keys\nnacho coins would of course be important in this scenario.\n\n>  if I need to be able to somehow indicate that a long-term-cold-storage\nUTXO has a signaling pubkey, I imagine this mechanism of indioating might\nitself require a softfork, so you have a chicken-and-egg problem...\n\nIf such a thing did need a soft fork, the chicken and egg question would be\neasy to answer: the soft fork comes first. We've done soft forks before\nhaving this mechanism, and if necessary we could do another one to enable\nit.\n\nHowever, I think taproot can enable this mechanism without a soft fork. It\nshould be possible to include a taproot leaf that has the data necessary to\nvalidate a signaling signature. The tapleaf would contain an invalid script\nthat has an alternative interpretation, where your poll message can include\nthe merkle path to tapleaf (the invalid-script), and the data at that leaf\nwould be a public key you can then verify the signaling signature against.\n\n@vjudeu\n\n> It should not be expressed in percents, but in amounts\n\nAgreed. You make a good case for that.\n\n> it could be just some kind of transaction, where you have utxo_id just as\ntransaction input, amount of coins as some output, and then add your\nmessage as \"OP_RETURN <commitment>\" in your input, in this way your\nsignature would be useless in a different context than voting.\n\nI don't quite understand this part. I don't understand how this would make\nyour signature useless in a different context. Could you elaborate?\n\n> it does not really matter if you store that commitments on-chain to\npreserve signalling results in consensus rules or if there would be some\nseparate chain for storing commitments and nothing else\n\nI don't think any kind of chain is necessary to store this data. I'm\nprimarily suggesting this as a method to help the debate about a soft fork\nhave better information about what broader users think about a particular\nsoft fork proposal, so such data would simply inform whether or not we\ndecide to continue work on an upgrade. I don't think you'd want to require\nany validation of this data by all full nodes, because the data could be\nhundreds of gigabytes in size (let's say 1 billion people respond). You'd\nhave to run some kind of random sampling (more like actual proof of stake)\nto get this data down to a manageable size.\n\n> It would be Proof of Stake, where users would put their coins at stake to\nvote.\n\nSure, as long as by this you mean simply proof of coin ownership. Just as\nany bitcoin transaction involves proof of coin ownership.\n\nI was pretty careful to avoid the word \"voting\", since I'm not proposing\nthat this be used with definite thresholds that trigger action, but more of\nan information gathering mechanism. Perhaps one day it could be used for\nsomething akin to voting, but certainly if we were going to implement this\nto help decide on the next soft fork, it would very likely be a quite\nbiased set of responders. We would want to take that into account when\ndeciding how to interpret the data. Even with biased data tho, it could be\na useful tool for resolving some contention.\n\nBut on that note, I was thinking that it might be interesting to have an\noptional human readable message into these poll messages. Those messages\ncould be then read through to gain a better understanding of why people are\nsupporting and why people are rejecting a particular thing. It could inform\nhow we might change how we explain a technical change to make it easier for\nless technical folks (who don't post on twitter) to understand. It could\npotentially give insight into an otherwise quiet majority (or large\nminority).\n\n> it sounds similar to \"Merged Signing\"\n\nInteresting. I'm not sure I fully grok his idea, but I think he was\nsuggesting that a proof of stake consensus protocol pay attention to\nbitcoin transactions formatted in a particular way. I think I've hopefully\nclarified above why the idea I'm suggesting is rather different from this\n(eg in that no special commitments need to be made).\n\nCheers,\nBT\n\n\n\n\n\n\n\nOn Fri, Mar 18, 2022 at 6:01 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Billy,\n>\n> > @Jorge\n> > > Any user polling system is going to be vulnerable to sybil attacks.\n> >\n> > Not the one I'll propose right here. What I propose specifically is\n> a coin-weighted signature-based poll with the following components:\n> > A. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90%\n> support:10%}> for each UTXO they want to respond to the poll with.\n> > B. A signed message like that is valid only while that UTXO has not been\n> spent.\n> > C. Poll results are considered only at each particular block height,\n> where the support and opposition responses are weighted by the UTXO amount\n> (and the support/oppose fraction in the message). This means you'd\n> basically see a rolling poll through the blockchain as new signed poll\n> messages come in and as their UTXOs are spent.\n> >\n> > This is not vulnerable to sybil attacks because it requires access to\n> UTXOs and response-weight is directly tied to UTXO amount. If someone signs\n> a poll message with a key that can unlock (or is in some other designated\n> way associated with) a UTXO, and then spends that UTXO, their poll response\n> stops being counted for all block heights after the UTXO was spent.\n> >\n> > Why put support and oppose fractions in the message? Who would want to\n> both support and oppose something? Any multiple participant UTXO would. Eg\n> lightning channels would, where each participant disagrees with the other.\n> They need to sign together, so they can have an agreement to sign for the\n> fractions that match their respective channel balances (using a force\n> channel close as a last resort against an uncooperative partner as usual).\n>\n> This does not quite work, as lightning channel balances can be changed at\n> any time.\n> I might agree that you have 90% of the channel and I have 10% of the\n> channel right now, but if you then send a request to forward your funds\n> out, I need to be able to invalidate the previous signal, one that is tied\n> to the fulfillment of the forwarding request.\n> This begins to add complexity.\n>\n> More pointedly, if the signaling is done onchain, then a forward on the LN\n> requires that I put up invalidations of previous signals, also onchain,\n> otherwise you could cheaty cheat your effective balance by moving your\n> funds around.\n> But the point of LN is to avoid putting typical everyday forwards onchain.\n>\n> > This does have the potential issue of public key exposure prior to\n> spending for current addresses. But that could be fixed with a new address\n> type that has two public keys / spend paths: one for spending and one for\n> signing.\n>\n> This issue is particularly relevant to vault constructions.\n> Typically a vault has a \"cold\" key that is the master owner of the fund,\n> with \"hot\" keys having partial access.\n> Semantically, we would consider the \"cold\" key to be the \"true\" owner of\n> the fund, with \"hot\" key being delegates who are semi-trusted, but not as\n> trusted as the \"cold\" key.\n>\n> So, we should consider a vote from the \"cold\" key only.\n> However, the point is that the \"cold\" key wants to be kept offline as much\n> as possible for security.\n>\n> I suppose the \"cold\" key could be put online just once to create the\n> signal message, but vault owners might not want to vote because of the\n> risk, and their weight might be enough to be important in your voting\n> scheme (consider that the point of vaults is to protect large funds).\n>\n>\n> A sub-issue here with the spend/signal pubkey idea is that if I need to be\n> able to somehow indicate that a long-term-cold-storage UTXO has a signaling\n> pubkey, I imagine this mechanism of indioating might itself require a\n> softfork, so you have a chicken-and-egg problem...\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220320/214af426/attachment.html>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-21T15:56:03",
                "message_text_only": "> I don't quite understand this part. I don't understand how this would make your signature useless in a different context. Could you elaborate?\n\nIt is simple. If you vote by making transactions, then someone could capture that and broadcast to nodes. If your signature is \"useless in a different context\", then you can only send that to your network. If it will be sent anywhere else, it will be invalid, so also useless. Another reason to sign transactions and not just some custom data is to make it compatible with \"signet way of making signatures\", the same as used in signet challenge.\n\n> I don't think any kind of chain is necessary to store this data.\n\nEven if it is not needed, it is kind of \"free\" if you take transaction size into account. Because each person moving coins on-chain could attach \"OP_RETURN <commitment>\" in TapScript, just to save commitments. Then, even if someone is not in your network from the very beginning, that person could still collect commitments and find out how they are connected with on-chain transactions.\n\n> Perhaps one day it could be used for something akin to voting, but certainly if we were going to implement this to help decide on the next soft fork, it would very likely be a quite biased set of responders.\n\nIf it will be ever implemented, it should be done in a similar way as difficulty: if you want 90%, you should calculate, what amount in satoshis is needed to reach that 90%, and update it every two weeks, based on all votes. In this way, you reduce floating-point operations to a bare minimum, and have a system, where you can compare uint64 amounts to quickly get \"yes/no\" answer to the question, if something should be triggered (also, you can compress it to 32 bits in the same way as 256-bit target is compressed).\n\n> But on that note, I was thinking that it might be interesting to have an optional human readable message into these poll messages.\n\nAs I said, \"OP_RETURN <commitment>\" inside TapScript is enough to produce all commitments of arbitrary size for \"free\", so that on-chain transaction size is constant, no matter how large that commitment is. And about storage: you could create a separate chain for that, you could store that in the same way as LN nodes store data, you could use something else, it doesn't really matter, because on-chain commitments could be constructed in the same way (also, as long as the transaction creator keeps those commitments as a secret, there is no way to get them; that means you can add them later if needed and easily pretend that \"it was always possible\").\n\n\nOn 2022-03-21 10:17:29 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nGood Evening ZmnSCPxj,\n\n\n>\u00a0 I need to be able to invalidate the previous signal, one that is tied to the fulfillment of the forwarding request.\n\n\nYou're right that there's some nuance there. You could add a block hash into the poll message and define things so any subsequent poll message sent with a newer block hash overrides the old poll message at the block with that hash and later blocks. That way if a channel balance changes significantly, a new poll message can be sent out.\u00a0\n\n\nOr you could remove the ability to specify\u00a0fractional support/opposition and exclude multiparty UTXOs from participation. I tend to like the idea of the possibility of full participation tho, even in a world that mainly uses lightning.\n\n\n> if the signaling is done onchain\n\n\nI don't think any of this signaling needs to be done on-chain. Anyone who wants to keep a count of the poll can simply collect together all these poll messages and count up the weighted preferences. Yes, it would be possible for one person to send out many conflicting poll messages, but this could be handled without any commitment to the blockchain. A simple thing to do would be to simply invalidate poll messages that conflict (ie include them both in your list of counted\u00a0messages, but ignore them in your weighted-sums of poll preferences). As long as these polls are simply used to inform action rather than to trigger action, it should be ok that someone can produce biased incomplete counts, since anyone can show a provably more complete set (a superset) of poll messages. Also, since this would generally be a time-bound thing, where this poll information would for example be used to gauge support for a soft fork, there isn't much of a need to keep the poll messages on an immutable ledger. Old poll data is inherently not very practically useful compared to recent poll data. So we can kind of side step things like history attacks by simply ignoring polls that aren't recent.\n\n\n> Semantically, we would consider the \"cold\" key to be the \"true\" owner of the fund, with \"hot\" key being delegates who are semi-trusted, but not as trusted as the \"cold\" key.\n\n\nI'm not sure I agree with those semantics as a hard rule. I don't consider a \"key\" to be an owner of anything. A person owns a key, which gives them access to funds. A key is a tool, and the owner of a key or wallet vault can define whatever semantics they want. If they want to designate a hot key\u00a0as their poll-signing key, that's their prerogative. If they want to require a cold-key as their message-signing key or even require multisig signing, that's up to them as well. You could even mirror wallet-vault constructs by overriding a poll message signed with fewer key using one signed with more keys. The trade offs you bring up are reasonable considerations, and I think which trade offs to choose may vary by the individual in question and their individual situation. However, I think the time-bound and non-binding nature of a poll makes the risks here pretty small for most situations you would want to use this in (eg in a soft-fork poll). It should be reasonable to consider any signed poll message valid, regardless of possibilities of theft or key renting shinanigans. Nacho keys nacho coins would of course be important in this scenario.\u00a0\n\n\n>\u00a0 if I need to be able to somehow indicate that a long-term-cold-storage UTXO has a signaling pubkey, I imagine this mechanism of indioating might itself require a softfork, so you have a chicken-and-egg problem...\n\n\nIf such a thing did need a soft fork, the chicken and egg question would be easy to answer: the soft fork comes first. We've done soft forks before having this mechanism, and if necessary we could do another one to enable it.\n\n\nHowever, I think\u00a0taproot can enable this mechanism without a soft fork. It should be possible to include a taproot leaf that has the data necessary to validate a signaling signature. The tapleaf would contain an invalid script that has an alternative interpretation, where your poll message can include the merkle path to tapleaf (the invalid-script), and the data at that leaf would be a public key you can then verify the signaling signature against.\u00a0\n\n\n@vjudeu\n\n> It should not be expressed in percents, but in amounts\n\n\nAgreed. You make a good case for that.\n\n\n>\u00a0it could be just some kind of transaction, where you have utxo_id just as transaction input, amount of coins as some output, and then add your message as \"OP_RETURN <commitment>\" in your input, in this way your signature would be useless in a different context than voting.\n\u00a0\nI don't quite understand this part. I don't understand how this would make your signature useless in a different context. Could you elaborate?\n\u00a0\n>\u00a0it does not really matter if you store that commitments on-chain to preserve signalling results in consensus rules or if there would be some separate chain for storing commitments and nothing else\n\u00a0\nI don't think any kind of chain is necessary to store this data. I'm primarily suggesting this as a method to help the debate about a soft fork have better information about what broader users think about a particular soft fork proposal, so such data would simply inform whether or not we decide to continue work on an upgrade. I don't think you'd want to require any validation of this data by all full nodes, because the data could be hundreds of gigabytes in size (let's say 1 billion people respond). You'd have to run some kind of random sampling (more like actual proof of stake) to get this data down to a manageable size.\u00a0\n\n\n> It would be Proof of Stake, where users would put their coins at stake to vote.\n\n\nSure, as long as by this you mean simply proof of coin ownership. Just as any bitcoin transaction involves proof of coin ownership.\n\n\nI was pretty careful to avoid the word \"voting\", since I'm not proposing that this be used with definite thresholds that trigger action, but more of an information gathering mechanism. Perhaps one day it could be used for something akin to voting, but certainly if we were going to implement this to help decide on the next soft fork, it would very likely be a quite biased set of responders. We would want to take that into account when deciding how to interpret the data. Even with biased data tho, it could be a useful tool for resolving some contention.\u00a0\n\n\nBut on that note, I was thinking that it might be interesting to have an optional human readable message into these poll messages. Those messages could be then read through to gain a better understanding of why people are supporting and why people are rejecting a particular thing. It could inform how we might change how we explain a technical change to make it easier for less technical folks (who don't post on twitter) to understand. It could potentially\u00a0give insight into an otherwise quiet majority (or large minority).\n\n\n> it sounds similar to \"Merged Signing\"\u00a0\n\n\nInteresting. I'm not sure I fully grok his idea, but I think he was suggesting that a proof of stake consensus protocol pay attention to bitcoin transactions formatted in a particular way. I think I've hopefully clarified above why the idea I'm suggesting is rather different from this (eg in that no special commitments need to be made).\n\n\nCheers,\nBT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Fri, Mar 18, 2022 at 6:01 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\nGood morning Billy,\n\n> @Jorge\n> > Any user polling system is going to be vulnerable to sybil attacks.\n>\n> Not the one I'll propose right here. What I propose specifically is a\u00a0coin-weighted signature-based poll with the following components:\n> A. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90% support:10%}> for each UTXO they want to respond to the poll with.\n> B. A signed message like that is valid only while that UTXO has not been spent.\n> C. Poll results are considered only at each particular block height, where the support and opposition responses are weighted by the UTXO amount (and the support/oppose fraction in the message). This means you'd basically see a rolling poll through the blockchain as new signed poll messages come in and as their UTXOs are spent.\u00a0\n>\n> This is not vulnerable to sybil attacks because it requires access to UTXOs and response-weight is directly tied to UTXO amount. If someone signs a poll message with a key that can unlock (or is in some other designated way associated with) a UTXO, and then spends that UTXO, their poll response stops being counted for all block heights after the UTXO was spent.\u00a0\n>\n> Why put support and oppose fractions in the message? Who would want to both support and oppose something? Any multiple participant UTXO would. Eg lightning channels would, where each participant disagrees with the other. They need to sign together, so they can have an agreement to sign for the fractions that match their respective channel balances (using a force channel close as a last resort against an uncooperative partner as usual).\u00a0\n\nThis does not quite work, as lightning channel balances can be changed at any time.\nI might agree that you have 90% of the channel and I have 10% of the channel right now, but if you then send a request to forward your funds out, I need to be able to invalidate the previous signal, one that is tied to the fulfillment of the forwarding request.\nThis begins to add complexity.\n\nMore pointedly, if the signaling is done onchain, then a forward on the LN requires that I put up invalidations of previous signals, also onchain, otherwise you could cheaty cheat your effective balance by moving your funds around.\nBut the point of LN is to avoid putting typical everyday forwards onchain.\n\n> This does have the potential issue of public key exposure prior to spending for current addresses. But that could be fixed with a new address type that has two public keys / spend paths: one for spending and one for signing.\u00a0\n\nThis issue is particularly relevant to vault constructions.\nTypically a vault has a \"cold\" key that is the master owner of the fund, with \"hot\" keys having partial access.\nSemantically, we would consider the \"cold\" key to be the \"true\" owner of the fund, with \"hot\" key being delegates who are semi-trusted, but not as trusted as the \"cold\" key.\n\nSo, we should consider a vote from the \"cold\" key only.\nHowever, the point is that the \"cold\" key wants to be kept offline as much as possible for security.\n\nI suppose the \"cold\" key could be put online just once to create the signal message, but vault owners might not want to vote because of the risk, and their weight might be enough to be important in your voting scheme (consider that the point of vaults is to protect large funds).\n\n\nA sub-issue here with the spend/signal pubkey idea is that if I need to be able to somehow indicate that a long-term-cold-storage UTXO has a signaling pubkey, I imagine this mechanism of indioating might itself require a softfork, so you have a chicken-and-egg problem...\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-22T15:19:30",
                "message_text_only": ">  If you vote by making transactions, then someone could capture that and\nbroadcast to nodes\n>  you can only send that to your network\n\nWhat do you mean \"capture that\" and \"your network\"? I was imagining a\nscenario where these poll messages are always broadcast globally. Are you\nimplying more of a private poll?\n\n> If it will be sent anywhere else, it will be invalid\n\nI still don't understand. Why would a signed transaction be invalid\nanywhere? Wouldn't a signed transaction be valid everywhere?\n\n> Another reason to sign transactions and not just some custom data is to\nmake it compatible with \"signet way of making signatures\", the same as used\nin signet challenge.\n\nPerhaps I don't understand how signet works well enough to understand this,\nbut I would think that signing an message would work with signet just as\nwell as mainnet. I get the feeling perhaps we're misunderstanding each\nother in some fundamental way.\n\n> Even if it is not needed, it is kind of \"free\" if you take transaction\nsize into account\n\nBut it would require an on-chain transaction. We don't want 6 billion\npeople to have to send an on-chain transaction all in the same week in\norder to register their preference on something.\n\nOn Mon, Mar 21, 2022 at 10:56 AM <vjudeu at gazeta.pl> wrote:\n\n> > I don't quite understand this part. I don't understand how this would\n> make your signature useless in a different context. Could you elaborate?\n>\n> It is simple. If you vote by making transactions, then someone could\n> capture that and broadcast to nodes. If your signature is \"useless in a\n> different context\", then you can only send that to your network. If it will\n> be sent anywhere else, it will be invalid, so also useless. Another reason\n> to sign transactions and not just some custom data is to make it compatible\n> with \"signet way of making signatures\", the same as used in signet\n> challenge.\n>\n> > I don't think any kind of chain is necessary to store this data.\n>\n> Even if it is not needed, it is kind of \"free\" if you take transaction\n> size into account. Because each person moving coins on-chain could attach\n> \"OP_RETURN <commitment>\" in TapScript, just to save commitments. Then, even\n> if someone is not in your network from the very beginning, that person\n> could still collect commitments and find out how they are connected with\n> on-chain transactions.\n>\n> > Perhaps one day it could be used for something akin to voting, but\n> certainly if we were going to implement this to help decide on the next\n> soft fork, it would very likely be a quite biased set of responders.\n>\n> If it will be ever implemented, it should be done in a similar way as\n> difficulty: if you want 90%, you should calculate, what amount in satoshis\n> is needed to reach that 90%, and update it every two weeks, based on all\n> votes. In this way, you reduce floating-point operations to a bare minimum,\n> and have a system, where you can compare uint64 amounts to quickly get\n> \"yes/no\" answer to the question, if something should be triggered (also,\n> you can compress it to 32 bits in the same way as 256-bit target is\n> compressed).\n>\n> > But on that note, I was thinking that it might be interesting to have an\n> optional human readable message into these poll messages.\n>\n> As I said, \"OP_RETURN <commitment>\" inside TapScript is enough to produce\n> all commitments of arbitrary size for \"free\", so that on-chain transaction\n> size is constant, no matter how large that commitment is. And about\n> storage: you could create a separate chain for that, you could store that\n> in the same way as LN nodes store data, you could use something else, it\n> doesn't really matter, because on-chain commitments could be constructed in\n> the same way (also, as long as the transaction creator keeps those\n> commitments as a secret, there is no way to get them; that means you can\n> add them later if needed and easily pretend that \"it was always possible\").\n>\n>\n> On 2022-03-21 10:17:29 user Billy Tetrud via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Good Evening ZmnSCPxj,\n>\n>\n> >  I need to be able to invalidate the previous signal, one that is tied\n> to the fulfillment of the forwarding request.\n>\n>\n> You're right that there's some nuance there. You could add a block hash\n> into the poll message and define things so any subsequent poll message sent\n> with a newer block hash overrides the old poll message at the block with\n> that hash and later blocks. That way if a channel balance changes\n> significantly, a new poll message can be sent out.\n>\n>\n> Or you could remove the ability to specify fractional support/opposition\n> and exclude multiparty UTXOs from participation. I tend to like the idea of\n> the possibility of full participation tho, even in a world that mainly uses\n> lightning.\n>\n>\n> > if the signaling is done onchain\n>\n>\n> I don't think any of this signaling needs to be done on-chain. Anyone who\n> wants to keep a count of the poll can simply collect together all these\n> poll messages and count up the weighted preferences. Yes, it would be\n> possible for one person to send out many conflicting poll messages, but\n> this could be handled without any commitment to the blockchain. A simple\n> thing to do would be to simply invalidate poll messages that conflict (ie\n> include them both in your list of counted messages, but ignore them in your\n> weighted-sums of poll preferences). As long as these polls are simply used\n> to inform action rather than to trigger action, it should be ok that\n> someone can produce biased incomplete counts, since anyone can show a\n> provably more complete set (a superset) of poll messages. Also, since this\n> would generally be a time-bound thing, where this poll information would\n> for example be used to gauge support for a soft fork, there isn't much of a\n> need to keep the poll messages on an immutable ledger. Old poll data is\n> inherently not very practically useful compared to recent poll data. So we\n> can kind of side step things like history attacks by simply ignoring polls\n> that aren't recent.\n>\n>\n> > Semantically, we would consider the \"cold\" key to be the \"true\" owner of\n> the fund, with \"hot\" key being delegates who are semi-trusted, but not as\n> trusted as the \"cold\" key.\n>\n>\n> I'm not sure I agree with those semantics as a hard rule. I don't consider\n> a \"key\" to be an owner of anything. A person owns a key, which gives them\n> access to funds. A key is a tool, and the owner of a key or wallet vault\n> can define whatever semantics they want. If they want to designate a hot\n> key as their poll-signing key, that's their prerogative. If they want to\n> require a cold-key as their message-signing key or even require multisig\n> signing, that's up to them as well. You could even mirror wallet-vault\n> constructs by overriding a poll message signed with fewer key using one\n> signed with more keys. The trade offs you bring up are reasonable\n> considerations, and I think which trade offs to choose may vary by the\n> individual in question and their individual situation. However, I think the\n> time-bound and non-binding nature of a poll makes the risks here pretty\n> small for most situations you would want to use this in (eg in a soft-fork\n> poll). It should be reasonable to consider any signed poll message valid,\n> regardless of possibilities of theft or key renting shinanigans. Nacho keys\n> nacho coins would of course be important in this scenario.\n>\n>\n> >  if I need to be able to somehow indicate that a long-term-cold-storage\n> UTXO has a signaling pubkey, I imagine this mechanism of indioating might\n> itself require a softfork, so you have a chicken-and-egg problem...\n>\n>\n> If such a thing did need a soft fork, the chicken and egg question would\n> be easy to answer: the soft fork comes first. We've done soft forks before\n> having this mechanism, and if necessary we could do another one to enable\n> it.\n>\n>\n> However, I think taproot can enable this mechanism without a soft fork. It\n> should be possible to include a taproot leaf that has the data necessary to\n> validate a signaling signature. The tapleaf would contain an invalid script\n> that has an alternative interpretation, where your poll message can include\n> the merkle path to tapleaf (the invalid-script), and the data at that leaf\n> would be a public key you can then verify the signaling signature against.\n>\n>\n> @vjudeu\n>\n> > It should not be expressed in percents, but in amounts\n>\n>\n> Agreed. You make a good case for that.\n>\n>\n> > it could be just some kind of transaction, where you have utxo_id just\n> as transaction input, amount of coins as some output, and then add your\n> message as \"OP_RETURN <commitment>\" in your input, in this way your\n> signature would be useless in a different context than voting.\n>\n> I don't quite understand this part. I don't understand how this would make\n> your signature useless in a different context. Could you elaborate?\n>\n> > it does not really matter if you store that commitments on-chain to\n> preserve signalling results in consensus rules or if there would be some\n> separate chain for storing commitments and nothing else\n>\n> I don't think any kind of chain is necessary to store this data. I'm\n> primarily suggesting this as a method to help the debate about a soft fork\n> have better information about what broader users think about a particular\n> soft fork proposal, so such data would simply inform whether or not we\n> decide to continue work on an upgrade. I don't think you'd want to require\n> any validation of this data by all full nodes, because the data could be\n> hundreds of gigabytes in size (let's say 1 billion people respond). You'd\n> have to run some kind of random sampling (more like actual proof of stake)\n> to get this data down to a manageable size.\n>\n>\n> > It would be Proof of Stake, where users would put their coins at stake\n> to vote.\n>\n>\n> Sure, as long as by this you mean simply proof of coin ownership. Just as\n> any bitcoin transaction involves proof of coin ownership.\n>\n>\n> I was pretty careful to avoid the word \"voting\", since I'm not proposing\n> that this be used with definite thresholds that trigger action, but more of\n> an information gathering mechanism. Perhaps one day it could be used for\n> something akin to voting, but certainly if we were going to implement this\n> to help decide on the next soft fork, it would very likely be a quite\n> biased set of responders. We would want to take that into account when\n> deciding how to interpret the data. Even with biased data tho, it could be\n> a useful tool for resolving some contention.\n>\n>\n> But on that note, I was thinking that it might be interesting to have an\n> optional human readable message into these poll messages. Those messages\n> could be then read through to gain a better understanding of why people are\n> supporting and why people are rejecting a particular thing. It could inform\n> how we might change how we explain a technical change to make it easier for\n> less technical folks (who don't post on twitter) to understand. It could\n> potentially give insight into an otherwise quiet majority (or large\n> minority).\n>\n>\n> > it sounds similar to \"Merged Signing\"\n>\n>\n> Interesting. I'm not sure I fully grok his idea, but I think he was\n> suggesting that a proof of stake consensus protocol pay attention to\n> bitcoin transactions formatted in a particular way. I think I've hopefully\n> clarified above why the idea I'm suggesting is rather different from this\n> (eg in that no special commitments need to be made).\n>\n>\n> Cheers,\n> BT\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n> On Fri, Mar 18, 2022 at 6:01 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n> Good morning Billy,\n>\n> > @Jorge\n> > > Any user polling system is going to be vulnerable to sybil attacks.\n> >\n> > Not the one I'll propose right here. What I propose specifically is\n> a coin-weighted signature-based poll with the following components:\n> > A. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90%\n> support:10%}> for each UTXO they want to respond to the poll with.\n> > B. A signed message like that is valid only while that UTXO has not been\n> spent.\n> > C. Poll results are considered only at each particular block height,\n> where the support and opposition responses are weighted by the UTXO amount\n> (and the support/oppose fraction in the message). This means you'd\n> basically see a rolling poll through the blockchain as new signed poll\n> messages come in and as their UTXOs are spent.\n> >\n> > This is not vulnerable to sybil attacks because it requires access to\n> UTXOs and response-weight is directly tied to UTXO amount. If someone signs\n> a poll message with a key that can unlock (or is in some other designated\n> way associated with) a UTXO, and then spends that UTXO, their poll response\n> stops being counted for all block heights after the UTXO was spent.\n> >\n> > Why put support and oppose fractions in the message? Who would want to\n> both support and oppose something? Any multiple participant UTXO would. Eg\n> lightning channels would, where each participant disagrees with the other.\n> They need to sign together, so they can have an agreement to sign for the\n> fractions that match their respective channel balances (using a force\n> channel close as a last resort against an uncooperative partner as usual).\n>\n> This does not quite work, as lightning channel balances can be changed at\n> any time.\n> I might agree that you have 90% of the channel and I have 10% of the\n> channel right now, but if you then send a request to forward your funds\n> out, I need to be able to invalidate the previous signal, one that is tied\n> to the fulfillment of the forwarding request.\n> This begins to add complexity.\n>\n> More pointedly, if the signaling is done onchain, then a forward on the LN\n> requires that I put up invalidations of previous signals, also onchain,\n> otherwise you could cheaty cheat your effective balance by moving your\n> funds around.\n> But the point of LN is to avoid putting typical everyday forwards onchain.\n>\n> > This does have the potential issue of public key exposure prior to\n> spending for current addresses. But that could be fixed with a new address\n> type that has two public keys / spend paths: one for spending and one for\n> signing.\n>\n> This issue is particularly relevant to vault constructions.\n> Typically a vault has a \"cold\" key that is the master owner of the fund,\n> with \"hot\" keys having partial access.\n> Semantically, we would consider the \"cold\" key to be the \"true\" owner of\n> the fund, with \"hot\" key being delegates who are semi-trusted, but not as\n> trusted as the \"cold\" key.\n>\n> So, we should consider a vote from the \"cold\" key only.\n> However, the point is that the \"cold\" key wants to be kept offline as much\n> as possible for security.\n>\n> I suppose the \"cold\" key could be put online just once to create the\n> signal message, but vault owners might not want to vote because of the\n> risk, and their weight might be enough to be important in your voting\n> scheme (consider that the point of vaults is to protect large funds).\n>\n>\n> A sub-issue here with the spend/signal pubkey idea is that if I need to be\n> able to somehow indicate that a long-term-cold-storage UTXO has a signaling\n> pubkey, I imagine this mechanism of indioating might itself require a\n> softfork, so you have a chicken-and-egg problem...\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220322/c5541312/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2022-03-22T15:45:55",
                "message_text_only": "> > Even if it is not needed, it is kind of \"free\" if you take transaction size into account\n> \n> But it would require an on-chain transaction. We don't want 6 billion people to have to send an on-chain transaction all in the same week in order to register their preference on something.\n\nI haven\u2019t followed this thread, so apologies if I\u2019m missing some context, but confirmed tx signaling remains miner signaling.\n\nRegardless, miners are the only actors who can create soft fork compatibility, so theirs is the only relevant signal. Otherwise people can just fork themselves at any time using any voting mechanism they want.\n\ne"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-22T16:37:01",
                "message_text_only": "> What do you mean \"capture that\" and \"your network\"? I was imagining a scenario where these poll messages are always broadcast globally. Are you implying more of a private poll?\n\nIf you vote by making a Bitcoin transaction, then someone could move real bitcoins, just by including your transaction into a block. I thought you only want to get some feedback, in this case you only need to sign things, not to move real coins. So, there will be one network for moving bitcoins and one network for signalling/voting/whatever. If you combine both of them to be the same network, then you end up in a situation, where moving coins is needed to signal anything (that may quickly fill mempools and increase on-chain fees).\n\nAlso, as you earlier proposed custom data format for signing, I thought you want to create a separate network.\n\n> I still don't understand. Why would a signed transaction be invalid anywhere? Wouldn't a signed transaction be valid everywhere?\n\nIt depends what is signed and how it is signed. A transaction moving \"1 BTC -> 1.5 BTC\" with SIGHASH_SINGLE|SIGHASH_ANYONECANPAY cannot be included directly into a block, but can be turned into a valid transaction, just by attaching more inputs. A signed \"Bitcoin Message\" can be used to prove ownership, but cannot be included into a block as a valid transaction. So, if you want to move coins and vote, you can just sign a transaction (or even just observe your mempool and receive new blocks, then you can use existing transactions and pretend they are all signalling for or against something). But if you want to only move coins or to only vote, then you need to carefully choose data for signing, just to do one thing and not the other.\n\n> Perhaps I don't understand how signet works well enough to understand this, but I would think that signing an message would work with signet just as well as mainnet. I get the feeling perhaps we're misunderstanding each other in some fundamental way.\n\nIn signet, whole transactions are signed. There are separate BIP's that describe signing in a different way than famous \"Bitcoin Message\". Because if you sign just some message, extending such format is complicated. But if you sign a transaction, then you can sign P2SH address, P2WSH address, Taproot address, and potentially even not-yet-implemented-future-soft-fork-address.\n\n> But it would require an on-chain transaction. We don't want 6 billion people to have to send an on-chain transaction all in the same week in order to register their preference on something.\n\nIt would require an on-chain transaction every sometimes, not every vote. If someone is going to do some on-chain transaction, then that person could attach some commitment for the whole network. So, instead of just doing regular transaction, people could attach commitments at the same cost, with the same on-chain transaction size. The only needed change is just tweaking their own keys and informing your network about pushed commitment.\n\n\nOn 2022-03-22 16:19:49 user Billy Tetrud <billy.tetrud at gmail.com> wrote:\n>\u00a0 If you vote by making transactions, then someone could capture that and broadcast to nodes\n>\u00a0 you can only send that to your network\n\n\n\nWhat do you mean \"capture that\" and \"your network\"? I was imagining a scenario where these poll messages are always broadcast globally. Are you implying more of a private poll?\n\n\n> If it will be sent anywhere else, it will be invalid\n\n\nI still don't understand. Why would a signed transaction be invalid anywhere? Wouldn't a signed transaction be valid everywhere?\u00a0\n\n\n> Another reason to sign transactions and not just some custom data is to make it compatible with \"signet way of making signatures\", the same as used in signet challenge.\n\n\nPerhaps I don't understand how signet works well enough to understand this, but I would think that signing an message would work with signet just as well as mainnet. I get the feeling perhaps we're misunderstanding each other in some fundamental way.\n\n\n> Even if it is not needed, it is kind of \"free\" if you take transaction size into account\n\n\nBut it would require an on-chain transaction. We don't want 6 billion people to have to send an on-chain transaction all in the same week in order to register their preference on something.\u00a0\n\n\nOn Mon, Mar 21, 2022 at 10:56 AM <vjudeu at gazeta.pl> wrote:\n\n> I don't quite understand this part. I don't understand how this would make your signature useless in a different context. Could you elaborate?\n\nIt is simple. If you vote by making transactions, then someone could capture that and broadcast to nodes. If your signature is \"useless in a different context\", then you can only send that to your network. If it will be sent anywhere else, it will be invalid, so also useless. Another reason to sign transactions and not just some custom data is to make it compatible with \"signet way of making signatures\", the same as used in signet challenge.\n\n> I don't think any kind of chain is necessary to store this data.\n\nEven if it is not needed, it is kind of \"free\" if you take transaction size into account. Because each person moving coins on-chain could attach \"OP_RETURN <commitment>\" in TapScript, just to save commitments. Then, even if someone is not in your network from the very beginning, that person could still collect commitments and find out how they are connected with on-chain transactions.\n\n> Perhaps one day it could be used for something akin to voting, but certainly if we were going to implement this to help decide on the next soft fork, it would very likely be a quite biased set of responders.\n\nIf it will be ever implemented, it should be done in a similar way as difficulty: if you want 90%, you should calculate, what amount in satoshis is needed to reach that 90%, and update it every two weeks, based on all votes. In this way, you reduce floating-point operations to a bare minimum, and have a system, where you can compare uint64 amounts to quickly get \"yes/no\" answer to the question, if something should be triggered (also, you can compress it to 32 bits in the same way as 256-bit target is compressed).\n\n> But on that note, I was thinking that it might be interesting to have an optional human readable message into these poll messages.\n\nAs I said, \"OP_RETURN <commitment>\" inside TapScript is enough to produce all commitments of arbitrary size for \"free\", so that on-chain transaction size is constant, no matter how large that commitment is. And about storage: you could create a separate chain for that, you could store that in the same way as LN nodes store data, you could use something else, it doesn't really matter, because on-chain commitments could be constructed in the same way (also, as long as the transaction creator keeps those commitments as a secret, there is no way to get them; that means you can add them later if needed and easily pretend that \"it was always possible\").\n\n\nOn 2022-03-21 10:17:29 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nGood Evening ZmnSCPxj,\n\n\n>\u00a0 I need to be able to invalidate the previous signal, one that is tied to the fulfillment of the forwarding request.\n\n\nYou're right that there's some nuance there. You could add a block hash into the poll message and define things so any subsequent poll message sent with a newer block hash overrides the old poll message at the block with that hash and later blocks. That way if a channel balance changes significantly, a new poll message can be sent out.\u00a0\n\n\nOr you could remove the ability to specify\u00a0fractional support/opposition and exclude multiparty UTXOs from participation. I tend to like the idea of the possibility of full participation tho, even in a world that mainly uses lightning.\n\n\n> if the signaling is done onchain\n\n\nI don't think any of this signaling needs to be done on-chain. Anyone who wants to keep a count of the poll can simply collect together all these poll messages and count up the weighted preferences. Yes, it would be possible for one person to send out many conflicting poll messages, but this could be handled without any commitment to the blockchain. A simple thing to do would be to simply invalidate poll messages that conflict (ie include them both in your list of counted\u00a0messages, but ignore them in your weighted-sums of poll preferences). As long as these polls are simply used to inform action rather than to trigger action, it should be ok that someone can produce biased incomplete counts, since anyone can show a provably more complete set (a superset) of poll messages. Also, since this would generally be a time-bound thing, where this poll information would for example be used to gauge support for a soft fork, there isn't much of a need to keep the poll messages on an immutable ledger. Old poll data is inherently not very practically useful compared to recent poll data. So we can kind of side step things like history attacks by simply ignoring polls that aren't recent.\n\n\n> Semantically, we would consider the \"cold\" key to be the \"true\" owner of the fund, with \"hot\" key being delegates who are semi-trusted, but not as trusted as the \"cold\" key.\n\n\nI'm not sure I agree with those semantics as a hard rule. I don't consider a \"key\" to be an owner of anything. A person owns a key, which gives them access to funds. A key is a tool, and the owner of a key or wallet vault can define whatever semantics they want. If they want to designate a hot key\u00a0as their poll-signing key, that's their prerogative. If they want to require a cold-key as their message-signing key or even require multisig signing, that's up to them as well. You could even mirror wallet-vault constructs by overriding a poll message signed with fewer key using one signed with more keys. The trade offs you bring up are reasonable considerations, and I think which trade offs to choose may vary by the individual in question and their individual situation. However, I think the time-bound and non-binding nature of a poll makes the risks here pretty small for most situations you would want to use this in (eg in a soft-fork poll). It should be reasonable to consider any signed poll message valid, regardless of possibilities of theft or key renting shinanigans. Nacho keys nacho coins would of course be important in this scenario.\u00a0\n\n\n>\u00a0 if I need to be able to somehow indicate that a long-term-cold-storage UTXO has a signaling pubkey, I imagine this mechanism of indioating might itself require a softfork, so you have a chicken-and-egg problem...\n\n\nIf such a thing did need a soft fork, the chicken and egg question would be easy to answer: the soft fork comes first. We've done soft forks before having this mechanism, and if necessary we could do another one to enable it.\n\n\nHowever, I think\u00a0taproot can enable this mechanism without a soft fork. It should be possible to include a taproot leaf that has the data necessary to validate a signaling signature. The tapleaf would contain an invalid script that has an alternative interpretation, where your poll message can include the merkle path to tapleaf (the invalid-script), and the data at that leaf would be a public key you can then verify the signaling signature against.\u00a0\n\n\n@vjudeu\n\n> It should not be expressed in percents, but in amounts\n\n\nAgreed. You make a good case for that.\n\n\n>\u00a0it could be just some kind of transaction, where you have utxo_id just as transaction input, amount of coins as some output, and then add your message as \"OP_RETURN <commitment>\" in your input, in this way your signature would be useless in a different context than voting.\n\u00a0\nI don't quite understand this part. I don't understand how this would make your signature useless in a different context. Could you elaborate?\n\u00a0\n>\u00a0it does not really matter if you store that commitments on-chain to preserve signalling results in consensus rules or if there would be some separate chain for storing commitments and nothing else\n\u00a0\nI don't think any kind of chain is necessary to store this data. I'm primarily suggesting this as a method to help the debate about a soft fork have better information about what broader users think about a particular soft fork proposal, so such data would simply inform whether or not we decide to continue work on an upgrade. I don't think you'd want to require any validation of this data by all full nodes, because the data could be hundreds of gigabytes in size (let's say 1 billion people respond). You'd have to run some kind of random sampling (more like actual proof of stake) to get this data down to a manageable size.\u00a0\n\n\n> It would be Proof of Stake, where users would put their coins at stake to vote.\n\n\nSure, as long as by this you mean simply proof of coin ownership. Just as any bitcoin transaction involves proof of coin ownership.\n\n\nI was pretty careful to avoid the word \"voting\", since I'm not proposing that this be used with definite thresholds that trigger action, but more of an information gathering mechanism. Perhaps one day it could be used for something akin to voting, but certainly if we were going to implement this to help decide on the next soft fork, it would very likely be a quite biased set of responders. We would want to take that into account when deciding how to interpret the data. Even with biased data tho, it could be a useful tool for resolving some contention.\u00a0\n\n\nBut on that note, I was thinking that it might be interesting to have an optional human readable message into these poll messages. Those messages could be then read through to gain a better understanding of why people are supporting and why people are rejecting a particular thing. It could inform how we might change how we explain a technical change to make it easier for less technical folks (who don't post on twitter) to understand. It could potentially\u00a0give insight into an otherwise quiet majority (or large minority).\n\n\n> it sounds similar to \"Merged Signing\"\u00a0\n\n\nInteresting. I'm not sure I fully grok his idea, but I think he was suggesting that a proof of stake consensus protocol pay attention to bitcoin transactions formatted in a particular way. I think I've hopefully clarified above why the idea I'm suggesting is rather different from this (eg in that no special commitments need to be made).\n\n\nCheers,\nBT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Fri, Mar 18, 2022 at 6:01 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\nGood morning Billy,\n\n> @Jorge\n> > Any user polling system is going to be vulnerable to sybil attacks.\n>\n> Not the one I'll propose right here. What I propose specifically is a\u00a0coin-weighted signature-based poll with the following components:\n> A. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90% support:10%}> for each UTXO they want to respond to the poll with.\n> B. A signed message like that is valid only while that UTXO has not been spent.\n> C. Poll results are considered only at each particular block height, where the support and opposition responses are weighted by the UTXO amount (and the support/oppose fraction in the message). This means you'd basically see a rolling poll through the blockchain as new signed poll messages come in and as their UTXOs are spent.\u00a0\n>\n> This is not vulnerable to sybil attacks because it requires access to UTXOs and response-weight is directly tied to UTXO amount. If someone signs a poll message with a key that can unlock (or is in some other designated way associated with) a UTXO, and then spends that UTXO, their poll response stops being counted for all block heights after the UTXO was spent.\u00a0\n>\n> Why put support and oppose fractions in the message? Who would want to both support and oppose something? Any multiple participant UTXO would. Eg lightning channels would, where each participant disagrees with the other. They need to sign together, so they can have an agreement to sign for the fractions that match their respective channel balances (using a force channel close as a last resort against an uncooperative partner as usual).\u00a0\n\nThis does not quite work, as lightning channel balances can be changed at any time.\nI might agree that you have 90% of the channel and I have 10% of the channel right now, but if you then send a request to forward your funds out, I need to be able to invalidate the previous signal, one that is tied to the fulfillment of the forwarding request.\nThis begins to add complexity.\n\nMore pointedly, if the signaling is done onchain, then a forward on the LN requires that I put up invalidations of previous signals, also onchain, otherwise you could cheaty cheat your effective balance by moving your funds around.\nBut the point of LN is to avoid putting typical everyday forwards onchain.\n\n> This does have the potential issue of public key exposure prior to spending for current addresses. But that could be fixed with a new address type that has two public keys / spend paths: one for spending and one for signing.\u00a0\n\nThis issue is particularly relevant to vault constructions.\nTypically a vault has a \"cold\" key that is the master owner of the fund, with \"hot\" keys having partial access.\nSemantically, we would consider the \"cold\" key to be the \"true\" owner of the fund, with \"hot\" key being delegates who are semi-trusted, but not as trusted as the \"cold\" key.\n\nSo, we should consider a vote from the \"cold\" key only.\nHowever, the point is that the \"cold\" key wants to be kept offline as much as possible for security.\n\nI suppose the \"cold\" key could be put online just once to create the signal message, but vault owners might not want to vote because of the risk, and their weight might be enough to be important in your voting scheme (consider that the point of vaults is to protect large funds).\n\n\nA sub-issue here with the spend/signal pubkey idea is that if I need to be able to somehow indicate that a long-term-cold-storage UTXO has a signaling pubkey, I imagine this mechanism of indioating might itself require a softfork, so you have a chicken-and-egg problem...\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-19T16:43:42",
                "message_text_only": "> A. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90% support:10%}> for each UTXO they want to respond to the poll with.\n\nIt should not be expressed in percents, but in amounts. It would be easier and more compatible with votes where there is 100% oppose or 100% support (and also easier to handle if some LN user would move one satoshi, because rounding percents would be tricky). Anyway, you need to convert percents to amounts, so better use amounts from the very beginning. Also, it could be just some kind of transaction, where you have utxo_id just as transaction input, amount of coins as some output, and then add your message as \"OP_RETURN <commitment>\" in your input, in this way your signature would be useless in a different context than voting.\n\nAlso note that such voting would be some kind of Proof of Stake. And it does not really matter if you store that commitments on-chain to preserve signalling results in consensus rules or if there would be some separate chain for storing commitments and nothing else. It would be Proof of Stake, where users would put their coins at stake to vote. Also, you probably solved \"nothing at stake\" problem in a nice way, because it would be protected by Proof of Work chain to decide who can vote. So, voters could only freeze their coins for getting some voting power or move their coins and lose their votes.\n\nFor me, it sounds similar to \"Merged Signing\" proposed by stwenhao here: https://bitcointalk.org/index.php?topic=5390027.0. I think it is kind of dangerous and unstoppable (so nobody could stop you if you would ignore any criticism and implement that). Fortunately, it is also possible to add some Proof of Work if any staking-like system would be present in Bitcoin, just OP_SUBSTR would do the trick (if enabled; if not, we could still use OP_HASH256 and force the target by some kind of soft-fork on top of your voting system).\n\n\nOn 2022-03-17 20:58:35 user Billy Tetrud via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n@Jorge\n> Any user polling system is going to be vulnerable to sybil attacks.\n\n\nNot the one I'll propose right here. What I propose specifically is a\u00a0coin-weighted signature-based poll with the following components:\nA. Every pollee signs messages like <utxo_id, {soft_fork: 9 oppose:90% support:10%}> for each UTXO they want to respond to the poll with.\nB. A signed message like that is valid only while that UTXO has not been spent.\nC. Poll results are considered only at each particular block height, where the support and opposition responses are weighted by the UTXO amount (and the support/oppose fraction in the message). This means you'd basically see a rolling poll through the blockchain as new signed poll messages come in and as their UTXOs are spent.\u00a0\n\n\nThis is not vulnerable to sybil attacks because it requires access to UTXOs and response-weight is directly tied to UTXO amount. If someone signs a poll message with a key that can unlock (or is in some other designated way associated with) a UTXO, and then spends that UTXO, their poll response stops being counted for all block heights after the UTXO was spent.\u00a0\n\n\nWhy put support and oppose fractions in the message? Who would want to both support and oppose something? Any multiple participant UTXO would. Eg lightning channels would, where each participant disagrees with the other. They need to sign together, so they can have an agreement to sign for the fractions that match their respective channel balances (using a force channel close as a last resort against an uncooperative partner as usual).\u00a0\n\n\nThis does have the potential issue of public key exposure prior to spending for current addresses. But that could be fixed with a new address type that has two public keys / spend paths: one for spending and one for signing.\u00a0\n\n\n\n> In perfect competition the mining power costs per chain tends to equal the rewards offered by that chain, both in subsidy and transaction fees.\n\n\nAgreed, but it takes time for an economic shock to reach its new equilibrium. That period of time, which might be rather precarious, should be considered in a plan to preserve a minority fork.\u00a0\n\n\n> Would you rather that proposal be deployed with speedy trial activation or with BIP8+LOT=true activation?\n\n\nFor a proposal I don't want to succeed, I absolutely would prefer speedy trial over BIP8+LOT=true. Speedy trial at 90% signaling threshold can quickly determine that the proposal (hopefully) does not have enough consensus among miners. By contrast, BIP8+LOT=true could polarize the debate, worsening the community's ability to communicate and talk through issues. It would also basically guarantee that a fork happens, which in the best case (in my hypothetical point of view where I don't like the proposal) would mean some small minority forks off the network, which reduces the main chain's value somewhat (at least temporarily). Worst case a small majority forces the issue at near 50% which would cause all sorts of blockchain issues and would have a high probability of leading to a hardfork\u00a0by the minority.\u00a0\n\n\nAll this sounds rather more tenable with speedy trial. Any proposal has less chance of causing an actual fork (soft or otherwise) with speedy trial vs LOT=true. LOT=true guarantees a fork if even a single person is running it. LOT=true could certainly come in handy to initiate a UASF, but IMO that's better left as a plan B or C.\u00a0\n\n\n> segwit... all the consequences of the change are not opt in.\n\n\nI definitely agree there. The consequences of a soft fork are not always opt in. That's basically what my example of a \"dumb majority soft fork\" is, and sounds like what your \"evil fork\" basically is.\u00a0\n\n\nOn Thu, Mar 17, 2022 at 7:19 AM Jorge Tim\u00f3n via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nOn Sat, Mar 12, 2022 at 2:35 PM Russell O'Connor via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> On Fri, Mar 11, 2022 at 9:03 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n> A mechanism of soft-forking against activation exists.\u00a0 What more do you want? Are we supposed to write the code on behalf of this hypothetical group of users who may or may not exist for them just so that they can have a node that remains stalled on Speedy Trial lockin?\u00a0 That simply isn't reasonable, but if you think it is, I invite you to create such a fork.\n\nI want BIP+LOT=true to be used. I want speedy trial not to be used.\nLuke wrote the code to resist BIP8+LOT=true, and if he didn't, I could\nwrite it myself, yes.\nIf you think that's not reasonable code to ever run, I don't think\nyou're really getting the \"softfork THAT YOU OPPOSE\" part of the\nhypothetical right. Let me try to help with an example, although I\nhope we don't get derailed in the implementation details of the\nhypothetical evil proposal.\n\nSuppose someone proposes a weight size limit increase by a extension\nblock softfork.\nOr instead of that, just imagine the final version of the covenants\nproposal has a backdoor in it or something.\n\n\nWould you rather that proposal be deployed with speedy trial\nactivation or with BIP8+LOT=true activation?\n\n>>\n>> Please, try to imagine an example for an activation that you wouldn't like yourself. Imagine it gets proposed and you, as a user, want to resist it.\n>\n>\n> If I believe I'm in the economic majority then I'll just refuse to upgrade my node, which was option 2. I don't know why you dismissed it.\n\nNot upgrading your node doesn't prevent the softfork from being\nactivated in your chain.\nA softfork may affect you indirectly even if you don't use the new\nfeatures yourself directly.\nYou may chose to stay in the old chain even if you don't consider\nyou're \"in the economic majority\" at that moment.\n\n> Not much can prevent a miner cartel from enforcing rules that users don't want other than hard forking a replacement POW.\u00a0 There is no effective difference between some developers releasing a malicious soft-fork of Bitcoin and the miners releasing a malicious version themselves.\u00a0 And when the miner cartel forms, they aren't necessarily going to be polite enough to give a transparent signal of their new rules.\u00a0 However, without the economic majority enforcing their set of rules, the cartel continuously risks falling apart from the temptation of transaction fees of the censored transactions.\n\nIt is true that a mining cartel doesn't need to use speedy trial or\nBIP8+LOT=true to apply rule changes they want just because we do.\nBut they would do if they wanted to maintain the appearance of benevolence.\n\n> On the other hand, If I find out I'm in the economic minority then I have little choice but to either accept the existence of the new rules or sell my Bitcoin.\u00a0 Look, you cannot have the perfect system of money all by your lonesome self.\u00a0 Money doesn't have economic value if no one else wants to trade you for it.\u00a0 Just ask that poor user who YOLO'd his own taproot activation in advance all by themselves.\u00a0 I'm sure they think they've got just the perfect money system, with taproot early and everything.\u00a0 But now their node is stuck at block 692261 and hasn't made progress since.\u00a0 No doubt they are hunkered down for the long term, absolutely committed to their fork and just waiting for the rest of the world to come around to how much better their version of Bitcoin is than the rest of us.\n\nWell, you could also have the option to stay in the old chain with the\neconomic minority, it doesn't have to be you alone.\nWe agree that one person alone can't use a currency.\n\n> Even though you've dismissed it, one of the considerations of taproot was that it is opt-in for users to use the functionality.\u00a0 Future soft-forks ought to have the same considerations to the extent possible.\n\nWell, the same could be said about segwit. And yet all the\nconsequences of the change are not opt in.\nFor example, segwit contained a block size limit increase.\nSure, you can just not validate the witnesses, but then you're no\nlonger a full node.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-15T15:45:49",
                "message_text_only": "On Fri, Mar 11, 2022 at 02:04:29PM +0000, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> >  Thirdly, if some users insist on a chain where taproot is\n> > \"not activated\", they can always softk-fork in their own rule that\n> > disallows the version bits that complete the Speedy Trial activation\n> > sequence, or alternatively soft-fork in a rule to make spending from (or\n> > to) taproot addresses illegal.\n> Since it's about activation in general and not about taproot specifically,\n> your third point is the one that applies.\n> Users could have coordinated to have \"activation x\" never activated in\n> their chains if they simply make a rule that activating a given proposal\n> (with bip8) is forbidden in their chain.\n> But coordination requires time.\n\nPeople opposed to having taproot transactions in their chain had over\nthree years to do that coordination before an activation method was merged\n[0], and then an additional seven months after the activation method was merged before taproot enforcement began [1].\n\n[0] 2018-01-23 was the original proposal, 2021-04-15 was when speedy\n    trial activation parameters for mainnet and testnet were merged.\n[1] 2021-11-14\n\nFor comparison, the UASF activation attempt for segwit took between 4\nto 6 months to coordinate, assuming you start counting from either the\n\"user activated soft fork\" concept being raised on bitcoin-dev or the\nfinal params for BIP 148 being merged into the bips repo, and stop\ncounting when segwit locked in.\n\n> Please, try to imagine an example for an activation that you wouldn't like\n> yourself. Imagine it gets proposed and you, as a user, want to resist it.\n\nSure. There's more steps than just \"fork off onto a minority chain\"\nthough.\n\n 1) The first and most important step is to explain why you want to\n    resist it, either to convince the proposers that there really is\n    a problem and they should stand down, or so someone can come up\n    with a way of fixing the proposal so you don't need to resist it.\n    Ideally, that's all that's needed to resolve the objections. (That's\n    what didn't happen with opposition to segwit)\n\n 2) If that somehow doesn't work, and people are pushing ahead with a\n    consensus change despite significant reasonable opposition; the next\n    thing to do would be to establish if either side is a paper tiger\n    and setup a futures market. That has the extra benefit of giving\n    miners some information about which (combination of) rules will be\n    most profitable to mine for.\n\n    Once that's setup and price discovery happens, one side or the other\n    will probably throw in the towel -- there's not much point have a\n    money that other people aren't interested in using. (And that more\n    or less is what happened with 2X)\n\n    If a futures market like that is going to be setup, I think it's\n    best if it happens before signalling for the soft fork starts --\n    the information miners will get from it is useful for figuring out\n    how much resources to invest in signalling, eg. I think it might even\n    be feasible to set something up even before activation parameters are\n    finalised; you need something more than just one-on-one twitter bets\n    to get meaningful price discovery, but I think you could probably\n    build something based on a reasonably unbiassed oracle declaring an\n    outcome, without precisely defined parameters fixed in a BIP.\n\n    So if acting like reasonable people and talking it through doesn't\n    work, this seems like the next step to me.\n\n 3) But maybe you try both those and they fail and people start trying\n    to activate the soft fork (or perhaps you just weren't paying\n    attention until it was too late, and missed the opportunity).\n\n    I think the speedy trial approach here is ideal for a last ditch\n    \"everyone stays on the same chain while avoiding this horrible change\"\n    attempt. The reason being that it allows everyone to agree to not\n    adopt the new rules with only very little cost: all you need is for\n    10% of hashpower to not signal over a three month period.\n\n    That's cheaper than bip9 (5% over 12 months requires 2x the\n    cumulative hashpower), and much cheaper than bip8 which requires\n    users to update their software\n\n 4) At this point, if you were able to prevent activation, hopefully\n    that's enough of a power move that people will take your concerns\n    seriously, and you get a second chance at step (1). If that still\n    results in an impasse, I'd expect there to be a second, non-speedy\n    activation of the soft fork, that either cannot be blocked at all, or\n    cannot be blocked without having control of at least 60% of hashpower.\n\n 5) If you weren't able to prevent activation (whether or not you\n    prevented speedy trial from working), then you should have a lot\n    of information:\n\n      - you weren't able to convince people there was a problem\n\n      - you either weren't in the economic majority and people don't\n        think your concept of bitcoin is more valuable (perhaps they\n\tdon't even think it's valuable enough to setup a futures market\n\tfor you)\n\n      - you can't get control of even 10% of hashpower for a few months\n\n    and your only option is to accept defeat or create a new chain.\n\n    Since your new chain won't have a hashpower majority, you'll likely\n    have significant problems if you don't hard fork in a change to\n    how proof-of-work works; my guess is you'd either want to switch\n    to a different proof-of-work algorithm, or make your chain able\n    to be merge-mined against bitcoin, though just following BCH/BSV's\n    example and tweaking the difficulty adjustment to be more dynamic\n    could work too.\n\n    (For comparison, apparently BCH has 0.8% of bitcoin's hashrate,\n    BSV has 0.2%. Meanwhile, Namecoin, RSK and Syscoin, which support\n    merge-mining, are apparently at 68%, 42% and 17% respectively)\n\n    At the point that you're doing a hard fork, making a clean split is\n    straightforward: schedule the hard fork for around the same time as\n    the start of enforcement of the soft fork you oppose, work out how\n    to make sure you're on your own p2p network, and figure out how\n    exchanges and lightning channels and everything else are going to\n    cope with the coin split.\n\n 6) There's potentially also the case where a soft fork locks-in\n    and later everyone realises the people who were opposing it were\n    right all along and the fork is a really bad idea.\n\n    If everyone agreed that some idea was irredeemably bad -- eg,\n    OP_VERIF -- then we could soft fork them out and just forbid\n    blocks/transactions that attempt to use them. Or conceivably we could\n    do a hardfork and have more options about how to fix the problem.\n\n    That's already true for various features that satoshi included and\n    that are still available today -- eg the CHECKMULTISIG bug where\n    it pops one too many things from the stack, or the timewarp bug,\n    or CODESEP/FindAndDelete validation complexity.\n\n    Those can be complicated to fix though; if people have lost their\n    private keys and are sitting on (timelocked?) pre-signed transactions,\n    even fixing the problem via a hard fork could cause loss of funds.\n\nBut those are really progressively worse options -- just talking to each\nother and solving the problem before it's a problem is a better approach\nthan risking money on futures markets; and that's better than having to\nbuy hashpower to try to block something that other people want; and that's\nbetter than forking the chain; and even that's better than doing things\nthat might cause irretrievable loss of funds from random other bitcoiners.\n\nCheers,\naj"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-17T14:04:32",
                "message_text_only": "On Tue, Mar 15, 2022 at 4:45 PM Anthony Towns <aj at erisian.com.au> wrote:\n>\n> On Fri, Mar 11, 2022 at 02:04:29PM +0000, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> People opposed to having taproot transactions in their chain had over\n> three years to do that coordination before an activation method was merged\n> [0], and then an additional seven months after the activation method was merged before taproot enforcement began [1].\n>\n> [0] 2018-01-23 was the original proposal, 2021-04-15 was when speedy\n>     trial activation parameters for mainnet and testnet were merged.\n> [1] 2021-11-14\n\nPeople may be opposed only to the final version, but not the initial\none or the fundamental concept.\nPlease, try to think of worse case scenarios.\nPerhaps there's no opposition until after activation code has been\nreleased and miners are already starting to signal.\nPerhaps at that moment a reviewer comes and points out a fatal flaw.\n\n> For comparison, the UASF activation attempt for segwit took between 4\n> to 6 months to coordinate, assuming you start counting from either the\n> \"user activated soft fork\" concept being raised on bitcoin-dev or the\n> final params for BIP 148 being merged into the bips repo, and stop\n> counting when segwit locked in.\n\nThat was extremely risky and could have been a disaster. It went well,\nbut in my opinion a BIP8 approach from the beginning would have been\nmuch less risky. Instead of improvising these things we should plan\nahead. But for \"user forced\" activations and for \"user forced\"\nrejections.\nJust remember you may reject your own code.\n\n> > Please, try to imagine an example for an activation that you wouldn't like\n> > yourself. Imagine it gets proposed and you, as a user, want to resist it.\n>\n> Sure. There's more steps than just \"fork off onto a minority chain\"\n> though.\n>\n>  1) The first and most important step is to explain why you want to\n>     resist it, either to convince the proposers that there really is\n>     a problem and they should stand down, or so someone can come up\n>     with a way of fixing the proposal so you don't need to resist it.\n>     Ideally, that's all that's needed to resolve the objections. (That's\n>     what didn't happen with opposition to segwit)\n\nAgreed, for any given proposal, the first approach should be rational\ndiscussion.\nSome times we consider other arguments irrational simply because we\ndon't understand them though.\n\n>  2) If that somehow doesn't work, and people are pushing ahead with a\n>     consensus change despite significant reasonable opposition; the next\n>     thing to do would be to establish if either side is a paper tiger\n>     and setup a futures market. That has the extra benefit of giving\n>     miners some information about which (combination of) rules will be\n>     most profitable to mine for.\n>\n>     Once that's setup and price discovery happens, one side or the other\n>     will probably throw in the towel -- there's not much point have a\n>     money that other people aren't interested in using. (And that more\n>     or less is what happened with 2X)\n\nFuture markets can be manipulated.\nRegarding 2x, that's not how I remember it. If I remember correctly,\n\"discovered\" a price in btc for bcash that was\norders of magnitude higher than what it is today.\n\n>     If a futures market like that is going to be setup, I think it's\n>     best if it happens before signalling for the soft fork starts --\n>     the information miners will get from it is useful for figuring out\n>     how much resources to invest in signalling, eg. I think it might even\n>     be feasible to set something up even before activation parameters are\n>     finalised; you need something more than just one-on-one twitter bets\n>     to get meaningful price discovery, but I think you could probably\n>     build something based on a reasonably unbiassed oracle declaring an\n>     outcome, without precisely defined parameters fixed in a BIP.\n\nWhatever miners signal, until there are two chains and their real\nrewards can be traded, it's hard to know what they will mine\nafterwards.\nThey could signal a change with 100% and then after it is activated on\none chain and resisted on another, they 95% of them may switch to the\nold chain simply because its rewards are 20 times more valuable. This\nmay happen 3 days after activation or 3 months, or more.\nIt could depend on how fast some relevant information about the new\nchange spreads.\nWhich is specially hard to estimate in a censored world like ours.\n\n>     So if acting like reasonable people and talking it through doesn't\n>     work, this seems like the next step to me.\n\nNot to me, but you're free to create your future markets or trade in them.\nI wouldn't do any of them, and I would advice against it.\n\n>  3) But maybe you try both those and they fail and people start trying\n>     to activate the soft fork (or perhaps you just weren't paying\n>     attention until it was too late, and missed the opportunity).\n\nYes, some changes may be rejected late because some people weren't\npaying attention or weren't paid attention, indeed.\nOr perhaps it's your own proposal and you realize it is flawed\nyourself. There are infinite hypothetical scenarios we could consider\nfor this to happen.\n\n>     I think the speedy trial approach here is ideal for a last ditch\n>     \"everyone stays on the same chain while avoiding this horrible change\"\n>     attempt. The reason being that it allows everyone to agree to not\n>     adopt the new rules with only very little cost: all you need is for\n>     10% of hashpower to not signal over a three month period.\n\nNo, 10% of hashpower is not \"very little cost\", that's very expensive.\n\n>     That's cheaper than bip9 (5% over 12 months requires 2x the\n>     cumulative hashpower), and much cheaper than bip8 which requires\n>     users to update their software\n\nUpdating software is not expensive. the code for bip8 could have been\nmerged long before taproot was even initially proposed.\nIt could be merged now before another proposal.\nUpdating software is certainly not more expensive than getting 10% of\nthe hashrate.\n\n>  4) At this point, if you were able to prevent activation, hopefully\n>     that's enough of a power move that people will take your concerns\n>     seriously, and you get a second chance at step (1). If that still\n>     results in an impasse, I'd expect there to be a second, non-speedy\n>     activation of the soft fork, that either cannot be blocked at all, or\n>     cannot be blocked without having control of at least 60% of hashpower.\n\nAnd if you never got 10% hashpower, we move to the next step, I guess.\n\n>  5) If you weren't able to prevent activation (whether or not you\n>     prevented speedy trial from working), then you should have a lot\n>     of information:\n>\n>       - you weren't able to convince people there was a problem\n>\n>       - you either weren't in the economic majority and people don't\n>         think your concept of bitcoin is more valuable (perhaps they\n>         don't even think it's valuable enough to setup a futures market\n>         for you)\n>\n>       - you can't get control of even 10% of hashpower for a few months\n>\n>     and your only option is to accept defeat or create a new chain.\n\nWhat if it's still the other people who are lacking information?\nIt wouldn't be a new chain, it would be the old chain without the new\nevil change, until you manage to show the other people that the change\nwas indeed evil.\nRemember, in this example, the new change being evil is not a\npossibility, but an assumption.\nWhat you're arguing is \"if you haven't been able to stop the evil\nchange, then perhaps it wasn't evil all along and the people trying to\nresist it were wrong and don't know it\".\nBut that contradicts the premise: an evil change being deployed using\nspeedy trial.\n\n>     Since your new chain won't have a hashpower majority, you'll likely\n>     have significant problems if you don't hard fork in a change to\n>     how proof-of-work works; my guess is you'd either want to switch\n>     to a different proof-of-work algorithm, or make your chain able\n>     to be merge-mined against bitcoin, though just following BCH/BSV's\n>     example and tweaking the difficulty adjustment to be more dynamic\n>     could work too.\n\nNo, I disagree. You'll just get the hashpower you pay for with subsidy and fees.\nA better difficulty update filter and merge mining could help you, I\nguess. But that could be a threat on its own.\nAlso, as pointed out earlier, \"mining majority\" is dynamic and depends\non the rewards.\n\n>     (For comparison, apparently BCH has 0.8% of bitcoin's hashrate,\n>     BSV has 0.2%. Meanwhile, Namecoin, RSK and Syscoin, which support\n>     merge-mining, are apparently at 68%, 42% and 17% respectively)\n\nGoogle tells me 0.0073BTC.\nIn perfect competition and leaving fees aside (in which probably\nbitcoin wins too), BCH should have approximately 0.0073% the hashrate\nbitcoin hash. This tells me someone who likes BCH is throwing money\naway to subsidize its security.\nOr perhaps it's something else I'm not taking into account or your\nestimate is wrong.\nBut BCH having 0.8% of bitcoin's hashrate sounds like too much to me.\nAnd yet, what did your future markers \"discovered\" pre hard fork?\n\n>     At the point that you're doing a hard fork, making a clean split is\n>     straightforward: schedule the hard fork for around the same time as\n>     the start of enforcement of the soft fork you oppose, work out how\n>     to make sure you're on your own p2p network, and figure out how\n>     exchanges and lightning channels and everything else are going to\n>     cope with the coin split.\n\nYou shouldn't need to do a hardfork to resist a consensus change you don't like.\n\"around the same time\", with bip8 and the resistance mechanism\nproposed by luke, it doesn't need to be \"around the same time\naccording to some expert who will tell you what to put in your\nsoftware\", but \"exactly at the same time, and you only need to know\nwhich pproposal version bit you're opposing\".\n\n>  6) There's potentially also the case where a soft fork locks-in\n>     and later everyone realises the people who were opposing it were\n>     right all along and the fork is a really bad idea.\n>\n>     If everyone agreed that some idea was irredeemably bad -- eg,\n>     OP_VERIF -- then we could soft fork them out and just forbid\n>     blocks/transactions that attempt to use them. Or conceivably we could\n>     do a hardfork and have more options about how to fix the problem.\n\nYeah, great example. It doesn't have to be an \"evil change\" as such,\nit can just be a \"deeply wrong change\" or something.\nOr if we were using BIP8 and had the resistance mechanism proposed by\nluke, all we would need to do is change one line and recompile:\nI don't remember his enumeration constants but, something like...\n\n- bip8Params.EvilProposalActivationMode = FORCE_ACTIVATION;\n+ bip8Params.EvilProposalActivationMode = FORBID_ACTIVATION;\n\nSay we discover it 3 days before forced activation.\nWell, that would still be much less rushed that the berkeleyDB thing,\nwouldn't it?\nAs you point out, after activation it is much more painful to fix\nthings. In some cases a hardfork may be the best solution a\nposteriori, but I guess that gets out of the scope for activation\nmechanisms.\nIf there's only opposition after it is deployed, whatever the\nactivation mechanism, in that particular case, would be irrelevant.\nWhatever evil change it was, we would have probably swallowed whatever\nthe activation mechanism, because we only thought it evil or wrong a\nposteriori."
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-22T23:49:51",
                "message_text_only": "On Thu, Mar 17, 2022 at 03:04:32PM +0100, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> On Tue, Mar 15, 2022 at 4:45 PM Anthony Towns <aj at erisian.com.au> wrote:\n> > On Fri, Mar 11, 2022 at 02:04:29PM +0000, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> > People opposed to having taproot transactions in their chain had over\n> > three years to do that coordination before an activation method was merged\n> > [0], and then an additional seven months after the activation method was merged before taproot enforcement began [1].\n> >\n> > [0] 2018-01-23 was the original proposal, 2021-04-15 was when speedy\n> >     trial activation parameters for mainnet and testnet were merged.\n> > [1] 2021-11-14\n> People may be opposed only to the final version, but not the initial\n> one or the fundamental concept.\n> Please, try to think of worse case scenarios.\n\nI mean, I've already spent a lot of time thinking through these worst\ncast scenarios, including the ones you bring up. Maybe I've come up with\nwrong or suboptimal conclusions about it, and I'm happy to discuss that,\nbut it's a bit hard to avoid taking offense at the suggestion that I\nhaven't even thought about it.\n\nIn the case of taproot, the final substantive update to the BIP was PR#982\nmerged on 2020-08-27 -- so even if you'd only been opposed to the changes\nin the final version (32B pubkeys perhaps?) you'd have had 1.5 months to\nraise those concerns before the code implementing taproot was merged,\nand 6 months to raise those concerns before activation parameters were\nset. If you'd been following the discussion outside of the code and BIP\ntext, in the case of 32B pubkeys, you'd have had an additional 15 months\nfrom the time the idea was proposed on 2019-05-22 (or 2019-05-29 if you\nonly follow optech's summaries) until it was included in the BIP.\n\n> Perhaps there's no opposition until after activation code has been\n> released and miners are already starting to signal.\n> Perhaps at that moment a reviewer comes and points out a fatal flaw.\n\nPerhaps there's no opposition until the change has been deployed and in\nwide use for 30 years. Aborting activation isn't the be-all and end-all\nof addressing problems with a proposal, and it's not going to be able to\ndeal with every problem. For any problems that can be found before the\nchange is deployed and in use, you want to find them while the proposal\nis being discussed.\n\n\n\nMore broadly, what I don't think you're getting is that *any* method you\ncan use to abort/veto/revert an activation that's occuring via BIP8 (with\nor without mandatory activation), can also be used to abort/veto/revert\na speedy trial activation.\n\nSpeedy trial simply changes two things: it allows a minority (~10%)\nof hashpower to abort the activation; and it guarantees a \"yes\" or \"no\"\nanswer within three months, while with BIP343 you initially don't know\nwhen within a ~1 year period activation will occur.\n\nIf you're part of an (apparent) minority trying to abort/veto/reject\nactivation, this gives you an additional option: if you can get support\nfrom ~10% of hashpower, you can force an initial \"no\" answer within\nthree months, at which point many of the people who were ignoring your\narguments up until then may be willing to reconsider them.\n\nFor example, I think Mark Friedenbach's concerns about unhashed pubkeys\nand quantum resistance don't make sense, and (therefore) aren't widely\nheld; but if 10% of blocks during taproot's speedy trial had included a\ntagline indicating otherwise and prevented activation, that would have\nbeen pretty clear objective evidence that the concern was more widely\nheld than I thought, and might be worth reconsidering. Likewise, there\ncould have somehow been other problems that somehow were being ignored,\nthat could have similarly been reprioritised in the same way.\n\nThat's not the way that you *want* things to work -- ideally people\nshould be raising the concerns beforehand, and they should be taken\nseriously and fixed or addressed beforehand. That did happen with Mark's\nconcerns -- heck, I raised it as a question ~6 hours after Greg's original\ntaproot proposal -- and it's directly addressed in the rationale section\nof BIP341.\n\nBut in the worst case; maybe that doesn't happen. Maybe bitcoin-dev and\nother places are somehow being censored, or sensible critics are being\ndemonised and ignored. The advantage of a hashrate veto here is that it's\nhard to fake and hard to censor -- whereas with mailing list messages and\nthe like, it's both easy to fake (setup sockpuppets and pay troll farms)\nand easy to censor (ban/moderate people for spamming say). So as a last\nditch \"we've been censored, please take us seriously\" method of protest,\nit seems worthwhile to have to me.\n\n(Of course, a 90% majority might *still* choose to not take the concerns\nof the 10% minority seriously, and just continue to ignore the concern\nand followup with an immediate mandatory activation. But if that's what\nhappening, you can't stop it; you can't only choose whether you want to\nbe a part of it, or leave)\n\nAnother example: if we'd had a 3-month speedy trial for segwit, that would\npresumably have run from 2016-11-15 to 2017-02-15, and been successfully\nblocked by people objecting to segwit activation. That would have left a\nclean slate for either a simple and safe BIP149 style UASF activation of\nsegwit (shaolinfry introduced the concept of \"user activated softfork\nactivation\" in a post on 2017-02-25), or redesigning segwit to be\ncompatible with covert ASICBoost (which Greg Maxwell revealed publicly\non 2017-04-05, after apparently realising the potential interaction\nwith segwit a month earlier) and retrying segwit activation with that\napproach via a new speedy trial later in the year.\n\n> > For comparison, the UASF activation attempt for segwit took between 4\n> > to 6 months to coordinate, assuming you start counting from either the\n> > \"user activated soft fork\" concept being raised on bitcoin-dev or the\n> > final params for BIP 148 being merged into the bips repo, and stop\n> > counting when segwit locked in.\n> That was extremely risky and could have been a disaster. \n\nThe question that comment was addressing wasn't whether BIP148 was a\ngood idea, it was how quickly users can coordinate a software update to\nrespond to consensus rules heading in a direction they find unacceptable.\n\nAll the risk and potential for disaster was due to the goals of BIP148:\nto get segwit locked in prior to its activation timeout in Nov 2017,\neven if only supported by a minority of hashrate.\n\n> >  2) If that somehow doesn't work, and people are pushing ahead with a\n> >     consensus change despite significant reasonable opposition; the next\n> >     thing to do would be to establish if either side is a paper tiger\n> >     and setup a futures market. That has the extra benefit of giving\n> >     miners some information about which (combination of) rules will be\n> >     most profitable to mine for.\n> >\n> >     Once that's setup and price discovery happens, one side or the other\n> >     will probably throw in the towel -- there's not much point have a\n> >     money that other people aren't interested in using. (And that more\n> >     or less is what happened with 2X)\n> Future markets can be manipulated.\n\nFutures markets measure people's beliefs weighted by wealth and\nconfidence; and unlike with hashrate signalling there's a real cost to\nlying/being wrong. They're certainly not perfect, but nothing is.\n\n> Regarding 2x, that's not how I remember it. If I remember correctly,\n> \"discovered\" a price in btc for bcash that was\n> orders of magnitude higher than what it is today.\n\n2x and BCH were two different things.\n\nFor BCH, the only futures market was run by viabtc (one of the main\nadvocates of BCH), was only available a week before the split, and was\n(I think?) only available to Chinese investors (at least, it was only\ntraded against CNY). Nevertheless, the price stabilised at around\n$300USD equivalent (0.1 BTC) prior to the split, and that was fairly\nin line with the spot price after the split had occurred. That price\ndropped during the next two weeks to ~0.07 BTC, then rose to ~0.2 BTC,\nand has since dropped to ~0.008 BTC. Coincidentally that's about $300USD\nin today's market, so if you're pricing things in USD, the futures market\nwas actually weirdly accurate.\n\nViabtc also launched a market for BIP148, though in addition to the\nproblems with its BCH market, it was pretty unusable in that if the\nBIP148-valid chain was the most-work chain, the BIP148 token wouldn't\nbe redeemed.\n\nBut the 2x market I was thinking of was bitfinex's; afaik bitfinex is\nreasonably unbiased, the market was fairly accessible and could be traded\nagainst the USD, and it was open for a month before the question of 2x\nwas definitevely resolved. The discovered price was about 0.2 BTC up\nuntil it was announced that 2x was being abandoned at which point it\ndropped to something like 0.02 BTC, representing holding costs until\nthe market was finalised about 2 months later.\n\n> >     If a futures market like that is going to be setup, I think it's\n> >     best if it happens before signalling for the soft fork starts --\n> >     the information miners will get from it is useful for figuring out\n> >     how much resources to invest in signalling, eg. I think it might even\n> >     be feasible to set something up even before activation parameters are\n> >     finalised; you need something more than just one-on-one twitter bets\n> >     to get meaningful price discovery, but I think you could probably\n> >     build something based on a reasonably unbiassed oracle declaring an\n> >     outcome, without precisely defined parameters fixed in a BIP.\n> Whatever miners signal, until there are two chains and their real\n> rewards can be traded, it's hard to know what they will mine\n> afterwards.\n\nI don't agree. The BCH futures market accurately predicted the rewards\n(and hence hashrate) for mining BCH in the first couple of weeks after\nthe split.\n\nOn the same basis, the 2x futures market predicted that mining the 2x\nchain would be massively unprofitable: immediately after the split,\nboth the 2x chain and the original-rules chain would have the same\ndifficulty and hence have the same expected cost to mine a block; but\nthe 2x chain would only have 25% of the reward (0.2 vs 0.8 valuation per\nthe futures market). Without someone subsidising the first 2016 blocks on\nthe 2x chain to the tune of about ~15,000 pre-split bitcoin (or ~75,000\npost-split 2x coins; or between $80M-$150M USD), either directly, or by\nmining at an economic loss, the 2x chain could only collapse.\n\nBCH avoided that fate by having a new difficulty adjustment algorithm\nthat allowed the difficulty to drop immediately, rather than only on\nthe next 2016 block boundary.\n\n> They could signal a change with 100% and then after it is activated on\n> one chain and resisted on another, they 95% of them may switch to the\n> old chain simply because its rewards are 20 times more valuable. This\n> may happen 3 days after activation or 3 months, or more.\n\nIf it's an either-or choice, it's likely that 99.9% of hashrate will\nswitch even if the rewards are only 0.1 times more valuable (or 1.1\ntimes as valuable if you prefer). That's why you run a futures market,\nto figure out which will be more valuable and by how much.\n\nWe saw the either-or case happen with BCH vs BTC; the difficulty of BCH\nwould drop quickly due to the \"EDA\", but only rise slowly, making BCH\nmining more profitable for an extended period so that opportunistic miners\nwould switch to BCH for a while until it got expensive again then switch\nback to BTC, causing both chains' hashrate to be unstable. \n\nBut if you don't hard fork to a different difficulty adjustment algorithm\nthe way BCH did on day one, then it doesn't matter how long miners\ndon't mine on your chain, your chain's difficulty won't adjust, and so\nyou'll need to instead wait until BTC's difficulty doubles or more,\nor its reward halves or more, or some combination of the two. That's\nlikely much more than 3 months away. I can't imagine why anyone would\nstill care about your proposed chain months or years later.\n\nSo hardforking in merge-mining (so it's not an either-or question) or\na new difficulty adjustment algorithm (so you don't have to wait months\nor years) seems a much more realistic approach.\n\n> >     So if acting like reasonable people and talking it through doesn't\n> >     work, this seems like the next step to me.\n> Not to me, but you're free to create your future markets or trade in them.\n> I wouldn't do any of them, and I would advice against it.\n\n*shrug* Do what you like (and I mean, I don't trade in futures markets\neither) but I think you'd be missing out on very useful information,\nand losing a chance for people who aren't devs to offer tangible and\nobjective support for your cause.\n\n> >     I think the speedy trial approach here is ideal for a last ditch\n> >     \"everyone stays on the same chain while avoiding this horrible change\"\n> >     attempt. The reason being that it allows everyone to agree to not\n> >     adopt the new rules with only very little cost: all you need is for\n> >     10% of hashpower to not signal over a three month period.\n> No, 10% of hashpower is not \"very little cost\", that's very expensive.\n\nIf we're talking about consensus changes, the target is 100% of hashpower,\nand also something approaching 100% of nodes. By comparison 10% of\nhashpower is *much* cheaper, especially when the 100% have to actively\nupgrade in order to support, while the 10% just have to not do anything\nin order to oppose.\n\nTo be clear: You don't have to setup the 10% of hashpower yourself,\nyou just have to convince the existing owners of 10% of hashpower to\nnot actively support the change.\n\n> >     That's cheaper than bip9 (5% over 12 months requires 2x the\n> >     cumulative hashpower), and much cheaper than bip8 which requires\n> >     users to update their software\n> Updating software is not expensive. the code for bip8 could have been\n> merged long before taproot was even initially proposed.\n> It could be merged now before another proposal.\n\nThe BIP8 spec we have today is very different to the BIP8 spec when\ntaproot was merged, let alone before it was even proposed. As it was,\nit had serious problems that hadn't been addressed, and the version we\nhave today likewise has significant problems that haven't been addressed,\nwhich is why it wasn't and shouldn't be merged.\n\n> Updating software is certainly not more expensive than getting 10% of\n> the hashrate.\n\nUpdating software (or not updating software) is precisely *how* to get\n10% of hashrate. It's not more or less expensive -- it *is* the expense.\n\n> >  4) At this point, if you were able to prevent activation, hopefully\n> >     that's enough of a power move that people will take your concerns\n> >     seriously, and you get a second chance at step (1). If that still\n> >     results in an impasse, I'd expect there to be a second, non-speedy\n> >     activation of the soft fork, that either cannot be blocked at all, or\n> >     cannot be blocked without having control of at least 60% of hashpower.\n> And if you never got 10% hashpower, we move to the next step, I guess.\n\nYes; you then move to the next step knowing that what level of\ninterest/support you actually have.\n\n> >  5) If you weren't able to prevent activation (whether or not you\n> >     prevented speedy trial from working), then you should have a lot\n> >     of information:\n> >\n> >       - you weren't able to convince people there was a problem\n> >\n> >       - you either weren't in the economic majority and people don't\n> >         think your concept of bitcoin is more valuable (perhaps they\n> >         don't even think it's valuable enough to setup a futures market\n> >         for you)\n> >\n> >       - you can't get control of even 10% of hashpower for a few months\n> >\n> >     and your only option is to accept defeat or create a new chain.\n> What if it's still the other people who are lacking information?\n\nIf it's other people that lack information, there's two options. One,\nyou might be able to explain things to them, so that they learn and gain\nthe information. The other is that for whatever reason they're not willing\nto listen to the truth and will remain ignorant. If it's the first case,\nyou'd have succeeded in an earlier step. If it's the latter, then it's\nnot something you can change, and it doesn't really matter in how you\ndecide what to do next.\n\n> It wouldn't be a new chain, it would be the old chain without the new\n> evil change, until you manage to show the other people that the change\n> was indeed evil.\n> Remember, in this example, the new change being evil is not a\n> possibility, but an assumption.\n\nIt's extremely unhelpful to call things \"evil\" if what you want is a\nreasonable discussion. And if reasonable discussion isn't what you want,\nyou're in the wrong place.\n\nAt this point in the hypothetical you're in a small minority, and have\nbeen unable to convince people of your point of view. Calling the people\nyou disagree with \"evil\" (and saying they support something that's evil\nis exactly that) isn't going to improve your situation, and doing it in\na hypothetical sure feels like bad faith.\n\n> What you're arguing is \"if you haven't been able to stop the evil\n> change, then perhaps it wasn't evil all along and the people trying to\n> resist it were wrong and don't know it\".\n\nIf it's an evil change, then good people will oppose it. You've tried\nconvincing devs in the \"discuss the proposal\" stage, whales in the\n\"futures market\" stage, and miners in the \"hashpower signalling\" phase,\nand failed each time because the good people in each of those groups\nhaven't opposed it. So yes, I think the most likely explanation is that\nyou're wrong in thinking it's evil.\n\nBut hey what about the worst case: what if everyone else in bitcoin\nis evil and supports doing evil things. And maybe that's not even\nimplausible: maybe it's not an \"evil\" thing per se, perhaps it's simply\nequally \"misguided\" as the things that central banks or wall street or\nsimilar are doing today. Perhaps bitcoin becomes the world currency,\nand in 100 or 200 years time, whether through complacency and forgetting\nthe lessons of the past, or too much adherence to dogma that no longer\nmatches reality, or just hitting some new problem that's never been seen\nbefore and an inability to perfectly predict the future, and as a result\nmost of the world opts into some change that will cause bitcoin to fail.\n\nIn that scenario, I think a hard fork is the best choice: split out a new\ncoin that will survive the upcoming crash, adjust the mining/difficulty\nalgorithm so it works from day one, and set it up so that you can\nmaintain it along with the people who support your vision, rather than\nhaving to constantly deal with well-meaning attacks from \"bitcoiners\"\nwho don't see the risks and have lost the plot.\n\nBasically: do what Satoshi did and create a better system, and let\neveryone else join you as the problems with the old one eventually become\nunavoidably obvious.\n\n> But that contradicts the premise: an evil change being deployed using\n> speedy trial.\n\nAgain: any change that could be avoided if it were deployed via BIP8,\ncan also be avoided *by the exact same techniques* if it were deployed\nvia speedy trial or a similar approach.\n\n> >     Since your new chain won't have a hashpower majority, you'll likely\n> >     have significant problems if you don't hard fork in a change to\n> >     how proof-of-work works; my guess is you'd either want to switch\n> >     to a different proof-of-work algorithm, or make your chain able\n> >     to be merge-mined against bitcoin, though just following BCH/BSV's\n> >     example and tweaking the difficulty adjustment to be more dynamic\n> >     could work too.\n> No, I disagree. You'll just get the hashpower you pay for with subsidy and fees.\n\nThe value of the subsidy is something you can directly figure out from\nrunning a futures market; and unless you're deliberately subsidising fees,\nthey'll almost certainly be ~0.\n\n> >     (For comparison, apparently BCH has 0.8% of bitcoin's hashrate,\n> >     BSV has 0.2%. Meanwhile, Namecoin, RSK and Syscoin, which support\n> >     merge-mining, are apparently at 68%, 42% and 17% respectively)\n> Google tells me 0.0073BTC.\n\nI think you're reading too much precision into those numbers? When\nI looked again the other day, I got a figure of 0.66%; today I get\n0.75%. I'm sure I rounded whatever figure I saw to one significant figure,\nso it might have been 0.75% then too.\n\nhttps://bitinfocharts.com/comparison/bitcoin-hashrate.html#3y\nhttps://bitinfocharts.com/comparison/bitcoin%20cash-hashrate.html#3y\n\n> In perfect competition and leaving fees aside (in which probably\n> bitcoin wins too), BCH should have approximately 0.0073% the hashrate\n> bitcoin hash.\n\nOh, or you're just getting the percentage conversion wrong -- 0.0073\nBTC is 0.73% of a BTC, and thus it would be expected to have about 0.73%\nof the hashrate.\n\n> >     At the point that you're doing a hard fork, making a clean split is\n> >     straightforward: schedule the hard fork for around the same time as\n> >     the start of enforcement of the soft fork you oppose, work out how\n> >     to make sure you're on your own p2p network, and figure out how\n> >     exchanges and lightning channels and everything else are going to\n> >     cope with the coin split.\n> You shouldn't need to do a hardfork to resist a consensus change you don't like.\n\nOf course; that's why option (1) is to talk to people about why it's a\nbad idea so it doesn't get proposed in the first place.\n\nBut if you want to resist a consensus change that is overwhelmingly\nsupported by the rest of the bitcoin economy, and for which your reasons\naren't even considered particularly logical by everyone else, then yeah,\nif you really want to go off on your own because everyone else is wrong,\nyou *should* do a hardfork.\n\nIf a change doesn't have overwhelming support, then hopefully the costs\nto get 90% of hashrate signalling is a significant impediment. If you do\nhave overwhelming support, then the cost to get 90% of hashrate signalling\n(or even apparently 99.8%, see getdeploymentinfo on block 693503) --\ndoesn't seem to be too bad.\n\n> \"around the same time\", with bip8 and the resistance mechanism\n> proposed by luke, it doesn't need to be \"around the same time\n> according to some expert who will tell you what to put in your\n> software\", but \"exactly at the same time, and you only need to know\n> which pproposal version bit you're opposing\".\n\n(Arguing semantics: You can't do the split at exactly the same time,\nbecause the split starts with each chain finding a new block, and blocks\nare found probabilistically depending on hashrate, so they won't be found\nat the same time. Or, alternatively, the split happens whenever either\nclient considers the other chain invalid, and always happens at the\n\"same\" time)\n\nIf you want to do things at exactly the same height, you can do that if\nthe soft fork is activated by speedy trial as well.\n\nI'd say the same height approach works better on speedy trial than\nwith BIP8/BIP343, since with speedy trial signalling is only for a\nshort period, and hence you know well in advance if and when you'll be\nsplitting, whereas with an extended signalling period that goes for a\nyear past the minimum activation height, you may find yourself splitting\nat any point in that year with as little as two week's notice.\n\nIf I were doing a hardfork coin split to avoid following some new soft\nforked rules that I think were horrible, I think I'd prefer to do the\nsplit in advance of the softfork -- that way exchanges/wallets/lightning\nchannels/etc that have to do work to deal with the coinsplit aren't\ndistracted by simultaneously having to pay attention to the new softfork.\nYMMV of course.\n\n> Yeah, great example. It doesn't have to be an \"evil change\" as such,\n> it can just be a \"deeply wrong change\" or something.\n> Or if we were using BIP8 and had the resistance mechanism proposed by\n> luke, all we would need to do is change one line and recompile:\n> I don't remember his enumeration constants but, something like...\n> - bip8Params.EvilProposalActivationMode = FORCE_ACTIVATION;\n> + bip8Params.EvilProposalActivationMode = FORBID_ACTIVATION;\n> Say we discover it 3 days before forced activation.\n> Well, that would still be much less rushed that the berkeleyDB thing,\n> wouldn't it?\n\nNo, exactly the opposite.\n\nIn order to abort a BIP8 activation, 100% of hashpower and 100% of\nnode software needs to downgrade from anything that specifies BIP8 with\nmandatory activation.\n\nThe \"berkelyDB thing\" was an accidental hard fork due to the updated\nsoftware with leveldb being able to accept larger blocks than the old\nbdb-based bitcoind could. \n\nThe result was two chains: one with a large block in it, that could\nonly be validated by the newer software, and a less work chain with only\nsmaller chains, that could be validated by both versions of the software;\nthe problem was ~60% of hashpower was on the larger-block chain, but\nmany nodes including those with ~40% hashpower. The problem was quickly\nmitigated by encouraging a majority of hashpower to downgrade to the\nold software, resulting in them rejecting the larger-block chain,\nat which point a majority of hashpower was mining the smaller-block\nchain, and the smaller-block chain eventually having more work than\nthe larger-block chain. At that point any newer nodes reorged to the\nmore-work, smaller-block chain, and everyone was following the same chain.\n\nWhat that means is that the operators of *two* pools downgraded their\nsoftware, and everything was fixed. That's a *lot* less work than\neveryone who upgraded their node having to downgrade/re-update, and\nit was done that way to *avoid* having to rush to get everyone to do\nan emergency update of their node software to be compatible with the\nlarger-block chain.\n\nSee https://bitcoin.org/en/alert/2013-03-11-chain-fork\nand https://github.com/bitcoin/bips/blob/master/bip-0050.mediawiki\n\nOn the other hand, that approach only works because it takes advantage\nof a lot of hashrate being centralised around a few pools; if we succeed\nin making block construction more decentralised, solutions here will\nonly become harder.\n\n> If there's only opposition after it is deployed, whatever the\n> activation mechanism, in that particular case, would be irrelevant.\n\nOnce you've released software with a softfork activated via BIP8 with\nmandatory activation (ie, lot=true), and it has achieved any significant\nadoption, the soft fork is already deployed and you need to treat it as\nsuch. If you want to have an easier way of undoing the softfork than\nyou would have for one that's already active on the network, you need\na different activation method than BIP8/lot=true.\n\nCheers,\naj"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-24T18:30:09",
                "message_text_only": "Sorry, I won't answer to everything, because it's clear you're not listening.\nIn the HYPOTHETICAL CASE that there's an evil for, the fork being evil\nis a PREMISE of that hypothetical case, a GIVEN.\nYour claim that \"if it's evil, good people would oppose it\" is a NON\nSEQUITUR, \"good people\" aren't necessarily perfect and all knowing.\ngood people can make mistakes, they can be fooled too.\nIn the hypothetical case that THERE'S AN EVIL FORK, if \"good people\"\ndon't complain, it is because they didn't realize that the given fork\nwas evil. Because in our hypothetical example THE EVIL FORK IS EVIL BY\nDEFINITION, THAT'S THE HYPOTHETICAL CASE I WANT TO DISCUSS, not the\nhypothetical case where there's a fork some people think it's evil but\nit's not really evil.\n\nRepeat with me: in the hypothetical case that there's an evil fork,\nthen the fork is evil by definition, that's the hypothetical case\nwe're discussing.\n\nOnce you understand what hypothetical case I'm talking about, maybe\nyou can understand the rest of my reasoning.\nBut if you don't understand the PREMISES of my example, it is\nimpossible that you can understand my reasonings about the\nhypothetical example.\n\nI'm sorry about the upper cases, but I really don't know how else I\ncould be clearer about the PREMISES being PREMISES and not just\npossibilities. If you can't imagine a scenario where good people don't\noppose an evil fork, then you can't imagine the scenario I'm talking\nabout, sorry.\n\nEvil fork deployed with speedy trial vs evil fork deployed with BIP8,\nthat's what I'm talking about.\nPlease, stop the \"then it's not an evil fork\" contradiction of the premises.\n\nAt this point, I don't think I can be clearer about the main premise\nof my example, sorry.\n\nOn Wed, Mar 23, 2022 at 12:50 AM Anthony Towns <aj at erisian.com.au> wrote:\n>\n> On Thu, Mar 17, 2022 at 03:04:32PM +0100, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> > On Tue, Mar 15, 2022 at 4:45 PM Anthony Towns <aj at erisian.com.au> wrote:\n> > > On Fri, Mar 11, 2022 at 02:04:29PM +0000, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> > > People opposed to having taproot transactions in their chain had over\n> > > three years to do that coordination before an activation method was merged\n> > > [0], and then an additional seven months after the activation method was merged before taproot enforcement began [1].\n> > >\n> > > [0] 2018-01-23 was the original proposal, 2021-04-15 was when speedy\n> > >     trial activation parameters for mainnet and testnet were merged.\n> > > [1] 2021-11-14\n> > People may be opposed only to the final version, but not the initial\n> > one or the fundamental concept.\n> > Please, try to think of worse case scenarios.\n>\n> I mean, I've already spent a lot of time thinking through these worst\n> cast scenarios, including the ones you bring up. Maybe I've come up with\n> wrong or suboptimal conclusions about it, and I'm happy to discuss that,\n> but it's a bit hard to avoid taking offense at the suggestion that I\n> haven't even thought about it.\n>\n> In the case of taproot, the final substantive update to the BIP was PR#982\n> merged on 2020-08-27 -- so even if you'd only been opposed to the changes\n> in the final version (32B pubkeys perhaps?) you'd have had 1.5 months to\n> raise those concerns before the code implementing taproot was merged,\n> and 6 months to raise those concerns before activation parameters were\n> set. If you'd been following the discussion outside of the code and BIP\n> text, in the case of 32B pubkeys, you'd have had an additional 15 months\n> from the time the idea was proposed on 2019-05-22 (or 2019-05-29 if you\n> only follow optech's summaries) until it was included in the BIP.\n>\n> > Perhaps there's no opposition until after activation code has been\n> > released and miners are already starting to signal.\n> > Perhaps at that moment a reviewer comes and points out a fatal flaw.\n>\n> Perhaps there's no opposition until the change has been deployed and in\n> wide use for 30 years. Aborting activation isn't the be-all and end-all\n> of addressing problems with a proposal, and it's not going to be able to\n> deal with every problem. For any problems that can be found before the\n> change is deployed and in use, you want to find them while the proposal\n> is being discussed.\n>\n>\n>\n> More broadly, what I don't think you're getting is that *any* method you\n> can use to abort/veto/revert an activation that's occuring via BIP8 (with\n> or without mandatory activation), can also be used to abort/veto/revert\n> a speedy trial activation.\n>\n> Speedy trial simply changes two things: it allows a minority (~10%)\n> of hashpower to abort the activation; and it guarantees a \"yes\" or \"no\"\n> answer within three months, while with BIP343 you initially don't know\n> when within a ~1 year period activation will occur.\n>\n> If you're part of an (apparent) minority trying to abort/veto/reject\n> activation, this gives you an additional option: if you can get support\n> from ~10% of hashpower, you can force an initial \"no\" answer within\n> three months, at which point many of the people who were ignoring your\n> arguments up until then may be willing to reconsider them.\n>\n> For example, I think Mark Friedenbach's concerns about unhashed pubkeys\n> and quantum resistance don't make sense, and (therefore) aren't widely\n> held; but if 10% of blocks during taproot's speedy trial had included a\n> tagline indicating otherwise and prevented activation, that would have\n> been pretty clear objective evidence that the concern was more widely\n> held than I thought, and might be worth reconsidering. Likewise, there\n> could have somehow been other problems that somehow were being ignored,\n> that could have similarly been reprioritised in the same way.\n>\n> That's not the way that you *want* things to work -- ideally people\n> should be raising the concerns beforehand, and they should be taken\n> seriously and fixed or addressed beforehand. That did happen with Mark's\n> concerns -- heck, I raised it as a question ~6 hours after Greg's original\n> taproot proposal -- and it's directly addressed in the rationale section\n> of BIP341.\n>\n> But in the worst case; maybe that doesn't happen. Maybe bitcoin-dev and\n> other places are somehow being censored, or sensible critics are being\n> demonised and ignored. The advantage of a hashrate veto here is that it's\n> hard to fake and hard to censor -- whereas with mailing list messages and\n> the like, it's both easy to fake (setup sockpuppets and pay troll farms)\n> and easy to censor (ban/moderate people for spamming say). So as a last\n> ditch \"we've been censored, please take us seriously\" method of protest,\n> it seems worthwhile to have to me.\n>\n> (Of course, a 90% majority might *still* choose to not take the concerns\n> of the 10% minority seriously, and just continue to ignore the concern\n> and followup with an immediate mandatory activation. But if that's what\n> happening, you can't stop it; you can't only choose whether you want to\n> be a part of it, or leave)\n>\n> Another example: if we'd had a 3-month speedy trial for segwit, that would\n> presumably have run from 2016-11-15 to 2017-02-15, and been successfully\n> blocked by people objecting to segwit activation. That would have left a\n> clean slate for either a simple and safe BIP149 style UASF activation of\n> segwit (shaolinfry introduced the concept of \"user activated softfork\n> activation\" in a post on 2017-02-25), or redesigning segwit to be\n> compatible with covert ASICBoost (which Greg Maxwell revealed publicly\n> on 2017-04-05, after apparently realising the potential interaction\n> with segwit a month earlier) and retrying segwit activation with that\n> approach via a new speedy trial later in the year.\n>\n> > > For comparison, the UASF activation attempt for segwit took between 4\n> > > to 6 months to coordinate, assuming you start counting from either the\n> > > \"user activated soft fork\" concept being raised on bitcoin-dev or the\n> > > final params for BIP 148 being merged into the bips repo, and stop\n> > > counting when segwit locked in.\n> > That was extremely risky and could have been a disaster.\n>\n> The question that comment was addressing wasn't whether BIP148 was a\n> good idea, it was how quickly users can coordinate a software update to\n> respond to consensus rules heading in a direction they find unacceptable.\n>\n> All the risk and potential for disaster was due to the goals of BIP148:\n> to get segwit locked in prior to its activation timeout in Nov 2017,\n> even if only supported by a minority of hashrate.\n>\n> > >  2) If that somehow doesn't work, and people are pushing ahead with a\n> > >     consensus change despite significant reasonable opposition; the next\n> > >     thing to do would be to establish if either side is a paper tiger\n> > >     and setup a futures market. That has the extra benefit of giving\n> > >     miners some information about which (combination of) rules will be\n> > >     most profitable to mine for.\n> > >\n> > >     Once that's setup and price discovery happens, one side or the other\n> > >     will probably throw in the towel -- there's not much point have a\n> > >     money that other people aren't interested in using. (And that more\n> > >     or less is what happened with 2X)\n> > Future markets can be manipulated.\n>\n> Futures markets measure people's beliefs weighted by wealth and\n> confidence; and unlike with hashrate signalling there's a real cost to\n> lying/being wrong. They're certainly not perfect, but nothing is.\n>\n> > Regarding 2x, that's not how I remember it. If I remember correctly,\n> > \"discovered\" a price in btc for bcash that was\n> > orders of magnitude higher than what it is today.\n>\n> 2x and BCH were two different things.\n>\n> For BCH, the only futures market was run by viabtc (one of the main\n> advocates of BCH), was only available a week before the split, and was\n> (I think?) only available to Chinese investors (at least, it was only\n> traded against CNY). Nevertheless, the price stabilised at around\n> $300USD equivalent (0.1 BTC) prior to the split, and that was fairly\n> in line with the spot price after the split had occurred. That price\n> dropped during the next two weeks to ~0.07 BTC, then rose to ~0.2 BTC,\n> and has since dropped to ~0.008 BTC. Coincidentally that's about $300USD\n> in today's market, so if you're pricing things in USD, the futures market\n> was actually weirdly accurate.\n>\n> Viabtc also launched a market for BIP148, though in addition to the\n> problems with its BCH market, it was pretty unusable in that if the\n> BIP148-valid chain was the most-work chain, the BIP148 token wouldn't\n> be redeemed.\n>\n> But the 2x market I was thinking of was bitfinex's; afaik bitfinex is\n> reasonably unbiased, the market was fairly accessible and could be traded\n> against the USD, and it was open for a month before the question of 2x\n> was definitevely resolved. The discovered price was about 0.2 BTC up\n> until it was announced that 2x was being abandoned at which point it\n> dropped to something like 0.02 BTC, representing holding costs until\n> the market was finalised about 2 months later.\n>\n> > >     If a futures market like that is going to be setup, I think it's\n> > >     best if it happens before signalling for the soft fork starts --\n> > >     the information miners will get from it is useful for figuring out\n> > >     how much resources to invest in signalling, eg. I think it might even\n> > >     be feasible to set something up even before activation parameters are\n> > >     finalised; you need something more than just one-on-one twitter bets\n> > >     to get meaningful price discovery, but I think you could probably\n> > >     build something based on a reasonably unbiassed oracle declaring an\n> > >     outcome, without precisely defined parameters fixed in a BIP.\n> > Whatever miners signal, until there are two chains and their real\n> > rewards can be traded, it's hard to know what they will mine\n> > afterwards.\n>\n> I don't agree. The BCH futures market accurately predicted the rewards\n> (and hence hashrate) for mining BCH in the first couple of weeks after\n> the split.\n>\n> On the same basis, the 2x futures market predicted that mining the 2x\n> chain would be massively unprofitable: immediately after the split,\n> both the 2x chain and the original-rules chain would have the same\n> difficulty and hence have the same expected cost to mine a block; but\n> the 2x chain would only have 25% of the reward (0.2 vs 0.8 valuation per\n> the futures market). Without someone subsidising the first 2016 blocks on\n> the 2x chain to the tune of about ~15,000 pre-split bitcoin (or ~75,000\n> post-split 2x coins; or between $80M-$150M USD), either directly, or by\n> mining at an economic loss, the 2x chain could only collapse.\n>\n> BCH avoided that fate by having a new difficulty adjustment algorithm\n> that allowed the difficulty to drop immediately, rather than only on\n> the next 2016 block boundary.\n>\n> > They could signal a change with 100% and then after it is activated on\n> > one chain and resisted on another, they 95% of them may switch to the\n> > old chain simply because its rewards are 20 times more valuable. This\n> > may happen 3 days after activation or 3 months, or more.\n>\n> If it's an either-or choice, it's likely that 99.9% of hashrate will\n> switch even if the rewards are only 0.1 times more valuable (or 1.1\n> times as valuable if you prefer). That's why you run a futures market,\n> to figure out which will be more valuable and by how much.\n>\n> We saw the either-or case happen with BCH vs BTC; the difficulty of BCH\n> would drop quickly due to the \"EDA\", but only rise slowly, making BCH\n> mining more profitable for an extended period so that opportunistic miners\n> would switch to BCH for a while until it got expensive again then switch\n> back to BTC, causing both chains' hashrate to be unstable.\n>\n> But if you don't hard fork to a different difficulty adjustment algorithm\n> the way BCH did on day one, then it doesn't matter how long miners\n> don't mine on your chain, your chain's difficulty won't adjust, and so\n> you'll need to instead wait until BTC's difficulty doubles or more,\n> or its reward halves or more, or some combination of the two. That's\n> likely much more than 3 months away. I can't imagine why anyone would\n> still care about your proposed chain months or years later.\n>\n> So hardforking in merge-mining (so it's not an either-or question) or\n> a new difficulty adjustment algorithm (so you don't have to wait months\n> or years) seems a much more realistic approach.\n>\n> > >     So if acting like reasonable people and talking it through doesn't\n> > >     work, this seems like the next step to me.\n> > Not to me, but you're free to create your future markets or trade in them.\n> > I wouldn't do any of them, and I would advice against it.\n>\n> *shrug* Do what you like (and I mean, I don't trade in futures markets\n> either) but I think you'd be missing out on very useful information,\n> and losing a chance for people who aren't devs to offer tangible and\n> objective support for your cause.\n>\n> > >     I think the speedy trial approach here is ideal for a last ditch\n> > >     \"everyone stays on the same chain while avoiding this horrible change\"\n> > >     attempt. The reason being that it allows everyone to agree to not\n> > >     adopt the new rules with only very little cost: all you need is for\n> > >     10% of hashpower to not signal over a three month period.\n> > No, 10% of hashpower is not \"very little cost\", that's very expensive.\n>\n> If we're talking about consensus changes, the target is 100% of hashpower,\n> and also something approaching 100% of nodes. By comparison 10% of\n> hashpower is *much* cheaper, especially when the 100% have to actively\n> upgrade in order to support, while the 10% just have to not do anything\n> in order to oppose.\n>\n> To be clear: You don't have to setup the 10% of hashpower yourself,\n> you just have to convince the existing owners of 10% of hashpower to\n> not actively support the change.\n>\n> > >     That's cheaper than bip9 (5% over 12 months requires 2x the\n> > >     cumulative hashpower), and much cheaper than bip8 which requires\n> > >     users to update their software\n> > Updating software is not expensive. the code for bip8 could have been\n> > merged long before taproot was even initially proposed.\n> > It could be merged now before another proposal.\n>\n> The BIP8 spec we have today is very different to the BIP8 spec when\n> taproot was merged, let alone before it was even proposed. As it was,\n> it had serious problems that hadn't been addressed, and the version we\n> have today likewise has significant problems that haven't been addressed,\n> which is why it wasn't and shouldn't be merged.\n>\n> > Updating software is certainly not more expensive than getting 10% of\n> > the hashrate.\n>\n> Updating software (or not updating software) is precisely *how* to get\n> 10% of hashrate. It's not more or less expensive -- it *is* the expense.\n>\n> > >  4) At this point, if you were able to prevent activation, hopefully\n> > >     that's enough of a power move that people will take your concerns\n> > >     seriously, and you get a second chance at step (1). If that still\n> > >     results in an impasse, I'd expect there to be a second, non-speedy\n> > >     activation of the soft fork, that either cannot be blocked at all, or\n> > >     cannot be blocked without having control of at least 60% of hashpower.\n> > And if you never got 10% hashpower, we move to the next step, I guess.\n>\n> Yes; you then move to the next step knowing that what level of\n> interest/support you actually have.\n>\n> > >  5) If you weren't able to prevent activation (whether or not you\n> > >     prevented speedy trial from working), then you should have a lot\n> > >     of information:\n> > >\n> > >       - you weren't able to convince people there was a problem\n> > >\n> > >       - you either weren't in the economic majority and people don't\n> > >         think your concept of bitcoin is more valuable (perhaps they\n> > >         don't even think it's valuable enough to setup a futures market\n> > >         for you)\n> > >\n> > >       - you can't get control of even 10% of hashpower for a few months\n> > >\n> > >     and your only option is to accept defeat or create a new chain.\n> > What if it's still the other people who are lacking information?\n>\n> If it's other people that lack information, there's two options. One,\n> you might be able to explain things to them, so that they learn and gain\n> the information. The other is that for whatever reason they're not willing\n> to listen to the truth and will remain ignorant. If it's the first case,\n> you'd have succeeded in an earlier step. If it's the latter, then it's\n> not something you can change, and it doesn't really matter in how you\n> decide what to do next.\n>\n> > It wouldn't be a new chain, it would be the old chain without the new\n> > evil change, until you manage to show the other people that the change\n> > was indeed evil.\n> > Remember, in this example, the new change being evil is not a\n> > possibility, but an assumption.\n>\n> It's extremely unhelpful to call things \"evil\" if what you want is a\n> reasonable discussion. And if reasonable discussion isn't what you want,\n> you're in the wrong place.\n>\n> At this point in the hypothetical you're in a small minority, and have\n> been unable to convince people of your point of view. Calling the people\n> you disagree with \"evil\" (and saying they support something that's evil\n> is exactly that) isn't going to improve your situation, and doing it in\n> a hypothetical sure feels like bad faith.\n>\n> > What you're arguing is \"if you haven't been able to stop the evil\n> > change, then perhaps it wasn't evil all along and the people trying to\n> > resist it were wrong and don't know it\".\n>\n> If it's an evil change, then good people will oppose it. You've tried\n> convincing devs in the \"discuss the proposal\" stage, whales in the\n> \"futures market\" stage, and miners in the \"hashpower signalling\" phase,\n> and failed each time because the good people in each of those groups\n> haven't opposed it. So yes, I think the most likely explanation is that\n> you're wrong in thinking it's evil.\n>\n> But hey what about the worst case: what if everyone else in bitcoin\n> is evil and supports doing evil things. And maybe that's not even\n> implausible: maybe it's not an \"evil\" thing per se, perhaps it's simply\n> equally \"misguided\" as the things that central banks or wall street or\n> similar are doing today. Perhaps bitcoin becomes the world currency,\n> and in 100 or 200 years time, whether through complacency and forgetting\n> the lessons of the past, or too much adherence to dogma that no longer\n> matches reality, or just hitting some new problem that's never been seen\n> before and an inability to perfectly predict the future, and as a result\n> most of the world opts into some change that will cause bitcoin to fail.\n>\n> In that scenario, I think a hard fork is the best choice: split out a new\n> coin that will survive the upcoming crash, adjust the mining/difficulty\n> algorithm so it works from day one, and set it up so that you can\n> maintain it along with the people who support your vision, rather than\n> having to constantly deal with well-meaning attacks from \"bitcoiners\"\n> who don't see the risks and have lost the plot.\n>\n> Basically: do what Satoshi did and create a better system, and let\n> everyone else join you as the problems with the old one eventually become\n> unavoidably obvious.\n>\n> > But that contradicts the premise: an evil change being deployed using\n> > speedy trial.\n>\n> Again: any change that could be avoided if it were deployed via BIP8,\n> can also be avoided *by the exact same techniques* if it were deployed\n> via speedy trial or a similar approach.\n>\n> > >     Since your new chain won't have a hashpower majority, you'll likely\n> > >     have significant problems if you don't hard fork in a change to\n> > >     how proof-of-work works; my guess is you'd either want to switch\n> > >     to a different proof-of-work algorithm, or make your chain able\n> > >     to be merge-mined against bitcoin, though just following BCH/BSV's\n> > >     example and tweaking the difficulty adjustment to be more dynamic\n> > >     could work too.\n> > No, I disagree. You'll just get the hashpower you pay for with subsidy and fees.\n>\n> The value of the subsidy is something you can directly figure out from\n> running a futures market; and unless you're deliberately subsidising fees,\n> they'll almost certainly be ~0.\n>\n> > >     (For comparison, apparently BCH has 0.8% of bitcoin's hashrate,\n> > >     BSV has 0.2%. Meanwhile, Namecoin, RSK and Syscoin, which support\n> > >     merge-mining, are apparently at 68%, 42% and 17% respectively)\n> > Google tells me 0.0073BTC.\n>\n> I think you're reading too much precision into those numbers? When\n> I looked again the other day, I got a figure of 0.66%; today I get\n> 0.75%. I'm sure I rounded whatever figure I saw to one significant figure,\n> so it might have been 0.75% then too.\n>\n> https://bitinfocharts.com/comparison/bitcoin-hashrate.html#3y\n> https://bitinfocharts.com/comparison/bitcoin%20cash-hashrate.html#3y\n>\n> > In perfect competition and leaving fees aside (in which probably\n> > bitcoin wins too), BCH should have approximately 0.0073% the hashrate\n> > bitcoin hash.\n>\n> Oh, or you're just getting the percentage conversion wrong -- 0.0073\n> BTC is 0.73% of a BTC, and thus it would be expected to have about 0.73%\n> of the hashrate.\n>\n> > >     At the point that you're doing a hard fork, making a clean split is\n> > >     straightforward: schedule the hard fork for around the same time as\n> > >     the start of enforcement of the soft fork you oppose, work out how\n> > >     to make sure you're on your own p2p network, and figure out how\n> > >     exchanges and lightning channels and everything else are going to\n> > >     cope with the coin split.\n> > You shouldn't need to do a hardfork to resist a consensus change you don't like.\n>\n> Of course; that's why option (1) is to talk to people about why it's a\n> bad idea so it doesn't get proposed in the first place.\n>\n> But if you want to resist a consensus change that is overwhelmingly\n> supported by the rest of the bitcoin economy, and for which your reasons\n> aren't even considered particularly logical by everyone else, then yeah,\n> if you really want to go off on your own because everyone else is wrong,\n> you *should* do a hardfork.\n>\n> If a change doesn't have overwhelming support, then hopefully the costs\n> to get 90% of hashrate signalling is a significant impediment. If you do\n> have overwhelming support, then the cost to get 90% of hashrate signalling\n> (or even apparently 99.8%, see getdeploymentinfo on block 693503) --\n> doesn't seem to be too bad.\n>\n> > \"around the same time\", with bip8 and the resistance mechanism\n> > proposed by luke, it doesn't need to be \"around the same time\n> > according to some expert who will tell you what to put in your\n> > software\", but \"exactly at the same time, and you only need to know\n> > which pproposal version bit you're opposing\".\n>\n> (Arguing semantics: You can't do the split at exactly the same time,\n> because the split starts with each chain finding a new block, and blocks\n> are found probabilistically depending on hashrate, so they won't be found\n> at the same time. Or, alternatively, the split happens whenever either\n> client considers the other chain invalid, and always happens at the\n> \"same\" time)\n>\n> If you want to do things at exactly the same height, you can do that if\n> the soft fork is activated by speedy trial as well.\n>\n> I'd say the same height approach works better on speedy trial than\n> with BIP8/BIP343, since with speedy trial signalling is only for a\n> short period, and hence you know well in advance if and when you'll be\n> splitting, whereas with an extended signalling period that goes for a\n> year past the minimum activation height, you may find yourself splitting\n> at any point in that year with as little as two week's notice.\n>\n> If I were doing a hardfork coin split to avoid following some new soft\n> forked rules that I think were horrible, I think I'd prefer to do the\n> split in advance of the softfork -- that way exchanges/wallets/lightning\n> channels/etc that have to do work to deal with the coinsplit aren't\n> distracted by simultaneously having to pay attention to the new softfork.\n> YMMV of course.\n>\n> > Yeah, great example. It doesn't have to be an \"evil change\" as such,\n> > it can just be a \"deeply wrong change\" or something.\n> > Or if we were using BIP8 and had the resistance mechanism proposed by\n> > luke, all we would need to do is change one line and recompile:\n> > I don't remember his enumeration constants but, something like...\n> > - bip8Params.EvilProposalActivationMode = FORCE_ACTIVATION;\n> > + bip8Params.EvilProposalActivationMode = FORBID_ACTIVATION;\n> > Say we discover it 3 days before forced activation.\n> > Well, that would still be much less rushed that the berkeleyDB thing,\n> > wouldn't it?\n>\n> No, exactly the opposite.\n>\n> In order to abort a BIP8 activation, 100% of hashpower and 100% of\n> node software needs to downgrade from anything that specifies BIP8 with\n> mandatory activation.\n>\n> The \"berkelyDB thing\" was an accidental hard fork due to the updated\n> software with leveldb being able to accept larger blocks than the old\n> bdb-based bitcoind could.\n>\n> The result was two chains: one with a large block in it, that could\n> only be validated by the newer software, and a less work chain with only\n> smaller chains, that could be validated by both versions of the software;\n> the problem was ~60% of hashpower was on the larger-block chain, but\n> many nodes including those with ~40% hashpower. The problem was quickly\n> mitigated by encouraging a majority of hashpower to downgrade to the\n> old software, resulting in them rejecting the larger-block chain,\n> at which point a majority of hashpower was mining the smaller-block\n> chain, and the smaller-block chain eventually having more work than\n> the larger-block chain. At that point any newer nodes reorged to the\n> more-work, smaller-block chain, and everyone was following the same chain.\n>\n> What that means is that the operators of *two* pools downgraded their\n> software, and everything was fixed. That's a *lot* less work than\n> everyone who upgraded their node having to downgrade/re-update, and\n> it was done that way to *avoid* having to rush to get everyone to do\n> an emergency update of their node software to be compatible with the\n> larger-block chain.\n>\n> See https://bitcoin.org/en/alert/2013-03-11-chain-fork\n> and https://github.com/bitcoin/bips/blob/master/bip-0050.mediawiki\n>\n> On the other hand, that approach only works because it takes advantage\n> of a lot of hashrate being centralised around a few pools; if we succeed\n> in making block construction more decentralised, solutions here will\n> only become harder.\n>\n> > If there's only opposition after it is deployed, whatever the\n> > activation mechanism, in that particular case, would be irrelevant.\n>\n> Once you've released software with a softfork activated via BIP8 with\n> mandatory activation (ie, lot=true), and it has achieved any significant\n> adoption, the soft fork is already deployed and you need to treat it as\n> such. If you want to have an easier way of undoing the softfork than\n> you would have for one that's already active on the network, you need\n> a different activation method than BIP8/lot=true.\n>\n> Cheers,\n> aj\n>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-26T01:45:46",
                "message_text_only": "On Thu, Mar 24, 2022 at 07:30:09PM +0100, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> Sorry, I won't answer to everything, because it's clear you're not listening.\n\nI'm not agreeing with you; that's different to not listening to you.\n\n> In the HYPOTHETICAL CASE that there's an evil for, the fork being evil\n> is a PREMISE of that hypothetical case, a GIVEN.\n\nDo you really find people more inclined to start agreeing with you when\nyou begin yelling at them? When other people start shouting at you,\ndo you feel like it's a discussion that you're engaged in?\n\n> Your claim that \"if it's evil, good people would oppose it\" is a NON\n> SEQUITUR, \"good people\" aren't necessarily perfect and all knowing.\n> good people can make mistakes, they can be fooled too.\n> In the hypothetical case that THERE'S AN EVIL FORK, if \"good people\"\n> don't complain, it is because they didn't realize that the given fork\n> was evil. Because in our hypothetical example THE EVIL FORK IS EVIL BY\n> DEFINITION, THAT'S THE HYPOTHETICAL CASE I WANT TO DISCUSS, not the\n> hypothetical case where there's a fork some people think it's evil but\n> it's not really evil.\n\nThe problem with that approach is that any solution we come up with\ndoesn't only have to deal with the hypotheticals you want to discuss.\n\nIn particular, any approach that allows you to block an evil fork,\neven when everyone else doesn't agree that it's evil, would also allow\nan enemy of bitcoin to block a good fork, that everyone else correctly\nrecognises is good. A solution that works for an implausible hypothetical\nand breaks when a single attacker decides to take advantage of it is\nnot a good design.\n\nAnd I did already address what to do in exactly that scenario:\n\n> > But hey what about the worst case: what if everyone else in bitcoin\n> > is evil and supports doing evil things. And maybe that's not even\n> > implausible: maybe it's not an \"evil\" thing per se, perhaps [...]\n> >\n> > In that scenario, I think a hard fork is the best choice: split out a new\n> > coin that will survive the upcoming crash, adjust the mining/difficulty\n> > algorithm so it works from day one, and set it up so that you can\n> > maintain it along with the people who support your vision, rather than\n> > having to constantly deal with well-meaning attacks from \"bitcoiners\"\n> > who don't see the risks and have lost the plot.\n> >\n> > Basically: do what Satoshi did and create a better system, and let\n> > everyone else join you as the problems with the old one eventually become\n> > unavoidably obvious.\n\n> Once you understand what hypothetical case I'm talking about, maybe\n> you can understand the rest of my reasoning.\n\nAs I understand it, your hypothetical is:\n\n 0) someone has come up with a bad idea\n 1) most of bitcoin is enthusiastically behind the idea\n 2) you are essentially alone in discovering that it's a bad idea\n 3) almost everyone remains enthusiastic, despite your explanations that\n    it's a bad idea\n 4) nevertheless, you and your colleagues who are aware the idea is bad\n    should have the power to stop the bad idea\n 5) bip8 gives you the power to stop the bad idea but speedy trial does not\n\nAgain given (0), I think (1) and (2) are already not very likely, and (3)\nis simply not plausible. But in the event that it does somehow occur,\nI disagree with (4) for the reasons I describe above; namely, that any\nmechanism that did allow that would be unable to distinguish between the\n\"bad idea\" case and something along the lines of:\n\n 0') someone has come up with a good idea (yay!)\n 1') most of bitcoin is enthusiastically behind the idea\n 2') an enemy of bitcoin is essentially alone in trying to stop it\n 3') almost everyone remains enthusiastic, despite that guy's incoherent\n     raving\n 4') nevertheless, the enemies of bitcoin should have the power to stop\n     the good idea\n\nAnd, as I said in the previous mail, I think (5) is false, independently\nof any of the other conditions.\n\n> But if you don't understand the PREMISES of my example, \n\nYou can come up with hypothetical premises that invalidate bitcoin,\nlet alone some activation method. For example, imagine if the Federal\nReserve Board are full of geniuses and know exactly when to keep issuance\npredictable and when to juice the economy? Having flexibility gives more\noptions than hardcoding \"21M\" somewhere, so clearly the USD's approach\nis the way to go, and everything is just a matter of appointing the\nright people to the board, not all this decentralised stuff. \n\nThe right answer is to reject bad premises, not to argue hypotheticals\nthat have zero relationship to reality.\n\nCheers,\naj"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-28T08:31:18",
                "message_text_only": "On Sat, Mar 26, 2022, 01:45 Anthony Towns <aj at erisian.com.au> wrote:\n\n> On Thu, Mar 24, 2022 at 07:30:09PM +0100, Jorge Tim\u00f3n via bitcoin-dev\n> wrote:\n> > Sorry, I won't answer to everything, because it's clear you're not\n> listening.\n>\n> I'm not agreeing with you; that's different to not listening to you.\n>\n\nYou're disagreeing with thw premises of the example. That's not\ndisagreeing, that's refusing to understand the example.\n\n\n> > In the HYPOTHETICAL CASE that there's an evil for, the fork being evil\n> > is a PREMISE of that hypothetical case, a GIVEN.\n>\n> Do you really find people more inclined to start agreeing with you when\n> you begin yelling at them? When other people start shouting at you,\n> do you feel like it's a discussion that you're engaged in?\n>\n\nI just wanted to make sure you catched the PREMISE word.\n\n\n> > Your claim that \"if it's evil, good people would oppose it\" is a NON\n> > SEQUITUR, \"good people\" aren't necessarily perfect and all knowing.\n> > good people can make mistakes, they can be fooled too.\n> > In the hypothetical case that THERE'S AN EVIL FORK, if \"good people\"\n> > don't complain, it is because they didn't realize that the given fork\n> > was evil. Because in our hypothetical example THE EVIL FORK IS EVIL BY\n> > DEFINITION, THAT'S THE HYPOTHETICAL CASE I WANT TO DISCUSS, not the\n> > hypothetical case where there's a fork some people think it's evil but\n> > it's not really evil.\n>\n> The problem with that approach is that any solution we come up with\n> doesn't only have to deal with the hypotheticals you want to discuss\n>\n\nSure, but if it doesn't deal with this hypothetical, one canbot pretending\nit does by explaing how it does in a different hypothetical.\n\nIn particular, any approach that allows you to block an evil fork,\n> even when everyone else doesn't agree that it's evil, would also allow\n> an enemy of bitcoin to block a good fork, that everyone else correctly\n> recognises is good. A solution that works for an implausible hypothetical\n> and breaks when a single attacker decides to take advantage of it is\n> not a good design.\n>\n\nLet's discuss those too. Feel free to point out how bip8 fails at some\nhypothetical cases speedy trial doesn't.\n\nAnd I did already address what to do in exactly that scenario:\n>\n> > > But hey what about the worst case: what if everyone else in bitcoin\n> > > is evil and supports doing evil things. And maybe that's not even\n> > > implausible: maybe it's not an \"evil\" thing per se, perhaps [...]\n> > >\n> > > In that scenario, I think a hard fork is the best choice: split out a\n> new\n> > > coin that will survive the upcoming crash, adjust the mining/difficulty\n> > > algorithm so it works from day one, and set it up so that you can\n> > > maintain it along with the people who support your vision, rather than\n> > > having to constantly deal with well-meaning attacks from \"bitcoiners\"\n> > > who don't see the risks and have lost the plot.\n> > >\n> > > Basically: do what Satoshi did and create a better system, and let\n> > > everyone else join you as the problems with the old one eventually\n> become\n> > > unavoidably obvious.\n>\n> > Once you understand what hypothetical case I'm talking about, maybe\n> > you can understand the rest of my reasoning.\n>\n> As I understand it, your hypothetical is:\n>\n>  0) someone has come up with a bad idea\n>  1) most of bitcoin is enthusiastically behind the idea\n>  2) you are essentially alone in discovering that it's a bad idea\n>  3) almost everyone remains enthusiastic, despite your explanations that\n>     it's a bad idea\n>  4) nevertheless, you and your colleagues who are aware the idea is bad\n>     should have the power to stop the bad idea\n>  5) bip8 gives you the power to stop the bad idea but speedy trial does not\n>\n\n\n\nAgain given (0), I think (1) and (2) are already not very likely, and (3)\n> is simply not plausible. But in the event that it does somehow occur,\n> I disagree with (4) for the reasons I describe above; namely, that any\n> mechanism that did allow that would be unable to distinguish between the\n> \"bad idea\" case and something along the lines of\n>\n\nOk, yeah, the bitcoin developers currently paying attention to the mailibg\nlist being fooled or making a review mistake is completely unfeasible.\nThey're all way to humble for that, obviously...sigh.\n\n 0') someone has come up with a good idea (yay!)\n>  1') most of bitcoin is enthusiastically behind the idea\n>  2') an enemy of bitcoin is essentially alone in trying to stop it\n>  3') almost everyone remains enthusiastic, despite that guy's incoherent\n>      raving\n>  4') nevertheless, the enemies of bitcoin should have the power to stop\n>      the good idea\n>\n> And, as I said in the previous mail, I think (5) is false, independently\n> of any of the other conditions.\n>\n\n\"That guy's incoherent raving\"\n\"I'm just disagreeing\".\n\nNever mind, anthony.\nYpu absolutely understood what I'm saying. It's just that I'm also\nincoherent to you, it seems. But, hey, again, no contradiction here, I\nguess.\n\n\n\n> But if you don't understand the PREMISES of my example,\n>\n> You can come up with hypothetical premises that invalidate bitcoin,\n> let alone some activation method. For example, imagine if the Federal\n> Reserve Board are full of geniuses and know exactly when to keep issuance\n> predictable and when to juice the economy? Having flexibility gives more\n> options than hardcoding \"21M\" somewhere, so clearly the USD's approach\n> is the way to go, and everything is just a matter of appointing the\n> right people to the board, not all this decentralised stuff.\n>\n> The right answer is to reject bad premises, not to argue hypotheticals\n> that have zero relationship to reality\n>\n\nOk, stop arguing a hypothetical you don't want to arhue about. But you\ncan't say both \"I don't want to consider that hypothetical\" and \"we\nconsidered all hypotheticals\" at the same time.\nI mean, you can, you only can't if you don't want to contradict yourself.\n\nI'll have to wait for someone who actually can both understand the\nhypothetical and ve willing to discuss it.\nI think you didn't understand it, but either way: thank you for admitting\nyou don't want to discuss it.\nLet's stop wasting each other's time then.\n\n\nCheers,\n> aj\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220328/8fa8285e/attachment.html>"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-30T04:21:06",
                "message_text_only": "On Mon, Mar 28, 2022 at 09:31:18AM +0100, Jorge Tim\u00f3n via bitcoin-dev wrote:\n> > In particular, any approach that allows you to block an evil fork,\n> > even when everyone else doesn't agree that it's evil, would also allow\n> > an enemy of bitcoin to block a good fork, that everyone else correctly\n> > recognises is good. A solution that works for an implausible hypothetical\n> > and breaks when a single attacker decides to take advantage of it is\n> > not a good design.\n> Let's discuss those too. Feel free to point out how bip8 fails at some\n> hypothetical cases speedy trial doesn't.\n\nAny case where a flawed proposal makes it through getting activation\nparameters set and released, but doesn't achieve supermajority hashpower\nsupport is made worse by bip8/lot=true in comparison to speedy trial.\n\nThat's true both because of the \"trial\" part, in that activation can fail\nand you can go back to the drawing board without having to get everyone\nupgrade a second time, and also the \"speedy\" part, in that you don't\nhave to wait a year or more before you even know what's going to happen.\n\n> >  0') someone has come up with a good idea (yay!)\n> >  1') most of bitcoin is enthusiastically behind the idea\n> >  2') an enemy of bitcoin is essentially alone in trying to stop it\n> >  3') almost everyone remains enthusiastic, despite that guy's incoherent\n> >      raving\n> >  4') nevertheless, the enemies of bitcoin should have the power to stop\n> >      the good idea\n> \"That guy's incoherent raving\"\n> \"I'm just disagreeing\".\n\nUh, you realise the above is an alternative hypothetical, and not talking\nabout you? I would have thought \"that guy\" being \"an enemy of bitcoin\"\nmade that obvious... I think you're mistaken; I don't think your emails\nare incoherent ravings.\n\nIt was intended to be the simplest possible case of where someone being\nable to block a change is undesirable: they're motivated by trying to\nharm bitcoin, they're as far as possible from being part of some economic\nmajority, and they don't even have a coherent rationale to provide for\nblocking the idea.\n\nCheers,\naj"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-11T16:26:21",
                "message_text_only": "Thanks for pointing out that PR @pushd. Looks like pretty good evidence for\nwhat the status of consensus was around BIP8 in the last 2 years.\n\n@Jorge, I tried to engage with you on the topic of activation rules last\nyear. This is where we left it\n<https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-June/019172.html>.\nIf I'm being frank, you were not clear about what you advocated for, you\ndidn't seem to take the time to understand what I was advocating for and\nwhat I was asking you and trying to discuss with you, and you ghosted some\nof my questions to you. I think you have some ideas that are important to\nconsider, but you're quite a bit more difficult to communicate with than\nthe average bitcoin-dev user, and I'd suggest that if you want your\nconcerns addressed, you figure out how to interact more constructively with\npeople on here. You're being very defensive and adversarial. Please take a\nstep back and try to be more objective. That is IHMO the best way for your\nthoughts to be heard and understood.\n\nI think involving users more in activation is a good avenue of thought for\nimproving how bitcoin does soft forks. I also think the idea you brought up\nof some way for people to signal opposition is a good idea. I've suggested\na mechanism for signature-based user polling\n<https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-May/019022.html>,\nI've also suggested a mechanism\n<https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-June/019117.html>\nwhere miners can actively signal for opposing a soft fork. It seems like\nthere should be some common ground between us in those ideas. Where it\nseems we may perhaps unreconcilably disagree are that A. miners are users\ntoo and generally have interests that are important and different than most\nusers, and giving them at least some mechanism to force discussion is\nappropriate, and B. chain splits are no joke and should almost never be\npossible accidentally and therefore we should make a significant effort to\navoid them, which almost definitely means orderly coordination of miners.\n\nDo you have anything concrete you want to propose? An example mechanism?\nAre you simply here advocating your support for BIP8+LOT=true?\n\n\nOn Fri, Mar 11, 2022 at 7:47 AM Russell O'Connor via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On Fri, Mar 11, 2022 at 7:18 AM Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n>\n>> I talked about this. But the \"no-divergent-rules\" faction doesn't like\n>> it, so we can pretend we have listened to this \"faction\" and addressed all\n>> its concerns, I guess.\n>> Or perhaps it's just \"prosectution complex\", but, hey, what do I know\n>> about psychology?\n>>\n>\n> Your accusations of bad faith on the part of myself and pretty much\n> everyone else makes me disinclined to continue this discussion with you.\n> I'll reply, but if you want me to continue beyond this, then you need to\n> knock it off with the accusations.\n>\n>\n>> A major contender to the Speedy Trial design at the time was to mandate\n>>> eventual forced signalling, championed by luke-jr.  It turns out that, at\n>>> the time of that proposal, a large amount of hash power simply did not have\n>>> the firmware required to support signalling.  That activation proposal\n>>> never got broad consensus, and rightly so, because in retrospect we see\n>>> that the design might have risked knocking a significant fraction of mining\n>>> power offline if it had been deployed.  Imagine if the firmware couldn't be\n>>> quickly updated or imagine if the problem had been hardware related.\n>>>\n>>\n>> Yes, I like this solution too, with a little caveat: an easy mechanism\n>> for users to actively oppose a proposal.\n>> Luke alao talked about this.\n>> If users oppose, they should use activation as a trigger to fork out of\n>> the network by invalidating the block that produces activation.\n>> The bad scenario here is that miners want to deploy something but users\n>> don't want to.\n>> \"But that may lead to a fork\". Yeah, I know.\n>> I hope imagining a scenario in which developers propose something that\n>> most miners accept but some users reject is not taboo.\n>>\n>\n> This topic is not taboo.\n>\n> There are a couple of ways of opting out of taproot.  Firstly, users can\n> just not use taproot.  Secondly, users can choose to not enforce taproot\n> either by running an older version of Bitcoin Core or otherwise forking the\n> source code.  Thirdly, if some users insist on a chain where taproot is\n> \"not activated\", they can always softk-fork in their own rule that\n> disallows the version bits that complete the Speedy Trial activation\n> sequence, or alternatively soft-fork in a rule to make spending from (or\n> to) taproot addresses illegal.\n>\n> As for mark, he wasn't talking about activation, but quantum computing\n>> concerns. Perhaps those have been \"addressed\"?\n>> I just don't know where.\n>>\n>\n> Quantum concerns were discussed.  Working from memory, the arguments were\n> (1) If you are worried about your funds not being secured by taproot, then\n> don't use taproot addresses, and (2) If you are worried about everyone\n> else's funds not being quantum secure by other people choosing to use\n> taproot, well it is already too late because over 5M BTC is currently\n> quantum insecure due to pubkey reuse <\n> https://nitter.net/pwuille/status/1108091924404027392>.  I think it is\n> unlikely that a quantum breakthrough will sneak up on us without time to\n> address the issue and, at the very least, warn people to move their funds\n> off of taproot and other reused addresses, if not forking in some quantum\n> secure alternative.  A recent paper <\n> https://www.sussex.ac.uk/physics/iqt/wp-content/uploads/2022/01/Webber-2022.pdf>\n> suggest that millions physical qubits will be needed to break EC in a day\n> with current error correction technology.  But even if taproot were to be\n> very suddenly banned, there is still a small possibility for recovery\n> because one can prove ownership of HD pubkeys by providing a zero-knowledge\n> proof of the chaincode used to derive them.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220311/932ee5d6/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2022-03-17T11:32:24",
                "message_text_only": "On Fri, Mar 11, 2022 at 5:32 PM Billy Tetrud via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> I think involving users more in activation is a good avenue of thought for improving how bitcoin does soft forks. I also think the idea you brought up of some way for people to signal opposition is a good idea. I've suggested a mechanism for signature-based user polling, I've also suggested a mechanism where miners can actively signal for opposing a soft fork. It seems like there should be some common ground between us in those ideas. Where it seems we may perhaps unreconcilably disagree are that A. miners are users too and generally have interests that are important and different than most users, and giving them at least some mechanism to force discussion is appropriate, and B. chain splits are no joke and should almost never be possible accidentally and therefore we should make a significant effort to avoid them, which almost definitely means orderly coordination of miners.\n\nAny user polling system is going to be vulnerable to sybil attacks.\n\n> Do you have anything concrete you want to propose? An example mechanism? Are you simply here advocating your support for BIP8+LOT=true?\n\nYes, I want BIP+LOT=true (aka the original bip8).\nI also want users to be easily able to coordinate resistance to any\ngiven change, as I described in this thread and others and luke has\ndone many times.\nI also generally oppose to speedy trial being used for any consensus\nrule change deployment.\n\nImagine someone comes and proposes a block size increase through\nextension block softfork.\nWould you like them to use speedy trial or BIP8+LOT=true for deployment?"
            },
            {
                "author": "pushd",
                "date": "2022-03-12T17:11:29",
                "message_text_only": "> A mechanism of soft-forking against activation exists. What more do you\nwant? Are we supposed to write the code on behalf of this hypothetical\ngroup of users who may or may not exist for them just so that they can have\na node that remains stalled on Speedy Trial lockin? That simply isn't\nreasonable, but if you think it is, I invite you to create such a fork.\nI want BIP 8. And less invitations to fork or provoke people.\n\n> If I believe I'm in the economic majority then I'll just refuse to upgrade\nmy node, which was option 2. I don't know why you dismissed it.\n\n> Not much can prevent a miner cartel from enforcing rules that users don't\nwant other than hard forking a replacement POW. There is no effective\ndifference between some developers releasing a malicious soft-fork of\nBitcoin and the miners releasing a malicious version themselves. And when\nthe miner cartel forms, they aren't necessarily going to be polite enough\nto give a transparent signal of their new rules.\n\nMiners get paid irrespective of rules as long as subsidy doesn't change. You can affect their fees, bitcoin and that should be termed as an attack on bitcoin.\n\n> However, without the\neconomic majority enforcing their set of rules, the cartel continuously\nrisks falling apart from the temptation of transaction fees of the censored\ntransactions.\n\nTransaction fee isn't as expected even if we leave censored transactions in a censorship resistant network. If cartel of developers affect it in long term, there will be a time when nobody wants to mine for loss or less profit.\n\n> Look, you cannot have the perfect system of money all by your\nlonesome self.\n\nI agree with this and I can't do the same thing with my local government.\n\npushd\n---\nparallel lines meet at infinity?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220312/63dc5fc7/attachment-0001.html>"
            },
            {
                "author": "pushd",
                "date": "2022-03-17T14:34:49",
                "message_text_only": "> I've done it in about 40 lines of python:\nhttps://github.com/jeremyrubin/forkd\n\nThis python script using `invalidateblock` RPC is an attack on Bitcoin. Just kidding although I won't be surprised if someone writes about it on reddit.\n\nThanks for writing the script, it will be helpful.\n\npushd\n---\nparallel lines meet at infinity?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220317/0de96e24/attachment.html>"
            },
            {
                "author": "pushd",
                "date": "2022-03-26T12:59:12",
                "message_text_only": "> 0') someone has come up with a good idea (yay!)\n> 1') most of bitcoin is enthusiastically behind the idea\n> 2') an enemy of bitcoin is essentially alone in trying to stop it\n> 3') almost everyone remains enthusiastic, despite that guy's incoherent raving\n> 4') nevertheless, the enemies of bitcoin should have the power to stop\nthe good idea\n\nHow do we know if someone is enemy or not in step 2 and step 4?\n\nwhy bip 8:\n\n- gives more power to users\n- miners signaling is not considered voting\n- less politics and controversies\n\nDuring the taproot activation parameters meeting on 2021-02-16,\nparticipants expressed their preferences with regards to BIP 8's lockinontimeout (LOT) parameter:\nhttps://gist.github.com/achow101/3e179501290abb7049de198d46894c7c\n\npushd\n---\nparallel lines meet at infinity?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220326/418e8cd9/attachment.html>"
            },
            {
                "author": "pushd",
                "date": "2022-03-30T10:34:45",
                "message_text_only": "> Any case where a flawed proposal makes it through getting activation\nparameters set and released, but doesn't achieve supermajority hashpowersupport is made worse by bip8/lot=true in comparison to speedy trial.\n\n- Flawed proposal making it through activation is a failure of review process\n\n- Supermajority hashpower percentage decided by bitcoin core developers can choose to not follow old or new consensus rules at any point\n\n- Speedy trial makes it worse by misleading lot of bitcoin users including miners to consider signaling as voting and majority votes decide if a soft fork gets activated\n\n- BIP 8/LOT=TRUE keeps things simple. Miners need to follow consensus rules as they do right now if they wish to mine blocks for subsidy and fees.\n\nNote: Mining pools or individual miners can participate in soft fork discussions regardless of activation method and share their concern which can be evaluated based on technical merits.\n\npushd\n---\nparallel lines meet at infinity?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220330/959159a9/attachment.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-30T20:10:49",
                "message_text_only": "@Pushd\n\n> Speedy trial makes it worse by misleading lot of bitcoin users including\nminers to consider signaling as voting and majority votes decide if a soft\nfork gets activated\n\nNo it does not. This narrative is the worst. A bad explanation of speedy\ntrial can mislead people into thinking miner signalling is how Bitcoin\nupgrades are voted in. But a bad explanation can explain anything badly.\nThe solution is not to change how we engineer soft forks, it's to explain\nspeedy trial better to this imaginary group of important people that think\nminer signaling is voting.\n\nWe shouldn't change how we engineer Bitcoin because of optics. I completely\nobject to that point continuing to be used.\n\nOn Wed, Mar 30, 2022, 05:36 pushd via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > Any case where a flawed proposal makes it through getting activation\n> parameters set and released, but doesn't achieve supermajority hashpower\n> support is made worse by bip8/lot=true in comparison to speedy trial.\n>\n> - Flawed proposal making it through activation is a failure of review\n> process\n>\n> - Supermajority hashpower percentage decided by bitcoin core developers\n> can choose to not follow old or new consensus rules at any point\n>\n> - Speedy trial makes it worse by misleading lot of bitcoin users including\n> miners to consider signaling as voting and majority votes decide if a soft\n> fork gets activated\n>\n> - BIP 8/LOT=TRUE keeps things simple. Miners need to follow consensus\n> rules as they do right now if they wish to mine blocks for subsidy and fees.\n>\n>\n> Note: Mining pools or individual miners can participate in soft fork\n> discussions regardless of activation method and share their concern which\n> can be evaluated based on technical merits.\n>\n>\n> pushd\n> ---\n>\n> parallel lines meet at infinity?\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220330/60a6697e/attachment.html>"
            },
            {
                "author": "pushd",
                "date": "2022-03-30T21:14:16",
                "message_text_only": "> No it does not. This narrative is the worst. A bad explanation of speedy trial can mislead people into thinking miner signalling is how Bitcoin upgrades are voted in. But a bad explanation can explain anything badly.\n\nI agree it is worst but why do you think this narrative exists? People have tried explaining it. Many users, miners and exchanges still think its voting. I think the problem is with activation method so BIP 8/LOT=TRUE is a solution.\n\n> The solution is not to change how we engineer soft forks, it's to explain speedy trial better to this imaginary group of important people that think miner signaling is voting.\n\nWe can suggest different solutions but the problem exists and it is not an imaginary group of people.\n\nOne example of a mining pool: https://archive.ph/oyH04\n\n> We shouldn't change how we engineer Bitcoin because of optics. I completely object to that point continuing to be used.\n\nVoting as described on wiki is quite similar to what happens during miners signaling followed by activation if a certain threshold is reached. If some participants in this process consider it voting instead of signaling for readiness then listing advantages of a better activation method should help everyone reading this thread/email.\n\nSorry, I don't understand your objection. I see a problem that exists since years and a better activation method fixes it. There are other positives for using BIP 8/LOT=TRUE which I shared in https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020178.html\n\nI will continue to discuss this problem with solutions until we use better activation methods for future soft forks in any discussion about activation methods.\n\npushd\n---\nparallel lines meet at infinity?\n\n------- Original Message -------\nOn Thursday, March 31st, 2022 at 1:40 AM, Billy Tetrud <billy.tetrud at gmail.com> wrote:\n\n> @Pushd\n>\n>> Speedy trial makes it worse by misleading lot of bitcoin users including miners to consider signaling as voting and majority votes decide if a soft fork gets activated\n>\n> No it does not. This narrative is the worst. A bad explanation of speedy trial can mislead people into thinking miner signalling is how Bitcoin upgrades are voted in. But a bad explanation can explain anything badly. The solution is not to change how we engineer soft forks, it's to explain speedy trial better to this imaginary group of important people that think miner signaling is voting.\n>\n> We shouldn't change how we engineer Bitcoin because of optics. I completely object to that point continuing to be used.\n>\n> On Wed, Mar 30, 2022, 05:36 pushd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>>> Any case where a flawed proposal makes it through getting activation\n>> parameters set and released, but doesn't achieve supermajority hashpowersupport is made worse by bip8/lot=true in comparison to speedy trial.\n>>\n>> - Flawed proposal making it through activation is a failure of review process\n>>\n>> - Supermajority hashpower percentage decided by bitcoin core developers can choose to not follow old or new consensus rules at any point\n>>\n>> - Speedy trial makes it worse by misleading lot of bitcoin users including miners to consider signaling as voting and majority votes decide if a soft fork gets activated\n>>\n>> - BIP 8/LOT=TRUE keeps things simple. Miners need to follow consensus rules as they do right now if they wish to mine blocks for subsidy and fees.\n>>\n>> Note: Mining pools or individual miners can participate in soft fork discussions regardless of activation method and share their concern which can be evaluated based on technical merits.\n>>\n>> pushd\n>> ---\n>> parallel lines meet at infinity?\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220330/0272365e/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-31T04:31:09",
                "message_text_only": ">  Many users, miners and exchanges still think its voting\n\nWhy do you care what they think? Why does it matter if they misunderstand?\n\n> it is not an imaginary group of people\n\nIf the people aren't imaginary, then its their importance that's imaginary.\n\n> One example of a mining pool\n\nThis isn't even sufficient evidence that they don't understand. Its quite\npossible they're using the word \"voting\" loosely or that they don't\nunderstand english very well. And again, so what if they tweet things that\nare not correctly worded? This is not a reason to change how we design\nbitcoin soft forks.\n\nIts not even wrong to say that a particular signaling round is very much\nlike voting. What's wrong is saying that bitcoin upgrades are made if and\nonly if miners vote to approve those changes.\n\n> I see a problem that exists\n\nYou haven't convinced me this is a significant problem. What are the\nconcrete downsides? Why do you think this can't be fixed by simple\npersistent explaining? You can find groups of people who misunderstand\nbasically any aspect of bitcoin. The solution to people misunderstanding\nthe design is never to change how bitcoin is designed.\n\n\nOn Wed, Mar 30, 2022 at 4:14 PM pushd <pushd at protonmail.com> wrote:\n\n> > No it does not. This narrative is the worst. A bad explanation of\n> speedy trial can mislead people into thinking miner signalling is how\n> Bitcoin upgrades are voted in. But a bad explanation can explain anything\n> badly.\n>\n> I agree it is worst but why do you think this narrative exists? People\n> have tried explaining it. Many users, miners and exchanges still think its\n> voting. I think the problem is with activation method so BIP 8/LOT=TRUE is\n> a solution.\n>\n>\n> > The solution is not to change how we engineer soft forks, it's to\n> explain speedy trial better to this imaginary group of important people\n> that think miner signaling is voting.\n>\n> We can suggest different solutions but the problem exists and it is not an\n> imaginary group of people.\n>\n> One example of a mining pool: https://archive.ph/oyH04\n>\n>\n> > We shouldn't change how we engineer Bitcoin because of optics. I\n> completely object to that point continuing to be used.\n>\n> Voting as described on wiki is quite similar to what happens during miners\n> signaling followed by activation if a certain threshold is reached. If some\n> participants in this process consider it voting instead of signaling for\n> readiness then listing advantages of a better activation method should help\n> everyone reading this thread/email.\n>\n> Sorry, I don't understand your objection. I see a problem that exists\n> since years and a better activation method fixes it. There are other\n> positives for using BIP 8/LOT=TRUE which I shared in\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020178.html\n>\n> I will continue to discuss this problem with solutions until we use better\n> activation methods for future soft forks in any discussion about activation\n> methods.\n>\n>\n> pushd\n> ---\n>\n> parallel lines meet at infinity?\n>\n> ------- Original Message -------\n> On Thursday, March 31st, 2022 at 1:40 AM, Billy Tetrud <\n> billy.tetrud at gmail.com> wrote:\n>\n> @Pushd\n>\n> > Speedy trial makes it worse by misleading lot of bitcoin users\n> including miners to consider signaling as voting and majority votes decide\n> if a soft fork gets activated\n>\n> No it does not. This narrative is the worst. A bad explanation of speedy\n> trial can mislead people into thinking miner signalling is how Bitcoin\n> upgrades are voted in. But a bad explanation can explain anything badly.\n> The solution is not to change how we engineer soft forks, it's to explain\n> speedy trial better to this imaginary group of important people that think\n> miner signaling is voting.\n>\n> We shouldn't change how we engineer Bitcoin because of optics. I\n> completely object to that point continuing to be used.\n>\n> On Wed, Mar 30, 2022, 05:36 pushd via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> > Any case where a flawed proposal makes it through getting activation\n>> parameters set and released, but doesn't achieve supermajority hashpower\n>> support is made worse by bip8/lot=true in comparison to speedy trial.\n>>\n>> - Flawed proposal making it through activation is a failure of review\n>> process\n>>\n>> - Supermajority hashpower percentage decided by bitcoin core developers\n>> can choose to not follow old or new consensus rules at any point\n>>\n>> - Speedy trial makes it worse by misleading lot of bitcoin users\n>> including miners to consider signaling as voting and majority votes decide\n>> if a soft fork gets activated\n>>\n>> - BIP 8/LOT=TRUE keeps things simple. Miners need to follow consensus\n>> rules as they do right now if they wish to mine blocks for subsidy and fees.\n>>\n>>\n>> Note: Mining pools or individual miners can participate in soft fork\n>> discussions regardless of activation method and share their concern which\n>> can be evaluated based on technical merits.\n>>\n>>\n>> pushd\n>> ---\n>>\n>> parallel lines meet at infinity?\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220330/88f65484/attachment.html>"
            },
            {
                "author": "pushd",
                "date": "2022-03-31T14:19:36",
                "message_text_only": "> Why do you care what they think? Why does it matter if they misunderstand?\n\nI care about improving soft fork activation mechanism and shared one of the advantages that helps avoid misleading things. It matters because they are participants in this process.\n\n> If the people aren't imaginary, then its their importance that's imaginary.\n\nNeither the people nor their importance is imaginary. They are a part of Bitcoin and as important as our opinion about soft forks on this mailing list.\n\n> This isn't even sufficient evidence that they don't understand.\n\nOne example of an exchange: https://i.postimg.cc/zv4M6MSp/2KM5tcE.png\n\nOne example of a user: https://bitcoin.stackexchange.com/questions/97043/is-there-an-active-list-of-bips-currently-open-for-voting/\n\n3 examples for each (user, mining pool and exchange) are enough to discuss a problem or list advantages of BIP 8/LOT=TRUE. I can create an archive with more if it helps during next soft fork.\n\n> You haven't convinced me this is a significant problem. What are the concrete downsides? Why do you think this can't be fixed by simple persistent explaining?\n\nI am not trying to convince you and we can have different opinions.\n\nDownsides:\n\n- Signaling period is a waste of time if mining pools that agreed on a soft fork earlier do politics or influenced by councils such as BMC or governments during signaling\n\n- It is considered as voting not just by people outside Bitcoin but the participants itself\n\n- It gives miners an edge over economic nodes that enforce consensus rules\nSimple persistent explaining has not helped in last few years. I don't see anything wrong in listing this as one of the advantages for BIP8/LOT=TRUE.\n\npushd\n---\nparallel lines meet at infinity?\n\n------- Original Message -------\nOn Thursday, March 31st, 2022 at 10:01 AM, Billy Tetrud <billy.tetrud at gmail.com> wrote:\n\n>> Many users, miners and exchanges still think its voting\n>\n> Why do you care what they think? Why does it matter if they misunderstand?\n>\n>> it is not an imaginary group of people\n>\n> If the people aren't imaginary, then its their importance that's imaginary.\n>\n>> One example of a mining pool\n>\n> This isn't even sufficient evidence that they don't understand. Its quite possible they're using the word \"voting\" loosely or that they don't understand english very well. And again, so what if they tweet things that are not correctly worded? This is not a reason to change how we design bitcoin soft forks.\n>\n> Its not even wrong to say that a particular signaling round is very much like voting. What's wrong is saying that bitcoin upgrades are made if and only if miners vote to approve those changes.\n>\n>> I see a problem that exists\n>\n> You haven't convinced me this is a significant problem. What are the concrete downsides? Why do you think this can't be fixed by simple persistent explaining? You can find groups of people who misunderstand basically any aspect of bitcoin. The solution to people misunderstanding the design is never to change how bitcoin is designed.\n>\n> On Wed, Mar 30, 2022 at 4:14 PM pushd <pushd at protonmail.com> wrote:\n>\n>>> No it does not. This narrative is the worst. A bad explanation of speedy trial can mislead people into thinking miner signalling is how Bitcoin upgrades are voted in. But a bad explanation can explain anything badly.\n>>\n>> I agree it is worst but why do you think this narrative exists? People have tried explaining it. Many users, miners and exchanges still think its voting. I think the problem is with activation method so BIP 8/LOT=TRUE is a solution.\n>>\n>>> The solution is not to change how we engineer soft forks, it's to explain speedy trial better to this imaginary group of important people that think miner signaling is voting.\n>>\n>> We can suggest different solutions but the problem exists and it is not an imaginary group of people.\n>>\n>> One example of a mining pool: https://archive.ph/oyH04\n>>\n>>> We shouldn't change how we engineer Bitcoin because of optics. I completely object to that point continuing to be used.\n>>\n>> Voting as described on wiki is quite similar to what happens during miners signaling followed by activation if a certain threshold is reached. If some participants in this process consider it voting instead of signaling for readiness then listing advantages of a better activation method should help everyone reading this thread/email.\n>>\n>> Sorry, I don't understand your objection. I see a problem that exists since years and a better activation method fixes it. There are other positives for using BIP 8/LOT=TRUE which I shared in https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020178.html\n>>\n>> I will continue to discuss this problem with solutions until we use better activation methods for future soft forks in any discussion about activation methods.\n>>\n>> pushd\n>> ---\n>> parallel lines meet at infinity?\n>>\n>> ------- Original Message -------\n>> On Thursday, March 31st, 2022 at 1:40 AM, Billy Tetrud <billy.tetrud at gmail.com> wrote:\n>>\n>>> @Pushd\n>>>\n>>>> Speedy trial makes it worse by misleading lot of bitcoin users including miners to consider signaling as voting and majority votes decide if a soft fork gets activated\n>>>\n>>> No it does not. This narrative is the worst. A bad explanation of speedy trial can mislead people into thinking miner signalling is how Bitcoin upgrades are voted in. But a bad explanation can explain anything badly. The solution is not to change how we engineer soft forks, it's to explain speedy trial better to this imaginary group of important people that think miner signaling is voting.\n>>>\n>>> We shouldn't change how we engineer Bitcoin because of optics. I completely object to that point continuing to be used.\n>>>\n>>> On Wed, Mar 30, 2022, 05:36 pushd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>>> Any case where a flawed proposal makes it through getting activation\n>>>> parameters set and released, but doesn't achieve supermajority hashpowersupport is made worse by bip8/lot=true in comparison to speedy trial.\n>>>>\n>>>> - Flawed proposal making it through activation is a failure of review process\n>>>>\n>>>> - Supermajority hashpower percentage decided by bitcoin core developers can choose to not follow old or new consensus rules at any point\n>>>>\n>>>> - Speedy trial makes it worse by misleading lot of bitcoin users including miners to consider signaling as voting and majority votes decide if a soft fork gets activated\n>>>>\n>>>> - BIP 8/LOT=TRUE keeps things simple. Miners need to follow consensus rules as they do right now if they wish to mine blocks for subsidy and fees.\n>>>>\n>>>> Note: Mining pools or individual miners can participate in soft fork discussions regardless of activation method and share their concern which can be evaluated based on technical merits.\n>>>>\n>>>> pushd\n>>>> ---\n>>>> parallel lines meet at infinity?\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220331/c868eee4/attachment-0001.html>"
            },
            {
                "author": "Billy Tetrud",
                "date": "2022-03-31T15:34:26",
                "message_text_only": "> I am not trying to convince you\n\n If that's really true then you're wasting my and everyone's time here.\n\n> Signaling period is a waste of time if mining pools that agreed on a soft\nfork earlier do politics\n\nThey can and will do politics regardless of why misunderstandings about\nsignaling. This is not a relevant point.\n\n> It is considered as voting not just by people outside Bitcoin but the\nparticipants itself\n\nThis is not a concrete downside. You are simply restating the premise.\n\n> It gives miners an edge over economic nodes that enforce consensus rules\n\nThis is completely false. I have to assume you don't include yourself in\nthe list of users who think a passing vote of miners is required to upgrade\nBitcoin. Am I wrong? If not, then you should know that this\nmisunderstanding gives no one an edge.\n\nSo I'm counting 0 concrete downsides of this misunderstanding of how\nsignaling works that are both relevant and true. I'm going to stick with my\nconclusion that this is a pointless dead end argument to make about soft\nfork deployment in particular, and literally any technical design in\ngeneral.\n\nYou will be able to find 3 people who misunderstand BIP8, or literally any\nother thing you come up with. You could probably find thousands. Or\nmillions if you ask people who've never heard of it. The argument that\nchanging the design will somehow improve that situation is perplexing, but\nthe argument that changing the idea might be a good idea on that basis is\ncompletely unconscionable.\n\n\nOn Thu, Mar 31, 2022, 09:19 pushd <pushd at protonmail.com> wrote:\n\n> > Why do you care what they think? Why does it matter if they\n> misunderstand?\n>\n> I care about improving soft fork activation mechanism and shared one of\n> the advantages that helps avoid misleading things. It matters because they\n> are participants in this process.\n>\n>\n> > If the people aren't imaginary, then its their importance that's\n> imaginary.\n>\n> Neither the people nor their importance is imaginary. They are a part of\n> Bitcoin and as important as our opinion about soft forks on this mailing\n> list.\n>\n>\n> > This isn't even sufficient evidence that they don't understand.\n>\n> One example of an exchange: https://i.postimg.cc/zv4M6MSp/2KM5tcE.png\n>\n> One example of a user:\n> https://bitcoin.stackexchange.com/questions/97043/is-there-an-active-list-of-bips-currently-open-for-voting/\n>\n> 3 examples for each (user, mining pool and exchange) are enough to discuss\n> a problem or list advantages of BIP 8/LOT=TRUE. I can create an archive\n> with more if it helps during next soft fork.\n>\n>\n> > You haven't convinced me this is a significant problem. What are the\n> concrete downsides? Why do you think this can't be fixed by simple\n> persistent explaining?\n>\n> I am not trying to convince you and we can have different opinions.\n>\n> Downsides:\n>\n> - Signaling period is a waste of time if mining pools that agreed on a\n> soft fork earlier do politics or influenced by councils such as BMC or\n> governments during signaling\n>\n> - It is considered as voting not just by people outside Bitcoin but the\n> participants itself\n>\n> - It gives miners an edge over economic nodes that enforce consensus rules\n>\n> Simple persistent explaining has not helped in last few years. I don't see\n> anything wrong in listing this as one of the advantages for BIP8/LOT=TRUE.\n>\n>\n> pushd\n> ---\n>\n> parallel lines meet at infinity?\n>\n> ------- Original Message -------\n> On Thursday, March 31st, 2022 at 10:01 AM, Billy Tetrud <\n> billy.tetrud at gmail.com> wrote:\n>\n> > Many users, miners and exchanges still think its voting\n>\n> Why do you care what they think? Why does it matter if they misunderstand?\n>\n> > it is not an imaginary group of people\n>\n> If the people aren't imaginary, then its their importance that's imaginary.\n>\n> > One example of a mining pool\n>\n> This isn't even sufficient evidence that they don't understand. Its quite\n> possible they're using the word \"voting\" loosely or that they don't\n> understand english very well. And again, so what if they tweet things that\n> are not correctly worded? This is not a reason to change how we design\n> bitcoin soft forks.\n>\n> Its not even wrong to say that a particular signaling round is very much\n> like voting. What's wrong is saying that bitcoin upgrades are made if and\n> only if miners vote to approve those changes.\n>\n> > I see a problem that exists\n>\n> You haven't convinced me this is a significant problem. What are the\n> concrete downsides? Why do you think this can't be fixed by simple\n> persistent explaining? You can find groups of people who misunderstand\n> basically any aspect of bitcoin. The solution to people misunderstanding\n> the design is never to change how bitcoin is designed.\n>\n>\n> On Wed, Mar 30, 2022 at 4:14 PM pushd <pushd at protonmail.com> wrote:\n>\n>> > No it does not. This narrative is the worst. A bad explanation of\n>> speedy trial can mislead people into thinking miner signalling is how\n>> Bitcoin upgrades are voted in. But a bad explanation can explain anything\n>> badly.\n>>\n>> I agree it is worst but why do you think this narrative exists? People\n>> have tried explaining it. Many users, miners and exchanges still think its\n>> voting. I think the problem is with activation method so BIP 8/LOT=TRUE is\n>> a solution.\n>>\n>>\n>> > The solution is not to change how we engineer soft forks, it's to\n>> explain speedy trial better to this imaginary group of important people\n>> that think miner signaling is voting.\n>>\n>> We can suggest different solutions but the problem exists and it is not\n>> an imaginary group of people.\n>>\n>> One example of a mining pool: https://archive.ph/oyH04\n>>\n>>\n>> > We shouldn't change how we engineer Bitcoin because of optics. I\n>> completely object to that point continuing to be used.\n>>\n>> Voting as described on wiki is quite similar to what happens during\n>> miners signaling followed by activation if a certain threshold is reached.\n>> If some participants in this process consider it voting instead of\n>> signaling for readiness then listing advantages of a better activation\n>> method should help everyone reading this thread/email.\n>>\n>> Sorry, I don't understand your objection. I see a problem that exists\n>> since years and a better activation method fixes it. There are other\n>> positives for using BIP 8/LOT=TRUE which I shared in\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020178.html\n>>\n>> I will continue to discuss this problem with solutions until we use\n>> better activation methods for future soft forks in any discussion about\n>> activation methods.\n>>\n>>\n>> pushd\n>> ---\n>>\n>> parallel lines meet at infinity?\n>>\n>> ------- Original Message -------\n>> On Thursday, March 31st, 2022 at 1:40 AM, Billy Tetrud <\n>> billy.tetrud at gmail.com> wrote:\n>>\n>> @Pushd\n>>\n>> > Speedy trial makes it worse by misleading lot of bitcoin users\n>> including miners to consider signaling as voting and majority votes decide\n>> if a soft fork gets activated\n>>\n>> No it does not. This narrative is the worst. A bad explanation of speedy\n>> trial can mislead people into thinking miner signalling is how Bitcoin\n>> upgrades are voted in. But a bad explanation can explain anything badly.\n>> The solution is not to change how we engineer soft forks, it's to explain\n>> speedy trial better to this imaginary group of important people that think\n>> miner signaling is voting.\n>>\n>> We shouldn't change how we engineer Bitcoin because of optics. I\n>> completely object to that point continuing to be used.\n>>\n>> On Wed, Mar 30, 2022, 05:36 pushd via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> > Any case where a flawed proposal makes it through getting activation\n>>> parameters set and released, but doesn't achieve supermajority hashpower\n>>> support is made worse by bip8/lot=true in comparison to speedy trial.\n>>>\n>>> - Flawed proposal making it through activation is a failure of review\n>>> process\n>>>\n>>> - Supermajority hashpower percentage decided by bitcoin core developers\n>>> can choose to not follow old or new consensus rules at any point\n>>>\n>>> - Speedy trial makes it worse by misleading lot of bitcoin users\n>>> including miners to consider signaling as voting and majority votes decide\n>>> if a soft fork gets activated\n>>>\n>>> - BIP 8/LOT=TRUE keeps things simple. Miners need to follow consensus\n>>> rules as they do right now if they wish to mine blocks for subsidy and fees.\n>>>\n>>>\n>>> Note: Mining pools or individual miners can participate in soft fork\n>>> discussions regardless of activation method and share their concern which\n>>> can be evaluated based on technical merits.\n>>>\n>>>\n>>> pushd\n>>> ---\n>>>\n>>> parallel lines meet at infinity?\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220331/772c73bb/attachment-0001.html>"
            },
            {
                "author": "pushd",
                "date": "2022-03-31T15:55:49",
                "message_text_only": "> If that's really true then you're wasting my and everyone's time here.\n\nIt isn't waste of time but important for everyone to understand different opinions about soft fork activations before moving to next soft fork.\n\nThe reason I am not trying to convince you or others but sharing my opinion: https://www.vox.com/science-and-health/2016/12/28/14088992/brain-study-change-minds\n\n> This is completely false. I have to assume you don't include yourself in the list of users who think a passing vote of miners is required to upgrade Bitcoin. Am I wrong? If not, then you should know that this misunderstanding gives no one an edge.\n\nIt is not false because it has been misused by mining pools in past so provides an edge to delay things and create contentious environment.\n\n> You will be able to find 3 people who misunderstand BIP8, or literally any other thing you come up with.\nSorry, I cannot ignore things or live in denial at least when we have better alternatives for activation available.\n\npushd\n---\nparallel lines meet at infinity?\n\n------- Original Message -------\nOn Thursday, March 31st, 2022 at 9:04 PM, Billy Tetrud <billy.tetrud at gmail.com> wrote:\n\n>> I am not trying to convince you\n>\n> If that's really true then you're wasting my and everyone's time here.\n>\n>> Signaling period is a waste of time if mining pools that agreed on a soft fork earlier do politics\n>\n> They can and will do politics regardless of why misunderstandings about signaling. This is not a relevant point.\n>\n>> It is considered as voting not just by people outside Bitcoin but the participants itself\n>\n> This is not a concrete downside. You are simply restating the premise.\n>\n>> It gives miners an edge over economic nodes that enforce consensus rules\n>\n> This is completely false. I have to assume you don't include yourself in the list of users who think a passing vote of miners is required to upgrade Bitcoin. Am I wrong? If not, then you should know that this misunderstanding gives no one an edge.\n>\n> So I'm counting 0 concrete downsides of this misunderstanding of how signaling works that are both relevant and true. I'm going to stick with my conclusion that this is a pointless dead end argument to make about soft fork deployment in particular, and literally any technical design in general.\n>\n> You will be able to find 3 people who misunderstand BIP8, or literally any other thing you come up with. You could probably find thousands. Or millions if you ask people who've never heard of it. The argument that changing the design will somehow improve that situation is perplexing, but the argument that changing the idea might be a good idea on that basis is completely unconscionable.\n>\n> On Thu, Mar 31, 2022, 09:19 pushd <pushd at protonmail.com> wrote:\n>\n>>> Why do you care what they think? Why does it matter if they misunderstand?\n>>\n>> I care about improving soft fork activation mechanism and shared one of the advantages that helps avoid misleading things. It matters because they are participants in this process.\n>>\n>>> If the people aren't imaginary, then its their importance that's imaginary.\n>>\n>> Neither the people nor their importance is imaginary. They are a part of Bitcoin and as important as our opinion about soft forks on this mailing list.\n>>\n>>> This isn't even sufficient evidence that they don't understand.\n>>\n>> One example of an exchange: https://i.postimg.cc/zv4M6MSp/2KM5tcE.png\n>>\n>> One example of a user: https://bitcoin.stackexchange.com/questions/97043/is-there-an-active-list-of-bips-currently-open-for-voting/\n>>\n>> 3 examples for each (user, mining pool and exchange) are enough to discuss a problem or list advantages of BIP 8/LOT=TRUE. I can create an archive with more if it helps during next soft fork.\n>>\n>>> You haven't convinced me this is a significant problem. What are the concrete downsides? Why do you think this can't be fixed by simple persistent explaining?\n>>\n>> I am not trying to convince you and we can have different opinions.\n>>\n>> Downsides:\n>>\n>> - Signaling period is a waste of time if mining pools that agreed on a soft fork earlier do politics or influenced by councils such as BMC or governments during signaling\n>>\n>> - It is considered as voting not just by people outside Bitcoin but the participants itself\n>>\n>> - It gives miners an edge over economic nodes that enforce consensus rules\n>> Simple persistent explaining has not helped in last few years. I don't see anything wrong in listing this as one of the advantages for BIP8/LOT=TRUE.\n>>\n>> pushd\n>> ---\n>> parallel lines meet at infinity?\n>>\n>> ------- Original Message -------\n>> On Thursday, March 31st, 2022 at 10:01 AM, Billy Tetrud <billy.tetrud at gmail.com> wrote:\n>>\n>>>> Many users, miners and exchanges still think its voting\n>>>\n>>> Why do you care what they think? Why does it matter if they misunderstand?\n>>>\n>>>> it is not an imaginary group of people\n>>>\n>>> If the people aren't imaginary, then its their importance that's imaginary.\n>>>\n>>>> One example of a mining pool\n>>>\n>>> This isn't even sufficient evidence that they don't understand. Its quite possible they're using the word \"voting\" loosely or that they don't understand english very well. And again, so what if they tweet things that are not correctly worded? This is not a reason to change how we design bitcoin soft forks.\n>>>\n>>> Its not even wrong to say that a particular signaling round is very much like voting. What's wrong is saying that bitcoin upgrades are made if and only if miners vote to approve those changes.\n>>>\n>>>> I see a problem that exists\n>>>\n>>> You haven't convinced me this is a significant problem. What are the concrete downsides? Why do you think this can't be fixed by simple persistent explaining? You can find groups of people who misunderstand basically any aspect of bitcoin. The solution to people misunderstanding the design is never to change how bitcoin is designed.\n>>>\n>>> On Wed, Mar 30, 2022 at 4:14 PM pushd <pushd at protonmail.com> wrote:\n>>>\n>>>>> No it does not. This narrative is the worst. A bad explanation of speedy trial can mislead people into thinking miner signalling is how Bitcoin upgrades are voted in. But a bad explanation can explain anything badly.\n>>>>\n>>>> I agree it is worst but why do you think this narrative exists? People have tried explaining it. Many users, miners and exchanges still think its voting. I think the problem is with activation method so BIP 8/LOT=TRUE is a solution.\n>>>>\n>>>>> The solution is not to change how we engineer soft forks, it's to explain speedy trial better to this imaginary group of important people that think miner signaling is voting.\n>>>>\n>>>> We can suggest different solutions but the problem exists and it is not an imaginary group of people.\n>>>>\n>>>> One example of a mining pool: https://archive.ph/oyH04\n>>>>\n>>>>> We shouldn't change how we engineer Bitcoin because of optics. I completely object to that point continuing to be used.\n>>>>\n>>>> Voting as described on wiki is quite similar to what happens during miners signaling followed by activation if a certain threshold is reached. If some participants in this process consider it voting instead of signaling for readiness then listing advantages of a better activation method should help everyone reading this thread/email.\n>>>>\n>>>> Sorry, I don't understand your objection. I see a problem that exists since years and a better activation method fixes it. There are other positives for using BIP 8/LOT=TRUE which I shared in https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020178.html\n>>>>\n>>>> I will continue to discuss this problem with solutions until we use better activation methods for future soft forks in any discussion about activation methods.\n>>>>\n>>>> pushd\n>>>> ---\n>>>> parallel lines meet at infinity?\n>>>>\n>>>> ------- Original Message -------\n>>>> On Thursday, March 31st, 2022 at 1:40 AM, Billy Tetrud <billy.tetrud at gmail.com> wrote:\n>>>>\n>>>>> @Pushd\n>>>>>\n>>>>>> Speedy trial makes it worse by misleading lot of bitcoin users including miners to consider signaling as voting and majority votes decide if a soft fork gets activated\n>>>>>\n>>>>> No it does not. This narrative is the worst. A bad explanation of speedy trial can mislead people into thinking miner signalling is how Bitcoin upgrades are voted in. But a bad explanation can explain anything badly. The solution is not to change how we engineer soft forks, it's to explain speedy trial better to this imaginary group of important people that think miner signaling is voting.\n>>>>>\n>>>>> We shouldn't change how we engineer Bitcoin because of optics. I completely object to that point continuing to be used.\n>>>>>\n>>>>> On Wed, Mar 30, 2022, 05:36 pushd via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>>\n>>>>>>> Any case where a flawed proposal makes it through getting activation\n>>>>>> parameters set and released, but doesn't achieve supermajority hashpowersupport is made worse by bip8/lot=true in comparison to speedy trial.\n>>>>>>\n>>>>>> - Flawed proposal making it through activation is a failure of review process\n>>>>>>\n>>>>>> - Supermajority hashpower percentage decided by bitcoin core developers can choose to not follow old or new consensus rules at any point\n>>>>>>\n>>>>>> - Speedy trial makes it worse by misleading lot of bitcoin users including miners to consider signaling as voting and majority votes decide if a soft fork gets activated\n>>>>>>\n>>>>>> - BIP 8/LOT=TRUE keeps things simple. Miners need to follow consensus rules as they do right now if they wish to mine blocks for subsidy and fees.\n>>>>>>\n>>>>>> Note: Mining pools or individual miners can participate in soft fork discussions regardless of activation method and share their concern which can be evaluated based on technical merits.\n>>>>>>\n>>>>>> pushd\n>>>>>> ---\n>>>>>> parallel lines meet at infinity?\n>>>>>> _______________________________________________\n>>>>>> bitcoin-dev mailing list\n>>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220331/807435d4/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Speedy Trial",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Voskuil",
                "Kate Salazar",
                "Anthony Towns",
                "Russell O'Connor",
                "pushd",
                "ZmnSCPxj",
                "Jorge Tim\u00f3n",
                "Luke Dashjr",
                "vjudeu at gazeta.pl",
                "Billy Tetrud",
                "Jeremy Rubin"
            ],
            "messages_count": 42,
            "total_messages_chars_count": 273913
        }
    },
    {
        "title": "[bitcoin-dev] Removing the Dust Limit",
        "thread_messages": [
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-12T13:02:24",
                "message_text_only": "> We should remove the dust limit from Bitcoin.\n\nAny node operator can do that. Just put \"dustrelayfee=0.00000000\" in your bitcoin.conf.\n\nAnd there is more: you can also conditionally allow free transactions:\n\nmintxfee=0.00000001\nminrelaytxfee=0.00000000\nblockmintxfee=0.00000000\n\nThen, when using getblocktemplate you will get transactions with the highest fees first anyway, and you include cheap or free transactions in the end, if there will be enough room for them.\n\nSo, all of those settings are in the hands of node operators, there is no need to change the source code, all you need is to convince nodes to change their settings.\n\n\nOn 2021-08-08 20:53:28 user Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\nWe should remove the dust limit from Bitcoin. Five reasons:\n\n\n1) it's not our business what outputs people want to create\n2) dust outputs can be used in various authentication/delegation smart contracts\n3) dust sized htlcs in lightning (https://bitcoin.stackexchange.com/questions/46730/can-you-send-amounts-that-would-typically-be-considered-dust-through-the-light) force channels to operate in a semi-trusted mode which has implications (AFAIU) for the regulatory classification of channels in various jurisdictions; agnostic treatment of fund transfers\u00a0would simplify this (like getting a 0.01 cent dividend check in the mail)\n4) thinly divisible colored coin protocols might make use of sats as value markers for transactions.\n5) should we ever do confidential transactions we can't prevent it without compromising\u00a0privacy / allowed transfers\n\n\nThe main reasons I'm aware of not allow dust creation is that:\n\n\n1) dust is spam\n2) dust fingerprinting attacks\n\n\n1 is (IMO) not valid given the 5 reasons above, and 2 is preventable by well behaved wallets to not redeem outputs that cost more in fees than they are worth.\n\n\ncheers,\n\n\njeremy"
            }
        ],
        "thread_summary": {
            "title": "Removing the Dust Limit",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "vjudeu at gazeta.pl"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1881
        }
    },
    {
        "title": "[bitcoin-dev] Covenants and feebumping",
        "thread_messages": [
            {
                "author": "darosior",
                "date": "2022-03-12T18:08:39",
                "message_text_only": "The idea of a soft fork to fix dynamic fee bumping was recently put back on the table. It might\nsound radical, as what prevents today reasonable fee bumping for contracts with presigned\ntransactions (pinning) has to do with nodes' relay policy. But the frustration is understandable\ngiven the complexity of designing fee bumping with today's primitives. [0]\nRecently too, there was a lot of discussions around covenants. Covenants (conceptually, not talking\nabout any specific proposal) seem to open lots of new use cases and to be desired by (some?) Bitcoin\napplication developers and users.\nI think that fee bumping using covenants has attractive properties, and it requires a soft fork that\nis already desirable beyond (trying) to fix fee bumping. However i could not come up with a solution\nas neat for other protocols than vaults. I'd like to hear from others about 1) taking this route for\nfee bumping 2) better ideas on applying this to other protocols.\n\n\nIn a vault construction you have a UTxO which can only be spent by an Unvaulting transaction, whose\noutput triggers a timelock before the expiration of which a revocation transaction may be confirmed.\nThe revocation transaction being signed in advance (typically before sharing the signature for the\nUnvault transaction) you need fee bumping in order for the contract to actually be enforceable.\n\nNow, with a covenant you could commit to the revocation tx instead of presigning it. And using a\nTaproot tree you could commit to different versions of it with increasing feerate. Any network\nmonitor (the brooadcaster, a watchtower, ..) would be able to RBF the revocation transaction if it\ndoesn't confirm by spending using a leaf with a higher-feerate transaction being committed to.\n\nOf course this makes for a perfect DoS: it would be trivial for a miner to infer that you are using\na specific vault standard and guess other leaves and replace the witness to use the highest-feerate\nspending path. You could require a signature from any of the participants. Or, at the cost of an\nadditional depth, in the tree you could \"salt\" each leaf by pairing it with -say- an OP_RETURN leaf.\nBut this leaves you with a possible internal blackmail for multi-party contracts (although it's less\nof an issue for vaults, and not one for single-party vaults).\nWhat you could do instead is attaching an increasing relative timelock to each leaf (as the committed\nrevocation feerate increases, so does the timelock). You need to be careful to note wreck miner\nincentives here (see [0], [1], [2] on \"miner harvesting\"), but this enables the nice property of a\nfeerate which \"adapts\" to the block space market. Another nice property of this approach is the\nintegrated anti fee sniping protection if the revocation transaction pays a non-trivial amount of\nfees.\n\nPaying fees from \"shared\" funds instead of a per-watchtower fee-bumping wallet opened up the\nblackmail from the previous section, but the benefits of paying from internal funds shouldn't be\nunderstated.\nNo need to decide on an amount to be refilled. No need to bother the user to refill the fee-bumping\nwallet (before they can participate in more contracts, or worse before a deadline at which all\ncontracts are closed). No need for a potentially large amount of funds to just sit on a hot wallet\n\"just in case\". No need to duplicate this amount as you replicate the number of network monitors\n(which is critical to the security of such contracts).\nIn addition, note how modifying the feerate of the revocation transaction in place is less expensive\nthan adding a (pair of) new input (and output), let alone adding an entire new transaction to CPFP.\nAside, and less importantly, it can be made to work with today's relay rules (just use fee thresholds\nadapted to the current RBF thresholds, potentially with some leeway to account for policy changes).\nPaying from shared funds (in addition to paying from internal funds) also prevents pervert\nincentives for contracts with more than 2 parties. In case one of the parties breaches it, all\nremaining parties have an incentive to enforce the contract.. But only one would otherwise pay for\nit! It would open up the door to some potential sneaky techniques to wait for another party to pay\nfor the fees, which is at odd with the reactive security model.\n\nLet's examine how it could be concretely designed. Say you have a vault wallet software for a setup\nwith 5 participants. The revocation delay is 144 blocks. You assume revocation to be infrequent (if\none happens it's probably a misconfigured watchtower that needs be fixed before the next\nunvaulting), so you can afford infrequent overpayments and larger fee thresholds. Participants\nassume the vault will be spent within a year and assume a maximum possible feerate for this year of\n10ksat/vb.\nThey create a Taproot tree of depth 7. First leaf is the spending path (open to whomever the vault\npays after the 144 blocks). Then the leaf `i` for `i` in `[1, 127]` is a covenant to the revocation\ntransaction with a feerate `i * 79` sats/vb and a relative timelock of `i - 1` blocks.\nAssuming the covenant to the revocation transaction is 33 bytes [3], that's a witness of:\n    1 + 33     + 1 + 33 + 7 * 32 = 292 WU (73 vb)\n    ^^^^^^       ^^^^^^^^^^^^^^\n    witscript     control block\nfor any of the revocation paths. The revocation transaction is 1-input 1-output, so in total it's\n    10.5 +   41 + 73      + 43    = 167.5 vb\n    ^^^^    ^^^^^^^^^^^    ^^^^\n    header  input|witness  output\nThe transaction size is not what you'd necessarily want to optimize for first, still, it is smaller\nin this case than using other feebumping primitives and has a smaller footprint on the UTxO set. For\ninstance for adding a feebumping input and change output assuming all Taproot inputs and outputs\n(CPFP is necessarily even larger):\n    5 * 64 +  1 + 5 * (32 + 1) + 1 + 33 = 520 WU (105 vb)\n    ^^^^^^    ^^^^^^^^^^^^^^^    ^^^^^^\n    witness      witscript       control\n    10.5  +  41 + 105      + 41 + 16.5         + 2 * 43  = 300 vb\n    ^^^^     ^^^^^^^^        ^^^^^^^^^           ^^^^^^\n    header   input|witness   fb input|witness    outputs\n>From there, you can afford more depths at the tiny cost of 8 more vbytes each. You might want them\nfor:\n- more granularity (if you can afford large enough timelocks)\n- optimizing for the spending path rather than the revocation one\n- adding a hashlock to prevent nuisance (with the above script a third party could malleate a\n  spending path into a revocation one). You can use the OP_RETURN trick from above to prevent that.\n\nUnfortunately, the timelocked-covenant approach to feebumping only applies to bumping the first\ntransaction of a chain (you can't pay for the parent with a timelock) so for instance it's not\nusable for HTLC transactions in Lightning to bump the parent commitment tx. The same goes for\nbumping the update tx in Coinpool.\nIt could be worked around by having a different covenant per participant (paying the fee from either\nof the participants' output) behind a signature check. Of course it requires funds to already be in\nthe contract (HTLC, Coinpool leaf) to pay for your own unilateral close, but if you don't have any\nfund in the contract it doesn't make sense to try to feebump it in the first place. The same goes\nfor small amounts: you'd only allocate up to the value of the contract (minus a dust preference) in\nfees in order to enforce it.\nThis is less nice for external monitors as it requires a private key (or another secret) to be\ncommitted to in advance) to be able to bump [4] and does not get rid of the \"who's gonna pay for the\nenforcement\" issue in >2-parties contracts. Still, it's more optimal and usable than CPFP or adding\na pair of input/output for all the reasons mentioned above.\n\n\nThoughts?\nAntoine\n\n\n[0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-November/019614.html\n[1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-November/019615.html\n[2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019627.html\n[3] That's obviously close to the CTV construction. But using another more flexible (and therefore\n    less optimized) construction would not be a big deal. It might in fact be necessary for more\n    elaborated (realistic?) usecases than the simple one detailed here.\n[4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html"
            },
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-13T02:33:48",
                "message_text_only": "Hi Antoine,\n\nI have a few high level thoughts on your post comparing these types of\nprimitive to an explicit soft fork approach:\n\n1) Transaction sponsors *is* a type of covenant. Precisely, it is very\nsimilar to an \"Impossible Input\" covenant in conjunction with a \"IUTXO\" I\ndefined in my 2017 workshop\nhttps://rubin.io/public/pdfs/multi-txn-contracts.pdf (I know, I know...\nself citation, not cool, but helps with context).\n\nHowever, for Sponsors itself we optimize the properties of how it works &\nis represented, as well as \"tighten the hatches\" on binding to specific TX\nvs merely spend of the outputs (which wouldn't work as well with APO).\n\nPerhaps thinking of something like sponsors as a form of covenant, rather\nthan a special purpose thing, is helpful?\n\nThere's a lot you could do with a general \"observe other txns in {this\nblock, the chain}\" primitive. The catch is that for sponsors we don't\n*care* to enable people to use this as a \"smart contracting primitive\", we\nwant to use it for fee bumping. So we don't care about programmability, we\ncare about being able to use the covenant to bump fees.\n\n2) On Chain Efficiency.\n\n\nA) Precommitted Levels\nAs you've noted, an approach like precomitted different fee levels might\nwork, but has substantial costs.\n\nHowever, with sponsors, the minimum viable version of this (not quite what\nis spec'd in my prior email, but it could be done this way if we care to\noptimize for bytes) would require 1 in and 1 out with only 32 bytes extra.\nSo that's around 40 bytes outpoint + 64 bytes signature + 40 bytes output +\n32 bytes metadata = 174 bytes per bump. Bumps in this way can also\namortize, so bumping >1 txn at the same time would hit the limit of 32\nbytes + 144/n  bytes to bump more than one thing. You can imagine cases\nwhere this might be popular, like \"close >1 of my LN channels\" or \"start\nwithdrawals for 5 of my JamesOB vaulted coins\"\n\nB) Fancy(er) Covenants\n\nWe might also have something with OP_CAT and CSFS where bumps are done as\nsome sort of covenant-y thing that lets you arbitrarily rewrite\ntransactions.\n\nNot too much to say other than that it is difficult to get these down in\nsize as the scripts become more complex, not to mention the (hotly\ndiscussed of late) ramifications of those covenants more generally.\n\nAbsent a concrete fancy covenant with fee bumping, I can't comment.\n\n3) On Capital Efficiency\n\nSomething like a precommitted or covenant fee bump requires the fee capital\nto be pre-committed inside the UTXO, whereas for something like Sponsors\nyou can use capital you get sometime later. In certain models -- e.g.,\nchannels -- where you might expect only log(N) of your channels to fail in\na given epoch, you don't need to allocate as much capital as if you were to\nhave to do it in-band. This is also true for vaults where you know you only\nwant to open 1 per month let's say, and not <all of your vaults> per month,\nwhich pre-committing requires.\n\n4) On Protocol Design\n\nIt's nice that you can abstract away your protocol design concerns as a\n\"second tier composition check\" v.s. having to modify your protocol to work\nwith a fee bumping thing.\n\nThere are a myriad of ways dynamic txns (e.g. for Eltoo) can lead to RBF\npinning and similar, Sponsor type things allow you to design such protocols\nto not have any native way of paying for fees inside the actual\n\"Transaction Intents\" and use an external system to create the intended\neffect. It seems (to me) more robust that we can prove that a Sponsors\nmechanism allows any transaction -- regardless of covenant stuff, bugs,\npinning, etc -- to move forward.\n\nStill... careful protocol design may permit the use of optimized\nconstructions! For example, in a vault rather than assigning *no fee* maybe\nyou can have a single branch with a reasonable estimated fee. If you are\ncorrect or overshot (let's say 50% chance?) then you don't need to add a\nsponsor. If you undershot, not to worry, just add a sponsor. Adopted\nbroadly, this would cut the expected value of using sponsors by <however\ngood you are at estimating future fees>. This basically enables all\nprotocols to try to be more efficient, but backstop that with a guaranteed\nto work safe mechanism.\n\n\n\nThere was something else I was going to say but I forgot about it... if it\ncomes to me I'll send a follow up email.\n\nCheers,\n\nJeremy\n\np.s.\n\n>\n\n\n> *Of course this makes for a perfect DoS: it would be trivial for a miner\n> to infer that you are using*\n> *a specific vault standard and guess other leaves and replace the witness\n> to use the highest-feerate*\n> *spending path. You could require a signature from any of the\n> participants. Or, at the cost of an**additional depth, in the tree you\n> could \"salt\" each leaf by pairing it with -say- an OP_RETURN leaf.*\n\n\n\nyou don't need a salt, you just need a unique payout addr (e.g. hardened\nderivation) per revocation txn and you cannot guess the branch.\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n\nOn Sat, Mar 12, 2022 at 10:34 AM darosior via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The idea of a soft fork to fix dynamic fee bumping was recently put back\n> on the table. It might\n> sound radical, as what prevents today reasonable fee bumping for contracts\n> with presigned\n> transactions (pinning) has to do with nodes' relay policy. But the\n> frustration is understandable\n> given the complexity of designing fee bumping with today's primitives. [0]\n> Recently too, there was a lot of discussions around covenants. Covenants\n> (conceptually, not talking\n> about any specific proposal) seem to open lots of new use cases and to be\n> desired by (some?) Bitcoin\n> application developers and users.\n> I think that fee bumping using covenants has attractive properties, and it\n> requires a soft fork that\n> is already desirable beyond (trying) to fix fee bumping. However i could\n> not come up with a solution\n> as neat for other protocols than vaults. I'd like to hear from others\n> about 1) taking this route for\n> fee bumping 2) better ideas on applying this to other protocols.\n>\n>\n> In a vault construction you have a UTxO which can only be spent by an\n> Unvaulting transaction, whose\n> output triggers a timelock before the expiration of which a revocation\n> transaction may be confirmed.\n> The revocation transaction being signed in advance (typically before\n> sharing the signature for the\n> Unvault transaction) you need fee bumping in order for the contract to\n> actually be enforceable.\n>\n> Now, with a covenant you could commit to the revocation tx instead of\n> presigning it. And using a\n> Taproot tree you could commit to different versions of it with increasing\n> feerate. Any network\n> monitor (the brooadcaster, a watchtower, ..) would be able to RBF the\n> revocation transaction if it\n> doesn't confirm by spending using a leaf with a higher-feerate transaction\n> being committed to.\n>\n> Of course this makes for a perfect DoS: it would be trivial for a miner to\n> infer that you are using\n> a specific vault standard and guess other leaves and replace the witness\n> to use the highest-feerate\n> spending path. You could require a signature from any of the participants.\n> Or, at the cost of an\n> additional depth, in the tree you could \"salt\" each leaf by pairing it\n> with -say- an OP_RETURN leaf.\n> But this leaves you with a possible internal blackmail for multi-party\n> contracts (although it's less\n> of an issue for vaults, and not one for single-party vaults).\n> What you could do instead is attaching an increasing relative timelock to\n> each leaf (as the committed\n> revocation feerate increases, so does the timelock). You need to be\n> careful to note wreck miner\n> incentives here (see [0], [1], [2] on \"miner harvesting\"), but this\n> enables the nice property of a\n> feerate which \"adapts\" to the block space market. Another nice property of\n> this approach is the\n> integrated anti fee sniping protection if the revocation transaction pays\n> a non-trivial amount of\n> fees.\n>\n> Paying fees from \"shared\" funds instead of a per-watchtower fee-bumping\n> wallet opened up the\n> blackmail from the previous section, but the benefits of paying from\n> internal funds shouldn't be\n> understated.\n> No need to decide on an amount to be refilled. No need to bother the user\n> to refill the fee-bumping\n> wallet (before they can participate in more contracts, or worse before a\n> deadline at which all\n> contracts are closed). No need for a potentially large amount of funds to\n> just sit on a hot wallet\n> \"just in case\". No need to duplicate this amount as you replicate the\n> number of network monitors\n> (which is critical to the security of such contracts).\n> In addition, note how modifying the feerate of the revocation transaction\n> in place is less expensive\n> than adding a (pair of) new input (and output), let alone adding an entire\n> new transaction to CPFP.\n> Aside, and less importantly, it can be made to work with today's relay\n> rules (just use fee thresholds\n> adapted to the current RBF thresholds, potentially with some leeway to\n> account for policy changes).\n> Paying from shared funds (in addition to paying from internal funds) also\n> prevents pervert\n> incentives for contracts with more than 2 parties. In case one of the\n> parties breaches it, all\n> remaining parties have an incentive to enforce the contract.. But only one\n> would otherwise pay for\n> it! It would open up the door to some potential sneaky techniques to wait\n> for another party to pay\n> for the fees, which is at odd with the reactive security model.\n>\n> Let's examine how it could be concretely designed. Say you have a vault\n> wallet software for a setup\n> with 5 participants. The revocation delay is 144 blocks. You assume\n> revocation to be infrequent (if\n> one happens it's probably a misconfigured watchtower that needs be fixed\n> before the next\n> unvaulting), so you can afford infrequent overpayments and larger fee\n> thresholds. Participants\n> assume the vault will be spent within a year and assume a maximum possible\n> feerate for this year of\n> 10ksat/vb.\n> They create a Taproot tree of depth 7. First leaf is the spending path\n> (open to whomever the vault\n> pays after the 144 blocks). Then the leaf `i` for `i` in `[1, 127]` is a\n> covenant to the revocation\n> transaction with a feerate `i * 79` sats/vb and a relative timelock of `i\n> - 1` blocks.\n> Assuming the covenant to the revocation transaction is 33 bytes [3],\n> that's a witness of:\n>     1 + 33     + 1 + 33 + 7 * 32 = 292 WU (73 vb)\n>     ^^^^^^       ^^^^^^^^^^^^^^\n>     witscript     control block\n> for any of the revocation paths. The revocation transaction is 1-input\n> 1-output, so in total it's\n>     10.5 +   41 + 73      + 43    = 167.5 vb\n>     ^^^^    ^^^^^^^^^^^    ^^^^\n>     header  input|witness  output\n> The transaction size is not what you'd necessarily want to optimize for\n> first, still, it is smaller\n> in this case than using other feebumping primitives and has a smaller\n> footprint on the UTxO set. For\n> instance for adding a feebumping input and change output assuming all\n> Taproot inputs and outputs\n> (CPFP is necessarily even larger):\n>     5 * 64 +  1 + 5 * (32 + 1) + 1 + 33 = 520 WU (105 vb)\n>     ^^^^^^    ^^^^^^^^^^^^^^^    ^^^^^^\n>     witness      witscript       control\n>     10.5  +  41 + 105      + 41 + 16.5         + 2 * 43  = 300 vb\n>     ^^^^     ^^^^^^^^        ^^^^^^^^^           ^^^^^^\n>     header   input|witness   fb input|witness    outputs\n> From there, you can afford more depths at the tiny cost of 8 more vbytes\n> each. You might want them\n> for:\n> - more granularity (if you can afford large enough timelocks)\n> - optimizing for the spending path rather than the revocation one\n> - adding a hashlock to prevent nuisance (with the above script a third\n> party could malleate a\n>   spending path into a revocation one). You can use the OP_RETURN trick\n> from above to prevent that.\n>\n> Unfortunately, the timelocked-covenant approach to feebumping only applies\n> to bumping the first\n> transaction of a chain (you can't pay for the parent with a timelock) so\n> for instance it's not\n> usable for HTLC transactions in Lightning to bump the parent commitment\n> tx. The same goes for\n> bumping the update tx in Coinpool.\n> It could be worked around by having a different covenant per participant\n> (paying the fee from either\n> of the participants' output) behind a signature check. Of course it\n> requires funds to already be in\n> the contract (HTLC, Coinpool leaf) to pay for your own unilateral close,\n> but if you don't have any\n> fund in the contract it doesn't make sense to try to feebump it in the\n> first place. The same goes\n> for small amounts: you'd only allocate up to the value of the contract\n> (minus a dust preference) in\n> fees in order to enforce it.\n> This is less nice for external monitors as it requires a private key (or\n> another secret) to be\n> committed to in advance) to be able to bump [4] and does not get rid of\n> the \"who's gonna pay for the\n> enforcement\" issue in >2-parties contracts. Still, it's more optimal and\n> usable than CPFP or adding\n> a pair of input/output for all the reasons mentioned above.\n>\n>\n> Thoughts?\n> Antoine\n>\n>\n> [0]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-November/019614.html\n> [1]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-November/019615.html\n> [2]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019627.html\n> [3] That's obviously close to the CTV construction. But using another more\n> flexible (and therefore\n>     less optimized) construction would not be a big deal. It might in fact\n> be necessary for more\n>     elaborated (realistic?) usecases than the simple one detailed here.\n> [4]\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220312/980f7dcc/attachment-0001.html>"
            },
            {
                "author": "darosior",
                "date": "2022-03-14T14:49:30",
                "message_text_only": "Hi Jeremy,\n\nThanks for the feedback. I indeed only compared it to existing fee-bumping methods. But sponsors are pretty\nsimilar to CPFP in usage anyways, they 'just' get rid of the complexity of managing transaction chains in the\nmempool. That's great, don't get me wrong, just it's much less ideal than a solution not requiring additional\nUTxOs to be reserved, managed, and additional onchain transactions.\n\nRegarding chain efficiency. First, you wrote:\n> As you've noted, an approach like precomitted different fee levels might work, but has substantial costs.\n\nWell, i noted that it *does* work (at least for vaults). And it does incur a cost, but it's inferior to the\nother solutions. Then, sure sponsors' -like CPFP's- cost can be amortized. The chain usage would still likely\nbe superior (depends on a case by case basis i'd say), but even then the \"direct\" chain usage cost isn't what\nmatters most. As mentioned, the cost of using funds not internal to the contract really is.\n\nRegarding capital efficiency, again as noted in the post, it's the entire point to use funds internal to the\ncontract (\"pre-committed\"). Sure external funding (by the means of sponsors or any other technique) allows you\nto allocate funds later on, or never. But we want contracts that are actually enforceable, i guess?\nOn the other hand, pre-committing to all the possible fee-bumped levels prevents you to dynamically add more\nfees eventually. That's why you need to pre-commit to levels up to your assumed \"max feerate before i close\nthe contract\". For \"cold contracts\" (vaults), timelocks prevent the DOS of immediately using a large feerate.\nFor \"hot contracts\" a signature challenge is used to achieve the same. I know the latter is imperfect, since\nthe lower the uptime risk (increase the number of network monitors) the higher the DOS risk (as you duplicate\nthe key).. That's why i asked if anybody had some thoughts about this and if there was a cleverer way of doing\nit.\n\n> This is also true for vaults where you know you only want to open 1 per month let's say, and not\n> your vaults> per month, which pre-committing requires.\n\nHuh? Pre-committing here is to pre-commit to levels of the revocation (\"Cancel\") transaction. It has nothing\nto do with \"activating\" (using Revault's terminology) a vault, done by sharing a signature for the Unvault\ntransaction.\nYou might have another vault design in mind whereby any deposited fund is unvault-able. In this case, and as\nwith any other active contract, i think you need to have funds ready to pay for the fees for the contract to\nbe enforceable. Whether these funds come from the contract's funds or from externally-reserved UTxOs.\n\n> you don't need a salt, you just need a unique payout addr (e.g. hardened derivation) per revocation txn and\n> you cannot guess the branch.\n\nYeah, i preferred to go with 8 more vbytes. First because relying on never reusing a derivation index is\nbrittle and also because it would make rescan much harder. Imagine having 256 fee levels, making 5 payments a\nday for 200 days in a year. You'd have 256000 derivation indexes per year to scan for if restoring frombackup.\n\n------- Original Message -------\nLe dimanche 13 mars 2022 \u00e0 3:33 AM, Jeremy Rubin <jeremy.l.rubin at gmail.com> a \u00e9crit :\n\n> Hi Antoine,\n>\n> I have a few high level thoughts on your post comparing these types of primitive to an explicit soft fork approach:\n>\n> 1) Transaction sponsors *is* a type of covenant. Precisely, it is very similar to an \"Impossible Input\" covenant in conjunction with a \"IUTXO\" I defined in my 2017 workshophttps://rubin.io/public/pdfs/multi-txn-contracts.pdf(I know, I know... self citation, not cool, but helps with context).\n>\n> However, for Sponsors itself we optimize the properties of how it works & is represented, as well as \"tighten the hatches\" on binding to specific TX vs merely spend of the outputs (which wouldn't work as well with APO).\n>\n> Perhaps thinking of something like sponsors as a form of covenant, rather than a special purpose thing, is helpful?\n>\n> There's a lot you could do with a general \"observe other txns in {this block, the chain}\" primitive. The catch is that for sponsors we don't *care* to enable people to use this as a \"smart contracting primitive\", we want to use it for fee bumping. So we don't care about programmability, we care about being able to use the covenant to bump fees.\n>\n> 2) On Chain Efficiency.\n>\n> A) Precommitted Levels\n> As you've noted, an approach like precomitted different fee levels might work, but has substantial costs.\n>\n> However, with sponsors, the minimum viable version of this (not quite what is spec'd in my prior email, but it could be done this way if we care to optimize for bytes) would require 1 in and 1 out with only 32 bytes extra. So that's around 40 bytes outpoint + 64 bytes signature + 40 bytes output + 32 bytes metadata = 174 bytes per bump. Bumps in this way can also amortize, so bumping >1 txn at the same time would hit the limit of 32 bytes + 144/n bytes to bump more than one thing. You can imagine cases where this might be popular, like \"close >1 of my LN channels\" or \"start withdrawals for 5 of my JamesOB vaulted coins\"\n>\n> B) Fancy(er) Covenants\n>\n> We might also have something with OP_CAT and CSFS where bumps are done as some sort of covenant-y thing that lets you arbitrarily rewrite transactions.\n>\n> Not too much to say other than that it is difficult to get these down in size as the scripts become more complex, not to mention the (hotly discussed of late) ramifications of those covenants more generally.\n>\n> Absent a concrete fancy covenant with fee bumping, I can't comment.\n>\n> 3) On Capital Efficiency\n>\n> Something like a precommitted or covenant fee bump requires the fee capital to be pre-committed inside the UTXO, whereas for something like Sponsors you can use capital you get sometime later. In certain models -- e.g., channels -- where you might expect only log(N) of your channels to fail in a given epoch, you don't need to allocate as much capital as if you were to have to do it in-band. This is also true for vaults where you know you only want to open 1 per month let's say, and not <all of your vaults> per month, which pre-committing requires.\n>\n> 4) On Protocol Design\n>\n> It's nice that you can abstract away your protocol design concerns as a \"second tier composition check\" v.s. having to modify your protocol to work with a fee bumping thing.\n>\n> There are a myriad of ways dynamic txns (e.g. for Eltoo) can lead to RBF pinning and similar, Sponsor type things allow you to design such protocols to not have any native way of paying for fees inside the actual \"Transaction Intents\" and use an external system to create the intended effect. It seems (to me) more robust that we can prove that a Sponsors mechanism allows any transaction -- regardless of covenant stuff, bugs, pinning, etc -- to move forward.\n>\n> Still... careful protocol design may permit the use of optimized constructions! For example, in a vault rather than assigning *no fee* maybe you can have a single branch with a reasonable estimated fee. If you are correct or overshot (let's say 50% chance?) then you don't need to add a sponsor. If you undershot, not to worry, just add a sponsor. Adopted broadly, this would cut the expected value of using sponsors by <however good you are at estimating future fees>. This basically enables all protocols to try to be more efficient, but backstop that with a guaranteed to work safe mechanism.\n>\n> There was something else I was going to say but I forgot about it... if it comes to me I'll send a follow up email.\n>\n> Cheers,\n>\n> Jeremy\n>\n> p.s.\n>\n>>\n>\n>> Of course this makes for a perfect DoS: it would be trivial for a miner to infer that you are usinga specific vault standard and guess other leaves and replace the witness to use the highest-feeratespending path. You could require a signature from any of the participants. Or, at the cost of anadditional depth, in the tree you could \"salt\" each leaf by pairing it with -say- an OP_RETURN leaf.\n>\n> you don't need a salt, you just need a unique payout addr (e.g. hardened derivation) per revocation txn and you cannot guess the branch.\n>\n> --\n> [@JeremyRubin](https://twitter.com/JeremyRubin)\n>\n> On Sat, Mar 12, 2022 at 10:34 AM darosior via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> The idea of a soft fork to fix dynamic fee bumping was recently put back on the table. It might\n>> sound radical, as what prevents today reasonable fee bumping for contracts with presigned\n>> transactions (pinning) has to do with nodes' relay policy. But the frustration is understandable\n>> given the complexity of designing fee bumping with today's primitives. [0]\n>> Recently too, there was a lot of discussions around covenants. Covenants (conceptually, not talking\n>> about any specific proposal) seem to open lots of new use cases and to be desired by (some?) Bitcoin\n>> application developers and users.\n>> I think that fee bumping using covenants has attractive properties, and it requires a soft fork that\n>> is already desirable beyond (trying) to fix fee bumping. However i could not come up with a solution\n>> as neat for other protocols than vaults. I'd like to hear from others about 1) taking this route for\n>> fee bumping 2) better ideas on applying this to other protocols.\n>>\n>> In a vault construction you have a UTxO which can only be spent by an Unvaulting transaction, whose\n>> output triggers a timelock before the expiration of which a revocation transaction may be confirmed.\n>> The revocation transaction being signed in advance (typically before sharing the signature for the\n>> Unvault transaction) you need fee bumping in order for the contract to actually be enforceable.\n>>\n>> Now, with a covenant you could commit to the revocation tx instead of presigning it. And using a\n>> Taproot tree you could commit to different versions of it with increasing feerate. Any network\n>> monitor (the brooadcaster, a watchtower, ..) would be able to RBF the revocation transaction if it\n>> doesn't confirm by spending using a leaf with a higher-feerate transaction being committed to.\n>>\n>> Of course this makes for a perfect DoS: it would be trivial for a miner to infer that you are using\n>> a specific vault standard and guess other leaves and replace the witness to use the highest-feerate\n>> spending path. You could require a signature from any of the participants. Or, at the cost of an\n>> additional depth, in the tree you could \"salt\" each leaf by pairing it with -say- an OP_RETURN leaf.\n>> But this leaves you with a possible internal blackmail for multi-party contracts (although it's less\n>> of an issue for vaults, and not one for single-party vaults).\n>> What you could do instead is attaching an increasing relative timelock to each leaf (as the committed\n>> revocation feerate increases, so does the timelock). You need to be careful to note wreck miner\n>> incentives here (see [0], [1], [2] on \"miner harvesting\"), but this enables the nice property of a\n>> feerate which \"adapts\" to the block space market. Another nice property of this approach is the\n>> integrated anti fee sniping protection if the revocation transaction pays a non-trivial amount of\n>> fees.\n>>\n>> Paying fees from \"shared\" funds instead of a per-watchtower fee-bumping wallet opened up the\n>> blackmail from the previous section, but the benefits of paying from internal funds shouldn't be\n>> understated.\n>> No need to decide on an amount to be refilled. No need to bother the user to refill the fee-bumping\n>> wallet (before they can participate in more contracts, or worse before a deadline at which all\n>> contracts are closed). No need for a potentially large amount of funds to just sit on a hot wallet\n>> \"just in case\". No need to duplicate this amount as you replicate the number of network monitors\n>> (which is critical to the security of such contracts).\n>> In addition, note how modifying the feerate of the revocation transaction in place is less expensive\n>> than adding a (pair of) new input (and output), let alone adding an entire new transaction to CPFP.\n>> Aside, and less importantly, it can be made to work with today's relay rules (just use fee thresholds\n>> adapted to the current RBF thresholds, potentially with some leeway to account for policy changes).\n>> Paying from shared funds (in addition to paying from internal funds) also prevents pervert\n>> incentives for contracts with more than 2 parties. In case one of the parties breaches it, all\n>> remaining parties have an incentive to enforce the contract.. But only one would otherwise pay for\n>> it! It would open up the door to some potential sneaky techniques to wait for another party to pay\n>> for the fees, which is at odd with the reactive security model.\n>>\n>> Let's examine how it could be concretely designed. Say you have a vault wallet software for a setup\n>> with 5 participants. The revocation delay is 144 blocks. You assume revocation to be infrequent (if\n>> one happens it's probably a misconfigured watchtower that needs be fixed before the next\n>> unvaulting), so you can afford infrequent overpayments and larger fee thresholds. Participants\n>> assume the vault will be spent within a year and assume a maximum possible feerate for this year of\n>> 10ksat/vb.\n>> They create a Taproot tree of depth 7. First leaf is the spending path (open to whomever the vault\n>> pays after the 144 blocks). Then the leaf `i` for `i` in `[1, 127]` is a covenant to the revocation\n>> transaction with a feerate `i * 79` sats/vb and a relative timelock of `i - 1` blocks.\n>> Assuming the covenant to the revocation transaction is 33 bytes [3], that's a witness of:\n>> 1 + 33 + 1 + 33 + 7 * 32 = 292 WU (73 vb)\n>> ^^^^^^ ^^^^^^^^^^^^^^\n>> witscript control block\n>> for any of the revocation paths. The revocation transaction is 1-input 1-output, so in total it's\n>> 10.5 + 41 + 73 + 43 = 167.5 vb\n>> ^^^^ ^^^^^^^^^^^ ^^^^\n>> header input|witness output\n>> The transaction size is not what you'd necessarily want to optimize for first, still, it is smaller\n>> in this case than using other feebumping primitives and has a smaller footprint on the UTxO set. For\n>> instance for adding a feebumping input and change output assuming all Taproot inputs and outputs\n>> (CPFP is necessarily even larger):\n>> 5 * 64 + 1 + 5 * (32 + 1) + 1 + 33 = 520 WU (105 vb)\n>> ^^^^^^ ^^^^^^^^^^^^^^^ ^^^^^^\n>> witness witscript control\n>> 10.5 + 41 + 105 + 41 + 16.5 + 2 * 43 = 300 vb\n>> ^^^^ ^^^^^^^^ ^^^^^^^^^ ^^^^^^\n>> header input|witness fb input|witness outputs\n>> From there, you can afford more depths at the tiny cost of 8 more vbytes each. You might want them\n>> for:\n>> - more granularity (if you can afford large enough timelocks)\n>> - optimizing for the spending path rather than the revocation one\n>> - adding a hashlock to prevent nuisance (with the above script a third party could malleate a\n>> spending path into a revocation one). You can use the OP_RETURN trick from above to prevent that.\n>>\n>> Unfortunately, the timelocked-covenant approach to feebumping only applies to bumping the first\n>> transaction of a chain (you can't pay for the parent with a timelock) so for instance it's not\n>> usable for HTLC transactions in Lightning to bump the parent commitment tx. The same goes for\n>> bumping the update tx in Coinpool.\n>> It could be worked around by having a different covenant per participant (paying the fee from either\n>> of the participants' output) behind a signature check. Of course it requires funds to already be in\n>> the contract (HTLC, Coinpool leaf) to pay for your own unilateral close, but if you don't have any\n>> fund in the contract it doesn't make sense to try to feebump it in the first place. The same goes\n>> for small amounts: you'd only allocate up to the value of the contract (minus a dust preference) in\n>> fees in order to enforce it.\n>> This is less nice for external monitors as it requires a private key (or another secret) to be\n>> committed to in advance) to be able to bump [4] and does not get rid of the \"who's gonna pay for the\n>> enforcement\" issue in >2-parties contracts. Still, it's more optimal and usable than CPFP or adding\n>> a pair of input/output for all the reasons mentioned above.\n>>\n>> Thoughts?\n>> Antoine\n>>\n>> [0] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-November/019614.html\n>> [1] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-November/019615.html\n>> [2] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019627.html\n>> [3] That's obviously close to the CTV construction. But using another more flexible (and therefore\n>> less optimized) construction would not be a big deal. It might in fact be necessary for more\n>> elaborated (realistic?) usecases than the simple one detailed here.\n>> [4] https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019879.html\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220314/dd75a8cb/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-16T23:29:42",
                "message_text_only": "Good morning Antoine,\n\n> For \"hot contracts\" a signature challenge is used to achieve the same. I know the latter is imperfect, since\n> the lower the uptime risk (increase the number of network monitors) the higher the DOS risk (as you duplicate\n> the key).. That's why i asked if anybody had some thoughts about this and if there was a cleverer way of doing\n> it.\n\nOkay, let me see if I understand your concern correctly.\n\nWhen using a signature challenge, the concern is that you need to presign multiple versions of a transaction with varying feerates.\n\nAnd you have a set of network monitors / watchtowers that are supposed to watch the chain on your behalf in case your ISP suddenly hates you for no reason.\n\nThe more monitors there are, the more likely that one of them will be corrupted by a miner and jump to the highest-feerate version, overpaying fees and making miners very happy.\nSuch is third-party trust.\n\nIs my understanding correct?\n\n\nA cleverer way, which requires consolidating (but is unable to eliminate) third-party trust, would be to use a DLC oracle.\nThe DLC oracle provides a set of points corresponding to a set of feerate ranges, and commits to publishing the scalar of one of those points at some particular future block height.\nOstensibly, the scalar it publishes is the one of the point that corresponds to the feerate range found at that future block height.\n\nYou then create adaptor signatures for each feerate version, corresponding to the feerate ranges the DLC oracle could eventually publish.\nThe adaptor signatures can only be completed if the DLC oracle publishes the corresponding scalar for that feerate range.\n\nYou can then send the adaptor signatures to multiple watchtowers, who can only publish one of the feerate versions, unless the DLC oracle is hacked and publishes multiple scalars (at which point the DLC oracle protocol reveals a privkey of the DLC oracle, which should be usable for slashing some bond of the DLC oracle).\nThis prevents any of them from publishing the highest-feerate version, as the adaptor signature cannot be completed unless that is what the oracle published.\n\nThere are still drawbacks:\n\n* Third-party trust risk: the oracle can still lie.\n  * DLC oracles are prevented from publishing multiple scalars; they cannot be prevented from publishing a single wrong scalar.\n* DLCs must be time bound.\n  * DLC oracles commit to publishing a particular point at a particular fixed time.\n  * For \"hot\" dynamic protocols, you need the ability to invoke the oracle at any time, not a particular fixed time.\n\nThe latter probably makes this unusable for hot protocols anyway, so maybe not so clever.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "darosior",
                "date": "2022-03-21T12:06:54",
                "message_text_only": "Hi ZmnSCPxj,\n\nThanks for the feedback. The DLC idea is interesting but you are centralizing the liveness requirements,\neffectively creating a SPOF: in order to bypass the revocation clause no need to make sure to down each and\nevery watchtower anymore, just down the oracle and you are sure no revocation transaction can be pushed.\n\n\n> Okay, let me see if I understand your concern correctly.\n> When using a signature challenge, the concern is that you need to presign multiple versions of a transaction with varying feerates.\n\nI was thinking of having a hot key (in this case probably shared amongst the monitors) where they would sign\nthe right fee level at broadcast time. Pre-signing makes it quickly too many signatures (and kills the purpose\nof having covenants in the first place).\n\n> And you have a set of network monitors / watchtowers that are supposed to watch the chain on your behalf in case your ISP suddenly hates you for no reason.\n> The more monitors there are, the more likely that one of them will be corrupted by a miner and jump to the highest-feerate version, overpaying fees and making miners very happy.\n> Such is third-party trust.\n> Is my understanding correct?\n\nYour understanding of the tradeoff is correct.\n\n------- Original Message -------\n\nLe jeudi 17 mars 2022 \u00e0 12:29 AM, ZmnSCPxj <ZmnSCPxj at protonmail.com> a \u00e9crit :\n\n> Good morning Antoine,\n>\n> > For \"hot contracts\" a signature challenge is used to achieve the same. I know the latter is imperfect, since\n> >\n> > the lower the uptime risk (increase the number of network monitors) the higher the DOS risk (as you duplicate\n> >\n> > the key).. That's why i asked if anybody had some thoughts about this and if there was a cleverer way of doing\n> >\n> > it.\n>\n> Okay, let me see if I understand your concern correctly.\n>\n> When using a signature challenge, the concern is that you need to presign multiple versions of a transaction with varying feerates.\n>\n> And you have a set of network monitors / watchtowers that are supposed to watch the chain on your behalf in case your ISP suddenly hates you for no reason.\n>\n> The more monitors there are, the more likely that one of them will be corrupted by a miner and jump to the highest-feerate version, overpaying fees and making miners very happy.\n>\n> Such is third-party trust.\n>\n> Is my understanding correct?\n>\n> A cleverer way, which requires consolidating (but is unable to eliminate) third-party trust, would be to use a DLC oracle.\n>\n> The DLC oracle provides a set of points corresponding to a set of feerate ranges, and commits to publishing the scalar of one of those points at some particular future block height.\n>\n> Ostensibly, the scalar it publishes is the one of the point that corresponds to the feerate range found at that future block height.\n>\n> You then create adaptor signatures for each feerate version, corresponding to the feerate ranges the DLC oracle could eventually publish.\n>\n> The adaptor signatures can only be completed if the DLC oracle publishes the corresponding scalar for that feerate range.\n>\n> You can then send the adaptor signatures to multiple watchtowers, who can only publish one of the feerate versions, unless the DLC oracle is hacked and publishes multiple scalars (at which point the DLC oracle protocol reveals a privkey of the DLC oracle, which should be usable for slashing some bond of the DLC oracle).\n>\n> This prevents any of them from publishing the highest-feerate version, as the adaptor signature cannot be completed unless that is what the oracle published.\n>\n> There are still drawbacks:\n>\n> * Third-party trust risk: the oracle can still lie.\n>\n> * DLC oracles are prevented from publishing multiple scalars; they cannot be prevented from publishing a single wrong scalar.\n>\n> * DLCs must be time bound.\n>\n> * DLC oracles commit to publishing a particular point at a particular fixed time.\n>\n> * For \"hot\" dynamic protocols, you need the ability to invoke the oracle at any time, not a particular fixed time.\n>\n> The latter probably makes this unusable for hot protocols anyway, so maybe not so clever.\n>\n> Regards,\n>\n> ZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Covenants and feebumping",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "darosior",
                "Jeremy Rubin"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 46773
        }
    },
    {
        "title": "[bitcoin-dev] CTV dramatically improves DLCs",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-15T17:28:05",
                "message_text_only": "I've created a prototype of this protocol in Sapio for your perusal:\n\nhttps://github.com/sapio-lang/sapio/blob/master/sapio-contrib/src/contracts/derivatives/dlc.rs\n\nFeel free to tweak the test and use it as a benchmark, i tested 1 oracle\nwith 100,000 different payouts and saw it take around 13s on a release\nbuild.\n\nI'll be playing around with this a bit (I doubt Sapio Studio can handle a\ngui for 100,000 nodes), but I figured it was worth a share.\n\nCheers,\n\nJeremy\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220315/cabe84ce/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV dramatically improves DLCs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 651
        }
    },
    {
        "title": "[bitcoin-dev] OP_RETURN inside TapScript",
        "thread_messages": [
            {
                "author": "Peter Todd",
                "date": "2022-03-16T18:21:30",
                "message_text_only": "On Thu, Feb 24, 2022 at 10:02:08AM +0100, vjudeu via bitcoin-dev wrote:\n> Since Taproot was activated, we no longer need separate OP_RETURN outputs to be pushed on-chain. If we want to attach any data to a transaction, we can create \"OP_RETURN <anything>\" as a branch in the TapScript. In this way, we can store that data off-chain and we can always prove that they are connected with some taproot address, that was pushed on-chain. Also, we can store more than 80 bytes for \"free\", because no such taproot branch will be ever pushed on-chain and used as an input. That means we can use \"OP_RETURN <1.5 GB of data>\", create some address having that taproot branch, and later prove to anyone that such \"1.5 GB of data\" is connected with our taproot address.\n\nThere are two use-cases for OP_RETURN: committing to data, and publishing data.\nYour proposal can only do the former, not the latter, and there are use-cases\nfor both.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220316/d213a1cc/attachment-0001.sig>"
            },
            {
                "author": "vjudeu at gazeta.pl",
                "date": "2022-03-19T18:32:00",
                "message_text_only": "> There are two use-cases for OP_RETURN: committing to data, and publishing data. Your proposal can only do the former, not the latter, and there are use-cases for both.\n\nOnly the former is needed. Pushing data on-chain is expensive and that kind of data is useful only to the transaction maker. Also, the latter can be pushed on a separate chain (or even a separate layer that is not a chain at all).\n\nAlso note that since Taproot we have the latter: we can spend by TapScript and reveal some public key and tapbranches. It is possible to push more than 80 bytes in this way, so why direct OP_RETURN is needed, except for backward-compatibility? (for example in Segwit commitments)\n\nThere is only one problem with spending by TapScript, when it comes to publishing data: only the first item is the public key. If we could use public keys instead of tapbranch hashes, we could literally replace \"OP_RETURN <commitment>\" with \"<tweakedPublicKey> <tweakedTapBranchKey1> <tweakedTapBranchKey2> <tweakedTapBranchKey3> ... <tweakedTapBranchKeyN>\". Then, we could use unspendable public keys to push data, so OP_RETURN would be obsolete.\n\nBy the way, committing to data has a lot of use cases, for example the whole idea of NameCoin could be implemented on such OP_RETURN's. Instead of creating some special transaction upfront, people could place some hidden commitment and reveal that later. Then, there would be no need to produce any new coins out of thin air, because everything would be merge-mined by default, providing Bitcoin-level Proof of Work protection all the time, 24/7/365. Then, people could store that revealed commitments on their own chain, just to keep track of who owns which name. And then, that network could easily turn on and off all Bitcoin features as they please. Lightning Network on NameCoin? No problem, even the same satoshis could be used to pay for domains!\n\nOn 2022-03-16 19:21:37 user Peter Todd <pete at petertodd.org> wrote:\n> On Thu, Feb 24, 2022 at 10:02:08AM +0100, vjudeu via bitcoin-dev wrote:\n> Since Taproot was activated, we no longer need separate OP_RETURN outputs to be pushed on-chain. If we want to attach any data to a transaction, we can create \"OP_RETURN <anything>\" as a branch in the TapScript. In this way, we can store that data off-chain and we can always prove that they are connected with some taproot address, that was pushed on-chain. Also, we can store more than 80 bytes for \"free\", because no such taproot branch will be ever pushed on-chain and used as an input. That means we can use \"OP_RETURN <1.5 GB of data>\", create some address having that taproot branch, and later prove to anyone that such \"1.5 GB of data\" is connected with our taproot address.\n\nThere are two use-cases for OP_RETURN: committing to data, and publishing data.\nYour proposal can only do the former, not the latter, and there are use-cases\nfor both.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org"
            },
            {
                "author": "Kostas Karasavvas",
                "date": "2022-03-21T11:00:38",
                "message_text_only": "Hi vjudeu,\n\nThere are use cases where your following assumption is wrong:  \".. and that\nkind of data is useful only to the transaction maker.\"\n\nNo one really publishes the actual data with an OP_RETURN. They publish the\nhash (typically merkle root) of that 1.5 GB of data. So the overhead is\njust 32 bytes for arbitrarily large data sets. What you gain with these 32\nbytes is that your hash is visible to anyone and they can verify it without\nactive participation of the hash publisher.\n\nRegards,\nKostas\n\n\nOn Sat, Mar 19, 2022 at 9:26 PM vjudeu via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> > There are two use-cases for OP_RETURN: committing to data, and\n> publishing data. Your proposal can only do the former, not the latter, and\n> there are use-cases for both.\n>\n> Only the former is needed. Pushing data on-chain is expensive and that\n> kind of data is useful only to the transaction maker. Also, the latter can\n> be pushed on a separate chain (or even a separate layer that is not a chain\n> at all).\n>\n> Also note that since Taproot we have the latter: we can spend by TapScript\n> and reveal some public key and tapbranches. It is possible to push more\n> than 80 bytes in this way, so why direct OP_RETURN is needed, except for\n> backward-compatibility? (for example in Segwit commitments)\n>\n> There is only one problem with spending by TapScript, when it comes to\n> publishing data: only the first item is the public key. If we could use\n> public keys instead of tapbranch hashes, we could literally replace\n> \"OP_RETURN <commitment>\" with \"<tweakedPublicKey> <tweakedTapBranchKey1>\n> <tweakedTapBranchKey2> <tweakedTapBranchKey3> ... <tweakedTapBranchKeyN>\".\n> Then, we could use unspendable public keys to push data, so OP_RETURN would\n> be obsolete.\n>\n> By the way, committing to data has a lot of use cases, for example the\n> whole idea of NameCoin could be implemented on such OP_RETURN's. Instead of\n> creating some special transaction upfront, people could place some hidden\n> commitment and reveal that later. Then, there would be no need to produce\n> any new coins out of thin air, because everything would be merge-mined by\n> default, providing Bitcoin-level Proof of Work protection all the time,\n> 24/7/365. Then, people could store that revealed commitments on their own\n> chain, just to keep track of who owns which name. And then, that network\n> could easily turn on and off all Bitcoin features as they please. Lightning\n> Network on NameCoin? No problem, even the same satoshis could be used to\n> pay for domains!\n>\n> On 2022-03-16 19:21:37 user Peter Todd <pete at petertodd.org> wrote:\n> > On Thu, Feb 24, 2022 at 10:02:08AM +0100, vjudeu via bitcoin-dev wrote:\n> > Since Taproot was activated, we no longer need separate OP_RETURN\n> outputs to be pushed on-chain. If we want to attach any data to a\n> transaction, we can create \"OP_RETURN <anything>\" as a branch in the\n> TapScript. In this way, we can store that data off-chain and we can always\n> prove that they are connected with some taproot address, that was pushed\n> on-chain. Also, we can store more than 80 bytes for \"free\", because no such\n> taproot branch will be ever pushed on-chain and used as an input. That\n> means we can use \"OP_RETURN <1.5 GB of data>\", create some address having\n> that taproot branch, and later prove to anyone that such \"1.5 GB of data\"\n> is connected with our taproot address.\n>\n> There are two use-cases for OP_RETURN: committing to data, and publishing\n> data.\n> Your proposal can only do the former, not the latter, and there are\n> use-cases\n> for both.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220321/73bed610/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "OP_RETURN inside TapScript",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "vjudeu at gazeta.pl",
                "Kostas Karasavvas",
                "Peter Todd"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 8227
        }
    },
    {
        "title": "[bitcoin-dev] CTV Meeting #6 Agenda (Tuesday, March 22nd, 12:00 PT / 7PM UTC)",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-21T21:32:14",
                "message_text_only": "Hi devs,\n\nThe 6th fortnightly CTV meeting will be held tomorrow March 22nd at noon\npacific 7 utc. *Note the Daylight Savings Time Change, 7PM UTC. Before it\nwas 8 UTC. Now it is not.*\n\nThe agenda for tomorrow will be a tutorial on using Sapio Studio, and it\nwill be building on the knowledge from the 4th meeting (so please review\nthe notes / that tutorial as well). Minimally, you will be expected to be\nable to build a sapio WASM blob and the CLI, but everything else will be\ncovered in detail in this meeting. If you don't like running NPM\ndependencies and don't want to trust me, please have a VM/burner set up\nwhere you can do so (although not sure how well that will work?).\n\nThe culmination of the tutorial will be trying out different contracts,\nincluding one similar to James O'Beirne's vault.\n\nAfter the tutorial, we can have a discussion about tooling and\ninfrastructure.\n\nfor meeting #4, see:\n\ntutorial:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019973.html\nnotes:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-February/019974.html\nlogs (helpful if stuck in tutorial):\nhttps://gnusha.org/ctv-bip-review/2022-02-22.log\n\nI will try to post by later tonight basic steps to follow for setting\nthings up from start to finish.\n\nA preview of the software you'll play with.\n\n[image: image.png]\n\n\nCheers,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220321/7b26903a/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: image.png\nType: image/png\nSize: 496599 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220321/7b26903a/attachment-0001.png>"
            }
        ],
        "thread_summary": {
            "title": "CTV Meeting #6 Agenda (Tuesday, March 22nd, 12:00 PT / 7PM UTC)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1858
        }
    },
    {
        "title": "[bitcoin-dev] Beyond Jets: Microcode: Consensus-Critical Jets Without Softforks",
        "thread_messages": [
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-22T05:37:03",
                "message_text_only": "Good morning list,\n\nIt is entirely possible that I have gotten into the deep end and am now drowning in insanity, but here goes....\n\nSubject: Beyond Jets: Microcode: Consensus-Critical Jets Without Softforks\n\nIntroduction\n============\n\nRecent (Early 2022) discussions on the bitcoin-dev mailing\nlist have largely focused on new constructs that enable new\nfunctionality.\n\nOne general idea can be summarized this way:\n\n* We should provide a very general language.\n  * Then later, once we have learned how to use this language,\n    we can softfork in new opcodes that compress sections of\n    programs written in this general language.\n\nThere are two arguments against this style:\n\n1.  One of the most powerful arguments the \"general\" side of\n    the \"general v specific\" debate is that softforks are\n    painful because people are going to keep reiterating the\n    activation parameters debate in a memoryless process, so\n    we want to keep the number of softforks low.\n    * So, we should just provide a very general language and\n      never softfork in any other change ever again.\n2.  One of the most powerful arguments the \"general\" side of\n    the \"general v specific\" debate is that softforks are\n    painful because people are going to keep reiterating the\n    activation parameters debate in a memoryless process, so\n    we want to keep the number of softforks low.\n    * So, we should just skip over the initial very general\n      language and individually activate small, specific\n      constructs, reducing the needed softforks by one.\n\nBy taking a page from microprocessor design, it seems to me\nthat we can use the same above general idea (a general base\nlanguage where we later \"bless\" some sequence of operations)\nwhile avoiding some of the arguments against it.\n\nDigression: Microcodes In CISC Microprocessors\n----------------------------------------------\n\nIn the 1980s and 1990s, two competing microprocessor design\nparadigms arose:\n\n* Complex Instruction Set Computing (CISC)\n  - Few registers, many addressing/indexing modes, variable\n    instruction length, many obscure instructions.\n* Reduced Instruction Set Computing (RISC)\n  - Many registers, usually only immediate and indexed\n    addressing modes, fixed instruction length, few\n    instructions.\n\nIn CISC, the microprocessor provides very application-specific\ninstructions, often with a small number of registers with\nspecific uses.\nThe instruction set was complicated, and often required\nmultiple specific circuits for each application-specific\ninstruction.\nInstructions had varying sizes and varying number of cycles.\n\nIn RISC, the micrprocessor provides fewer instructions, and\nprogrammers (or compilers) are supposed to generate the code\nfor all application-specific needs.\nThe processor provided large register banks which could be\nused very generically and interchangeably.\nInstructions had the same size and every instruction took a\nfixed number of cycles.\n\nIn CISC you usually had shorter code which could be written\nby human programmers in assembly language or machine language.\nIn RISC, you generally had longer code, often difficult for\nhuman programmers to write, and you *needed* a compiler to\ngenerate it (unless you were very careful, or insane enough\nyou could scroll over multiple pages of instructions without\nbecoming more insane), or else you might forget about stuff\nlike jump slots.\n\nFor the most part, RISC lost, since most modern processors\ntoday are x86 or x86-64, an instruction set with varying\ninstruction sizes, varying number of cycles per instruction,\nand complex instructions with application-specific uses.\n\nOr at least, it *looks like* RISC lost.\nIn the 90s, Intel was struggling since their big beefy CISC\ndesigns were becoming too complicated.\nBugs got past testing and into mass-produced silicon.\nRISC processors were beating the pants off 386s in terms of\nraw number of computations per second.\n\nRISC processors had the major advantage that they were\ninherently simpler, due to having fewer specific circuits\nand filling up their silicon with general-purpose registers\n(which are large but very simple circuits) to compensate.\nThis meant that processor designers could fit more of the\ndesign in their merely human meat brains, and were less\nlikely to make mistakes.\nThe fixed number of cycles per instruction made it trivial\nto create a fixed-length pipeline for instruction processing,\nand practical RISC processors could deliver one instruction\nper clock cycle.\nWorse, the simplicity of RISC meant that smaller and less\nexperienced teams could produce viable competitors to the\nIntel x86s.\n\nSo what Intel did was to use a RISC processor, and add a\nspecial Instruction Decoder unit.\nThe Instruction Decoder would take the CISC instruction\nstream accepted by classic Intel x86 processors, and emit\nRISC instructions for the internal RISC processor.\nCISC instructions might be variable length and have variable\nnumber of cycles, but the emitted RISC instructions were\nindividually fixed length and fixed number of cycles.\nA CISC instruction might be equivalent to a single RISC\ninstruction, or several.\n\nWith this technique, Intel could deliver performance\napproaching their RISC-only competition, while retaining\nback-compatibility with existing software written for their\nclassic CISC processors.\n\nAt its core, the Instruction Decoder was a table-driven\nparser.\nThis lookup table could be stored into on-chip flash memory.\nThis had the advantage that the on-chip flash memory could be\nupdated in case of bugs in the implementation of CISC\ninstructions.\nThis on-chip flash memory was then termed \"microcode\".\n\nImportant advantages of this \"microcode\" technique were:\n\n* Back-compatibility with existing instruction sets.\n* Easier and more scalable underlying design due to ability\n  to use RISC techniques while still supporting CISC instruction\n  sets.\n* Possible to fix bugs in implementations of complex CISC\n  instructions by uploading new microcode.\n\n(Obviously I have elided a bunch of stuff, but the above\nrough sketch should be sufficient as introduction.)\n\nBitcoin Consensus Layer As Hardware\n-----------------------------------\n\nWhile Bitcoin fullnode implementations are software, because\nof the need for consensus, this software is not actually very\n\"soft\".\nOne can consider that, just as it would take a long time for\nnew hardware to be designed with a changed instruction set,\nit is similarly taking a long time to change Bitcoin to\nsupport changed feature sets.\n\nThus, we should really consider the Bitcoin consensus layer,\nand its SCRIPT, as hardware that other Bitcoin software and\nlayers run on top of.\n\nThis thus opens up the thought of using techniques that were\nuseful in hardware design.\nSuch as microcode: a translation layer from \"old\" instruction\nsets to \"new\" instruction sets, with the ability to modify this\nmapping.\n\nMicrocode For Bitcoin SCRIPT\n============================\n\nI propose:\n\n* Define a generic, low-level language (the \"RISC language\").\n* Define a mapping from a specific, high-level language to\n  the above language (the microcode).\n* Allow users to sacrifice Bitcoins to define a new microcode.\n* Have users indicate the microcode they wish to use to\n  interpret their Tapscripts.\n\nAs a concrete example, let us consider the current Bitcoin\nSCRIPT as the \"CISC\" language.\n\nWe can then support a \"RISC\" language that is composed of\ngeneral instructions, such as arithmetic, SECP256K1 scalar\nand point math, bytevector concatenation, sha256 midstates,\nbytevector bit manipulation, transaction introspection, and\nso on.\nThis \"RISC\" language would also be stack-based.\nAs the \"RISC\" language would have more possible opcodes,\nwe may need to use 2-byte opcodes for the \"RISC\" language\ninstead of 1-byte opcodes.\nLet us call this \"RISC\" language the micro-opcode language.\n\nThen, the \"microcode\" simply maps the existing Bitcoin\nSCRIPT `OP_` codes to one or more `UOP_` micro-opcodes.\n\nAn interesting fact is that stack-based languages have\nautomatic referential transparency; that is, if I define\nsome new word in a stack-based language and use that word,\nI can replace verbatim the text of the new word in that\nplace without issue.\nCompare this to a language like C, where macro authors\nhave to be very careful about inadvertent variable\ncapture, wrapping `do { ... } while(0)` to avoid problems\nwith `if` and multiple statements, multiple execution, and\nso on.\n\nThus, a sequence of `OP_` opcodes can be mapped to a\nsequence of equivalent `UOP_` micro-opcodes without\nchanging the interpretation of the source language, an\nimportant property when considering such a \"compiled\"\nlanguage.\n\nWe start with a default microcode which is equivalent\nto the current Bitcoin language.\nWhen users want to define a new microcode to implement\nnew `OP_` codes or change existing `OP_` codes, they\ncan refer to a \"base\" microcode, and only have to\nprovide the new mappings.\n\nA microcode is fundamentally just a mapping from an\n`OP_` code to a variable-length sequence of `UOP_`\nmicro-opcodes.\n\n```Haskell\nimport Data.Map\n-- type Opcode\n-- type UOpcode\nnewtype Microcode = Microcode (Map.Map Opcode [UOpcode])\n```\n\nSemantically, the SCRIPT interpreter processes `UOP_`\nmicro-opcodes.\n\n```Haskell\n-- instance Monad Interpreter -- can `fail`.\ninterpreter :: Transaction -> TxInput -> [UOpcode] -> Interpreter ()\n```\n\nExample\n-------\n\nSuppose a user wants to re-enable `OP_CAT`, and nothing\nelse.\n\nThat user creates a microcode, referring to the current\ndefault Bitcoin SCRIPT microcode as the \"base\".\nThe base microcode defines `OP_CAT` as equal to the\nsequence `UOP_FAIL` i.e. a micro-opcode that always fails.\nHowever, the new microcode will instead redefine the\n`OP_CAT` as the micro-opcode sequence `UOP_CAT`.\n\nMicrocodes then have a standard way of being represented\nas a byte sequence.\nThe user serializes their new microcode as a byte\nsequence.\n\nThen, the user creates a new transaction where one of\nthe outputs contains, say, 1.0 Bitcoins (exact required\nvalue TBD), and has the `scriptPubKey` of\n`OP_TRUE OP_RETURN <serialized_microcode>`.\nThis output is a \"microcode introduction output\", which\nis provably unspendable, thus burning the Bitcoins.\n\n(It need not be a single user, multiple users can\ncoordinate by signing a single transaction that commits\ntheir funds to the microcode introduction.)\n\nOnce the above transaction has been deeply confirmed,\nthe user can then take the hash of the microcode\nserialization.\nThen the user can use a SCRIPT with `OP_CAT` enabled,\nby using a Tapscript with, say, version `0xce`, and\nwith the SCRIPT having the microcode hash as its first\nbytes, followed by the `OP_` codes.\n\nFullnodes will then process recognized microcode\nintroduction outputs and store mappings from their\nhashes to the microcodes in a new microcodes index.\nFullnodes can then process version-`0xce` Tapscripts\nby checking if the microcodes index has the indicated\nmicrocode hash.\n\nSemantically, fullnodes take the SCRIPT, and for each\n`OP_` code in it, expands it to a sequence of `UOP_`\nmicro-opcodes, then concatenates each such sequence.\nThen, the SCRIPT interpreter operates over a sequence\nof `UOP_` micro-opcodes.\n\nOptimizing Microcodes\n---------------------\n\nSuppose there is some new microcode that users have\npublished onchain.\n\nWe want to be able to execute the defined microcode\nfaster than expanding an `OP_`-code SCRIPT to a\n`UOP_`-code SCRIPT and having an interpreter loop\nover the `UOP_`-code SCRIPT.\n\nWe can use LLVM.\n\nWARNING: LLVM might not be appropriate for\nnetwork-facing security-sensitive applications.\nIn particular, LLVM bugs. especially nondeterminism\nbugs, can lead to consensus divergence and disastrous\nchainsplits!\nOn the other hand, LLVM bugs are compiler bugs and\nthe same bugs can hit the static compiler `cc`, too,\nsince the same LLVM code runs in both JIT and static\ncompilation, so this risk already exists for Bitcoin.\n(i.e. we already rely on LLVM not being buggy enough\nto trigger Bitcoin consensus divergence, else we would\nhave written Bitcoin Core SCRIPT interpreter in\nassembly.)\n\nEach `UOP_`-code has an equivalent tree of LLVM code.\nFor each `Opcode` in the microcode, we take its\nsequence of `UOpcode`s and expand them to this tree,\nconcatenating the equivalent trees for each `UOpcode`\nin the sequence.\nThen we ask LLVM to JIT-compile this code to a new\nfunction, running LLVM-provided optimizers.\nThen we put a pointer to this compiled function to a\n256-long array of functions, where the array index is\nthe `OP_` code.\n\nThe SCRIPT interpreter then simply iterates over the\n`OP_` code SCRIPT and calls each of the JIT-compiled\nfunctions.\nThis reduces much of the overhead of the `UOP_` layer\nand makes it approach the current performance of the\nexisting `OP_` interpreter.\n\nFor the default Bitcoin SCRIPT, the opcodes array\ncontains pointers to statically-compiled functions.\nA microcode that is based on the default Bitcoin\nSCRIPT copies this opcodes array, then overwrites\nthe entries.\n\nFuture versions of Bitcoin Core can \"bless\"\nparticular microcodes by providing statically-compiled\nfunctions for those microcodes.\nThis leads to even better performance (there is\nno need to recompile ancient onchain microcodes each\ntime Bitcoin Core starts) without any consensus\ndivergence.\nIt is a pure optimization and does not imply a\ntightening of rules, and is thus not a softfork.\n\n(To reduce the chance of network faults being used\nto poke into `W|X` memory (since `W|X` memory is\nneeded in order to actually JIT compile) we can\nisolate the SCRIPT interpreter into its own process\nseparate from the network-facing code.\nThis does imply additional overhead in serializing\ntransactions we want to ask the SCRIPT interpreter\nto validate.)\n\nComparison To Jets\n------------------\n\nThis technique allows users to define \"jets\", i.e.\nsequences of low-level general operations that users\nhave determined are common enough they should just\nbe implemented as faster code that is executed\ndirectly by the underlying hardware processor rather\nthan via a software interpreter.\nBasically, each redefined `OP_` code is a jet of a\nsequence of `UOP_` micro-opcodes.\n\nWe implement this by dynamically JIT-compiling the\nproposed jets, as described above.\nSCRIPTs using jetted code remain smaller, as the\njet definition is done in a previous transaction and\ndoes not require copy-pasta (Do Not Repeat Yourself!).\nAt the same time, jettification is not tied to\ndevelopers, thus removing the need to keep softforking\nnew features --- we only need define a sufficiently\ngeneral language and then we can implement pretty much\nanything worth implementing (and a bunch of other things\nthat should not be implemented, but hey, users gonna\nuse...).\n\nBugs in existing microcodes can be fixed by basing a\nnew microcode from the existing microcode, and\nredefining the buggy implementation.\nExisting Tapscripts need to be re-spent to point to\nthe new bugfixed microcode, but if you used the\npoint-spend branch as an N-of-N of all participants\nyou have an upgrade mechanism for free.\n\nIn order to ensure that the JIT-compilation of new\nmicrocodes is not triggered trivially, we require\nthat users petitioning for the jettification of some\noperations (i.e. introducing a new microcode) must\nsacrifice Bitcoins.\n\nBurning Bitcoins is better than increasing the weight\nof microcode introduction outputs; all fullnodes are\naffected by the need to JIT-compile the new microcode,\nso they benefit from the reduction in supply, thus\ngetting compensated for the work of JIT-compiling the\nnew microcode.\nOhter mechanisms for making microcode introduction\noutputs expensive are also possible.\n\nNothing really requires that we use a stack-based\nlanguage for this; any sufficiently FP language\nshould allow referential transparency."
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-03-22T15:08:33",
                "message_text_only": "Setting aside my thoughts that something like Simplicity would make a\nbetter platform than Bitcoin Script (due to expression operating on a more\nnarrow interface than the entire stack (I'm looking at you OP_DEPTH)) there\nis an issue with namespace management.\n\nIf I understand correctly, your implication was that once opcodes are\nredefined by an OP_RETURN transaction, subsequent transactions of that\nopcode refer to the new microtransaction.  But then we have a race\ncondition between people submitting transactions expecting the outputs to\nrefer to the old code and having their code redefined by the time they do\nget confirmed  (or worse having them reorged).\n\nI've partially addressed this issue in my Simplicity design where the\ncommitment of a Simplicity program in a scriptpubkey covers the hash of the\nspecification of the jets used, which makes commits unambiguously to the\nsemantics (rightly or wrongly).  But the issue resurfaces at redemption\ntime where I (currently) have a consensus critical map of codes to jets\nthat is used to decode the witness data into a Simplicity program.  If one\nwere to allow this map of codes to jets to be replaced (rather than just\nextended) then it would cause redemption to fail, because the hash of the\nnew jets would no longer match the hash of the jets appearing the the\ninput's scriptpubkey commitment.  While this is still not good and I don't\nrecommend it, it is probably better than letting the semantics of your\nprograms be changed out from under you.\n\nThis comment is not meant as an endorsement of ths idea, which is a little\nbit out there, at least as far as Bitcoin is concerned. :)\n\nMy long term plans are to move this consensus critical map of codes out of\nthe consensus layer and into the p2p layer where peers can negotiate their\nown encodings between each other.  But that plan is also a little bit out\nthere, and it still doesn't solve the issue of how to weight reused jets,\nwhere weight is still consensus critical.\n\nOn Tue, Mar 22, 2022 at 1:37 AM ZmnSCPxj via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Good morning list,\n>\n> It is entirely possible that I have gotten into the deep end and am now\n> drowning in insanity, but here goes....\n>\n> Subject: Beyond Jets: Microcode: Consensus-Critical Jets Without Softforks\n>\n> Introduction\n> ============\n>\n> Recent (Early 2022) discussions on the bitcoin-dev mailing\n> list have largely focused on new constructs that enable new\n> functionality.\n>\n> One general idea can be summarized this way:\n>\n> * We should provide a very general language.\n>   * Then later, once we have learned how to use this language,\n>     we can softfork in new opcodes that compress sections of\n>     programs written in this general language.\n>\n> There are two arguments against this style:\n>\n> 1.  One of the most powerful arguments the \"general\" side of\n>     the \"general v specific\" debate is that softforks are\n>     painful because people are going to keep reiterating the\n>     activation parameters debate in a memoryless process, so\n>     we want to keep the number of softforks low.\n>     * So, we should just provide a very general language and\n>       never softfork in any other change ever again.\n> 2.  One of the most powerful arguments the \"general\" side of\n>     the \"general v specific\" debate is that softforks are\n>     painful because people are going to keep reiterating the\n>     activation parameters debate in a memoryless process, so\n>     we want to keep the number of softforks low.\n>     * So, we should just skip over the initial very general\n>       language and individually activate small, specific\n>       constructs, reducing the needed softforks by one.\n>\n> By taking a page from microprocessor design, it seems to me\n> that we can use the same above general idea (a general base\n> language where we later \"bless\" some sequence of operations)\n> while avoiding some of the arguments against it.\n>\n> Digression: Microcodes In CISC Microprocessors\n> ----------------------------------------------\n>\n> In the 1980s and 1990s, two competing microprocessor design\n> paradigms arose:\n>\n> * Complex Instruction Set Computing (CISC)\n>   - Few registers, many addressing/indexing modes, variable\n>     instruction length, many obscure instructions.\n> * Reduced Instruction Set Computing (RISC)\n>   - Many registers, usually only immediate and indexed\n>     addressing modes, fixed instruction length, few\n>     instructions.\n>\n> In CISC, the microprocessor provides very application-specific\n> instructions, often with a small number of registers with\n> specific uses.\n> The instruction set was complicated, and often required\n> multiple specific circuits for each application-specific\n> instruction.\n> Instructions had varying sizes and varying number of cycles.\n>\n> In RISC, the micrprocessor provides fewer instructions, and\n> programmers (or compilers) are supposed to generate the code\n> for all application-specific needs.\n> The processor provided large register banks which could be\n> used very generically and interchangeably.\n> Instructions had the same size and every instruction took a\n> fixed number of cycles.\n>\n> In CISC you usually had shorter code which could be written\n> by human programmers in assembly language or machine language.\n> In RISC, you generally had longer code, often difficult for\n> human programmers to write, and you *needed* a compiler to\n> generate it (unless you were very careful, or insane enough\n> you could scroll over multiple pages of instructions without\n> becoming more insane), or else you might forget about stuff\n> like jump slots.\n>\n> For the most part, RISC lost, since most modern processors\n> today are x86 or x86-64, an instruction set with varying\n> instruction sizes, varying number of cycles per instruction,\n> and complex instructions with application-specific uses.\n>\n> Or at least, it *looks like* RISC lost.\n> In the 90s, Intel was struggling since their big beefy CISC\n> designs were becoming too complicated.\n> Bugs got past testing and into mass-produced silicon.\n> RISC processors were beating the pants off 386s in terms of\n> raw number of computations per second.\n>\n> RISC processors had the major advantage that they were\n> inherently simpler, due to having fewer specific circuits\n> and filling up their silicon with general-purpose registers\n> (which are large but very simple circuits) to compensate.\n> This meant that processor designers could fit more of the\n> design in their merely human meat brains, and were less\n> likely to make mistakes.\n> The fixed number of cycles per instruction made it trivial\n> to create a fixed-length pipeline for instruction processing,\n> and practical RISC processors could deliver one instruction\n> per clock cycle.\n> Worse, the simplicity of RISC meant that smaller and less\n> experienced teams could produce viable competitors to the\n> Intel x86s.\n>\n> So what Intel did was to use a RISC processor, and add a\n> special Instruction Decoder unit.\n> The Instruction Decoder would take the CISC instruction\n> stream accepted by classic Intel x86 processors, and emit\n> RISC instructions for the internal RISC processor.\n> CISC instructions might be variable length and have variable\n> number of cycles, but the emitted RISC instructions were\n> individually fixed length and fixed number of cycles.\n> A CISC instruction might be equivalent to a single RISC\n> instruction, or several.\n>\n> With this technique, Intel could deliver performance\n> approaching their RISC-only competition, while retaining\n> back-compatibility with existing software written for their\n> classic CISC processors.\n>\n> At its core, the Instruction Decoder was a table-driven\n> parser.\n> This lookup table could be stored into on-chip flash memory.\n> This had the advantage that the on-chip flash memory could be\n> updated in case of bugs in the implementation of CISC\n> instructions.\n> This on-chip flash memory was then termed \"microcode\".\n>\n> Important advantages of this \"microcode\" technique were:\n>\n> * Back-compatibility with existing instruction sets.\n> * Easier and more scalable underlying design due to ability\n>   to use RISC techniques while still supporting CISC instruction\n>   sets.\n> * Possible to fix bugs in implementations of complex CISC\n>   instructions by uploading new microcode.\n>\n> (Obviously I have elided a bunch of stuff, but the above\n> rough sketch should be sufficient as introduction.)\n>\n> Bitcoin Consensus Layer As Hardware\n> -----------------------------------\n>\n> While Bitcoin fullnode implementations are software, because\n> of the need for consensus, this software is not actually very\n> \"soft\".\n> One can consider that, just as it would take a long time for\n> new hardware to be designed with a changed instruction set,\n> it is similarly taking a long time to change Bitcoin to\n> support changed feature sets.\n>\n> Thus, we should really consider the Bitcoin consensus layer,\n> and its SCRIPT, as hardware that other Bitcoin software and\n> layers run on top of.\n>\n> This thus opens up the thought of using techniques that were\n> useful in hardware design.\n> Such as microcode: a translation layer from \"old\" instruction\n> sets to \"new\" instruction sets, with the ability to modify this\n> mapping.\n>\n> Microcode For Bitcoin SCRIPT\n> ============================\n>\n> I propose:\n>\n> * Define a generic, low-level language (the \"RISC language\").\n> * Define a mapping from a specific, high-level language to\n>   the above language (the microcode).\n> * Allow users to sacrifice Bitcoins to define a new microcode.\n> * Have users indicate the microcode they wish to use to\n>   interpret their Tapscripts.\n>\n> As a concrete example, let us consider the current Bitcoin\n> SCRIPT as the \"CISC\" language.\n>\n> We can then support a \"RISC\" language that is composed of\n> general instructions, such as arithmetic, SECP256K1 scalar\n> and point math, bytevector concatenation, sha256 midstates,\n> bytevector bit manipulation, transaction introspection, and\n> so on.\n> This \"RISC\" language would also be stack-based.\n> As the \"RISC\" language would have more possible opcodes,\n> we may need to use 2-byte opcodes for the \"RISC\" language\n> instead of 1-byte opcodes.\n> Let us call this \"RISC\" language the micro-opcode language.\n>\n> Then, the \"microcode\" simply maps the existing Bitcoin\n> SCRIPT `OP_` codes to one or more `UOP_` micro-opcodes.\n>\n> An interesting fact is that stack-based languages have\n> automatic referential transparency; that is, if I define\n> some new word in a stack-based language and use that word,\n> I can replace verbatim the text of the new word in that\n> place without issue.\n> Compare this to a language like C, where macro authors\n> have to be very careful about inadvertent variable\n> capture, wrapping `do { ... } while(0)` to avoid problems\n> with `if` and multiple statements, multiple execution, and\n> so on.\n>\n> Thus, a sequence of `OP_` opcodes can be mapped to a\n> sequence of equivalent `UOP_` micro-opcodes without\n> changing the interpretation of the source language, an\n> important property when considering such a \"compiled\"\n> language.\n>\n> We start with a default microcode which is equivalent\n> to the current Bitcoin language.\n> When users want to define a new microcode to implement\n> new `OP_` codes or change existing `OP_` codes, they\n> can refer to a \"base\" microcode, and only have to\n> provide the new mappings.\n>\n> A microcode is fundamentally just a mapping from an\n> `OP_` code to a variable-length sequence of `UOP_`\n> micro-opcodes.\n>\n> ```Haskell\n> import Data.Map\n> -- type Opcode\n> -- type UOpcode\n> newtype Microcode = Microcode (Map.Map Opcode [UOpcode])\n> ```\n>\n> Semantically, the SCRIPT interpreter processes `UOP_`\n> micro-opcodes.\n>\n> ```Haskell\n> -- instance Monad Interpreter -- can `fail`.\n> interpreter :: Transaction -> TxInput -> [UOpcode] -> Interpreter ()\n> ```\n>\n> Example\n> -------\n>\n> Suppose a user wants to re-enable `OP_CAT`, and nothing\n> else.\n>\n> That user creates a microcode, referring to the current\n> default Bitcoin SCRIPT microcode as the \"base\".\n> The base microcode defines `OP_CAT` as equal to the\n> sequence `UOP_FAIL` i.e. a micro-opcode that always fails.\n> However, the new microcode will instead redefine the\n> `OP_CAT` as the micro-opcode sequence `UOP_CAT`.\n>\n> Microcodes then have a standard way of being represented\n> as a byte sequence.\n> The user serializes their new microcode as a byte\n> sequence.\n>\n> Then, the user creates a new transaction where one of\n> the outputs contains, say, 1.0 Bitcoins (exact required\n> value TBD), and has the `scriptPubKey` of\n> `OP_TRUE OP_RETURN <serialized_microcode>`.\n> This output is a \"microcode introduction output\", which\n> is provably unspendable, thus burning the Bitcoins.\n>\n> (It need not be a single user, multiple users can\n> coordinate by signing a single transaction that commits\n> their funds to the microcode introduction.)\n>\n> Once the above transaction has been deeply confirmed,\n> the user can then take the hash of the microcode\n> serialization.\n> Then the user can use a SCRIPT with `OP_CAT` enabled,\n> by using a Tapscript with, say, version `0xce`, and\n> with the SCRIPT having the microcode hash as its first\n> bytes, followed by the `OP_` codes.\n>\n> Fullnodes will then process recognized microcode\n> introduction outputs and store mappings from their\n> hashes to the microcodes in a new microcodes index.\n> Fullnodes can then process version-`0xce` Tapscripts\n> by checking if the microcodes index has the indicated\n> microcode hash.\n>\n> Semantically, fullnodes take the SCRIPT, and for each\n> `OP_` code in it, expands it to a sequence of `UOP_`\n> micro-opcodes, then concatenates each such sequence.\n> Then, the SCRIPT interpreter operates over a sequence\n> of `UOP_` micro-opcodes.\n>\n> Optimizing Microcodes\n> ---------------------\n>\n> Suppose there is some new microcode that users have\n> published onchain.\n>\n> We want to be able to execute the defined microcode\n> faster than expanding an `OP_`-code SCRIPT to a\n> `UOP_`-code SCRIPT and having an interpreter loop\n> over the `UOP_`-code SCRIPT.\n>\n> We can use LLVM.\n>\n> WARNING: LLVM might not be appropriate for\n> network-facing security-sensitive applications.\n> In particular, LLVM bugs. especially nondeterminism\n> bugs, can lead to consensus divergence and disastrous\n> chainsplits!\n> On the other hand, LLVM bugs are compiler bugs and\n> the same bugs can hit the static compiler `cc`, too,\n> since the same LLVM code runs in both JIT and static\n> compilation, so this risk already exists for Bitcoin.\n> (i.e. we already rely on LLVM not being buggy enough\n> to trigger Bitcoin consensus divergence, else we would\n> have written Bitcoin Core SCRIPT interpreter in\n> assembly.)\n>\n> Each `UOP_`-code has an equivalent tree of LLVM code.\n> For each `Opcode` in the microcode, we take its\n> sequence of `UOpcode`s and expand them to this tree,\n> concatenating the equivalent trees for each `UOpcode`\n> in the sequence.\n> Then we ask LLVM to JIT-compile this code to a new\n> function, running LLVM-provided optimizers.\n> Then we put a pointer to this compiled function to a\n> 256-long array of functions, where the array index is\n> the `OP_` code.\n>\n> The SCRIPT interpreter then simply iterates over the\n> `OP_` code SCRIPT and calls each of the JIT-compiled\n> functions.\n> This reduces much of the overhead of the `UOP_` layer\n> and makes it approach the current performance of the\n> existing `OP_` interpreter.\n>\n> For the default Bitcoin SCRIPT, the opcodes array\n> contains pointers to statically-compiled functions.\n> A microcode that is based on the default Bitcoin\n> SCRIPT copies this opcodes array, then overwrites\n> the entries.\n>\n> Future versions of Bitcoin Core can \"bless\"\n> particular microcodes by providing statically-compiled\n> functions for those microcodes.\n> This leads to even better performance (there is\n> no need to recompile ancient onchain microcodes each\n> time Bitcoin Core starts) without any consensus\n> divergence.\n> It is a pure optimization and does not imply a\n> tightening of rules, and is thus not a softfork.\n>\n> (To reduce the chance of network faults being used\n> to poke into `W|X` memory (since `W|X` memory is\n> needed in order to actually JIT compile) we can\n> isolate the SCRIPT interpreter into its own process\n> separate from the network-facing code.\n> This does imply additional overhead in serializing\n> transactions we want to ask the SCRIPT interpreter\n> to validate.)\n>\n> Comparison To Jets\n> ------------------\n>\n> This technique allows users to define \"jets\", i.e.\n> sequences of low-level general operations that users\n> have determined are common enough they should just\n> be implemented as faster code that is executed\n> directly by the underlying hardware processor rather\n> than via a software interpreter.\n> Basically, each redefined `OP_` code is a jet of a\n> sequence of `UOP_` micro-opcodes.\n>\n> We implement this by dynamically JIT-compiling the\n> proposed jets, as described above.\n> SCRIPTs using jetted code remain smaller, as the\n> jet definition is done in a previous transaction and\n> does not require copy-pasta (Do Not Repeat Yourself!).\n> At the same time, jettification is not tied to\n> developers, thus removing the need to keep softforking\n> new features --- we only need define a sufficiently\n> general language and then we can implement pretty much\n> anything worth implementing (and a bunch of other things\n> that should not be implemented, but hey, users gonna\n> use...).\n>\n> Bugs in existing microcodes can be fixed by basing a\n> new microcode from the existing microcode, and\n> redefining the buggy implementation.\n> Existing Tapscripts need to be re-spent to point to\n> the new bugfixed microcode, but if you used the\n> point-spend branch as an N-of-N of all participants\n> you have an upgrade mechanism for free.\n>\n> In order to ensure that the JIT-compilation of new\n> microcodes is not triggered trivially, we require\n> that users petitioning for the jettification of some\n> operations (i.e. introducing a new microcode) must\n> sacrifice Bitcoins.\n>\n> Burning Bitcoins is better than increasing the weight\n> of microcode introduction outputs; all fullnodes are\n> affected by the need to JIT-compile the new microcode,\n> so they benefit from the reduction in supply, thus\n> getting compensated for the work of JIT-compiling the\n> new microcode.\n> Ohter mechanisms for making microcode introduction\n> outputs expensive are also possible.\n>\n> Nothing really requires that we use a stack-based\n> language for this; any sufficiently FP language\n> should allow referential transparency.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220322/402ad6eb/attachment-0001.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-22T16:22:55",
                "message_text_only": "Good morning Russell,\n\n> Setting aside my thoughts that something like Simplicity would make a better platform than Bitcoin Script (due to expression operating on a more narrow interface than the entire stack (I'm looking at you OP_DEPTH)) there is an issue with namespace management.\n>\n> If I understand correctly, your implication was that once opcodes are redefined by an OP_RETURN transaction, subsequent transactions of that opcode refer to the new microtransaction.\u00a0 But then we have a race condition between people submitting transactions expecting the outputs to refer to the old code and having their code redefined by the time they do get confirmed\u00a0 (or worse having them reorged).\n\nNo, use of specific microcodes is opt-in: you have to use a specific `0xce` Tapscript version, ***and*** refer to the microcode you want to use via the hash of the microcode.\n\nThe only race condition is reorging out a newly-defined microcode.\nThis can be avoided by waiting for deep confirmation of a newly-defined microcode before actually using it.\n\nBut once the microcode introduction outpoint of a particular microcode has been deeply confirmed, then your Tapscript can refer to the microcode, and its meaning does not change.\n\nFullnodes may need to maintain multiple microcodes, which is why creating new microcodes is expensive; they not only require JIT compilation, they also require that fullnodes keep an index that cannot have items deleted.\n\n\nThe advantage of the microcode scheme is that the size of the SCRIPT can be used as a proxy for CPU load ---- just as it is done for current Bitcoin SCRIPT.\nAs long as the number of `UOP_` micro-opcodes that an `OP_` code can expand to is bounded, and we avoid looping constructs, then the CPU load is also bounded and the size of the SCRIPT approximates the amount of processing needed, thus microcode does not require a softfork to modify weight calculations in the future.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Russell O'Connor",
                "date": "2022-03-22T16:28:21",
                "message_text_only": "Thanks for the clarification.\n\nYou don't think referring to the microcode via its hash, effectively using\n32-byte encoding of opcodes, is still rather long winded?\n\nOn Tue, Mar 22, 2022 at 12:23 PM ZmnSCPxj <ZmnSCPxj at protonmail.com> wrote:\n\n> Good morning Russell,\n>\n> > Setting aside my thoughts that something like Simplicity would make a\n> better platform than Bitcoin Script (due to expression operating on a more\n> narrow interface than the entire stack (I'm looking at you OP_DEPTH)) there\n> is an issue with namespace management.\n> >\n> > If I understand correctly, your implication was that once opcodes are\n> redefined by an OP_RETURN transaction, subsequent transactions of that\n> opcode refer to the new microtransaction.  But then we have a race\n> condition between people submitting transactions expecting the outputs to\n> refer to the old code and having their code redefined by the time they do\n> get confirmed  (or worse having them reorged).\n>\n> No, use of specific microcodes is opt-in: you have to use a specific\n> `0xce` Tapscript version, ***and*** refer to the microcode you want to use\n> via the hash of the microcode.\n>\n> The only race condition is reorging out a newly-defined microcode.\n> This can be avoided by waiting for deep confirmation of a newly-defined\n> microcode before actually using it.\n>\n> But once the microcode introduction outpoint of a particular microcode has\n> been deeply confirmed, then your Tapscript can refer to the microcode, and\n> its meaning does not change.\n>\n> Fullnodes may need to maintain multiple microcodes, which is why creating\n> new microcodes is expensive; they not only require JIT compilation, they\n> also require that fullnodes keep an index that cannot have items deleted.\n>\n>\n> The advantage of the microcode scheme is that the size of the SCRIPT can\n> be used as a proxy for CPU load ---- just as it is done for current Bitcoin\n> SCRIPT.\n> As long as the number of `UOP_` micro-opcodes that an `OP_` code can\n> expand to is bounded, and we avoid looping constructs, then the CPU load is\n> also bounded and the size of the SCRIPT approximates the amount of\n> processing needed, thus microcode does not require a softfork to modify\n> weight calculations in the future.\n>\n> Regards,\n> ZmnSCPxj\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220322/4fd9e0be/attachment.html>"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-22T16:39:10",
                "message_text_only": "Good morning Russell,\n\n> Thanks for the clarification.\n>\n> You don't think referring to the microcode via its hash, effectively using 32-byte encoding of opcodes, is still rather long winded?\n\nA microcode is a *mapping* of `OP_` codes to a variable-length sequence of `UOP_` micro-opcodes.\nSo a microcode hash refers to an entire language of redefined `OP_` codes, not each individual opcode in the language.\n\nIf it costs 1 Bitcoin to create a new microcode, then there are only 21 million possible microcodes, and I think about 50 bits of hash is sufficient to specify those with low probability of collision.\nWe could use a 20-byte RIPEMD . SHA256 instead for 160 bits, that should be more than sufficient with enough margin.\nThough perhaps it is now easier to deliberately attack...\n\nAlso, if you have a common SCRIPT whose non-`OP_PUSH` opcodes are more than say 32 + 1 bytes (or 20 + 1 if using RIPEMD), and you can fit their equivalent `UOP_` codes into the max limit for a *single* opcode, you can save bytes by redefining some random `OP_` code into the sequence of all the `UOP_` codes.\nYou would have a hash reference to the microcode, and a single byte for the actual \"SCRIPT\" which is just a jet of the entire SCRIPT.\nUsers of multiple *different* such SCRIPTs can band together to define a single microcode, mapping their SCRIPTs to different `OP_` codes and sharing the cost of defining the new microcode that shortens all their SCRIPTs.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-22T16:47:33",
                "message_text_only": "Good morning again Russell,\n\n> Good morning Russell,\n>\n> > Thanks for the clarification.\n> > You don't think referring to the microcode via its hash, effectively using 32-byte encoding of opcodes, is still rather long winded?\n\nFor that matter, since an entire microcode represents a language (based on the current OG Bitcoin SCRIPT language), with a little more coordination, we could entirely replace Tapscript versions --- every Tapscript version is a slot for a microcode, and the current OG Bitcoin SCRIPT is just the one in slot `0xc2`.\nFilled slots cannot be changed, but new microcodes can use some currently-empty Tapscript version slot, and have it properly defined in a microcode introduction outpoint.\n\nThen indication of a microcode would take only one byte, that is already needed currently anyway.\n\nThat does limit us to only 255 new microcodes, thus the cost of one microcode would have to be a good bit higher.\n\nAgain, remember, microcodes represent an entire language that is an extension of OG Bitcoin SCRIPT, not individual operations in that language.\n\nRegards,\nZmnSCPxj"
            },
            {
                "author": "Anthony Towns",
                "date": "2022-03-22T23:11:05",
                "message_text_only": "On Tue, Mar 22, 2022 at 05:37:03AM +0000, ZmnSCPxj via bitcoin-dev wrote:\n> Subject: Beyond Jets: Microcode: Consensus-Critical Jets Without Softforks\n\n(Have you considered applying a jit or some other compression algorithm\nto your emails?)\n\n> Microcode For Bitcoin SCRIPT\n> ============================\n> I propose:\n> * Define a generic, low-level language (the \"RISC language\").\n\nThis is pretty much what Simplicity does, if you optimise the low-level\nlanguage to minimise the number of primitives and maximise the ability\nto apply tooling to reason about it, which seem like good things for a\nRISC language to optimise.\n\n> * Define a mapping from a specific, high-level language to\n>   the above language (the microcode).\n> * Allow users to sacrifice Bitcoins to define a new microcode.\n\nI think you're defining \"the microcode\" as the \"mapping\" here.\n\nThis is pretty similar to the suggestion Bram Cohen was making a couple\nof months ago:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019722.html\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019773.html\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019803.html\n\nI believe this is done in chia via the block being able to\ninclude-by-reference prior blocks' transaction generators:\n\n] transactions_generator_ref_list: List[uint32]: A list of block heights of previous generators referenced by this block's generator.\n  - https://docs.chia.net/docs/05block-validation/block_format\n\n(That approach comes at the cost of not being able to do full validation\nif you're running a pruning node. The alternative is to effectively\nintroduce a parallel \"utxo\" set -- where you're mapping the \"sacrificed\"\nBTC as the nValue and instead of just mapping it to a scriptPubKey for\na later spend, you're permanently storing the definition of the new\nCISC opcode)\n\n> We can then support a \"RISC\" language that is composed of\n> general instructions, such as arithmetic, SECP256K1 scalar\n> and point math, bytevector concatenation, sha256 midstates,\n> bytevector bit manipulation, transaction introspection, and\n> so on.\n\nA language that includes instructions for each operation we can think\nof isn't very \"RISC\"... More importantly it gets straight back to the\n\"we've got a new zk system / ECC curve / ... that we want to include,\nlet's do a softfork\" problem you were trying to avoid in the first place.\n\n> Then, the user creates a new transaction where one of\n> the outputs contains, say, 1.0 Bitcoins (exact required\n> value TBD),\n\nLikely, the \"fair\" price would be the cost of introducing however many\nadditional bytes to the utxo set that it would take to represent your\nmicrocode, and the cost it would take to run jit(your microcode script)\nif that were a validation function. Both seem pretty hard to manage.\n\n\"Ideally\", I think you'd want to be able to say \"this old microcode\nno longer has any value, let's forget it, and instead replace it with\nthis new microcode that is much better\" -- that way nodes don't have to\nkeep around old useless data, and you've reduced the cost of introducing\nnew functionality.\n\nAdditionally, I think it has something of a tragedy-of-the-commons\nproblem: whoever creates the microcode pays the cost, but then anyone\ncan use it and gain the benefit. That might even end up creating\ncentralisation pressure: if you design a highly decentralised L2 system,\nit ends up expensive because people can't coordinate to pay for the\nnew microcode that would make it cheaper; but if you design a highly\ncentralised L2 system, you can just pay for the microcode yourself and\nmake it even cheaper.\n\nThis approach isn't very composable -- if there's a clever opcode\ndefined in one microcode spec, and another one in some other microcode,\nthe only way to use both of them in the same transaction is to burn 1\nBTC to define a new microcode that includes both of them.\n\n> We want to be able to execute the defined microcode\n> faster than expanding an `OP_`-code SCRIPT to a\n> `UOP_`-code SCRIPT and having an interpreter loop\n> over the `UOP_`-code SCRIPT.\n>\n> We can use LLVM.\n\nWe've not long ago gone to the effort of removing openssl as a consensus\ncritical dependency; and likewise previously removed bdb.  Introducing a\nhuge new dependency to the definition of consensus seems like an enormous\nstep backwards.\n\nThis would also mean we'd be stuck at the performance of whatever version\nof llvm we initially adopted, as any performance improvements introduced\nin later llvm versions would be a hard fork.\n\n> On the other hand, LLVM bugs are compiler bugs and\n> the same bugs can hit the static compiler `cc`, too,\n\n\"Well, you could hit Achilles in the heel, so really, what's the point\nof trying to be invulnerable anywhere else?\"\n\n> Then we put a pointer to this compiled function to a\n> 256-long array of functions, where the array index is\n> the `OP_` code.\n\nThat's a 256-long array of functions for each microcode, which increases\nthe \"microcode-utxo\" database storage size substantially.\n\nPresuming there are different jit targets (x86 vs arm?) it seems\ndifficulty to come up with a consistent interpretation of the cost for\nthese opcodes.\n\nI'm skeptical that a jit would be sufficient for increasing the\nperformance of an implementation just based on basic arithmetic opcodes\nif we're talking about something like sha512 or bls12-381 or similar.\n\n> Bugs in existing microcodes can be fixed by basing a\n> new microcode from the existing microcode, and\n> redefining the buggy implementation.\n> Existing Tapscripts need to be re-spent to point to\n> the new bugfixed microcode, but if you used the\n> point-spend branch as an N-of-N of all participants\n> you have an upgrade mechanism for free.\n\nIt's not free if you have to do an on-chain spend... \n\nThe \"1 BTC\" cost to fix the bug, and the extra storage in every node's\n\"utxo\" set because they now have to keep both the buggy and fixed versions\naround permanently sure isn't free either. If you're re-jitting every\nmicrocode on startup, that could get pretty painful too.\n\nIf you're proposing introducing byte vector manipulation and OP_CAT and\nsimilar, which enables recursive covenants, then it might be good to\nexplain how this proposal addresses the concerns raised at the end of\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020092.html\n\nCheers,\naj"
            },
            {
                "author": "ZmnSCPxj",
                "date": "2022-03-23T00:20:16",
                "message_text_only": "Good morning aj,\n\n> On Tue, Mar 22, 2022 at 05:37:03AM +0000, ZmnSCPxj via bitcoin-dev wrote:\n>\n> > Subject: Beyond Jets: Microcode: Consensus-Critical Jets Without Softforks\n>\n> (Have you considered applying a jit or some other compression algorithm\n> to your emails?)\n>\n> > Microcode For Bitcoin SCRIPT\n> >\n> > =============================\n> >\n> > I propose:\n> >\n> > -   Define a generic, low-level language (the \"RISC language\").\n>\n> This is pretty much what Simplicity does, if you optimise the low-level\n> language to minimise the number of primitives and maximise the ability\n> to apply tooling to reason about it, which seem like good things for a\n> RISC language to optimise.\n>\n> > -   Define a mapping from a specific, high-level language to\n> >     the above language (the microcode).\n> >\n> > -   Allow users to sacrifice Bitcoins to define a new microcode.\n>\n> I think you're defining \"the microcode\" as the \"mapping\" here.\n\nYes.\n\n>\n> This is pretty similar to the suggestion Bram Cohen was making a couple\n> of months ago:\n>\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2021-December/019722.html\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019773.html\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-January/019803.html\n>\n> I believe this is done in chia via the block being able to\n> include-by-reference prior blocks' transaction generators:\n>\n> ] transactions_generator_ref_list: List[uint32]: A list of block heights of previous generators referenced by this block's generator.\n>\n> -   https://docs.chia.net/docs/05block-validation/block_format\n>\n>     (That approach comes at the cost of not being able to do full validation\n>     if you're running a pruning node. The alternative is to effectively\n>     introduce a parallel \"utxo\" set -- where you're mapping the \"sacrificed\"\n>     BTC as the nValue and instead of just mapping it to a scriptPubKey for\n>     a later spend, you're permanently storing the definition of the new\n>     CISC opcode)\n>\n>\n\nYes, the latter is basically what microcode is.\n\n> > We can then support a \"RISC\" language that is composed of\n> > general instructions, such as arithmetic, SECP256K1 scalar\n> > and point math, bytevector concatenation, sha256 midstates,\n> > bytevector bit manipulation, transaction introspection, and\n> > so on.\n>\n> A language that includes instructions for each operation we can think\n> of isn't very \"RISC\"... More importantly it gets straight back to the\n> \"we've got a new zk system / ECC curve / ... that we want to include,\n> let's do a softfork\" problem you were trying to avoid in the first place.\n\n`libsecp256k1` can run on purely RISC machines like ARM, so saying that a \"RISC\" set of opcodes cannot implement some arbitrary ECC curve, when the instruction set does not directly support that ECC curve, seems incorrect.\n\nAny new zk system / ECC curve would have to be implementable in C++, so if you have micro-operations that would be needed for it, such as XORing two multi-byte vectors together, multiplying multi-byte precision numbers, etc., then any new zk system or ECC curve would be implementable in microcode.\nFor that matter, you could re-write `libsecp256k1` there.\n\n> > Then, the user creates a new transaction where one of\n> > the outputs contains, say, 1.0 Bitcoins (exact required\n> > value TBD),\n>\n> Likely, the \"fair\" price would be the cost of introducing however many\n> additional bytes to the utxo set that it would take to represent your\n> microcode, and the cost it would take to run jit(your microcode script)\n> if that were a validation function. Both seem pretty hard to manage.\n>\n> \"Ideally\", I think you'd want to be able to say \"this old microcode\n> no longer has any value, let's forget it, and instead replace it with\n> this new microcode that is much better\" -- that way nodes don't have to\n> keep around old useless data, and you've reduced the cost of introducing\n> new functionality.\n\nYes, but that invites \"I accidentally the smart contract\" behavior.\n\n> Additionally, I think it has something of a tragedy-of-the-commons\n> problem: whoever creates the microcode pays the cost, but then anyone\n> can use it and gain the benefit. That might even end up creating\n> centralisation pressure: if you design a highly decentralised L2 system,\n> it ends up expensive because people can't coordinate to pay for the\n> new microcode that would make it cheaper; but if you design a highly\n> centralised L2 system, you can just pay for the microcode yourself and\n> make it even cheaper.\n\nThe same \"tragedy of the commons\" applies to FOSS.\n\"whoever creates the FOSS pays the cost, but then anyone can use it and gain the benefit\"\nThis seems like an argument against releasing a FOSS node software.\n\nRemember, microcode is software too, and copying software does not have a tragedy of the commons --- the main point of a tragedy of the commons is that the commons is *degraded* by the use but nobody has incentive to maintain against the degradation.\nBut using software does not degrade the software, if I give you a copy of my software then I do not lose my software, which is why FOSS works.\n\nIn order to make a highly-decentralized L2, you need to cooperate with total strangers, possibly completely anonymously, in handling your money.\nI imagine that the level of cooperation needed in, say, Lightning network, would be far above what is necessary to gather funds from multiple people who want a particular microcode to happen until enough funds have been gathered to make the microcode happen.\n\nFor example, create a fresh address for an amount you, personally, are willing to contribute in order to make the microcode happen.\n(If you are willing to spend the time and energy arguing on bitcoin-dev, then you are willing to contribute, even if others get the benefit in addition to yourself, and that time and energy has a corresponding Bitcoin value)\nThen spend it using a `SIGHASH_ANYONECANPAY | SIGHASH_SINGLE`, with the microcode introduction outpoint as the single output you are signing.\nGather enough such signatures from a community around a decentralized L2, and you can achieve the necessary total funds for the microcode to happen.\n\n\n> This approach isn't very composable -- if there's a clever opcode\n> defined in one microcode spec, and another one in some other microcode,\n> the only way to use both of them in the same transaction is to burn 1\n> BTC to define a new microcode that includes both of them.\n\nYes, that is indeed a problem.\n\n> > We want to be able to execute the defined microcode\n> > faster than expanding an `OP_`-code SCRIPT to a\n> > `UOP_`-code SCRIPT and having an interpreter loop\n> > over the `UOP_`-code SCRIPT.\n> > We can use LLVM.\n>\n> We've not long ago gone to the effort of removing openssl as a consensus\n> critical dependency; and likewise previously removed bdb. Introducing a\n> huge new dependency to the definition of consensus seems like an enormous\n> step backwards.\n>\n> This would also mean we'd be stuck at the performance of whatever version\n> of llvm we initially adopted, as any performance improvements introduced\n> in later llvm versions would be a hard fork.\n\nYes, LLVM is indeed the weak link in this idea.\nWe could use NaCl instead, that has probably fewer issues /s.\n\n> > On the other hand, LLVM bugs are compiler bugs and\n> > the same bugs can hit the static compiler `cc`, too,\n>\n> \"Well, you could hit Achilles in the heel, so really, what's the point\n> of trying to be invulnerable anywhere else?\"\n\nYes, LLVM is indeed the weak point here.\n\nWe could just concatenate some C++ code together when a new microcode is introduced, and compile it statically, then store the resulting binary somewhere, and invoke it at the appropriate time to run validation.\nAt least LLVM would be isolated into its own process in that case.\n\n> > Then we put a pointer to this compiled function to a\n> > 256-long array of functions, where the array index is\n> > the `OP_` code.\n>\n> That's a 256-long array of functions for each microcode, which increases\n> the \"microcode-utxo\" database storage size substantially.\n>\n> Presuming there are different jit targets (x86 vs arm?) it seems\n> difficulty to come up with a consistent interpretation of the cost for\n> these opcodes.\n>\n> I'm skeptical that a jit would be sufficient for increasing the\n> performance of an implementation just based on basic arithmetic opcodes\n> if we're talking about something like sha512 or bls12-381 or similar.\n\nStatic compilation seems to work well enough --- and JIT vs static is a spectrum, not either/or.\nThe difference is really how much optimization you are willing to use.\nIf microcodes are costly enough that they happen rarely, then using optimizations that are often used only in static compilation, seems a reasonable tradeoff\n\n> > Bugs in existing microcodes can be fixed by basing a\n> > new microcode from the existing microcode, and\n> > redefining the buggy implementation.\n> > Existing Tapscripts need to be re-spent to point to\n> > the new bugfixed microcode, but if you used the\n> > point-spend branch as an N-of-N of all participants\n> > you have an upgrade mechanism for free.\n>\n> It's not free if you have to do an on-chain spend...\n>\n> The \"1 BTC\" cost to fix the bug, and the extra storage in every node's\n> \"utxo\" set because they now have to keep both the buggy and fixed versions\n> around permanently sure isn't free either.\n\nHeh, poor word choice.\n\nWhat I meant is that we do not need a separate upgrade mechanism, the design work here is \"free\".\n*Using* the upgrade mechanism is costly and hence not \"free\".\n\n> If you're re-jitting every\n> microcode on startup, that could get pretty painful too.\n\nWhen LLVM is used in a static compiler, it writes the resulting code on-disk, I imagine the same mechanism can be used.\n\n> If you're proposing introducing byte vector manipulation and OP_CAT and\n> similar, which enables recursive covenants, then it might be good to\n> explain how this proposal addresses the concerns raised at the end of\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020092.html\n\nIt does not, I am currently exploring and generating ideas, not particularly tying myself to one idea or another.\n\nRegards,\nZmnSCPxj"
            }
        ],
        "thread_summary": {
            "title": "Beyond Jets: Microcode: Consensus-Critical Jets Without Softforks",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "ZmnSCPxj",
                "Anthony Towns",
                "Russell O'Connor"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 58361
        }
    },
    {
        "title": "[bitcoin-dev] mempool transaction witness-replacement",
        "thread_messages": [
            {
                "author": "Larry Ruane",
                "date": "2022-03-22T19:04:26",
                "message_text_only": "Greetings list,\n\nThis is my first time posting here.\n\nQuestion for you:\n\nShould the Bitcoin Core mempool replace an existing transaction with one\nthat has the same txid (having the same effect, same spends and outputs)\nbut a sufficiently smaller witness (different wtxid) and thus a higher\nfeerate? This is what https://github.com/bitcoin/bitcoin/pull/24007\nproposes, and I'd like to get opinions on two questions:\n\n1. Is this a beneficial change? Specifically, is anyone creating an\napplication that would broadcast transactions with the same txid but\ndifferent witnesses as an earlier transaction?\n\n2. If this change has benefit, what should be considered a sufficiently\nbetter feerate or reduction in witness size?\n\nAn advantage of this mempool-accept policy change is that it's\nminer-incentive compatible (miners would prefer to mine a transaction\nwith a higher feerate). But there is of course a code complexity cost,\nand transaction-relay DoS concern.\n\nBeing miner-incentive compatible is good, but is that sufficient\njustification for merging? I'm posting to the mailing list in hopes that\nthere are use-cases that we (the PR authors) aren't aware of. Please\nreply here or on the PR if you can think of any.\n\nA perhaps subtle advantage: This PR may provide a defense against a\nmempool pinning attack: if you have a transaction shared with other\nparties, and one of them broadcasts the transaction with a bloated\nwitness (thus intentionally reducing the feerate in hopes of delaying\nor preventing confirmation), you currently have no way to change it.\nIf there is an application out there that uses same-txid-different-witness\ntransactions shared between counterparties, this PR would help make\nthose applications safe.\n\nQuestion 2 gets at a DoS tradeoff: If the new transaction may have\nonly a very slightly smaller witness, an attacker might re-broadcast it\nmany times, consuming a lot of relay bandwidth, and CPU to update\nthe mempool. On the other hand, if the new transaction must have a much\nsmaller witness, then it wouldn't be possible to replace a transaction with\na beneficially-smaller one.\n\nThis could be a per-node setting, but it's desirable for the node\nnetwork to largely agree on relay policies (although a configuration\noption might be useful for testing and experimentation).\n\nBackground:\n\nBip125 (Replace-by-fee) allows an incoming transaction to replace one\nor more existing conflicting transactions if certain DoS-mitigation\nconditions are met:\n\nhttps://github.com/bitcoin/bitcoin/blob/master/doc/policy/mempool-replacements.md\n\nWitness-replacement is similar to RBF, but differs in a few ways:\n\n- RBF rule 4 requires an absolute fee increase, which is not possible if\nthe txid isn't changing (since the inputs, outputs, and amounts must be\nthe same). So if transaction witness-replacement (same txid but different\nwtxid) is allowed, it can't be considered just a special case of an RBF,\nalthough it may have some similar policies (and for the same reasons).\n\n- With witness-replacement, it's not necessary to evict mempool\ndescendant transactions because their inputs' txid references to their\nparent (who is being replaced) remain valid.\n\n- The new transaction replaces exactly one existing transaction since\nthe inputs are the same. (In general, with RBF, the new transaction may\nconflict-out multiple existing mempool transactions, namely, all that\nspend the same outputs as the new transaction.)\n\n- RBF requires the original transaction to signal replaceability\n(rule 1). This is so that recipients are warned that their payment may\ndisappear if the transaction is replaced. But signaling isn't required\nby witness-replacement since the outputs can't change (the descendants\nremain valid).\n\nThanks for your time!\n\nLarry Ruane (with lots of help from Gloria Zhao)"
            },
            {
                "author": "darosior",
                "date": "2022-03-22T19:57:23",
                "message_text_only": "Hi Larry,\n\n\nThanks for bringing this up. I'm curious to know if this is helpful for pinning as long as you have a way to\nstatically analyze Script to prevent witness stuffing [0]. I agree it *could* still be useful for miners, but\nsubject to all the complications of RBF.\n\n> An advantage of this mempool-accept policy change is that it's\n> miner-incentive compatible (miners would prefer to mine a transaction\n> with a higher feerate).\n\nThere is more to be \"miner-incentive compatible\" than increasing feerate. For instance, the latest RBF\ndiscussions made the miner incentive to maximize absolute fees more well known. I think the same goes for\nwitness replacement: if you don't have as many MBs of transaction you are comfortable with in your mempool,\nyou don't want it to shrink further.\n\n\nAntoine\n\n[0] See the 'Malleability' section of https://bitcoin.sipa.be/miniscript/. Note however this currently only\n    applies to third party malleability (in pinning attacks the aversary is internal to the contract). On the\n    other hand Miniscript already allows you to get the maximum satisfaction size, so you can cover for the worst\n    case scenario already.\n\n------- Original Message -------\n\nLe mardi 22 mars 2022 \u00e0 8:04 PM, Larry Ruane via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> a \u00e9crit :\n\n> Greetings list,\n>\n> This is my first time posting here.\n>\n> Question for you:\n>\n> Should the Bitcoin Core mempool replace an existing transaction with one\n>\n> that has the same txid (having the same effect, same spends and outputs)\n>\n> but a sufficiently smaller witness (different wtxid) and thus a higher\n>\n> feerate? This is what https://github.com/bitcoin/bitcoin/pull/24007\n>\n> proposes, and I'd like to get opinions on two questions:\n>\n> 1. Is this a beneficial change? Specifically, is anyone creating an\n>\n> application that would broadcast transactions with the same txid but\n>\n> different witnesses as an earlier transaction?\n>\n> 2. If this change has benefit, what should be considered a sufficiently\n>\n> better feerate or reduction in witness size?\n>\n> An advantage of this mempool-accept policy change is that it's\n>\n> miner-incentive compatible (miners would prefer to mine a transaction\n>\n> with a higher feerate). But there is of course a code complexity cost,\n>\n> and transaction-relay DoS concern.\n>\n> Being miner-incentive compatible is good, but is that sufficient\n>\n> justification for merging? I'm posting to the mailing list in hopes that\n>\n> there are use-cases that we (the PR authors) aren't aware of. Please\n>\n> reply here or on the PR if you can think of any.\n>\n> A perhaps subtle advantage: This PR may provide a defense against a\n>\n> mempool pinning attack: if you have a transaction shared with other\n>\n> parties, and one of them broadcasts the transaction with a bloated\n>\n> witness (thus intentionally reducing the feerate in hopes of delaying\n>\n> or preventing confirmation), you currently have no way to change it.\n>\n> If there is an application out there that uses same-txid-different-witness\n>\n> transactions shared between counterparties, this PR would help make\n>\n> those applications safe.\n>\n> Question 2 gets at a DoS tradeoff: If the new transaction may have\n>\n> only a very slightly smaller witness, an attacker might re-broadcast it\n>\n> many times, consuming a lot of relay bandwidth, and CPU to update\n>\n> the mempool. On the other hand, if the new transaction must have a much\n>\n> smaller witness, then it wouldn't be possible to replace a transaction with\n>\n> a beneficially-smaller one.\n>\n> This could be a per-node setting, but it's desirable for the node\n>\n> network to largely agree on relay policies (although a configuration\n>\n> option might be useful for testing and experimentation).\n>\n> Background:\n>\n> Bip125 (Replace-by-fee) allows an incoming transaction to replace one\n>\n> or more existing conflicting transactions if certain DoS-mitigation\n>\n> conditions are met:\n>\n> https://github.com/bitcoin/bitcoin/blob/master/doc/policy/mempool-replacements.md\n>\n> Witness-replacement is similar to RBF, but differs in a few ways:\n>\n> - RBF rule 4 requires an absolute fee increase, which is not possible if\n>\n> the txid isn't changing (since the inputs, outputs, and amounts must be\n>\n> the same). So if transaction witness-replacement (same txid but different\n>\n> wtxid) is allowed, it can't be considered just a special case of an RBF,\n>\n> although it may have some similar policies (and for the same reasons).\n>\n> - With witness-replacement, it's not necessary to evict mempool\n>\n> descendant transactions because their inputs' txid references to their\n>\n> parent (who is being replaced) remain valid.\n>\n> - The new transaction replaces exactly one existing transaction since\n>\n> the inputs are the same. (In general, with RBF, the new transaction may\n>\n> conflict-out multiple existing mempool transactions, namely, all that\n>\n> spend the same outputs as the new transaction.)\n>\n> - RBF requires the original transaction to signal replaceability\n>\n> (rule 1). This is so that recipients are warned that their payment may\n>\n> disappear if the transaction is replaced. But signaling isn't required\n>\n> by witness-replacement since the outputs can't change (the descendants\n>\n> remain valid).\n>\n> Thanks for your time!\n>\n> Larry Ruane (with lots of help from Gloria Zhao)\n>\n> _______________________________________________\n>\n> bitcoin-dev mailing list\n>\n> bitcoin-dev at lists.linuxfoundation.org\n>\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "mempool transaction witness-replacement",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "darosior",
                "Larry Ruane"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 9324
        }
    },
    {
        "title": "[bitcoin-dev] CTV BIP Meeting #6 Notes on Sapio Studio Tutorial",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-22T21:33:34",
                "message_text_only": "Devs,\n\nTutorial: https://rubin.io/bitcoin/2022/03/22/sapio-studio-btc-dev-mtg-6/\nMeeting Logs:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2022-March/020157.html\n\nSummary:\n\nThe 6th CTV meeting was a Sapio Studio tutorial. Sapio Studio is a Bitcoin\nWallet / IDE for playing with Bitcoin Smart Contracts. It is clearly \"Alpha\nSoftware\", but gets better and better!\n\nThe tutorial primarily covers setting up Sapio Studio and then using it to\ncreate an instance of a Bitcoin Vault similar to the variety James O'Beirne\nshared recently on this list.\n\nParticipants had trouble with:\n\n1) Build System Stuff\n2) Passing in Valid Arguments\n3) Minrelay Fees\n4) Minor GUI bugs in the software\n\nBut overall, the software was able to be used successfully similar to the\nscreenshots in the tutorial, including restarting and resuming a session,\nrecompiling with effect updates (essentially a form of multisig enforced\nrecursive covenant which can be made compatible with arbitrary covenant\nupgrades), and more.\n\nBased on the meeting, there are some clear areas of improvement needed to\nmake this GUI more intuitive that will be incorporated in the coming weeks.\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220322/6a8ea5c1/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "CTV BIP Meeting #6 Notes on Sapio Studio Tutorial",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1407
        }
    },
    {
        "title": "[bitcoin-dev] Pleb.fi/miami2022 Invitation + CTV Meeting #7 postponement",
        "thread_messages": [
            {
                "author": "Jeremy Rubin",
                "date": "2022-03-22T21:53:32",
                "message_text_only": "Devs,\n\nI warmly invite you to join for pleb.fi/miami2022 if you are interested to\nparticipate. It will be April 4th and 5th near miami.\n\nThe focus of this pleb.fi event will be the ins and outs of building\nbitcoin stuff in rust with a focus on Sapio and a hackathon.\n\nAs the CTV Meeting overlaps with the programming for pleb.fi, regrettably I\nwill be unable to host it.\n\nWe'll resume with meeting #7 at the time meeting #8 would be otherwise.\n\nBest,\n\nJeremy\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220322/8dcaaec9/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Pleb.fi/miami2022 Invitation + CTV Meeting #7 postponement",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy Rubin"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 692
        }
    },
    {
        "title": "[bitcoin-dev] Silent Payments \u2013 Non-interactive private payments with no on-chain overhead",
        "thread_messages": [
            {
                "author": "Ruben Somsen",
                "date": "2022-03-28T15:27:56",
                "message_text_only": "Hi all,\n\nI'm publishing a new scheme for private non-interactive address generation\nwithout on-chain overhead. It has upsides as well as downsides, so I\nsuspect the main discussion will revolve around whether this is worth\npursuing or not. There is a list of open questions at the end.\n\nI added the full write-up in plain text below, though I recommend reading\nthe gist for improved formatting and in order to benefit from potential\nfuture edits:\nhttps://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8\n\nCheers,\nRuben\n\n\n\nSilent Payments\n\nReceive private payments from anyone on a single static address without\nrequiring any interaction or on-chain overhead\n\n\n\nOVERVIEW\n\n\nThe recipient generates a so-called silent payment address and makes it\npublicly known. The sender then takes a public key from one of their chosen\ninputs for the payment, and uses it to derive a shared secret that is then\nused to tweak the silent payment address. The recipient detects the payment\nby scanning every transaction in the blockchain.\n\nCompared to previous schemes[1], this scheme avoids using the Bitcoin\nblockchain as a messaging layer[2] and requires no interaction between\nsender and recipient[3] (other than needing to know the silent payment\naddress). The main downsides are the scanning requirement, the lack of\nlight client support, and the requirement to control your own input(s). An\nexample use case would be private one-time donations.\n\nWhile most of the individual parts of this idea aren\u2019t novel, the resulting\nprotocol has never been seriously considered and may be reasonably viable,\nparticularly if we limit ourselves to detecting only unspent payments by\nscanning the UTXO set. We\u2019ll start by describing a basic scheme, and then\nintroduce a few improvements.\n\n\n\nBASIC SCHEME\n\n\nThe recipient publishes their silent payment address, a single 32 byte\npublic key:\nX = x*G\n\nThe sender picks an input containing a public key:\nI = i*G\n\nThe sender tweaks the silent payment address with the public key of their\ninput:\nX' = hash(i*X)*G + X\n\nSince i*X == x*I (Diffie-Hellman Key Exchange), the recipient can detect\nthe payment by calculating hash(x*I)*G + X for each input key I in the\nblockchain and seeing if it matches an output in the corresponding\ntransaction.\n\n\n\nIMPROVEMENTS\n\n\nUTXO set scanning\n\nIf we forgo detection of historic transactions and only focus on the\ncurrent balance, we can limit the protocol to only scanning the\ntransactions that are part of the UTXO set when restoring from backup,\nwhich may be faster.\n\nJonas Nick was kind enough to go through the numbers and run a benchmark of\nhash(x*I)*G + X on his 3.9GHz Intel\u00ae Core\u2122 i7-7820HQ CPU, which took\nroughly 72 microseconds per calculation on a single core. The UTXO set\ncurrently has 80 million entries, the average transaction has 2.3 inputs,\nwhich puts us at 2.3*80000000*72/1000/1000/60 = 221 minutes for a single\ncore (under 2 hours for two cores).\n\nWhat these numbers do not take into account is database lookups. We need to\nfetch the transaction of every UTXO, as well as every transaction for every\nsubsequent input in order to extract the relevant public key, resulting in\n(1+2.3)*80000000 = 264 million lookups. How slow this is and what can be\ndone to improve it is an open question.\n\nOnce we\u2019re at the tip, every new unspent output will have to be scanned.\nIt\u2019s theoretically possible to scan e.g. once a day and skip transactions\nwith fully spent outputs, but that would probably not be worth the added\ncomplexity. If we only scan transactions with taproot outputs, we can\nfurther limit our efforts, but this advantage is expected to dissipate once\ntaproot use becomes more common.\n\n\nVariant using all inputs\n\nInstead of tweaking the silent payment address with one input, we could\ninstead tweak it with the combination of all input keys of a transaction.\nThe benefit is that this further lowers the scanning cost, since now we\nonly need to calculate one tweak per transaction, instead of one tweak per\ninput, which is roughly half the work, though database lookups remain\nunaffected.\n\nThe downside is that if you want to combine your inputs with those of\nothers (i.e. coinjoin), every participant has to be willing to assist you\nin following the Silent Payment protocol in order to let you make your\npayment. There are also privacy considerations which are discussed in the\n\u201cPreventing input linkage\u201d section.\n\nConcretely, if there are three inputs (I1, I2, I3), the scheme becomes:\nhash(i1*X + i2*X + i3*X)*G + X == hash(x*(I1+I2+I3))*G + X.\n\n\nScanning key\n\nWe can extend the silent payment address with a scanning key, which allows\nfor separation of detecting and spending payments. We redefine the silent\npayment address as the concatenation of X_scan, X_spend, and derivation\nbecomes X' = hash(i*X_scan)*G + X_spend. This allows your\ninternet-connected node to hold the private key of X_scan to detect\nincoming payments, while your hardware wallet controls X_spend to make\npayments. If X_scan is compromised, privacy is lost, but your funds are not.\n\n\nAddress reuse prevention\n\nIf the sender sends more than one payment, and the chosen input has the\nsame key due to address reuse, then the recipient address will also be the\nsame. To prevent this, we can hash the txid and index of the input, to\nensure each address is unique, resulting in X' = hash(i*X,txid,index)*G +\nX. Note this would make light client support harder.\n\n\n\nNOTEWORTHY DETAILS\n\n\nLight clients\n\nLight clients cannot easily be supported due to the need for scanning. The\nbest we could do is give up on address reuse prevention (so we don\u2019t\nrequire the txid and index), only consider unspent taproot outputs, and\ndownload a standardized list of relevant input keys for each block over\nwifi each night when charging. These input keys can then be tweaked, and\nthe results can be matched against compact block filters. Possible, but not\nsimple.\n\n\nEffect on BIP32 HD keys\n\nOne side-benefit of silent payments is that BIP32 HD keys[4] won\u2019t be\nneeded for address generation, since every address will automatically be\nunique. This also means we won\u2019t have to deal with a gap limit.\n\n\nDifferent inputs\n\nWhile the simplest thing would be to only support one input type (e.g.\ntaproot key spend), this would also mean only a subset of users can make\npayments to silent addresses, so this seems undesirable. The protocol\nshould ideally support any input containing at least one public key, and\nsimply pick the first key if more than one is present.\n\nPay-to-(witness-)public-key-hash inputs actually end up being easiest to\nscan, since the public key is present in the input script, instead of the\noutput script of the previous transaction (which requires one extra\ntransaction lookup).\n\n\nSignature nonce instead of input key\n\nAnother consideration was to tweak the silent payment address with the\nsignature nonce[5], but unfortunately this breaks compatibility with MuSig2\nand MuSig-DN, since in those schemes the signature nonce changes depending\non the transaction hash. If we let the output address depend on the nonce,\nthen the transaction hash will change, causing a circular reference.\n\n\nSending wallet compatibility\n\nAny wallet that wants to support making silent payments needs to support a\nnew address format, pick inputs for the payment, tweak the silent payment\naddress using the private key of one of the chosen inputs, and then proceed\nto sign the transaction. The scanning requirement is not relevant to the\nsender, only the recipient.\n\n\n\nPREVENTING INPUT LINKAGE\n\n\nA potential weakness of Silent Payments is that the input is linked to the\noutput. A coinjoin transaction with multiple inputs from other users can\nnormally obfuscate the sender input from the recipient, but Silent Payments\nreveal that link. This weakness can be mitigated with the \u201cvariant using\nall inputs\u201d, but this variant introduces a different weakness \u2013 you now\nrequire all other coinjoin users to tweak the silent payment address, which\nmeans you\u2019re revealing the intended recipient to them.\n\nLuckily, a blinding scheme[6] exists that allows us to hide the silent\npayment address from the other participants. Concretely, let\u2019s say there\nare two inputs, I1 and I2, and the latter one is ours. We add a secret\nblinding factor to the silent payment address, X + blinding_factor*G = X',\nthen we receive X1' = i1*X' (together with a DLEQ to prove correctness, see\nfull write-up[6]) from the owner of the first input and remove the blinding\nfactor with X1' - blinding_factor*I1 = X1 (which is equal to i1*X).\nFinally, we calculate the tweaked address with hash(X1 + i2*X)*G + X. The\nrecipient can simply recognize the payment with hash(x*(I1+I2))*G + X. Note\nthat the owner of the first input cannot reconstruct the resulting address\nbecause they don\u2019t know i2*X.\n\nThe blinding protocol above solves our coinjoin privacy concerns (at the\nexpense of more interaction complexity), but we\u2019re left with one more issue\n\u2013 what if you want to make a silent payment, but you control none of the\ninputs (e.g. sending from an exchange)? In this scenario we can still\nutilize the blinding protocol, but now the third party sender can try to\nuncover the intended recipient by brute forcing their inputs on all known\nsilent payment addresses (i.e. calculate hash(i*X)*G + X for every publicly\nknown X). While this is computationally expensive, it\u2019s by no means\nimpossible. No solution is known at this time, so as it stands this is a\nlimitation of the protocol \u2013 the sender must control one of the inputs in\norder to be fully private.\n\n\n\nCOMPARISON\n\n\nThese are the most important protocols that provide similar functionality\nwith slightly different tradeoffs. All of them provide fresh address\ngeneration and are compatible with one-time seed backups. The main benefits\nof the protocols listed below are that there is no scanning requirement,\nbetter light client support, and they don\u2019t require control over the inputs\nof the transaction.\n\n\nPayment code sharing\n\nThis is BIP47[2]. An OP_RETURN message is sent on-chain to the recipient to\nestablish a shared secret prior to making payments. Using the blockchain as\na messaging layer like this is generally considered an inefficient use of\non-chain resources. This concern can theoretically be alleviated by using\nother means of communicating, but data availability needs to be guaranteed\nto ensure the recipient doesn\u2019t lose access to the funds. Another concern\nis that the input(s) used to establish the shared secret may leak privacy\nif not kept separate.\n\n\nXpub sharing\n\nUpon first payment, hand out an xpub instead of an address in order to\nenable repeat payments. I believe Kixunil\u2019s recently published scheme[3] is\nequivalent to this and could be implemented with relative ease. It\u2019s\nunclear how practical this protocol is, as it assumes sender and recipient\nare able to interact once, yet subsequent interaction is impossible.\n\n\nRegular address sharing\n\nThis is how Bitcoin is commonly used today and may therefore be obvious,\nbut it does satisfy similar privacy requirements. The sender interacts with\nthe recipient each time they want to make a payment, and requests a new\naddress. The main downside is that it requires interaction for every single\npayment.\n\n\n\nOPEN QUESTIONS\n\n\nExactly how slow are the required database lookups? Is there a better\napproach?\n\nIs there any way to make light client support more viable?\n\nWhat is preferred \u2013 single input tweaking (revealing an input to the\nrecipient) or using all inputs (increased coinjoin complexity)?\n\nAre there any security issues with the proposed cryptography?\n\nIn general, compared to alternatives, is this scheme worth the added\ncomplexity?\n\n\n\nACKNOWLEDGEMENTS\n\n\nThanks to Kixunil, Calvin Kim, and Jonas Nick, holihawt and Lloyd Fournier\nfor their help/comments, as well as all the authors of previous schemes.\nAny mistakes are my own.\n\n\n\nREFERENCES\n\n\n[1] Stealth Payments, Peter Todd:\nhttps://github.com/genjix/bips/blob/master/bip-stealth.mediawiki \u21a9\ufe0e\n\n[2] BIP47 payment codes, Justus Ranvier:\nhttps://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n\n[3] Reusable taproot addresses, Kixunil:\nhttps://gist.github.com/Kixunil/0ddb3a9cdec33342b97431e438252c0a\n\n[4] BIP32 HD keys, Pieter Wuille:\nhttps://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\n\n[5] 2020-01-23 ##taproot-bip-review, starting at 18:25:\nhttps://gnusha.org/taproot-bip-review/2020-01-23.log\n\n[6] Blind Diffie-Hellman Key Exchange, David Wagner:\nhttps://gist.github.com/RubenSomsen/be7a4760dd4596d06963d67baf140406\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220328/be32d781/attachment-0001.html>"
            },
            {
                "author": "Billy",
                "date": "2022-03-29T14:57:33",
                "message_text_only": "Hi Ruben,\n\nVery interesting protocol. This reminds me of how monero stealth addresses\nwork, which gives monero the same downsides regarding light clients (among\nother things). I was a bit confused by the following:\n\n> without requiring any interaction or on-chain overhead\n\nAfter reading through, I have to assume it was rather misleading to say \"no\non-chain overhead\". This still requires an on-chain transaction to be sent\nto the tweaked address, I believe. Maybe it would have been more accurate\nto say no *extra* on chain overhead (over a normal transaction)?\n\nIt seems the primary benefit of this is privacy for the recipient. To that\nend, it seems like a pretty useful protocol. It's definitely a level of\nprivacy one would only care about if they might receive a lot money related\nto that address. However of course someone might not know they'll receive\nan amount of money they want to be private until they receive it. So the\ninability to easily do this without a full node is slightly less than\nideal. But it's another good reason to run a full node.\n\nPerhaps there could be a standard that can identify tweaked address, such\nthat only those addresses can be downloaded and checked by light clients.\nIt reduces the anonymity set a bit, but it would probably still be\nsufficient.\n\n\n\nOn Mon, Mar 28, 2022, 10:29 Ruben Somsen via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> I'm publishing a new scheme for private non-interactive address generation\n> without on-chain overhead. It has upsides as well as downsides, so I\n> suspect the main discussion will revolve around whether this is worth\n> pursuing or not. There is a list of open questions at the end.\n>\n> I added the full write-up in plain text below, though I recommend reading\n> the gist for improved formatting and in order to benefit from potential\n> future edits:\n> https://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8\n>\n> Cheers,\n> Ruben\n>\n>\n>\n> Silent Payments\n>\n> Receive private payments from anyone on a single static address without\n> requiring any interaction or on-chain overhead\n>\n>\n>\n> OVERVIEW\n>\n>\n> The recipient generates a so-called silent payment address and makes it\n> publicly known. The sender then takes a public key from one of their chosen\n> inputs for the payment, and uses it to derive a shared secret that is then\n> used to tweak the silent payment address. The recipient detects the payment\n> by scanning every transaction in the blockchain.\n>\n> Compared to previous schemes[1], this scheme avoids using the Bitcoin\n> blockchain as a messaging layer[2] and requires no interaction between\n> sender and recipient[3] (other than needing to know the silent payment\n> address). The main downsides are the scanning requirement, the lack of\n> light client support, and the requirement to control your own input(s). An\n> example use case would be private one-time donations.\n>\n> While most of the individual parts of this idea aren\u2019t novel, the\n> resulting protocol has never been seriously considered and may be\n> reasonably viable, particularly if we limit ourselves to detecting only\n> unspent payments by scanning the UTXO set. We\u2019ll start by describing a\n> basic scheme, and then introduce a few improvements.\n>\n>\n>\n> BASIC SCHEME\n>\n>\n> The recipient publishes their silent payment address, a single 32 byte\n> public key:\n> X = x*G\n>\n> The sender picks an input containing a public key:\n> I = i*G\n>\n> The sender tweaks the silent payment address with the public key of their\n> input:\n> X' = hash(i*X)*G + X\n>\n> Since i*X == x*I (Diffie-Hellman Key Exchange), the recipient can detect\n> the payment by calculating hash(x*I)*G + X for each input key I in the\n> blockchain and seeing if it matches an output in the corresponding\n> transaction.\n>\n>\n>\n> IMPROVEMENTS\n>\n>\n> UTXO set scanning\n>\n> If we forgo detection of historic transactions and only focus on the\n> current balance, we can limit the protocol to only scanning the\n> transactions that are part of the UTXO set when restoring from backup,\n> which may be faster.\n>\n> Jonas Nick was kind enough to go through the numbers and run a benchmark\n> of hash(x*I)*G + X on his 3.9GHz Intel\u00ae Core\u2122 i7-7820HQ CPU, which took\n> roughly 72 microseconds per calculation on a single core. The UTXO set\n> currently has 80 million entries, the average transaction has 2.3 inputs,\n> which puts us at 2.3*80000000*72/1000/1000/60 = 221 minutes for a single\n> core (under 2 hours for two cores).\n>\n> What these numbers do not take into account is database lookups. We need\n> to fetch the transaction of every UTXO, as well as every transaction for\n> every subsequent input in order to extract the relevant public key,\n> resulting in (1+2.3)*80000000 = 264 million lookups. How slow this is and\n> what can be done to improve it is an open question.\n>\n> Once we\u2019re at the tip, every new unspent output will have to be scanned.\n> It\u2019s theoretically possible to scan e.g. once a day and skip transactions\n> with fully spent outputs, but that would probably not be worth the added\n> complexity. If we only scan transactions with taproot outputs, we can\n> further limit our efforts, but this advantage is expected to dissipate once\n> taproot use becomes more common.\n>\n>\n> Variant using all inputs\n>\n> Instead of tweaking the silent payment address with one input, we could\n> instead tweak it with the combination of all input keys of a transaction.\n> The benefit is that this further lowers the scanning cost, since now we\n> only need to calculate one tweak per transaction, instead of one tweak per\n> input, which is roughly half the work, though database lookups remain\n> unaffected.\n>\n> The downside is that if you want to combine your inputs with those of\n> others (i.e. coinjoin), every participant has to be willing to assist you\n> in following the Silent Payment protocol in order to let you make your\n> payment. There are also privacy considerations which are discussed in the\n> \u201cPreventing input linkage\u201d section.\n>\n> Concretely, if there are three inputs (I1, I2, I3), the scheme becomes:\n> hash(i1*X + i2*X + i3*X)*G + X == hash(x*(I1+I2+I3))*G + X.\n>\n>\n> Scanning key\n>\n> We can extend the silent payment address with a scanning key, which allows\n> for separation of detecting and spending payments. We redefine the silent\n> payment address as the concatenation of X_scan, X_spend, and derivation\n> becomes X' = hash(i*X_scan)*G + X_spend. This allows your\n> internet-connected node to hold the private key of X_scan to detect\n> incoming payments, while your hardware wallet controls X_spend to make\n> payments. If X_scan is compromised, privacy is lost, but your funds are not.\n>\n>\n> Address reuse prevention\n>\n> If the sender sends more than one payment, and the chosen input has the\n> same key due to address reuse, then the recipient address will also be the\n> same. To prevent this, we can hash the txid and index of the input, to\n> ensure each address is unique, resulting in X' = hash(i*X,txid,index)*G +\n> X. Note this would make light client support harder.\n>\n>\n>\n> NOTEWORTHY DETAILS\n>\n>\n> Light clients\n>\n> Light clients cannot easily be supported due to the need for scanning. The\n> best we could do is give up on address reuse prevention (so we don\u2019t\n> require the txid and index), only consider unspent taproot outputs, and\n> download a standardized list of relevant input keys for each block over\n> wifi each night when charging. These input keys can then be tweaked, and\n> the results can be matched against compact block filters. Possible, but not\n> simple.\n>\n>\n> Effect on BIP32 HD keys\n>\n> One side-benefit of silent payments is that BIP32 HD keys[4] won\u2019t be\n> needed for address generation, since every address will automatically be\n> unique. This also means we won\u2019t have to deal with a gap limit.\n>\n>\n> Different inputs\n>\n> While the simplest thing would be to only support one input type (e.g.\n> taproot key spend), this would also mean only a subset of users can make\n> payments to silent addresses, so this seems undesirable. The protocol\n> should ideally support any input containing at least one public key, and\n> simply pick the first key if more than one is present.\n>\n> Pay-to-(witness-)public-key-hash inputs actually end up being easiest to\n> scan, since the public key is present in the input script, instead of the\n> output script of the previous transaction (which requires one extra\n> transaction lookup).\n>\n>\n> Signature nonce instead of input key\n>\n> Another consideration was to tweak the silent payment address with the\n> signature nonce[5], but unfortunately this breaks compatibility with MuSig2\n> and MuSig-DN, since in those schemes the signature nonce changes depending\n> on the transaction hash. If we let the output address depend on the nonce,\n> then the transaction hash will change, causing a circular reference.\n>\n>\n> Sending wallet compatibility\n>\n> Any wallet that wants to support making silent payments needs to support a\n> new address format, pick inputs for the payment, tweak the silent payment\n> address using the private key of one of the chosen inputs, and then proceed\n> to sign the transaction. The scanning requirement is not relevant to the\n> sender, only the recipient.\n>\n>\n>\n> PREVENTING INPUT LINKAGE\n>\n>\n> A potential weakness of Silent Payments is that the input is linked to the\n> output. A coinjoin transaction with multiple inputs from other users can\n> normally obfuscate the sender input from the recipient, but Silent Payments\n> reveal that link. This weakness can be mitigated with the \u201cvariant using\n> all inputs\u201d, but this variant introduces a different weakness \u2013 you now\n> require all other coinjoin users to tweak the silent payment address, which\n> means you\u2019re revealing the intended recipient to them.\n>\n> Luckily, a blinding scheme[6] exists that allows us to hide the silent\n> payment address from the other participants. Concretely, let\u2019s say there\n> are two inputs, I1 and I2, and the latter one is ours. We add a secret\n> blinding factor to the silent payment address, X + blinding_factor*G = X',\n> then we receive X1' = i1*X' (together with a DLEQ to prove correctness, see\n> full write-up[6]) from the owner of the first input and remove the blinding\n> factor with X1' - blinding_factor*I1 = X1 (which is equal to i1*X).\n> Finally, we calculate the tweaked address with hash(X1 + i2*X)*G + X. The\n> recipient can simply recognize the payment with hash(x*(I1+I2))*G + X. Note\n> that the owner of the first input cannot reconstruct the resulting address\n> because they don\u2019t know i2*X.\n>\n> The blinding protocol above solves our coinjoin privacy concerns (at the\n> expense of more interaction complexity), but we\u2019re left with one more issue\n> \u2013 what if you want to make a silent payment, but you control none of the\n> inputs (e.g. sending from an exchange)? In this scenario we can still\n> utilize the blinding protocol, but now the third party sender can try to\n> uncover the intended recipient by brute forcing their inputs on all known\n> silent payment addresses (i.e. calculate hash(i*X)*G + X for every publicly\n> known X). While this is computationally expensive, it\u2019s by no means\n> impossible. No solution is known at this time, so as it stands this is a\n> limitation of the protocol \u2013 the sender must control one of the inputs in\n> order to be fully private.\n>\n>\n>\n> COMPARISON\n>\n>\n> These are the most important protocols that provide similar functionality\n> with slightly different tradeoffs. All of them provide fresh address\n> generation and are compatible with one-time seed backups. The main benefits\n> of the protocols listed below are that there is no scanning requirement,\n> better light client support, and they don\u2019t require control over the inputs\n> of the transaction.\n>\n>\n> Payment code sharing\n>\n> This is BIP47[2]. An OP_RETURN message is sent on-chain to the recipient\n> to establish a shared secret prior to making payments. Using the blockchain\n> as a messaging layer like this is generally considered an inefficient use\n> of on-chain resources. This concern can theoretically be alleviated by\n> using other means of communicating, but data availability needs to be\n> guaranteed to ensure the recipient doesn\u2019t lose access to the funds.\n> Another concern is that the input(s) used to establish the shared secret\n> may leak privacy if not kept separate.\n>\n>\n> Xpub sharing\n>\n> Upon first payment, hand out an xpub instead of an address in order to\n> enable repeat payments. I believe Kixunil\u2019s recently published scheme[3] is\n> equivalent to this and could be implemented with relative ease. It\u2019s\n> unclear how practical this protocol is, as it assumes sender and recipient\n> are able to interact once, yet subsequent interaction is impossible.\n>\n>\n> Regular address sharing\n>\n> This is how Bitcoin is commonly used today and may therefore be obvious,\n> but it does satisfy similar privacy requirements. The sender interacts with\n> the recipient each time they want to make a payment, and requests a new\n> address. The main downside is that it requires interaction for every single\n> payment.\n>\n>\n>\n> OPEN QUESTIONS\n>\n>\n> Exactly how slow are the required database lookups? Is there a better\n> approach?\n>\n> Is there any way to make light client support more viable?\n>\n> What is preferred \u2013 single input tweaking (revealing an input to the\n> recipient) or using all inputs (increased coinjoin complexity)?\n>\n> Are there any security issues with the proposed cryptography?\n>\n> In general, compared to alternatives, is this scheme worth the added\n> complexity?\n>\n>\n>\n> ACKNOWLEDGEMENTS\n>\n>\n> Thanks to Kixunil, Calvin Kim, and Jonas Nick, holihawt and Lloyd Fournier\n> for their help/comments, as well as all the authors of previous schemes.\n> Any mistakes are my own.\n>\n>\n>\n> REFERENCES\n>\n>\n> [1] Stealth Payments, Peter Todd:\n> https://github.com/genjix/bips/blob/master/bip-stealth.mediawiki \u21a9\ufe0e\n>\n> [2] BIP47 payment codes, Justus Ranvier:\n> https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n>\n> [3] Reusable taproot addresses, Kixunil:\n> https://gist.github.com/Kixunil/0ddb3a9cdec33342b97431e438252c0a\n>\n> [4] BIP32 HD keys, Pieter Wuille:\n> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\n>\n> [5] 2020-01-23 ##taproot-bip-review, starting at 18:25:\n> https://gnusha.org/taproot-bip-review/2020-01-23.log\n>\n> [6] Blind Diffie-Hellman Key Exchange, David Wagner:\n> https://gist.github.com/RubenSomsen/be7a4760dd4596d06963d67baf140406\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220329/7b52a0f3/attachment-0001.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2022-03-29T15:36:13",
                "message_text_only": "Hi Billy,\n\nThanks for taking a look.\n\n>Maybe it would have been more accurate to say no *extra* on chain overhead\n\nI can see how it can be misinterpreted. I updated the gist to be more\nspecific.\n\n>primary benefit of this is privacy for the recipient\n\nFair, but just wanted to note the sender can get in trouble too if they\nsend money to e.g. blacklisted addresses.\n\n>there could be a standard that [...] reduces the anonymity set a bit\n\nThis has occurred to me but I am reluctant to make that trade-off. It seems\nbest to first see how well this can be optimized without resorting to\nreducing anonymity, and it's hard to analyze exactly how impactful the\nanonymity degradation is (I suspect it's worse than you think because it\ncan help strengthen existing heuristics about output ownership).\n\nCheers,\nRuben\n\n\n\nOn Tue, Mar 29, 2022 at 4:57 PM Billy <fresheneesz at gmail.com> wrote:\n\n> Hi Ruben,\n>\n> Very interesting protocol. This reminds me of how monero stealth addresses\n> work, which gives monero the same downsides regarding light clients (among\n> other things). I was a bit confused by the following:\n>\n> > without requiring any interaction or on-chain overhead\n>\n> After reading through, I have to assume it was rather misleading to say\n> \"no on-chain overhead\". This still requires an on-chain transaction to be\n> sent to the tweaked address, I believe. Maybe it would have been more\n> accurate to say no *extra* on chain overhead (over a normal transaction)?\n>\n> It seems the primary benefit of this is privacy for the recipient. To that\n> end, it seems like a pretty useful protocol. It's definitely a level of\n> privacy one would only care about if they might receive a lot money related\n> to that address. However of course someone might not know they'll receive\n> an amount of money they want to be private until they receive it. So the\n> inability to easily do this without a full node is slightly less than\n> ideal. But it's another good reason to run a full node.\n>\n> Perhaps there could be a standard that can identify tweaked address, such\n> that only those addresses can be downloaded and checked by light clients.\n> It reduces the anonymity set a bit, but it would probably still be\n> sufficient.\n>\n>\n>\n> On Mon, Mar 28, 2022, 10:29 Ruben Somsen via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi all,\n>>\n>> I'm publishing a new scheme for private non-interactive address\n>> generation without on-chain overhead. It has upsides as well as downsides,\n>> so I suspect the main discussion will revolve around whether this is worth\n>> pursuing or not. There is a list of open questions at the end.\n>>\n>> I added the full write-up in plain text below, though I recommend reading\n>> the gist for improved formatting and in order to benefit from potential\n>> future edits:\n>> https://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8\n>>\n>> Cheers,\n>> Ruben\n>>\n>>\n>>\n>> Silent Payments\n>>\n>> Receive private payments from anyone on a single static address without\n>> requiring any interaction or on-chain overhead\n>>\n>>\n>>\n>> OVERVIEW\n>>\n>>\n>> The recipient generates a so-called silent payment address and makes it\n>> publicly known. The sender then takes a public key from one of their chosen\n>> inputs for the payment, and uses it to derive a shared secret that is then\n>> used to tweak the silent payment address. The recipient detects the payment\n>> by scanning every transaction in the blockchain.\n>>\n>> Compared to previous schemes[1], this scheme avoids using the Bitcoin\n>> blockchain as a messaging layer[2] and requires no interaction between\n>> sender and recipient[3] (other than needing to know the silent payment\n>> address). The main downsides are the scanning requirement, the lack of\n>> light client support, and the requirement to control your own input(s). An\n>> example use case would be private one-time donations.\n>>\n>> While most of the individual parts of this idea aren\u2019t novel, the\n>> resulting protocol has never been seriously considered and may be\n>> reasonably viable, particularly if we limit ourselves to detecting only\n>> unspent payments by scanning the UTXO set. We\u2019ll start by describing a\n>> basic scheme, and then introduce a few improvements.\n>>\n>>\n>>\n>> BASIC SCHEME\n>>\n>>\n>> The recipient publishes their silent payment address, a single 32 byte\n>> public key:\n>> X = x*G\n>>\n>> The sender picks an input containing a public key:\n>> I = i*G\n>>\n>> The sender tweaks the silent payment address with the public key of their\n>> input:\n>> X' = hash(i*X)*G + X\n>>\n>> Since i*X == x*I (Diffie-Hellman Key Exchange), the recipient can detect\n>> the payment by calculating hash(x*I)*G + X for each input key I in the\n>> blockchain and seeing if it matches an output in the corresponding\n>> transaction.\n>>\n>>\n>>\n>> IMPROVEMENTS\n>>\n>>\n>> UTXO set scanning\n>>\n>> If we forgo detection of historic transactions and only focus on the\n>> current balance, we can limit the protocol to only scanning the\n>> transactions that are part of the UTXO set when restoring from backup,\n>> which may be faster.\n>>\n>> Jonas Nick was kind enough to go through the numbers and run a benchmark\n>> of hash(x*I)*G + X on his 3.9GHz Intel\u00ae Core\u2122 i7-7820HQ CPU, which took\n>> roughly 72 microseconds per calculation on a single core. The UTXO set\n>> currently has 80 million entries, the average transaction has 2.3 inputs,\n>> which puts us at 2.3*80000000*72/1000/1000/60 = 221 minutes for a single\n>> core (under 2 hours for two cores).\n>>\n>> What these numbers do not take into account is database lookups. We need\n>> to fetch the transaction of every UTXO, as well as every transaction for\n>> every subsequent input in order to extract the relevant public key,\n>> resulting in (1+2.3)*80000000 = 264 million lookups. How slow this is and\n>> what can be done to improve it is an open question.\n>>\n>> Once we\u2019re at the tip, every new unspent output will have to be scanned.\n>> It\u2019s theoretically possible to scan e.g. once a day and skip transactions\n>> with fully spent outputs, but that would probably not be worth the added\n>> complexity. If we only scan transactions with taproot outputs, we can\n>> further limit our efforts, but this advantage is expected to dissipate once\n>> taproot use becomes more common.\n>>\n>>\n>> Variant using all inputs\n>>\n>> Instead of tweaking the silent payment address with one input, we could\n>> instead tweak it with the combination of all input keys of a transaction.\n>> The benefit is that this further lowers the scanning cost, since now we\n>> only need to calculate one tweak per transaction, instead of one tweak per\n>> input, which is roughly half the work, though database lookups remain\n>> unaffected.\n>>\n>> The downside is that if you want to combine your inputs with those of\n>> others (i.e. coinjoin), every participant has to be willing to assist you\n>> in following the Silent Payment protocol in order to let you make your\n>> payment. There are also privacy considerations which are discussed in the\n>> \u201cPreventing input linkage\u201d section.\n>>\n>> Concretely, if there are three inputs (I1, I2, I3), the scheme becomes:\n>> hash(i1*X + i2*X + i3*X)*G + X == hash(x*(I1+I2+I3))*G + X.\n>>\n>>\n>> Scanning key\n>>\n>> We can extend the silent payment address with a scanning key, which\n>> allows for separation of detecting and spending payments. We redefine the\n>> silent payment address as the concatenation of X_scan, X_spend, and\n>> derivation becomes X' = hash(i*X_scan)*G + X_spend. This allows your\n>> internet-connected node to hold the private key of X_scan to detect\n>> incoming payments, while your hardware wallet controls X_spend to make\n>> payments. If X_scan is compromised, privacy is lost, but your funds are not.\n>>\n>>\n>> Address reuse prevention\n>>\n>> If the sender sends more than one payment, and the chosen input has the\n>> same key due to address reuse, then the recipient address will also be the\n>> same. To prevent this, we can hash the txid and index of the input, to\n>> ensure each address is unique, resulting in X' = hash(i*X,txid,index)*G +\n>> X. Note this would make light client support harder.\n>>\n>>\n>>\n>> NOTEWORTHY DETAILS\n>>\n>>\n>> Light clients\n>>\n>> Light clients cannot easily be supported due to the need for scanning.\n>> The best we could do is give up on address reuse prevention (so we don\u2019t\n>> require the txid and index), only consider unspent taproot outputs, and\n>> download a standardized list of relevant input keys for each block over\n>> wifi each night when charging. These input keys can then be tweaked, and\n>> the results can be matched against compact block filters. Possible, but not\n>> simple.\n>>\n>>\n>> Effect on BIP32 HD keys\n>>\n>> One side-benefit of silent payments is that BIP32 HD keys[4] won\u2019t be\n>> needed for address generation, since every address will automatically be\n>> unique. This also means we won\u2019t have to deal with a gap limit.\n>>\n>>\n>> Different inputs\n>>\n>> While the simplest thing would be to only support one input type (e.g.\n>> taproot key spend), this would also mean only a subset of users can make\n>> payments to silent addresses, so this seems undesirable. The protocol\n>> should ideally support any input containing at least one public key, and\n>> simply pick the first key if more than one is present.\n>>\n>> Pay-to-(witness-)public-key-hash inputs actually end up being easiest to\n>> scan, since the public key is present in the input script, instead of the\n>> output script of the previous transaction (which requires one extra\n>> transaction lookup).\n>>\n>>\n>> Signature nonce instead of input key\n>>\n>> Another consideration was to tweak the silent payment address with the\n>> signature nonce[5], but unfortunately this breaks compatibility with MuSig2\n>> and MuSig-DN, since in those schemes the signature nonce changes depending\n>> on the transaction hash. If we let the output address depend on the nonce,\n>> then the transaction hash will change, causing a circular reference.\n>>\n>>\n>> Sending wallet compatibility\n>>\n>> Any wallet that wants to support making silent payments needs to support\n>> a new address format, pick inputs for the payment, tweak the silent payment\n>> address using the private key of one of the chosen inputs, and then proceed\n>> to sign the transaction. The scanning requirement is not relevant to the\n>> sender, only the recipient.\n>>\n>>\n>>\n>> PREVENTING INPUT LINKAGE\n>>\n>>\n>> A potential weakness of Silent Payments is that the input is linked to\n>> the output. A coinjoin transaction with multiple inputs from other users\n>> can normally obfuscate the sender input from the recipient, but Silent\n>> Payments reveal that link. This weakness can be mitigated with the \u201cvariant\n>> using all inputs\u201d, but this variant introduces a different weakness \u2013 you\n>> now require all other coinjoin users to tweak the silent payment address,\n>> which means you\u2019re revealing the intended recipient to them.\n>>\n>> Luckily, a blinding scheme[6] exists that allows us to hide the silent\n>> payment address from the other participants. Concretely, let\u2019s say there\n>> are two inputs, I1 and I2, and the latter one is ours. We add a secret\n>> blinding factor to the silent payment address, X + blinding_factor*G = X',\n>> then we receive X1' = i1*X' (together with a DLEQ to prove correctness, see\n>> full write-up[6]) from the owner of the first input and remove the blinding\n>> factor with X1' - blinding_factor*I1 = X1 (which is equal to i1*X).\n>> Finally, we calculate the tweaked address with hash(X1 + i2*X)*G + X. The\n>> recipient can simply recognize the payment with hash(x*(I1+I2))*G + X. Note\n>> that the owner of the first input cannot reconstruct the resulting address\n>> because they don\u2019t know i2*X.\n>>\n>> The blinding protocol above solves our coinjoin privacy concerns (at the\n>> expense of more interaction complexity), but we\u2019re left with one more issue\n>> \u2013 what if you want to make a silent payment, but you control none of the\n>> inputs (e.g. sending from an exchange)? In this scenario we can still\n>> utilize the blinding protocol, but now the third party sender can try to\n>> uncover the intended recipient by brute forcing their inputs on all known\n>> silent payment addresses (i.e. calculate hash(i*X)*G + X for every publicly\n>> known X). While this is computationally expensive, it\u2019s by no means\n>> impossible. No solution is known at this time, so as it stands this is a\n>> limitation of the protocol \u2013 the sender must control one of the inputs in\n>> order to be fully private.\n>>\n>>\n>>\n>> COMPARISON\n>>\n>>\n>> These are the most important protocols that provide similar functionality\n>> with slightly different tradeoffs. All of them provide fresh address\n>> generation and are compatible with one-time seed backups. The main benefits\n>> of the protocols listed below are that there is no scanning requirement,\n>> better light client support, and they don\u2019t require control over the inputs\n>> of the transaction.\n>>\n>>\n>> Payment code sharing\n>>\n>> This is BIP47[2]. An OP_RETURN message is sent on-chain to the recipient\n>> to establish a shared secret prior to making payments. Using the blockchain\n>> as a messaging layer like this is generally considered an inefficient use\n>> of on-chain resources. This concern can theoretically be alleviated by\n>> using other means of communicating, but data availability needs to be\n>> guaranteed to ensure the recipient doesn\u2019t lose access to the funds.\n>> Another concern is that the input(s) used to establish the shared secret\n>> may leak privacy if not kept separate.\n>>\n>>\n>> Xpub sharing\n>>\n>> Upon first payment, hand out an xpub instead of an address in order to\n>> enable repeat payments. I believe Kixunil\u2019s recently published scheme[3] is\n>> equivalent to this and could be implemented with relative ease. It\u2019s\n>> unclear how practical this protocol is, as it assumes sender and recipient\n>> are able to interact once, yet subsequent interaction is impossible.\n>>\n>>\n>> Regular address sharing\n>>\n>> This is how Bitcoin is commonly used today and may therefore be obvious,\n>> but it does satisfy similar privacy requirements. The sender interacts with\n>> the recipient each time they want to make a payment, and requests a new\n>> address. The main downside is that it requires interaction for every single\n>> payment.\n>>\n>>\n>>\n>> OPEN QUESTIONS\n>>\n>>\n>> Exactly how slow are the required database lookups? Is there a better\n>> approach?\n>>\n>> Is there any way to make light client support more viable?\n>>\n>> What is preferred \u2013 single input tweaking (revealing an input to the\n>> recipient) or using all inputs (increased coinjoin complexity)?\n>>\n>> Are there any security issues with the proposed cryptography?\n>>\n>> In general, compared to alternatives, is this scheme worth the added\n>> complexity?\n>>\n>>\n>>\n>> ACKNOWLEDGEMENTS\n>>\n>>\n>> Thanks to Kixunil, Calvin Kim, and Jonas Nick, holihawt and Lloyd\n>> Fournier for their help/comments, as well as all the authors of previous\n>> schemes. Any mistakes are my own.\n>>\n>>\n>>\n>> REFERENCES\n>>\n>>\n>> [1] Stealth Payments, Peter Todd:\n>> https://github.com/genjix/bips/blob/master/bip-stealth.mediawiki \u21a9\ufe0e\n>>\n>> [2] BIP47 payment codes, Justus Ranvier:\n>> https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n>>\n>> [3] Reusable taproot addresses, Kixunil:\n>> https://gist.github.com/Kixunil/0ddb3a9cdec33342b97431e438252c0a\n>>\n>> [4] BIP32 HD keys, Pieter Wuille:\n>> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\n>>\n>> [5] 2020-01-23 ##taproot-bip-review, starting at 18:25:\n>> https://gnusha.org/taproot-bip-review/2020-01-23.log\n>>\n>> [6] Blind Diffie-Hellman Key Exchange, David Wagner:\n>> https://gist.github.com/RubenSomsen/be7a4760dd4596d06963d67baf140406\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220329/ce02b752/attachment-0001.html>"
            },
            {
                "author": "Billy",
                "date": "2022-03-30T05:58:18",
                "message_text_only": ">  the sender can get in trouble too if they send money\n\nGood point.\n\n> how well this can be optimized without resorting to reducing anonymity\n\nComplete shot in the dark, but I wonder if something akin to compact block\nfilters could be done to support this case. If, for example, the tweaked\nkey were defined without hashing, I think something like that could be done:\n\nX'  =  i*X*G + X  =  x*I*G + X\n\nYour compact-block-filter-like things could then store a set of each `item =\n{recipient: X' % N, sender: I%N}`, and a light client would download this\ndata and do the following to detect a likely payment for each filter item:\n\nitem.recipient - X%N == x*item.sender*G\n\nYou can then scale N to the proper tradeoff between filter size and false\npositives. I suppose this might make it possible to deprivitize a tweaked\nkey by checking to see what non-tweaked keys evenly divide it. Perhaps\nthat's what hashing was being used to solve. What if we added the shared\ndiffie hellman secret modulo N to remove this correlation:\n\nX' = i*X*G + X + (i*X)%N =  x*I*G + X + (x*I)%N\n\nThen for each `item = {recipient: X' % N, sender: I%N}`, we detect via\n`item.recipient - X%N == x*item.sender*(1+G)`. Is my math right here? I'm\nthinking this should work because (a+b%N)%N == (a%N + b%N)%N.\n\n\n\nOn Tue, Mar 29, 2022 at 10:36 AM Ruben Somsen <rsomsen at gmail.com> wrote:\n\n> Hi Billy,\n>\n> Thanks for taking a look.\n>\n> >Maybe it would have been more accurate to say no *extra* on chain overhead\n>\n> I can see how it can be misinterpreted. I updated the gist to be more\n> specific.\n>\n> >primary benefit of this is privacy for the recipient\n>\n> Fair, but just wanted to note the sender can get in trouble too if they\n> send money to e.g. blacklisted addresses.\n>\n> >there could be a standard that [...] reduces the anonymity set a bit\n>\n> This has occurred to me but I am reluctant to make that trade-off. It\n> seems best to first see how well this can be optimized without resorting to\n> reducing anonymity, and it's hard to analyze exactly how impactful the\n> anonymity degradation is (I suspect it's worse than you think because it\n> can help strengthen existing heuristics about output ownership).\n>\n> Cheers,\n> Ruben\n>\n>\n>\n> On Tue, Mar 29, 2022 at 4:57 PM Billy <fresheneesz at gmail.com> wrote:\n>\n>> Hi Ruben,\n>>\n>> Very interesting protocol. This reminds me of how monero stealth\n>> addresses work, which gives monero the same downsides regarding light\n>> clients (among other things). I was a bit confused by the following:\n>>\n>> > without requiring any interaction or on-chain overhead\n>>\n>> After reading through, I have to assume it was rather misleading to say\n>> \"no on-chain overhead\". This still requires an on-chain transaction to be\n>> sent to the tweaked address, I believe. Maybe it would have been more\n>> accurate to say no *extra* on chain overhead (over a normal transaction)?\n>>\n>> It seems the primary benefit of this is privacy for the recipient. To\n>> that end, it seems like a pretty useful protocol. It's definitely a level\n>> of privacy one would only care about if they might receive a lot money\n>> related to that address. However of course someone might not know they'll\n>> receive an amount of money they want to be private until they receive it.\n>> So the inability to easily do this without a full node is slightly less\n>> than ideal. But it's another good reason to run a full node.\n>>\n>> Perhaps there could be a standard that can identify tweaked address, such\n>> that only those addresses can be downloaded and checked by light clients.\n>> It reduces the anonymity set a bit, but it would probably still be\n>> sufficient.\n>>\n>>\n>>\n>> On Mon, Mar 28, 2022, 10:29 Ruben Somsen via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Hi all,\n>>>\n>>> I'm publishing a new scheme for private non-interactive address\n>>> generation without on-chain overhead. It has upsides as well as downsides,\n>>> so I suspect the main discussion will revolve around whether this is worth\n>>> pursuing or not. There is a list of open questions at the end.\n>>>\n>>> I added the full write-up in plain text below, though I recommend\n>>> reading the gist for improved formatting and in order to benefit from\n>>> potential future edits:\n>>> https://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8\n>>>\n>>> Cheers,\n>>> Ruben\n>>>\n>>>\n>>>\n>>> Silent Payments\n>>>\n>>> Receive private payments from anyone on a single static address without\n>>> requiring any interaction or on-chain overhead\n>>>\n>>>\n>>>\n>>> OVERVIEW\n>>>\n>>>\n>>> The recipient generates a so-called silent payment address and makes it\n>>> publicly known. The sender then takes a public key from one of their chosen\n>>> inputs for the payment, and uses it to derive a shared secret that is then\n>>> used to tweak the silent payment address. The recipient detects the payment\n>>> by scanning every transaction in the blockchain.\n>>>\n>>> Compared to previous schemes[1], this scheme avoids using the Bitcoin\n>>> blockchain as a messaging layer[2] and requires no interaction between\n>>> sender and recipient[3] (other than needing to know the silent payment\n>>> address). The main downsides are the scanning requirement, the lack of\n>>> light client support, and the requirement to control your own input(s). An\n>>> example use case would be private one-time donations.\n>>>\n>>> While most of the individual parts of this idea aren\u2019t novel, the\n>>> resulting protocol has never been seriously considered and may be\n>>> reasonably viable, particularly if we limit ourselves to detecting only\n>>> unspent payments by scanning the UTXO set. We\u2019ll start by describing a\n>>> basic scheme, and then introduce a few improvements.\n>>>\n>>>\n>>>\n>>> BASIC SCHEME\n>>>\n>>>\n>>> The recipient publishes their silent payment address, a single 32 byte\n>>> public key:\n>>> X = x*G\n>>>\n>>> The sender picks an input containing a public key:\n>>> I = i*G\n>>>\n>>> The sender tweaks the silent payment address with the public key of\n>>> their input:\n>>> X' = hash(i*X)*G + X\n>>>\n>>> Since i*X == x*I (Diffie-Hellman Key Exchange), the recipient can detect\n>>> the payment by calculating hash(x*I)*G + X for each input key I in the\n>>> blockchain and seeing if it matches an output in the corresponding\n>>> transaction.\n>>>\n>>>\n>>>\n>>> IMPROVEMENTS\n>>>\n>>>\n>>> UTXO set scanning\n>>>\n>>> If we forgo detection of historic transactions and only focus on the\n>>> current balance, we can limit the protocol to only scanning the\n>>> transactions that are part of the UTXO set when restoring from backup,\n>>> which may be faster.\n>>>\n>>> Jonas Nick was kind enough to go through the numbers and run a benchmark\n>>> of hash(x*I)*G + X on his 3.9GHz Intel\u00ae Core\u2122 i7-7820HQ CPU, which took\n>>> roughly 72 microseconds per calculation on a single core. The UTXO set\n>>> currently has 80 million entries, the average transaction has 2.3 inputs,\n>>> which puts us at 2.3*80000000*72/1000/1000/60 = 221 minutes for a single\n>>> core (under 2 hours for two cores).\n>>>\n>>> What these numbers do not take into account is database lookups. We need\n>>> to fetch the transaction of every UTXO, as well as every transaction for\n>>> every subsequent input in order to extract the relevant public key,\n>>> resulting in (1+2.3)*80000000 = 264 million lookups. How slow this is and\n>>> what can be done to improve it is an open question.\n>>>\n>>> Once we\u2019re at the tip, every new unspent output will have to be scanned.\n>>> It\u2019s theoretically possible to scan e.g. once a day and skip transactions\n>>> with fully spent outputs, but that would probably not be worth the added\n>>> complexity. If we only scan transactions with taproot outputs, we can\n>>> further limit our efforts, but this advantage is expected to dissipate once\n>>> taproot use becomes more common.\n>>>\n>>>\n>>> Variant using all inputs\n>>>\n>>> Instead of tweaking the silent payment address with one input, we could\n>>> instead tweak it with the combination of all input keys of a transaction.\n>>> The benefit is that this further lowers the scanning cost, since now we\n>>> only need to calculate one tweak per transaction, instead of one tweak per\n>>> input, which is roughly half the work, though database lookups remain\n>>> unaffected.\n>>>\n>>> The downside is that if you want to combine your inputs with those of\n>>> others (i.e. coinjoin), every participant has to be willing to assist you\n>>> in following the Silent Payment protocol in order to let you make your\n>>> payment. There are also privacy considerations which are discussed in the\n>>> \u201cPreventing input linkage\u201d section.\n>>>\n>>> Concretely, if there are three inputs (I1, I2, I3), the scheme becomes:\n>>> hash(i1*X + i2*X + i3*X)*G + X == hash(x*(I1+I2+I3))*G + X.\n>>>\n>>>\n>>> Scanning key\n>>>\n>>> We can extend the silent payment address with a scanning key, which\n>>> allows for separation of detecting and spending payments. We redefine the\n>>> silent payment address as the concatenation of X_scan, X_spend, and\n>>> derivation becomes X' = hash(i*X_scan)*G + X_spend. This allows your\n>>> internet-connected node to hold the private key of X_scan to detect\n>>> incoming payments, while your hardware wallet controls X_spend to make\n>>> payments. If X_scan is compromised, privacy is lost, but your funds are not.\n>>>\n>>>\n>>> Address reuse prevention\n>>>\n>>> If the sender sends more than one payment, and the chosen input has the\n>>> same key due to address reuse, then the recipient address will also be the\n>>> same. To prevent this, we can hash the txid and index of the input, to\n>>> ensure each address is unique, resulting in X' = hash(i*X,txid,index)*G +\n>>> X. Note this would make light client support harder.\n>>>\n>>>\n>>>\n>>> NOTEWORTHY DETAILS\n>>>\n>>>\n>>> Light clients\n>>>\n>>> Light clients cannot easily be supported due to the need for scanning.\n>>> The best we could do is give up on address reuse prevention (so we don\u2019t\n>>> require the txid and index), only consider unspent taproot outputs, and\n>>> download a standardized list of relevant input keys for each block over\n>>> wifi each night when charging. These input keys can then be tweaked, and\n>>> the results can be matched against compact block filters. Possible, but not\n>>> simple.\n>>>\n>>>\n>>> Effect on BIP32 HD keys\n>>>\n>>> One side-benefit of silent payments is that BIP32 HD keys[4] won\u2019t be\n>>> needed for address generation, since every address will automatically be\n>>> unique. This also means we won\u2019t have to deal with a gap limit.\n>>>\n>>>\n>>> Different inputs\n>>>\n>>> While the simplest thing would be to only support one input type (e.g.\n>>> taproot key spend), this would also mean only a subset of users can make\n>>> payments to silent addresses, so this seems undesirable. The protocol\n>>> should ideally support any input containing at least one public key, and\n>>> simply pick the first key if more than one is present.\n>>>\n>>> Pay-to-(witness-)public-key-hash inputs actually end up being easiest to\n>>> scan, since the public key is present in the input script, instead of the\n>>> output script of the previous transaction (which requires one extra\n>>> transaction lookup).\n>>>\n>>>\n>>> Signature nonce instead of input key\n>>>\n>>> Another consideration was to tweak the silent payment address with the\n>>> signature nonce[5], but unfortunately this breaks compatibility with MuSig2\n>>> and MuSig-DN, since in those schemes the signature nonce changes depending\n>>> on the transaction hash. If we let the output address depend on the nonce,\n>>> then the transaction hash will change, causing a circular reference.\n>>>\n>>>\n>>> Sending wallet compatibility\n>>>\n>>> Any wallet that wants to support making silent payments needs to support\n>>> a new address format, pick inputs for the payment, tweak the silent payment\n>>> address using the private key of one of the chosen inputs, and then proceed\n>>> to sign the transaction. The scanning requirement is not relevant to the\n>>> sender, only the recipient.\n>>>\n>>>\n>>>\n>>> PREVENTING INPUT LINKAGE\n>>>\n>>>\n>>> A potential weakness of Silent Payments is that the input is linked to\n>>> the output. A coinjoin transaction with multiple inputs from other users\n>>> can normally obfuscate the sender input from the recipient, but Silent\n>>> Payments reveal that link. This weakness can be mitigated with the \u201cvariant\n>>> using all inputs\u201d, but this variant introduces a different weakness \u2013 you\n>>> now require all other coinjoin users to tweak the silent payment address,\n>>> which means you\u2019re revealing the intended recipient to them.\n>>>\n>>> Luckily, a blinding scheme[6] exists that allows us to hide the silent\n>>> payment address from the other participants. Concretely, let\u2019s say there\n>>> are two inputs, I1 and I2, and the latter one is ours. We add a secret\n>>> blinding factor to the silent payment address, X + blinding_factor*G = X',\n>>> then we receive X1' = i1*X' (together with a DLEQ to prove correctness, see\n>>> full write-up[6]) from the owner of the first input and remove the blinding\n>>> factor with X1' - blinding_factor*I1 = X1 (which is equal to i1*X).\n>>> Finally, we calculate the tweaked address with hash(X1 + i2*X)*G + X. The\n>>> recipient can simply recognize the payment with hash(x*(I1+I2))*G + X. Note\n>>> that the owner of the first input cannot reconstruct the resulting address\n>>> because they don\u2019t know i2*X.\n>>>\n>>> The blinding protocol above solves our coinjoin privacy concerns (at the\n>>> expense of more interaction complexity), but we\u2019re left with one more issue\n>>> \u2013 what if you want to make a silent payment, but you control none of the\n>>> inputs (e.g. sending from an exchange)? In this scenario we can still\n>>> utilize the blinding protocol, but now the third party sender can try to\n>>> uncover the intended recipient by brute forcing their inputs on all known\n>>> silent payment addresses (i.e. calculate hash(i*X)*G + X for every publicly\n>>> known X). While this is computationally expensive, it\u2019s by no means\n>>> impossible. No solution is known at this time, so as it stands this is a\n>>> limitation of the protocol \u2013 the sender must control one of the inputs in\n>>> order to be fully private.\n>>>\n>>>\n>>>\n>>> COMPARISON\n>>>\n>>>\n>>> These are the most important protocols that provide similar\n>>> functionality with slightly different tradeoffs. All of them provide fresh\n>>> address generation and are compatible with one-time seed backups. The main\n>>> benefits of the protocols listed below are that there is no scanning\n>>> requirement, better light client support, and they don\u2019t require control\n>>> over the inputs of the transaction.\n>>>\n>>>\n>>> Payment code sharing\n>>>\n>>> This is BIP47[2]. An OP_RETURN message is sent on-chain to the recipient\n>>> to establish a shared secret prior to making payments. Using the blockchain\n>>> as a messaging layer like this is generally considered an inefficient use\n>>> of on-chain resources. This concern can theoretically be alleviated by\n>>> using other means of communicating, but data availability needs to be\n>>> guaranteed to ensure the recipient doesn\u2019t lose access to the funds.\n>>> Another concern is that the input(s) used to establish the shared secret\n>>> may leak privacy if not kept separate.\n>>>\n>>>\n>>> Xpub sharing\n>>>\n>>> Upon first payment, hand out an xpub instead of an address in order to\n>>> enable repeat payments. I believe Kixunil\u2019s recently published scheme[3] is\n>>> equivalent to this and could be implemented with relative ease. It\u2019s\n>>> unclear how practical this protocol is, as it assumes sender and recipient\n>>> are able to interact once, yet subsequent interaction is impossible.\n>>>\n>>>\n>>> Regular address sharing\n>>>\n>>> This is how Bitcoin is commonly used today and may therefore be obvious,\n>>> but it does satisfy similar privacy requirements. The sender interacts with\n>>> the recipient each time they want to make a payment, and requests a new\n>>> address. The main downside is that it requires interaction for every single\n>>> payment.\n>>>\n>>>\n>>>\n>>> OPEN QUESTIONS\n>>>\n>>>\n>>> Exactly how slow are the required database lookups? Is there a better\n>>> approach?\n>>>\n>>> Is there any way to make light client support more viable?\n>>>\n>>> What is preferred \u2013 single input tweaking (revealing an input to the\n>>> recipient) or using all inputs (increased coinjoin complexity)?\n>>>\n>>> Are there any security issues with the proposed cryptography?\n>>>\n>>> In general, compared to alternatives, is this scheme worth the added\n>>> complexity?\n>>>\n>>>\n>>>\n>>> ACKNOWLEDGEMENTS\n>>>\n>>>\n>>> Thanks to Kixunil, Calvin Kim, and Jonas Nick, holihawt and Lloyd\n>>> Fournier for their help/comments, as well as all the authors of previous\n>>> schemes. Any mistakes are my own.\n>>>\n>>>\n>>>\n>>> REFERENCES\n>>>\n>>>\n>>> [1] Stealth Payments, Peter Todd:\n>>> https://github.com/genjix/bips/blob/master/bip-stealth.mediawiki \u21a9\ufe0e\n>>>\n>>> [2] BIP47 payment codes, Justus Ranvier:\n>>> https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n>>>\n>>> [3] Reusable taproot addresses, Kixunil:\n>>> https://gist.github.com/Kixunil/0ddb3a9cdec33342b97431e438252c0a\n>>>\n>>> [4] BIP32 HD keys, Pieter Wuille:\n>>> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\n>>>\n>>> [5] 2020-01-23 ##taproot-bip-review, starting at 18:25:\n>>> https://gnusha.org/taproot-bip-review/2020-01-23.log\n>>>\n>>> [6] Blind Diffie-Hellman Key Exchange, David Wagner:\n>>> https://gist.github.com/RubenSomsen/be7a4760dd4596d06963d67baf140406\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220330/66aa593a/attachment-0001.html>"
            },
            {
                "author": "Billy",
                "date": "2022-03-30T16:09:22",
                "message_text_only": "Hi Ruben,\n\nAfter sending that last night, I realized the solution I had to\ndeprivatizing the sender wouldn't work because it had the same problem of\neven divisibility in modulo N. And my math was incomplete I think. Also\nMarco D'Agostini pointed out other errors. And all this assumes that a\nmodulus operator is defined for elliptic curve points in a way that makes\nthese valid, which I'm not sure is true. But here's another try anyway:\n\nX' = X + i*X*hash((i*X)%N) =  X + x*I*hash((x*I)%N)\n\nitem = {recipient: X' % N, sender: I%N} // As before.\n\nTest for each filter item: (item.recipient - X) % N == (\nx*item.sender*hash((x*item.sender) % N) ) % N\n\nSo to muse further about the properties of this, in a block full of taproot\nsends you might have an upper limit of something like 13,000 transactions.\nN=2^8 would I think mean an 18% collision rate (ie 20% false positive rate)\nbecause `(1-1/2^8)^13000 = 0.82...`. If we were to go with that, each item\nis 4 bytes (1 byte per point component?) which would mean a 52kb filter\nwithout collisions, and an average of 43kb with 18% collisions (which can\nbe removed as dupes). Maybe Golomb-Rice coding could help here as well like\nit does in the usual compact block filters. And since each collision with\nan address a client is watching on means downloading a whole block they\ndon't need, maybe 18% collisions is too high, and we want to choose N =\n2^10 or something to get down to 2% collisions.\n\nIn any case, all this could be wrong if ECC modulus doesn't work this way.\nBut was interesting to think about anyway.\n\nOn Wed, Mar 30, 2022 at 12:58 AM Billy <fresheneesz at gmail.com> wrote:\n\n> >  the sender can get in trouble too if they send money\n>\n> Good point.\n>\n> > how well this can be optimized without resorting to reducing anonymity\n>\n> Complete shot in the dark, but I wonder if something akin to compact block\n> filters could be done to support this case. If, for example, the tweaked\n> key were defined without hashing, I think something like that could be done:\n>\n> X'  =  i*X*G + X  =  x*I*G + X\n>\n> Your compact-block-filter-like things could then store a set of each `item\n> = {recipient: X' % N, sender: I%N}`, and a light client would download\n> this data and do the following to detect a likely payment for each filter\n> item:\n>\n> item.recipient - X%N == x*item.sender*G\n>\n> You can then scale N to the proper tradeoff between filter size and false\n> positives. I suppose this might make it possible to deprivitize a tweaked\n> key by checking to see what non-tweaked keys evenly divide it. Perhaps\n> that's what hashing was being used to solve. What if we added the shared\n> diffie hellman secret modulo N to remove this correlation:\n>\n> X' = i*X*G + X + (i*X)%N =  x*I*G + X + (x*I)%N\n>\n> Then for each `item = {recipient: X' % N, sender: I%N}`, we detect via\n> `item.recipient - X%N == x*item.sender*(1+G)`. Is my math right here? I'm\n> thinking this should work because (a+b%N)%N == (a%N + b%N)%N.\n>\n>\n>\n> On Tue, Mar 29, 2022 at 10:36 AM Ruben Somsen <rsomsen at gmail.com> wrote:\n>\n>> Hi Billy,\n>>\n>> Thanks for taking a look.\n>>\n>> >Maybe it would have been more accurate to say no *extra* on chain\n>> overhead\n>>\n>> I can see how it can be misinterpreted. I updated the gist to be more\n>> specific.\n>>\n>> >primary benefit of this is privacy for the recipient\n>>\n>> Fair, but just wanted to note the sender can get in trouble too if they\n>> send money to e.g. blacklisted addresses.\n>>\n>> >there could be a standard that [...] reduces the anonymity set a bit\n>>\n>> This has occurred to me but I am reluctant to make that trade-off. It\n>> seems best to first see how well this can be optimized without resorting to\n>> reducing anonymity, and it's hard to analyze exactly how impactful the\n>> anonymity degradation is (I suspect it's worse than you think because it\n>> can help strengthen existing heuristics about output ownership).\n>>\n>> Cheers,\n>> Ruben\n>>\n>>\n>>\n>> On Tue, Mar 29, 2022 at 4:57 PM Billy <fresheneesz at gmail.com> wrote:\n>>\n>>> Hi Ruben,\n>>>\n>>> Very interesting protocol. This reminds me of how monero stealth\n>>> addresses work, which gives monero the same downsides regarding light\n>>> clients (among other things). I was a bit confused by the following:\n>>>\n>>> > without requiring any interaction or on-chain overhead\n>>>\n>>> After reading through, I have to assume it was rather misleading to say\n>>> \"no on-chain overhead\". This still requires an on-chain transaction to be\n>>> sent to the tweaked address, I believe. Maybe it would have been more\n>>> accurate to say no *extra* on chain overhead (over a normal transaction)?\n>>>\n>>> It seems the primary benefit of this is privacy for the recipient. To\n>>> that end, it seems like a pretty useful protocol. It's definitely a level\n>>> of privacy one would only care about if they might receive a lot money\n>>> related to that address. However of course someone might not know they'll\n>>> receive an amount of money they want to be private until they receive it.\n>>> So the inability to easily do this without a full node is slightly less\n>>> than ideal. But it's another good reason to run a full node.\n>>>\n>>> Perhaps there could be a standard that can identify tweaked address,\n>>> such that only those addresses can be downloaded and checked by light\n>>> clients. It reduces the anonymity set a bit, but it would probably still be\n>>> sufficient.\n>>>\n>>>\n>>>\n>>> On Mon, Mar 28, 2022, 10:29 Ruben Somsen via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Hi all,\n>>>>\n>>>> I'm publishing a new scheme for private non-interactive address\n>>>> generation without on-chain overhead. It has upsides as well as downsides,\n>>>> so I suspect the main discussion will revolve around whether this is worth\n>>>> pursuing or not. There is a list of open questions at the end.\n>>>>\n>>>> I added the full write-up in plain text below, though I recommend\n>>>> reading the gist for improved formatting and in order to benefit from\n>>>> potential future edits:\n>>>> https://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8\n>>>>\n>>>> Cheers,\n>>>> Ruben\n>>>>\n>>>>\n>>>>\n>>>> Silent Payments\n>>>>\n>>>> Receive private payments from anyone on a single static address without\n>>>> requiring any interaction or on-chain overhead\n>>>>\n>>>>\n>>>>\n>>>> OVERVIEW\n>>>>\n>>>>\n>>>> The recipient generates a so-called silent payment address and makes it\n>>>> publicly known. The sender then takes a public key from one of their chosen\n>>>> inputs for the payment, and uses it to derive a shared secret that is then\n>>>> used to tweak the silent payment address. The recipient detects the payment\n>>>> by scanning every transaction in the blockchain.\n>>>>\n>>>> Compared to previous schemes[1], this scheme avoids using the Bitcoin\n>>>> blockchain as a messaging layer[2] and requires no interaction between\n>>>> sender and recipient[3] (other than needing to know the silent payment\n>>>> address). The main downsides are the scanning requirement, the lack of\n>>>> light client support, and the requirement to control your own input(s). An\n>>>> example use case would be private one-time donations.\n>>>>\n>>>> While most of the individual parts of this idea aren\u2019t novel, the\n>>>> resulting protocol has never been seriously considered and may be\n>>>> reasonably viable, particularly if we limit ourselves to detecting only\n>>>> unspent payments by scanning the UTXO set. We\u2019ll start by describing a\n>>>> basic scheme, and then introduce a few improvements.\n>>>>\n>>>>\n>>>>\n>>>> BASIC SCHEME\n>>>>\n>>>>\n>>>> The recipient publishes their silent payment address, a single 32 byte\n>>>> public key:\n>>>> X = x*G\n>>>>\n>>>> The sender picks an input containing a public key:\n>>>> I = i*G\n>>>>\n>>>> The sender tweaks the silent payment address with the public key of\n>>>> their input:\n>>>> X' = hash(i*X)*G + X\n>>>>\n>>>> Since i*X == x*I (Diffie-Hellman Key Exchange), the recipient can\n>>>> detect the payment by calculating hash(x*I)*G + X for each input key I in\n>>>> the blockchain and seeing if it matches an output in the corresponding\n>>>> transaction.\n>>>>\n>>>>\n>>>>\n>>>> IMPROVEMENTS\n>>>>\n>>>>\n>>>> UTXO set scanning\n>>>>\n>>>> If we forgo detection of historic transactions and only focus on the\n>>>> current balance, we can limit the protocol to only scanning the\n>>>> transactions that are part of the UTXO set when restoring from backup,\n>>>> which may be faster.\n>>>>\n>>>> Jonas Nick was kind enough to go through the numbers and run a\n>>>> benchmark of hash(x*I)*G + X on his 3.9GHz Intel\u00ae Core\u2122 i7-7820HQ CPU,\n>>>> which took roughly 72 microseconds per calculation on a single core. The\n>>>> UTXO set currently has 80 million entries, the average transaction has 2.3\n>>>> inputs, which puts us at 2.3*80000000*72/1000/1000/60 = 221 minutes for a\n>>>> single core (under 2 hours for two cores).\n>>>>\n>>>> What these numbers do not take into account is database lookups. We\n>>>> need to fetch the transaction of every UTXO, as well as every transaction\n>>>> for every subsequent input in order to extract the relevant public key,\n>>>> resulting in (1+2.3)*80000000 = 264 million lookups. How slow this is and\n>>>> what can be done to improve it is an open question.\n>>>>\n>>>> Once we\u2019re at the tip, every new unspent output will have to be\n>>>> scanned. It\u2019s theoretically possible to scan e.g. once a day and skip\n>>>> transactions with fully spent outputs, but that would probably not be worth\n>>>> the added complexity. If we only scan transactions with taproot outputs, we\n>>>> can further limit our efforts, but this advantage is expected to dissipate\n>>>> once taproot use becomes more common.\n>>>>\n>>>>\n>>>> Variant using all inputs\n>>>>\n>>>> Instead of tweaking the silent payment address with one input, we could\n>>>> instead tweak it with the combination of all input keys of a transaction.\n>>>> The benefit is that this further lowers the scanning cost, since now we\n>>>> only need to calculate one tweak per transaction, instead of one tweak per\n>>>> input, which is roughly half the work, though database lookups remain\n>>>> unaffected.\n>>>>\n>>>> The downside is that if you want to combine your inputs with those of\n>>>> others (i.e. coinjoin), every participant has to be willing to assist you\n>>>> in following the Silent Payment protocol in order to let you make your\n>>>> payment. There are also privacy considerations which are discussed in the\n>>>> \u201cPreventing input linkage\u201d section.\n>>>>\n>>>> Concretely, if there are three inputs (I1, I2, I3), the scheme becomes:\n>>>> hash(i1*X + i2*X + i3*X)*G + X == hash(x*(I1+I2+I3))*G + X.\n>>>>\n>>>>\n>>>> Scanning key\n>>>>\n>>>> We can extend the silent payment address with a scanning key, which\n>>>> allows for separation of detecting and spending payments. We redefine the\n>>>> silent payment address as the concatenation of X_scan, X_spend, and\n>>>> derivation becomes X' = hash(i*X_scan)*G + X_spend. This allows your\n>>>> internet-connected node to hold the private key of X_scan to detect\n>>>> incoming payments, while your hardware wallet controls X_spend to make\n>>>> payments. If X_scan is compromised, privacy is lost, but your funds are not.\n>>>>\n>>>>\n>>>> Address reuse prevention\n>>>>\n>>>> If the sender sends more than one payment, and the chosen input has the\n>>>> same key due to address reuse, then the recipient address will also be the\n>>>> same. To prevent this, we can hash the txid and index of the input, to\n>>>> ensure each address is unique, resulting in X' = hash(i*X,txid,index)*G +\n>>>> X. Note this would make light client support harder.\n>>>>\n>>>>\n>>>>\n>>>> NOTEWORTHY DETAILS\n>>>>\n>>>>\n>>>> Light clients\n>>>>\n>>>> Light clients cannot easily be supported due to the need for scanning.\n>>>> The best we could do is give up on address reuse prevention (so we don\u2019t\n>>>> require the txid and index), only consider unspent taproot outputs, and\n>>>> download a standardized list of relevant input keys for each block over\n>>>> wifi each night when charging. These input keys can then be tweaked, and\n>>>> the results can be matched against compact block filters. Possible, but not\n>>>> simple.\n>>>>\n>>>>\n>>>> Effect on BIP32 HD keys\n>>>>\n>>>> One side-benefit of silent payments is that BIP32 HD keys[4] won\u2019t be\n>>>> needed for address generation, since every address will automatically be\n>>>> unique. This also means we won\u2019t have to deal with a gap limit.\n>>>>\n>>>>\n>>>> Different inputs\n>>>>\n>>>> While the simplest thing would be to only support one input type (e.g.\n>>>> taproot key spend), this would also mean only a subset of users can make\n>>>> payments to silent addresses, so this seems undesirable. The protocol\n>>>> should ideally support any input containing at least one public key, and\n>>>> simply pick the first key if more than one is present.\n>>>>\n>>>> Pay-to-(witness-)public-key-hash inputs actually end up being easiest\n>>>> to scan, since the public key is present in the input script, instead of\n>>>> the output script of the previous transaction (which requires one extra\n>>>> transaction lookup).\n>>>>\n>>>>\n>>>> Signature nonce instead of input key\n>>>>\n>>>> Another consideration was to tweak the silent payment address with the\n>>>> signature nonce[5], but unfortunately this breaks compatibility with MuSig2\n>>>> and MuSig-DN, since in those schemes the signature nonce changes depending\n>>>> on the transaction hash. If we let the output address depend on the nonce,\n>>>> then the transaction hash will change, causing a circular reference.\n>>>>\n>>>>\n>>>> Sending wallet compatibility\n>>>>\n>>>> Any wallet that wants to support making silent payments needs to\n>>>> support a new address format, pick inputs for the payment, tweak the silent\n>>>> payment address using the private key of one of the chosen inputs, and then\n>>>> proceed to sign the transaction. The scanning requirement is not relevant\n>>>> to the sender, only the recipient.\n>>>>\n>>>>\n>>>>\n>>>> PREVENTING INPUT LINKAGE\n>>>>\n>>>>\n>>>> A potential weakness of Silent Payments is that the input is linked to\n>>>> the output. A coinjoin transaction with multiple inputs from other users\n>>>> can normally obfuscate the sender input from the recipient, but Silent\n>>>> Payments reveal that link. This weakness can be mitigated with the \u201cvariant\n>>>> using all inputs\u201d, but this variant introduces a different weakness \u2013 you\n>>>> now require all other coinjoin users to tweak the silent payment address,\n>>>> which means you\u2019re revealing the intended recipient to them.\n>>>>\n>>>> Luckily, a blinding scheme[6] exists that allows us to hide the silent\n>>>> payment address from the other participants. Concretely, let\u2019s say there\n>>>> are two inputs, I1 and I2, and the latter one is ours. We add a secret\n>>>> blinding factor to the silent payment address, X + blinding_factor*G = X',\n>>>> then we receive X1' = i1*X' (together with a DLEQ to prove correctness, see\n>>>> full write-up[6]) from the owner of the first input and remove the blinding\n>>>> factor with X1' - blinding_factor*I1 = X1 (which is equal to i1*X).\n>>>> Finally, we calculate the tweaked address with hash(X1 + i2*X)*G + X. The\n>>>> recipient can simply recognize the payment with hash(x*(I1+I2))*G + X. Note\n>>>> that the owner of the first input cannot reconstruct the resulting address\n>>>> because they don\u2019t know i2*X.\n>>>>\n>>>> The blinding protocol above solves our coinjoin privacy concerns (at\n>>>> the expense of more interaction complexity), but we\u2019re left with one more\n>>>> issue \u2013 what if you want to make a silent payment, but you control none of\n>>>> the inputs (e.g. sending from an exchange)? In this scenario we can still\n>>>> utilize the blinding protocol, but now the third party sender can try to\n>>>> uncover the intended recipient by brute forcing their inputs on all known\n>>>> silent payment addresses (i.e. calculate hash(i*X)*G + X for every publicly\n>>>> known X). While this is computationally expensive, it\u2019s by no means\n>>>> impossible. No solution is known at this time, so as it stands this is a\n>>>> limitation of the protocol \u2013 the sender must control one of the inputs in\n>>>> order to be fully private.\n>>>>\n>>>>\n>>>>\n>>>> COMPARISON\n>>>>\n>>>>\n>>>> These are the most important protocols that provide similar\n>>>> functionality with slightly different tradeoffs. All of them provide fresh\n>>>> address generation and are compatible with one-time seed backups. The main\n>>>> benefits of the protocols listed below are that there is no scanning\n>>>> requirement, better light client support, and they don\u2019t require control\n>>>> over the inputs of the transaction.\n>>>>\n>>>>\n>>>> Payment code sharing\n>>>>\n>>>> This is BIP47[2]. An OP_RETURN message is sent on-chain to the\n>>>> recipient to establish a shared secret prior to making payments. Using the\n>>>> blockchain as a messaging layer like this is generally considered an\n>>>> inefficient use of on-chain resources. This concern can theoretically be\n>>>> alleviated by using other means of communicating, but data availability\n>>>> needs to be guaranteed to ensure the recipient doesn\u2019t lose access to the\n>>>> funds. Another concern is that the input(s) used to establish the shared\n>>>> secret may leak privacy if not kept separate.\n>>>>\n>>>>\n>>>> Xpub sharing\n>>>>\n>>>> Upon first payment, hand out an xpub instead of an address in order to\n>>>> enable repeat payments. I believe Kixunil\u2019s recently published scheme[3] is\n>>>> equivalent to this and could be implemented with relative ease. It\u2019s\n>>>> unclear how practical this protocol is, as it assumes sender and recipient\n>>>> are able to interact once, yet subsequent interaction is impossible.\n>>>>\n>>>>\n>>>> Regular address sharing\n>>>>\n>>>> This is how Bitcoin is commonly used today and may therefore be\n>>>> obvious, but it does satisfy similar privacy requirements. The sender\n>>>> interacts with the recipient each time they want to make a payment, and\n>>>> requests a new address. The main downside is that it requires interaction\n>>>> for every single payment.\n>>>>\n>>>>\n>>>>\n>>>> OPEN QUESTIONS\n>>>>\n>>>>\n>>>> Exactly how slow are the required database lookups? Is there a better\n>>>> approach?\n>>>>\n>>>> Is there any way to make light client support more viable?\n>>>>\n>>>> What is preferred \u2013 single input tweaking (revealing an input to the\n>>>> recipient) or using all inputs (increased coinjoin complexity)?\n>>>>\n>>>> Are there any security issues with the proposed cryptography?\n>>>>\n>>>> In general, compared to alternatives, is this scheme worth the added\n>>>> complexity?\n>>>>\n>>>>\n>>>>\n>>>> ACKNOWLEDGEMENTS\n>>>>\n>>>>\n>>>> Thanks to Kixunil, Calvin Kim, and Jonas Nick, holihawt and Lloyd\n>>>> Fournier for their help/comments, as well as all the authors of previous\n>>>> schemes. Any mistakes are my own.\n>>>>\n>>>>\n>>>>\n>>>> REFERENCES\n>>>>\n>>>>\n>>>> [1] Stealth Payments, Peter Todd:\n>>>> https://github.com/genjix/bips/blob/master/bip-stealth.mediawiki \u21a9\ufe0e\n>>>>\n>>>> [2] BIP47 payment codes, Justus Ranvier:\n>>>> https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n>>>>\n>>>> [3] Reusable taproot addresses, Kixunil:\n>>>> https://gist.github.com/Kixunil/0ddb3a9cdec33342b97431e438252c0a\n>>>>\n>>>> [4] BIP32 HD keys, Pieter Wuille:\n>>>> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\n>>>>\n>>>> [5] 2020-01-23 ##taproot-bip-review, starting at 18:25:\n>>>> https://gnusha.org/taproot-bip-review/2020-01-23.log\n>>>>\n>>>> [6] Blind Diffie-Hellman Key Exchange, David Wagner:\n>>>> https://gist.github.com/RubenSomsen/be7a4760dd4596d06963d67baf140406\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220330/5828c7e6/attachment-0001.html>"
            },
            {
                "author": "Ruben Somsen",
                "date": "2022-03-31T10:48:41",
                "message_text_only": "Hi Billy,\n\n>i*X*G\n\nI believe you understand this now, but just to be clear, it's not possible\nto multiply a point by another point. At best you can take the x coordinate\nof i*X and multiply that by G.\n\n>all this assumes that a modulus operator is defined for elliptic curve\npoints in a way that makes these valid, which I'm not sure is true\n\nI don't think I was 100% able to follow your math, but I assume your goal\nis to reduce the anonymity set by lowering the entropy using modulo. As you\nguessed, this won't work with curve points.\n\nI'm also not sure if we're on the same page with regards to my previous\npost: 1.) you can't reduce the scanning burden without also reducing the\nanonymity set, 2.) I'm hopeful the scanning requirement won't be so bad\nthat we'd need to consider this tradeoff, and 3.) I'm concerned that the\nimpact on anonymity is quite severe, even if you leak just a single bit and\ncut the anonymity set in half (e.g. you could figure out if a tx with a\nbunch of inputs are likely to originate from the same owner).\n\n>You can then scale N to the proper tradeoff between filter size and false\npositives\n\nYes, the nice thing is that every person who follows this protocol has to\nscan the exact same number of potential keys per block, so it should be\npossible to create a custom block filter with the exact optimal false\npositive rate.\n\nSo at a high level, the way I envision light clients working are as follows:\n- The server derives a list of public keys from each block (~9MB per 144\nblocks without cut-through)\n- The server also creates a block filter containing all taproot output keys\n(unsure what the size would be)\n- The client downloads both, performs Diffie-Hellman on the public keys,\nchecks each result with the filter, and downloads relevant blocks\n\nYou can find some more details about how this would work in one of my gist\ncomments:\nhttps://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8?permalink_comment_id=4113518#gistcomment-4113518\n\nCheers,\nRuben\n\n\n\n\n\nOn Wed, Mar 30, 2022 at 6:09 PM Billy <fresheneesz at gmail.com> wrote:\n\n> Hi Ruben,\n>\n> After sending that last night, I realized the solution I had to\n> deprivatizing the sender wouldn't work because it had the same problem of\n> even divisibility in modulo N. And my math was incomplete I think. Also\n> Marco D'Agostini pointed out other errors. And all this assumes that a\n> modulus operator is defined for elliptic curve points in a way that makes\n> these valid, which I'm not sure is true. But here's another try anyway:\n>\n> X' = X + i*X*hash((i*X)%N) =  X + x*I*hash((x*I)%N)\n>\n> item = {recipient: X' % N, sender: I%N} // As before.\n>\n> Test for each filter item: (item.recipient - X) % N == (\n> x*item.sender*hash((x*item.sender) % N) ) % N\n>\n> So to muse further about the properties of this, in a block full of\n> taproot sends you might have an upper limit of something like 13,000\n> transactions. N=2^8 would I think mean an 18% collision rate (ie 20% false\n> positive rate) because `(1-1/2^8)^13000 = 0.82...`. If we were to go with\n> that, each item is 4 bytes (1 byte per point component?) which would mean a\n> 52kb filter without collisions, and an average of 43kb with 18% collisions\n> (which can be removed as dupes). Maybe Golomb-Rice coding could help here\n> as well like it does in the usual compact block filters. And since each\n> collision with an address a client is watching on means downloading a whole\n> block they don't need, maybe 18% collisions is too high, and we want to\n> choose N = 2^10 or something to get down to 2% collisions.\n>\n> In any case, all this could be wrong if ECC modulus doesn't work this way.\n> But was interesting to think about anyway.\n>\n> On Wed, Mar 30, 2022 at 12:58 AM Billy <fresheneesz at gmail.com> wrote:\n>\n>> >  the sender can get in trouble too if they send money\n>>\n>> Good point.\n>>\n>> > how well this can be optimized without resorting to reducing anonymity\n>>\n>> Complete shot in the dark, but I wonder if something akin to compact\n>> block filters could be done to support this case. If, for example, the\n>> tweaked key were defined without hashing, I think something like that could\n>> be done:\n>>\n>> X'  =  i*X*G + X  =  x*I*G + X\n>>\n>> Your compact-block-filter-like things could then store a set of each\n>> `item = {recipient: X' % N, sender: I%N}`, and a light client would\n>> download this data and do the following to detect a likely payment for each\n>> filter item:\n>>\n>> item.recipient - X%N == x*item.sender*G\n>>\n>> You can then scale N to the proper tradeoff between filter size and false\n>> positives. I suppose this might make it possible to deprivitize a tweaked\n>> key by checking to see what non-tweaked keys evenly divide it. Perhaps\n>> that's what hashing was being used to solve. What if we added the shared\n>> diffie hellman secret modulo N to remove this correlation:\n>>\n>> X' = i*X*G + X + (i*X)%N =  x*I*G + X + (x*I)%N\n>>\n>> Then for each `item = {recipient: X' % N, sender: I%N}`, we detect via\n>> `item.recipient - X%N == x*item.sender*(1+G)`. Is my math right here?\n>> I'm thinking this should work because (a+b%N)%N == (a%N + b%N)%N.\n>>\n>>\n>>\n>> On Tue, Mar 29, 2022 at 10:36 AM Ruben Somsen <rsomsen at gmail.com> wrote:\n>>\n>>> Hi Billy,\n>>>\n>>> Thanks for taking a look.\n>>>\n>>> >Maybe it would have been more accurate to say no *extra* on chain\n>>> overhead\n>>>\n>>> I can see how it can be misinterpreted. I updated the gist to be more\n>>> specific.\n>>>\n>>> >primary benefit of this is privacy for the recipient\n>>>\n>>> Fair, but just wanted to note the sender can get in trouble too if they\n>>> send money to e.g. blacklisted addresses.\n>>>\n>>> >there could be a standard that [...] reduces the anonymity set a bit\n>>>\n>>> This has occurred to me but I am reluctant to make that trade-off. It\n>>> seems best to first see how well this can be optimized without resorting to\n>>> reducing anonymity, and it's hard to analyze exactly how impactful the\n>>> anonymity degradation is (I suspect it's worse than you think because it\n>>> can help strengthen existing heuristics about output ownership).\n>>>\n>>> Cheers,\n>>> Ruben\n>>>\n>>>\n>>>\n>>> On Tue, Mar 29, 2022 at 4:57 PM Billy <fresheneesz at gmail.com> wrote:\n>>>\n>>>> Hi Ruben,\n>>>>\n>>>> Very interesting protocol. This reminds me of how monero stealth\n>>>> addresses work, which gives monero the same downsides regarding light\n>>>> clients (among other things). I was a bit confused by the following:\n>>>>\n>>>> > without requiring any interaction or on-chain overhead\n>>>>\n>>>> After reading through, I have to assume it was rather misleading to say\n>>>> \"no on-chain overhead\". This still requires an on-chain transaction to be\n>>>> sent to the tweaked address, I believe. Maybe it would have been more\n>>>> accurate to say no *extra* on chain overhead (over a normal transaction)?\n>>>>\n>>>> It seems the primary benefit of this is privacy for the recipient. To\n>>>> that end, it seems like a pretty useful protocol. It's definitely a level\n>>>> of privacy one would only care about if they might receive a lot money\n>>>> related to that address. However of course someone might not know they'll\n>>>> receive an amount of money they want to be private until they receive it.\n>>>> So the inability to easily do this without a full node is slightly less\n>>>> than ideal. But it's another good reason to run a full node.\n>>>>\n>>>> Perhaps there could be a standard that can identify tweaked address,\n>>>> such that only those addresses can be downloaded and checked by light\n>>>> clients. It reduces the anonymity set a bit, but it would probably still be\n>>>> sufficient.\n>>>>\n>>>>\n>>>>\n>>>> On Mon, Mar 28, 2022, 10:29 Ruben Somsen via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>>> Hi all,\n>>>>>\n>>>>> I'm publishing a new scheme for private non-interactive address\n>>>>> generation without on-chain overhead. It has upsides as well as downsides,\n>>>>> so I suspect the main discussion will revolve around whether this is worth\n>>>>> pursuing or not. There is a list of open questions at the end.\n>>>>>\n>>>>> I added the full write-up in plain text below, though I recommend\n>>>>> reading the gist for improved formatting and in order to benefit from\n>>>>> potential future edits:\n>>>>> https://gist.github.com/RubenSomsen/c43b79517e7cb701ebf77eec6dbb46b8\n>>>>>\n>>>>> Cheers,\n>>>>> Ruben\n>>>>>\n>>>>>\n>>>>>\n>>>>> Silent Payments\n>>>>>\n>>>>> Receive private payments from anyone on a single static address\n>>>>> without requiring any interaction or on-chain overhead\n>>>>>\n>>>>>\n>>>>>\n>>>>> OVERVIEW\n>>>>>\n>>>>>\n>>>>> The recipient generates a so-called silent payment address and makes\n>>>>> it publicly known. The sender then takes a public key from one of their\n>>>>> chosen inputs for the payment, and uses it to derive a shared secret that\n>>>>> is then used to tweak the silent payment address. The recipient detects the\n>>>>> payment by scanning every transaction in the blockchain.\n>>>>>\n>>>>> Compared to previous schemes[1], this scheme avoids using the Bitcoin\n>>>>> blockchain as a messaging layer[2] and requires no interaction between\n>>>>> sender and recipient[3] (other than needing to know the silent payment\n>>>>> address). The main downsides are the scanning requirement, the lack of\n>>>>> light client support, and the requirement to control your own input(s). An\n>>>>> example use case would be private one-time donations.\n>>>>>\n>>>>> While most of the individual parts of this idea aren\u2019t novel, the\n>>>>> resulting protocol has never been seriously considered and may be\n>>>>> reasonably viable, particularly if we limit ourselves to detecting only\n>>>>> unspent payments by scanning the UTXO set. We\u2019ll start by describing a\n>>>>> basic scheme, and then introduce a few improvements.\n>>>>>\n>>>>>\n>>>>>\n>>>>> BASIC SCHEME\n>>>>>\n>>>>>\n>>>>> The recipient publishes their silent payment address, a single 32 byte\n>>>>> public key:\n>>>>> X = x*G\n>>>>>\n>>>>> The sender picks an input containing a public key:\n>>>>> I = i*G\n>>>>>\n>>>>> The sender tweaks the silent payment address with the public key of\n>>>>> their input:\n>>>>> X' = hash(i*X)*G + X\n>>>>>\n>>>>> Since i*X == x*I (Diffie-Hellman Key Exchange), the recipient can\n>>>>> detect the payment by calculating hash(x*I)*G + X for each input key I in\n>>>>> the blockchain and seeing if it matches an output in the corresponding\n>>>>> transaction.\n>>>>>\n>>>>>\n>>>>>\n>>>>> IMPROVEMENTS\n>>>>>\n>>>>>\n>>>>> UTXO set scanning\n>>>>>\n>>>>> If we forgo detection of historic transactions and only focus on the\n>>>>> current balance, we can limit the protocol to only scanning the\n>>>>> transactions that are part of the UTXO set when restoring from backup,\n>>>>> which may be faster.\n>>>>>\n>>>>> Jonas Nick was kind enough to go through the numbers and run a\n>>>>> benchmark of hash(x*I)*G + X on his 3.9GHz Intel\u00ae Core\u2122 i7-7820HQ CPU,\n>>>>> which took roughly 72 microseconds per calculation on a single core. The\n>>>>> UTXO set currently has 80 million entries, the average transaction has 2.3\n>>>>> inputs, which puts us at 2.3*80000000*72/1000/1000/60 = 221 minutes for a\n>>>>> single core (under 2 hours for two cores).\n>>>>>\n>>>>> What these numbers do not take into account is database lookups. We\n>>>>> need to fetch the transaction of every UTXO, as well as every transaction\n>>>>> for every subsequent input in order to extract the relevant public key,\n>>>>> resulting in (1+2.3)*80000000 = 264 million lookups. How slow this is and\n>>>>> what can be done to improve it is an open question.\n>>>>>\n>>>>> Once we\u2019re at the tip, every new unspent output will have to be\n>>>>> scanned. It\u2019s theoretically possible to scan e.g. once a day and skip\n>>>>> transactions with fully spent outputs, but that would probably not be worth\n>>>>> the added complexity. If we only scan transactions with taproot outputs, we\n>>>>> can further limit our efforts, but this advantage is expected to dissipate\n>>>>> once taproot use becomes more common.\n>>>>>\n>>>>>\n>>>>> Variant using all inputs\n>>>>>\n>>>>> Instead of tweaking the silent payment address with one input, we\n>>>>> could instead tweak it with the combination of all input keys of a\n>>>>> transaction. The benefit is that this further lowers the scanning cost,\n>>>>> since now we only need to calculate one tweak per transaction, instead of\n>>>>> one tweak per input, which is roughly half the work, though database\n>>>>> lookups remain unaffected.\n>>>>>\n>>>>> The downside is that if you want to combine your inputs with those of\n>>>>> others (i.e. coinjoin), every participant has to be willing to assist you\n>>>>> in following the Silent Payment protocol in order to let you make your\n>>>>> payment. There are also privacy considerations which are discussed in the\n>>>>> \u201cPreventing input linkage\u201d section.\n>>>>>\n>>>>> Concretely, if there are three inputs (I1, I2, I3), the scheme\n>>>>> becomes: hash(i1*X + i2*X + i3*X)*G + X == hash(x*(I1+I2+I3))*G + X.\n>>>>>\n>>>>>\n>>>>> Scanning key\n>>>>>\n>>>>> We can extend the silent payment address with a scanning key, which\n>>>>> allows for separation of detecting and spending payments. We redefine the\n>>>>> silent payment address as the concatenation of X_scan, X_spend, and\n>>>>> derivation becomes X' = hash(i*X_scan)*G + X_spend. This allows your\n>>>>> internet-connected node to hold the private key of X_scan to detect\n>>>>> incoming payments, while your hardware wallet controls X_spend to make\n>>>>> payments. If X_scan is compromised, privacy is lost, but your funds are not.\n>>>>>\n>>>>>\n>>>>> Address reuse prevention\n>>>>>\n>>>>> If the sender sends more than one payment, and the chosen input has\n>>>>> the same key due to address reuse, then the recipient address will also be\n>>>>> the same. To prevent this, we can hash the txid and index of the input, to\n>>>>> ensure each address is unique, resulting in X' = hash(i*X,txid,index)*G +\n>>>>> X. Note this would make light client support harder.\n>>>>>\n>>>>>\n>>>>>\n>>>>> NOTEWORTHY DETAILS\n>>>>>\n>>>>>\n>>>>> Light clients\n>>>>>\n>>>>> Light clients cannot easily be supported due to the need for scanning.\n>>>>> The best we could do is give up on address reuse prevention (so we don\u2019t\n>>>>> require the txid and index), only consider unspent taproot outputs, and\n>>>>> download a standardized list of relevant input keys for each block over\n>>>>> wifi each night when charging. These input keys can then be tweaked, and\n>>>>> the results can be matched against compact block filters. Possible, but not\n>>>>> simple.\n>>>>>\n>>>>>\n>>>>> Effect on BIP32 HD keys\n>>>>>\n>>>>> One side-benefit of silent payments is that BIP32 HD keys[4] won\u2019t be\n>>>>> needed for address generation, since every address will automatically be\n>>>>> unique. This also means we won\u2019t have to deal with a gap limit.\n>>>>>\n>>>>>\n>>>>> Different inputs\n>>>>>\n>>>>> While the simplest thing would be to only support one input type (e.g.\n>>>>> taproot key spend), this would also mean only a subset of users can make\n>>>>> payments to silent addresses, so this seems undesirable. The protocol\n>>>>> should ideally support any input containing at least one public key, and\n>>>>> simply pick the first key if more than one is present.\n>>>>>\n>>>>> Pay-to-(witness-)public-key-hash inputs actually end up being easiest\n>>>>> to scan, since the public key is present in the input script, instead of\n>>>>> the output script of the previous transaction (which requires one extra\n>>>>> transaction lookup).\n>>>>>\n>>>>>\n>>>>> Signature nonce instead of input key\n>>>>>\n>>>>> Another consideration was to tweak the silent payment address with the\n>>>>> signature nonce[5], but unfortunately this breaks compatibility with MuSig2\n>>>>> and MuSig-DN, since in those schemes the signature nonce changes depending\n>>>>> on the transaction hash. If we let the output address depend on the nonce,\n>>>>> then the transaction hash will change, causing a circular reference.\n>>>>>\n>>>>>\n>>>>> Sending wallet compatibility\n>>>>>\n>>>>> Any wallet that wants to support making silent payments needs to\n>>>>> support a new address format, pick inputs for the payment, tweak the silent\n>>>>> payment address using the private key of one of the chosen inputs, and then\n>>>>> proceed to sign the transaction. The scanning requirement is not relevant\n>>>>> to the sender, only the recipient.\n>>>>>\n>>>>>\n>>>>>\n>>>>> PREVENTING INPUT LINKAGE\n>>>>>\n>>>>>\n>>>>> A potential weakness of Silent Payments is that the input is linked to\n>>>>> the output. A coinjoin transaction with multiple inputs from other users\n>>>>> can normally obfuscate the sender input from the recipient, but Silent\n>>>>> Payments reveal that link. This weakness can be mitigated with the \u201cvariant\n>>>>> using all inputs\u201d, but this variant introduces a different weakness \u2013 you\n>>>>> now require all other coinjoin users to tweak the silent payment address,\n>>>>> which means you\u2019re revealing the intended recipient to them.\n>>>>>\n>>>>> Luckily, a blinding scheme[6] exists that allows us to hide the silent\n>>>>> payment address from the other participants. Concretely, let\u2019s say there\n>>>>> are two inputs, I1 and I2, and the latter one is ours. We add a secret\n>>>>> blinding factor to the silent payment address, X + blinding_factor*G = X',\n>>>>> then we receive X1' = i1*X' (together with a DLEQ to prove correctness, see\n>>>>> full write-up[6]) from the owner of the first input and remove the blinding\n>>>>> factor with X1' - blinding_factor*I1 = X1 (which is equal to i1*X).\n>>>>> Finally, we calculate the tweaked address with hash(X1 + i2*X)*G + X. The\n>>>>> recipient can simply recognize the payment with hash(x*(I1+I2))*G + X. Note\n>>>>> that the owner of the first input cannot reconstruct the resulting address\n>>>>> because they don\u2019t know i2*X.\n>>>>>\n>>>>> The blinding protocol above solves our coinjoin privacy concerns (at\n>>>>> the expense of more interaction complexity), but we\u2019re left with one more\n>>>>> issue \u2013 what if you want to make a silent payment, but you control none of\n>>>>> the inputs (e.g. sending from an exchange)? In this scenario we can still\n>>>>> utilize the blinding protocol, but now the third party sender can try to\n>>>>> uncover the intended recipient by brute forcing their inputs on all known\n>>>>> silent payment addresses (i.e. calculate hash(i*X)*G + X for every publicly\n>>>>> known X). While this is computationally expensive, it\u2019s by no means\n>>>>> impossible. No solution is known at this time, so as it stands this is a\n>>>>> limitation of the protocol \u2013 the sender must control one of the inputs in\n>>>>> order to be fully private.\n>>>>>\n>>>>>\n>>>>>\n>>>>> COMPARISON\n>>>>>\n>>>>>\n>>>>> These are the most important protocols that provide similar\n>>>>> functionality with slightly different tradeoffs. All of them provide fresh\n>>>>> address generation and are compatible with one-time seed backups. The main\n>>>>> benefits of the protocols listed below are that there is no scanning\n>>>>> requirement, better light client support, and they don\u2019t require control\n>>>>> over the inputs of the transaction.\n>>>>>\n>>>>>\n>>>>> Payment code sharing\n>>>>>\n>>>>> This is BIP47[2]. An OP_RETURN message is sent on-chain to the\n>>>>> recipient to establish a shared secret prior to making payments. Using the\n>>>>> blockchain as a messaging layer like this is generally considered an\n>>>>> inefficient use of on-chain resources. This concern can theoretically be\n>>>>> alleviated by using other means of communicating, but data availability\n>>>>> needs to be guaranteed to ensure the recipient doesn\u2019t lose access to the\n>>>>> funds. Another concern is that the input(s) used to establish the shared\n>>>>> secret may leak privacy if not kept separate.\n>>>>>\n>>>>>\n>>>>> Xpub sharing\n>>>>>\n>>>>> Upon first payment, hand out an xpub instead of an address in order to\n>>>>> enable repeat payments. I believe Kixunil\u2019s recently published scheme[3] is\n>>>>> equivalent to this and could be implemented with relative ease. It\u2019s\n>>>>> unclear how practical this protocol is, as it assumes sender and recipient\n>>>>> are able to interact once, yet subsequent interaction is impossible.\n>>>>>\n>>>>>\n>>>>> Regular address sharing\n>>>>>\n>>>>> This is how Bitcoin is commonly used today and may therefore be\n>>>>> obvious, but it does satisfy similar privacy requirements. The sender\n>>>>> interacts with the recipient each time they want to make a payment, and\n>>>>> requests a new address. The main downside is that it requires interaction\n>>>>> for every single payment.\n>>>>>\n>>>>>\n>>>>>\n>>>>> OPEN QUESTIONS\n>>>>>\n>>>>>\n>>>>> Exactly how slow are the required database lookups? Is there a better\n>>>>> approach?\n>>>>>\n>>>>> Is there any way to make light client support more viable?\n>>>>>\n>>>>> What is preferred \u2013 single input tweaking (revealing an input to the\n>>>>> recipient) or using all inputs (increased coinjoin complexity)?\n>>>>>\n>>>>> Are there any security issues with the proposed cryptography?\n>>>>>\n>>>>> In general, compared to alternatives, is this scheme worth the added\n>>>>> complexity?\n>>>>>\n>>>>>\n>>>>>\n>>>>> ACKNOWLEDGEMENTS\n>>>>>\n>>>>>\n>>>>> Thanks to Kixunil, Calvin Kim, and Jonas Nick, holihawt and Lloyd\n>>>>> Fournier for their help/comments, as well as all the authors of previous\n>>>>> schemes. Any mistakes are my own.\n>>>>>\n>>>>>\n>>>>>\n>>>>> REFERENCES\n>>>>>\n>>>>>\n>>>>> [1] Stealth Payments, Peter Todd:\n>>>>> https://github.com/genjix/bips/blob/master/bip-stealth.mediawiki \u21a9\ufe0e\n>>>>>\n>>>>> [2] BIP47 payment codes, Justus Ranvier:\n>>>>> https://github.com/bitcoin/bips/blob/master/bip-0047.mediawiki\n>>>>>\n>>>>> [3] Reusable taproot addresses, Kixunil:\n>>>>> https://gist.github.com/Kixunil/0ddb3a9cdec33342b97431e438252c0a\n>>>>>\n>>>>> [4] BIP32 HD keys, Pieter Wuille:\n>>>>> https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki\n>>>>>\n>>>>> [5] 2020-01-23 ##taproot-bip-review, starting at 18:25:\n>>>>> https://gnusha.org/taproot-bip-review/2020-01-23.log\n>>>>>\n>>>>> [6] Blind Diffie-Hellman Key Exchange, David Wagner:\n>>>>> https://gist.github.com/RubenSomsen/be7a4760dd4596d06963d67baf140406\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20220331/1b94d559/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Silent Payments \u2013 Non-interactive private payments with no on-chain overhead",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Billy",
                "Ruben Somsen"
            ],
            "messages_count": 6,
            "total_messages_chars_count": 103982
        }
    },
    {
        "title": "[bitcoin-dev] WabiSabi P2EP / Wormhole 2.0",
        "thread_messages": [
            {
                "author": "Max Hillebrand",
                "date": "2022-03-30T11:57:47",
                "message_text_only": "Hello List,\n\ntl;dr, users of WabiSabi coinjoin can pay arbitrary amounts of bitcoin, \nso that the sender does not learn the address/output of the receiver, \nand the receiver does not learn the input of the sender. This improves \nthe previously proposed 'Wormhole' for Chaumian blind signature \ncoinjoin, by allowing arbitrary amount payments and by reducing block \nspace for change decomposition. \nhttps://www.mail-archive.com/bitcoin-dev@lists.linuxfoundation.org/msg08622.html\n\n\nAssume that the sender and the receiver are both online and have a \ndirect communication channel [P2EP]. The sender registers an input \n[let's say 1 btc value] with a third-party WabiSabi coordinator. In \nexchange, the sender receives a keyed-verified-anonymous-credential. The \nsender can present the 1 btc credential, and request a reissuance of two \nnew credentials worth for example 0.3 btc and 0.7 btc. Since Pedersen \ncommitments are the attributes of the KVAC, the coordinator does not \nlearn any of those amounts.\n\nNext, the sender gives the receiver through the P2EP connection the KVAC \ncorresponding to the amount that is due pay. Now the receiver presents \nthis credential to the coordinator, and requests a reissuance, which can \nagain be split up into two credentials, for example 0.1 and 0.2. At this \npoint, the sender can no longer present the old credential, the \ncoordinator ensures double spending protection.\n\nLater during output registration, the sender registers with the \ncoordinator his \"payment change outputs\", which again can be decomposed \nclient side into multiple outputs, let's say 0.5 and 0.2. Likewise, the \nreceiver presents his KVACs, and registers his desired output addresses \ndirectly with the coordinator.\n\nAfter output registration, the coordinator aggregates the PSBT with all \nregistered inputs and outputs, and presents the unsigned coinjoin \ntransaction to all Alices [Tor identities who registered inputs]. Since \nthe sender does not know what the receivers output address is, he has to \nask the receiver through the P2EP connection if this coinjoin is good. \nIf response is ACK, then the sender signs for his inputs and registers \nthe signatures with the coordinator.\n\nIf all inputs sign, we have a successful coinjoin, which includes a \npayment, where the sender never learns the address of the receiver, and \nthe receiver never learns the inputs or change outputs of the sender. \nThe coordinator can not differentiate users who make self-spends from \nthose who do payments, this is entirely client side.\n\n\nOf course the sender still knows the amount of bitcoin of the credential \nthat he passed on to the receiver [the invoiced payment amount]. \nHowever, similar to PayJoin, the receiver can likewise register inputs \nwith the coordinator in the same round. Unlike PayJoin, there are many \nother inputs who do not belong to sender or receiver, which provides the \ndesired anonymity set. Such a receiver will have the KVAC originated \nfrom his own input[s], as well as the KVAC that the sender gave him. Two \nKVACs can be reissued to one, thus anonymous consolidation of inputs + \npayment amount is possible. Since neither the coordinator, nor the \nsender, know the input[s] of the receiver, the final amount on-chain \n[even if only one receiver output is created] does not correspond to the \npayment amount, thus the sender can not identify the output of the \nreceiver based on amounts.\n\n\nA blinded coinjoin coordinator is a PSBT whiteboard, where users \npurchase eCash tokens by registering inputs, and users spend eCash \ntokens in order to register outputs. Users can self-spend the eCash to \nincrease anonymity set of those access rights. However, nothing prevents \nthe user to make an actual eCash \"payment\" to someone else, effectively \nabdicating the right to register outputs. If [and only if] the final \ncoinjoin has sufficient number of inputs and outputs to provide \neffective blockchain ambiguity, then the resulting payment has \nbreathtaking privacy guarantees.\n\n\nSkol\nMax Hillebrand"
            }
        ],
        "thread_summary": {
            "title": "WabiSabi P2EP / Wormhole 2.0",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Max Hillebrand"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 4025
        }
    }
]