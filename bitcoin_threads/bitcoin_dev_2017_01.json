[
    {
        "title": "[bitcoin-dev] Committed bloom filters for improved wallet performance and SPV security",
        "thread_messages": [
            {
                "author": "Jonas Schnelli",
                "date": "2017-01-01T21:01:23",
                "message_text_only": "Hi\n> We introduce several concepts that rework the lightweight Bitcoin\n> client model in a manner which is secure, efficient and privacy\n> compatible.\n>\n> The BFD can be used verbatim in replacement of BIP37, where the filter\n> can be cached between clients without needing to be recomputed. It can\n> also be used by normal pruned nodes to do re-scans locally of their\n> wallet without needing to have the block data available to scan, or\n> without reading the entire block chain from disk.\nI started exploring the potential of BFD after this specification.\n\nWhat would be the preferred/recommended way to handle 0-conf/mempool\nfiltering \u2013 if & once BDF would have been deployed (any type,\nsemi-trusted oracles or protocol-level/softfork)?\n\nFrom the user-experience perspective, this is probably pretty important\n(otherwise the experience will be that incoming funds can take serval\nminutes to hours until they appear).\nUsing BIP37 bloom filters just for mempool filtering would obviously\nresult in the same unwanted privacy-setup.\n\n</jonas>\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170101/91d3f504/attachment.sig>"
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-01-03T20:18:59",
                "message_text_only": "The concept combined with the weak blocks system where miners commit\nto potential transaction inclusion with fractional difficulty blocks\nis possible. I'm not personally convinced that unconfirmed transaction\ndisplay in a wallet is worth the privacy trade-off. The user has very\nlittle to gain from this knowledge until the txn is in a block.\n\n\nOn 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n> Hi\n>> We introduce several concepts that rework the lightweight Bitcoin\n>> client model in a manner which is secure, efficient and privacy\n>> compatible.\n>> \n>> The BFD can be used verbatim in replacement of BIP37, where the filter\n>> can be cached between clients without needing to be recomputed. It can\n>> also be used by normal pruned nodes to do re-scans locally of their\n>> wallet without needing to have the block data available to scan, or\n>> without reading the entire block chain from disk.\n> I started exploring the potential of BFD after this specification.\n> \n> What would be the preferred/recommended way to handle 0-conf/mempool\n> filtering \u2013 if & once BDF would have been deployed (any type,\n> semi-trusted oracles or protocol-level/softfork)?\n> \n> From the user-experience perspective, this is probably pretty important\n> (otherwise the experience will be that incoming funds can take serval\n> minutes to hours until they appear).\n> Using BIP37 bloom filters just for mempool filtering would obviously\n> result in the same unwanted privacy-setup.\n> \n> </jonas>\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "Aaron Voisine",
                "date": "2017-01-03T22:18:21",
                "message_text_only": "Unconfirmed transactions are incredibly important for real world use.\nMerchants for instance are willing to accept credit card payments of\nthousands of dollars and ship the goods despite the fact that the\ntransaction can be reversed up to 60 days later. There is a very large cost\nto losing the ability to have instant transactions in many or even most\nsituations. This cost is typically well above the fraud risk.\n\nIt's important to recognize that bitcoin serves a wide variety of use cases\nwith different profiles for time sensitivity and fraud risk.\n\nAaron\n\nOn Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> The concept combined with the weak blocks system where miners commit\n>\n> to potential transaction inclusion with fractional difficulty blocks\n>\n> is possible. I'm not personally convinced that unconfirmed transaction\n>\n> display in a wallet is worth the privacy trade-off. The user has very\n>\n> little to gain from this knowledge until the txn is in a block.\n>\n>\n>\n>\n>\n> On 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n>\n> > Hi\n>\n> >> We introduce several concepts that rework the lightweight Bitcoin\n>\n> >> client model in a manner which is secure, efficient and privacy\n>\n> >> compatible.\n>\n> >>\n>\n> >> The BFD can be used verbatim in replacement of BIP37, where the filter\n>\n> >> can be cached between clients without needing to be recomputed. It can\n>\n> >> also be used by normal pruned nodes to do re-scans locally of their\n>\n> >> wallet without needing to have the block data available to scan, or\n>\n> >> without reading the entire block chain from disk.\n>\n> > I started exploring the potential of BFD after this specification.\n>\n> >\n>\n> > What would be the preferred/recommended way to handle 0-conf/mempool\n>\n> > filtering \u2013 if & once BDF would have been deployed (any type,\n>\n> > semi-trusted oracles or protocol-level/softfork)?\n>\n> >\n>\n> > From the user-experience perspective, this is probably pretty important\n>\n> > (otherwise the experience will be that incoming funds can take serval\n>\n> > minutes to hours until they appear).\n>\n> > Using BIP37 bloom filters just for mempool filtering would obviously\n>\n> > result in the same unwanted privacy-setup.\n>\n> >\n>\n> > </jonas>\n>\n> >\n>\n> >\n>\n> > _______________________________________________\n>\n> > bitcoin-dev mailing list\n>\n> > bitcoin-dev at lists.linuxfoundation.org\n>\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n> _______________________________________________\n>\n> bitcoin-dev mailing list\n>\n> bitcoin-dev at lists.linuxfoundation.org\n>\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/56eb8944/attachment-0001.html>"
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-01-03T22:28:56",
                "message_text_only": "The concept was not particularly targeted towards businesses, but\nallowing for significantly improved wallet performance and reducing\nprivacy for lite clients. You would expect that a business has the\ncapacity to run a fully validating, fully storing node of their own.\nIf they\u2019re not something is fundamentally broken with Bitcoin, or\ntheir rationale of continuing to use it.\n\n\nOn 2017-01-03 14:18, Aaron Voisine wrote:\n> Unconfirmed transactions are incredibly important for real world use.\n> Merchants for instance are willing to accept credit card payments of\n> thousands of dollars and ship the goods despite the fact that the\n> transaction can be reversed up to 60 days later. There is a very large\n> cost to losing the ability to have instant transactions in many or\n> even most situations. This cost is typically well above the fraud\n> risk.\n> \n> It's important to recognize that bitcoin serves a wide variety of use\n> cases with different profiles for time sensitivity and fraud risk.\n> \n> Aaron\n> \n> On Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> The concept combined with the weak blocks system where miners commit\n>> \n>> to potential transaction inclusion with fractional difficulty blocks\n>> \n>> is possible. I'm not personally convinced that unconfirmed\n>> transaction\n>> \n>> display in a wallet is worth the privacy trade-off. The user has\n>> very\n>> \n>> little to gain from this knowledge until the txn is in a block.\n>> \n>> On 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n>> \n>>> Hi\n>> \n>>>> We introduce several concepts that rework the lightweight Bitcoin\n>> \n>>>> client model in a manner which is secure, efficient and privacy\n>> \n>>>> compatible.\n>> \n>>>> \n>> \n>>>> The BFD can be used verbatim in replacement of BIP37, where the\n>> filter\n>> \n>>>> can be cached between clients without needing to be recomputed.\n>> It can\n>> \n>>>> also be used by normal pruned nodes to do re-scans locally of\n>> their\n>> \n>>>> wallet without needing to have the block data available to scan,\n>> or\n>> \n>>>> without reading the entire block chain from disk.\n>> \n>>> I started exploring the potential of BFD after this specification.\n>> \n>>> \n>> \n>>> What would be the preferred/recommended way to handle\n>> 0-conf/mempool\n>> \n>>> filtering \u2013 if & once BDF would have been deployed (any type,\n>> \n>>> semi-trusted oracles or protocol-level/softfork)?\n>> \n>>> \n>> \n>>> From the user-experience perspective, this is probably pretty\n>> important\n>> \n>>> (otherwise the experience will be that incoming funds can take\n>> serval\n>> \n>>> minutes to hours until they appear).\n>> \n>>> Using BIP37 bloom filters just for mempool filtering would\n>> obviously\n>> \n>>> result in the same unwanted privacy-setup.\n>> \n>>> \n>> \n>>> </jonas>\n>> \n>>> \n>> \n>>> \n>> \n>>> _______________________________________________\n>> \n>>> bitcoin-dev mailing list\n>> \n>>> bitcoin-dev at lists.linuxfoundation.org\n>> \n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> \n>> _______________________________________________\n>> \n>> bitcoin-dev mailing list\n>> \n>> bitcoin-dev at lists.linuxfoundation.org\n>> \n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "adiabat",
                "date": "2017-01-03T23:06:26",
                "message_text_only": "Mempool transactions have their place, but \"unconfirmed\" and \"SPV\" don't\nbelong together.  Only a full node can tell if a transaction may get\nconfirmed, or is nonsense.  Unfortunately all the light / SPV wallets I\nknow of show mempool transactions, which makes it hard to go back... (e.g.\n\"why doesn't your software show 0-conf! your wallet is broken!\", somewhat\nakin to people complaining about RBF)\n\nSo, this is easy, just don't worry about mempool filtering.  Why are light\nclients looking at the mempool anyway?  Maybe if there were some way to\nprovide SPV proofs of all inputs, but that's a bit of a mess for full nodes\nto do.\n\nWithout mempool filtering, I think the committed bloom filters would be a\ngreat improvement over the current bloom filter setup, especially for\nlightning network use cases (with lightning, not finding out about a\ntransaction can make you lose money).  I want to work on it and may be able\nto at some point as it's somewhat related to lightning.\n\nAlso, if you're running a light client, and storing the filters the way you\nstore block headers, there's really no reason to go all the way back to\nheight 0.  You can start grabbing headers at some point a while ago, before\nyour set of keys was generated.  I think it'd be very worth it even with\nGB-scale disk usage.\n\n-Tadge\n\n\nOn Tue, Jan 3, 2017 at 5:18 PM, Aaron Voisine via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Unconfirmed transactions are incredibly important for real world use.\n> Merchants for instance are willing to accept credit card payments of\n> thousands of dollars and ship the goods despite the fact that the\n> transaction can be reversed up to 60 days later. There is a very large cost\n> to losing the ability to have instant transactions in many or even most\n> situations. This cost is typically well above the fraud risk.\n>\n> It's important to recognize that bitcoin serves a wide variety of use\n> cases with different profiles for time sensitivity and fraud risk.\n>\n> Aaron\n>\n> On Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev <bitcoin-dev at lists.\n> linuxfoundation.org> wrote:\n>\n>> The concept combined with the weak blocks system where miners commit\n>>\n>> to potential transaction inclusion with fractional difficulty blocks\n>>\n>> is possible. I'm not personally convinced that unconfirmed transaction\n>>\n>> display in a wallet is worth the privacy trade-off. The user has very\n>>\n>> little to gain from this knowledge until the txn is in a block.\n>>\n>>\n>>\n>>\n>>\n>> On 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n>>\n>> > Hi\n>>\n>> >> We introduce several concepts that rework the lightweight Bitcoin\n>>\n>> >> client model in a manner which is secure, efficient and privacy\n>>\n>> >> compatible.\n>>\n>> >>\n>>\n>> >> The BFD can be used verbatim in replacement of BIP37, where the filter\n>>\n>> >> can be cached between clients without needing to be recomputed. It can\n>>\n>> >> also be used by normal pruned nodes to do re-scans locally of their\n>>\n>> >> wallet without needing to have the block data available to scan, or\n>>\n>> >> without reading the entire block chain from disk.\n>>\n>> > I started exploring the potential of BFD after this specification.\n>>\n>> >\n>>\n>> > What would be the preferred/recommended way to handle 0-conf/mempool\n>>\n>> > filtering \u2013 if & once BDF would have been deployed (any type,\n>>\n>> > semi-trusted oracles or protocol-level/softfork)?\n>>\n>> >\n>>\n>> > From the user-experience perspective, this is probably pretty important\n>>\n>> > (otherwise the experience will be that incoming funds can take serval\n>>\n>> > minutes to hours until they appear).\n>>\n>> > Using BIP37 bloom filters just for mempool filtering would obviously\n>>\n>> > result in the same unwanted privacy-setup.\n>>\n>> >\n>>\n>> > </jonas>\n>>\n>> >\n>>\n>> >\n>>\n>> > _______________________________________________\n>>\n>> > bitcoin-dev mailing list\n>>\n>> > bitcoin-dev at lists.linuxfoundation.org\n>>\n>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>> _______________________________________________\n>>\n>> bitcoin-dev mailing list\n>>\n>> bitcoin-dev at lists.linuxfoundation.org\n>>\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/399f0f9b/attachment.html>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2017-01-03T23:46:00",
                "message_text_only": "If the sender doesn't control the receiver's network connection, then the\ninformation the receiver gains by watching the mempool is if the\ntransaction has propagated across the bitcoin network. This is useful to\nknow in all kinds of situations.\n\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com>\n\nOn Tue, Jan 3, 2017 at 3:06 PM, adiabat <rx at awsomnet.org> wrote:\n\n> Mempool transactions have their place, but \"unconfirmed\" and \"SPV\" don't\n> belong together.  Only a full node can tell if a transaction may get\n> confirmed, or is nonsense.  Unfortunately all the light / SPV wallets I\n> know of show mempool transactions, which makes it hard to go back... (e.g.\n> \"why doesn't your software show 0-conf! your wallet is broken!\", somewhat\n> akin to people complaining about RBF)\n>\n> So, this is easy, just don't worry about mempool filtering.  Why are light\n> clients looking at the mempool anyway?  Maybe if there were some way to\n> provide SPV proofs of all inputs, but that's a bit of a mess for full nodes\n> to do.\n>\n> Without mempool filtering, I think the committed bloom filters would be a\n> great improvement over the current bloom filter setup, especially for\n> lightning network use cases (with lightning, not finding out about a\n> transaction can make you lose money).  I want to work on it and may be able\n> to at some point as it's somewhat related to lightning.\n>\n> Also, if you're running a light client, and storing the filters the way\n> you store block headers, there's really no reason to go all the way back to\n> height 0.  You can start grabbing headers at some point a while ago, before\n> your set of keys was generated.  I think it'd be very worth it even with\n> GB-scale disk usage.\n>\n> -Tadge\n>\n>\n> On Tue, Jan 3, 2017 at 5:18 PM, Aaron Voisine via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Unconfirmed transactions are incredibly important for real world use.\n>> Merchants for instance are willing to accept credit card payments of\n>> thousands of dollars and ship the goods despite the fact that the\n>> transaction can be reversed up to 60 days later. There is a very large cost\n>> to losing the ability to have instant transactions in many or even most\n>> situations. This cost is typically well above the fraud risk.\n>>\n>> It's important to recognize that bitcoin serves a wide variety of use\n>> cases with different profiles for time sensitivity and fraud risk.\n>>\n>> Aaron\n>>\n>> On Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> The concept combined with the weak blocks system where miners commit\n>>>\n>>> to potential transaction inclusion with fractional difficulty blocks\n>>>\n>>> is possible. I'm not personally convinced that unconfirmed transaction\n>>>\n>>> display in a wallet is worth the privacy trade-off. The user has very\n>>>\n>>> little to gain from this knowledge until the txn is in a block.\n>>>\n>>>\n>>>\n>>>\n>>>\n>>> On 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n>>>\n>>> > Hi\n>>>\n>>> >> We introduce several concepts that rework the lightweight Bitcoin\n>>>\n>>> >> client model in a manner which is secure, efficient and privacy\n>>>\n>>> >> compatible.\n>>>\n>>> >>\n>>>\n>>> >> The BFD can be used verbatim in replacement of BIP37, where the filter\n>>>\n>>> >> can be cached between clients without needing to be recomputed. It can\n>>>\n>>> >> also be used by normal pruned nodes to do re-scans locally of their\n>>>\n>>> >> wallet without needing to have the block data available to scan, or\n>>>\n>>> >> without reading the entire block chain from disk.\n>>>\n>>> > I started exploring the potential of BFD after this specification.\n>>>\n>>> >\n>>>\n>>> > What would be the preferred/recommended way to handle 0-conf/mempool\n>>>\n>>> > filtering \u2013 if & once BDF would have been deployed (any type,\n>>>\n>>> > semi-trusted oracles or protocol-level/softfork)?\n>>>\n>>> >\n>>>\n>>> > From the user-experience perspective, this is probably pretty important\n>>>\n>>> > (otherwise the experience will be that incoming funds can take serval\n>>>\n>>> > minutes to hours until they appear).\n>>>\n>>> > Using BIP37 bloom filters just for mempool filtering would obviously\n>>>\n>>> > result in the same unwanted privacy-setup.\n>>>\n>>> >\n>>>\n>>> > </jonas>\n>>>\n>>> >\n>>>\n>>> >\n>>>\n>>> > _______________________________________________\n>>>\n>>> > bitcoin-dev mailing list\n>>>\n>>> > bitcoin-dev at lists.linuxfoundation.org\n>>>\n>>> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>> _______________________________________________\n>>>\n>>> bitcoin-dev mailing list\n>>>\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>>\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/7695adaf/attachment-0001.html>"
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-01-04T00:10:14",
                "message_text_only": "Unfortunately a non validating SPV wallet has absolutely no idea if\nthe information about an unconfirmed transaction they are seeing is\nanything but properly formatted. They are connecting to an easily\nmanipulated, sybil attacked, and untrusted network and then asking\nthem for financial information. Seeing an unconfirmed transaction in a\nwallet that's not also fully validating is at best meaningless.\n\n\nOn 2017-01-03 15:46, Aaron Voisine wrote:\n> If the sender doesn't control the receiver's network connection, then\n> the information the receiver gains by watching the mempool is if the\n> transaction has propagated across the bitcoin network. This is useful\n> to know in all kinds of situations.\n> \n> Aaron Voisine\n> co-founder and CEO\n> breadwallet [2]\n> On Tue, Jan 3, 2017 at 3:06 PM, adiabat <rx at awsomnet.org> wrote:\n> \n>> Mempool transactions have their place, but \"unconfirmed\" and \"SPV\"\n>> don't belong together.  Only a full node can tell if a transaction\n>> may get confirmed, or is nonsense.  Unfortunately all the light /\n>> SPV wallets I know of show mempool transactions, which makes it hard\n>> to go back... (e.g. \"why doesn't your software show 0-conf! your\n>> wallet is broken!\", somewhat akin to people complaining about RBF)\n>> \n>> So, this is easy, just don't worry about mempool filtering.  Why are\n>> light clients looking at the mempool anyway?  Maybe if there were\n>> some way to provide SPV proofs of all inputs, but that's a bit of a\n>> mess for full nodes to do.\n>> \n>> Without mempool filtering, I think the committed bloom filters would\n>> be a great improvement over the current bloom filter setup,\n>> especially for lightning network use cases (with lightning, not\n>> finding out about a transaction can make you lose money).  I want to\n>> work on it and may be able to at some point as it's somewhat related\n>> to lightning.\n>> \n>> Also, if you're running a light client, and storing the filters the\n>> way you store block headers, there's really no reason to go all the\n>> way back to height 0.  You can start grabbing headers at some point\n>> a while ago, before your set of keys was generated.  I think it'd be\n>> very worth it even with GB-scale disk usage.\n>> \n>> -Tadge\n>> \n>> On Tue, Jan 3, 2017 at 5:18 PM, Aaron Voisine via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>> Unconfirmed transactions are incredibly important for real world\n>> use. Merchants for instance are willing to accept credit card\n>> payments of thousands of dollars and ship the goods despite the fact\n>> that the transaction can be reversed up to 60 days later. There is a\n>> very large cost to losing the ability to have instant transactions\n>> in many or even most situations. This cost is typically well above\n>> the fraud risk.\n>> \n>> It's important to recognize that bitcoin serves a wide variety of\n>> use cases with different profiles for time sensitivity and fraud\n>> risk.\n>> \n>> Aaron\n>> \n>> On Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> The concept combined with the weak blocks system where miners commit\n>> \n>> to potential transaction inclusion with fractional difficulty blocks\n>> \n>> is possible. I'm not personally convinced that unconfirmed\n>> transaction\n>> \n>> display in a wallet is worth the privacy trade-off. The user has\n>> very\n>> \n>> little to gain from this knowledge until the txn is in a block.\n>> \n>> On 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n>> \n>>> Hi\n>> \n>>>> We introduce several concepts that rework the lightweight Bitcoin\n>> \n>>>> client model in a manner which is secure, efficient and privacy\n>> \n>>>> compatible.\n>> \n>>>> \n>> \n>>>> The BFD can be used verbatim in replacement of BIP37, where the\n>> filter\n>> \n>>>> can be cached between clients without needing to be recomputed.\n>> It can\n>> \n>>>> also be used by normal pruned nodes to do re-scans locally of\n>> their\n>> \n>>>> wallet without needing to have the block data available to scan,\n>> or\n>> \n>>>> without reading the entire block chain from disk.\n>> \n>>> I started exploring the potential of BFD after this specification.\n>> \n>>> \n>> \n>>> What would be the preferred/recommended way to handle\n>> 0-conf/mempool\n>> \n>>> filtering \u2013 if & once BDF would have been deployed (any type,\n>> \n>>> semi-trusted oracles or protocol-level/softfork)?\n>> \n>>> \n>> \n>>> From the user-experience perspective, this is probably pretty\n>> important\n>> \n>>> (otherwise the experience will be that incoming funds can take\n>> serval\n>> \n>>> minutes to hours until they appear).\n>> \n>>> Using BIP37 bloom filters just for mempool filtering would\n>> obviously\n>> \n>>> result in the same unwanted privacy-setup.\n>> \n>>> \n>> \n>>> </jonas>\n>> \n>>> \n>> \n>>> \n>> \n>>> _______________________________________________\n>> \n>>> bitcoin-dev mailing list\n>> \n>>> bitcoin-dev at lists.linuxfoundation.org\n>> \n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>> \n>> _______________________________________________\n>> \n>> bitcoin-dev mailing list\n>> \n>> bitcoin-dev at lists.linuxfoundation.org\n>> \n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>> \n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n> \n> \n> \n> Links:\n> ------\n> [1] https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> [2] http://breadwallet.com"
            },
            {
                "author": "Aaron Voisine",
                "date": "2017-01-04T00:36:34",
                "message_text_only": "Knowing that a transaction is property formatted and that it has been\nbroadcast to the gossip network is useful in many situations. You're only\nthinking about whether you can know a transaction is valid and/or settled.\nThis is not the only possible useful information in actual real world use.\nAny situation where credit card transactions are accepted today for\ninstance, it is useful to know that a transaction has been initiated, even\nthough it can be reversed at any time up to 60 days later.\n\nAaron Voisine\nco-founder and CEO\nbreadwallet <http://breadwallet.com>\n\nOn Tue, Jan 3, 2017 at 4:10 PM, <bfd at cock.lu> wrote:\n\n> Unfortunately a non validating SPV wallet has absolutely no idea if\n> the information about an unconfirmed transaction they are seeing is\n> anything but properly formatted. They are connecting to an easily\n> manipulated, sybil attacked, and untrusted network and then asking\n> them for financial information. Seeing an unconfirmed transaction in a\n> wallet that's not also fully validating is at best meaningless.\n>\n>\n> On 2017-01-03 15:46, Aaron Voisine wrote:\n>\n>> If the sender doesn't control the receiver's network connection, then\n>> the information the receiver gains by watching the mempool is if the\n>> transaction has propagated across the bitcoin network. This is useful\n>> to know in all kinds of situations.\n>>\n>> Aaron Voisine\n>> co-founder and CEO\n>> breadwallet [2]\n>>\n>> On Tue, Jan 3, 2017 at 3:06 PM, adiabat <rx at awsomnet.org> wrote:\n>>\n>> Mempool transactions have their place, but \"unconfirmed\" and \"SPV\"\n>>> don't belong together.  Only a full node can tell if a transaction\n>>> may get confirmed, or is nonsense.  Unfortunately all the light /\n>>> SPV wallets I know of show mempool transactions, which makes it hard\n>>> to go back... (e.g. \"why doesn't your software show 0-conf! your\n>>> wallet is broken!\", somewhat akin to people complaining about RBF)\n>>>\n>>> So, this is easy, just don't worry about mempool filtering.  Why are\n>>> light clients looking at the mempool anyway?  Maybe if there were\n>>> some way to provide SPV proofs of all inputs, but that's a bit of a\n>>> mess for full nodes to do.\n>>>\n>>> Without mempool filtering, I think the committed bloom filters would\n>>> be a great improvement over the current bloom filter setup,\n>>> especially for lightning network use cases (with lightning, not\n>>> finding out about a transaction can make you lose money).  I want to\n>>> work on it and may be able to at some point as it's somewhat related\n>>> to lightning.\n>>>\n>>> Also, if you're running a light client, and storing the filters the\n>>> way you store block headers, there's really no reason to go all the\n>>> way back to height 0.  You can start grabbing headers at some point\n>>> a while ago, before your set of keys was generated.  I think it'd be\n>>> very worth it even with GB-scale disk usage.\n>>>\n>>> -Tadge\n>>>\n>>> On Tue, Jan 3, 2017 at 5:18 PM, Aaron Voisine via bitcoin-dev\n>>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> Unconfirmed transactions are incredibly important for real world\n>>> use. Merchants for instance are willing to accept credit card\n>>> payments of thousands of dollars and ship the goods despite the fact\n>>> that the transaction can be reversed up to 60 days later. There is a\n>>> very large cost to losing the ability to have instant transactions\n>>> in many or even most situations. This cost is typically well above\n>>> the fraud risk.\n>>>\n>>> It's important to recognize that bitcoin serves a wide variety of\n>>> use cases with different profiles for time sensitivity and fraud\n>>> risk.\n>>>\n>>> Aaron\n>>>\n>>> On Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev\n>>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> The concept combined with the weak blocks system where miners commit\n>>>\n>>> to potential transaction inclusion with fractional difficulty blocks\n>>>\n>>> is possible. I'm not personally convinced that unconfirmed\n>>> transaction\n>>>\n>>> display in a wallet is worth the privacy trade-off. The user has\n>>> very\n>>>\n>>> little to gain from this knowledge until the txn is in a block.\n>>>\n>>> On 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n>>>\n>>> Hi\n>>>>\n>>>\n>>> We introduce several concepts that rework the lightweight Bitcoin\n>>>>>\n>>>>\n>>> client model in a manner which is secure, efficient and privacy\n>>>>>\n>>>>\n>>> compatible.\n>>>>>\n>>>>\n>>>\n>>>>>\n>>> The BFD can be used verbatim in replacement of BIP37, where the\n>>>>>\n>>>> filter\n>>>\n>>> can be cached between clients without needing to be recomputed.\n>>>>>\n>>>> It can\n>>>\n>>> also be used by normal pruned nodes to do re-scans locally of\n>>>>>\n>>>> their\n>>>\n>>> wallet without needing to have the block data available to scan,\n>>>>>\n>>>> or\n>>>\n>>> without reading the entire block chain from disk.\n>>>>>\n>>>>\n>>> I started exploring the potential of BFD after this specification.\n>>>>\n>>>\n>>>\n>>>>\n>>> What would be the preferred/recommended way to handle\n>>>>\n>>> 0-conf/mempool\n>>>\n>>> filtering \u2013 if & once BDF would have been deployed (any type,\n>>>>\n>>>\n>>> semi-trusted oracles or protocol-level/softfork)?\n>>>>\n>>>\n>>>\n>>>>\n>>> From the user-experience perspective, this is probably pretty\n>>>>\n>>> important\n>>>\n>>> (otherwise the experience will be that incoming funds can take\n>>>>\n>>> serval\n>>>\n>>> minutes to hours until they appear).\n>>>>\n>>>\n>>> Using BIP37 bloom filters just for mempool filtering would\n>>>>\n>>> obviously\n>>>\n>>> result in the same unwanted privacy-setup.\n>>>>\n>>>\n>>>\n>>>>\n>>> </jonas>\n>>>>\n>>>\n>>>\n>>>>\n>>>\n>>>>\n>>> _______________________________________________\n>>>>\n>>>\n>>> bitcoin-dev mailing list\n>>>>\n>>>\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>\n>>>\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>>>>\n>>>\n>>> _______________________________________________\n>>>\n>>> bitcoin-dev mailing list\n>>>\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>>\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>>>\n>>\n>>\n>>\n>> Links:\n>> ------\n>> [1] https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> [2] http://breadwallet.com\n>>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/06669e05/attachment-0001.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-01-04T06:06:31",
                "message_text_only": "Credit card reversals involve an escrow agent with control over the entire network and with a strong interest in preserving the network. A better analogy would be blind acceptance of any slip of paper under the assumption that it is sufficient currency. It may or may not be so, but you are on your own in either case.\n\ne\n\n> On Jan 3, 2017, at 4:36 PM, Aaron Voisine via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> Knowing that a transaction is property formatted and that it has been broadcast to the gossip network is useful in many situations. You're only thinking about whether you can know a transaction is valid and/or settled. This is not the only possible useful information in actual real world use. Any situation where credit card transactions are accepted today for instance, it is useful to know that a transaction has been initiated, even though it can be reversed at any time up to 60 days later.\n> \n> Aaron Voisine\n> co-founder and CEO\n> breadwallet\n> \n>> On Tue, Jan 3, 2017 at 4:10 PM, <bfd at cock.lu> wrote:\n>> Unfortunately a non validating SPV wallet has absolutely no idea if\n>> the information about an unconfirmed transaction they are seeing is\n>> anything but properly formatted. They are connecting to an easily\n>> manipulated, sybil attacked, and untrusted network and then asking\n>> them for financial information. Seeing an unconfirmed transaction in a\n>> wallet that's not also fully validating is at best meaningless.\n>> \n>> \n>>> On 2017-01-03 15:46, Aaron Voisine wrote:\n>>> If the sender doesn't control the receiver's network connection, then\n>>> the information the receiver gains by watching the mempool is if the\n>>> transaction has propagated across the bitcoin network. This is useful\n>>> to know in all kinds of situations.\n>>> \n>>> Aaron Voisine\n>>> co-founder and CEO\n>>> breadwallet [2]\n>>> \n>>> On Tue, Jan 3, 2017 at 3:06 PM, adiabat <rx at awsomnet.org> wrote:\n>>> \n>>>> Mempool transactions have their place, but \"unconfirmed\" and \"SPV\"\n>>>> don't belong together.  Only a full node can tell if a transaction\n>>>> may get confirmed, or is nonsense.  Unfortunately all the light /\n>>>> SPV wallets I know of show mempool transactions, which makes it hard\n>>>> to go back... (e.g. \"why doesn't your software show 0-conf! your\n>>>> wallet is broken!\", somewhat akin to people complaining about RBF)\n>>>> \n>>>> So, this is easy, just don't worry about mempool filtering.  Why are\n>>>> light clients looking at the mempool anyway?  Maybe if there were\n>>>> some way to provide SPV proofs of all inputs, but that's a bit of a\n>>>> mess for full nodes to do.\n>>>> \n>>>> Without mempool filtering, I think the committed bloom filters would\n>>>> be a great improvement over the current bloom filter setup,\n>>>> especially for lightning network use cases (with lightning, not\n>>>> finding out about a transaction can make you lose money).  I want to\n>>>> work on it and may be able to at some point as it's somewhat related\n>>>> to lightning.\n>>>> \n>>>> Also, if you're running a light client, and storing the filters the\n>>>> way you store block headers, there's really no reason to go all the\n>>>> way back to height 0.  You can start grabbing headers at some point\n>>>> a while ago, before your set of keys was generated.  I think it'd be\n>>>> very worth it even with GB-scale disk usage.\n>>>> \n>>>> -Tadge\n>>>> \n>>>> On Tue, Jan 3, 2017 at 5:18 PM, Aaron Voisine via bitcoin-dev\n>>>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>> \n>>>> Unconfirmed transactions are incredibly important for real world\n>>>> use. Merchants for instance are willing to accept credit card\n>>>> payments of thousands of dollars and ship the goods despite the fact\n>>>> that the transaction can be reversed up to 60 days later. There is a\n>>>> very large cost to losing the ability to have instant transactions\n>>>> in many or even most situations. This cost is typically well above\n>>>> the fraud risk.\n>>>> \n>>>> It's important to recognize that bitcoin serves a wide variety of\n>>>> use cases with different profiles for time sensitivity and fraud\n>>>> risk.\n>>>> \n>>>> Aaron\n>>>> \n>>>> On Tue, Jan 3, 2017 at 12:41 PM bfd--- via bitcoin-dev\n>>>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>> The concept combined with the weak blocks system where miners commit\n>>>> \n>>>> to potential transaction inclusion with fractional difficulty blocks\n>>>> \n>>>> is possible. I'm not personally convinced that unconfirmed\n>>>> transaction\n>>>> \n>>>> display in a wallet is worth the privacy trade-off. The user has\n>>>> very\n>>>> \n>>>> little to gain from this knowledge until the txn is in a block.\n>>>> \n>>>> On 2017-01-01 13:01, Jonas Schnelli via bitcoin-dev wrote:\n>>>> \n>>>>> Hi\n>>>> \n>>>>>> We introduce several concepts that rework the lightweight Bitcoin\n>>>> \n>>>>>> client model in a manner which is secure, efficient and privacy\n>>>> \n>>>>>> compatible.\n>>>> \n>>>>>> \n>>>> \n>>>>>> The BFD can be used verbatim in replacement of BIP37, where the\n>>>> filter\n>>>> \n>>>>>> can be cached between clients without needing to be recomputed.\n>>>> It can\n>>>> \n>>>>>> also be used by normal pruned nodes to do re-scans locally of\n>>>> their\n>>>> \n>>>>>> wallet without needing to have the block data available to scan,\n>>>> or\n>>>> \n>>>>>> without reading the entire block chain from disk.\n>>>> \n>>>>> I started exploring the potential of BFD after this specification.\n>>>> \n>>>>> \n>>>> \n>>>>> What would be the preferred/recommended way to handle\n>>>> 0-conf/mempool\n>>>> \n>>>>> filtering \u2013 if & once BDF would have been deployed (any type,\n>>>> \n>>>>> semi-trusted oracles or protocol-level/softfork)?\n>>>> \n>>>>> \n>>>> \n>>>>> From the user-experience perspective, this is probably pretty\n>>>> important\n>>>> \n>>>>> (otherwise the experience will be that incoming funds can take\n>>>> serval\n>>>> \n>>>>> minutes to hours until they appear).\n>>>> \n>>>>> Using BIP37 bloom filters just for mempool filtering would\n>>>> obviously\n>>>> \n>>>>> result in the same unwanted privacy-setup.\n>>>> \n>>>>> \n>>>> \n>>>>> </jonas>\n>>>> \n>>>>> \n>>>> \n>>>>> \n>>>> \n>>>>> _______________________________________________\n>>>> \n>>>>> bitcoin-dev mailing list\n>>>> \n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> \n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>>>> \n>>>> _______________________________________________\n>>>> \n>>>> bitcoin-dev mailing list\n>>>> \n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> \n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>>>> \n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev [1]\n>>> \n>>> \n>>> \n>>> Links:\n>>> ------\n>>> [1] https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>> [2] http://breadwallet.com\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/0950eb13/attachment-0001.html>"
            },
            {
                "author": "Leo Wandersleb",
                "date": "2017-01-04T16:13:41",
                "message_text_only": "On 01/04/2017 12:06 AM, adiabat via bitcoin-dev wrote:\n> Also, if you're running a light client, and storing the filters the way you\n> store block headers, there's really no reason to go all the way back to height\n> 0.  You can start grabbing headers at some point a while ago, before your set\n> of keys was generated.  I think it'd be very worth it even with GB-scale disk\n> usage.\n\nThe really great benefit of having this index is that you could implement rather\nefficient cold wallet spending once the wallet has the full index.\n\nWith Mycelium you can currently spend funds from a paper wallet or a BIP39\nsentence but at the cost of sharing all addresses with our servers. Schildbach\nwould share addresses with random full nodes for hours or days to find funds of\na new private key with unknown creation date. With CBF it would still be a\nmatter of maybe minutes on a phone to identify relevant blocks and download\nthese but it would be very feasible to implement a private, cold storage\nspending feature.\n\nAlso the index could be further partitioned by P2PKH, P2PK, P2SH, \u2026 This would\nlead to a very minor privacy leak for a reasonable reduction in index size.\n\n-- \n\nLeo Wandersleb\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 473 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170104/0accaaba/attachment.sig>"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2017-01-04T07:47:10",
                "message_text_only": "Hi\n\n> Unconfirmed transactions are incredibly important for real world use.\n> Merchants for instance are willing to accept credit card payments of\n> thousands of dollars and ship the goods despite the fact that the\n> transaction can be reversed up to 60 days later. There is a very large\n> cost to losing the ability to have instant transactions in many or\n> even most situations. This cost is typically well above the fraud risk. \n>\n> It's important to recognize that bitcoin serves a wide variety of use\n> cases with different profiles for time sensitivity and fraud risk.\n>\nI agree that unconfirmed transactions are incredibly important, but not\nover SPV against random peers.\n\nIf you offer users/merchants a feature (SPV 0-conf against random\npeers), that is fundamentally insecure, it will \u2013 sooner or later \u2013 lead\nto some large scale fiasco, hurting Bitcoins reputation and trust from\nmerchants.\n\nMerchants using and trusting 0-conf SPV transactions (retrieved from\nrandom peers) is something we should **really eliminate** through\neducation and by offering different solution.\n\nThere are plenty, more sane options. If you can't run your own full-node\nas a merchant (trivial), maybe co-use a wallet-service with centralized\nverification (maybe use two of them), I guess Copay would be one of\nthose wallets (as an example). Use them in watch-only mode.\n\nFor end-users SPV software, I think it would be recommended to...\n... disable unconfirmed transactions during SPV against random peers\n... enable unconfirmed transactions when using SPV against a trusted\npeer with preshared keys after BIP150\n... if unconfirmed transactions are disabled, show how it can be enabled\n(how to run a full-node [in a box, etc.])\n... educate, inform users that a transaction with no confirmation can be\n\"stopped\" or \"redirected\" any time, also inform about the risks during\nlow-conf phase (1-5).\n\nI though see the point that it's nice to make use of the \"incoming\nfunds...\" feature in SPV wallets. But \u2013 for the sake of stability and\n(risk-)scaling \u2013 we may want to recommend to scarify this feature and \u2013\nin the same turn \u2013 to use privacy-preserving BFD's.\n\n</jonas>\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170104/76132d83/attachment.sig>"
            },
            {
                "author": "Aaron Voisine",
                "date": "2017-01-04T08:56:21",
                "message_text_only": "It's easy enough to mark a transaction as \"pending\". People with bank\naccounts are familiar with the concept.\n\nAlthough the risk of accepting gossip information from multiple random\npeers, in the case where the sender does not control the receivers network\nis still minimal. Random node operators have no incentive to send fake\ntransactions, and would need to control all the nodes a client connects to,\nand find a non-false-positive address belonging to the victims wallet.\n\nIt's not impossible, but it's non trivial, would only temporarily show a\npending transaction, and provide no benefit to the node operator. There are\nmuch juicier targets for an attacker with the ability to sybil attack the\nentire bitcoin p2p network.\n\nAaron\n\nOn Tue, Jan 3, 2017 at 11:47 PM Jonas Schnelli <dev at jonasschnelli.ch> wrote:\n\n> Hi\n>\n>\n>\n> > Unconfirmed transactions are incredibly important for real world use.\n>\n> > Merchants for instance are willing to accept credit card payments of\n>\n> > thousands of dollars and ship the goods despite the fact that the\n>\n> > transaction can be reversed up to 60 days later. There is a very large\n>\n> > cost to losing the ability to have instant transactions in many or\n>\n> > even most situations. This cost is typically well above the fraud risk.\n>\n> >\n>\n> > It's important to recognize that bitcoin serves a wide variety of use\n>\n> > cases with different profiles for time sensitivity and fraud risk.\n>\n> >\n>\n> I agree that unconfirmed transactions are incredibly important, but not\n>\n> over SPV against random peers.\n>\n>\n>\n> If you offer users/merchants a feature (SPV 0-conf against random\n>\n> peers), that is fundamentally insecure, it will \u2013 sooner or later \u2013 lead\n>\n> to some large scale fiasco, hurting Bitcoins reputation and trust from\n>\n> merchants.\n>\n>\n>\n> Merchants using and trusting 0-conf SPV transactions (retrieved from\n>\n> random peers) is something we should **really eliminate** through\n>\n> education and by offering different solution.\n>\n>\n>\n> There are plenty, more sane options. If you can't run your own full-node\n>\n> as a merchant (trivial), maybe co-use a wallet-service with centralized\n>\n> verification (maybe use two of them), I guess Copay would be one of\n>\n> those wallets (as an example). Use them in watch-only mode.\n>\n>\n>\n> For end-users SPV software, I think it would be recommended to...\n>\n> ... disable unconfirmed transactions during SPV against random peers\n>\n> ... enable unconfirmed transactions when using SPV against a trusted\n>\n> peer with preshared keys after BIP150\n>\n> ... if unconfirmed transactions are disabled, show how it can be enabled\n>\n> (how to run a full-node [in a box, etc.])\n>\n> ... educate, inform users that a transaction with no confirmation can be\n>\n> \"stopped\" or \"redirected\" any time, also inform about the risks during\n>\n> low-conf phase (1-5).\n>\n>\n>\n> I though see the point that it's nice to make use of the \"incoming\n>\n> funds...\" feature in SPV wallets. But \u2013 for the sake of stability and\n>\n> (risk-)scaling \u2013 we may want to recommend to scarify this feature and \u2013\n>\n> in the same turn \u2013 to use privacy-preserving BFD's.\n>\n>\n>\n> </jonas>\n>\n>\n>\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170104/793748c8/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-01-04T10:13:02",
                "message_text_only": "There were talks about implementing spv mode for bitcoin core without using\nbloom filters. Less efficient because it downloads full blocks, but better\nfor privacy. Perhaps other spv implementations should consider doing the\nsame instead of committing the filters in the block?\n\nNow I feel I was missing something. I guess you can download the whole\nblock you're interested in instead of only your txs and that gives you\nprivacy.\nBut how do you get to know which blocks are you interested in?\n\nIf the questions are too basic or offtopic for the thread, I'm happy\ngetting answers privately  (but then maybe I get them more than once).\n\n\nOn 4 Jan 2017 09:57, \"Aaron Voisine via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nIt's easy enough to mark a transaction as \"pending\". People with bank\naccounts are familiar with the concept.\n\nAlthough the risk of accepting gossip information from multiple random\npeers, in the case where the sender does not control the receivers network\nis still minimal. Random node operators have no incentive to send fake\ntransactions, and would need to control all the nodes a client connects to,\nand find a non-false-positive address belonging to the victims wallet.\n\nIt's not impossible, but it's non trivial, would only temporarily show a\npending transaction, and provide no benefit to the node operator. There are\nmuch juicier targets for an attacker with the ability to sybil attack the\nentire bitcoin p2p network.\n\nAaron\n\nOn Tue, Jan 3, 2017 at 11:47 PM Jonas Schnelli <dev at jonasschnelli.ch> wrote:\n\n> Hi\n>\n>\n>\n> > Unconfirmed transactions are incredibly important for real world use.\n>\n> > Merchants for instance are willing to accept credit card payments of\n>\n> > thousands of dollars and ship the goods despite the fact that the\n>\n> > transaction can be reversed up to 60 days later. There is a very large\n>\n> > cost to losing the ability to have instant transactions in many or\n>\n> > even most situations. This cost is typically well above the fraud risk.\n>\n> >\n>\n> > It's important to recognize that bitcoin serves a wide variety of use\n>\n> > cases with different profiles for time sensitivity and fraud risk.\n>\n> >\n>\n> I agree that unconfirmed transactions are incredibly important, but not\n>\n> over SPV against random peers.\n>\n>\n>\n> If you offer users/merchants a feature (SPV 0-conf against random\n>\n> peers), that is fundamentally insecure, it will \u2013 sooner or later \u2013 lead\n>\n> to some large scale fiasco, hurting Bitcoins reputation and trust from\n>\n> merchants.\n>\n>\n>\n> Merchants using and trusting 0-conf SPV transactions (retrieved from\n>\n> random peers) is something we should **really eliminate** through\n>\n> education and by offering different solution.\n>\n>\n>\n> There are plenty, more sane options. If you can't run your own full-node\n>\n> as a merchant (trivial), maybe co-use a wallet-service with centralized\n>\n> verification (maybe use two of them), I guess Copay would be one of\n>\n> those wallets (as an example). Use them in watch-only mode.\n>\n>\n>\n> For end-users SPV software, I think it would be recommended to...\n>\n> ... disable unconfirmed transactions during SPV against random peers\n>\n> ... enable unconfirmed transactions when using SPV against a trusted\n>\n> peer with preshared keys after BIP150\n>\n> ... if unconfirmed transactions are disabled, show how it can be enabled\n>\n> (how to run a full-node [in a box, etc.])\n>\n> ... educate, inform users that a transaction with no confirmation can be\n>\n> \"stopped\" or \"redirected\" any time, also inform about the risks during\n>\n> low-conf phase (1-5).\n>\n>\n>\n> I though see the point that it's nice to make use of the \"incoming\n>\n> funds...\" feature in SPV wallets. But \u2013 for the sake of stability and\n>\n> (risk-)scaling \u2013 we may want to recommend to scarify this feature and \u2013\n>\n> in the same turn \u2013 to use privacy-preserving BFD's.\n>\n>\n>\n> </jonas>\n>\n>\n>\n>\n>\n>\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170104/f762bf24/attachment-0001.html>"
            },
            {
                "author": "Adam Back",
                "date": "2017-01-04T11:00:55",
                "message_text_only": "I think this discussion started from the block bloom filter where\nthere is a bloom filter commitment in the block which can be\ndownloaded and is much smaller than the block.  An SPV node based on\nthat model would download headers and bloom filters, verify the bloom\nfilter is committed to, and test locally if any addresses managed by\nthe wallet are in the filter (or false positives for being in it), and\nthen download blocks with hits.  Apparently there are maybe 50% more\ncompact alternatives to bloom filters but people have been using bloom\nfilter as a short-hand for that.  The block bloom filter does seem to\nhave higher overhead than the query model, but it offers much better\nprivacy.  I think there was previous discussion about maybe doing\nsomething with portions of blocks so you can know which half or\nquarter of the block etc.\n\nAdam\n\n\nOn 4 January 2017 at 10:13, Jorge Tim\u00f3n via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> There were talks about implementing spv mode for bitcoin core without using\n> bloom filters. Less efficient because it downloads full blocks, but better\n> for privacy. Perhaps other spv implementations should consider doing the\n> same instead of committing the filters in the block?\n>\n> Now I feel I was missing something. I guess you can download the whole block\n> you're interested in instead of only your txs and that gives you privacy.\n> But how do you get to know which blocks are you interested in?\n>\n> If the questions are too basic or offtopic for the thread, I'm happy getting\n> answers privately  (but then maybe I get them more than once).\n>\n>\n> On 4 Jan 2017 09:57, \"Aaron Voisine via bitcoin-dev\"\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> It's easy enough to mark a transaction as \"pending\". People with bank\n> accounts are familiar with the concept.\n>\n> Although the risk of accepting gossip information from multiple random\n> peers, in the case where the sender does not control the receivers network\n> is still minimal. Random node operators have no incentive to send fake\n> transactions, and would need to control all the nodes a client connects to,\n> and find a non-false-positive address belonging to the victims wallet.\n>\n> It's not impossible, but it's non trivial, would only temporarily show a\n> pending transaction, and provide no benefit to the node operator. There are\n> much juicier targets for an attacker with the ability to sybil attack the\n> entire bitcoin p2p network.\n>\n> Aaron\n>\n> On Tue, Jan 3, 2017 at 11:47 PM Jonas Schnelli <dev at jonasschnelli.ch> wrote:\n>>\n>> Hi\n>>\n>>\n>>\n>> > Unconfirmed transactions are incredibly important for real world use.\n>>\n>> > Merchants for instance are willing to accept credit card payments of\n>>\n>> > thousands of dollars and ship the goods despite the fact that the\n>>\n>> > transaction can be reversed up to 60 days later. There is a very large\n>>\n>> > cost to losing the ability to have instant transactions in many or\n>>\n>> > even most situations. This cost is typically well above the fraud risk.\n>>\n>> >\n>>\n>> > It's important to recognize that bitcoin serves a wide variety of use\n>>\n>> > cases with different profiles for time sensitivity and fraud risk.\n>>\n>> >\n>>\n>> I agree that unconfirmed transactions are incredibly important, but not\n>>\n>> over SPV against random peers.\n>>\n>>\n>>\n>> If you offer users/merchants a feature (SPV 0-conf against random\n>>\n>> peers), that is fundamentally insecure, it will \u2013 sooner or later \u2013 lead\n>>\n>> to some large scale fiasco, hurting Bitcoins reputation and trust from\n>>\n>> merchants.\n>>\n>>\n>>\n>> Merchants using and trusting 0-conf SPV transactions (retrieved from\n>>\n>> random peers) is something we should **really eliminate** through\n>>\n>> education and by offering different solution.\n>>\n>>\n>>\n>> There are plenty, more sane options. If you can't run your own full-node\n>>\n>> as a merchant (trivial), maybe co-use a wallet-service with centralized\n>>\n>> verification (maybe use two of them), I guess Copay would be one of\n>>\n>> those wallets (as an example). Use them in watch-only mode.\n>>\n>>\n>>\n>> For end-users SPV software, I think it would be recommended to...\n>>\n>> ... disable unconfirmed transactions during SPV against random peers\n>>\n>> ... enable unconfirmed transactions when using SPV against a trusted\n>>\n>> peer with preshared keys after BIP150\n>>\n>> ... if unconfirmed transactions are disabled, show how it can be enabled\n>>\n>> (how to run a full-node [in a box, etc.])\n>>\n>> ... educate, inform users that a transaction with no confirmation can be\n>>\n>> \"stopped\" or \"redirected\" any time, also inform about the risks during\n>>\n>> low-conf phase (1-5).\n>>\n>>\n>>\n>> I though see the point that it's nice to make use of the \"incoming\n>>\n>> funds...\" feature in SPV wallets. But \u2013 for the sake of stability and\n>>\n>> (risk-)scaling \u2013 we may want to recommend to scarify this feature and \u2013\n>>\n>> in the same turn \u2013 to use privacy-preserving BFD's.\n>>\n>>\n>>\n>> </jonas>\n>>\n>>\n>>\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-01-06T02:15:26",
                "message_text_only": "With a credit card you have an institution worth billions of dollars\nasserting that a payment has been made, with the option that it may be\nretracted under special circumstances by the card issuer.\n\nUnconfirmed Bitcoin transactions with a SPV client has you trusting\nthat the un-authenticated DNS seed lookup has not been tampered with,\nthe connection to the random node that you connect to has not been\ntampered with, and that the seed nor the node are attempting to\nmanipulate you.\n\nThe two scenarios aren\u2019t even remotely comparable.\n\n\nOn 2017-01-04 00:56, Aaron Voisine wrote:\n> It's easy enough to mark a transaction as \"pending\". People with bank\n> accounts are familiar with the concept.\n> \n> Although the risk of accepting gossip information from multiple random\n> peers, in the case where the sender does not control the receivers\n> network is still minimal. Random node operators have no incentive to\n> send fake transactions, and would need to control all the nodes a\n> client connects to, and find a non-false-positive address belonging to\n> the victims wallet.\n> \n> It's not impossible, but it's non trivial, would only temporarily show\n> a pending transaction, and provide no benefit to the node operator.\n> There are much juicier targets for an attacker with the ability to\n> sybil attack the entire bitcoin p2p network.\n> \n> Aaron\n> \n> On Tue, Jan 3, 2017 at 11:47 PM Jonas Schnelli <dev at jonasschnelli.ch>\n> wrote:\n> \n>> Hi\n>> \n>>> Unconfirmed transactions are incredibly important for real world\n>> use.\n>> \n>>> Merchants for instance are willing to accept credit card payments\n>> of\n>> \n>>> thousands of dollars and ship the goods despite the fact that the\n>> \n>>> transaction can be reversed up to 60 days later. There is a very\n>> large\n>> \n>>> cost to losing the ability to have instant transactions in many or\n>> \n>>> even most situations. This cost is typically well above the fraud\n>> risk.\n>> \n>>> \n>> \n>>> It's important to recognize that bitcoin serves a wide variety of\n>> use\n>> \n>>> cases with different profiles for time sensitivity and fraud risk.\n>> \n>>> \n>> \n>> I agree that unconfirmed transactions are incredibly important, but\n>> not\n>> \n>> over SPV against random peers.\n>> \n>> If you offer users/merchants a feature (SPV 0-conf against random\n>> \n>> peers), that is fundamentally insecure, it will \u2013 sooner or later\n>> \u2013 lead\n>> \n>> to some large scale fiasco, hurting Bitcoins reputation and trust\n>> from\n>> \n>> merchants.\n>> \n>> Merchants using and trusting 0-conf SPV transactions (retrieved from\n>> \n>> random peers) is something we should **really eliminate** through\n>> \n>> education and by offering different solution.\n>> \n>> There are plenty, more sane options. If you can't run your own\n>> full-node\n>> \n>> as a merchant (trivial), maybe co-use a wallet-service with\n>> centralized\n>> \n>> verification (maybe use two of them), I guess Copay would be one of\n>> \n>> those wallets (as an example). Use them in watch-only mode.\n>> \n>> For end-users SPV software, I think it would be recommended to...\n>> \n>> ... disable unconfirmed transactions during SPV against random peers\n>> \n>> ... enable unconfirmed transactions when using SPV against a trusted\n>> \n>> peer with preshared keys after BIP150\n>> \n>> ... if unconfirmed transactions are disabled, show how it can be\n>> enabled\n>> \n>> (how to run a full-node [in a box, etc.])\n>> \n>> ... educate, inform users that a transaction with no confirmation\n>> can be\n>> \n>> \"stopped\" or \"redirected\" any time, also inform about the risks\n>> during\n>> \n>> low-conf phase (1-5).\n>> \n>> I though see the point that it's nice to make use of the \"incoming\n>> \n>> funds...\" feature in SPV wallets. But \u2013 for the sake of stability\n>> and\n>> \n>> (risk-)scaling \u2013 we may want to recommend to scarify this feature\n>> and \u2013\n>> \n>> in the same turn \u2013 to use privacy-preserving BFD's.\n>> \n>> </jonas>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-05T07:06:36",
                "message_text_only": "On 1/3/17, Jonas Schnelli via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> There are plenty, more sane options. If you can't run your own full-node\n> as a merchant (trivial), maybe co-use a wallet-service with centralized\n> verification (maybe use two of them), I guess Copay would be one of\n> those wallets (as an example). Use them in watch-only mode.\n\nThe best way is to connect to the mempool of each miner and check to\nsee if they have your txid in their mempool.\n\nhttps://www.antpool.com/api/is_in_mempool?txid=334847bb...\nhttps://www.f2pool.com/api/is_in_mempool?txid=334847bb...\nhttps://bw.com/api/is_in_mempool?txid=334847bb...\nhttps://bitfury.com/api/is_in_mempool?txid=334847bb...\nhttps://btcc.com/api/is_in_mempool?txid=334847bb...\n\nIf each of these services return \"True\", and you know those services\nso not engage in RBF, then you can assume with great confidence that\nyour transaction will be in the next block, or in a block very soon.\nIf any one of those services return \"False\", then you must assume that\nit is possible that there is a double spend floating around, and that\nyou should wait to see if that tx gets confirmed. The problem is that\nnot every pool runs such a service to check the contents of their\nmempool...\n\nThis is an example of mining centralization increasing the security of\nzero confirm. If more people mined, this method will not work as well\nbecause it would require you to call the API of hundreds of different\npotential block creators."
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-01-05T07:45:18",
                "message_text_only": "On 01/04/2017 11:06 PM, Chris Priest via bitcoin-dev wrote:\n> On 1/3/17, Jonas Schnelli via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> There are plenty, more sane options. If you can't run your own full-node\n>> as a merchant (trivial), maybe co-use a wallet-service with centralized\n>> verification (maybe use two of them), I guess Copay would be one of\n>> those wallets (as an example). Use them in watch-only mode.\n> \n> The best way is to connect to the mempool of each miner and check to\n> see if they have your txid in their mempool.\n> \n> https://www.antpool.com/api/is_in_mempool?txid=334847bb...\n> https://www.f2pool.com/api/is_in_mempool?txid=334847bb...\n> https://bw.com/api/is_in_mempool?txid=334847bb...\n> https://bitfury.com/api/is_in_mempool?txid=334847bb...\n> https://btcc.com/api/is_in_mempool?txid=334847bb...\n> \n> If each of these services return \"True\", and you know those services\n> so not engage in RBF, then you can assume with great confidence that\n> your transaction will be in the next block, or in a block very soon.\n> If any one of those services return \"False\", then you must assume that\n> it is possible that there is a double spend floating around, and that\n> you should wait to see if that tx gets confirmed. The problem is that\n> not every pool runs such a service to check the contents of their\n> mempool...\n> \n> This is an example of mining centralization increasing the security of\n> zero confirm.\n\nA world connected up to a few web services to determine payment validity\nis an example of a bitcoin security catastrophe.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170104/4df3d1a9/attachment.sig>"
            },
            {
                "author": "Christian Decker",
                "date": "2017-01-05T14:48:33",
                "message_text_only": "On Wed, Jan 04, 2017 at 11:45:18PM -0800, Eric Voskuil via bitcoin-dev wrote:\n> On 01/04/2017 11:06 PM, Chris Priest via bitcoin-dev wrote:\n> > The best way is to connect to the mempool of each miner and check to\n> > see if they have your txid in their mempool.\n> > \n> > https://www.antpool.com/api/is_in_mempool?txid=334847bb...\n> > https://www.f2pool.com/api/is_in_mempool?txid=334847bb...\n> > https://bw.com/api/is_in_mempool?txid=334847bb...\n> > https://bitfury.com/api/is_in_mempool?txid=334847bb...\n> > https://btcc.com/api/is_in_mempool?txid=334847bb...\n> > \n> > If each of these services return \"True\", and you know those services\n> > so not engage in RBF, then you can assume with great confidence that\n> > your transaction will be in the next block, or in a block very soon.\n> > If any one of those services return \"False\", then you must assume that\n> > it is possible that there is a double spend floating around, and that\n> > you should wait to see if that tx gets confirmed. The problem is that\n> > not every pool runs such a service to check the contents of their\n> > mempool...\n> > \n> > This is an example of mining centralization increasing the security of\n> > zero confirm.\n> \n> A world connected up to a few web services to determine payment validity\n> is an example of a bitcoin security catastrophe.\n> \n> e\n> \n\nAnd it's a great way to tell every miner who you are and what\ntransactions you are sending/receiving. An absolute privacy\nnightmare...\n\n-- cdecker"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-06T20:15:46",
                "message_text_only": "Its a method for determining the probability that a valid tx will be\nmined in a block before that tx actually gets mined, which is useful\nwhen accepting payments in situations when you can't wait for the full\nconfirmation. No one is saying all tx validation should be performed\nby querying miners mempools, that's ridiculous. Obviously once the tx\ngets it's first confirmation, you go back to determining validity the\nway you always have. There is no \"security catastrophe\".\n\nEven if you're running a full node, you can't know for certain that\nany given tx will make it into a future block. You can't be certain\nthe future miner who finally does mine that tx will mine your TXID or\nanother TXID that spends the same inputs to another address (a double\nspend). The only way to actually know for certain is to query every\nsingle large hashpower mempool.\n\nOn 1/4/17, Eric Voskuil <eric at voskuil.org> wrote:\n> On 01/04/2017 11:06 PM, Chris Priest via bitcoin-dev wrote:\n>> On 1/3/17, Jonas Schnelli via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> There are plenty, more sane options. If you can't run your own full-node\n>>> as a merchant (trivial), maybe co-use a wallet-service with centralized\n>>> verification (maybe use two of them), I guess Copay would be one of\n>>> those wallets (as an example). Use them in watch-only mode.\n>>\n>> The best way is to connect to the mempool of each miner and check to\n>> see if they have your txid in their mempool.\n>>\n>> https://www.antpool.com/api/is_in_mempool?txid=334847bb...\n>> https://www.f2pool.com/api/is_in_mempool?txid=334847bb...\n>> https://bw.com/api/is_in_mempool?txid=334847bb...\n>> https://bitfury.com/api/is_in_mempool?txid=334847bb...\n>> https://btcc.com/api/is_in_mempool?txid=334847bb...\n>>\n>> If each of these services return \"True\", and you know those services\n>> so not engage in RBF, then you can assume with great confidence that\n>> your transaction will be in the next block, or in a block very soon.\n>> If any one of those services return \"False\", then you must assume that\n>> it is possible that there is a double spend floating around, and that\n>> you should wait to see if that tx gets confirmed. The problem is that\n>> not every pool runs such a service to check the contents of their\n>> mempool...\n>>\n>> This is an example of mining centralization increasing the security of\n>> zero confirm.\n>\n> A world connected up to a few web services to determine payment validity\n> is an example of a bitcoin security catastrophe.\n>\n> e\n>\n>"
            },
            {
                "author": "James MacWhyte",
                "date": "2017-01-06T21:35:58",
                "message_text_only": "It's my opinion that the purpose of this list and bitcoin protocol\ndevelopment in general is to build the base functionality that other\ncompanies and individuals require to provide usability to the end-user. The\n0-conf debate is a UX issue. If end users shouldn't rely on 0-conf, it is\nup to wallet developers to hide 0-conf transactions or mark them\nappropriately. Instead of using this list to debate what wallet designers\nshould or shouldn't do, we should just provide the tools and \"let the\nmarket sort it out\". If wallet developers start getting inundated with\ncomplaints that 0-conf transactions are causing confusion and loss, they\nwill find a solution. If the tools they require for the solution don't\nexist, they will come to this list to request action.\n\nAm I wrong?\n\nOn Fri, Jan 6, 2017 at 12:16 PM Chris Priest via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Its a method for determining the probability that a valid tx will be\n> mined in a block before that tx actually gets mined, which is useful\n> when accepting payments in situations when you can't wait for the full\n> confirmation. No one is saying all tx validation should be performed\n> by querying miners mempools, that's ridiculous. Obviously once the tx\n> gets it's first confirmation, you go back to determining validity the\n> way you always have. There is no \"security catastrophe\".\n>\n> Even if you're running a full node, you can't know for certain that\n> any given tx will make it into a future block. You can't be certain\n> the future miner who finally does mine that tx will mine your TXID or\n> another TXID that spends the same inputs to another address (a double\n> spend). The only way to actually know for certain is to query every\n> single large hashpower mempool.\n>\n> On 1/4/17, Eric Voskuil <eric at voskuil.org> wrote:\n> > On 01/04/2017 11:06 PM, Chris Priest via bitcoin-dev wrote:\n> >> On 1/3/17, Jonas Schnelli via bitcoin-dev\n> >> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >>>\n> >>> There are plenty, more sane options. If you can't run your own\n> full-node\n> >>> as a merchant (trivial), maybe co-use a wallet-service with centralized\n> >>> verification (maybe use two of them), I guess Copay would be one of\n> >>> those wallets (as an example). Use them in watch-only mode.\n> >>\n> >> The best way is to connect to the mempool of each miner and check to\n> >> see if they have your txid in their mempool.\n> >>\n> >> https://www.antpool.com/api/is_in_mempool?txid=334847bb...\n> >> https://www.f2pool.com/api/is_in_mempool?txid=334847bb...\n> >> https://bw.com/api/is_in_mempool?txid=334847bb...\n> >> https://bitfury.com/api/is_in_mempool?txid=334847bb...\n> >> https://btcc.com/api/is_in_mempool?txid=334847bb...\n> >>\n> >> If each of these services return \"True\", and you know those services\n> >> so not engage in RBF, then you can assume with great confidence that\n> >> your transaction will be in the next block, or in a block very soon.\n> >> If any one of those services return \"False\", then you must assume that\n> >> it is possible that there is a double spend floating around, and that\n> >> you should wait to see if that tx gets confirmed. The problem is that\n> >> not every pool runs such a service to check the contents of their\n> >> mempool...\n> >>\n> >> This is an example of mining centralization increasing the security of\n> >> zero confirm.\n> >\n> > A world connected up to a few web services to determine payment validity\n> > is an example of a bitcoin security catastrophe.\n> >\n> > e\n> >\n> >\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170106/55cde129/attachment.html>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-01-06T21:50:47",
                "message_text_only": "It is a useful aspect of discussion at this level as it helps higher lever developers understand the actual tradeoffs. Clearly some do not. The market will eventually sort them out, but the discussion both gives developers the necessary information.\n\nIt also helps core development prioritize resources. I personally would not prioritize core work to facilitate zero conf. I would even spend time to discourage it, as others have done.\n\nI think the cautions in this thread about doing privacy and system security damaging things (like checking mining pools for zero conf transactions) will prevent some wasted time, which benefits everyone.\n\ne\n\n> On Jan 6, 2017, at 1:35 PM, James MacWhyte via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> It's my opinion that the purpose of this list and bitcoin protocol development in general is to build the base functionality that other companies and individuals require to provide usability to the end-user. The 0-conf debate is a UX issue. If end users shouldn't rely on 0-conf, it is up to wallet developers to hide 0-conf transactions or mark them appropriately. Instead of using this list to debate what wallet designers should or shouldn't do, we should just provide the tools and \"let the market sort it out\". If wallet developers start getting inundated with complaints that 0-conf transactions are causing confusion and loss, they will find a solution. If the tools they require for the solution don't exist, they will come to this list to request action.\n> \n> Am I wrong?\n> \n> On Fri, Jan 6, 2017 at 12:16 PM Chris Priest via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> Its a method for determining the probability that a valid tx will be\n>> mined in a block before that tx actually gets mined, which is useful\n>> when accepting payments in situations when you can't wait for the full\n>> confirmation. No one is saying all tx validation should be performed\n>> by querying miners mempools, that's ridiculous. Obviously once the tx\n>> gets it's first confirmation, you go back to determining validity the\n>> way you always have. There is no \"security catastrophe\".\n>> \n>> Even if you're running a full node, you can't know for certain that\n>> any given tx will make it into a future block. You can't be certain\n>> the future miner who finally does mine that tx will mine your TXID or\n>> another TXID that spends the same inputs to another address (a double\n>> spend). The only way to actually know for certain is to query every\n>> single large hashpower mempool.\n>> \n>> On 1/4/17, Eric Voskuil <eric at voskuil.org> wrote:\n>> > On 01/04/2017 11:06 PM, Chris Priest via bitcoin-dev wrote:\n>> >> On 1/3/17, Jonas Schnelli via bitcoin-dev\n>> >> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >>>\n>> >>> There are plenty, more sane options. If you can't run your own full-node\n>> >>> as a merchant (trivial), maybe co-use a wallet-service with centralized\n>> >>> verification (maybe use two of them), I guess Copay would be one of\n>> >>> those wallets (as an example). Use them in watch-only mode.\n>> >>\n>> >> The best way is to connect to the mempool of each miner and check to\n>> >> see if they have your txid in their mempool.\n>> >>\n>> >> https://www.antpool.com/api/is_in_mempool?txid=334847bb...\n>> >> https://www.f2pool.com/api/is_in_mempool?txid=334847bb...\n>> >> https://bw.com/api/is_in_mempool?txid=334847bb...\n>> >> https://bitfury.com/api/is_in_mempool?txid=334847bb...\n>> >> https://btcc.com/api/is_in_mempool?txid=334847bb...\n>> >>\n>> >> If each of these services return \"True\", and you know those services\n>> >> so not engage in RBF, then you can assume with great confidence that\n>> >> your transaction will be in the next block, or in a block very soon.\n>> >> If any one of those services return \"False\", then you must assume that\n>> >> it is possible that there is a double spend floating around, and that\n>> >> you should wait to see if that tx gets confirmed. The problem is that\n>> >> not every pool runs such a service to check the contents of their\n>> >> mempool...\n>> >>\n>> >> This is an example of mining centralization increasing the security of\n>> >> zero confirm.\n>> >\n>> > A world connected up to a few web services to determine payment validity\n>> > is an example of a bitcoin security catastrophe.\n>> >\n>> > e\n>> >\n>> >\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170106/0a3b8a1f/attachment-0001.html>"
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-01-06T02:04:22",
                "message_text_only": "You might as well replace Bitcoin with a system where these parties\nsign transactions and skip mining altogether, it would have the same\nproperties and be significantly more effient.\n\n\nOn 2017-01-04 23:06, Chris Priest wrote:\n> On 1/3/17, Jonas Schnelli via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \n>> There are plenty, more sane options. If you can't run your own \n>> full-node\n>> as a merchant (trivial), maybe co-use a wallet-service with \n>> centralized\n>> verification (maybe use two of them), I guess Copay would be one of\n>> those wallets (as an example). Use them in watch-only mode.\n> \n> The best way is to connect to the mempool of each miner and check to\n> see if they have your txid in their mempool.\n> \n> https://www.antpool.com/api/is_in_mempool?txid=334847bb...\n> https://www.f2pool.com/api/is_in_mempool?txid=334847bb...\n> https://bw.com/api/is_in_mempool?txid=334847bb...\n> https://bitfury.com/api/is_in_mempool?txid=334847bb...\n> https://btcc.com/api/is_in_mempool?txid=334847bb...\n> \n> If each of these services return \"True\", and you know those services\n> so not engage in RBF, then you can assume with great confidence that\n> your transaction will be in the next block, or in a block very soon.\n> If any one of those services return \"False\", then you must assume that\n> it is possible that there is a double spend floating around, and that\n> you should wait to see if that tx gets confirmed. The problem is that\n> not every pool runs such a service to check the contents of their\n> mempool...\n> \n> This is an example of mining centralization increasing the security of\n> zero confirm. If more people mined, this method will not work as well\n> because it would require you to call the API of hundreds of different\n> potential block creators."
            },
            {
                "author": "bfd at cock.lu",
                "date": "2017-01-03T20:24:35",
                "message_text_only": "I believe the filter can be more compact than this, but even if not an\norder of magnitude saving of disk space is still significant.\n\n\nOn 2016-05-11 13:29, Bob McElrath wrote:\n> Eerrrr....let me revise that last paragraph.  That's 12 *GB* of filters \n> at\n> today's block height (at fixed false-positive rate 1e-6.  Compared to \n> block\n> headers only which are about 33 MB today.  So this proposal is not \n> really\n> compatible with such a wallet being \"light\"...\n> \n> Damn units...\n> \n> Bob McElrath via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] \n> wrote:\n>> I like this idea, but let's run some numbers...\n>> \n>> bfd--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n>> > A Bloom Filter Digest is deterministically created of every block\n>> \n>> Bloom filters completely obfuscate the required size of the filter for \n>> a desired\n>> false-positive rate.  But, an optimal filter is linear in the number \n>> of elements\n>> it contains for fixed false-positive rate, and logarithmic in the \n>> false-positive\n>> rate.  (This comment applies to a RLL encoded Bloom filter Greg \n>> mentioned, but\n>> that's not the only way)  That is for N elements and false positive \n>> rate\n>> \\epsilon:\n>> \n>>     filter size = - N \\log_2 \\epsilon\n>> \n>> Given that the data that would be put into this particular filter is \n>> *already*\n>> hashed, it makes more sense and is faster to use a Cuckoo[1] filter, \n>> choosing a\n>> fixed false-positive rate, given expected wallet sizes.  For Bloom \n>> filters,\n>> multiply the above formula by 1.44.\n>> \n>> To prevent light clients from downloading more blocks than necessary, \n>> the\n>> false-positive rate should be roughly less than 1/(block height).  If \n>> we take\n>> the false positive rate to be 1e-6 for today's block height ~ 410000, \n>> this is\n>> about 20 bits per element.  So for todays block's, this is a 30kb \n>> filter, for a\n>> 3% increase in block size, if blocks commit to the filter.  Thus the \n>> required\n>> size of the filter commitment is roughly:\n>> \n>>     filter size = N \\log_2 H\n>> \n>> where H is the block height.  If bitcoin had these filters from the \n>> beginning, a\n>> light client today would have to download about 12MB of data in \n>> filters.  My\n>> personal SPV wallet is using 31MB currently.  It's not clear this is a \n>> bandwidth\n>> win, though it's definitely a win for computing load on full nodes.\n>> \n>> \n>> [1] https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\n>> \n>> --\n>> Cheers, Bob McElrath\n>> \n>> \"For every complex problem, there is a solution that is simple, neat, \n>> and wrong.\"\n>>     -- H. L. Mencken\n>> \n>> \n>> \n>> !DSPAM:5733934b206851108912031!\n> \n> \n> \n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> \n>> \n>> !DSPAM:5733934b206851108912031!\n> \n> --\n> Cheers, Bob McElrath\n> \n> \"For every complex problem, there is a solution that is simple, neat,\n> and wrong.\"\n>     -- H. L. Mencken"
            },
            {
                "author": "Aaron Voisine",
                "date": "2017-01-06T07:07:34",
                "message_text_only": "Credit card transactions are simply an expample of a widely used payment\nsystem that has frequent fraud and chargebacks. The argument I'm making is\nthat different people in different situations value speed and convenience\nover a known fraud risk. Instant zero confirmation transactions are\nextremely useful despite the risk in all kinds of real world situations.\nYou can't substitute your own value judgements for everyone in every\nsituation.\n\nAaron\n\nOn Thu, Jan 5, 2017 at 6:15 PM <bfd at cock.lu> wrote:\n\n> With a credit card you have an institution worth billions of dollars\n>\n> asserting that a payment has been made, with the option that it may be\n>\n> retracted under special circumstances by the card issuer.\n>\n>\n>\n> Unconfirmed Bitcoin transactions with a SPV client has you trusting\n>\n> that the un-authenticated DNS seed lookup has not been tampered with,\n>\n> the connection to the random node that you connect to has not been\n>\n> tampered with, and that the seed nor the node are attempting to\n>\n> manipulate you.\n>\n>\n>\n> The two scenarios aren\u2019t even remotely comparable.\n>\n>\n>\n>\n>\n> On 2017-01-04 00:56, Aaron Voisine wrote:\n>\n> > It's easy enough to mark a transaction as \"pending\". People with bank\n>\n> > accounts are familiar with the concept.\n>\n> >\n>\n> > Although the risk of accepting gossip information from multiple random\n>\n> > peers, in the case where the sender does not control the receivers\n>\n> > network is still minimal. Random node operators have no incentive to\n>\n> > send fake transactions, and would need to control all the nodes a\n>\n> > client connects to, and find a non-false-positive address belonging to\n>\n> > the victims wallet.\n>\n> >\n>\n> > It's not impossible, but it's non trivial, would only temporarily show\n>\n> > a pending transaction, and provide no benefit to the node operator.\n>\n> > There are much juicier targets for an attacker with the ability to\n>\n> > sybil attack the entire bitcoin p2p network.\n>\n> >\n>\n> > Aaron\n>\n> >\n>\n> > On Tue, Jan 3, 2017 at 11:47 PM Jonas Schnelli <dev at jonasschnelli.ch>\n>\n> > wrote:\n>\n> >\n>\n> >> Hi\n>\n> >>\n>\n> >>> Unconfirmed transactions are incredibly important for real world\n>\n> >> use.\n>\n> >>\n>\n> >>> Merchants for instance are willing to accept credit card payments\n>\n> >> of\n>\n> >>\n>\n> >>> thousands of dollars and ship the goods despite the fact that the\n>\n> >>\n>\n> >>> transaction can be reversed up to 60 days later. There is a very\n>\n> >> large\n>\n> >>\n>\n> >>> cost to losing the ability to have instant transactions in many or\n>\n> >>\n>\n> >>> even most situations. This cost is typically well above the fraud\n>\n> >> risk.\n>\n> >>\n>\n> >>>\n>\n> >>\n>\n> >>> It's important to recognize that bitcoin serves a wide variety of\n>\n> >> use\n>\n> >>\n>\n> >>> cases with different profiles for time sensitivity and fraud risk.\n>\n> >>\n>\n> >>>\n>\n> >>\n>\n> >> I agree that unconfirmed transactions are incredibly important, but\n>\n> >> not\n>\n> >>\n>\n> >> over SPV against random peers.\n>\n> >>\n>\n> >> If you offer users/merchants a feature (SPV 0-conf against random\n>\n> >>\n>\n> >> peers), that is fundamentally insecure, it will \u2013 sooner or later\n>\n> >> \u2013 lead\n>\n> >>\n>\n> >> to some large scale fiasco, hurting Bitcoins reputation and trust\n>\n> >> from\n>\n> >>\n>\n> >> merchants.\n>\n> >>\n>\n> >> Merchants using and trusting 0-conf SPV transactions (retrieved from\n>\n> >>\n>\n> >> random peers) is something we should **really eliminate** through\n>\n> >>\n>\n> >> education and by offering different solution.\n>\n> >>\n>\n> >> There are plenty, more sane options. If you can't run your own\n>\n> >> full-node\n>\n> >>\n>\n> >> as a merchant (trivial), maybe co-use a wallet-service with\n>\n> >> centralized\n>\n> >>\n>\n> >> verification (maybe use two of them), I guess Copay would be one of\n>\n> >>\n>\n> >> those wallets (as an example). Use them in watch-only mode.\n>\n> >>\n>\n> >> For end-users SPV software, I think it would be recommended to...\n>\n> >>\n>\n> >> ... disable unconfirmed transactions during SPV against random peers\n>\n> >>\n>\n> >> ... enable unconfirmed transactions when using SPV against a trusted\n>\n> >>\n>\n> >> peer with preshared keys after BIP150\n>\n> >>\n>\n> >> ... if unconfirmed transactions are disabled, show how it can be\n>\n> >> enabled\n>\n> >>\n>\n> >> (how to run a full-node [in a box, etc.])\n>\n> >>\n>\n> >> ... educate, inform users that a transaction with no confirmation\n>\n> >> can be\n>\n> >>\n>\n> >> \"stopped\" or \"redirected\" any time, also inform about the risks\n>\n> >> during\n>\n> >>\n>\n> >> low-conf phase (1-5).\n>\n> >>\n>\n> >> I though see the point that it's nice to make use of the \"incoming\n>\n> >>\n>\n> >> funds...\" feature in SPV wallets. But \u2013 for the sake of stability\n>\n> >> and\n>\n> >>\n>\n> >> (risk-)scaling \u2013 we may want to recommend to scarify this feature\n>\n> >> and \u2013\n>\n> >>\n>\n> >> in the same turn \u2013 to use privacy-preserving BFD's.\n>\n> >>\n>\n> >> </jonas>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170106/2aaca08b/attachment.html>"
            },
            {
                "author": "Erik Aronesty",
                "date": "2017-01-06T22:07:36",
                "message_text_only": "- N \\log_2 \\epsilon * 1.44\n\nN = 41000 blocks\nepsilon = 1/41000 (fp rate)\n\n= 904689.8bits\n\n~ 1 MB\n\n\nOn Thu, Jul 28, 2016 at 5:07 PM, Leo Wandersleb via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> gmaxwell just made me aware of this mail thread [0]. Some days ago I had\n> independently and naively started implementing \"something similar\" [1].\n>\n> My version totally ignored the commitment and signing part but I'm pretty\n> sure\n> that 12GB is overkill. My code is currently broken and I have no time to\n> work on\n> it much but I thought it might be helpful to chime in.\n>\n> At this point in time the difference between 80GB and 3GB (as my current\n> 1.5GB\n> of only outputs would suggest if I had covered the inputs) or even 12GB\n> makes\n> the difference of being able to store it on a phone, vs. not being able\n> to. 80GB\n> \"compressed\" to 3GB is not that bad at all. Unfortunately, with segWit\n> this will\n> be worse, with the higher transaction count per MB.\n>\n> Regards,\n>\n> Leo\n>\n> [0]\n> https://www.reddit.com/r/Bitcoin/comments/4v28jl/how_\n> have_fungiblity_problems_affected_you_in/d5ux6aq\n> [1] https://github.com/Giszmo/TransactionFinder\n>\n> On 05/11/2016 10:29 PM, Bob McElrath via bitcoin-dev wrote:\n> > Eerrrr....let me revise that last paragraph.  That's 12 *GB* of filters\n> at\n> > today's block height (at fixed false-positive rate 1e-6.  Compared to\n> block\n> > headers only which are about 33 MB today.  So this proposal is not really\n> > compatible with such a wallet being \"light\"...\n> >\n> > Damn units...\n> >\n> > Bob McElrath via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org]\n> wrote:\n> >> I like this idea, but let's run some numbers...\n> >>\n> >> bfd--- via bitcoin-dev [bitcoin-dev at lists.linuxfoundation.org] wrote:\n> >>> A Bloom Filter Digest is deterministically created of every block\n> >> Bloom filters completely obfuscate the required size of the filter for\n> a desired\n> >> false-positive rate.  But, an optimal filter is linear in the number of\n> elements\n> >> it contains for fixed false-positive rate, and logarithmic in the\n> false-positive\n> >> rate.  (This comment applies to a RLL encoded Bloom filter Greg\n> mentioned, but\n> >> that's not the only way)  That is for N elements and false positive rate\n> >> \\epsilon:\n> >>\n> >>     filter size = - N \\log_2 \\epsilon\n> >>\n> >> Given that the data that would be put into this particular filter is\n> *already*\n> >> hashed, it makes more sense and is faster to use a Cuckoo[1] filter,\n> choosing a\n> >> fixed false-positive rate, given expected wallet sizes.  For Bloom\n> filters,\n> >> multiply the above formula by 1.44.\n> >>\n> >> To prevent light clients from downloading more blocks than necessary,\n> the\n> >> false-positive rate should be roughly less than 1/(block height).  If\n> we take\n> >> the false positive rate to be 1e-6 for today's block height ~ 410000,\n> this is\n> >> about 20 bits per element.  So for todays block's, this is a 30kb\n> filter, for a\n> >> 3% increase in block size, if blocks commit to the filter.  Thus the\n> required\n> >> size of the filter commitment is roughly:\n> >>\n> >>     filter size = N \\log_2 H\n> >>\n> >> where H is the block height.  If bitcoin had these filters from the\n> beginning, a\n> >> light client today would have to download about 12MB of data in\n> filters.  My\n> >> personal SPV wallet is using 31MB currently.  It's not clear this is a\n> bandwidth\n> >> win, though it's definitely a win for computing load on full nodes.\n> >>\n> >>\n> >> [1] https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\n> >>\n> >> --\n> >> Cheers, Bob McElrath\n> >>\n> >> \"For every complex problem, there is a solution that is simple, neat,\n> and wrong.\"\n> >>     -- H. L. Mencken\n> >>\n> >>\n> >>\n> >> !DSPAM:5733934b206851108912031!\n> >\n> >\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>\n> >>\n> >> !DSPAM:5733934b206851108912031!\n> > --\n> > Cheers, Bob McElrath\n> >\n> > \"For every complex problem, there is a solution that is simple, neat,\n> and wrong.\"\n> >     -- H. L. Mencken\n> >\n> >\n> >\n> > _______________________________________________\n> > bitcoin-dev mailing list\n> > bitcoin-dev at lists.linuxfoundation.org\n> > https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170106/6199b1c2/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Committed bloom filters for improved wallet performance and SPV security",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Voskuil",
                "adiabat",
                "Adam Back",
                "Chris Priest",
                "bfd at cock.lu",
                "Jorge Tim\u00f3n",
                "James MacWhyte",
                "Erik Aronesty",
                "Leo Wandersleb",
                "Aaron Voisine",
                "Jonas Schnelli",
                "Christian Decker"
            ],
            "messages_count": 25,
            "total_messages_chars_count": 89683
        }
    },
    {
        "title": "[bitcoin-dev] BIP - 'Block75' - New algorithm",
        "thread_messages": [
            {
                "author": "t. khan",
                "date": "2017-01-02T18:04:37",
                "message_text_only": "Based on feedback from this list and further simulations, here is a new\nalgorithm for Block75:\n\nnew max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY)\n\nTARGET_CAPACITY = 0.75\nAVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a\ndecimal\nx = current max block size\n\nPlease note that this algorithm actually tries to keep blocks 75% full,\nunlike the old one that unnecessarily capped growth at 250KB. While this\nwould theoretically allow a maximum increase of 25% over the previous max\nblock size, in practice it's not possible to get that high.\n\nThis would be calculated every 2016 blocks along with difficulty.\n\nBlock75 should maintain transaction fees at about the level they were in\nMay/June 2016 when blocks started hitting 75% full somewhat consistently.\n\nThoughts? For any predictions as to how this would behave, please provide\nthe numbers used to arrive at any conclusions.\n\nOther questions:\n1. How do we make Block75 play nice with SegWit?\n2. Is there any need for a minimum max blocksize? Block75 allows for\ndecreasing the size as well as increasing it.\n\nActivation:\nTo help negate some of the risk associated with a hard fork and to prevent\na single relatively small mining pool from blocking Block75's adoption,\nactivation would occur once 900 of the last 1,000 blocks mined signaled\nsupport, with a grace period of 4,032 blocks.\n\nThank you again to all those who commented on the previous Block75 thread.\nTogether, we can make 2017 the year the block size debate ends (hopefully\nforever).\n\nHappy New Year!\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170102/dab8e778/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-01-02T19:01:11",
                "message_text_only": "On Monday, 2 January 2017 13:04:37 CET t. khan via bitcoin-dev wrote:\n> Thoughts? \n\nThis proposal doesn't change the block size, it only changes the maximum \nblock size. Which is expected, nothing bad there.\n\nThe direct consequence of this, though is that the miners set the maximum \nblock size. Because they decide on the actual created block size.\n\nThis leads me to the simple question why we can't just give the miners full \ncontrol of the maximum block size directly?\n\nThe fact of the matter is that miners have for the full history of Bitcoin \nbeen able to set the block size, until they hit the 1MB limit.\nAnd your proposal keeps that property, but why have a maximum in the \nprotocol?\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "t. khan",
                "date": "2017-01-02T19:32:24",
                "message_text_only": "Math should decide the max block size, not humans (miners in this\ncase). The goal of Block75 is to manage the max block size without any\nhuman intervention.\n\nUnder Block75, miners don't have any direct control but could still choose\nto mine smaller blocks (same as now), though doing so would cost them the\nfees from transactions they didn't include in their blocks.\n\nA maximum block size is necessary to prevent a single nefarious miner from\ncreating a ridiculously large block which would break the network.\n\n- t.k.\n\nOn Mon, Jan 2, 2017 at 2:01 PM, Tom Zander <tomz at freedommail.ch> wrote:\n\n> On Monday, 2 January 2017 13:04:37 CET t. khan via bitcoin-dev wrote:\n> > Thoughts?\n>\n> This proposal doesn't change the block size, it only changes the maximum\n> block size. Which is expected, nothing bad there.\n>\n> The direct consequence of this, though is that the miners set the maximum\n> block size. Because they decide on the actual created block size.\n>\n> This leads me to the simple question why we can't just give the miners full\n> control of the maximum block size directly?\n>\n> The fact of the matter is that miners have for the full history of Bitcoin\n> been able to set the block size, until they hit the 1MB limit.\n> And your proposal keeps that property, but why have a maximum in the\n> protocol?\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170102/b2a3d4b7/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-01-02T20:35:40",
                "message_text_only": "On Monday, 2 January 2017 14:32:24 CET t. khan wrote:\n> Math should decide the max block size, not humans (miners in this\n> case). The goal of Block75 is to manage the max block size without any\n> human intervention.\n\nIf the input of your math is completely free and human created, how does it \nfollow that it was math that created it ?\n\nWhy do you want it math created anyway?\n\n> A maximum block size is necessary to prevent a single nefarious miner from\n> creating a ridiculously large block which would break the network.\n\nA maximum is needed, yes. But does it have to be part of the protocol?\nA simple policy which is set by node operators (reject block if greater than \nX bytes) will solve this just fine, no?\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-01-02T21:19:10",
                "message_text_only": "On Monday, January 02, 2017 8:35:40 PM Tom Zander via bitcoin-dev wrote:\n> A maximum is needed, yes. But does it have to be part of the protocol?\n> A simple policy which is set by node operators (reject block if greater\n> than X bytes) will solve this just fine, no?\n\nIf you reject a block based on a particular condition, that is BY DEFINITION \npart of the consensus protocol, and NOT a policy. The protocol is literally \nthe set of rules by which blocks are determined to be valid or invalid.\n\nPolicies are things that can vary node-to-node without affecting the validity \njudgement of blocks.\n\nOn Monday, January 02, 2017 8:41:42 PM t. khan wrote:\n> On Mon, Jan 2, 2017 at 3:04 PM, Luke Dashjr <luke at dashjr.org> wrote:\n> > It would probably behave as an ever-increasing block size limit. Spam has\n> > typically filled blocks to the max, and miners have stopped\n> > self-enforcing reasonable limits.\n> \n> Using the growth rate over the last year as a model (\n> https://blockchain.info/charts/avg-block-size?daysAverageString=14 ),\n> Block75 would also have frequently decreased the limit. Though, yes, more\n> transactions would equal larger blocks over time, but that's the entire\n> point of this.\n\nThen it doesn't solve the actual problems of either miner spam or growth in \nresource requirements, which are the entire purpose of the block size limit.\n\n> What is your definition of \"spam\"?\n\nAnything that consumes more data than necessary to properly convey the \nintended transfer of value (bitcoins) from one entity to another, including \nall data that is not intended for such a purpose.\n\n> > I doubt you'll get consensus for such a fundamentally broken proposal.\n> > I certainly don't foresee any circumstance where I could reasonably\n> > support it... The block size limit exists to restrict miners; it makes no\n> > sense to put it in their control.\n> \n> Specifically, what is broken about it?\n\nPutting group X in control of a limit that exists for the sole purpose of \nrestricting group X.\n\n> There would still be a block size limit, it would just change slightly\n> every two weeks. I agree that miners shouldn't have control of this, and\n> Block75 doesn't give them any (at least none they can make a profit on).\n\nIt gives miners complete control over the limit. They can make blocks of any \nsize (within the current limit), thus triggering the conditions by which your \nproposal would raise the limit further.\n\nLuke"
            },
            {
                "author": "Tom Zander",
                "date": "2017-01-02T22:01:08",
                "message_text_only": "On Monday, 2 January 2017 21:19:10 CET Luke Dashjr wrote:\n> On Monday, January 02, 2017 8:35:40 PM Tom Zander via bitcoin-dev wrote:\n> > A maximum is needed, yes. But does it have to be part of the protocol?\n> > A simple policy which is set by node operators (reject block if greater\n> > than X bytes) will solve this just fine, no?\n> \n> If you reject a block based on a particular condition, that is BY\n> DEFINITION part of the consensus protocol, and NOT a policy. The protocol\n> is literally the set of rules by which blocks are determined to be valid\n> or invalid.\n> \n> Policies are things that can vary node-to-node without affecting the\n> validity judgement of blocks.\n\nPolicy is thus expanded to allow an individual node to reject blocks that \nare technically speaking valid, just unacceptable to them.\n\nIt would be fun to ponder of the effect of that when applied to many nodes. \nMany people already have pondered that question and find it intriguing. So \ndon't reject it out of hand :)\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "t. khan",
                "date": "2017-01-03T14:28:27",
                "message_text_only": "On Mon, Jan 2, 2017 at 4:19 PM, Luke Dashjr <luke at dashjr.org\n<javascript:_e(%7B%7D,'cvml','luke at dashjr.org');>> wrote:\n\n> > Using the growth rate over the last year as a model (\n> > https://blockchain.info/charts/avg-block-size?daysAverageString=14 ),\n> > Block75 would also have frequently decreased the limit. Though, yes, more\n> > transactions would equal larger blocks over time, but that's the entire\n> > point of this.\n>\n> Then it doesn't solve the actual problems of either miner spam or growth in\n> resource requirements, which are the entire purpose of the block size\n> limit.\n>\n\nDo you have any direct evidence of \"miner spam\"? If so, please link to a\ntransaction. Also, what could a miner possibly gain from this?\n\nFor resource requirements (bandwidth/disk space), please run the numbers on\nthe Block75 algorithm and compare with growth rates of both.\n\n> What is your definition of \"spam\"?\n>\n> Anything that consumes more data than necessary to properly convey the\n> intended transfer of value (bitcoins) from one entity to another, including\n> all data that is not intended for such a purpose.\n>\n\nBy this definition, any transaction which transfers ownership of an asset\n(stock certificate, deeds to property, copyrights, etc.) is spam. But these\nare legitimate, fee paying transactions. They are not spam. Yes, they're\nonly moving 0.0001 btc. Some will eventually move to Lightning (or\nsomething like it), but many will not as they are unsuitable for off-chain.\n\n> > I doubt you'll get consensus for such a fundamentally broken proposal.\n> > > I certainly don't foresee any circumstance where I could reasonably\n> > > support it... The block size limit exists to restrict miners; it makes\n> no\n> > > sense to put it in their control.\n> >\n> > Specifically, what is broken about it?\n>\n> Putting group X in control of a limit that exists for the sole purpose of\n> restricting group X.\n>\n\nNo, it actually gives them less power than they have now. Consider the\ntwo-week recalculation: the result of any attempt to manipulate the block\nsize (up or down) will only last for two weeks. There's no way for a miner\nto profit from this (in fact, they'd lose money this way). The best outcome\nthey could hope for is a very small increase or decrease for two weeks. So\nwhy would anyone do this?\n\nAs soon as such a manipulation ended, Block75 would correct the max block\nsize back to an appropriate level (defined as: average block 75% full).\n\n> There would still be a block size limit, it would just change slightly\n> > every two weeks. I agree that miners shouldn't have control of this, and\n> > Block75 doesn't give them any (at least none they can make a profit on).\n>\n> It gives miners complete control over the limit. They can make blocks of\n> any\n> size (within the current limit), thus triggering the conditions by which\n> your\n> proposal would raise the limit further.\n\n\nWe covered this ad nauseum in the other Block75 thread, but basically no\none has been able to come up with a realistic scenario wherein miners would\nbe motivated to do this *and* there aren't any pools large enough now (nor\nhave there ever been) to have anything more than a minor and temporary\neffect.\n\nIf such a scenario actually does exist, please describe it in detail.\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/e28ff284/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-01-02T20:04:56",
                "message_text_only": "On Monday, January 02, 2017 6:04:37 PM t. khan via bitcoin-dev wrote:\n> Thoughts? For any predictions as to how this would behave, please provide\n> the numbers used to arrive at any conclusions.\n\nIt would probably behave as an ever-increasing block size limit. Spam has \ntypically filled blocks to the max, and miners have stopped self-enforcing \nreasonable limits.\n\n> 2. Is there any need for a minimum max blocksize? Block75 allows for\n> decreasing the size as well as increasing it.\n\nProbably it should never make it so small that a reasonable generation \ntransaction cannot fit. But I'm not sure this needs explicit enforcement.\n\n> To help negate some of the risk associated with a hard fork and to prevent\n> a single relatively small mining pool from blocking Block75's adoption,\n> activation would occur once 900 of the last 1,000 blocks mined signaled\n> support, with a grace period of 4,032 blocks.\n\nIf you can't trust miners to signal based on the community's consensus, then \ndon't use miner signalling at all. Just set a block height it activates.\n\n> Thank you again to all those who commented on the previous Block75 thread.\n> Together, we can make 2017 the year the block size debate ends (hopefully\n> forever).\n\nI doubt you'll get consensus for such a fundamentally broken proposal.\nI certainly don't foresee any circumstance where I could reasonably support \nit... The block size limit exists to restrict miners; it makes no sense to put \nit in their control.\n\nLuke"
            },
            {
                "author": "t. khan",
                "date": "2017-01-02T20:41:42",
                "message_text_only": "On Mon, Jan 2, 2017 at 3:04 PM, Luke Dashjr <luke at dashjr.org> wrote:\n\n> It would probably behave as an ever-increasing block size limit. Spam has\n> typically filled blocks to the max, and miners have stopped self-enforcing\n> reasonable limits.\n\n\nUsing the growth rate over the last year as a model (\nhttps://blockchain.info/charts/avg-block-size?daysAverageString=14 ),\nBlock75 would also have frequently decreased the limit. Though, yes, more\ntransactions would equal larger blocks over time, but that's the entire\npoint of this.\n\nWhat is your definition of \"spam\"? Also, can you point to data that\nsupports the hypothesis that spam is filling blocks?\n\n\n> I doubt you'll get consensus for such a fundamentally broken proposal.\n>\nI certainly don't foresee any circumstance where I could reasonably support\n> it... The block size limit exists to restrict miners; it makes no sense to\n> put\n> it in their control.\n>\n\nSpecifically, what is broken about it?\n\nThere would still be a block size limit, it would just change slightly\nevery two weeks. I agree that miners shouldn't have control of this, and\nBlock75 doesn't give them any (at least none they can make a profit on).\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170102/8116b645/attachment.html>"
            },
            {
                "author": "t. khan",
                "date": "2017-01-02T21:05:58",
                "message_text_only": "On Mon, Jan 2, 2017 at 3:35 PM, Tom Zander <tomz at freedommail.ch> wrote:\n\n> If the input of your math is completely free and human created, how does it\n> follow that it was math that created it ?\n> Why do you want it math created anyway?\n\n\nThe beauty of math is that everyone on the planet agrees how it works.\nEverything in Bitcoin is math, with the exception of the blocksize limit\n(1MB) which was a stop-gap solution at the time.\n\n\n> A maximum is needed, yes. But does it have to be part of the protocol?\n> A simple policy which is set by node operators (reject block if greater\n> than\n> X bytes) will solve this just fine, no?\n\n\nNo. That would be an epic disaster. There's no such thing as a \"simple\npolicy\" when humans are involved. Obviously no one would agree on what X\nbytes would be and you'd have some nodes rejecting blocks that others\nalready accepted.\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170102/4082357f/attachment-0001.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-01-02T22:33:16",
                "message_text_only": "On Monday, 2 January 2017 16:05:58 CET t. khan wrote:\n> On Mon, Jan 2, 2017 at 3:35 PM, Tom Zander <tomz at freedommail.ch> wrote:\n> > If the input of your math is completely free and human created, how does\n> > it follow that it was math that created it ?\n> > Why do you want it math created anyway?\n> \n> The beauty of math is that everyone on the planet agrees how it works.\n> Everything in Bitcoin is math, with the exception of the blocksize limit\n> (1MB) which was a stop-gap solution at the time.\n\nIn actual fact the block size *is* set by miners, not math. And always has \nbeen.\nIn your proposal the max blocksize continues to be set by miners as a \nsecondary effect of them choosing the block size.\n\nSaying the max is actually math is painting an illusion that is rather thin \nand easy to see through because every single usecase for your suggestion \nstarts with the choice of blocksize that a human makes. There is not really \nany other input except some rather simple algorithm.\n\n> > A maximum is needed, yes. But does it have to be part of the protocol?\n> > A simple policy which is set by node operators (reject block if greater\n> > than\n> > X bytes) will solve this just fine, no?\n> \n> No. That would be an epic disaster. There's no such thing as a \"simple\n> policy\" when humans are involved.\n\nThis is ignoring history where miners have successfully set policy on block \nsize for years now.\n\n> Obviously no one would agree on what X\n> bytes would be and you'd have some nodes rejecting blocks that others\n> already accepted.\n\nNot sure about your \"obviously\". I don't agree. In fact, there is plenty of \nreason to think it does work.\n\nMiners have always been the ones to decide on the block size, and they have \nalways done this in a coordinated fashion. This is a natural consequence of \nthe rather elegant (economic) design of Bitcoin.\n\n*  Miners earn more fee-based income when they produce bigger blocks.\n*  Miners take more risk of their blocks being orphaned with bigger blocks.\n*  Miners want to avoid emptying the memory pool every block as that removes \na total need for users to pay fees.\n*  Miners want to make sure the mempool does not become backlogged because \nusers that do not see their transactions confirmed will get disappointed and \nfind other means to do payments. Which hurts the price and in effect hurts \nthe miners income.\n\nThis behaviour in block size means blocks will not get huge and they will \nnot stay small either, because there are reasons for both. And so the size \nwill be something in the middle.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            }
        ],
        "thread_summary": {
            "title": "BIP - 'Block75' - New algorithm",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "t. khan",
                "Tom Zander",
                "Luke Dashjr"
            ],
            "messages_count": 11,
            "total_messages_chars_count": 18452
        }
    },
    {
        "title": "[bitcoin-dev] Script Abuse Potential?",
        "thread_messages": [
            {
                "author": "Steve Davis",
                "date": "2017-01-02T21:39:24",
                "message_text_only": "Hi all,\n\nSuppose someone were to use the following pk_script:\n\n[op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)..., op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n\nThis still seems to be valid AFAICS, and may be a potential attack vector?\n\nThanks.\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170102/ac515115/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2017-01-03T03:27:44",
                "message_text_only": "It is an unfortunate script, but can't actually\n\u200bdo\n that much\n\u200b it seems\u200b\n. The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.\n\u200b Thus, it would seem the worst you could do with this would be to\n(10000-520*2)*520*2\nbytes  ~=~ 10 MB.\n\n\u200bMuch more concerning would be the op_dup/op_cat style bug, which under a\nsimilar script \u200bwould certainly cause out of memory errors :)\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\nOn Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hi all,\n>\n> Suppose someone were to use the following pk_script:\n>\n> [op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)...,\n> op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n>\n> This still seems to be valid AFAICS, and may be a potential attack vector?\n>\n> Thanks.\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170102/91e5e8d8/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-03T03:39:19",
                "message_text_only": "No, there could only have not more than 201 opcodes in a script. So you may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB\n\nFor OP_CAT, just check if the returned item is within the 520 bytes limit.\n\n> On 3 Jan 2017, at 11:27, Jeremy via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> It is an unfortunate script, but can't actually \u200bdo that much\u200b it seems\u200b. The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.\u200b Thus, it would seem the worst you could do with this would be to (10000-520*2)*520*2 bytes  ~=~ 10 MB.\n> \n> \u200bMuch more concerning would be the op_dup/op_cat style bug, which under a similar script \u200bwould certainly cause out of memory errors :)\n> \n> \n> \n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin> <https://twitter.com/JeremyRubin>\n> On Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>> wrote:\n> Hi all,\n> \n> Suppose someone were to use the following pk_script:\n> \n> [op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)..., op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n> \n> This still seems to be valid AFAICS, and may be a potential attack vector?\n> \n> Thanks.\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev <https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev>\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/fe247bca/attachment-0001.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2017-01-03T05:04:50",
                "message_text_only": "OP_2DUP?  Why not OP_3DUP?\n\nOn Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> No, there could only have not more than 201 opcodes in a script. So you\n> may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB\n>\n> For OP_CAT, just check if the returned item is within the 520 bytes limit.\n>\n> On 3 Jan 2017, at 11:27, Jeremy via bitcoin-dev <bitcoin-dev at lists.\n> linuxfoundation.org> wrote:\n>\n> It is an unfortunate script, but can't actually\n> \u200bdo\n>  that much\n> \u200b it seems\u200b\n> . The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.\n> \u200b Thus, it would seem the worst you could do with this would be to (10000-520*2)*520*2\n> bytes  ~=~ 10 MB.\n>\n> \u200bMuch more concerning would be the op_dup/op_cat style bug, which under a\n> similar script \u200bwould certainly cause out of memory errors :)\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n> On Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi all,\n>>\n>> Suppose someone were to use the following pk_script:\n>>\n>> [op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)...,\n>> op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n>>\n>> This still seems to be valid AFAICS, and may be a potential attack vector?\n>>\n>> Thanks.\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/af36f94a/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2017-01-04T00:13:39",
                "message_text_only": "Sure, was just upper bounding it anyways. Even less of a problem!\n\n\nRE: OP_CAT, not as OP_CAT was specified, which is why it was disabled. As\nfar as I know, the elements alpha proposal to reenable a limited op_cat to\n520 bytes is somewhat controversial...\n\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\nOn Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau <jl2012 at xbt.hk> wrote:\n\n> No, there could only have not more than 201 opcodes in a script. So you\n> may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB\n>\n> For OP_CAT, just check if the returned item is within the 520 bytes limit.\n>\n> On 3 Jan 2017, at 11:27, Jeremy via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> It is an unfortunate script, but can't actually\n> \u200bdo\n>  that much\n> \u200b it seems\u200b\n> . The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.\n> \u200b Thus, it would seem the worst you could do with this would be to (10000-520*2)*520*2\n> bytes  ~=~ 10 MB.\n>\n> \u200bMuch more concerning would be the op_dup/op_cat style bug, which under a\n> similar script \u200bwould certainly cause out of memory errors :)\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n> On Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Hi all,\n>>\n>> Suppose someone were to use the following pk_script:\n>>\n>> [op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)...,\n>> op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n>>\n>> This still seems to be valid AFAICS, and may be a potential attack vector?\n>>\n>> Thanks.\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/593a2965/attachment.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2017-01-04T03:13:54",
                "message_text_only": "For the record, the OP_CAT limit of 520 bytes was added by Satoshi\n<https://github.com/bitcoin/bitcoin/commit/4bd188c4383d6e614e18f79dc337fbabe8464c82#diff-8458adcedc17d046942185cb709ff5c3R425>\non the famous August 15, 2010 \"misc\" commit, at the same time that OP_CAT\nwas disabled.\nThe previous limit was 5000 bytes.\n\nOn Tue, Jan 3, 2017 at 7:13 PM, Jeremy via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Sure, was just upper bounding it anyways. Even less of a problem!\n>\n>\n> RE: OP_CAT, not as OP_CAT was specified, which is why it was disabled. As\n> far as I know, the elements alpha proposal to reenable a limited op_cat to\n> 520 bytes is somewhat controversial...\n>\n>\n>\n> --\n> @JeremyRubin <https://twitter.com/JeremyRubin>\n> <https://twitter.com/JeremyRubin>\n>\n> On Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau <jl2012 at xbt.hk> wrote:\n>\n>> No, there could only have not more than 201 opcodes in a script. So you\n>> may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB\n>>\n>> For OP_CAT, just check if the returned item is within the 520 bytes limit.\n>>\n>> On 3 Jan 2017, at 11:27, Jeremy via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> It is an unfortunate script, but can't actually\n>> \u200bdo\n>>  that much\n>> \u200b it seems\u200b\n>> . The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.\n>> \u200b Thus, it would seem the worst you could do with this would be to (10000-520*2)*520*2\n>> bytes  ~=~ 10 MB.\n>>\n>> \u200bMuch more concerning would be the op_dup/op_cat style bug, which under a\n>> similar script \u200bwould certainly cause out of memory errors :)\n>>\n>>\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>> <https://twitter.com/JeremyRubin>\n>>\n>> On Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Hi all,\n>>>\n>>> Suppose someone were to use the following pk_script:\n>>>\n>>> [op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)...,\n>>> op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n>>>\n>>> This still seems to be valid AFAICS, and may be a potential attack\n>>> vector?\n>>>\n>>> Thanks.\n>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170103/6e35ebe6/attachment.html>"
            },
            {
                "author": "Jorge Tim\u00f3n",
                "date": "2017-01-04T14:45:54",
                "message_text_only": "I would assume that the controversial part of op_cat comes from the fact\nthat it enables covenants. Are there more concerns than that?\n\nOn 4 Jan 2017 04:14, \"Russell O'Connor via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> For the record, the OP_CAT limit of 520 bytes was added by Satoshi\n> <https://github.com/bitcoin/bitcoin/commit/4bd188c4383d6e614e18f79dc337fbabe8464c82#diff-8458adcedc17d046942185cb709ff5c3R425>\n> on the famous August 15, 2010 \"misc\" commit, at the same time that OP_CAT\n> was disabled.\n> The previous limit was 5000 bytes.\n>\n> On Tue, Jan 3, 2017 at 7:13 PM, Jeremy via bitcoin-dev <bitcoin-dev at lists.\n> linuxfoundation.org> wrote:\n>\n>> Sure, was just upper bounding it anyways. Even less of a problem!\n>>\n>>\n>> RE: OP_CAT, not as OP_CAT was specified, which is why it was disabled. As\n>> far as I know, the elements alpha proposal to reenable a limited op_cat to\n>> 520 bytes is somewhat controversial...\n>>\n>>\n>>\n>> --\n>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>> <https://twitter.com/JeremyRubin>\n>>\n>> On Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau <jl2012 at xbt.hk> wrote:\n>>\n>>> No, there could only have not more than 201 opcodes in a script. So you\n>>> may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB\n>>>\n>>> For OP_CAT, just check if the returned item is within the 520 bytes\n>>> limit.\n>>>\n>>> On 3 Jan 2017, at 11:27, Jeremy via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>> It is an unfortunate script, but can't actually\n>>> \u200bdo\n>>>  that much\n>>> \u200b it seems\u200b\n>>> . The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.\n>>> \u200b Thus, it would seem the worst you could do with this would be to (10000-520*2)*520*2\n>>> bytes  ~=~ 10 MB.\n>>>\n>>> \u200bMuch more concerning would be the op_dup/op_cat style bug, which under\n>>> a similar script \u200bwould certainly cause out of memory errors :)\n>>>\n>>>\n>>>\n>>> --\n>>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>>> <https://twitter.com/JeremyRubin>\n>>>\n>>> On Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <\n>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>\n>>>> Hi all,\n>>>>\n>>>> Suppose someone were to use the following pk_script:\n>>>>\n>>>> [op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)...,\n>>>> op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n>>>>\n>>>> This still seems to be valid AFAICS, and may be a potential attack\n>>>> vector?\n>>>>\n>>>> Thanks.\n>>>>\n>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170104/9fc14d3f/attachment.html>"
            },
            {
                "author": "Jeremy",
                "date": "2017-01-05T16:22:34",
                "message_text_only": "@Russell: Appreciate the historical note, but as that op code was\nsimultaneously disabled in that patch I don't think we can look back to how\nit was non-functionally changed (that number means nothing... maybe Satoshi\nwas trying it out with 520 bytes but then just decided to all-out disable\nit and accidentally included that code change? Hard to say what the intent\nwas.).\n\n@Jorge:\nThat's one part of it that is worth hesitation and consideration. I'm not a\nfan of the 520 byte limit as well. My gut feeling is that the \"right\"\nanswer is to compute the memory weight of the entire stack before/after\neach operation and reasonably bound it.\n\nBelow text is from the chain core documentation:\n\n\"\"\"\nMost instructions use only the data stack by removing some items and then\nplacing some items back on the stack. For these operations, we define the\nstandard memory cost applied as follows:\n\nInstruction\u2019s memory cost value is set to zero.\nFor each item removed from the data stack, instruction\u2019s memory cost is\ndecreased by 8+L where L is the length of the item in bytes.\nFor each item added to the data stack the cost is increased by 8+L where L\nis the length of the item in bytes.\n\u200b----\u200b\nEvery instruction has a cost that affects VM run limit. Total instruction\ncost consists of execution costand memory cost. Execution cost always\nreduces remaining run limit, while memory usage cost can be refunded\n(increasing the run limit) when previously used memory is released during\nVM execution.\n\"\"\"\n\n\u200bIs there a reason to favor one approach over the other? I think one reason\nto favor a direct limit on op_cat is it favors what\u200b\n\u200b\n\u200b I'll dub \"context free\" analysis, where the performance doesn't depend on\nwhat else is on the stack (perhaps by passing very large arguments to a\nscript you can cause bad behavior with a general memory limit?).\u200b On the\nother hand, the reason I prefer the general memory limit is it solves the\nproblem for all future memory-risky opcodes (or present day memory risks!).\nFurther, OP_CAT is also a bit leaky, in that you could be catting onto a\npassed in large string.  The chief argument I'm aware of against a general\nmemory limit argument is that it is tricky to make a non-implementation\ndependent memory limit (e.g., can't just call DynamicMemoryUsage on the\nstack), but I don't think this is a strong argument for several\n(semi-obvious? I can go into them if need be) reasons.\n\n\n--\n@JeremyRubin <https://twitter.com/JeremyRubin>\n<https://twitter.com/JeremyRubin>\n\nOn Wed, Jan 4, 2017 at 9:45 AM, Jorge Tim\u00f3n <jtimon at jtimon.cc> wrote:\n\n> I would assume that the controversial part of op_cat comes from the fact\n> that it enables covenants. Are there more concerns than that?\n>\n> On 4 Jan 2017 04:14, \"Russell O'Connor via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> For the record, the OP_CAT limit of 520 bytes was added by Satoshi\n>> <https://github.com/bitcoin/bitcoin/commit/4bd188c4383d6e614e18f79dc337fbabe8464c82#diff-8458adcedc17d046942185cb709ff5c3R425>\n>> on the famous August 15, 2010 \"misc\" commit, at the same time that OP_CAT\n>> was disabled.\n>> The previous limit was 5000 bytes.\n>>\n>> On Tue, Jan 3, 2017 at 7:13 PM, Jeremy via bitcoin-dev <\n>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>>> Sure, was just upper bounding it anyways. Even less of a problem!\n>>>\n>>>\n>>> RE: OP_CAT, not as OP_CAT was specified, which is why it was disabled.\n>>> As far as I know, the elements alpha proposal to reenable a limited op_cat\n>>> to 520 bytes is somewhat controversial...\n>>>\n>>>\n>>>\n>>> --\n>>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>>> <https://twitter.com/JeremyRubin>\n>>>\n>>> On Mon, Jan 2, 2017 at 10:39 PM, Johnson Lau <jl2012 at xbt.hk> wrote:\n>>>\n>>>> No, there could only have not more than 201 opcodes in a script. So you\n>>>> may have 198 OP_2DUP at most, i.e. 198 * 520 * 2 = 206kB\n>>>>\n>>>> For OP_CAT, just check if the returned item is within the 520 bytes\n>>>> limit.\n>>>>\n>>>> On 3 Jan 2017, at 11:27, Jeremy via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>> It is an unfortunate script, but can't actually\n>>>> \u200bdo\n>>>>  that much\n>>>> \u200b it seems\u200b\n>>>> . The MAX_SCRIPT_ELEMENT_SIZE = 520 Bytes.\n>>>> \u200b Thus, it would seem the worst you could do with this would be to (10000-520*2)*520*2\n>>>> bytes  ~=~ 10 MB.\n>>>>\n>>>> \u200bMuch more concerning would be the op_dup/op_cat style bug, which under\n>>>> a similar script \u200bwould certainly cause out of memory errors :)\n>>>>\n>>>>\n>>>>\n>>>> --\n>>>> @JeremyRubin <https://twitter.com/JeremyRubin>\n>>>> <https://twitter.com/JeremyRubin>\n>>>>\n>>>> On Mon, Jan 2, 2017 at 4:39 PM, Steve Davis via bitcoin-dev <\n>>>> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>>\n>>>>> Hi all,\n>>>>>\n>>>>> Suppose someone were to use the following pk_script:\n>>>>>\n>>>>> [op_2dup, op_2dup, op_2dup, op_2dup, op_2dup, ...(to limit)...,\n>>>>> op_2dup, op_hash160, <addr_hash>, op_equalverify, op_checksig]\n>>>>>\n>>>>> This still seems to be valid AFAICS, and may be a potential attack\n>>>>> vector?\n>>>>>\n>>>>> Thanks.\n>>>>>\n>>>>>\n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>>\n>>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>>>\n>>>>\n>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170105/c28f6e09/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Script Abuse Potential?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Jeremy",
                "Steve Davis",
                "Johnson Lau",
                "Russell O'Connor",
                "Jorge Tim\u00f3n"
            ],
            "messages_count": 8,
            "total_messages_chars_count": 20349
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Core 0.13.2 released",
        "thread_messages": [
            {
                "author": "Wladimir J. van der Laan",
                "date": "2017-01-03T08:47:36",
                "message_text_only": "-----BEGIN PGP SIGNED MESSAGE-----\nHash: SHA512\n\nBitcoin Core version 0.13.2 is now available from:\n\n  <https://bitcoin.org/bin/bitcoin-core-0.13.2/>\n\nOr by bittorrent:\n\n  magnet:?xt=urn:btih:746697d03db3ff531158b1133bab5d1e4cef4e5a&dn=bitcoin-core-0.13.2&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2F\n\nThis is a new minor version release, including various bugfixes and\nperformance improvements, as well as updated translations.\n\nPlease report bugs using the issue tracker at github:\n\n  <https://github.com/bitcoin/bitcoin/issues>\n\nTo receive security and update notifications, please subscribe to:\n\n  <https://bitcoincore.org/en/list/announcements/join/>\n\nCompatibility\n==============\n\nMicrosoft ended support for Windows XP on [April 8th, 2014](https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support),\nan OS initially released in 2001. This means that not even critical security\nupdates will be released anymore. Without security updates, using a bitcoin\nwallet on a XP machine is irresponsible at least.\n\nIn addition to that, with 0.12.x there have been varied reports of Bitcoin Core\nrandomly crashing on Windows XP. It is [not clear](https://github.com/bitcoin/bitcoin/issues/7681#issuecomment-217439891)\nwhat the source of these crashes is, but it is likely that upstream\nlibraries such as Qt are no longer being tested on XP.\n\nWe do not have time nor resources to provide support for an OS that is\nend-of-life. From 0.13.0 on, Windows XP is no longer supported. Users are\nsuggested to upgrade to a newer version of Windows, or install an alternative OS\nthat is supported.\n\nNo attempt is made to prevent installing or running the software on Windows XP,\nyou can still do so at your own risk, but do not expect it to work: do not\nreport issues about Windows XP to the issue tracker.\n\n- From 0.13.1 onwards OS X 10.7 is no longer supported. 0.13.0 was intended to work on 10.7+, \nbut severe issues with the libc++ version on 10.7.x keep it from running reliably. \n0.13.1 now requires 10.8+, and will communicate that to 10.7 users, rather than crashing unexpectedly.\n\nNotable changes\n===============\n\nChange to wallet handling of mempool rejection\n- -----------------------------------------------\n\nWhen a newly created transaction failed to enter the mempool due to\nthe limits on chains of unconfirmed transactions the sending RPC\ncalls would return an error.  The transaction would still be queued\nin the wallet and, once some of the parent transactions were\nconfirmed, broadcast after the software was restarted.\n\nThis behavior has been changed to return success and to reattempt\nmempool insertion at the same time transaction rebroadcast is\nattempted, avoiding a need for a restart.\n\nTransactions in the wallet which cannot be accepted into the mempool\ncan be abandoned with the previously existing abandontransaction RPC\n(or in the GUI via a context menu on the transaction).\n\n\n0.13.2 Change log\n=================\n\nDetailed release notes follow. This overview includes changes that affect\nbehavior, not code moves, refactors and string updates. For convenience in locating\nthe code changes and accompanying discussion, both the pull request and\ngit merge commit are mentioned.\n\n### Consensus\n- - #9293 `e591c10` [0.13 Backport #9053] IBD using chainwork instead of height and not using header timestamp (gmaxwell)\n- - #9053 `5b93eee` IBD using chainwork instead of height and not using header timestamps (gmaxwell)\n\n### RPC and other APIs\n- - #8845 `1d048b9` Don't return the address of a P2SH of a P2SH (jnewbery)\n- - #9041 `87fbced` keypoololdest denote Unix epoch, not GMT (s-matthew-english)\n- - #9122 `f82c81b` fix getnettotals RPC description about timemillis (visvirial)\n- - #9042 `5bcb05d` [rpc] ParseHash: Fail when length is not 64 (MarcoFalke)\n- - #9194 `f26dab7` Add option to return non-segwit serialization via rpc (instagibbs)\n- - #9347 `b711390` [0.13.2] wallet/rpc backports (MarcoFalke)\n- - #9292 `c365556` Complain when unknown rpcserialversion is specified (sipa)\n- - #9322 `49a612f` [qa] Don't set unknown rpcserialversion (MarcoFalke)\n\n### Block and transaction handling\n- - #8357 `ce0d817` [mempool] Fix relaypriority calculation error (maiiz)\n- - #9267 `0a4aa87` [0.13 backport #9239] Disable fee estimates for a confirm target of 1 block (morcos)\n- - #9196 `0c09d9f` Send tip change notification from invalidateblock (ryanofsky)\n\n### P2P protocol and network code\n- - #8995 `9ef3875` Add missing cs_main lock to ::GETBLOCKTXN processing (TheBlueMatt)\n- - #9234 `94531b5` torcontrol: Explicitly request RSA1024 private key (laanwj)\n- - #8637 `2cad5db` Compact Block Tweaks (rebase of #8235) (sipa)\n- - #9058 `286e548` Fixes for p2p-compactblocks.py test timeouts on travis (#8842) (ryanofsky)\n- - #8865 `4c71fc4` Decouple peer-processing-logic from block-connection-logic (TheBlueMatt)\n- - #9117 `6fe3981` net: don't send feefilter messages before the version handshake is complete (theuni)\n- - #9188 `ca1fd75` Make orphan parent fetching ask for witnesses (gmaxwell)\n- - #9052 `3a3bcbf` Use RelevantServices instead of node_network in AttemptToEvict (gmaxwell)\n- - #9048 `9460771` [0.13 backport #9026] Fix handling of invalid compact blocks (sdaftuar)\n- - #9357 `03b6f62` [0.13 backport #9352] Attempt reconstruction from all compact block announcements (sdaftuar)\n- - #9189 `b96a8f7` Always add default_witness_commitment with GBT client support (sipa)\n- - #9253 `28d0f22` Fix calculation of number of bound sockets to use (TheBlueMatt)\n- - #9199 `da5a16b` Always drop the least preferred HB peer when adding a new one (gmaxwell)\n\n### Build system\n- - #9169 `d1b4da9` build: fix qt5.7 build under macOS (theuni)\n- - #9326 `a0f7ece` Update for OpenSSL 1.1 API (gmaxwell)\n- - #9224 `396c405` Prevent FD_SETSIZE error building on OpenBSD (ivdsangen)\n\n### GUI\n- - #8972 `6f86b53` Make warnings label selectable (jonasschnelli) (MarcoFalke)\n- - #9185 `6d70a73` Fix coincontrol sort issue (jonasschnelli)\n- - #9094 `5f3a12c` Use correct conversion function for boost::path datadir (laanwj)\n- - #8908 `4a974b2` Update bitcoin-qt.desktop (s-matthew-english)\n- - #9190 `dc46b10` Plug many memory leaks (laanwj)\n\n### Wallet\n- - #9290 `35174a0` Make RelayWalletTransaction attempt to AcceptToMemoryPool (gmaxwell)\n- - #9295 `43bcfca` Bugfix: Fundrawtransaction: don't terminate when keypool is empty (jonasschnelli)\n- - #9302 `f5d606e` Return txid even if ATMP fails for new transaction (sipa)\n- - #9262 `fe39f26` Prefer coins that have fewer ancestors, sanity check txn before ATMP (instagibbs)\n\n### Tests and QA\n- - #9159 `eca9b46` Wait for specific block announcement in p2p-compactblocks (ryanofsky)\n- - #9186 `dccdc3a` Fix use-after-free in scheduler tests (laanwj)\n- - #9168 `3107280` Add assert_raises_message to check specific error message (mrbandrews)\n- - #9191 `29435db` 0.13.2 Backports (MarcoFalke)\n- - #9077 `1d4c884` Increase wallet-dump RPC timeout (ryanofsky)\n- - #9098 `ecd7db5` Handle zombies and cluttered tmpdirs (MarcoFalke)\n- - #8927 `387ec9d` Add script tests for FindAndDelete in pre-segwit and segwit scripts (jl2012)\n- - #9200 `eebc699` bench: Fix subtle counting issue when rescaling iteration count (laanwj)\n\n### Miscellaneous\n- - #8838 `094848b` Calculate size and weight of block correctly in CreateNewBlock() (jnewbery)\n- - #8920 `40169dc` Set minimum required Boost to 1.47.0 (fanquake)\n- - #9251 `a710a43` Improvement of documentation of command line parameter 'whitelist' (wodry)\n- - #8932 `106da69` Allow bitcoin-tx to create v2 transactions (btcdrak)\n- - #8929 `12428b4` add software-properties-common (sigwo)\n- - #9120 `08d1c90` bug: Missed one \"return false\" in recent refactoring in #9067 (UdjinM6)\n- - #9067 `f85ee01` Fix exit codes (UdjinM6)\n- - #9340 `fb987b3` [0.13] Update secp256k1 subtree (MarcoFalke)\n- - #9229 `b172377` Remove calls to getaddrinfo_a (TheBlueMatt)\n\nCredits\n=======\n\nThanks to everyone who directly contributed to this release:\n\n- - Alex Morcos\n- - BtcDrak\n- - Cory Fields\n- - fanquake\n- - Gregory Maxwell\n- - Gregory Sanders\n- - instagibbs\n- - Ivo van der Sangen\n- - jnewbery\n- - Johnson Lau\n- - Jonas Schnelli\n- - Luke Dashjr\n- - maiiz\n- - MarcoFalke\n- - Masahiko Hyuga\n- - Matt Corallo\n- - matthias\n- - mrbandrews\n- - Pavel Jan\u00edk\n- - Pieter Wuille\n- - randy-waterhouse\n- - Russell Yanofsky\n- - S. Matthew English\n- - Steven\n- - Suhas Daftuar\n- - UdjinM6\n- - Wladimir J. van der Laan\n- - wodry\n\nAs well as everyone that helped translating on [Transifex](https://www.transifex.com/projects/p/bitcoin/).\n\n-----BEGIN PGP SIGNATURE-----\nVersion: GnuPG v1\n\niQEcBAEBCgAGBQJYa2IbAAoJEHSBCwEjRsmmiQsIALbkHVVwO7nViQKH1Ub2qpD4\nTplOuAP0/4vYotizuI12Gqdnu8SjPmhKwAgIXhVinE6TS4OzGNjy+6LtWGzpcpud\nB1pcziZ72Mlfxdbdd1UhDMWEjoBumS9RmXMSqzTlMVlHRv4iiISzdaAROu1jHvdF\nYTsnmKXB8OvcXOecxRMY9LrnpSzLALM2MYTDmYwlhhExHIA8ZqI2niky6GCfyfDi\nKD7bgfIFJzlgFTpAdhQXOXtWoRV5iHqN7T29ot8Y+yIhVCRhHYXS93Z50GKbkqYV\nMXsVAkpZF3qqcKYSPFjbif7faMdrMqcEiII6QhXdDTRGI/35IfuTDbWzzQlnVyY=\n=ncCY\n-----END PGP SIGNATURE-----"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Core 0.13.2 released",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Wladimir J. van der Laan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9227
        }
    },
    {
        "title": "[bitcoin-dev] [Meta] Re:  Bitcoin Core 0.13.2 released",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2017-01-07T03:25:47",
                "message_text_only": "I don't think release announcements are really appropriate for the bitcoin-dev \nmailing list. People who want these can subscribe to the bitcoin-core-dev list \nand/or the Core announce mailing list. Maybe sending to bitcoin-discuss would \nalso make sense, but not bitcoin-dev...\n\nLuke\n\n\nOn Tuesday, January 03, 2017 8:47:36 AM Wladimir J. van der Laan via bitcoin-\ndev wrote:\n> Bitcoin Core version 0.13.2 is now available from:\n> \n>   <https://bitcoin.org/bin/bitcoin-core-0.13.2/>\n> \n> Or by bittorrent:\n> \n>  \n> magnet:?xt=urn:btih:746697d03db3ff531158b1133bab5d1e4cef4e5a&dn=bitcoin-co\n> re-0.13.2&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%\n> 3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.ccc.de%\n> 3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%\n> 2Ftracker.leechers-paradise.org%3A6969&ws=https%3A%2F%2Fbitcoin.org%2Fbin%2\n> F\n> \n> This is a new minor version release, including various bugfixes and\n> performance improvements, as well as updated translations.\n> \n> Please report bugs using the issue tracker at github:\n> \n>   <https://github.com/bitcoin/bitcoin/issues>\n> \n> To receive security and update notifications, please subscribe to:\n> \n>   <https://bitcoincore.org/en/list/announcements/join/>\n> \n> Compatibility\n> ==============\n> \n> Microsoft ended support for Windows XP on [April 8th,\n> 2014](https://www.microsoft.com/en-us/WindowsForBusiness/end-of-xp-support\n> ), an OS initially released in 2001. This means that not even critical\n> security updates will be released anymore. Without security updates, using\n> a bitcoin wallet on a XP machine is irresponsible at least.\n> \n> In addition to that, with 0.12.x there have been varied reports of Bitcoin\n> Core randomly crashing on Windows XP. It is [not\n> clear](https://github.com/bitcoin/bitcoin/issues/7681#issuecomment-2174398\n> 91) what the source of these crashes is, but it is likely that upstream\n> libraries such as Qt are no longer being tested on XP.\n> \n> We do not have time nor resources to provide support for an OS that is\n> end-of-life. From 0.13.0 on, Windows XP is no longer supported. Users are\n> suggested to upgrade to a newer version of Windows, or install an\n> alternative OS that is supported.\n> \n> No attempt is made to prevent installing or running the software on Windows\n> XP, you can still do so at your own risk, but do not expect it to work: do\n> not report issues about Windows XP to the issue tracker.\n> \n> From 0.13.1 onwards OS X 10.7 is no longer supported. 0.13.0 was intended\n> to work on 10.7+, but severe issues with the libc++ version on 10.7.x keep\n> it from running reliably. 0.13.1 now requires 10.8+, and will communicate\n> that to 10.7 users, rather than crashing unexpectedly.\n> \n> Notable changes\n> ===============\n> \n> Change to wallet handling of mempool rejection\n> -----------------------------------------------\n> \n> When a newly created transaction failed to enter the mempool due to\n> the limits on chains of unconfirmed transactions the sending RPC\n> calls would return an error.  The transaction would still be queued\n> in the wallet and, once some of the parent transactions were\n> confirmed, broadcast after the software was restarted.\n> \n> This behavior has been changed to return success and to reattempt\n> mempool insertion at the same time transaction rebroadcast is\n> attempted, avoiding a need for a restart.\n> \n> Transactions in the wallet which cannot be accepted into the mempool\n> can be abandoned with the previously existing abandontransaction RPC\n> (or in the GUI via a context menu on the transaction).\n> \n> \n> 0.13.2 Change log\n> =================\n> \n> Detailed release notes follow. This overview includes changes that affect\n> behavior, not code moves, refactors and string updates. For convenience in\n> locating the code changes and accompanying discussion, both the pull\n> request and git merge commit are mentioned.\n> \n> ### Consensus\n> - #9293 `e591c10` [0.13 Backport #9053] IBD using chainwork instead of\n> height and not using header timestamp (gmaxwell) - #9053 `5b93eee` IBD\n> using chainwork instead of height and not using header timestamps\n> (gmaxwell)\n> \n> ### RPC and other APIs\n> - #8845 `1d048b9` Don't return the address of a P2SH of a P2SH (jnewbery)\n> - #9041 `87fbced` keypoololdest denote Unix epoch, not GMT\n> (s-matthew-english) - #9122 `f82c81b` fix getnettotals RPC description\n> about timemillis (visvirial) - #9042 `5bcb05d` [rpc] ParseHash: Fail when\n> length is not 64 (MarcoFalke) - #9194 `f26dab7` Add option to return\n> non-segwit serialization via rpc (instagibbs) - #9347 `b711390` [0.13.2]\n> wallet/rpc backports (MarcoFalke)\n> - #9292 `c365556` Complain when unknown rpcserialversion is specified\n> (sipa) - #9322 `49a612f` [qa] Don't set unknown rpcserialversion\n> (MarcoFalke)\n> \n> ### Block and transaction handling\n> - #8357 `ce0d817` [mempool] Fix relaypriority calculation error (maiiz)\n> - #9267 `0a4aa87` [0.13 backport #9239] Disable fee estimates for a confirm\n> target of 1 block (morcos) - #9196 `0c09d9f` Send tip change notification\n> from invalidateblock (ryanofsky)\n> \n> ### P2P protocol and network code\n> - #8995 `9ef3875` Add missing cs_main lock to ::GETBLOCKTXN processing\n> (TheBlueMatt) - #9234 `94531b5` torcontrol: Explicitly request RSA1024\n> private key (laanwj) - #8637 `2cad5db` Compact Block Tweaks (rebase of\n> #8235) (sipa)\n> - #9058 `286e548` Fixes for p2p-compactblocks.py test timeouts on travis\n> (#8842) (ryanofsky) - #8865 `4c71fc4` Decouple peer-processing-logic from\n> block-connection-logic (TheBlueMatt) - #9117 `6fe3981` net: don't send\n> feefilter messages before the version handshake is complete (theuni) -\n> #9188 `ca1fd75` Make orphan parent fetching ask for witnesses (gmaxwell) -\n> #9052 `3a3bcbf` Use RelevantServices instead of node_network in\n> AttemptToEvict (gmaxwell) - #9048 `9460771` [0.13 backport #9026] Fix\n> handling of invalid compact blocks (sdaftuar) - #9357 `03b6f62` [0.13\n> backport #9352] Attempt reconstruction from all compact block\n> announcements (sdaftuar) - #9189 `b96a8f7` Always add\n> default_witness_commitment with GBT client support (sipa) - #9253\n> `28d0f22` Fix calculation of number of bound sockets to use (TheBlueMatt)\n> - #9199 `da5a16b` Always drop the least preferred HB peer when adding a\n> new one (gmaxwell)\n> \n> ### Build system\n> - #9169 `d1b4da9` build: fix qt5.7 build under macOS (theuni)\n> - #9326 `a0f7ece` Update for OpenSSL 1.1 API (gmaxwell)\n> - #9224 `396c405` Prevent FD_SETSIZE error building on OpenBSD (ivdsangen)\n> \n> ### GUI\n> - #8972 `6f86b53` Make warnings label selectable (jonasschnelli)\n> (MarcoFalke) - #9185 `6d70a73` Fix coincontrol sort issue (jonasschnelli)\n> - #9094 `5f3a12c` Use correct conversion function for boost::path datadir\n> (laanwj) - #8908 `4a974b2` Update bitcoin-qt.desktop (s-matthew-english)\n> - #9190 `dc46b10` Plug many memory leaks (laanwj)\n> \n> ### Wallet\n> - #9290 `35174a0` Make RelayWalletTransaction attempt to AcceptToMemoryPool\n> (gmaxwell) - #9295 `43bcfca` Bugfix: Fundrawtransaction: don't terminate\n> when keypool is empty (jonasschnelli) - #9302 `f5d606e` Return txid even\n> if ATMP fails for new transaction (sipa) - #9262 `fe39f26` Prefer coins\n> that have fewer ancestors, sanity check txn before ATMP (instagibbs)\n> \n> ### Tests and QA\n> - #9159 `eca9b46` Wait for specific block announcement in p2p-compactblocks\n> (ryanofsky) - #9186 `dccdc3a` Fix use-after-free in scheduler tests\n> (laanwj)\n> - #9168 `3107280` Add assert_raises_message to check specific error message\n> (mrbandrews) - #9191 `29435db` 0.13.2 Backports (MarcoFalke)\n> - #9077 `1d4c884` Increase wallet-dump RPC timeout (ryanofsky)\n> - #9098 `ecd7db5` Handle zombies and cluttered tmpdirs (MarcoFalke)\n> - #8927 `387ec9d` Add script tests for FindAndDelete in pre-segwit and\n> segwit scripts (jl2012) - #9200 `eebc699` bench: Fix subtle counting issue\n> when rescaling iteration count (laanwj)\n> \n> ### Miscellaneous\n> - #8838 `094848b` Calculate size and weight of block correctly in\n> CreateNewBlock() (jnewbery) - #8920 `40169dc` Set minimum required Boost\n> to 1.47.0 (fanquake)\n> - #9251 `a710a43` Improvement of documentation of command line parameter\n> 'whitelist' (wodry) - #8932 `106da69` Allow bitcoin-tx to create v2\n> transactions (btcdrak) - #8929 `12428b4` add software-properties-common\n> (sigwo)\n> - #9120 `08d1c90` bug: Missed one \"return false\" in recent refactoring in\n> #9067 (UdjinM6) - #9067 `f85ee01` Fix exit codes (UdjinM6)\n> - #9340 `fb987b3` [0.13] Update secp256k1 subtree (MarcoFalke)\n> - #9229 `b172377` Remove calls to getaddrinfo_a (TheBlueMatt)\n> \n> Credits\n> =======\n> \n> Thanks to everyone who directly contributed to this release:\n> \n> - Alex Morcos\n> - BtcDrak\n> - Cory Fields\n> - fanquake\n> - Gregory Maxwell\n> - Gregory Sanders\n> - instagibbs\n> - Ivo van der Sangen\n> - jnewbery\n> - Johnson Lau\n> - Jonas Schnelli\n> - Luke Dashjr\n> - maiiz\n> - MarcoFalke\n> - Masahiko Hyuga\n> - Matt Corallo\n> - matthias\n> - mrbandrews\n> - Pavel Jan\u00edk\n> - Pieter Wuille\n> - randy-waterhouse\n> - Russell Yanofsky\n> - S. Matthew English\n> - Steven\n> - Suhas Daftuar\n> - UdjinM6\n> - Wladimir J. van der Laan\n> - wodry\n> \n> As well as everyone that helped translating on\n> [Transifex](https://www.transifex.com/projects/p/bitcoin/).\n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Re:  Bitcoin Core 0.13.2 released",
            "categories": [
                "bitcoin-dev",
                "Meta"
            ],
            "authors": [
                "Luke Dashjr"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 9517
        }
    },
    {
        "title": "[bitcoin-dev] Bitcoin Classic 1.2.0 released",
        "thread_messages": [
            {
                "author": "Tom Zander",
                "date": "2017-01-06T10:16:28",
                "message_text_only": "Bitcoin Classic version 1.2.0 is now available from;\n\n <https://bitcoinclassic.com/gettingstarted.html>\n\nThis is a new major version release, including new features, various \nbugfixes and performance improvements.\n\nThis release marks a change in strategy for Bitcoin Classic, moving from the \nvery conservative block size proposal based on compromise to one where \nClassic truly innovates and provides a long term solution for the market to \nchoose and leave behind the restrictions of the old.\n\nThe most visible change in this version is the decentralised block size \nsolution where node operators decide on the maximum size.\n\nBitcoin Classic is focused on providing users a way to get onto the Bitcoin \nnetwork using a high quality validating node for a large set of use cases. \nClassic presents top notch quality processes in this release, to help anyone \nrunning Bitcoin.\n\nWe include in this release various projects with the beta label. People who \nwant to use the Classic node as an on-ramp to Bitcoin will find them \ninteresting. These projects will need to be enabled in the config by those \nthat want to test them.\n\nMore background information on this release and Classic can be seen in this \nvideo: https://vimeo.com/192789752\nThe full release notes are on github at \nhttps://github.com/bitcoinclassic/bitcoinclassic/releases/tag/v1.2.0\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Jonas Schnelli",
                "date": "2017-01-07T08:13:09",
                "message_text_only": "Hi Tom\n\nPlease don't post release announcements for software that is\nincompatible with the current bitcoin consensus rules here.\nOtherwise we give green-lights to any sorts of altcoin to post their\nreleases here.\n\nThanks\n\n</jonas>\n> Bitcoin Classic version 1.2.0 is now available from;\n>\n>  <https://bitcoinclassic.com/gettingstarted.html>\n>\n> This is a new major version release, including new features, various \n> bugfixes and performance improvements.\n>\n> This release marks a change in strategy for Bitcoin Classic, moving from the \n> very conservative block size proposal based on compromise to one where \n> Classic truly innovates and provides a long term solution for the market to \n> choose and leave behind the restrictions of the old.\n>\n> The most visible change in this version is the decentralised block size \n> solution where node operators decide on the maximum size.\n>\n> Bitcoin Classic is focused on providing users a way to get onto the Bitcoin \n> network using a high quality validating node for a large set of use cases. \n> Classic presents top notch quality processes in this release, to help anyone \n> running Bitcoin.\n>\n> We include in this release various projects with the beta label. People who \n> want to use the Classic node as an on-ramp to Bitcoin will find them \n> interesting. These projects will need to be enabled in the config by those \n> that want to test them.\n>\n> More background information on this release and Classic can be seen in this \n> video: https://vimeo.com/192789752\n> The full release notes are on github at \n> https://github.com/bitcoinclassic/bitcoinclassic/releases/tag/v1.2.0\n>\n\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 833 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/c17797bd/attachment.sig>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2017-01-07T08:55:19",
                "message_text_only": "Your release announcement does not make it clear that Bitcoin Classic is\nincompatible with the current Bitcoin network and its consensus rules. It\nis a hard fork on mainnet with no safe activation as well as including\nother unsafe changes. There is also no BIP for the hard fork. There is also\nno evidence of community wide consensus for such a hard fork. This is\ndangerous and irresponsible.\n\n\nIt's wrong to announce software without correctly informing people about\nthe contents or risks. Furthermore, there are no release notes in\nhttps://github.com/bitcoinclassic/bitcoinclassic/tree/v1.2.0/doc nor\nchangelog. Without those, it is almost impossible for average users to know\nwhat is under the hood or what has changed and time consuming for\ndevelopers to assess.\n\nOn Fri, Jan 6, 2017 at 2:16 AM, Tom Zander via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Bitcoin Classic version 1.2.0 is now available from;\n>\n>  <https://bitcoinclassic.com/gettingstarted.html>\n>\n> This is a new major version release, including new features, various\n> bugfixes and performance improvements.\n>\n> This release marks a change in strategy for Bitcoin Classic, moving from\n> the\n> very conservative block size proposal based on compromise to one where\n> Classic truly innovates and provides a long term solution for the market to\n> choose and leave behind the restrictions of the old.\n>\n> The most visible change in this version is the decentralised block size\n> solution where node operators decide on the maximum size.\n>\n> Bitcoin Classic is focused on providing users a way to get onto the Bitcoin\n> network using a high quality validating node for a large set of use cases.\n> Classic presents top notch quality processes in this release, to help\n> anyone\n> running Bitcoin.\n>\n> We include in this release various projects with the beta label. People who\n> want to use the Classic node as an on-ramp to Bitcoin will find them\n> interesting. These projects will need to be enabled in the config by those\n> that want to test them.\n>\n> More background information on this release and Classic can be seen in this\n> video: https://vimeo.com/192789752\n> The full release notes are on github at\n> https://github.com/bitcoinclassic/bitcoinclassic/releases/tag/v1.2.0\n>\n> --\n> Tom Zander\n> Blog: https://zander.github.io\n> Vlog: https://vimeo.com/channels/tomscryptochannel\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/dd0f3671/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-01-07T15:15:30",
                "message_text_only": "On Saturday, 7 January 2017 00:55:19 CET Eric Lombrozo wrote:\n> Your release announcement does not make it clear that Bitcoin Classic is\n> incompatible with the current Bitcoin network and its consensus rules.\n\nTo explain why I didn't write that;\n\nBitcoin Classic is not incompatible with the current Bitcoin network and its \nconsensus rules.\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-07T20:12:29",
                "message_text_only": "Bitcoin Classic only activates if 75% of the network adopts it. That\nis not irresponsible or dangerous. It would only be dangerous if it\nactivates at 50%, because that would create a situation where its not\nclear which side of the fork has the most proof of work.\n\nOn 1/7/17, Eric Lombrozo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Your release announcement does not make it clear that Bitcoin Classic is\n> incompatible with the current Bitcoin network and its consensus rules. It\n> is a hard fork on mainnet with no safe activation as well as including\n> other unsafe changes. There is also no BIP for the hard fork. There is also\n> no evidence of community wide consensus for such a hard fork. This is\n> dangerous and irresponsible.\n>\n>\n> It's wrong to announce software without correctly informing people about\n> the contents or risks. Furthermore, there are no release notes in\n> https://github.com/bitcoinclassic/bitcoinclassic/tree/v1.2.0/doc nor\n> changelog. Without those, it is almost impossible for average users to know\n> what is under the hood or what has changed and time consuming for\n> developers to assess.\n>\n> On Fri, Jan 6, 2017 at 2:16 AM, Tom Zander via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Bitcoin Classic version 1.2.0 is now available from;\n>>\n>>  <https://bitcoinclassic.com/gettingstarted.html>\n>>\n>> This is a new major version release, including new features, various\n>> bugfixes and performance improvements.\n>>\n>> This release marks a change in strategy for Bitcoin Classic, moving from\n>> the\n>> very conservative block size proposal based on compromise to one where\n>> Classic truly innovates and provides a long term solution for the market\n>> to\n>> choose and leave behind the restrictions of the old.\n>>\n>> The most visible change in this version is the decentralised block size\n>> solution where node operators decide on the maximum size.\n>>\n>> Bitcoin Classic is focused on providing users a way to get onto the\n>> Bitcoin\n>> network using a high quality validating node for a large set of use\n>> cases.\n>> Classic presents top notch quality processes in this release, to help\n>> anyone\n>> running Bitcoin.\n>>\n>> We include in this release various projects with the beta label. People\n>> who\n>> want to use the Classic node as an on-ramp to Bitcoin will find them\n>> interesting. These projects will need to be enabled in the config by\n>> those\n>> that want to test them.\n>>\n>> More background information on this release and Classic can be seen in\n>> this\n>> video: https://vimeo.com/192789752\n>> The full release notes are on github at\n>> https://github.com/bitcoinclassic/bitcoinclassic/releases/tag/v1.2.0\n>>\n>> --\n>> Tom Zander\n>> Blog: https://zander.github.io\n>> Vlog: https://vimeo.com/channels/tomscryptochannel\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>"
            },
            {
                "author": "David Vorick",
                "date": "2017-01-07T20:17:58",
                "message_text_only": "No, Bitcoin classic only activates if 75% of the _miners_ adopt it. That\nsays nothing about the broader network and indeed is much easier to achieve\nthrough politicking, bribery, coercion, and other tomfoolery as 75% of the\nhashrate is ultimately only a dozen people or so.\n\nYou have plenty of channels through which you can make your announcements,\nthis particular one is not okay.\n\nOn Jan 7, 2017 3:12 PM, \"Chris Priest via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Bitcoin Classic only activates if 75% of the network adopts it. That\n> is not irresponsible or dangerous. It would only be dangerous if it\n> activates at 50%, because that would create a situation where its not\n> clear which side of the fork has the most proof of work.\n>\n> On 1/7/17, Eric Lombrozo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Your release announcement does not make it clear that Bitcoin Classic is\n> > incompatible with the current Bitcoin network and its consensus rules. It\n> > is a hard fork on mainnet with no safe activation as well as including\n> > other unsafe changes. There is also no BIP for the hard fork. There is\n> also\n> > no evidence of community wide consensus for such a hard fork. This is\n> > dangerous and irresponsible.\n> >\n> >\n> > It's wrong to announce software without correctly informing people about\n> > the contents or risks. Furthermore, there are no release notes in\n> > https://github.com/bitcoinclassic/bitcoinclassic/tree/v1.2.0/doc nor\n> > changelog. Without those, it is almost impossible for average users to\n> know\n> > what is under the hood or what has changed and time consuming for\n> > developers to assess.\n> >\n> > On Fri, Jan 6, 2017 at 2:16 AM, Tom Zander via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> >> Bitcoin Classic version 1.2.0 is now available from;\n> >>\n> >>  <https://bitcoinclassic.com/gettingstarted.html>\n> >>\n> >> This is a new major version release, including new features, various\n> >> bugfixes and performance improvements.\n> >>\n> >> This release marks a change in strategy for Bitcoin Classic, moving from\n> >> the\n> >> very conservative block size proposal based on compromise to one where\n> >> Classic truly innovates and provides a long term solution for the market\n> >> to\n> >> choose and leave behind the restrictions of the old.\n> >>\n> >> The most visible change in this version is the decentralised block size\n> >> solution where node operators decide on the maximum size.\n> >>\n> >> Bitcoin Classic is focused on providing users a way to get onto the\n> >> Bitcoin\n> >> network using a high quality validating node for a large set of use\n> >> cases.\n> >> Classic presents top notch quality processes in this release, to help\n> >> anyone\n> >> running Bitcoin.\n> >>\n> >> We include in this release various projects with the beta label. People\n> >> who\n> >> want to use the Classic node as an on-ramp to Bitcoin will find them\n> >> interesting. These projects will need to be enabled in the config by\n> >> those\n> >> that want to test them.\n> >>\n> >> More background information on this release and Classic can be seen in\n> >> this\n> >> video: https://vimeo.com/192789752\n> >> The full release notes are on github at\n> >> https://github.com/bitcoinclassic/bitcoinclassic/releases/tag/v1.2.0\n> >>\n> >> --\n> >> Tom Zander\n> >> Blog: https://zander.github.io\n> >> Vlog: https://vimeo.com/channels/tomscryptochannel\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>\n> >\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/cbe422c4/attachment.html>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-07T20:26:30",
                "message_text_only": "Bitcoin Classic only changes the block format (by changing the rule\nthat they have to be 1MB or less). Miners are the only ones who make\nblocks, so they are the only ones who mater when it comes to changing\nblock rules. Nodes, wallets and other software are not affected by\nchanging block rules. Unlike segwit, where *everybody* has to write\ncode to support the new transaction format.\n\nAlso, it doesn't matter that 75% of hashpower is made up of a dozen\npeople. That's how the system works, it's not a matter of opinion. If\nyou are just a node or just a wallet, and you want your voice to\nmatter, then you need to get a hold of some hashpower.\n\n\nOn 1/7/17, David Vorick <david.vorick at gmail.com> wrote:\n> No, Bitcoin classic only activates if 75% of the _miners_ adopt it. That\n> says nothing about the broader network and indeed is much easier to achieve\n> through politicking, bribery, coercion, and other tomfoolery as 75% of the\n> hashrate is ultimately only a dozen people or so.\n>\n> You have plenty of channels through which you can make your announcements,\n> this particular one is not okay.\n>\n> On Jan 7, 2017 3:12 PM, \"Chris Priest via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> Bitcoin Classic only activates if 75% of the network adopts it. That\n>> is not irresponsible or dangerous. It would only be dangerous if it\n>> activates at 50%, because that would create a situation where its not\n>> clear which side of the fork has the most proof of work.\n>>\n>> On 1/7/17, Eric Lombrozo via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> > Your release announcement does not make it clear that Bitcoin Classic\n>> > is\n>> > incompatible with the current Bitcoin network and its consensus rules.\n>> > It\n>> > is a hard fork on mainnet with no safe activation as well as including\n>> > other unsafe changes. There is also no BIP for the hard fork. There is\n>> also\n>> > no evidence of community wide consensus for such a hard fork. This is\n>> > dangerous and irresponsible.\n>> >\n>> >\n>> > It's wrong to announce software without correctly informing people\n>> > about\n>> > the contents or risks. Furthermore, there are no release notes in\n>> > https://github.com/bitcoinclassic/bitcoinclassic/tree/v1.2.0/doc nor\n>> > changelog. Without those, it is almost impossible for average users to\n>> know\n>> > what is under the hood or what has changed and time consuming for\n>> > developers to assess.\n>> >\n>> > On Fri, Jan 6, 2017 at 2:16 AM, Tom Zander via bitcoin-dev <\n>> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> >\n>> >> Bitcoin Classic version 1.2.0 is now available from;\n>> >>\n>> >>  <https://bitcoinclassic.com/gettingstarted.html>\n>> >>\n>> >> This is a new major version release, including new features, various\n>> >> bugfixes and performance improvements.\n>> >>\n>> >> This release marks a change in strategy for Bitcoin Classic, moving\n>> >> from\n>> >> the\n>> >> very conservative block size proposal based on compromise to one where\n>> >> Classic truly innovates and provides a long term solution for the\n>> >> market\n>> >> to\n>> >> choose and leave behind the restrictions of the old.\n>> >>\n>> >> The most visible change in this version is the decentralised block\n>> >> size\n>> >> solution where node operators decide on the maximum size.\n>> >>\n>> >> Bitcoin Classic is focused on providing users a way to get onto the\n>> >> Bitcoin\n>> >> network using a high quality validating node for a large set of use\n>> >> cases.\n>> >> Classic presents top notch quality processes in this release, to help\n>> >> anyone\n>> >> running Bitcoin.\n>> >>\n>> >> We include in this release various projects with the beta label.\n>> >> People\n>> >> who\n>> >> want to use the Classic node as an on-ramp to Bitcoin will find them\n>> >> interesting. These projects will need to be enabled in the config by\n>> >> those\n>> >> that want to test them.\n>> >>\n>> >> More background information on this release and Classic can be seen in\n>> >> this\n>> >> video: https://vimeo.com/192789752\n>> >> The full release notes are on github at\n>> >> https://github.com/bitcoinclassic/bitcoinclassic/releases/tag/v1.2.0\n>> >>\n>> >> --\n>> >> Tom Zander\n>> >> Blog: https://zander.github.io\n>> >> Vlog: https://vimeo.com/channels/tomscryptochannel\n>> >> _______________________________________________\n>> >> bitcoin-dev mailing list\n>> >> bitcoin-dev at lists.linuxfoundation.org\n>> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>> >>\n>> >\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-01-07T21:14:28",
                "message_text_only": "On 01/07/2017 12:26 PM, Chris Priest via bitcoin-dev wrote:\n\n> ... it doesn't matter that 75% of hashpower is made up of a dozen\n> people. That's how the system works, it's not a matter of opinion.\n\nIt's a bug, not a feature.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/f6ddc01a/attachment.sig>"
            },
            {
                "author": "Aymeric Vitte",
                "date": "2017-01-07T23:10:50",
                "message_text_only": "Le 07/01/2017 \u00e0 21:26, Chris Priest via bitcoin-dev a \u00e9crit :\n> Bitcoin Classic only changes the block format (by changing the rule\n> that they have to be 1MB or less). Miners are the only ones who make\n> blocks, so they are the only ones who mater when it comes to changing\n> block rules.\n\nCertainly not\n\n>  Nodes, wallets and other software are not affected by\n> changing block rules. Unlike segwit, where *everybody* has to write\n> code to support the new transaction format.\n\nThis is what we could call a decentralized system, when everybody is\naffected\n\n>\n> Also, it doesn't matter that 75% of hashpower is made up of a dozen\n> people. That's how the system works, it's not a matter of opinion.\n\nThat's an obvious weakness of the system\n\n>  If\n> you are just a node or just a wallet, and you want your voice to\n> matter, then you need to get a hold of some hashpower.\n\nWell, probably you did not mean this, this is not fair. \"Just a node\"...\n\nStill wondering why you guys don't care about the ridiculous number of\nfull nodes, no incentive to run one and what would happen if someone\nwere to control a majority of full nodes\n\n-- \nZcash wallets made simple: https://github.com/Ayms/zcash-wallets\nBitcoin wallets made simple: https://github.com/Ayms/bitcoin-wallets\nGet the torrent dynamic blocklist: http://peersm.com/getblocklist\nCheck the 10 M passwords list: http://peersm.com/findmyass\nAnti-spies and private torrents, dynamic blocklist: http://torrent-live.org\nPeersm : http://www.peersm.com\ntorrent-live: https://github.com/Ayms/torrent-live\nnode-Tor : https://www.github.com/Ayms/node-Tor\nGitHub : https://www.github.com/Ayms"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-01-07T23:49:10",
                "message_text_only": "On 01/07/2017 03:10 PM, Aymeric Vitte via bitcoin-dev wrote:\n> Still wondering why you guys don't care about the ridiculous number of\n> full nodes, no incentive to run one and what would happen if someone\n> were to control a majority of full nodes\n\nThe level of control over a majority of full nodes is irrelevant. If\nthis was truly a measure of control over Bitcoin someone would simply\nspin up a bunch of nodes and take control at trivial cost.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/30f8d40b/attachment.sig>"
            },
            {
                "author": "Eric Lombrozo",
                "date": "2017-01-08T00:28:34",
                "message_text_only": "Can you guys please take this discussion elsewhere? Perhaps to\nbitcoin-discuss? This is not the place to rehash discussions that have\ntaken place a million times already. The behavior of the network under\ncontentious hard forks has been discussed ad nauseum. This mailing list is\nfor the discussion of new ideas and proposals.\n\nMuch appreciated. Thanks.\n\nOn Sat, Jan 7, 2017 at 3:49 PM, Eric Voskuil via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> On 01/07/2017 03:10 PM, Aymeric Vitte via bitcoin-dev wrote:\n> > Still wondering why you guys don't care about the ridiculous number of\n> > full nodes, no incentive to run one and what would happen if someone\n> > were to control a majority of full nodes\n>\n> The level of control over a majority of full nodes is irrelevant. If\n> this was truly a measure of control over Bitcoin someone would simply\n> spin up a bunch of nodes and take control at trivial cost.\n>\n> e\n>\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/f5a91a61/attachment.html>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-08T01:58:27",
                "message_text_only": "Its too bad you're not the one who decides what gets posted here or\nnot. If you don't like whats being discussed, then don't open those\nemails.\n\nOn 1/7/17, Eric Lombrozo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> Can you guys please take this discussion elsewhere? Perhaps to\n> bitcoin-discuss? This is not the place to rehash discussions that have\n> taken place a million times already. The behavior of the network under\n> contentious hard forks has been discussed ad nauseum. This mailing list is\n> for the discussion of new ideas and proposals.\n>\n> Much appreciated. Thanks.\n>\n> On Sat, Jan 7, 2017 at 3:49 PM, Eric Voskuil via bitcoin-dev <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> On 01/07/2017 03:10 PM, Aymeric Vitte via bitcoin-dev wrote:\n>> > Still wondering why you guys don't care about the ridiculous number of\n>> > full nodes, no incentive to run one and what would happen if someone\n>> > were to control a majority of full nodes\n>>\n>> The level of control over a majority of full nodes is irrelevant. If\n>> this was truly a measure of control over Bitcoin someone would simply\n>> spin up a bunch of nodes and take control at trivial cost.\n>>\n>> e\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n>>\n>"
            },
            {
                "author": "Btc Drak",
                "date": "2017-01-07T21:15:11",
                "message_text_only": "There actually isn't an activation threshold in Bitcoin Classic. The hard\nfork rules are active the moment you install the software. As was noted,\nthere aren't any release notes, so you can be forgiven for not knowing that\nBIP109 support was removed and the proposal rejected. Classic recently\nadopted a new set of hard fork rules for which there is no written\nspecification.\n\nBitcoin software vendors should take great pains to document software\nfeatures and changes from version to version. Bitcoin Core for example,\nalways has extensive release notes, and a full changelog extracted from the\nsource code for each version. In the case of consensus rule change\nproposals, we follow the BIPs process which exists to help ecosystem-wide\nco-ordination. A detailed and complete specification allows others to\nre-implement the BIP in their own software and also acts as part of the\nconsensus building process and peer review process.\n\nThere's nothing wrong with hard forks per se, and this list is certain a\ngood place to discuss proposals, but releasing hard fork software without\nestablishing community wide consensus and without clearly labelling your\nproduct as such is just not cricket. If I may cast your attention back a\nfew weeks ago, Johnson Lau released a hard fork client _testnet_ as part of\nhis research project which was announced on this list. It was clearly\nlabelled. This Bitcoin Classic announcement was not clearly labelled (and\nreleased on mainnet).\n\n\nOn Sat, Jan 7, 2017 at 8:12 PM, Chris Priest via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Bitcoin Classic only activates if 75% of the network adopts it. That\n> is not irresponsible or dangerous. It would only be dangerous if it\n> activates at 50%, because that would create a situation where its not\n> clear which side of the fork has the most proof of work.\n>\n> On 1/7/17, Eric Lombrozo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> > Your release announcement does not make it clear that Bitcoin Classic is\n> > incompatible with the current Bitcoin network and its consensus rules. It\n> > is a hard fork on mainnet with no safe activation as well as including\n> > other unsafe changes. There is also no BIP for the hard fork. There is\n> also\n> > no evidence of community wide consensus for such a hard fork. This is\n> > dangerous and irresponsible.\n> >\n> >\n> > It's wrong to announce software without correctly informing people about\n> > the contents or risks. Furthermore, there are no release notes in\n> > https://github.com/bitcoinclassic/bitcoinclassic/tree/v1.2.0/doc nor\n> > changelog. Without those, it is almost impossible for average users to\n> know\n> > what is under the hood or what has changed and time consuming for\n> > developers to assess.\n> >\n> > On Fri, Jan 6, 2017 at 2:16 AM, Tom Zander via bitcoin-dev <\n> > bitcoin-dev at lists.linuxfoundation.org> wrote:\n> >\n> >> Bitcoin Classic version 1.2.0 is now available from;\n> >>\n> >>  <https://bitcoinclassic.com/gettingstarted.html>\n> >>\n> >> This is a new major version release, including new features, various\n> >> bugfixes and performance improvements.\n> >>\n> >> This release marks a change in strategy for Bitcoin Classic, moving from\n> >> the\n> >> very conservative block size proposal based on compromise to one where\n> >> Classic truly innovates and provides a long term solution for the market\n> >> to\n> >> choose and leave behind the restrictions of the old.\n> >>\n> >> The most visible change in this version is the decentralised block size\n> >> solution where node operators decide on the maximum size.\n> >>\n> >> Bitcoin Classic is focused on providing users a way to get onto the\n> >> Bitcoin\n> >> network using a high quality validating node for a large set of use\n> >> cases.\n> >> Classic presents top notch quality processes in this release, to help\n> >> anyone\n> >> running Bitcoin.\n> >>\n> >> We include in this release various projects with the beta label. People\n> >> who\n> >> want to use the Classic node as an on-ramp to Bitcoin will find them\n> >> interesting. These projects will need to be enabled in the config by\n> >> those\n> >> that want to test them.\n> >>\n> >> More background information on this release and Classic can be seen in\n> >> this\n> >> video: https://vimeo.com/192789752\n> >> The full release notes are on github at\n> >> https://github.com/bitcoinclassic/bitcoinclassic/releases/tag/v1.2.0\n> >>\n> >> --\n> >> Tom Zander\n> >> Blog: https://zander.github.io\n> >> Vlog: https://vimeo.com/channels/tomscryptochannel\n> >> _______________________________________________\n> >> bitcoin-dev mailing list\n> >> bitcoin-dev at lists.linuxfoundation.org\n> >> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n> >>\n> >\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/44dbd214/attachment.html>"
            },
            {
                "author": "Tom Zander",
                "date": "2017-01-07T23:08:29",
                "message_text_only": "On Saturday, 7 January 2017 21:15:11 CET Btc Drak via bitcoin-dev wrote:\n> There actually isn't an activation threshold in Bitcoin Classic.\n\nThats partly correct.\nThere is just not a formal one, there very much is an informal and practical \nthreshold.\n\nI, and I'm not alone in this, think that a formal vote or an algorithm to \ndecide something will happen or not reeks too much like central planning and \nmore importantly that it is too inflexible for real world use.\nIts fine for simple upgrades, and we have seen lots of success there.\n\nIt would be a mistake to think that miners can just start mining with \nClassic and make something that Core doesn't understand. That would have \nnegative effects and thus won't happen. Less social people will ask why and \nmaybe ask how we avoid this. They misunderstand the social and economic \nparts of Bitcoin.\n\nThe block size is an ongoing debate. \nI find it very hard to believe that all the people replying in outrage to my \nrelease announcement completely missed this.\n\nI see no point in bringing it up in a BIP or on this list as some central \ncabal that can make decisions for or against.  It is in actual fact being \ndecided in the real world, out of yours and my control.\n\nClassic is a tool to that end. No more. No less.\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-01-08T00:32:25",
                "message_text_only": "On 01/07/2017 12:55 AM, Eric Lombrozo via bitcoin-dev wrote:\n> Your release announcement does not make it clear that Bitcoin Classic is\n> incompatible with the current Bitcoin network and its consensus rules.\n> It is a hard fork on mainnet with no safe activation as well as\n> including other unsafe changes. There is also no BIP for the hard fork.\n> There is also no evidence of community wide consensus for such a hard\n> fork. This is dangerous and irresponsible.\n\nWhile I agree with the sentiment, to be fair one should acknowledge that\nBitcoin Core has intentionally implemented two hard forks since Nov\n2015. The earlier is released, and I assume the latter will be.\n\nNeither was subject to activation, or prior public debate (see Buried\nDeployments threads):\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-November/thread.html\n\nThere was at least some internal discussion about whether a BIP should\ndocument the latter having occurred, and that question was put to the list:\n\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-November/013275.html\n\nSome have argued that these are inconsequential changes. I disagree, as\nthe arguments is base on provably invalid assumptions. Nevertheless, if\nhard fork is the threshold criteria here, Core has not met it.\n\ne\n\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 490 bytes\nDesc: OpenPGP digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170107/71abdebd/attachment.sig>"
            }
        ],
        "thread_summary": {
            "title": "Bitcoin Classic 1.2.0 released",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Lombrozo",
                "Eric Voskuil",
                "David Vorick",
                "Chris Priest",
                "Tom Zander",
                "Aymeric Vitte",
                "Btc Drak",
                "Jonas Schnelli"
            ],
            "messages_count": 15,
            "total_messages_chars_count": 31904
        }
    },
    {
        "title": "[bitcoin-dev] Announcements on bitcoin-dev",
        "thread_messages": [
            {
                "author": "Btc Drak",
                "date": "2017-01-08T06:50:52",
                "message_text_only": "The purpose of this list is Bitcoin protocol discussion of all kinds,\nincluding consensus rules that require hard and soft forks and there\nhave been many discussions about both. There is also a clear technical\nprocess for proposing, discussing and peer reviewing consensus rule\nchanges via the BIPs process which this list has traditionally played\na large role. BIP specifications allow community wide coordination\nacross multiple implementations.\n\nSince the recent thread announcing a new version of Bitcoin Classic,\nmany people have complained on and off-list that is should not be\nallowed on the bitcoin-dev mailing list because it is not consensus\ncompatible nor is it a change which has been arrived at through wide\ncommunity consensus. There isn't even a formal specification that\nother implementations could follow if they wanted to. The general\nfeeling seems to be that announcements for consensus compatible\nimplementations is ok on this list. If there is ever community wide\nconsensus for a hard fork, then that too would be ok since there would\nbe consensus.\n\nThis list does strive to be somewhat high signal to noise ratio where\npossible and we need to be clear about the list remit. So let's be\nclear, announcing/advertising software that is consensus incompatible\nis off-topic, however, discussion of hard forks, peer review etc has\nalways been, and remains, on topic.\n\nI've copied the current list remit below for completeness. General\ndiscussions should be directed at bitcoin-discuss, where as actual\nprotocol development discussion belongs on bitcoin-dev.\n\nBitcoin development and protocol discussion.\n\nThis list is lightly moderated.\n\n- No offensive posts, no personal attacks.\n- Posts must concern development of bitcoin protocol.\n- Posts should be technical or academic in nature.\n- Generally encouraged: patches, notification of pull requests, BIP\nproposals, academic paper announcements. And discussions that follow.\n- Generally discouraged: shower thoughts, wild speculation, jokes,\n+1s, non-technical bitcoin issues, rehashing settled topics without\nnew data, moderation concerns.\n- Detailed patch discussion generally better on a GitHub PR.\n- Meta-discussion is better on bitcoin-discuss\n(https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-discuss)\n\nThanks"
            }
        ],
        "thread_summary": {
            "title": "Announcements on bitcoin-dev",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Btc Drak"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2290
        }
    },
    {
        "title": "[bitcoin-dev] Mutli-push op_return",
        "thread_messages": [
            {
                "author": "Chris",
                "date": "2017-01-09T02:09:09",
                "message_text_only": "Would there be an objection to making op_return outputs with two\npushdatas standard (same max data size)?\n\nUse case is mostly tagging transactions so they can be returned by bloom\nfiltering nodes:\n\nOP_RETURN <tag> <data>\n\nSince bip37 nodes test each data element in each output script (which I\nbelieve applies to op_return as well?) it provides a lightweight way of\nfetching transactions where the tag matches a specific pattern.\n\nIt appears a sizable number of nodes/miners already accept such\ntransactions as this one was mined in the first block...\nhttps://blockchain.info/tx/400b4738f1e4eab4062e085623b9a3a71670f5c0d42e32dbe5a4e71da5baabe0\n\n- Chris"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-01-09T03:08:56",
                "message_text_only": "On Monday, January 09, 2017 2:09:09 AM Chris via bitcoin-dev wrote:\n> Would there be an objection to making op_return outputs with two\n> pushdatas standard (same max data size)?\n\nStandards (BIPs) need to describe a specific use case and protocol for doing \nit.\n\nAs you note, the default policy on most nodes is to allow such outputs.\n\nLuke"
            },
            {
                "author": "Chris",
                "date": "2017-01-09T04:32:53",
                "message_text_only": "Maybe I was too premature. The script wiki (which I realize might not be\nup to date) says only one pushdata is allowed but I don't see that in\nthe code.\n\nhttps://github.com/bitcoin/bitcoin/blob/e8cfe1ee2d01c493b758a67ad14707dca15792ea/src/policy/policy.cpp#L49\n\nAm I missing something or am I correct to assume that multiple pushdatas\nin op_return would normally be considered standard?\n\n\nOn 01/08/2017 10:08 PM, Luke Dashjr wrote:\n> On Monday, January 09, 2017 2:09:09 AM Chris via bitcoin-dev wrote:\n>> Would there be an objection to making op_return outputs with two\n>> pushdatas standard (same max data size)?\n> Standards (BIPs) need to describe a specific use case and protocol for doing \n> it.\n>\n> As you note, the default policy on most nodes is to allow such outputs.\n>\n> Luke\n>"
            }
        ],
        "thread_summary": {
            "title": "Mutli-push op_return",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Luke Dashjr",
                "Chris"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 1777
        }
    },
    {
        "title": "[bitcoin-dev] A BIP for partially-signed/not-signed raw transaction serialization; would it be useful?",
        "thread_messages": [
            {
                "author": "\u6728\u30ce\u4e0b\u3058\u3087\u306a",
                "date": "2017-01-09T09:36:49",
                "message_text_only": "I have been seeing issues like the following many times on all the\ndifferent projects that support multisig with users responsible for partial\ntransaction transport.\n\nhttps://github.com/OutCast3k/coinbin/issues/73\n\nI would like to gather opinions before proposing a BIP, (like whether we\nneed one or not) so please let me know what you think.\n\nBasically, Electrum, Copay, Coinb.in, Bitcoin Core, etc. all have different\nmethodology for serializing partially signed multisig raw transactions, as\nwell as not-signed raw transactions regardless of scriptPubkey.\n\nI think we should all be on the same page when serializing not-signed and\npartially signed transactions so that the person could look at the data\nalone and know what is necessary and how to manipulate it to sign and\ncomplete the transaction.\n\n- Jon\n\n-- \n-----BEGIN PGP PUBLIC KEY BLOCK-----\nComment: http://openpgpjs.org\n\nxsBNBFTmJ8oBB/9rd+7XLxZG/x/KnhkVK2WBG8ySx91fs+qQfHIK1JrakSV3\nx6x0cK3XLClASLLDomm7Od3Q/fMFzdwCEqj6z60T8wgKxsjWYSGL3mq8ucdv\niBjC3wGauk5dQKtT7tkCFyQQbX/uMsBM4ccGBICoDmIJlwJIj7fAZVqGxGOM\nbO1RhYb4dbQA2qxYP7wSsHJ6/ZNAXyEphOj6blUzdqO0exAbCOZWWF+E/1SC\nEuKO4RmL7Imdep7uc2Qze1UpJCZx7ASHl2IZ4UD0G3Qr3pI6/jvNlaqCTa3U\n3/YeJwEubFsd0AVy0zs809RcKKgX3W1q+hVDTeWinem9RiOG/vT+Eec/ABEB\nAAHNI2tpbm9zaGl0YSA8a2lub3NoaXRham9uYUBnbWFpbC5jb20+wsByBBAB\nCAAmBQJU5ifRBgsJCAcDAgkQRB9iZ30dlisEFQgCCgMWAgECGwMCHgEAAC6Z\nB/9otobf0ASHYdlUBeIPXdDopyjQhR2RiZGYaS0VZ5zzHYLDDMW6ZIYm5CjO\nFc09ETLGKFxH2RcCOK2dzwz+KRU4xqOrt/l5gyd50cFE1nOhUN9+/XaPgrou\nWhyT9xLeGit7Xqhht93z2+VanTtJAG6lWbAZLIZAMGMuLX6sJDCO0GiO5zxa\n02Q2D3kh5GL57A5+oVOna12JBRaIA5eBGKVCp3KToT/z48pxBe3WAmLo0zXr\nhEgTSzssfb2zTwtB3Ogoedj+cU2bHJvJ8upS/jMr3TcdguySmxJlGpocVC/e\nqxq12Njv+LiETOrD8atGmXCnA+nFNljBkz+l6ADl93jHzsBNBFTmJ9EBCACu\nQq9ZnP+aLU/Rt6clAfiHfTFBsJvLKsdIKeE6qHzsU1E7A7bGQKTtLEnhCCQE\nW+OQP+sgbOWowIdH9PpwLJ3Op+NhvLlMxRvbT36LwCmBL0yD7bMqxxmmVj8n\nvlMMRSe4wDSIG19Oy7701imnHZPm/pnPlneg/Meu/UffpcDWYBbAFX8nrXPY\nvkVULcI/qTcCxW/+S9fwoXjQhWHaiJJ6y3cYOSitN31W9zgcMvLwLX3JgDxE\nflkwq/M+ZkfCYnS3GAPEt8GkVKy2eHtCJuNkGFlCAmKMX0yWzHRAkqOMN5KP\nLFbkKY2GQl13ztWp82QYJZpj5af6dmyUosurn6AZABEBAAHCwF8EGAEIABMF\nAlTmJ9QJEEQfYmd9HZYrAhsMAABKbgf/Ulu5JAk4fXgH0DtkMmdkFiKEFdkW\n0Wkw7Vhd5eZ4NzeP9kOkD01OGweT9hqzwhfT2CNXCGxh4UnvEM1ZMFypIKdq\n0XpLLJMrDOQO021UjAa56vHZPAVmAM01z5VzHJ7ekjgwrgMLmVkm0jWKEKaO\nn/MW7CyphG7QcZ6cJX2f6uJcekBlZRw9TNYRnojMjkutlOVhYJ3J78nc/k0p\nkcgV63GB6D7wHRF4TVe4xIBqKpbBhhN+ISwFN1z+gx3lfyRMSmiTSrGdKEQe\nXSIQKG8XZQZUDhLNkqPS+7EMV1g7+lOfT4GhLL68dUXDa1e9YxGH6zkpVECw\nSpe3vsHZr6CqFg==\n=/vUJ\n-----END PGP PUBLIC KEY BLOCK-----\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/15c830b5/attachment.html>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-09T22:15:17",
                "message_text_only": "I approve of this idea. Counterparty has the same problem. Their API\nreturns a unsigned transaction that is formed differently from how\nother unsigned transactions, which causes friction. Someone needs to\nwrite up a specification that is standardized so that all unsigned\ntransactions are of the same form. Basically the signature section of\nthe should contains all the information required to make the\nsignature, and it needs to be encoded in a way that the signing\napplication (a blockchain library like BitcoinJ or BitcoinJS) can tell\nthat it is unsigned.\n\nOn 1/9/17, \u6728\u30ce\u4e0b\u3058\u3087\u306a via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> I have been seeing issues like the following many times on all the\n> different projects that support multisig with users responsible for partial\n> transaction transport.\n>\n> https://github.com/OutCast3k/coinbin/issues/73\n>\n> I would like to gather opinions before proposing a BIP, (like whether we\n> need one or not) so please let me know what you think.\n>\n> Basically, Electrum, Copay, Coinb.in, Bitcoin Core, etc. all have different\n> methodology for serializing partially signed multisig raw transactions, as\n> well as not-signed raw transactions regardless of scriptPubkey.\n>\n> I think we should all be on the same page when serializing not-signed and\n> partially signed transactions so that the person could look at the data\n> alone and know what is necessary and how to manipulate it to sign and\n> complete the transaction.\n>\n> - Jon\n>\n> --\n> -----BEGIN PGP PUBLIC KEY BLOCK-----\n> Comment: http://openpgpjs.org\n>\n> xsBNBFTmJ8oBB/9rd+7XLxZG/x/KnhkVK2WBG8ySx91fs+qQfHIK1JrakSV3\n> x6x0cK3XLClASLLDomm7Od3Q/fMFzdwCEqj6z60T8wgKxsjWYSGL3mq8ucdv\n> iBjC3wGauk5dQKtT7tkCFyQQbX/uMsBM4ccGBICoDmIJlwJIj7fAZVqGxGOM\n> bO1RhYb4dbQA2qxYP7wSsHJ6/ZNAXyEphOj6blUzdqO0exAbCOZWWF+E/1SC\n> EuKO4RmL7Imdep7uc2Qze1UpJCZx7ASHl2IZ4UD0G3Qr3pI6/jvNlaqCTa3U\n> 3/YeJwEubFsd0AVy0zs809RcKKgX3W1q+hVDTeWinem9RiOG/vT+Eec/ABEB\n> AAHNI2tpbm9zaGl0YSA8a2lub3NoaXRham9uYUBnbWFpbC5jb20+wsByBBAB\n> CAAmBQJU5ifRBgsJCAcDAgkQRB9iZ30dlisEFQgCCgMWAgECGwMCHgEAAC6Z\n> B/9otobf0ASHYdlUBeIPXdDopyjQhR2RiZGYaS0VZ5zzHYLDDMW6ZIYm5CjO\n> Fc09ETLGKFxH2RcCOK2dzwz+KRU4xqOrt/l5gyd50cFE1nOhUN9+/XaPgrou\n> WhyT9xLeGit7Xqhht93z2+VanTtJAG6lWbAZLIZAMGMuLX6sJDCO0GiO5zxa\n> 02Q2D3kh5GL57A5+oVOna12JBRaIA5eBGKVCp3KToT/z48pxBe3WAmLo0zXr\n> hEgTSzssfb2zTwtB3Ogoedj+cU2bHJvJ8upS/jMr3TcdguySmxJlGpocVC/e\n> qxq12Njv+LiETOrD8atGmXCnA+nFNljBkz+l6ADl93jHzsBNBFTmJ9EBCACu\n> Qq9ZnP+aLU/Rt6clAfiHfTFBsJvLKsdIKeE6qHzsU1E7A7bGQKTtLEnhCCQE\n> W+OQP+sgbOWowIdH9PpwLJ3Op+NhvLlMxRvbT36LwCmBL0yD7bMqxxmmVj8n\n> vlMMRSe4wDSIG19Oy7701imnHZPm/pnPlneg/Meu/UffpcDWYBbAFX8nrXPY\n> vkVULcI/qTcCxW/+S9fwoXjQhWHaiJJ6y3cYOSitN31W9zgcMvLwLX3JgDxE\n> flkwq/M+ZkfCYnS3GAPEt8GkVKy2eHtCJuNkGFlCAmKMX0yWzHRAkqOMN5KP\n> LFbkKY2GQl13ztWp82QYJZpj5af6dmyUosurn6AZABEBAAHCwF8EGAEIABMF\n> AlTmJ9QJEEQfYmd9HZYrAhsMAABKbgf/Ulu5JAk4fXgH0DtkMmdkFiKEFdkW\n> 0Wkw7Vhd5eZ4NzeP9kOkD01OGweT9hqzwhfT2CNXCGxh4UnvEM1ZMFypIKdq\n> 0XpLLJMrDOQO021UjAa56vHZPAVmAM01z5VzHJ7ekjgwrgMLmVkm0jWKEKaO\n> n/MW7CyphG7QcZ6cJX2f6uJcekBlZRw9TNYRnojMjkutlOVhYJ3J78nc/k0p\n> kcgV63GB6D7wHRF4TVe4xIBqKpbBhhN+ISwFN1z+gx3lfyRMSmiTSrGdKEQe\n> XSIQKG8XZQZUDhLNkqPS+7EMV1g7+lOfT4GhLL68dUXDa1e9YxGH6zkpVECw\n> Spe3vsHZr6CqFg==\n> =/vUJ\n> -----END PGP PUBLIC KEY BLOCK-----\n>"
            },
            {
                "author": "\u6728\u30ce\u4e0b\u3058\u3087\u306a",
                "date": "2017-01-10T12:35:07",
                "message_text_only": "Hey Thomas,\n\nJust to clear the air, I am not OutCast3k, but I have submitted various PRs\nto coinb.in under multiple handle names, and also Copay and Electrum. (You\ncould say I have multisig in my blood if you want to be cheesy)\n\nI think I'm going to go ahead and draft up a BIP and submit it here just to\nget the ball rolling.\n\nAny ideas or input can be sent to me directly via email. I'll also start a\nTelegram group chat https://t.me/joinchat/AAAAAAutbBADJuHRD7YvHw in case\nanyone wants to join in on the discussion.\n\nThanks,\nJon\n\n2017-01-10 18:18 GMT+09:00 Thomas Kerin <me at thomaskerin.io>:\n\n> Hey,\n>\n> Firstly, your project coinb.in is really cool, I've used it a bit back in\n> the day :-)\n>\n> It makes sense why you're looking for this proposal. I'm pretty sure on\n> top of a serialisation of an unsigned tx with the scriptpubkey (it is to\n> deliver signing data to the wallet too?) then you'll also want a protocol\n> to request signatures.\n>\n> I worked on this problem before, when I was using your site and a tool of\n> mine to carry out transactions. I would up writing a BIP that's basically\n> the same (in message contents, not format) as Jonas's hardware signing BIP.\n>\n> I think he also realised that it's not just for hardware wallets, it's\n> also perfect for a BitGo wallet to use to talk to a GreenAddress wallet. It\n> seems to cover the web case nicely.\n>\n> (My app was one where users could supply an xpub for multisigs to the\n> server, get txs, and do signing offline or in the browser)\n>\n> Maybe have a look over it and see if it starts to capture some of the\n> things you would want!\n>\n> All the best,\n> Thomas\n>\n>\n>\n> On 9 January 2017 10:36:49 CET, \"\u6728\u30ce\u4e0b\u3058\u3087\u306a via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>\n>> I have been seeing issues like the following many times on all the\n>> different projects that support multisig with users responsible for partial\n>> transaction transport.\n>>\n>> https://github.com/OutCast3k/coinbin/issues/73\n>>\n>> I would like to gather opinions before proposing a BIP, (like whether we\n>> need one or not) so please let me know what you think.\n>>\n>> Basically, Electrum, Copay, Coinb.in, Bitcoin Core, etc. all have\n>> different methodology for serializing partially signed multisig raw\n>> transactions, as well as not-signed raw transactions regardless of\n>> scriptPubkey.\n>>\n>> I think we should all be on the same page when serializing not-signed and\n>> partially signed transactions so that the person could look at the data\n>> alone and know what is necessary and how to manipulate it to sign and\n>> complete the transaction.\n>>\n>> - Jon\n>>\n>> --\n>> -----BEGIN PGP PUBLIC KEY BLOCK-----\n>> Comment: http://openpgpjs.org\n>>\n>> xsBNBFTmJ8oBB/9rd+7XLxZG/x/KnhkVK2WBG8ySx91fs+qQfHIK1JrakSV3\n>> x6x0cK3XLClASLLDomm7Od3Q/fMFzdwCEqj6z60T8wgKxsjWYSGL3mq8ucdv\n>> iBjC3wGauk5dQKtT7tkCFyQQbX/uMsBM4ccGBICoDmIJlwJIj7fAZVqGxGOM\n>> bO1RhYb4dbQA2qxYP7wSsHJ6/ZNAXyEphOj6blUzdqO0exAbCOZWWF+E/1SC\n>> EuKO4RmL7Imdep7uc2Qze1UpJCZx7ASHl2IZ4UD0G3Qr3pI6/jvNlaqCTa3U\n>> 3/YeJwEubFsd0AVy0zs809RcKKgX3W1q+hVDTeWinem9RiOG/vT+Eec/ABEB\n>> AAHNI2tpbm9zaGl0YSA8a2lub3NoaXRham9uYUBnbWFpbC5jb20+wsByBBAB\n>> CAAmBQJU5ifRBgsJCAcDAgkQRB9iZ30dlisEFQgCCgMWAgECGwMCHgEAAC6Z\n>> B/9otobf0ASHYdlUBeIPXdDopyjQhR2RiZGYaS0VZ5zzHYLDDMW6ZIYm5CjO\n>> Fc09ETLGKFxH2RcCOK2dzwz+KRU4xqOrt/l5gyd50cFE1nOhUN9+/XaPgrou\n>> WhyT9xLeGit7Xqhht93z2+VanTtJAG6lWbAZLIZAMGMuLX6sJDCO0GiO5zxa\n>> 02Q2D3kh5GL57A5+oVOna12JBRaIA5eBGKVCp3KToT/z48pxBe3WAmLo0zXr\n>> hEgTSzssfb2zTwtB3Ogoedj+cU2bHJvJ8upS/jMr3TcdguySmxJlGpocVC/e\n>> qxq12Njv+LiETOrD8atGmXCnA+nFNljBkz+l6ADl93jHzsBNBFTmJ9EBCACu\n>> Qq9ZnP+aLU/Rt6clAfiHfTFBsJvLKsdIKeE6qHzsU1E7A7bGQKTtLEnhCCQE\n>> W+OQP+sgbOWowIdH9PpwLJ3Op+NhvLlMxRvbT36LwCmBL0yD7bMqxxmmVj8n\n>> vlMMRSe4wDSIG19Oy7701imnHZPm/pnPlneg/Meu/UffpcDWYBbAFX8nrXPY\n>> vkVULcI/qTcCxW/+S9fwoXjQhWHaiJJ6y3cYOSitN31W9zgcMvLwLX3JgDxE\n>> flkwq/M+ZkfCYnS3GAPEt8GkVKy2eHtCJuNkGFlCAmKMX0yWzHRAkqOMN5KP\n>> LFbkKY2GQl13ztWp82QYJZpj5af6dmyUosurn6AZABEBAAHCwF8EGAEIABMF\n>> AlTmJ9QJEEQfYmd9HZYrAhsMAABKbgf/Ulu5JAk4fXgH0DtkMmdkFiKEFdkW\n>> 0Wkw7Vhd5eZ4NzeP9kOkD01OGweT9hqzwhfT2CNXCGxh4UnvEM1ZMFypIKdq\n>> 0XpLLJMrDOQO021UjAa56vHZPAVmAM01z5VzHJ7ekjgwrgMLmVkm0jWKEKaO\n>> n/MW7CyphG7QcZ6cJX2f6uJcekBlZRw9TNYRnojMjkutlOVhYJ3J78nc/k0p\n>> kcgV63GB6D7wHRF4TVe4xIBqKpbBhhN+ISwFN1z+gx3lfyRMSmiTSrGdKEQe\n>> XSIQKG8XZQZUDhLNkqPS+7EMV1g7+lOfT4GhLL68dUXDa1e9YxGH6zkpVECw\n>> Spe3vsHZr6CqFg==\n>> =/vUJ\n>> -----END PGP PUBLIC KEY BLOCK-----\n>>\n>\n> --\n> Sent from my Android device with K-9 Mail. Please excuse my brevity.\n>\n\n\n\n-- \n-----BEGIN PGP PUBLIC KEY BLOCK-----\nComment: http://openpgpjs.org\n\nxsBNBFTmJ8oBB/9rd+7XLxZG/x/KnhkVK2WBG8ySx91fs+qQfHIK1JrakSV3\nx6x0cK3XLClASLLDomm7Od3Q/fMFzdwCEqj6z60T8wgKxsjWYSGL3mq8ucdv\niBjC3wGauk5dQKtT7tkCFyQQbX/uMsBM4ccGBICoDmIJlwJIj7fAZVqGxGOM\nbO1RhYb4dbQA2qxYP7wSsHJ6/ZNAXyEphOj6blUzdqO0exAbCOZWWF+E/1SC\nEuKO4RmL7Imdep7uc2Qze1UpJCZx7ASHl2IZ4UD0G3Qr3pI6/jvNlaqCTa3U\n3/YeJwEubFsd0AVy0zs809RcKKgX3W1q+hVDTeWinem9RiOG/vT+Eec/ABEB\nAAHNI2tpbm9zaGl0YSA8a2lub3NoaXRham9uYUBnbWFpbC5jb20+wsByBBAB\nCAAmBQJU5ifRBgsJCAcDAgkQRB9iZ30dlisEFQgCCgMWAgECGwMCHgEAAC6Z\nB/9otobf0ASHYdlUBeIPXdDopyjQhR2RiZGYaS0VZ5zzHYLDDMW6ZIYm5CjO\nFc09ETLGKFxH2RcCOK2dzwz+KRU4xqOrt/l5gyd50cFE1nOhUN9+/XaPgrou\nWhyT9xLeGit7Xqhht93z2+VanTtJAG6lWbAZLIZAMGMuLX6sJDCO0GiO5zxa\n02Q2D3kh5GL57A5+oVOna12JBRaIA5eBGKVCp3KToT/z48pxBe3WAmLo0zXr\nhEgTSzssfb2zTwtB3Ogoedj+cU2bHJvJ8upS/jMr3TcdguySmxJlGpocVC/e\nqxq12Njv+LiETOrD8atGmXCnA+nFNljBkz+l6ADl93jHzsBNBFTmJ9EBCACu\nQq9ZnP+aLU/Rt6clAfiHfTFBsJvLKsdIKeE6qHzsU1E7A7bGQKTtLEnhCCQE\nW+OQP+sgbOWowIdH9PpwLJ3Op+NhvLlMxRvbT36LwCmBL0yD7bMqxxmmVj8n\nvlMMRSe4wDSIG19Oy7701imnHZPm/pnPlneg/Meu/UffpcDWYBbAFX8nrXPY\nvkVULcI/qTcCxW/+S9fwoXjQhWHaiJJ6y3cYOSitN31W9zgcMvLwLX3JgDxE\nflkwq/M+ZkfCYnS3GAPEt8GkVKy2eHtCJuNkGFlCAmKMX0yWzHRAkqOMN5KP\nLFbkKY2GQl13ztWp82QYJZpj5af6dmyUosurn6AZABEBAAHCwF8EGAEIABMF\nAlTmJ9QJEEQfYmd9HZYrAhsMAABKbgf/Ulu5JAk4fXgH0DtkMmdkFiKEFdkW\n0Wkw7Vhd5eZ4NzeP9kOkD01OGweT9hqzwhfT2CNXCGxh4UnvEM1ZMFypIKdq\n0XpLLJMrDOQO021UjAa56vHZPAVmAM01z5VzHJ7ekjgwrgMLmVkm0jWKEKaO\nn/MW7CyphG7QcZ6cJX2f6uJcekBlZRw9TNYRnojMjkutlOVhYJ3J78nc/k0p\nkcgV63GB6D7wHRF4TVe4xIBqKpbBhhN+ISwFN1z+gx3lfyRMSmiTSrGdKEQe\nXSIQKG8XZQZUDhLNkqPS+7EMV1g7+lOfT4GhLL68dUXDa1e9YxGH6zkpVECw\nSpe3vsHZr6CqFg==\n=/vUJ\n-----END PGP PUBLIC KEY BLOCK-----\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170110/d3ec291b/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "A BIP for partially-signed/not-signed raw transaction serialization; would it be useful?",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Chris Priest",
                "\u6728\u30ce\u4e0b\u3058\u3087\u306a"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 12395
        }
    },
    {
        "title": "[bitcoin-dev] BIP - Block75 - Historical and future projections",
        "thread_messages": [
            {
                "author": "t. khan",
                "date": "2017-01-09T19:52:31",
                "message_text_only": "Using daily average block size over the past year (source:\nhttps://blockchain.info/charts/avg-block-size?daysAverageString=14&timespan=1year\n), here's how Block75 would have altered max block sizes:\n\n[image: Inline image 1]\n\nAs of today, the max block size would be 1,135KB.\n\nLooking forward and using the last year's growth rate as a model:\n\n[image: Inline image 2]\n\nThis shows the max block size one year from now would be 2,064KB, if\nBlock75 activated today.\n\nOf course, this is just an estimate, but even accounting for a substantial\nincrease in transactions in the last quarter of 2017 and changes brought\nabout by SegWit (hopefully) activating, Block75 alters the max size in such\na way that allows for growth, keeps blocks as small as possible, *and*\nmaintains transaction fees at a level similar to May/June 2016.\n\nIf anyone has an alternate way to model future behavior, please run it\nthrough the Block75 algorithm.\n\nEvery 2016 blocks, do this:\n\nnew max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY))\n\nTARGET_CAPACITY = 0.75    //Block75's target of keeping blocks 75% full\nAVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a\ndecimal\nx = current max block size\n\n\nThanks,\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment-0001.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: Block75 2016.png\nType: image/png\nSize: 32088 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment-0002.png>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: Block75 2017.png\nType: image/png\nSize: 33176 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment-0003.png>"
            }
        ],
        "thread_summary": {
            "title": "BIP - Block75 - Historical and future projections",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "t. khan"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 1942
        }
    },
    {
        "title": "[bitcoin-dev] BIP - Block75 - Historical and future projections (t. khan)",
        "thread_messages": [
            {
                "author": "Ryan J Martin",
                "date": "2017-01-10T04:14:55",
                "message_text_only": "The adaptive/automatic block size notion has been around for a while--- others would be better able to speak to why it hasn't gotten traction. However my concern with something like that is that it doesn't regard the optimal economic equilibrium for tx fees/size---not that the current limit does either but the concern with an auto-adjusting size limit that ignores this  would be the potential to create unforeseen externalities for miners/users. Miners may decide it is more profitable to mine very small blocks to constrict supply and increase marginal fees and with how centralized mining is, where a dozen pools have 85% hashrate, a couple of pools could do this. Then on the other side, maybe the prisoner's dilemma would hold and all miners would have minrelaytxfee set at zero and users would push the blocks to larger and larger sizes causing higher and higher latency and network issues. \n    Perhaps something like this could work (I can only speak to the economic side anyway) but it would have to have some solid code that has a social benefit model built in to adjust to an equilibrium that is able to optimize---as in maximizes benefit/minimize cost for both sides via a Marshallian surplus model--- for each size point. \n     To be clear, I'm not saying an auto-adjusting limit is unworkable (again only from an economic standpoint), just that it would need to have these considerations built in. \n\n-Ryan J. Martin\n\n\n________________________________________\n\n------------------------------\n\nMessage: 2\nDate: Mon, 9 Jan 2017 14:52:31 -0500\nFrom: \"t. khan\" <teekhan42 at gmail.com>\nTo: Bitcoin Protocol Discussion\n        <bitcoin-dev at lists.linuxfoundation.org>\nSubject: [bitcoin-dev] BIP - Block75 - Historical and future\n        projections\nMessage-ID:\n        <CAGCNRJpSV9zKxhVvqpMVPyFyXco_ABB9a7_ihaDKEKFPQ9v3sw at mail.gmail.com>\nContent-Type: text/plain; charset=\"utf-8\"\n\nUsing daily average block size over the past year (source:\nhttps://blockchain.info/charts/avg-block-size?daysAverageString=14&timespan=1year\n), here's how Block75 would have altered max block sizes:\n\n[image: Inline image 1]\n\nAs of today, the max block size would be 1,135KB.\n\nLooking forward and using the last year's growth rate as a model:\n\n[image: Inline image 2]\n\nThis shows the max block size one year from now would be 2,064KB, if\nBlock75 activated today.\n\nOf course, this is just an estimate, but even accounting for a substantial\nincrease in transactions in the last quarter of 2017 and changes brought\nabout by SegWit (hopefully) activating, Block75 alters the max size in such\na way that allows for growth, keeps blocks as small as possible, *and*\nmaintains transaction fees at a level similar to May/June 2016.\n\nIf anyone has an alternate way to model future behavior, please run it\nthrough the Block75 algorithm.\n\nEvery 2016 blocks, do this:\n\nnew max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY))\n\nTARGET_CAPACITY = 0.75    //Block75's target of keeping blocks 75% full\nAVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a\ndecimal\nx = current max block size\n\n\nThanks,\n\n- t.k.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment.html>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: Block75 2016.png\nType: image/png\nSize: 32088 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment.png>\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: Block75 2017.png\nType: image/png\nSize: 33176 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment-0001.png>\n\n------------------------------\n\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n\nEnd of bitcoin-dev Digest, Vol 20, Issue 21\n*******************************************"
            },
            {
                "author": "Adam Back",
                "date": "2017-01-10T10:04:27",
                "message_text_only": "See discussion on bitcoin-discuss on this topic last few days.  People\nmay want to subscribe to that for more free wheeling discussion.\n\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-discuss\n\nAdam\n\nOn 10 January 2017 at 04:14, Ryan J Martin via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>      The adaptive/automatic block size notion has been around for a while--- others would be better able to speak to why it hasn't gotten traction. However my concern with something like that is that it doesn't regard the optimal economic equilibrium for tx fees/size---not that the current limit does either but the concern with an auto-adjusting size limit that ignores this  would be the potential to create unforeseen externalities for miners/users. Miners may decide it is more profitable to mine very small blocks to constrict supply and increase marginal fees and with how centralized mining is, where a dozen pools have 85% hashrate, a couple of pools could do this. Then on the other side, maybe the prisoner's dilemma would hold and all miners would have minrelaytxfee set at zero and users would push the blocks to larger and larger sizes causing higher and higher latency and network issues.\n>     Perhaps something like this could work (I can only speak to the economic side anyway) but it would have to have some solid code that has a social benefit model built in to adjust to an equilibrium that is able to optimize---as in maximizes benefit/minimize cost for both sides via a Marshallian surplus model--- for each size point.\n>      To be clear, I'm not saying an auto-adjusting limit is unworkable (again only from an economic standpoint), just that it would need to have these considerations built in.\n>\n> -Ryan J. Martin\n>\n>\n> ________________________________________\n>\n> ------------------------------\n>\n> Message: 2\n> Date: Mon, 9 Jan 2017 14:52:31 -0500\n> From: \"t. khan\" <teekhan42 at gmail.com>\n> To: Bitcoin Protocol Discussion\n>         <bitcoin-dev at lists.linuxfoundation.org>\n> Subject: [bitcoin-dev] BIP - Block75 - Historical and future\n>         projections\n> Message-ID:\n>         <CAGCNRJpSV9zKxhVvqpMVPyFyXco_ABB9a7_ihaDKEKFPQ9v3sw at mail.gmail.com>\n> Content-Type: text/plain; charset=\"utf-8\"\n>\n> Using daily average block size over the past year (source:\n> https://blockchain.info/charts/avg-block-size?daysAverageString=14&timespan=1year\n> ), here's how Block75 would have altered max block sizes:\n>\n> [image: Inline image 1]\n>\n> As of today, the max block size would be 1,135KB.\n>\n> Looking forward and using the last year's growth rate as a model:\n>\n> [image: Inline image 2]\n>\n> This shows the max block size one year from now would be 2,064KB, if\n> Block75 activated today.\n>\n> Of course, this is just an estimate, but even accounting for a substantial\n> increase in transactions in the last quarter of 2017 and changes brought\n> about by SegWit (hopefully) activating, Block75 alters the max size in such\n> a way that allows for growth, keeps blocks as small as possible, *and*\n> maintains transaction fees at a level similar to May/June 2016.\n>\n> If anyone has an alternate way to model future behavior, please run it\n> through the Block75 algorithm.\n>\n> Every 2016 blocks, do this:\n>\n> new max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY))\n>\n> TARGET_CAPACITY = 0.75    //Block75's target of keeping blocks 75% full\n> AVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a\n> decimal\n> x = current max block size\n>\n>\n> Thanks,\n>\n> - t.k.\n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment.html>\n> -------------- next part --------------\n> A non-text attachment was scrubbed...\n> Name: Block75 2016.png\n> Type: image/png\n> Size: 32088 bytes\n> Desc: not available\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment.png>\n> -------------- next part --------------\n> A non-text attachment was scrubbed...\n> Name: Block75 2017.png\n> Type: image/png\n> Size: 33176 bytes\n> Desc: not available\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170109/b0e0b713/attachment-0001.png>\n>\n> ------------------------------\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> End of bitcoin-dev Digest, Vol 20, Issue 21\n> *******************************************\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            },
            {
                "author": "t. khan",
                "date": "2017-01-10T19:09:42",
                "message_text_only": "As Block75 maintains blocks at 75% full (average over time), it\nautomatically stabilizes transaction fees at about the level they were in\nMay/June 2016. It will even do so through changes in transaction size and\nvolume caused by SegWit and Lightning.\n\n- t.k.\n\nOn Mon, Jan 9, 2017 at 11:14 PM, Ryan J Martin via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>      The adaptive/automatic block size notion has been around for a\n> while--- others would be better able to speak to why it hasn't gotten\n> traction. However my concern with something like that is that it doesn't\n> regard the optimal economic equilibrium for tx fees/size---not that the\n> current limit does either but the concern with an auto-adjusting size limit\n> that ignores this  would be the potential to create unforeseen\n> externalities for miners/users. Miners may decide it is more profitable to\n> mine very small blocks to constrict supply and increase marginal fees and\n> with how centralized mining is, where a dozen pools have 85% hashrate, a\n> couple of pools could do this. Then on the other side, maybe the prisoner's\n> dilemma would hold and all miners would have minrelaytxfee set at zero and\n> users would push the blocks to larger and larger sizes causing higher and\n> higher latency and network issues.\n>     Perhaps something like this could work (I can only speak to the\n> economic side anyway) but it would have to have some solid code that has a\n> social benefit model built in to adjust to an equilibrium that is able to\n> optimize---as in maximizes benefit/minimize cost for both sides via a\n> Marshallian surplus model--- for each size point.\n>      To be clear, I'm not saying an auto-adjusting limit is unworkable\n> (again only from an economic standpoint), just that it would need to have\n> these considerations built in.\n>\n> -Ryan J. Martin\n>\n>\n> ________________________________________\n>\n> ------------------------------\n>\n> Message: 2\n> Date: Mon, 9 Jan 2017 14:52:31 -0500\n> From: \"t. khan\" <teekhan42 at gmail.com>\n> To: Bitcoin Protocol Discussion\n>         <bitcoin-dev at lists.linuxfoundation.org>\n> Subject: [bitcoin-dev] BIP - Block75 - Historical and future\n>         projections\n> Message-ID:\n>         <CAGCNRJpSV9zKxhVvqpMVPyFyXco_ABB9a7_ihaDKEKFPQ9v3sw at mail.\n> gmail.com>\n> Content-Type: text/plain; charset=\"utf-8\"\n>\n> Using daily average block size over the past year (source:\n> https://blockchain.info/charts/avg-block-size?\n> daysAverageString=14&timespan=1year\n> ), here's how Block75 would have altered max block sizes:\n>\n> [image: Inline image 1]\n>\n> As of today, the max block size would be 1,135KB.\n>\n> Looking forward and using the last year's growth rate as a model:\n>\n> [image: Inline image 2]\n>\n> This shows the max block size one year from now would be 2,064KB, if\n> Block75 activated today.\n>\n> Of course, this is just an estimate, but even accounting for a substantial\n> increase in transactions in the last quarter of 2017 and changes brought\n> about by SegWit (hopefully) activating, Block75 alters the max size in such\n> a way that allows for growth, keeps blocks as small as possible, *and*\n> maintains transaction fees at a level similar to May/June 2016.\n>\n> If anyone has an alternate way to model future behavior, please run it\n> through the Block75 algorithm.\n>\n> Every 2016 blocks, do this:\n>\n> new max blocksize = x + (x * (AVERAGE_CAPACITY - TARGET_CAPACITY))\n>\n> TARGET_CAPACITY = 0.75    //Block75's target of keeping blocks 75% full\n> AVERAGE_CAPACITY = average percentage full of the last 2016 blocks, as a\n> decimal\n> x = current max block size\n>\n>\n> Thanks,\n>\n> - t.k.\n> -------------- next part --------------\n> An HTML attachment was scrubbed...\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> attachments/20170109/b0e0b713/attachment.html>\n> -------------- next part --------------\n> A non-text attachment was scrubbed...\n> Name: Block75 2016.png\n> Type: image/png\n> Size: 32088 bytes\n> Desc: not available\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> attachments/20170109/b0e0b713/attachment.png>\n> -------------- next part --------------\n> A non-text attachment was scrubbed...\n> Name: Block75 2017.png\n> Type: image/png\n> Size: 33176 bytes\n> Desc: not available\n> URL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/\n> attachments/20170109/b0e0b713/attachment-0001.png>\n>\n> ------------------------------\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n> End of bitcoin-dev Digest, Vol 20, Issue 21\n> *******************************************\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170110/fc2fd209/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "BIP - Block75 - Historical and future projections (t. khan)",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Adam Back",
                "t. khan",
                "Ryan J Martin"
            ],
            "messages_count": 3,
            "total_messages_chars_count": 13996
        }
    },
    {
        "title": "[bitcoin-dev] Transaction Replacement by Fee",
        "thread_messages": [
            {
                "author": "Police Terror",
                "date": "2017-01-12T14:02:00",
                "message_text_only": "Hello,\n\nWhere can I find the rules on transaction replacement?\n\nFor instance what are the valid ranges for the sequence values in\ntransaction inputs? Normally I set this value to MAX_UINT32, and the\nlocktime to 0.\n\nCan I change the outputs? Can I add more inputs to pay a higher fee?\n\nJust looking for clarity on this aspect of Bitcoin. Any resources would\nbe much appreciated.\n\nThanks."
            },
            {
                "author": "Greg Sanders",
                "date": "2017-01-12T14:13:23",
                "message_text_only": "BIP125 is the standard way to signal:\nhttps://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n\nShould explain everything you need.\n\nOn Thu, Jan 12, 2017 at 9:02 AM, Police Terror via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> Hello,\n>\n> Where can I find the rules on transaction replacement?\n>\n> For instance what are the valid ranges for the sequence values in\n> transaction inputs? Normally I set this value to MAX_UINT32, and the\n> locktime to 0.\n>\n> Can I change the outputs? Can I add more inputs to pay a higher fee?\n>\n> Just looking for clarity on this aspect of Bitcoin. Any resources would\n> be much appreciated.\n>\n> Thanks.\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170112/31f51bed/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-01-12T19:58:30",
                "message_text_only": "On Thu, Jan 12, 2017 at 09:13:23AM -0500, Greg Sanders via bitcoin-dev wrote:\n> BIP125 is the standard way to signal:\n> https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n> \n> Should explain everything you need.\n\nAdditionally some miners mine full replace-by-fee, which has no limitations on\nnSequence. My implementation (for v0.13.2) is here:\n\n    https://github.com/petertodd/bitcoin/tree/replace-by-fee-v0.13.2\n\nand is identical to Bitcoin Core modulo the nSequence stuff being removed, and\na special service bit added to allow full-rbf nodes to preferentially peer with\neach other to make sure replacement transactions get propagated.\n\nIn practice full-RBF works fairly well, so while it's even faster to use the\nnSequence signalling specified in BIP-125, doing so is not mandatory so long as\nyou can et your replacement transaction to a full-RBF node.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170112/3af4525d/attachment.sig>"
            },
            {
                "author": "Andrew",
                "date": "2017-01-28T18:34:58",
                "message_text_only": "Hi, recently been trying to get RBF working on a multisig input. I set\nthe nSequence to 0, but script didn't verify (used python-bitcoinlib).\nShould it work for this type of transaction? I am using the\nSignatureHash(...) method of signing, not rpc.signrawtransaction(...).\n\nThanks\n\nOn Thu, Jan 12, 2017 at 7:58 PM, Peter Todd via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> On Thu, Jan 12, 2017 at 09:13:23AM -0500, Greg Sanders via bitcoin-dev wrote:\n>> BIP125 is the standard way to signal:\n>> https://github.com/bitcoin/bips/blob/master/bip-0125.mediawiki\n>>\n>> Should explain everything you need.\n>\n> Additionally some miners mine full replace-by-fee, which has no limitations on\n> nSequence. My implementation (for v0.13.2) is here:\n>\n>     https://github.com/petertodd/bitcoin/tree/replace-by-fee-v0.13.2\n>\n> and is identical to Bitcoin Core modulo the nSequence stuff being removed, and\n> a special service bit added to allow full-rbf nodes to preferentially peer with\n> each other to make sure replacement transactions get propagated.\n>\n> In practice full-RBF works fairly well, so while it's even faster to use the\n> nSequence signalling specified in BIP-125, doing so is not mandatory so long as\n> you can et your replacement transaction to a full-RBF node.\n>\n> --\n> https://petertodd.org 'peter'[:-1]@petertodd.org\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n\n\n\n-- \nPGP: B6AC 822C 451D 6304 6A28  49E9 7DB7 011C D53B 5647"
            }
        ],
        "thread_summary": {
            "title": "Transaction Replacement by Fee",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Police Terror",
                "Andrew",
                "Peter Todd",
                "Greg Sanders"
            ],
            "messages_count": 4,
            "total_messages_chars_count": 4215
        }
    },
    {
        "title": "[bitcoin-dev] Forcenet: an experimental network with a new header format",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2017-01-14T21:14:55",
                "message_text_only": "I created a second version of forcenet with more experimental features and stopped my forcenet1 node.\n\n1. It has a new header format: Height (4), BIP9 signalling field (4), hardfork signalling field (2), Hash TMR (32), Hash WMR (32), Merkle sum root (32), number of tx (4), prev hash (32), timestamp (4), nBits (4), nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), merkle branches leading to header C (compactSize + 32 bit hashes)\n\n2. Anti-tx-replay. If, after masking the highest byte, the tx nVersion is >=3, the sighash for both segwit and non-segwit outputs is calculated with BIP143, except 0x2000000 is added to the nHashType. Such signatures are invalid for legacy nodes. But since they are non-std due the nVersion, they won\u2019t be relayed nor validated by legacy nodes. This also removes the O(n^2) sighash problem when spending non-segwit outputs. (anti-replay is a long story and I will discuss in a separate post/BIP)\n\n3. Block sighashlimit (https://github.com/jl2012/bips/blob/sighash/bip-sighash.mediawiki <https://github.com/jl2012/bips/blob/sighash/bip-sighash.mediawiki>). Due to point 2, SigHashSize is counted only for legacy non-segwit inputs (with masked tx nVersion < 3). We have to support legacy signature to make sure time-locked txs made before the hard fork are still valid.\n\n4. A totally new way to define tx weight. Tx weight is the maximum of the following metrics:\na. SigHashSize (see the bip in point 3)\nb. Witness serialised size * 2 * 90\nc. Adjusted size * 90. Adjusted size = tx weight (BIP141) + (number of non-OP_RETURN outputs - number of inputs) * 41 * 4\nd. nSigOps * 50 * 90. All SigOps are equal (no witness scaling). For non-segwit txs, the sigops in output scriptPubKey are not counted, while the sigops in input scriptPubKey are counted.\n\n90 is the scaling factor for SigHashSize, to maintain the 1:90 ratio (see the BIP in point 3)\n50 is the scaling factor for nSigOps, maintaining the 1:50 ratio in BIP141\n\nRationale for adjusted size: 4 is witness scaling factor. 41 is the minimum size for an input (32 hash + 4 index + 4 nSequence + 1 scriptSig). This requires people to pre-pay majority of the fee of spending an UTXO. It makes creation of UTXO more expensive, while spending of UTXO cheaper, creates a strong incentive to limit the growth of UTXO set.\n\nRationale for taking the maximum of different metrics: this indirectly set an upper block resources for _every_ metrics, while making the tx fee estimation a linear function. Currently, there are 2 block resources limits: block weight and nSigOp cost (BIP141). However, since users do not know what the other txs are included in the next block, it is difficult to determine whether tx weight of nSigOp cost is a more important factor in determining the tx fee. (This is not a real problem now, because weight is more important in most cases). With an unified definition of tx weight, the fee estimation becomes a linear problem.\n\nTranslating to new metric, the current BIP141 limit is 360,000,000. This is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of adjusted size, or 80000 nSigOp.\n\nAny new block-level limit metrics could be added to tx weight using soft forks.\n\n5. Smooth halving: the reward of the last 2016 blocks in a halving cycle will be reduced by 25%, which is contributed to the first 2016 blocks of the new halving cycle. (different parameters for forcenet) This makes a more graceful transition but we will lose some fun around halving.\n\n6. A new coinbase tx format. BIP34 is removed. Coinbase tx may have more than 1 input. The prevout hash of first input must be the hash of previous block, and index must be 0xffffffff. The other inputs (if any) must come from UTXOs with valid signatures. Spending of previous coinbase outputs in a coinbase tx is exempted from the 100 block maturity requirement. Therefore, miners of an earlier block may pay other miners to convince them to confirm their blocks.\n\n7. Merkle sum tree: it allows generating of fraud-proof for fee and weight. A special softfork (bit 15) is defined. When this softfork is activated, the full node will not validate the sum tree. This is needed because when the definition of tx weight is changed through a softfork (e.g. a new script version introducing new sigop), olds nodes won\u2019t know the new rules and will find the sum tree invalid. Disabling the sum tree validation won\u2019t degrade the security of a full node by more than an usual softfork, because the full node would still validate all other known rules.\n\nHowever, it is still not possible to create fraud proof for spending of non-existing UTXO. This requires commitment of the block height of inputs, and the tx index in the block. I\u2019m not quire sure how this could be implemented because a re-org may change such info (I think validation is easy but mining is more tricky)\n\nHow to join: codes at https://github.com/jl2012/bitcoin/tree/forcenet2 <https://github.com/jl2012/bitcoin/tree/forcenet2> , start with \"bitcoind \u2014forcenet\" .\nConnection: I\u2019m running a node at 8333.info <http://8333.info/> with default port (39901)\nMining: there is only basic internal mining support. To use the internal miner, writeup a shell script to repeatedly call \u201cbitcoin-cli \u2014forcenet generate 1\u201d\n\njl2012\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170115/539d75e2/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-01-28T02:32:26",
                "message_text_only": "Looks cool, though I have a few comments inline.\n\nOne general note - it looks like you're letting complexity run away from\nyou a bit here. If the motivation for something is only weak, its\nprobably not worth doing! A hard fork is something that must be\nundertaken cautiously because it has so much inherent risk, lets not add\ntons to it.\n\nMatt\n\nOn 01/14/17 21:14, Johnson Lau via bitcoin-dev wrote:\n> I created a second version of forcenet with more experimental features\n> and stopped my forcenet1 node.\n> \n> 1. It has a new header format: Height (4), BIP9 signalling field (4),\n> hardfork signalling field (2), Hash TMR (32), Hash WMR (32), Merkle sum\n> root (32), number of tx (4), prev hash (32), timestamp (4), nBits (4),\n> nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), merkle branches\n> leading to header C (compactSize + 32 bit hashes)\n\nIn order of appearance:\n\nFirst of all lets try to minimize header size. We really dont want any\nmore space taken up here than we absolutely need to.\n\nI'm super unconvinced that we need more than one merkle tree for\ntransactions. Lets just have one merkle tree who's leaves are\ntransactions hashed 2 ways (without witnesses and only witnesses).\n\nWhy duplicate the nBits here? shouldn't the PoW proof be the\nresponsibility of the parent header?\n\nI have to agree with Tadge here, variable-length header fields are evil,\nlets avoid them.\n\nWhy have merkle branches to yet another header? Lets just leave it as an\nopaque commitment header (32).\n\nFinally, lets not jump through hoops here - the transaction merkle root\nof the \"old-style\" (now PoW) header should simply be the hash of the new\nheader. No coinbase transaction, just the hash of the secondary header.\nThis saves space without giving up utility - SPV nodes are already not\nlooking at the coinbase transaction, so no harm in not having one to give.\n\n> 2. Anti-tx-replay. If, after masking the highest byte, the tx nVersion\n> is >=3, the sighash for both segwit and non-segwit outputs is calculated\n> with BIP143, except 0x2000000 is added to the nHashType. Such signatures\n> are invalid for legacy nodes. But since they are non-std due the\n> nVersion, they won\u2019t be relayed nor validated by legacy nodes. This also\n> removes the O(n^2) sighash problem when spending non-segwit outputs.\n> (anti-replay is a long story and I will discuss in a separate post/BIP)\n\nWill comment on the anti-replay post.\n\n> 3. Block sighashlimit\n> (https://github.com/jl2012/bips/blob/sighash/bip-sighash.mediawiki). Due\n> to point 2, SigHashSize is counted only for legacy non-segwit inputs\n> (with masked tx nVersion < 3). We have to support legacy signature to\n> make sure time-locked txs made before the hard fork are still valid.\n> \n> 4. A totally new way to define tx weight. Tx weight is the maximum of\n> the following metrics:\n> a. SigHashSize (see the bip in point 3)\n> b. Witness serialised size * 2 * 90\n> c. Adjusted size * 90. Adjusted size = tx weight (BIP141) + (number of\n> non-OP_RETURN outputs - number of inputs) * 41 * 4\n> d. nSigOps * 50 * 90. All SigOps are equal (no witness scaling). For\n> non-segwit txs, the sigops in output scriptPubKey are not counted, while\n> the sigops in input scriptPubKey are counted.\n\nThis is definitely too much. On the one hand its certainly nice to be\nable to use max() for limits, and nice to add all the reasonable limits\nwe might want to, but on the other hand this can make things like coin\nselection super complicated - how do you take into consideration the 4\ndifferent limits? Can we do something much, much simpler like\nmax(serialized size with some input discount, nSigOps * X) (which is\nwhat we effectively already have in our mining code)?\n\n> 90 is the scaling factor for SigHashSize, to maintain the 1:90 ratio\n> (see the BIP in point 3)\n> 50 is the scaling factor for nSigOps, maintaining the 1:50 ratio in BIP141\n> \n> Rationale for adjusted size: 4 is witness scaling factor. 41 is the\n> minimum size for an input (32 hash + 4 index + 4 nSequence + 1\n> scriptSig). This requires people to pre-pay majority of the fee of\n> spending an UTXO. It makes creation of UTXO more expensive, while\n> spending of UTXO cheaper, creates a strong incentive to limit the growth\n> of UTXO set.\n> \n> Rationale for taking the maximum of different metrics: this indirectly\n> set an upper block resources for _every_ metrics, while making the tx\n> fee estimation a linear function. Currently, there are 2 block resources\n> limits: block weight and nSigOp cost (BIP141). However, since users do\n> not know what the other txs are included in the next block, it is\n> difficult to determine whether tx weight of nSigOp cost is a more\n> important factor in determining the tx fee. (This is not a real problem\n> now, because weight is more important in most cases). With an unified\n> definition of tx weight, the fee estimation becomes a linear problem.\n> \n> Translating to new metric, the current BIP141 limit is 360,000,000. This\n> is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of\n> adjusted size, or 80000 nSigOp.\n> \n> Any new block-level limit metrics could be added to tx weight using soft\n> forks.\n> \n> 5. Smooth halving: the reward of the last 2016 blocks in a halving cycle\n> will be reduced by 25%, which is contributed to the first 2016 blocks of\n> the new halving cycle. (different parameters for forcenet) This makes a\n> more graceful transition but we will lose some fun around halving.\n\nHum, not sure this is sufficient. Its still stair-stepping at big enough\njumps that we could conceivably see super slow block times around\nhalvings in the distant future. Maybe instead of 100%-75%-75%-50% (I\nbelieve that's what you're proposing here?),\n100%-87.5%-75%-75%-62.5%-50% might be smoother?\n\n> 6. A new coinbase tx format. BIP34 is removed. Coinbase tx may have more\n> than 1 input. The prevout hash of first input must be the hash of\n> previous block, and index must be 0xffffffff.\n\nI'm not necessarily opposed to this, but what is the justification for it?\n\n> The other inputs (if any)\n> must come from UTXOs with valid signatures. Spending of previous\n> coinbase outputs in a coinbase tx is exempted from the 100 block\n> maturity requirement. Therefore, miners of an earlier block may pay\n> other miners to convince them to confirm their blocks.\n\nSounds good.\n\n> 7. Merkle sum tree: it allows generating of fraud-proof for fee and\n> weight. A special softfork (bit 15) is defined. When this softfork is\n> activated, the full node will not validate the sum tree. This is needed\n> because when the definition of tx weight is changed through a softfork\n> (e.g. a new script version introducing new sigop), olds nodes won\u2019t know\n> the new rules and will find the sum tree invalid. Disabling the sum tree\n> validation won\u2019t degrade the security of a full node by more than an\n> usual softfork, because the full node would still validate all other\n> known rules.\n> \n> However, it is still not possible to create fraud proof for spending of\n> non-existing UTXO. This requires commitment of the block height of\n> inputs, and the tx index in the block. I\u2019m not quire sure how this could\n> be implemented because a re-org may change such info (I think validation\n> is easy but mining is more tricky)\n\nIf we cant build wholesale proofs, then lets not jump through hoops and\nadd special bits to build partial ones? Its not clear to me that it\nwould be any reduction in soft-fork-ability later down the road to not\nhave this - if you're changing the definition of tx weight, you're\nlikely doing something like segwit where you're adding something else,\nnot trying to re-adjust weights.\n\n> How to join: codes at https://github.com/jl2012/bitcoin/tree/forcenet2 ,\n> start with \"bitcoind \u2014forcenet\" .\n> Connection: I\u2019m running a node at 8333.info <http://8333.info> with\n> default port (39901)\n> Mining: there is only basic internal mining support. To use the internal\n> miner, writeup a shell script to repeatedly call \u201cbitcoin-cli \u2014forcenet\n> generate 1\u201d\n> \n> jl2012\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-01-28T03:02:21",
                "message_text_only": "Oops, forgot to mention, in the \"parent\" (ie old) block header, we should:\n\n1) fix the version field so its a static constant\n2) swap first 2 bytes of the merkle root with the timestamp's two\nhigh-order bytes (preferably more, I'm not sure how much ASIC hardware\nhas timestamp-rolling in it anymore, but if there is none left we should\ntake all 4 bytes from the timestamp field).\n\nMatt\n\nOn 01/28/17 02:32, Matt Corallo via bitcoin-dev wrote:\n> Looks cool, though I have a few comments inline.\n> \n> One general note - it looks like you're letting complexity run away from\n> you a bit here. If the motivation for something is only weak, its\n> probably not worth doing! A hard fork is something that must be\n> undertaken cautiously because it has so much inherent risk, lets not add\n> tons to it.\n> \n> Matt\n> \n> On 01/14/17 21:14, Johnson Lau via bitcoin-dev wrote:\n>> I created a second version of forcenet with more experimental features\n>> and stopped my forcenet1 node.\n>>\n>> 1. It has a new header format: Height (4), BIP9 signalling field (4),\n>> hardfork signalling field (2), Hash TMR (32), Hash WMR (32), Merkle sum\n>> root (32), number of tx (4), prev hash (32), timestamp (4), nBits (4),\n>> nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), merkle branches\n>> leading to header C (compactSize + 32 bit hashes)\n> \n> In order of appearance:\n> \n> First of all lets try to minimize header size. We really dont want any\n> more space taken up here than we absolutely need to.\n> \n> I'm super unconvinced that we need more than one merkle tree for\n> transactions. Lets just have one merkle tree who's leaves are\n> transactions hashed 2 ways (without witnesses and only witnesses).\n> \n> Why duplicate the nBits here? shouldn't the PoW proof be the\n> responsibility of the parent header?\n> \n> I have to agree with Tadge here, variable-length header fields are evil,\n> lets avoid them.\n> \n> Why have merkle branches to yet another header? Lets just leave it as an\n> opaque commitment header (32).\n> \n> Finally, lets not jump through hoops here - the transaction merkle root\n> of the \"old-style\" (now PoW) header should simply be the hash of the new\n> header. No coinbase transaction, just the hash of the secondary header.\n> This saves space without giving up utility - SPV nodes are already not\n> looking at the coinbase transaction, so no harm in not having one to give.\n> \n>> 2. Anti-tx-replay. If, after masking the highest byte, the tx nVersion\n>> is >=3, the sighash for both segwit and non-segwit outputs is calculated\n>> with BIP143, except 0x2000000 is added to the nHashType. Such signatures\n>> are invalid for legacy nodes. But since they are non-std due the\n>> nVersion, they won\u2019t be relayed nor validated by legacy nodes. This also\n>> removes the O(n^2) sighash problem when spending non-segwit outputs.\n>> (anti-replay is a long story and I will discuss in a separate post/BIP)\n> \n> Will comment on the anti-replay post.\n> \n>> 3. Block sighashlimit\n>> (https://github.com/jl2012/bips/blob/sighash/bip-sighash.mediawiki). Due\n>> to point 2, SigHashSize is counted only for legacy non-segwit inputs\n>> (with masked tx nVersion < 3). We have to support legacy signature to\n>> make sure time-locked txs made before the hard fork are still valid.\n>>\n>> 4. A totally new way to define tx weight. Tx weight is the maximum of\n>> the following metrics:\n>> a. SigHashSize (see the bip in point 3)\n>> b. Witness serialised size * 2 * 90\n>> c. Adjusted size * 90. Adjusted size = tx weight (BIP141) + (number of\n>> non-OP_RETURN outputs - number of inputs) * 41 * 4\n>> d. nSigOps * 50 * 90. All SigOps are equal (no witness scaling). For\n>> non-segwit txs, the sigops in output scriptPubKey are not counted, while\n>> the sigops in input scriptPubKey are counted.\n> \n> This is definitely too much. On the one hand its certainly nice to be\n> able to use max() for limits, and nice to add all the reasonable limits\n> we might want to, but on the other hand this can make things like coin\n> selection super complicated - how do you take into consideration the 4\n> different limits? Can we do something much, much simpler like\n> max(serialized size with some input discount, nSigOps * X) (which is\n> what we effectively already have in our mining code)?\n> \n>> 90 is the scaling factor for SigHashSize, to maintain the 1:90 ratio\n>> (see the BIP in point 3)\n>> 50 is the scaling factor for nSigOps, maintaining the 1:50 ratio in BIP141\n>>\n>> Rationale for adjusted size: 4 is witness scaling factor. 41 is the\n>> minimum size for an input (32 hash + 4 index + 4 nSequence + 1\n>> scriptSig). This requires people to pre-pay majority of the fee of\n>> spending an UTXO. It makes creation of UTXO more expensive, while\n>> spending of UTXO cheaper, creates a strong incentive to limit the growth\n>> of UTXO set.\n>>\n>> Rationale for taking the maximum of different metrics: this indirectly\n>> set an upper block resources for _every_ metrics, while making the tx\n>> fee estimation a linear function. Currently, there are 2 block resources\n>> limits: block weight and nSigOp cost (BIP141). However, since users do\n>> not know what the other txs are included in the next block, it is\n>> difficult to determine whether tx weight of nSigOp cost is a more\n>> important factor in determining the tx fee. (This is not a real problem\n>> now, because weight is more important in most cases). With an unified\n>> definition of tx weight, the fee estimation becomes a linear problem.\n>>\n>> Translating to new metric, the current BIP141 limit is 360,000,000. This\n>> is equivalent to 360MB of sighashing, 2MB of serialised size, 4MB of\n>> adjusted size, or 80000 nSigOp.\n>>\n>> Any new block-level limit metrics could be added to tx weight using soft\n>> forks.\n>>\n>> 5. Smooth halving: the reward of the last 2016 blocks in a halving cycle\n>> will be reduced by 25%, which is contributed to the first 2016 blocks of\n>> the new halving cycle. (different parameters for forcenet) This makes a\n>> more graceful transition but we will lose some fun around halving.\n> \n> Hum, not sure this is sufficient. Its still stair-stepping at big enough\n> jumps that we could conceivably see super slow block times around\n> halvings in the distant future. Maybe instead of 100%-75%-75%-50% (I\n> believe that's what you're proposing here?),\n> 100%-87.5%-75%-75%-62.5%-50% might be smoother?\n> \n>> 6. A new coinbase tx format. BIP34 is removed. Coinbase tx may have more\n>> than 1 input. The prevout hash of first input must be the hash of\n>> previous block, and index must be 0xffffffff.\n> \n> I'm not necessarily opposed to this, but what is the justification for it?\n> \n>> The other inputs (if any)\n>> must come from UTXOs with valid signatures. Spending of previous\n>> coinbase outputs in a coinbase tx is exempted from the 100 block\n>> maturity requirement. Therefore, miners of an earlier block may pay\n>> other miners to convince them to confirm their blocks.\n> \n> Sounds good.\n> \n>> 7. Merkle sum tree: it allows generating of fraud-proof for fee and\n>> weight. A special softfork (bit 15) is defined. When this softfork is\n>> activated, the full node will not validate the sum tree. This is needed\n>> because when the definition of tx weight is changed through a softfork\n>> (e.g. a new script version introducing new sigop), olds nodes won\u2019t know\n>> the new rules and will find the sum tree invalid. Disabling the sum tree\n>> validation won\u2019t degrade the security of a full node by more than an\n>> usual softfork, because the full node would still validate all other\n>> known rules.\n>>\n>> However, it is still not possible to create fraud proof for spending of\n>> non-existing UTXO. This requires commitment of the block height of\n>> inputs, and the tx index in the block. I\u2019m not quire sure how this could\n>> be implemented because a re-org may change such info (I think validation\n>> is easy but mining is more tricky)\n> \n> If we cant build wholesale proofs, then lets not jump through hoops and\n> add special bits to build partial ones? Its not clear to me that it\n> would be any reduction in soft-fork-ability later down the road to not\n> have this - if you're changing the definition of tx weight, you're\n> likely doing something like segwit where you're adding something else,\n> not trying to re-adjust weights.\n> \n>> How to join: codes at https://github.com/jl2012/bitcoin/tree/forcenet2 ,\n>> start with \"bitcoind \u2014forcenet\" .\n>> Connection: I\u2019m running a node at 8333.info <http://8333.info> with\n>> default port (39901)\n>> Mining: there is only basic internal mining support. To use the internal\n>> miner, writeup a shell script to repeatedly call \u201cbitcoin-cli \u2014forcenet\n>> generate 1\u201d\n>>\n>> jl2012\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-28T07:28:14",
                "message_text_only": "> On 28 Jan 2017, at 10:32, Matt Corallo <lf-lists at mattcorallo.com> wrote:\n> \n> Looks cool, though I have a few comments inline.\n> \n> One general note - it looks like you're letting complexity run away from\n> you a bit here. If the motivation for something is only weak, its\n> probably not worth doing! A hard fork is something that must be\n> undertaken cautiously because it has so much inherent risk, lets not add\n> tons to it.\n> \n\nI think the following features are necessary for a hardfork. The rest are optional:\n\n1. A secondary header\n2. Anti-replay\n3. SigHash limit for old scripts\n4. New tx weight accounting\n\nOptional:\n1. New coinbase format is nice but not strictly needed. But this can\u2019t be reintroduced later with softfork due to the 100 block maturity requirement\n2. Smooth halving: could be a less elegant softfork\n3. Mekle sum tree: definitely could be a softfork\n\n> Matt\n> \n> On 01/14/17 21:14, Johnson Lau via bitcoin-dev wrote:\n>> I created a second version of forcenet with more experimental features\n>> and stopped my forcenet1 node.\n>> \n>> 1. It has a new header format: Height (4), BIP9 signalling field (4),\n>> hardfork signalling field (2), Hash TMR (32), Hash WMR (32), Merkle sum\n>> root (32), number of tx (4), prev hash (32), timestamp (4), nBits (4),\n>> nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), merkle branches\n>> leading to header C (compactSize + 32 bit hashes)\n> \n> In order of appearance:\n> \n> First of all lets try to minimize header size. We really dont want any\n> more space taken up here than we absolutely need to.\n> \n> I'm super unconvinced that we need more than one merkle tree for\n> transactions. Lets just have one merkle tree who's leaves are\n> transactions hashed 2 ways (without witnesses and only witnesses).\n> \n> Why duplicate the nBits here? shouldn't the PoW proof be the\n> responsibility of the parent header?\n> \n\nWithout nBits in the header, the checking of PoW become contextual and I think that may involve too much change. The saving of these 4 bytes, if it is really desired, might be done on a p2p level \n\n> I have to agree with Tadge here, variable-length header fields are evil,\n> lets avoid them.\n> \n> Why have merkle branches to yet another header? Lets just leave it as an\n> opaque commitment header (32).\n> \n> Finally, lets not jump through hoops here - the transaction merkle root\n> of the \"old-style\" (now PoW) header should simply be the hash of the new\n> header. No coinbase transaction, just the hash of the secondary header.\n> This saves space without giving up utility - SPV nodes are already not\n> looking at the coinbase transaction, so no harm in not having one to give.\n\n\nRegarding the header format, a big question we never came into consensus is the format of the hardfork. Although I designed forcenet to be a soft-hardfork, I am now more inclined to suggest a simple hardfork, given that the warning system is properly fixed (at the minimum: https://github.com/bitcoin/bitcoin/pull/9443 <https://github.com/bitcoin/bitcoin/pull/9443>)\n\nAssuming a simple hardfork is made, the next question is whether we want to keep existing light wallets functioning without upgrade, cheating them by hiding the hash of the new header somewhere in the transaction merkle tree.\n\nWe also need to think about the Stratum protocol. Ideally we should not require firmware upgrade.\n\nFor the primary 80 bytes header, I think it will always be a fixed size. But for the secondary header, I\u2019m not quite sure. Actually, one may argue that we already have a secondary header (i.e. coinbase tx), and it is not fixed size.\n\n>> \n>> 4. A totally new way to define tx weight. Tx weight is the maximum of\n>> the following metrics:\n>> a. SigHashSize (see the bip in point 3)\n>> b. Witness serialised size * 2 * 90\n>> c. Adjusted size * 90. Adjusted size = tx weight (BIP141) + (number of\n>> non-OP_RETURN outputs - number of inputs) * 41 * 4\n>> d. nSigOps * 50 * 90. All SigOps are equal (no witness scaling). For\n>> non-segwit txs, the sigops in output scriptPubKey are not counted, while\n>> the sigops in input scriptPubKey are counted.\n> \n> This is definitely too much. On the one hand its certainly nice to be\n> able to use max() for limits, and nice to add all the reasonable limits\n> we might want to, but on the other hand this can make things like coin\n> selection super complicated - how do you take into consideration the 4\n> different limits? Can we do something much, much simpler like\n> max(serialized size with some input discount, nSigOps * X) (which is\n> what we effectively already have in our mining code)?\n> \n\nThe max() is at transaction level, not block level. Unless your wallet is full of different types of UTXOs, coin selection would not be more difficult than current.\n\nAmong the 4 limits, the SigHash limit is mostly a safety limit that will never be hit by a tx smaller than 100kB. As part of the replay attack fix, a linear SigHash may be optionally used. So wallets may just ignore this limit in coin selection\n\nSimilarly, the SigOp limit is also unlikely to be hit, unless you are using a very big multi-sig. Again, this is very uncommon and wallets primarily dealing with signal sig may safely ignore this\n\nFinally, an important principle here is to encourage spending of UTXO, and limiting creation of UTXO. This might be a bit difficult to fully optimise for this, but I think this is necessary evil.\n\nMore discussion at: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013504.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013504.html>\n\n>> \n>> \n>> 5. Smooth halving: the reward of the last 2016 blocks in a halving cycle\n>> will be reduced by 25%, which is contributed to the first 2016 blocks of\n>> the new halving cycle. (different parameters for forcenet) This makes a\n>> more graceful transition but we will lose some fun around halving.\n> \n> Hum, not sure this is sufficient. Its still stair-stepping at big enough\n> jumps that we could conceivably see super slow block times around\n> halvings in the distant future. Maybe instead of 100%-75%-75%-50% (I\n> believe that's what you're proposing here?),\n> 100%-87.5%-75%-75%-62.5%-50% might be smoother?\n> \n\nYes, but maybe we just don\u2019t need this at all. This could also be done with a softfork using OP_CSV, just a bit ugly.\n\n>> 6. A new coinbase tx format. BIP34 is removed. Coinbase tx may have more\n>> than 1 input. The prevout hash of first input must be the hash of\n>> previous block, and index must be 0xffffffff.\n> \n> I'm not necessarily opposed to this, but what is the justification for it?\n\nThis allows people to sign an input, to be part of a coinbase tx, but limited to a particular previous block hash. This is currently not possible, but through a later softfork we could introduce a new SigHash function that allows something between SIGHASH_ALL and SIGHASH_ANYONECANPAY, so people may sign its own input and another input, while ignoring the rests of input. (in other words: change the name SIGHASH_ANYONECANPAY to SIGHASH_SINGLE_INPUT, and we introduce SIGHASH_DUAL_INPUT. But we don\u2019t need to do this in this hardfork)\n\n\n>> The other inputs (if any)\n>> must come from UTXOs with valid signatures. Spending of previous\n>> coinbase outputs in a coinbase tx is exempted from the 100 block\n>> maturity requirement. Therefore, miners of an earlier block may pay\n>> other miners to convince them to confirm their blocks.\n> \n> Sounds good.\n> \n>> 7. Merkle sum tree: it allows generating of fraud-proof for fee and\n>> weight. A special softfork (bit 15) is defined. When this softfork is\n>> activated, the full node will not validate the sum tree. This is needed\n>> because when the definition of tx weight is changed through a softfork\n>> (e.g. a new script version introducing new sigop), olds nodes won\u2019t know\n>> the new rules and will find the sum tree invalid. Disabling the sum tree\n>> validation won\u2019t degrade the security of a full node by more than an\n>> usual softfork, because the full node would still validate all other\n>> known rules.\n>> \n>> However, it is still not possible to create fraud proof for spending of\n>> non-existing UTXO. This requires commitment of the block height of\n>> inputs, and the tx index in the block. I\u2019m not quire sure how this could\n>> be implemented because a re-org may change such info (I think validation\n>> is easy but mining is more tricky)\n> \n> If we cant build wholesale proofs, then lets not jump through hoops and\n> add special bits to build partial ones? Its not clear to me that it\n> would be any reduction in soft-fork-ability later down the road to not\n> have this - if you're changing the definition of tx weight, you're\n> likely doing something like segwit where you're adding something else,\n> not trying to re-adjust weights.\n\nThis is just a demo, and I agree this could be added through a softfork later. But even if we add this as a softfork, we have to have the ability to disable it through a special softfork. I think I have explained the reason but let me try again.\n\nHere, when I talking about \u201ctx weight\u201d, it\u2019s the \u201ctx weight\u201d defined in point 4, which covers not only size, but also other limits like SigOp. For a fraud proof to be really useful, it has to cover every type of block level limits. One feature of segwit is the script versioning, which allows introduction of new scripts. In the process, we will change the definition of SigOp: previous 0 SigOp scripts now carries some amount of SigOp. This is by itself a softfork (we did this type of softfork twice already: P2SH and segwit). However, if we have a merkle sum root covering the SigOp, old nodes won\u2019t count these new SigOps, and they will fail to validate the sum root.\n\nWithout a backdoor to disable the sum tree validation in old nodes, the only way would be keeping the original sum tree untouched, while create another sum tree, every time we have a new script version. This is not acceptable at all.\n\nBut even such backdoor would not be harmful to the security of full nodes because they will still fully verify the tx and witness merkle root.\n\nI\u2019d argue that any fraud proof related commitment: sum tree, delayed UTXO commitment etc will require such a backdoor to disable. Maybe we should just remove this from here and make this a new topic. We could even do this as a softfork today.\n\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170128/dff3b78c/attachment-0001.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-01-28T17:14:02",
                "message_text_only": "Replies inline.\n\nOn 01/28/17 07:28, Johnson Lau wrote:\n> \n>> On 28 Jan 2017, at 10:32, Matt Corallo <lf-lists at mattcorallo.com\n>> <mailto:lf-lists at mattcorallo.com>> wrote:\n>>\n>> Looks cool, though I have a few comments inline.\n>>\n>> One general note - it looks like you're letting complexity run away from\n>> you a bit here. If the motivation for something is only weak, its\n>> probably not worth doing! A hard fork is something that must be\n>> undertaken cautiously because it has so much inherent risk, lets not add\n>> tons to it.\n>>\n> \n> I think the following features are necessary for a hardfork. The rest\n> are optional:\n> \n> 1. A secondary header\n> 2. Anti-replay\n> 3. SigHash limit for old scripts\n> 4. New tx weight accounting\n\nAgreed.\n\n> Optional:\n> 1. New coinbase format is nice but not strictly needed. But this can\u2019t\n> be reintroduced later with softfork due to the 100 block maturity\n> requirement\n> 2. Smooth halving: could be a less elegant softfork\n> 3. Mekle sum tree: definitely could be a softfork\n\nAgreed. Would like 1, dont care about 2, not a fan of 3. 2 could even be\nimplemented easily as a softfork if we allow the\nspend-other-coinbase-outputs from 1.\n\n>>\n>> On 01/14/17 21:14, Johnson Lau via bitcoin-dev wrote:\n>>> I created a second version of forcenet with more experimental features\n>>> and stopped my forcenet1 node.\n>>>\n>>> 1. It has a new header format: Height (4), BIP9 signalling field (4),\n>>> hardfork signalling field (2), Hash TMR (32), Hash WMR (32), Merkle sum\n>>> root (32), number of tx (4), prev hash (32), timestamp (4), nBits (4),\n>>> nonce1 (4), nonce2 (4), nonce3 (compactSize + variable), merkle branches\n>>> leading to header C (compactSize + 32 bit hashes)\n>>\n>> In order of appearance:\n>>\n>> First of all lets try to minimize header size. We really dont want any\n>> more space taken up here than we absolutely need to.\n>>\n>> I'm super unconvinced that we need more than one merkle tree for\n>> transactions. Lets just have one merkle tree who's leaves are\n>> transactions hashed 2 ways (without witnesses and only witnesses).\n>>\n>> Why duplicate the nBits here? shouldn't the PoW proof be the\n>> responsibility of the parent header?\n>>\n> \n> Without nBits in the header, the checking of PoW become contextual and I\n> think that may involve too much change. The saving of these 4 bytes, if\n> it is really desired, might be done on a p2p level \n\nHmm? I'm saying that \"the header\" should be viewed as both the\n\"top-level\" PoW-proving header, and the sub-header. There is no need to\nhave nBits in both?\n\n>> I have to agree with Tadge here, variable-length header fields are evil,\n>> lets avoid them.\n>>\n>> Why have merkle branches to yet another header? Lets just leave it as an\n>> opaque commitment header (32).\n>>\n>> Finally, lets not jump through hoops here - the transaction merkle root\n>> of the \"old-style\" (now PoW) header should simply be the hash of the new\n>> header. No coinbase transaction, just the hash of the secondary header.\n>> This saves space without giving up utility - SPV nodes are already not\n>> looking at the coinbase transaction, so no harm in not having one to give.\n> \n> \n> Regarding the header format, a big question we never came into consensus\n> is the format of the hardfork. Although I designed forcenet to be a\n> soft-hardfork, I am now more inclined to suggest a simple hardfork,\n> given that the warning system is properly fixed (at the\n> minimum: https://github.com/bitcoin/bitcoin/pull/9443)\n> \n> Assuming a simple hardfork is made, the next question is whether we want\n> to keep existing light wallets functioning without upgrade, cheating\n> them by hiding the hash of the new header somewhere in the transaction\n> merkle tree.\n> \n> We also need to think about the Stratum protocol. Ideally we should not\n> require firmware upgrade.\n> \n> For the primary 80 bytes header, I think it will always be a fixed size.\n> But for the secondary header, I\u2019m not quite sure. Actually, one may\n> argue that we already have a secondary header (i.e. coinbase tx), and it\n> is not fixed size.\n\nWe can safely disable SPV clients post-fork by just keeping the header\nformat sufficiently compatible with PR#9443 without caring about the\ncoinbase transaction, which I think should be the goal.\n\nRegarding firmware upgrade, you make a valid point. I suppose we need\nsomething that looks sufficiently like a coinbase transaction that\nminers can do nonce-rolling using existing algorithms. Personally, I'd\nkinda prefer something like a two-leaf merkle tree root as the merkle\nroot in the \"primary 80-byte header\" (can we agree on terminology for\nthis before we go any further?) - the left one is a\ncoinbase-transaction-looking thing, the right one the header of the new\nblock header.\n\n>>>\n>>> 4. A totally new way to define tx weight. Tx weight is the maximum of\n>>> the following metrics:\n>>> a. SigHashSize (see the bip in point 3)\n>>> b. Witness serialised size * 2 * 90\n>>> c. Adjusted size * 90. Adjusted size = tx weight (BIP141) + (number of\n>>> non-OP_RETURN outputs - number of inputs) * 41 * 4\n>>> d. nSigOps * 50 * 90. All SigOps are equal (no witness scaling). For\n>>> non-segwit txs, the sigops in output scriptPubKey are not counted, while\n>>> the sigops in input scriptPubKey are counted.\n>>\n>> This is definitely too much. On the one hand its certainly nice to be\n>> able to use max() for limits, and nice to add all the reasonable limits\n>> we might want to, but on the other hand this can make things like coin\n>> selection super complicated - how do you take into consideration the 4\n>> different limits? Can we do something much, much simpler like\n>> max(serialized size with some input discount, nSigOps * X) (which is\n>> what we effectively already have in our mining code)?\n>>\n> \n> The max() is at transaction level, not block level. Unless your wallet\n> is full of different types of UTXOs, coin selection would not be more\n> difficult than current.\n\nYes, I got the max() being at the transaction level (at the block level\nwould just be stupid) :).\n\nThis does not, however, make UTXO selection trivial, indeed, the second\nyou start having not-completely-homogeneous UTXOs in your wallet you\nhave to consider \"what if the selection of this UTXO would switch my\ncriteria from one to another\", which I believe makes this nonlinear.\n\n> Among the 4 limits, the SigHash limit is mostly a safety limit that will\n> never be hit by a tx smaller than 100kB. As part of the replay attack\n> fix, a linear SigHash may be optionally used. So wallets may just ignore\n> this limit in coin selection\n\nSo lets apply this only to non-Segwit-hashed transactions (incl\ntransactions which opted into the new sighash rules using the\nanti-replay stuff)?\n\n> Similarly, the SigOp limit is also unlikely to be hit, unless you are\n> using a very big multi-sig. Again, this is very uncommon and wallets\n> primarily dealing with signal sig may safely ignore this\n\nYes, I tend to agree that there isnt much way around a sigops limit (as\nwe have now).\n\n> Finally, an important principle here is to encourage spending of UTXO,\n> and limiting creation of UTXO. This might be a bit difficult to fully\n> optimise for this, but I think this is necessary evil.\n\nTotally agree there, but we can easily discount inputs more than outputs\nto accomplish this for most potential outputs, I believe.\n\nDid I miss a justification for there being a separate b (witness\nserialized size) and c (txweight-with-discounts?).\n\n> More discussion\n> at: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013504.html\n> \n>>>\n>>>\n>>> 5. Smooth halving: the reward of the last 2016 blocks in a halving cycle\n>>> will be reduced by 25%, which is contributed to the first 2016 blocks of\n>>> the new halving cycle. (different parameters for forcenet) This makes a\n>>> more graceful transition but we will lose some fun around halving.\n>>\n>> Hum, not sure this is sufficient. Its still stair-stepping at big enough\n>> jumps that we could conceivably see super slow block times around\n>> halvings in the distant future. Maybe instead of 100%-75%-75%-50% (I\n>> believe that's what you're proposing here?),\n>> 100%-87.5%-75%-75%-62.5%-50% might be smoother?\n>>\n> \n> Yes, but maybe we just don\u2019t need this at all. This could also be done\n> with a softfork using OP_CSV, just a bit ugly.\n\nOr by allowing coinbase txn to spend previous coinbase outputs. This\nseems to not be an issue at present, though is something to consider in\nfuture soft forks, so, agreed, lets table this and make sure we're set\nup to do it in a soft fork if we need to.\n\n>>> 6. A new coinbase tx format. BIP34 is removed. Coinbase tx may have more\n>>> than 1 input. The prevout hash of first input must be the hash of\n>>> previous block, and index must be 0xffffffff.\n>>\n>> I'm not necessarily opposed to this, but what is the justification for it?\n> \n> This allows people to sign an input, to be part of a coinbase tx, but\n> limited to a particular previous block hash. This is currently not\n> possible, but through a later softfork we could introduce a new SigHash\n> function that allows something between SIGHASH_ALL and\n> SIGHASH_ANYONECANPAY, so people may sign its own input and another\n> input, while ignoring the rests of input. (in other words: change the\n> name SIGHASH_ANYONECANPAY to SIGHASH_SINGLE_INPUT, and we introduce\n> SIGHASH_DUAL_INPUT. But we don\u2019t need to do this in this hardfork)\n\nHmm, cant we accomplish this with a future sighash mode in which you\nsimply include the block's hash in the sighash and then spend to an\nanyone-can-spend?\n\n-snip-\n\n>>> 7. Merkle sum tree: it allows generating of fraud-proof for fee and\n>>> weight. A special softfork (bit 15) is defined. When this softfork is\n>>> activated, the full node will not validate the sum tree. This is needed\n>>> because when the definition of tx weight is changed through a softfork\n>>> (e.g. a new script version introducing new sigop), olds nodes won\u2019t know\n>>> the new rules and will find the sum tree invalid. Disabling the sum tree\n>>> validation won\u2019t degrade the security of a full node by more than an\n>>> usual softfork, because the full node would still validate all other\n>>> known rules.\n>>>\n>>> However, it is still not possible to create fraud proof for spending of\n>>> non-existing UTXO. This requires commitment of the block height of\n>>> inputs, and the tx index in the block. I\u2019m not quire sure how this could\n>>> be implemented because a re-org may change such info (I think validation\n>>> is easy but mining is more tricky)\n>>\n>> If we cant build wholesale proofs, then lets not jump through hoops and\n>> add special bits to build partial ones? Its not clear to me that it\n>> would be any reduction in soft-fork-ability later down the road to not\n>> have this - if you're changing the definition of tx weight, you're\n>> likely doing something like segwit where you're adding something else,\n>> not trying to re-adjust weights.\n> \n> This is just a demo, and I agree this could be added through a softfork\n> later. But even if we add this as a softfork, we have to have the\n> ability to disable it through a special softfork. I think I have\n> explained the reason but let me try again.\n> \n> Here, when I talking about \u201ctx weight\u201d, it\u2019s the \u201ctx weight\u201d defined in\n> point 4, which covers not only size, but also other limits like SigOp.\n> For a fraud proof to be really useful, it has to cover every type of\n> block level limits. One feature of segwit is the script versioning,\n> which allows introduction of new scripts. In the process, we will change\n> the definition of SigOp: previous 0 SigOp scripts now carries some\n> amount of SigOp. This is by itself a softfork (we did this type of\n> softfork twice already: P2SH and segwit). However, if we have a merkle\n> sum root covering the SigOp, old nodes won\u2019t count these new SigOps, and\n> they will fail to validate the sum root.\n> \n> Without a backdoor to disable the sum tree validation in old nodes, the\n> only way would be keeping the original sum tree untouched, while create\n> another sum tree, every time we have a new script version. This is not\n> acceptable at all.\n\nHmm, I think you missed my point - if we're soft-forking in a new limit,\nwe can trivially add a new merkle tree over only that limit, which is\nsufficient to make fraud proofs for the new limit.\n\n> But even such backdoor would not be harmful to the security of full\n> nodes because they will still fully verify the tx and witness merkle root.\n\nSure, but now miners can disable fraud proofs using a simple majority,\nwhich really, super sucks.\n\n> I\u2019d argue that any fraud proof related commitment: sum tree, delayed\n> UTXO commitment etc will require such a backdoor to disable. Maybe we\n> should just remove this from here and make this a new topic. We could\n> even do this as a softfork today.\n\nYes, lets skip it for now, I dont see much value in debating it for a HF\nnow."
            }
        ],
        "thread_summary": {
            "title": "Forcenet: an experimental network with a new header format",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Matt Corallo",
                "Johnson Lau"
            ],
            "messages_count": 5,
            "total_messages_chars_count": 46156
        }
    },
    {
        "title": "[bitcoin-dev] Changing the transaction version number to be varint",
        "thread_messages": [
            {
                "author": "Tom Zander",
                "date": "2017-01-20T14:02:22",
                "message_text_only": "Hi all,\n\nIn the transaction today we have a version field which is always 4 bytes.\nThe rest of the integer encoding in a transaction is variable-size because \nit saves on bytes.\n\nSpecifically, in practice this means that almost all of the transaction have \nbytes 2, 3 & 4 set to zero[1].\n\nThe question that I was pondering is that when we accept a new version of \ntransaction format (flextrans uses 4), what would the impact be of also \nchanging the way that the version number is actually serialized to be var \nint.\n\nThe benefit would be that each and every transaction looses 3 bytes. These \ncan be used differently in v1 transactions and are not needed at all to be \nthere for newer transaction formats.\nThe secondairy benefit is that, at least for FlexTrans[2], 100% of all the \nintegers in the transaction are following exactly the same encoding, the\nvar-int encoding.\n\nThere is currently no consensus rule that rejects transactions which lie \nabout their version, so obviously this rule should not and can not be \nintroduced retro-actively. It will be from a certain block-height.\n\nThe way to do this is that from a certain block-height the current \ntransaction format labels bytes 2, 3 & 4 to be unused.\n>From that same block height the interpretation of the first byte is as \nvarint.\nLast, we add the rule from that block-height that only transactions that do \nnot lie about their version number are valid. Which means version 1.\n\nDo people see any problems with this?\nThis could be done as a soft fork.\n\n1) It should be 100% because there is no transaction version defined that \nsets them to non-zero, but there is no consensus rule that rejects \ntransactions that lie about their version number.\n2) https://bitcoinclassic.com/devel/Flexible%20Transactions.html\n\n-- \nTom Zander\nBlog: https://zander.github.io\nVlog: https://vimeo.com/channels/tomscryptochannel"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-26T12:57:32",
                "message_text_only": "> On 20 Jan 2017, at 22:02, Tom Zander via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> The way to do this is that from a certain block-height the current \n> transaction format labels bytes 2, 3 & 4 to be unused.\n> From that same block height the interpretation of the first byte is as \n> varint.\n> Last, we add the rule from that block-height that only transactions that do \n> not lie about their version number are valid. Which means version 1.\n> \n> Do people see any problems with this?\n> This could be done as a soft fork.\n\nYes, because:\n\na) what you are talking is a hardfork, because existing nodes will not be able to deserialise the transaction. They will forever interpret the first 4 bytes as nVersion.\n\nb) it is not a \u201clie\u201d to use non-version 1 txs. It is permitted since v0.1. And version 2 txs is already used due to BIP68.\n\nc) if you are talking about changing the tx serialisation just for network transfer, it\u2019s just a p2p protocol upgrade, not softfork nor hardfork\n\n-------------------------\n\nThere are 3 ways to introduce new tx formats:\n\n1. through a softfork, and make the old clients blind to the new format. That\u2019s the segwit approach\n\n2. through a hardfork. Forget the old clients and require new clients to understand the new format. That\u2019s the FlexTran approach (in my understanding)\n\n3. p2p only, which won\u2019t affect consensus. No one could stop you if you try to copy a block by writing in your native language and pass to your peer.\n\nIn either way, one could introduce whatever new format one wants."
            }
        ],
        "thread_summary": {
            "title": "Changing the transaction version number to be varint",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Tom Zander",
                "Johnson Lau"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 3417
        }
    },
    {
        "title": "[bitcoin-dev] Anti-transaction replay in a hardfork",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2017-01-24T14:33:29",
                "message_text_only": "This is a pre-BIP. Just need some formatting to make it a formal BIP\n\nMotivation:\n\nIn general, hardforks are consensus rule changes that make currently invalid transactions / blocks valid. It requires a very high degree of consensus and all economic active users migrate to the new rules at the same time. If a significant amount of users refuse to follow, a permanent ledger split may happen, as demonstrated by Ethereum (\u201cDAO hardfork\"). In the design of DAO hardfork, a permanent split was not anticipated and no precaution has been taken to protect against transaction replay attack, which led to significant financial loss for some users.\n\nA replay attack is an attempt to replay a transaction of one network on another network. It is normally impossible, for example between Bitcoin and Litecoin, as different networks have completely different ledgers. The txid as SHA256 hash guarantees that replay across network is impossible. In a blockchain split, however, since both forks share the same historical ledger, replay attack would be possible, unless some precautions are taken.\n\nUnfortunately, fixing problems in bitcoin is like repairing a flying plane. Preventing replay attack is constrained by the requirement of backward compatibility. This proposal has the following objectives:\n\nA. For users on both existing and new fork, anti-replay is an option, not mandatory.\n\nB. For transactions created before this proposal is made, they are not protected from anti-replay. The new fork has to accept these transactions, as there is no guarantee that the existing fork would survive nor maintain any value. People made time-locked transactions in anticipation that they would be accepted later. In order to maximise the value of such transactions, the only way is to make them accepted by any potential hardforks.\n\nC. It doesn\u2019t require any consensus changes in the existing network to avoid unnecessary debate.\n\nD. As a beneficial side effect, the O(n^2) signature checking bug could be fixed for non-segregated witness inputs, optionally.\n\nDefinitions:\n\n\u201cNetwork characteristic byte\u201d is the most significant byte of the nVersion field of a transaction. It is interpreted as a bit vector, and denotes up to 8 networks sharing a common history.\n\n\u201cMasked version\u201d is the transaction nVersion with the network characteristic byte masked.\n\n\u201cExisting network\u201d is the Bitcoin network with existing rules, before a hardfork. \u201cNew network\u201d is the Bitcoin network with hardfork rules. (In the case of DAO hardfork, Ethereum Classic is the existing network, and the now called Ethereum is the new network)\n\n\u201cExisting network characteristic bit\u201d is the lowest bit of network characteristic byte\n\n\u201cNew network characteristic bit\u201d is the second lowest bit of network characteristic byte\n\nRules in new network:\n\n1. If the network characteristic byte is non-zero, and the new network characteristic bit is not set, this transaction is invalid in the new network. (softfork)\n\n2. If the network characteristic byte is zero, go to 4\n\n3. If the network characteristic byte is non-zero, and the new network characteristic bit is set, go to 4, regardless of the status of the other bits.\n\n4. If the masked version is 2 or below, the new network must verify the transaction with the existing script rules. (no change)\n\n5. If the masked version is 3 or above, the new network must verify the signatures with a new SignatureHash algorithm (hardfork). Segwit and non-segwit txs will use the same algorithm. It is same as BIP143, except that 0x2000000 is added to the nHashType before the hash is calculated.\n\nRules in the existing network:\n\n6. No consensus rule changes is made in the existing network.\n\n7. If the network characteristic byte is non-zero, and the existing network characteristic bit is not set, this transaction is not relayed nor mined by default (no change)\n\n8. If the network characteristic byte is zero, no change\n\n9. If the network characteristic byte is non-zero, and the existing network characteristic bit is set, the masked version is used to determine whether a transaction should be mined or relayed (policy change)\n\n10. Wallet may provide an option for setting the existing network characteristic bit.\n\n\nRationales (by rule number):\n\n1. This makes sure transactions with only existing network characteristic bit set is invalid in the new network (opt-in anti-replay for existing network transactions on the new network, objective A)\n\n2+4. This makes sure time-locked transactions made before this proposals are valid in the new network (objective B)\n\n2+5. This makes sure transactions made specifically for the new network are invalid in the existing network (anti-replay for new network transactions on the old network); also fixing the O(n^2) bug (objectives A and D)\n\n3. This is to prepare for the next hardfork from the new network (objective A)\n\n6, 7, 8. These minimise the change to the existing network (objective C)\n\n9, 10. These are not strictly needed until a hardfork is really anticipated. Without a significant portion of the network and miners implement this policy, however, no one should create such transactions. (objective A)\n\n\nLimitations:\n\n* It is not possible to protect transactions made before the proposal. To avoid a replay of such transactions, users should first spend at least a relevant UTXO on the new network so the replay transaction would be invalidated.\n\n* It is up to the designer of a hardfork to decide whether this proposal is respected. As the DAO hardfork has shown how harmful replay attack could be, all hardfork proposals (except trivial and totally uncontroversial ones) should take this into account\n\n* The size of network characteristic byte is limited to 8 bits. However, if we are sure that some of the networks are completely abandoned, the bits might be reused.\n\n\nReference implementation:\n\nA demo is available in my forcenet2 branch: https://github.com/jl2012/bitcoin/commit/7c2593946c4f3e210683110782d82f55473c682a <https://github.com/jl2012/bitcoin/commit/7c2593946c4f3e210683110782d82f55473c682a>\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170124/a3516bfa/attachment.html>"
            },
            {
                "author": "Tom Harding",
                "date": "2017-01-24T18:52:27",
                "message_text_only": "On 1/24/2017 6:33 AM, Johnson Lau via bitcoin-dev wrote:\n> 9. If the network characteristic byte is non-zero, and the existing\n> network characteristic bit is set, the masked version is used to\n> determine whether a transaction should be mined or relayed (policy change)\n\nJohnson,\n\nYour proposal supports 8 opt-out bits compatible with may earlier\ndescription:\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-July/012917.html.\n\nIf the existing network really wants to play along, it should execute a\nsoft fork as soon as possible to support its own hard-fork opt-out bit\n(\"network characteristic bit\").  It is totally inadequate for a new\nnetwork to rely on non-standardness in the existing network to prevent\nreplay there.  Instead, in the absence of a supported opt-out bit in the\nexisting network, a responsible new network would allow something that\nis invalid in the existing network, for transactions where replay to the\nexisting network is undesirable.\n\nIt is an overreach for your BIP to suggest specific changes to be\nincluded in the new network, such as the specific O(n^2) fix you\nsuggest.  This is a matter for the new network itself."
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-25T04:03:40",
                "message_text_only": "Yes, it\u2019s similar. I\u2019ll quote your design if/when I formalise my BIP. But it seems they are not the same: yours is opt-out, while mine is opt-in.\n\nHowever, my proposal in nowhere depends on standardness for the protection. It depends on the new network enforcing a new SignatureHash for txs with an nVersion not used in the existing network. This itself is a hardfork and the existing network would never accept such txs.\n\nThis is to avoid requiring any consensus changes to the existing network, as there is no guarantee that such softfork would be accepted by the existing network. If the new network wants to protect their users, it\u2019d be trivial for them to include a SignatureHash hardfork like this, along with other other hardfork changes. Further hardforks will only require changing the network characteristic bit, but not the SignatureHash.\n\nIf the hardfork designers don\u2019t like the fix of BIP143, there are many other options. The simplest one would be a trivial change to Satoshi\u2019s SignatureHash, such as adding an extra value at the end of the algorithm. I just don\u2019t see any technical reasons not to fix the O(n^2) problem altogether, if it is trivial (but not that trivial if the hardfork is not based on segwit)\n\n\n> On 25 Jan 2017, at 02:52, Tom Harding <tomh at thinlink.com> wrote:\n> \n> On 1/24/2017 6:33 AM, Johnson Lau via bitcoin-dev wrote:\n>> 9. If the network characteristic byte is non-zero, and the existing\n>> network characteristic bit is set, the masked version is used to\n>> determine whether a transaction should be mined or relayed (policy change)\n> \n> Johnson,\n> \n> Your proposal supports 8 opt-out bits compatible with may earlier\n> description:\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2016-July/012917.html.\n> \n> If the existing network really wants to play along, it should execute a\n> soft fork as soon as possible to support its own hard-fork opt-out bit\n> (\"network characteristic bit\").  It is totally inadequate for a new\n> network to rely on non-standardness in the existing network to prevent\n> replay there.  Instead, in the absence of a supported opt-out bit in the\n> existing network, a responsible new network would allow something that\n> is invalid in the existing network, for transactions where replay to the\n> existing network is undesirable.\n> \n> It is an overreach for your BIP to suggest specific changes to be\n> included in the new network, such as the specific O(n^2) fix you\n> suggest.  This is a matter for the new network itself.\n> \n>"
            },
            {
                "author": "Tom Harding",
                "date": "2017-01-25T19:32:25",
                "message_text_only": "On 1/24/2017 8:03 PM, Johnson Lau wrote:\n> it seems they are not the same: yours is opt-out, while mine is opt-in.\n\nI missed this.  So in fact you propose a self-defeating requirement on \nthe new network, which would force unmodified yet otherwise compatible \nsystems to change to support the new network at all. This is unlikely to \nbe included in new network designs.\n\nI suggest that the opt-out bits proposal comes from a more realistic \nposition that would actually make sense for everyone."
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-27T20:47:22",
                "message_text_only": "> On 26 Jan 2017, at 03:32, Tom Harding <tomh at thinlink.com> wrote:\n> \n> On 1/24/2017 8:03 PM, Johnson Lau wrote:\n>> it seems they are not the same: yours is opt-out, while mine is opt-in.\n> \n> I missed this.  So in fact you propose a self-defeating requirement on the new network, which would force unmodified yet otherwise compatible systems to change to support the new network at all. This is unlikely to be included in new network designs.\n> \n> I suggest that the opt-out bits proposal comes from a more realistic position that would actually make sense for everyone.\n> \n\nI think there are some misunderstanding. You\u2019d better read my source code if my explanation is not clear.\n\nFrom my understanding our proposals are the same, just with a bitwise not (~) before the network characteristic byte. So you set a bit to opt-out a network, while I set a bit to opt-in a network (and opt-out any other)"
            },
            {
                "author": "Tom Harding",
                "date": "2017-01-27T22:11:02",
                "message_text_only": "Johnson,\n\nIt's actually clear from your original post - you treat \"all zeros\" in a \nspecial way - as the equivalent of all ones.  The semantics match the \nimpression I got originally.  Sorry we got sidetracked."
            },
            {
                "author": "Natanael",
                "date": "2017-01-25T01:22:44",
                "message_text_only": "Den 24 jan. 2017 15:33 skrev \"Johnson Lau via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\n\n\nB. For transactions created before this proposal is made, they are not\nprotected from anti-replay. The new fork has to accept these transactions,\nas there is no guarantee that the existing fork would survive nor maintain\nany value. People made time-locked transactions in anticipation that they\nwould be accepted later. In order to maximise the value of such\ntransactions, the only way is to make them accepted by any potential\nhardforks.\n\n\nThis can be fixed.\n\nMake old-format transactions valid *only when paired with a fork-only\nfollow-up transaction* which is spending at least one (or all) of the\noutputs of the old-format transaction.\n\n(Yes, I know this introduces new statefulness into the block validation\nlogic. Unfortunately it is necessary for maximal fork safety. It can be\ndisabled at a later time if ever deemed no longer necessary.)\n\nMeanwhile, the old network SHOULD soft-fork in an identical rule with a\nfollow-up transaction format incompatible with the fork.\n\nThis means that old transactions can not be replayed across forks/networks,\nbecause they're not valid when stand-alone. It also means that all wallet\nclients either needs to be updated OR paired with software that intercepts\ngenerated transactions, and automatically generates the correct follow-up\ntransaction for it (old network only).\n\nThe rules should be that old-format transactions can't reference new-format\ntransactions, even if only a softfork change differ between the formats.\nThis prevents an unnecessary amount of transactions pairs generated by old\nwallets. Thus they can spend old outputs, but not spend new ones.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170125/48d948aa/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-25T07:05:59",
                "message_text_only": "What you describe is not a fix of replay attack. By confirming the same tx in both network, the tx has been already replayed. Their child txs do not matter.\n\n> On 25 Jan 2017, at 09:22, Natanael <natanael.l at gmail.com> wrote:\n> \n> \n> \n> Den 24 jan. 2017 15:33 skrev \"Johnson Lau via bitcoin-dev\" <bitcoin-dev at lists.linuxfoundation.org <mailto:bitcoin-dev at lists.linuxfoundation.org>>:\n> \n> \n> B. For transactions created before this proposal is made, they are not protected from anti-replay. The new fork has to accept these transactions, as there is no guarantee that the existing fork would survive nor maintain any value. People made time-locked transactions in anticipation that they would be accepted later. In order to maximise the value of such transactions, the only way is to make them accepted by any potential hardforks.\n> \n> This can be fixed. \n> \n> Make old-format transactions valid *only when paired with a fork-only follow-up transaction* which is spending at least one (or all) of the outputs of the old-format transaction. \n> \n> (Yes, I know this introduces new statefulness into the block validation logic. Unfortunately it is necessary for maximal fork safety. It can be disabled at a later time if ever deemed no longer necessary.)\n> \n> Meanwhile, the old network SHOULD soft-fork in an identical rule with a follow-up transaction format incompatible with the fork. \n> \n> This means that old transactions can not be replayed across forks/networks, because they're not valid when stand-alone. It also means that all wallet clients either needs to be updated OR paired with software that intercepts generated transactions, and automatically generates the correct follow-up transaction for it (old network only). \n> \n> The rules should be that old-format transactions can't reference new-format transactions, even if only a softfork change differ between the formats. This prevents an unnecessary amount of transactions pairs generated by old wallets. Thus they can spend old outputs, but not spend new ones. \n> \n> \n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170125/7ef54e56/attachment.html>"
            },
            {
                "author": "Natanael",
                "date": "2017-01-25T07:15:14",
                "message_text_only": "Den 25 jan. 2017 08:06 skrev \"Johnson Lau\" <jl2012 at xbt.hk>:\n\nWhat you describe is not a fix of replay attack. By confirming the same tx\nin both network, the tx has been already replayed. Their child txs do not\nmatter.\n\n\nRead it again.\n\nThe validation algorithm would be extended so that the transaction can't be\nreplayed, because replaying it in the other network REQUIRES a child\ntransaction in the same block that is valid, a child transaction the is\nunique to the network. By doing this policy change simultaneously in both\nnetworks, old pre-signed transactions *can not be replayed by anybody but\nthe owner* of the coins (as he must spend them immediately in the child\ntransaction).\n\nIt means that as soon as spent, the UTXO sets immediately and irrevocably\ndiverges across the two networks. Which is the entire point, isn't it?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170125/14eae723/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-25T07:21:57",
                "message_text_only": "Assuming Alice is paying Bob with an old style time-locked tx. Under your proposal, after the hardfork, Bob is still able to confirm the time-locked tx on both networks. To fulfil your new rules he just needs to send the outputs to himself again (with different tx format). But as Bob gets all the money on both forks, it is already a successful replay\n\n\n> On 25 Jan 2017, at 15:15, Natanael <natanael.l at gmail.com> wrote:\n> \n> \n> Den 25 jan. 2017 08:06 skrev \"Johnson Lau\" <jl2012 at xbt.hk <mailto:jl2012 at xbt.hk>>:\n> What you describe is not a fix of replay attack. By confirming the same tx in both network, the tx has been already replayed. Their child txs do not matter.\n> \n> Read it again. \n> \n> The validation algorithm would be extended so that the transaction can't be replayed, because replaying it in the other network REQUIRES a child transaction in the same block that is valid, a child transaction the is unique to the network. By doing this policy change simultaneously in both networks, old pre-signed transactions *can not be replayed by anybody but the owner* of the coins (as he must spend them immediately in the child transaction). \n> \n> It means that as soon as spent, the UTXO sets immediately and irrevocably diverges across the two networks. Which is the entire point, isn't it? \n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170125/2ead65d2/attachment-0001.html>"
            },
            {
                "author": "Natanael",
                "date": "2017-01-25T07:29:13",
                "message_text_only": "Den 25 jan. 2017 08:22 skrev \"Johnson Lau\" <jl2012 at xbt.hk>:\n\nAssuming Alice is paying Bob with an old style time-locked tx. Under your\nproposal, after the hardfork, Bob is still able to confirm the time-locked\ntx on both networks. To fulfil your new rules he just needs to send the\noutputs to himself again (with different tx format). But as Bob gets all\nthe money on both forks, it is already a successful replay\n\n\nWhy would Alice be sitting on an old-style signed transaction with UTXO:s\nnone of which she controls (paying somebody else), with NO ability to\nsubstitute the transaction for one where she DOES control an output,\nleaving her unable to be the one spending the replay protecting child\ntransaction?\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170125/3a1d6361/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-25T07:42:13",
                "message_text_only": "> On 25 Jan 2017, at 15:29, Natanael <natanael.l at gmail.com> wrote:\n> \n> \n> Den 25 jan. 2017 08:22 skrev \"Johnson Lau\" <jl2012 at xbt.hk <mailto:jl2012 at xbt.hk>>:\n> Assuming Alice is paying Bob with an old style time-locked tx. Under your proposal, after the hardfork, Bob is still able to confirm the time-locked tx on both networks. To fulfil your new rules he just needs to send the outputs to himself again (with different tx format). But as Bob gets all the money on both forks, it is already a successful replay\n> \n> Why would Alice be sitting on an old-style signed transaction with UTXO:s none of which she controls (paying somebody else), with NO ability to substitute the transaction for one where she DOES control an output, leaving her unable to be the one spending the replay protecting child transaction? \n\nIf Alice still has full control, she is already protected by my proposal, which does not require any protecting child transaction.\n\nBut in many cases she may not have full control. Make it clearer, consider that\u2019s actually a 2-of-2 multisig of Alice and Bob, and the time locked tx is sending to Bob. If the time locked tx is unprotected in the first place, Bob will get all the money from both forks anyway, as there is no reason for him to renegotiate with Alice.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170125/e4e33b95/attachment.html>"
            },
            {
                "author": "Matt Corallo",
                "date": "2017-01-26T03:29:14",
                "message_text_only": "\"A. For users on both existing and new fork, anti-replay is an option,\nnot mandatory\"\n\nTo maximize fork divergence, it might make sense to require this. Any\nsensible proposal for a hard fork would include a change to the sighash\nanyway, so might as well make it required, no?\n\nMatt\n\nOn 01/24/17 14:33, Johnson Lau via bitcoin-dev wrote:\n> This is a pre-BIP. Just need some formatting to make it a formal BIP\n> \n> Motivation:\n> \n> In general, hardforks are consensus rule changes that make currently\n> invalid transactions / blocks valid. It requires a very high degree of\n> consensus and all economic active users migrate to the new rules at the\n> same time. If a significant amount of users refuse to follow, a\n> permanent ledger split may happen, as demonstrated by Ethereum (\u201cDAO\n> hardfork\"). In the design of DAO hardfork, a permanent split was not\n> anticipated and no precaution has been taken to protect against\n> transaction replay attack, which led to significant financial loss for\n> some users.\n> \n> A replay attack is an attempt to replay a transaction of one network on\n> another network. It is normally impossible, for example between Bitcoin\n> and Litecoin, as different networks have completely different ledgers.\n> The txid as SHA256 hash guarantees that replay across network is\n> impossible. In a blockchain split, however, since both forks share the\n> same historical ledger, replay attack would be possible, unless some\n> precautions are taken.\n> \n> Unfortunately, fixing problems in bitcoin is like repairing a flying\n> plane. Preventing replay attack is constrained by the requirement of\n> backward compatibility. This proposal has the following objectives:\n> \n> A. For users on both existing and new fork, anti-replay is an option,\n> not mandatory.\n> \n> B. For transactions created before this proposal is made, they are not\n> protected from anti-replay. The new fork has to accept these\n> transactions, as there is no guarantee that the existing fork would\n> survive nor maintain any value. People made time-locked transactions in\n> anticipation that they would be accepted later. In order to maximise the\n> value of such transactions, the only way is to make them accepted by any\n> potential hardforks.\n> \n> C. It doesn\u2019t require any consensus changes in the existing network to\n> avoid unnecessary debate.\n> \n> D. As a beneficial side effect, the O(n^2) signature checking bug could\n> be fixed for non-segregated witness inputs, optionally.\n> \n> Definitions:\n> \n> \u201cNetwork characteristic byte\u201d is the most significant byte of the\n> nVersion field of a transaction. It is interpreted as a bit vector, and\n> denotes up to 8 networks sharing a common history.\n> \n> \u201cMasked version\u201d is the transaction nVersion with the network\n> characteristic byte masked.\n> \n> \u201cExisting network\u201d is the Bitcoin network with existing rules, before a\n> hardfork. \u201cNew network\u201d is the Bitcoin network with hardfork rules. (In\n> the case of DAO hardfork, Ethereum Classic is the existing network, and\n> the now called Ethereum is the new network)\n> \n> \u201cExisting network characteristic bit\u201d is the lowest bit of network\n> characteristic byte\n> \n> \u201cNew network characteristic bit\u201d is the second lowest bit of network\n> characteristic byte\n> \n> Rules in new network:\n> \n> 1. If the network characteristic byte is non-zero, and the new network\n> characteristic bit is not set, this transaction is invalid in the new\n> network. (softfork)\n> \n> 2. If the network characteristic byte is zero, go to 4\n> \n> 3. If the network characteristic byte is non-zero, and the new network\n> characteristic bit is set, go to 4, regardless of the status of the\n> other bits.\n> \n> 4. If the masked version is 2 or below, the new network must verify the\n> transaction with the existing script rules. (no change)\n> \n> 5. If the masked version is 3 or above, the new network must verify the\n> signatures with a new SignatureHash algorithm (hardfork). Segwit and\n> non-segwit txs will use the same algorithm. It is same as BIP143, except\n> that 0x2000000 is added to the nHashType before the hash is calculated.\n> \n> Rules in the existing network:\n> \n> 6. No consensus rule changes is made in the existing network.\n> \n> 7. If the network characteristic byte is non-zero, and the existing\n> network characteristic bit is not set, this transaction is not relayed\n> nor mined by default (no change)\n> \n> 8. If the network characteristic byte is zero, no change\n> \n> 9. If the network characteristic byte is non-zero, and the existing\n> network characteristic bit is set, the masked version is used to\n> determine whether a transaction should be mined or relayed (policy change)\n> \n> 10. Wallet may provide an option for setting the existing network\n> characteristic bit.\n> \n> \n> Rationales (by rule number):\n> \n> 1. This makes sure transactions with only existing network\n> characteristic bit set is invalid in the new network (opt-in anti-replay\n> for existing network transactions on the new network, objective A)\n> \n> 2+4. This makes sure time-locked transactions made before this proposals\n> are valid in the new network (objective B)\n> \n> 2+5. This makes sure transactions made specifically for the new network\n> are invalid in the existing network (anti-replay for new network\n> transactions on the old network); also fixing the O(n^2) bug (objectives\n> A and D)\n> \n> 3. This is to prepare for the next hardfork from the new network\n> (objective A)\n> \n> 6, 7, 8. These minimise the change to the existing network (objective C)\n> \n> 9, 10. These are not strictly needed until a hardfork is really\n> anticipated. Without a significant portion of the network and miners\n> implement this policy, however, no one should create such transactions.\n> (objective A)\n> \n> \n> Limitations:\n> \n> * It is not possible to protect transactions made before the proposal.\n> To avoid a replay of such transactions, users should first spend at\n> least a relevant UTXO on the new network so the replay transaction would\n> be invalidated.\n> \n> * It is up to the designer of a hardfork to decide whether this proposal\n> is respected. As the DAO hardfork has shown how harmful replay attack\n> could be, all hardfork proposals (except trivial and totally\n> uncontroversial ones) should take this into account\n> \n> * The size of network characteristic byte is limited to 8 bits. However,\n> if we are sure that some of the networks are completely abandoned, the\n> bits might be reused.\n> \n> \n> Reference implementation:\n> \n> A demo is available in my forcenet2\n> branch: https://github.com/jl2012/bitcoin/commit/7c2593946c4f3e210683110782d82f55473c682a\n> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html\n> \n> \n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-26T07:03:23",
                "message_text_only": "I don't think the solution should be to \"fix the replay attack\", but\nrather to \"force the replay effect\". The fact that transactions can be\nrelayed should be seen as a good thing, and not something that should\nbe fixed, or even called an \"attack\".\n\nThe solution should be to create a \"bridge\" that replays all\ntransactions from one network over to the other, and vice-versa. A\nfork should be transparent to the end-user. Forcing the user to choose\nwhich network to use is bad, because 99% of people that use bitcoin\ndon't care about developer drama, and will only be confused by the\nchoice. When a user moves coins mined before the fork date, both\nblockchains should record that transaction. Also a rule should be\nintroduced that prevents users \"tainting\" their prefork-mined coins\nwith coins mined after the fork. All pre-fork mined coins should\n\"belong\" to the network with hashpower majority. No other networks\nshould be able to claim pre-forked coins as being part of their\nissuance (and therefore part of market cap). Market cap may be\nbullshit, but it is used a lot in the cryptosphere to compare coins to\neach other.\n\nThe advantage of pre-fork coins being recorded on both forks is that\nif one fork goes extinct, no one loses any money. This setup\nencourages the minority chain to die,and unity returned. If pre-fork\ncoins change hands on either fork (and not on the other), then holders\nhave an incentive to not let their chain die, and the networks will be\nirreversibly split forever. The goal should be unity not permanent\ndivision.\n\nOn 1/25/17, Matt Corallo via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \"A. For users on both existing and new fork, anti-replay is an option,\n> not mandatory\"\n>\n> To maximize fork divergence, it might make sense to require this. Any\n> sensible proposal for a hard fork would include a change to the sighash\n> anyway, so might as well make it required, no?\n>\n> Matt\n>\n> On 01/24/17 14:33, Johnson Lau via bitcoin-dev wrote:\n>> This is a pre-BIP. Just need some formatting to make it a formal BIP\n>>\n>> Motivation:\n>>\n>> In general, hardforks are consensus rule changes that make currently\n>> invalid transactions / blocks valid. It requires a very high degree of\n>> consensus and all economic active users migrate to the new rules at the\n>> same time. If a significant amount of users refuse to follow, a\n>> permanent ledger split may happen, as demonstrated by Ethereum (\u201cDAO\n>> hardfork\"). In the design of DAO hardfork, a permanent split was not\n>> anticipated and no precaution has been taken to protect against\n>> transaction replay attack, which led to significant financial loss for\n>> some users.\n>>\n>> A replay attack is an attempt to replay a transaction of one network on\n>> another network. It is normally impossible, for example between Bitcoin\n>> and Litecoin, as different networks have completely different ledgers.\n>> The txid as SHA256 hash guarantees that replay across network is\n>> impossible. In a blockchain split, however, since both forks share the\n>> same historical ledger, replay attack would be possible, unless some\n>> precautions are taken.\n>>\n>> Unfortunately, fixing problems in bitcoin is like repairing a flying\n>> plane. Preventing replay attack is constrained by the requirement of\n>> backward compatibility. This proposal has the following objectives:\n>>\n>> A. For users on both existing and new fork, anti-replay is an option,\n>> not mandatory.\n>>\n>> B. For transactions created before this proposal is made, they are not\n>> protected from anti-replay. The new fork has to accept these\n>> transactions, as there is no guarantee that the existing fork would\n>> survive nor maintain any value. People made time-locked transactions in\n>> anticipation that they would be accepted later. In order to maximise the\n>> value of such transactions, the only way is to make them accepted by any\n>> potential hardforks.\n>>\n>> C. It doesn\u2019t require any consensus changes in the existing network to\n>> avoid unnecessary debate.\n>>\n>> D. As a beneficial side effect, the O(n^2) signature checking bug could\n>> be fixed for non-segregated witness inputs, optionally.\n>>\n>> Definitions:\n>>\n>> \u201cNetwork characteristic byte\u201d is the most significant byte of the\n>> nVersion field of a transaction. It is interpreted as a bit vector, and\n>> denotes up to 8 networks sharing a common history.\n>>\n>> \u201cMasked version\u201d is the transaction nVersion with the network\n>> characteristic byte masked.\n>>\n>> \u201cExisting network\u201d is the Bitcoin network with existing rules, before a\n>> hardfork. \u201cNew network\u201d is the Bitcoin network with hardfork rules. (In\n>> the case of DAO hardfork, Ethereum Classic is the existing network, and\n>> the now called Ethereum is the new network)\n>>\n>> \u201cExisting network characteristic bit\u201d is the lowest bit of network\n>> characteristic byte\n>>\n>> \u201cNew network characteristic bit\u201d is the second lowest bit of network\n>> characteristic byte\n>>\n>> Rules in new network:\n>>\n>> 1. If the network characteristic byte is non-zero, and the new network\n>> characteristic bit is not set, this transaction is invalid in the new\n>> network. (softfork)\n>>\n>> 2. If the network characteristic byte is zero, go to 4\n>>\n>> 3. If the network characteristic byte is non-zero, and the new network\n>> characteristic bit is set, go to 4, regardless of the status of the\n>> other bits.\n>>\n>> 4. If the masked version is 2 or below, the new network must verify the\n>> transaction with the existing script rules. (no change)\n>>\n>> 5. If the masked version is 3 or above, the new network must verify the\n>> signatures with a new SignatureHash algorithm (hardfork). Segwit and\n>> non-segwit txs will use the same algorithm. It is same as BIP143, except\n>> that 0x2000000 is added to the nHashType before the hash is calculated.\n>>\n>> Rules in the existing network:\n>>\n>> 6. No consensus rule changes is made in the existing network.\n>>\n>> 7. If the network characteristic byte is non-zero, and the existing\n>> network characteristic bit is not set, this transaction is not relayed\n>> nor mined by default (no change)\n>>\n>> 8. If the network characteristic byte is zero, no change\n>>\n>> 9. If the network characteristic byte is non-zero, and the existing\n>> network characteristic bit is set, the masked version is used to\n>> determine whether a transaction should be mined or relayed (policy\n>> change)\n>>\n>> 10. Wallet may provide an option for setting the existing network\n>> characteristic bit.\n>>\n>>\n>> Rationales (by rule number):\n>>\n>> 1. This makes sure transactions with only existing network\n>> characteristic bit set is invalid in the new network (opt-in anti-replay\n>> for existing network transactions on the new network, objective A)\n>>\n>> 2+4. This makes sure time-locked transactions made before this proposals\n>> are valid in the new network (objective B)\n>>\n>> 2+5. This makes sure transactions made specifically for the new network\n>> are invalid in the existing network (anti-replay for new network\n>> transactions on the old network); also fixing the O(n^2) bug (objectives\n>> A and D)\n>>\n>> 3. This is to prepare for the next hardfork from the new network\n>> (objective A)\n>>\n>> 6, 7, 8. These minimise the change to the existing network (objective C)\n>>\n>> 9, 10. These are not strictly needed until a hardfork is really\n>> anticipated. Without a significant portion of the network and miners\n>> implement this policy, however, no one should create such transactions.\n>> (objective A)\n>>\n>>\n>> Limitations:\n>>\n>> * It is not possible to protect transactions made before the proposal.\n>> To avoid a replay of such transactions, users should first spend at\n>> least a relevant UTXO on the new network so the replay transaction would\n>> be invalidated.\n>>\n>> * It is up to the designer of a hardfork to decide whether this proposal\n>> is respected. As the DAO hardfork has shown how harmful replay attack\n>> could be, all hardfork proposals (except trivial and totally\n>> uncontroversial ones) should take this into account\n>>\n>> * The size of network characteristic byte is limited to 8 bits. However,\n>> if we are sure that some of the networks are completely abandoned, the\n>> bits might be reused.\n>>\n>>\n>> Reference implementation:\n>>\n>> A demo is available in my forcenet2\n>> branch:\n>> https://github.com/jl2012/bitcoin/commit/7c2593946c4f3e210683110782d82f55473c682a\n>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html\n>>\n>>\n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>"
            },
            {
                "author": "Gavin Andresen",
                "date": "2017-01-26T17:21:37",
                "message_text_only": "On Wed, Jan 25, 2017 at 10:29 PM, Matt Corallo via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> To maximize fork divergence, it might make sense to require this. Any\n> sensible proposal for a hard fork would include a change to the sighash\n> anyway, so might as well make it required, no?\n>\n\nCompatibility with existing transaction-signing software and hardware\nshould be considered.\n\nI think any hard fork proposal should support a reasonable number of\nreasonable-size old-sighash transactions, to allow a smooth transaction of\nwallet software and hardware and to support anybody who might have a\nhardware wallet locked away in a safe deposit box for years.\n\n-- \n--\nGavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170126/ec58cbf4/attachment.html>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-26T07:14:52",
                "message_text_only": "I don\u2019t think this is how the blockchain consensus works. If there is a split, it becomes 2 incompatible ledgers. Bitcoin is not a trademark, and you don\u2019t need a permission to hardfork it. And what you suggest is also technically infeasible, as the miners on the new chain may not have a consensus only what\u2019s happening in the old chain.\n\n> On 26 Jan 2017, at 15:03, Chris Priest <cp368202 at ohiou.edu> wrote:\n> \n> I don't think the solution should be to \"fix the replay attack\", but\n> rather to \"force the replay effect\". The fact that transactions can be\n> relayed should be seen as a good thing, and not something that should\n> be fixed, or even called an \"attack\".\n> \n> The solution should be to create a \"bridge\" that replays all\n> transactions from one network over to the other, and vice-versa. A\n> fork should be transparent to the end-user. Forcing the user to choose\n> which network to use is bad, because 99% of people that use bitcoin\n> don't care about developer drama, and will only be confused by the\n> choice. When a user moves coins mined before the fork date, both\n> blockchains should record that transaction. Also a rule should be\n> introduced that prevents users \"tainting\" their prefork-mined coins\n> with coins mined after the fork. All pre-fork mined coins should\n> \"belong\" to the network with hashpower majority. No other networks\n> should be able to claim pre-forked coins as being part of their\n> issuance (and therefore part of market cap). Market cap may be\n> bullshit, but it is used a lot in the cryptosphere to compare coins to\n> each other.\n> \n> The advantage of pre-fork coins being recorded on both forks is that\n> if one fork goes extinct, no one loses any money. This setup\n> encourages the minority chain to die,and unity returned. If pre-fork\n> coins change hands on either fork (and not on the other), then holders\n> have an incentive to not let their chain die, and the networks will be\n> irreversibly split forever. The goal should be unity not permanent\n> division.\n> \n> On 1/25/17, Matt Corallo via bitcoin-dev\n> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>> \"A. For users on both existing and new fork, anti-replay is an option,\n>> not mandatory\"\n>> \n>> To maximize fork divergence, it might make sense to require this. Any\n>> sensible proposal for a hard fork would include a change to the sighash\n>> anyway, so might as well make it required, no?\n>> \n>> Matt\n>> \n>> On 01/24/17 14:33, Johnson Lau via bitcoin-dev wrote:\n>>> This is a pre-BIP. Just need some formatting to make it a formal BIP\n>>> \n>>> Motivation:\n>>> \n>>> In general, hardforks are consensus rule changes that make currently\n>>> invalid transactions / blocks valid. It requires a very high degree of\n>>> consensus and all economic active users migrate to the new rules at the\n>>> same time. If a significant amount of users refuse to follow, a\n>>> permanent ledger split may happen, as demonstrated by Ethereum (\u201cDAO\n>>> hardfork\"). In the design of DAO hardfork, a permanent split was not\n>>> anticipated and no precaution has been taken to protect against\n>>> transaction replay attack, which led to significant financial loss for\n>>> some users.\n>>> \n>>> A replay attack is an attempt to replay a transaction of one network on\n>>> another network. It is normally impossible, for example between Bitcoin\n>>> and Litecoin, as different networks have completely different ledgers.\n>>> The txid as SHA256 hash guarantees that replay across network is\n>>> impossible. In a blockchain split, however, since both forks share the\n>>> same historical ledger, replay attack would be possible, unless some\n>>> precautions are taken.\n>>> \n>>> Unfortunately, fixing problems in bitcoin is like repairing a flying\n>>> plane. Preventing replay attack is constrained by the requirement of\n>>> backward compatibility. This proposal has the following objectives:\n>>> \n>>> A. For users on both existing and new fork, anti-replay is an option,\n>>> not mandatory.\n>>> \n>>> B. For transactions created before this proposal is made, they are not\n>>> protected from anti-replay. The new fork has to accept these\n>>> transactions, as there is no guarantee that the existing fork would\n>>> survive nor maintain any value. People made time-locked transactions in\n>>> anticipation that they would be accepted later. In order to maximise the\n>>> value of such transactions, the only way is to make them accepted by any\n>>> potential hardforks.\n>>> \n>>> C. It doesn\u2019t require any consensus changes in the existing network to\n>>> avoid unnecessary debate.\n>>> \n>>> D. As a beneficial side effect, the O(n^2) signature checking bug could\n>>> be fixed for non-segregated witness inputs, optionally.\n>>> \n>>> Definitions:\n>>> \n>>> \u201cNetwork characteristic byte\u201d is the most significant byte of the\n>>> nVersion field of a transaction. It is interpreted as a bit vector, and\n>>> denotes up to 8 networks sharing a common history.\n>>> \n>>> \u201cMasked version\u201d is the transaction nVersion with the network\n>>> characteristic byte masked.\n>>> \n>>> \u201cExisting network\u201d is the Bitcoin network with existing rules, before a\n>>> hardfork. \u201cNew network\u201d is the Bitcoin network with hardfork rules. (In\n>>> the case of DAO hardfork, Ethereum Classic is the existing network, and\n>>> the now called Ethereum is the new network)\n>>> \n>>> \u201cExisting network characteristic bit\u201d is the lowest bit of network\n>>> characteristic byte\n>>> \n>>> \u201cNew network characteristic bit\u201d is the second lowest bit of network\n>>> characteristic byte\n>>> \n>>> Rules in new network:\n>>> \n>>> 1. If the network characteristic byte is non-zero, and the new network\n>>> characteristic bit is not set, this transaction is invalid in the new\n>>> network. (softfork)\n>>> \n>>> 2. If the network characteristic byte is zero, go to 4\n>>> \n>>> 3. If the network characteristic byte is non-zero, and the new network\n>>> characteristic bit is set, go to 4, regardless of the status of the\n>>> other bits.\n>>> \n>>> 4. If the masked version is 2 or below, the new network must verify the\n>>> transaction with the existing script rules. (no change)\n>>> \n>>> 5. If the masked version is 3 or above, the new network must verify the\n>>> signatures with a new SignatureHash algorithm (hardfork). Segwit and\n>>> non-segwit txs will use the same algorithm. It is same as BIP143, except\n>>> that 0x2000000 is added to the nHashType before the hash is calculated.\n>>> \n>>> Rules in the existing network:\n>>> \n>>> 6. No consensus rule changes is made in the existing network.\n>>> \n>>> 7. If the network characteristic byte is non-zero, and the existing\n>>> network characteristic bit is not set, this transaction is not relayed\n>>> nor mined by default (no change)\n>>> \n>>> 8. If the network characteristic byte is zero, no change\n>>> \n>>> 9. If the network characteristic byte is non-zero, and the existing\n>>> network characteristic bit is set, the masked version is used to\n>>> determine whether a transaction should be mined or relayed (policy\n>>> change)\n>>> \n>>> 10. Wallet may provide an option for setting the existing network\n>>> characteristic bit.\n>>> \n>>> \n>>> Rationales (by rule number):\n>>> \n>>> 1. This makes sure transactions with only existing network\n>>> characteristic bit set is invalid in the new network (opt-in anti-replay\n>>> for existing network transactions on the new network, objective A)\n>>> \n>>> 2+4. This makes sure time-locked transactions made before this proposals\n>>> are valid in the new network (objective B)\n>>> \n>>> 2+5. This makes sure transactions made specifically for the new network\n>>> are invalid in the existing network (anti-replay for new network\n>>> transactions on the old network); also fixing the O(n^2) bug (objectives\n>>> A and D)\n>>> \n>>> 3. This is to prepare for the next hardfork from the new network\n>>> (objective A)\n>>> \n>>> 6, 7, 8. These minimise the change to the existing network (objective C)\n>>> \n>>> 9, 10. These are not strictly needed until a hardfork is really\n>>> anticipated. Without a significant portion of the network and miners\n>>> implement this policy, however, no one should create such transactions.\n>>> (objective A)\n>>> \n>>> \n>>> Limitations:\n>>> \n>>> * It is not possible to protect transactions made before the proposal.\n>>> To avoid a replay of such transactions, users should first spend at\n>>> least a relevant UTXO on the new network so the replay transaction would\n>>> be invalidated.\n>>> \n>>> * It is up to the designer of a hardfork to decide whether this proposal\n>>> is respected. As the DAO hardfork has shown how harmful replay attack\n>>> could be, all hardfork proposals (except trivial and totally\n>>> uncontroversial ones) should take this into account\n>>> \n>>> * The size of network characteristic byte is limited to 8 bits. However,\n>>> if we are sure that some of the networks are completely abandoned, the\n>>> bits might be reused.\n>>> \n>>> \n>>> Reference implementation:\n>>> \n>>> A demo is available in my forcenet2\n>>> branch:\n>>> https://github.com/jl2012/bitcoin/commit/7c2593946c4f3e210683110782d82f55473c682a\n>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html\n>>> \n>>> \n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>> \n>> _______________________________________________\n>> bitcoin-dev mailing list\n>> bitcoin-dev at lists.linuxfoundation.org\n>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>"
            },
            {
                "author": "Chris Priest",
                "date": "2017-01-26T08:59:27",
                "message_text_only": "> If there is a split, it becomes 2 incompatible ledgers.\n\nNot necessarily. When the BIP50 hard fork happened, it didn't create\ntwo incompatible ledgers. It *could* have, but it didn't. If every\nsingle transaction mined during that time has been \"double spent\" on\nthe other chain, then it would have created a very bad situation. When\none side of the fork gets abandoned, actual users would have lost\nmoney. Since only one person was able to perform this double spend,\nonly the miners and that one double spender lost money when the one\nside was abandoned. If there had been a significant number of users\nwho had value only on the chain that was eventually abandoned, that\nchain would have incentive to not be abandoned and that *would* have\nresulted in a permanent incompatible split. It was essentially the\nreplay *effect* (not \"attack\") that allowed bitcoin to survive that\nhard fork. BIP50 was written before the term \"replay attack\" or\n\"replay effect\" has been coined, so it doesn't say much about how\ntransactions replayed...\n\nOn 1/25/17, Johnson Lau <jl2012 at xbt.hk> wrote:\n> I don\u2019t think this is how the blockchain consensus works. If there is a\n> split, it becomes 2 incompatible ledgers. Bitcoin is not a trademark, and\n> you don\u2019t need a permission to hardfork it. And what you suggest is also\n> technically infeasible, as the miners on the new chain may not have a\n> consensus only what\u2019s happening in the old chain.\n>\n>> On 26 Jan 2017, at 15:03, Chris Priest <cp368202 at ohiou.edu> wrote:\n>>\n>> I don't think the solution should be to \"fix the replay attack\", but\n>> rather to \"force the replay effect\". The fact that transactions can be\n>> relayed should be seen as a good thing, and not something that should\n>> be fixed, or even called an \"attack\".\n>>\n>> The solution should be to create a \"bridge\" that replays all\n>> transactions from one network over to the other, and vice-versa. A\n>> fork should be transparent to the end-user. Forcing the user to choose\n>> which network to use is bad, because 99% of people that use bitcoin\n>> don't care about developer drama, and will only be confused by the\n>> choice. When a user moves coins mined before the fork date, both\n>> blockchains should record that transaction. Also a rule should be\n>> introduced that prevents users \"tainting\" their prefork-mined coins\n>> with coins mined after the fork. All pre-fork mined coins should\n>> \"belong\" to the network with hashpower majority. No other networks\n>> should be able to claim pre-forked coins as being part of their\n>> issuance (and therefore part of market cap). Market cap may be\n>> bullshit, but it is used a lot in the cryptosphere to compare coins to\n>> each other.\n>>\n>> The advantage of pre-fork coins being recorded on both forks is that\n>> if one fork goes extinct, no one loses any money. This setup\n>> encourages the minority chain to die,and unity returned. If pre-fork\n>> coins change hands on either fork (and not on the other), then holders\n>> have an incentive to not let their chain die, and the networks will be\n>> irreversibly split forever. The goal should be unity not permanent\n>> division.\n>>\n>> On 1/25/17, Matt Corallo via bitcoin-dev\n>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>> \"A. For users on both existing and new fork, anti-replay is an option,\n>>> not mandatory\"\n>>>\n>>> To maximize fork divergence, it might make sense to require this. Any\n>>> sensible proposal for a hard fork would include a change to the sighash\n>>> anyway, so might as well make it required, no?\n>>>\n>>> Matt\n>>>\n>>> On 01/24/17 14:33, Johnson Lau via bitcoin-dev wrote:\n>>>> This is a pre-BIP. Just need some formatting to make it a formal BIP\n>>>>\n>>>> Motivation:\n>>>>\n>>>> In general, hardforks are consensus rule changes that make currently\n>>>> invalid transactions / blocks valid. It requires a very high degree of\n>>>> consensus and all economic active users migrate to the new rules at the\n>>>> same time. If a significant amount of users refuse to follow, a\n>>>> permanent ledger split may happen, as demonstrated by Ethereum (\u201cDAO\n>>>> hardfork\"). In the design of DAO hardfork, a permanent split was not\n>>>> anticipated and no precaution has been taken to protect against\n>>>> transaction replay attack, which led to significant financial loss for\n>>>> some users.\n>>>>\n>>>> A replay attack is an attempt to replay a transaction of one network on\n>>>> another network. It is normally impossible, for example between Bitcoin\n>>>> and Litecoin, as different networks have completely different ledgers.\n>>>> The txid as SHA256 hash guarantees that replay across network is\n>>>> impossible. In a blockchain split, however, since both forks share the\n>>>> same historical ledger, replay attack would be possible, unless some\n>>>> precautions are taken.\n>>>>\n>>>> Unfortunately, fixing problems in bitcoin is like repairing a flying\n>>>> plane. Preventing replay attack is constrained by the requirement of\n>>>> backward compatibility. This proposal has the following objectives:\n>>>>\n>>>> A. For users on both existing and new fork, anti-replay is an option,\n>>>> not mandatory.\n>>>>\n>>>> B. For transactions created before this proposal is made, they are not\n>>>> protected from anti-replay. The new fork has to accept these\n>>>> transactions, as there is no guarantee that the existing fork would\n>>>> survive nor maintain any value. People made time-locked transactions in\n>>>> anticipation that they would be accepted later. In order to maximise\n>>>> the\n>>>> value of such transactions, the only way is to make them accepted by\n>>>> any\n>>>> potential hardforks.\n>>>>\n>>>> C. It doesn\u2019t require any consensus changes in the existing network to\n>>>> avoid unnecessary debate.\n>>>>\n>>>> D. As a beneficial side effect, the O(n^2) signature checking bug could\n>>>> be fixed for non-segregated witness inputs, optionally.\n>>>>\n>>>> Definitions:\n>>>>\n>>>> \u201cNetwork characteristic byte\u201d is the most significant byte of the\n>>>> nVersion field of a transaction. It is interpreted as a bit vector, and\n>>>> denotes up to 8 networks sharing a common history.\n>>>>\n>>>> \u201cMasked version\u201d is the transaction nVersion with the network\n>>>> characteristic byte masked.\n>>>>\n>>>> \u201cExisting network\u201d is the Bitcoin network with existing rules, before a\n>>>> hardfork. \u201cNew network\u201d is the Bitcoin network with hardfork rules. (In\n>>>> the case of DAO hardfork, Ethereum Classic is the existing network, and\n>>>> the now called Ethereum is the new network)\n>>>>\n>>>> \u201cExisting network characteristic bit\u201d is the lowest bit of network\n>>>> characteristic byte\n>>>>\n>>>> \u201cNew network characteristic bit\u201d is the second lowest bit of network\n>>>> characteristic byte\n>>>>\n>>>> Rules in new network:\n>>>>\n>>>> 1. If the network characteristic byte is non-zero, and the new network\n>>>> characteristic bit is not set, this transaction is invalid in the new\n>>>> network. (softfork)\n>>>>\n>>>> 2. If the network characteristic byte is zero, go to 4\n>>>>\n>>>> 3. If the network characteristic byte is non-zero, and the new network\n>>>> characteristic bit is set, go to 4, regardless of the status of the\n>>>> other bits.\n>>>>\n>>>> 4. If the masked version is 2 or below, the new network must verify the\n>>>> transaction with the existing script rules. (no change)\n>>>>\n>>>> 5. If the masked version is 3 or above, the new network must verify the\n>>>> signatures with a new SignatureHash algorithm (hardfork). Segwit and\n>>>> non-segwit txs will use the same algorithm. It is same as BIP143,\n>>>> except\n>>>> that 0x2000000 is added to the nHashType before the hash is calculated.\n>>>>\n>>>> Rules in the existing network:\n>>>>\n>>>> 6. No consensus rule changes is made in the existing network.\n>>>>\n>>>> 7. If the network characteristic byte is non-zero, and the existing\n>>>> network characteristic bit is not set, this transaction is not relayed\n>>>> nor mined by default (no change)\n>>>>\n>>>> 8. If the network characteristic byte is zero, no change\n>>>>\n>>>> 9. If the network characteristic byte is non-zero, and the existing\n>>>> network characteristic bit is set, the masked version is used to\n>>>> determine whether a transaction should be mined or relayed (policy\n>>>> change)\n>>>>\n>>>> 10. Wallet may provide an option for setting the existing network\n>>>> characteristic bit.\n>>>>\n>>>>\n>>>> Rationales (by rule number):\n>>>>\n>>>> 1. This makes sure transactions with only existing network\n>>>> characteristic bit set is invalid in the new network (opt-in\n>>>> anti-replay\n>>>> for existing network transactions on the new network, objective A)\n>>>>\n>>>> 2+4. This makes sure time-locked transactions made before this\n>>>> proposals\n>>>> are valid in the new network (objective B)\n>>>>\n>>>> 2+5. This makes sure transactions made specifically for the new network\n>>>> are invalid in the existing network (anti-replay for new network\n>>>> transactions on the old network); also fixing the O(n^2) bug\n>>>> (objectives\n>>>> A and D)\n>>>>\n>>>> 3. This is to prepare for the next hardfork from the new network\n>>>> (objective A)\n>>>>\n>>>> 6, 7, 8. These minimise the change to the existing network (objective\n>>>> C)\n>>>>\n>>>> 9, 10. These are not strictly needed until a hardfork is really\n>>>> anticipated. Without a significant portion of the network and miners\n>>>> implement this policy, however, no one should create such transactions.\n>>>> (objective A)\n>>>>\n>>>>\n>>>> Limitations:\n>>>>\n>>>> * It is not possible to protect transactions made before the proposal.\n>>>> To avoid a replay of such transactions, users should first spend at\n>>>> least a relevant UTXO on the new network so the replay transaction\n>>>> would\n>>>> be invalidated.\n>>>>\n>>>> * It is up to the designer of a hardfork to decide whether this\n>>>> proposal\n>>>> is respected. As the DAO hardfork has shown how harmful replay attack\n>>>> could be, all hardfork proposals (except trivial and totally\n>>>> uncontroversial ones) should take this into account\n>>>>\n>>>> * The size of network characteristic byte is limited to 8 bits.\n>>>> However,\n>>>> if we are sure that some of the networks are completely abandoned, the\n>>>> bits might be reused.\n>>>>\n>>>>\n>>>> Reference implementation:\n>>>>\n>>>> A demo is available in my forcenet2\n>>>> branch:\n>>>> https://github.com/jl2012/bitcoin/commit/7c2593946c4f3e210683110782d82f55473c682a\n>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html\n>>>>\n>>>>\n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>\n>>> _______________________________________________\n>>> bitcoin-dev mailing list\n>>> bitcoin-dev at lists.linuxfoundation.org\n>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>\n>\n>\n>"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-26T09:20:54",
                "message_text_only": "BIP50 was an accident, and my proposal is just for a planned hardfork. You can\u2019t anti-replay if you don\u2019t even know a hardfork might happen. And I think your hypothesis (replay reduces the incentive of split) is not supported by the ETC/ETH split.\n\nAside the philosophical argument, your proposal is not technically feasible. In my understanding, you require the new chain to replay all the txs in the old chain. But this is not possible because there could be orphaning in the old chain, and different miners of the new chain may see a different history of the old chain. Not to mention that mining is a random process, and the hashing power is going up and down. Just by chance, 10 blocks might be generated in the old chain while no block is generated in the new chain. This is also unfair to the new chain miners, as they may not satisfied with the fees paid while they are forced to include those txs from the old chain (remember that people may just pay the old chain miners out of band, and pay no fee in the transaction)\n\nI don\u2019t think these technical issues are solvable when both forks are decentralised mining. If time machines, for example, are not technically feasible, there is not much point to talk about the benefits of time machines.\n\n> On 26 Jan 2017, at 16:59, Chris Priest <cp368202 at ohiou.edu> wrote:\n> \n>> If there is a split, it becomes 2 incompatible ledgers.\n> \n> Not necessarily. When the BIP50 hard fork happened, it didn't create\n> two incompatible ledgers. It *could* have, but it didn't. If every\n> single transaction mined during that time has been \"double spent\" on\n> the other chain, then it would have created a very bad situation. When\n> one side of the fork gets abandoned, actual users would have lost\n> money. Since only one person was able to perform this double spend,\n> only the miners and that one double spender lost money when the one\n> side was abandoned. If there had been a significant number of users\n> who had value only on the chain that was eventually abandoned, that\n> chain would have incentive to not be abandoned and that *would* have\n> resulted in a permanent incompatible split. It was essentially the\n> replay *effect* (not \"attack\") that allowed bitcoin to survive that\n> hard fork. BIP50 was written before the term \"replay attack\" or\n> \"replay effect\" has been coined, so it doesn't say much about how\n> transactions replayed...\n> \n> On 1/25/17, Johnson Lau <jl2012 at xbt.hk> wrote:\n>> I don\u2019t think this is how the blockchain consensus works. If there is a\n>> split, it becomes 2 incompatible ledgers. Bitcoin is not a trademark, and\n>> you don\u2019t need a permission to hardfork it. And what you suggest is also\n>> technically infeasible, as the miners on the new chain may not have a\n>> consensus only what\u2019s happening in the old chain.\n>> \n>>> On 26 Jan 2017, at 15:03, Chris Priest <cp368202 at ohiou.edu> wrote:\n>>> \n>>> I don't think the solution should be to \"fix the replay attack\", but\n>>> rather to \"force the replay effect\". The fact that transactions can be\n>>> relayed should be seen as a good thing, and not something that should\n>>> be fixed, or even called an \"attack\".\n>>> \n>>> The solution should be to create a \"bridge\" that replays all\n>>> transactions from one network over to the other, and vice-versa. A\n>>> fork should be transparent to the end-user. Forcing the user to choose\n>>> which network to use is bad, because 99% of people that use bitcoin\n>>> don't care about developer drama, and will only be confused by the\n>>> choice. When a user moves coins mined before the fork date, both\n>>> blockchains should record that transaction. Also a rule should be\n>>> introduced that prevents users \"tainting\" their prefork-mined coins\n>>> with coins mined after the fork. All pre-fork mined coins should\n>>> \"belong\" to the network with hashpower majority. No other networks\n>>> should be able to claim pre-forked coins as being part of their\n>>> issuance (and therefore part of market cap). Market cap may be\n>>> bullshit, but it is used a lot in the cryptosphere to compare coins to\n>>> each other.\n>>> \n>>> The advantage of pre-fork coins being recorded on both forks is that\n>>> if one fork goes extinct, no one loses any money. This setup\n>>> encourages the minority chain to die,and unity returned. If pre-fork\n>>> coins change hands on either fork (and not on the other), then holders\n>>> have an incentive to not let their chain die, and the networks will be\n>>> irreversibly split forever. The goal should be unity not permanent\n>>> division.\n>>> \n>>> On 1/25/17, Matt Corallo via bitcoin-dev\n>>> <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>>>> \"A. For users on both existing and new fork, anti-replay is an option,\n>>>> not mandatory\"\n>>>> \n>>>> To maximize fork divergence, it might make sense to require this. Any\n>>>> sensible proposal for a hard fork would include a change to the sighash\n>>>> anyway, so might as well make it required, no?\n>>>> \n>>>> Matt\n>>>> \n>>>> On 01/24/17 14:33, Johnson Lau via bitcoin-dev wrote:\n>>>>> This is a pre-BIP. Just need some formatting to make it a formal BIP\n>>>>> \n>>>>> Motivation:\n>>>>> \n>>>>> In general, hardforks are consensus rule changes that make currently\n>>>>> invalid transactions / blocks valid. It requires a very high degree of\n>>>>> consensus and all economic active users migrate to the new rules at the\n>>>>> same time. If a significant amount of users refuse to follow, a\n>>>>> permanent ledger split may happen, as demonstrated by Ethereum (\u201cDAO\n>>>>> hardfork\"). In the design of DAO hardfork, a permanent split was not\n>>>>> anticipated and no precaution has been taken to protect against\n>>>>> transaction replay attack, which led to significant financial loss for\n>>>>> some users.\n>>>>> \n>>>>> A replay attack is an attempt to replay a transaction of one network on\n>>>>> another network. It is normally impossible, for example between Bitcoin\n>>>>> and Litecoin, as different networks have completely different ledgers.\n>>>>> The txid as SHA256 hash guarantees that replay across network is\n>>>>> impossible. In a blockchain split, however, since both forks share the\n>>>>> same historical ledger, replay attack would be possible, unless some\n>>>>> precautions are taken.\n>>>>> \n>>>>> Unfortunately, fixing problems in bitcoin is like repairing a flying\n>>>>> plane. Preventing replay attack is constrained by the requirement of\n>>>>> backward compatibility. This proposal has the following objectives:\n>>>>> \n>>>>> A. For users on both existing and new fork, anti-replay is an option,\n>>>>> not mandatory.\n>>>>> \n>>>>> B. For transactions created before this proposal is made, they are not\n>>>>> protected from anti-replay. The new fork has to accept these\n>>>>> transactions, as there is no guarantee that the existing fork would\n>>>>> survive nor maintain any value. People made time-locked transactions in\n>>>>> anticipation that they would be accepted later. In order to maximise\n>>>>> the\n>>>>> value of such transactions, the only way is to make them accepted by\n>>>>> any\n>>>>> potential hardforks.\n>>>>> \n>>>>> C. It doesn\u2019t require any consensus changes in the existing network to\n>>>>> avoid unnecessary debate.\n>>>>> \n>>>>> D. As a beneficial side effect, the O(n^2) signature checking bug could\n>>>>> be fixed for non-segregated witness inputs, optionally.\n>>>>> \n>>>>> Definitions:\n>>>>> \n>>>>> \u201cNetwork characteristic byte\u201d is the most significant byte of the\n>>>>> nVersion field of a transaction. It is interpreted as a bit vector, and\n>>>>> denotes up to 8 networks sharing a common history.\n>>>>> \n>>>>> \u201cMasked version\u201d is the transaction nVersion with the network\n>>>>> characteristic byte masked.\n>>>>> \n>>>>> \u201cExisting network\u201d is the Bitcoin network with existing rules, before a\n>>>>> hardfork. \u201cNew network\u201d is the Bitcoin network with hardfork rules. (In\n>>>>> the case of DAO hardfork, Ethereum Classic is the existing network, and\n>>>>> the now called Ethereum is the new network)\n>>>>> \n>>>>> \u201cExisting network characteristic bit\u201d is the lowest bit of network\n>>>>> characteristic byte\n>>>>> \n>>>>> \u201cNew network characteristic bit\u201d is the second lowest bit of network\n>>>>> characteristic byte\n>>>>> \n>>>>> Rules in new network:\n>>>>> \n>>>>> 1. If the network characteristic byte is non-zero, and the new network\n>>>>> characteristic bit is not set, this transaction is invalid in the new\n>>>>> network. (softfork)\n>>>>> \n>>>>> 2. If the network characteristic byte is zero, go to 4\n>>>>> \n>>>>> 3. If the network characteristic byte is non-zero, and the new network\n>>>>> characteristic bit is set, go to 4, regardless of the status of the\n>>>>> other bits.\n>>>>> \n>>>>> 4. If the masked version is 2 or below, the new network must verify the\n>>>>> transaction with the existing script rules. (no change)\n>>>>> \n>>>>> 5. If the masked version is 3 or above, the new network must verify the\n>>>>> signatures with a new SignatureHash algorithm (hardfork). Segwit and\n>>>>> non-segwit txs will use the same algorithm. It is same as BIP143,\n>>>>> except\n>>>>> that 0x2000000 is added to the nHashType before the hash is calculated.\n>>>>> \n>>>>> Rules in the existing network:\n>>>>> \n>>>>> 6. No consensus rule changes is made in the existing network.\n>>>>> \n>>>>> 7. If the network characteristic byte is non-zero, and the existing\n>>>>> network characteristic bit is not set, this transaction is not relayed\n>>>>> nor mined by default (no change)\n>>>>> \n>>>>> 8. If the network characteristic byte is zero, no change\n>>>>> \n>>>>> 9. If the network characteristic byte is non-zero, and the existing\n>>>>> network characteristic bit is set, the masked version is used to\n>>>>> determine whether a transaction should be mined or relayed (policy\n>>>>> change)\n>>>>> \n>>>>> 10. Wallet may provide an option for setting the existing network\n>>>>> characteristic bit.\n>>>>> \n>>>>> \n>>>>> Rationales (by rule number):\n>>>>> \n>>>>> 1. This makes sure transactions with only existing network\n>>>>> characteristic bit set is invalid in the new network (opt-in\n>>>>> anti-replay\n>>>>> for existing network transactions on the new network, objective A)\n>>>>> \n>>>>> 2+4. This makes sure time-locked transactions made before this\n>>>>> proposals\n>>>>> are valid in the new network (objective B)\n>>>>> \n>>>>> 2+5. This makes sure transactions made specifically for the new network\n>>>>> are invalid in the existing network (anti-replay for new network\n>>>>> transactions on the old network); also fixing the O(n^2) bug\n>>>>> (objectives\n>>>>> A and D)\n>>>>> \n>>>>> 3. This is to prepare for the next hardfork from the new network\n>>>>> (objective A)\n>>>>> \n>>>>> 6, 7, 8. These minimise the change to the existing network (objective\n>>>>> C)\n>>>>> \n>>>>> 9, 10. These are not strictly needed until a hardfork is really\n>>>>> anticipated. Without a significant portion of the network and miners\n>>>>> implement this policy, however, no one should create such transactions.\n>>>>> (objective A)\n>>>>> \n>>>>> \n>>>>> Limitations:\n>>>>> \n>>>>> * It is not possible to protect transactions made before the proposal.\n>>>>> To avoid a replay of such transactions, users should first spend at\n>>>>> least a relevant UTXO on the new network so the replay transaction\n>>>>> would\n>>>>> be invalidated.\n>>>>> \n>>>>> * It is up to the designer of a hardfork to decide whether this\n>>>>> proposal\n>>>>> is respected. As the DAO hardfork has shown how harmful replay attack\n>>>>> could be, all hardfork proposals (except trivial and totally\n>>>>> uncontroversial ones) should take this into account\n>>>>> \n>>>>> * The size of network characteristic byte is limited to 8 bits.\n>>>>> However,\n>>>>> if we are sure that some of the networks are completely abandoned, the\n>>>>> bits might be reused.\n>>>>> \n>>>>> \n>>>>> Reference implementation:\n>>>>> \n>>>>> A demo is available in my forcenet2\n>>>>> branch:\n>>>>> https://github.com/jl2012/bitcoin/commit/7c2593946c4f3e210683110782d82f55473c682a\n>>>>> https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html\n>>>>> \n>>>>> \n>>>>> _______________________________________________\n>>>>> bitcoin-dev mailing list\n>>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>>> \n>>>> _______________________________________________\n>>>> bitcoin-dev mailing list\n>>>> bitcoin-dev at lists.linuxfoundation.org\n>>>> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>>>> \n>> \n>> \n>>"
            },
            {
                "author": "Edmund Edgar",
                "date": "2017-01-26T10:55:41",
                "message_text_only": "On 26 January 2017 at 18:20, Johnson Lau via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>You can\u2019t anti-replay if you don\u2019t even know a hardfork might happen. And I think your hypothesis (replay reduces the incentive of split) is not supported by the ETC/ETH split.\n\nI agree with the general point you're making, but you *could*\nanti-replay without knowing about the fork, at least from a few dozen\nblocks into it. For example you could allow transactions to specify a\nrecent block hash (or some of the bytes thereof) and declare that they\nwant to be invalid if that block isn't in the parent chain.\n\nThis would potentially have benefits beyond economic hard-fork\nsituations: As a general principle, if the network that you're\ntransacting with doesn't look like the one you think you're\ntransacting with, you're going to have a bad day.\n\n-- \n-- \nEdmund Edgar\nFounder, Social Minds Inc (KK)\nTwitter: @edmundedgar\nLinked In: edmundedgar\nSkype: edmundedgar\nhttp://www.socialminds.jp\n\nReality Keys\n@realitykeys\ned at realitykeys.com\nhttps://www.realitykeys.com"
            },
            {
                "author": "Tom Harding",
                "date": "2017-01-26T15:58:23",
                "message_text_only": "Even more to the point, new post- fork coins are fork-specific.  The \nlonger both forks persist, the more transactions become unavoidably \nfork-specific through the mixing in of these coins.  Any attempt to \nmaximize replay will become less effective with time.\n\nThe rationality of actors in this situation essentially defines the \nlimited solution that is possible.  Upgraded software can create \ntransactions guaranteed not to execute to one fork or the other, or that \nis not prevented from execution on either fork.  I see no downside to \nthis, and the advantage is that markets can be much less chaotic.  In \nfact exchanges will be much better off if they require that post-fork \ntrading, deposits and withdrawals are exclusively chain-specific, which \nwill also result in well determined prices for the two currencies.\n\nNone of this precludes the possibility of further forks on either side, \nand the difficulty consideration alone suggests a likely counter-fork by \n(part of) the existing network.\n\n\nOn 1/26/2017 1:20 AM, Johnson Lau via bitcoin-dev wrote:\n> Not to mention that mining is a random process, and the hashing power is going up and down."
            },
            {
                "author": "Matt Corallo",
                "date": "2017-01-26T17:41:55",
                "message_text_only": "Excuse me, yes, for previously-signed transactions this is required. We might consider some limits on UTXO-chain-from-before-the-fork-length and likely something like move towards only allowing one transaction per block from the old mode over time.\n\nI highly disagree that compatibility with existing transaction signing software should be considered (but for hardware which cannot be upgraded easily we do need to consider it). Wallets which can upgrade should, as much as possible, upgrade to a new form to maximize chain divergence and are going to end up having to upgrade to know a new header format anyway, so am extra few lines of code to change a transaction version should be trivial.\n\nOn January 26, 2017 12:21:37 PM EST, Gavin Andresen <gavinandresen at gmail.com> wrote:\n>On Wed, Jan 25, 2017 at 10:29 PM, Matt Corallo via bitcoin-dev <\n>bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n>> To maximize fork divergence, it might make sense to require this. Any\n>> sensible proposal for a hard fork would include a change to the\n>sighash\n>> anyway, so might as well make it required, no?\n>>\n>\n>Compatibility with existing transaction-signing software and hardware\n>should be considered.\n>\n>I think any hard fork proposal should support a reasonable number of\n>reasonable-size old-sighash transactions, to allow a smooth transaction\n>of\n>wallet software and hardware and to support anybody who might have a\n>hardware wallet locked away in a safe deposit box for years.\n>\n>-- \n>--\n>Gavin Andresen\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170126/d9ce67ac/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Anti-transaction replay in a hardfork",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Natanael",
                "Chris Priest",
                "Johnson Lau",
                "Edmund Edgar",
                "Matt Corallo",
                "Gavin Andresen",
                "Tom Harding"
            ],
            "messages_count": 21,
            "total_messages_chars_count": 74073
        }
    },
    {
        "title": "[bitcoin-dev] Separating mining from tx verification by enabling paying to valid POW header",
        "thread_messages": [
            {
                "author": "Rune K. Svendsen",
                "date": "2017-01-24T18:57:59",
                "message_text_only": "As mining works now, miners have to verify all Bitcoin transactions in the\nblocks they mine, because they would otherwise risk producing an invalid\nblock. This is problematic because many miners are Chinese, and thus have\npoor Internet connectivity, so it would be preferable to separate the task\nof creating valid proof-of-work from the task of collecting valid\ntransactions.\n\nThis could be made possible by adding an opcode that checks whether the\ntop-most stack item is a valid block header, we could call it\nOP_VALID_HEADER(VERIFY), thus allowing miners to be paid for a valid block\nheader through a regular Bitcoin transaction, rather than through the\ncoinbase transaction only. This allows a different group to simply act as\ncollectors of transactions, and create OP_VALID_HEADER-transactions that\npay to block headers with a merkle root that includes all the highest-fee\ntransactions.\n\nSo, these collectors would accumulate as many connections as possible\nwithin the Bitcoin P2P network, and collect all the highest fee\ntransactions they can find. Then construct a block which includes all these\ntransactions, and a coinbase tx that pays the block reward plus fees to the\ncollector.\n\nWith this block the collector would then create a Bitcoin transaction, with\na OP_VALID_HEADER-output that can be redeemed by supplying the block header\nin the script but with a modified nonce/timestamp such that the\nproof-of-work+timestamp is valid. Miners would then only have to look for\nthese Bitcoin transactions from the collectors, and mine on whichever\nheader pays them the most, without having to care about whether the block\nin question includes invalid transactions, because the miner is paid for\njust a valid proof-of-work hash. When the miner finds a solution, it\npublishes the transaction, the collector see this transaction, gets it\nvalid header, and publishes the block.\n\nA side bonus of this is that botnet miners can now participate on basically\nequal footing with traditional miners: they just listen to the P2P network\nfor the transaction from the collector who pays them the most, which will\ninclude as many transactions as possible to earn the most in fees, thus\nverifying transactions without having to do the work.\n\n\n\n\n      /Rune\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170124/61599ef3/attachment-0001.html>"
            }
        ],
        "thread_summary": {
            "title": "Separating mining from tx verification by enabling paying to valid POW header",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Rune K. Svendsen"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 2432
        }
    },
    {
        "title": "[bitcoin-dev] Extension block softfork proposal",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2017-01-26T09:39:43",
                "message_text_only": "This is a pre-BIP which allows extra block space through a soft-fork. It is completely transparent to existing wallets (both send and receive), but new wallets taking advantage of the extra block space will have a very different user experience.\n\nI\u2019m sure this is controversial but I think it\u2019s an interesting academic topic. If we\u2019d ever have any fully consensus enforced 2-way-peg side chain design, that\u2019d be something like this.\n\nObjectives:\n\n1. Provide more block space through a soft forks\n2. Completely transparent to existing wallets\n3. Not breaking any current security assumptions\n\n\nSpecification and Terminology:\n\nMain block / block: the current bitcoin block (with witness if BIP141 is activated)\n\nMain transaction / tx: txs in the current bitcoin network (with witness)\n\nMain UTXO / UTXO: the normal UTXO\n\nExtension transaction / xtx: transactions with a format same as the witness tx format described in BIP141, without scriptSig field, and the \u201cflag\u201d as 0x02. Only witness program are allowed for scriptPubKey of xtx\n\nExtension block / xblock: xblock is a collection of xtx. Each block may have 0 or 1 xblock when this softfork is activated.\n\nExtension UTXO / xUTXO: the UTXO set for of the extension block.\n\nBridging witness program: A new type of witness program is defined. The witness script version is OP_2. The program length could be 4 to 40. The first byte (\"direction flag\u201d[note 1]) must be 0x00 (indicating block->xblock) or 0x01 (indicating xblock->block). Like P2WPKH and P2WSH, the bridging program could be wrapped by P2SH. There are 2 ways to spend this program type on the main block:\n  1) Spend it like a usual witness program with a tx. For example, if the bridging program is OP_2 <0x000014{20 bytes}>, it could be spent like a version-0 20bytes programme, i.e. P2WPKH. Nothing special would happen in this case\n  2) Spend it like a usual witness program with a special xtx, the genesis xtx. In this case, the miner including this xtx will need to do more as described below.\n\nIntegrating UTXO: a special UTXO with a value >= the total value of all existing xUTXO and scriptPubKey is OP_1. (to make the spec easier to read, here we assume that now we have a zero value UTXO with its outpoint hardcoded as the initial integrating UTXO. In practice we may have the first miner making xblock to create the initial integrating UTXO)\n\nIntegrating transaction: if a block has an xblock, the second transaction in the block must be the integrating transaction. The inputs include the spent UTXO of all the genesis xtx in this xblock. If it is a bare witness program, the witness must be empty. If it is a P2SH witness program, the scriptSig must be the bridging witness program and the witness must be empty. The last input must be the original integrating UTXO, with empty witness and scriptSig. If no one is trying to send money back from the xblock to the main block, the only output is the updated integrating UTXO, which the value must be >= the total value of all xUTXO\n\n\u2014\u2014\u2014\u2014\nUp to now, I have described how we could send bitcoins from the main UTXO to the xUTXO. Simply speaking, people send money to a new form of witness programme. They have the flexibility to spend it in the main block or xblock. Nothing special would happen if they send to the main block. If they send to the xblock, the value of such UTXO will be collected by the integrating UTXO.\n\nAfter people sent money to xblock, they could trade inside the xblock just like in the main block. Since xblock is invisible to the pre-softfork users, we could have whatever size limit for the xblock, which is not a topic of this proposal.\n\nThe tricky part is sending from xblock to main block.\n\nReturning transaction: returning transaction is a special xtx, sending money to a bridging witness program, with a direction flag of 0x01. These bridging witness program won\u2019t be recorded in the xUTXO set. Instead, an output is added to the integrating tx, with the bridging witness program and corresponding value, called the \u201creturning UTXO\u201d. The returning UTXOs are not spendable until confirmed by 100 blocks. The updated integrating UTXO is the last output, and is not restricted by the 100-block requirement\n\nFees collection in xblock: Same as normal tx, people pay fee in xblock by making output value < input value. Since the value of the integrating UTXO is >= the total value of all existing xUTXO, if fees are paid in the xblock, that will reduce the value of the integrating UTXO, and miners are paid through the usual coinbase tx as fee.\n\nxblock commitment: 2 xblock merkle root, with and without witness, are placed exactly after the witness commitment in the coinbase tx.(maybe we could use the coinbase reserved witness value, details TBD). If there is no xblock commitment, xblock must be empty and integrating tx is not allowed.\n\n\u2014\u2014\u2014\u2014\nSame as any 2-way-peg proposal, sending money from the side chain to the main chain is always the most tricky part. Different from other side chain proposals like Rootstock, extension block is fully consensus enforced, and has the same security level as existing bitcoin transactions. To ensure this, an 100-block maturity is needed for the returning UTXO, as the TXID of the integrating transaction is *very* likely to change after a reorg, which will break the transaction chains coming from it. The 100-block maturity requirement bring us back to the usual assumption that txs become permanent after 100 confirmations.\n\nPlease note that this drastically changes the user experience, as no current users (expect miners) would expect such 100-block freezing. That\u2019s why I don\u2019t allow the returning UTXO to have an arbitrary scriptPubKey, as users of current wallet would never expect such freezing. Using a special output scriptPubKey guarantees that the recipient must understand the implications. Users of the new wallet should be warned that despite they may enjoy lower fees in the xblock, it may be difficult for them to send money to legacy wallets. This is a huge limitation.\n\nMaybe we could have some decentralised market (using simple hash-time-locked txs) allowing people to exchange value between block and xblock, bypassing the 100 block requirement. This is actually cheaper, because a full returning is a 2-step process, while p2p exchange is only 1-step.\n\n\u2014\u2014\u2014\u2014\n\nQuestions:\n\n1. Is it possible to simplify the design, without compromising security?\n2. Is it acceptable to do it without the 100-block maturity requirement, thus breaking some long-held assumptions? (This would vastly improve the usability, until a reorg happens)\n3. Even with maturity requirement, is 100-block an overkill? We never had a fork over maybe 20 blocks. Also, breaking of transaction chain due to reorg is already possible, as people may double spend during a reorg.\n\n[note 1] the direction flag is needed to make sure a recipient won\u2019t be paid with a returning transaction, unless explicitly requested. It might be combined with the serialised witness version to save one byte."
            },
            {
                "author": "Matt Corallo",
                "date": "2017-01-28T00:35:55",
                "message_text_only": "Hey Johnson,\n\nAs you know I've always been a rather large critic of this approach.\n\nFirst a bit of background. Pieter's excellent post on the security of\nsoft forks [1] covers pretty well why soft forks are preferable to hard\nforks by debunking much of the \"soft forks are less secure\" arguments.\nWhile those arguments apply readily to your proposal, what wasn't\ncovered are the \"soft forks are coercive\" arguments. Indeed, many of\nthose arguments are also bogus. After all, soft forks are not \"forks\"\nwithout buy-in from the economically relevant community running nodes\nwhich enforce the new rules (ie fork-by-miner-censorship isn't all that\nmuch of a fork at all, and has security properties which I would be\nhesitant to use for anything but the smallest of value).\n\nThat said, when we start talking about extension blocks, I believe we\nstart to rapidly enter this \"coerciveness\" territory. With segwit, we've\nseen pretty clearly that the community, much to its detriment, can be\neasily made unwilling to speak up for or against a fork, making\nconsensus an incredibly murky thing.\n\nLuckily, as noted in Pieter's original post, there isn't much harm in\nthe passive observer not making their voice heard and going along and\nenforcing SegWit. SegWit maintains UTXO compatibility and transactions\ncontinue to work as normal, only hiding information necessary to apply\nthe soft fork's rules from old nodes. This is not significantly\ndifferent from any other softfork, where declining to enforce its rules\nresults in you missing information (only in this case in the form of\nadditional validity rules instead of signatures themselves, which you\notherwise don't know what to do with). Even better, the bandwidth\nincreases for fully-validating nodes have been more than offset by other\ntechnology upgrades.\n\nMuch of this goes out the window with extension blocks. Instead of the\nextra data being reasonable to ignore if you choose to not enforce the\nsoft fork's rules, all of a sudden a majority (or at least significant\nchunk) of transactions on the network are happening in the data you've\nchosen to ignore. Instead of being able to reasonably walk back\ntransaction history to identify risk based on potential\ncensorship-enforced-transactions (ie transactions in a soft fork you're\nnot aware of, potentially that only miners are enforcing), all\ntransactions will look risky. Instead of being able to enforce\nfundamental network rules like the 21 million coin limit, you're left to\ntrust that what is happening on the extension block (which all miners\nare largely forced to mine due to the fee revenue opportunity cost).\nThis ultimately makes it a social cost, not an individual trust problem\n- instead of opting into a soft fork's security (or lack thereof) for\nyour own transaction, the entire network is forced to trust the\nextension block.\n\nFinally, this sets us up for some pretty terrible precedent. As we noted\nin a footnote of the original sidechains paper, the idea that miners\nwill start soft-forking in sidechains is a massive risk - it allows\nindividual large miners and individual economic users to force others to\nswitch to new consensus rules, with potentially little consensus or review.\n\n[1]\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2015-December/012014.html\n\nOn January 26, 2017 4:39:43 AM EST, Johnson Lau via bitcoin-dev\n<bitcoin-dev at lists.linuxfoundation.org> wrote:\n>This is a pre-BIP which allows extra block space through a soft-fork.\n>It is completely transparent to existing wallets (both send and\n>receive), but new wallets taking advantage of the extra block space\n>will have a very different user experience.\n>\n>I\u2019m sure this is controversial but I think it\u2019s an interesting academic\n>topic. If we\u2019d ever have any fully consensus enforced 2-way-peg side\n>chain design, that\u2019d be something like this.\n>\n>Objectives:\n>\n>1. Provide more block space through a soft forks\n>2. Completely transparent to existing wallets\n>3. Not breaking any current security assumptions\n>\n>\n>Specification and Terminology:\n>\n>Main block / block: the current bitcoin block (with witness if BIP141\n>is activated)\n>\n>Main transaction / tx: txs in the current bitcoin network (with\n>witness)\n>\n>Main UTXO / UTXO: the normal UTXO\n>\n>Extension transaction / xtx: transactions with a format same as the\n>witness tx format described in BIP141, without scriptSig field, and the\n>\u201cflag\u201d as 0x02. Only witness program are allowed for scriptPubKey of\n>xtx\n>\n>Extension block / xblock: xblock is a collection of xtx. Each block may\n>have 0 or 1 xblock when this softfork is activated.\n>\n>Extension UTXO / xUTXO: the UTXO set for of the extension block.\n>\n>Bridging witness program: A new type of witness program is defined. The\n>witness script version is OP_2. The program length could be 4 to 40.\n>The first byte (\"direction flag\u201d[note 1]) must be 0x00 (indicating\n>block->xblock) or 0x01 (indicating xblock->block). Like P2WPKH and\n>P2WSH, the bridging program could be wrapped by P2SH. There are 2 ways\n>to spend this program type on the main block:\n>1) Spend it like a usual witness program with a tx. For example, if the\n>bridging program is OP_2 <0x000014{20 bytes}>, it could be spent like a\n>version-0 20bytes programme, i.e. P2WPKH. Nothing special would happen\n>in this case\n>2) Spend it like a usual witness program with a special xtx, the\n>genesis xtx. In this case, the miner including this xtx will need to do\n>more as described below.\n>\n>Integrating UTXO: a special UTXO with a value >= the total value of all\n>existing xUTXO and scriptPubKey is OP_1. (to make the spec easier to\n>read, here we assume that now we have a zero value UTXO with its\n>outpoint hardcoded as the initial integrating UTXO. In practice we may\n>have the first miner making xblock to create the initial integrating\n>UTXO)\n>\n>Integrating transaction: if a block has an xblock, the second\n>transaction in the block must be the integrating transaction. The\n>inputs include the spent UTXO of all the genesis xtx in this xblock. If\n>it is a bare witness program, the witness must be empty. If it is a\n>P2SH witness program, the scriptSig must be the bridging witness\n>program and the witness must be empty. The last input must be the\n>original integrating UTXO, with empty witness and scriptSig. If no one\n>is trying to send money back from the xblock to the main block, the\n>only output is the updated integrating UTXO, which the value must be >=\n>the total value of all xUTXO\n>\n>\u2014\u2014\u2014\u2014\n>Up to now, I have described how we could send bitcoins from the main\n>UTXO to the xUTXO. Simply speaking, people send money to a new form of\n>witness programme. They have the flexibility to spend it in the main\n>block or xblock. Nothing special would happen if they send to the main\n>block. If they send to the xblock, the value of such UTXO will be\n>collected by the integrating UTXO.\n>\n>After people sent money to xblock, they could trade inside the xblock\n>just like in the main block. Since xblock is invisible to the\n>pre-softfork users, we could have whatever size limit for the xblock,\n>which is not a topic of this proposal.\n>\n>The tricky part is sending from xblock to main block.\n>\n>Returning transaction: returning transaction is a special xtx, sending\n>money to a bridging witness program, with a direction flag of 0x01.\n>These bridging witness program won\u2019t be recorded in the xUTXO set.\n>Instead, an output is added to the integrating tx, with the bridging\n>witness program and corresponding value, called the \u201creturning UTXO\u201d.\n>The returning UTXOs are not spendable until confirmed by 100 blocks.\n>The updated integrating UTXO is the last output, and is not restricted\n>by the 100-block requirement\n>\n>Fees collection in xblock: Same as normal tx, people pay fee in xblock\n>by making output value < input value. Since the value of the\n>integrating UTXO is >= the total value of all existing xUTXO, if fees\n>are paid in the xblock, that will reduce the value of the integrating\n>UTXO, and miners are paid through the usual coinbase tx as fee.\n>\n>xblock commitment: 2 xblock merkle root, with and without witness, are\n>placed exactly after the witness commitment in the coinbase tx.(maybe\n>we could use the coinbase reserved witness value, details TBD). If\n>there is no xblock commitment, xblock must be empty and integrating tx\n>is not allowed.\n>\n>\u2014\u2014\u2014\u2014\n>Same as any 2-way-peg proposal, sending money from the side chain to\n>the main chain is always the most tricky part. Different from other\n>side chain proposals like Rootstock, extension block is fully consensus\n>enforced, and has the same security level as existing bitcoin\n>transactions. To ensure this, an 100-block maturity is needed for the\n>returning UTXO, as the TXID of the integrating transaction is *very*\n>likely to change after a reorg, which will break the transaction chains\n>coming from it. The 100-block maturity requirement bring us back to the\n>usual assumption that txs become permanent after 100 confirmations.\n>\n>Please note that this drastically changes the user experience, as no\n>current users (expect miners) would expect such 100-block freezing.\n>That\u2019s why I don\u2019t allow the returning UTXO to have an arbitrary\n>scriptPubKey, as users of current wallet would never expect such\n>freezing. Using a special output scriptPubKey guarantees that the\n>recipient must understand the implications. Users of the new wallet\n>should be warned that despite they may enjoy lower fees in the xblock,\n>it may be difficult for them to send money to legacy wallets. This is a\n>huge limitation.\n>\n>Maybe we could have some decentralised market (using simple\n>hash-time-locked txs) allowing people to exchange value between block\n>and xblock, bypassing the 100 block requirement. This is actually\n>cheaper, because a full returning is a 2-step process, while p2p\n>exchange is only 1-step.\n>\n>\u2014\u2014\u2014\u2014\n>\n>Questions:\n>\n>1. Is it possible to simplify the design, without compromising\n>security?\n>2. Is it acceptable to do it without the 100-block maturity\n>requirement, thus breaking some long-held assumptions? (This would\n>vastly improve the usability, until a reorg happens)\n>3. Even with maturity requirement, is 100-block an overkill? We never\n>had a fork over maybe 20 blocks. Also, breaking of transaction chain\n>due to reorg is already possible, as people may double spend during a\n>reorg.\n>\n>[note 1] the direction flag is needed to make sure a recipient won\u2019t be\n>paid with a returning transaction, unless explicitly requested. It\n>might be combined with the serialised witness version to save one byte.\n>_______________________________________________\n>bitcoin-dev mailing list\n>bitcoin-dev at lists.linuxfoundation.org\n>https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev"
            }
        ],
        "thread_summary": {
            "title": "Extension block softfork proposal",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Matt Corallo",
                "Johnson Lau"
            ],
            "messages_count": 2,
            "total_messages_chars_count": 17749
        }
    },
    {
        "title": "[bitcoin-dev] Three hardfork-related BIPs",
        "thread_messages": [
            {
                "author": "Luke Dashjr",
                "date": "2017-01-27T01:06:59",
                "message_text_only": "I've put together three hardfork-related BIPs. This is parallel to the ongoing \nresearch into the MMHF/SHF WIP BIP, which might still be best long-term.\n\n1) The first is a block size limit protocol change. It also addresses three \ncriticisms of segwit: 1) segwit increases the block size limit which is \nalready considered by many to be too large; 2) segwit treats pre-segwit \ntransactions \u201cunfairly\u201d by giving the witness discount only to segwit \ntransactions; and 3) that spam blocks can be larger than blocks mining \nlegitimate transactions. This proposal may (depending on activation date) \ninitially reduce the block size limit to a more sustainable size in the short-\nterm, and gradually increase it up over the long-term to 31 MB; it will also \nextend the witness discount to non-segwit transactions. Should the initial \nblock size limit reduction prove to be too controversial, miners can simply \nwait to activate it until closer to the point where it becomes acceptable \nand/or increases the limit. However, since the BIP includes a hardfork, the \neventual block size increase needs community consensus before it can be \ndeployed. Proponents of block size increases should note that this BIP does \nnot interfere with another more aggressive block size increase hardfork in the \nmeantime. I believe I can immediately recommend this for adoption; however, \npeer and community review are welcome to suggest changes.\nText: https://github.com/luke-jr/bips/blob/bip-blksize/bip-blksize.mediawiki\nCode: https://github.com/bitcoin/bitcoin/compare/master...luke-jr:bip-blksize \n(consensus code changes only)\n\n2) The second is a *preparatory* change, that should allow trivially \ntransforming certain classes of hardforks into softforks in the future. It \nessentially says that full nodes should relax their rule enforcement, after \nsufficient time that would virtually guarantee they have ceased to be \nenforcing the full set of rules anyway. This allows these relaxed rules to be \nmodified or removed in a softfork, provided the proposal to do so is accepted \nand implemented with enough advance notice. Attempting to implement this has \nproven more complicated than I originally expected, and it may make more sense \nfor full nodes to simply stop functioning (with a user override) after the \ncut-off date). In light of this, I do not yet recommend its adoption, but am \nposting it for review and comments only.\nText: https://github.com/luke-jr/bips/blob/bip-hfprep/bip-hfprep.mediawiki\n\n3) Third is an anti-replay softfork which can be used to prevent replay \nattacks whether induced by a hardfork-related chain split, or even in ordinary \noperation. It does this by using a new opcode (OP_CHECKBLOCKATHEIGHT) for the \nBitcoin scripting system that allows construction of transactions which are \nvalid only on specific blockchains.\nText: https://github.com/luke-jr/bips/blob/bip-noreplay/bip-noreplay.mediawiki\n\nLuke"
            },
            {
                "author": "Johnson Lau",
                "date": "2017-01-27T04:21:21",
                "message_text_only": "I can\u2019t recommend your first 2 proposals. But I only have the time to talk about the first one for now.\n\nThere are 2 different views on this topic:\n\n1. \u201cThe block size is too small and people can\u2019t buy a coffee with an on-chain transaction. Let\u2019s just remove the limit\u201d\n\n2. \u201cThe block size is too big and people can\u2019t run full nodes or do initial blockchain download (IBD). Let\u2019s just reduce the limit\u201d\n\nFor me, both approaches just show the lack of creativity, and lack of responsibility. Both just try to solve one problem, disregarding all the other consequences.\n\nThe 1MB is here, no matter you like it or not, it\u2019s the current consensus. Any attempts to change this limit (up or down) require wide consensus of the whole community, which might be difficult.\n\nYes, I agree with you that the current 1MB block size is already too big for many people to run a full node. That\u2019s bad, but it doesn\u2019t mean we have no options other than reducing the block size. Just to cite some:\n\n1. Blockchain pruning is already available, so the storage of blockchain is already an O(1) problem. The block size is not that important for this part\n2. UTXO size is an O(n) problem, but we could limit its growth without limit the block size, by charging more for UTXO creation, and offer incentive for UTXO spending  **\n3. For non-mining full node, latency is not critical. 1MB per 10 minutes is not a problem unless with mobile network. But I don\u2019t think mobile network is ever considered as a suitable way for running a full node\n4. For mining nodes, we already have compact block and xthin block, and FIBRE\n5. For IBD, reducing the size won\u2019t help much as it is already too big for many people. The right way to solve the IBD issue is to implement long latency UTXO commitment. Nodes will calculate a UTXO commitment every 1000 block, and commit to the UTXO status of the previous 1000 block (e.g. block 11000 will commit to the UTXO of block 10000). This is a background process and the overhead is negligible. When such commitments are confirmed for sufficiently long (e.g. 1 year), people will assume it is correct, and start IBD from that point by downloading UTXO from some untrusted sources. That will drastically reduce the time for IBD\n6. No matter we change the block size limit or not, we need to implement a fraud-proof system to allow probabilistic validation by SPV nodes. So even a smartphone may validate 0.1% of the blockchain, and with many people using phone wallet, it will only be a net gain to the network security \n\nFor points 2 and 6 above, I have some idea implemented in my experimental hardfork.\nhttps://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html>\n\n\n> On 27 Jan 2017, at 09:06, Luke Dashjr via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n> I've put together three hardfork-related BIPs. This is parallel to the ongoing \n> research into the MMHF/SHF WIP BIP, which might still be best long-term.\n> \n> 1) The first is a block size limit protocol change. It also addresses three \n> criticisms of segwit: 1) segwit increases the block size limit which is \n> already considered by many to be too large; 2) segwit treats pre-segwit \n> transactions \u201cunfairly\u201d by giving the witness discount only to segwit \n> transactions; and 3) that spam blocks can be larger than blocks mining \n> legitimate transactions. This proposal may (depending on activation date) \n> initially reduce the block size limit to a more sustainable size in the short-\n> term, and gradually increase it up over the long-term to 31 MB; it will also \n> extend the witness discount to non-segwit transactions. Should the initial \n> block size limit reduction prove to be too controversial, miners can simply \n> wait to activate it until closer to the point where it becomes acceptable \n> and/or increases the limit. However, since the BIP includes a hardfork, the \n> eventual block size increase needs community consensus before it can be \n> deployed. Proponents of block size increases should note that this BIP does \n> not interfere with another more aggressive block size increase hardfork in the \n> meantime. I believe I can immediately recommend this for adoption; however, \n> peer and community review are welcome to suggest changes.\n> Text: https://github.com/luke-jr/bips/blob/bip-blksize/bip-blksize.mediawiki\n> Code: https://github.com/bitcoin/bitcoin/compare/master...luke-jr:bip-blksize \n> (consensus code changes only)\n> \n> 2) The second is a *preparatory* change, that should allow trivially \n> transforming certain classes of hardforks into softforks in the future. It \n> essentially says that full nodes should relax their rule enforcement, after \n> sufficient time that would virtually guarantee they have ceased to be \n> enforcing the full set of rules anyway. This allows these relaxed rules to be \n> modified or removed in a softfork, provided the proposal to do so is accepted \n> and implemented with enough advance notice. Attempting to implement this has \n> proven more complicated than I originally expected, and it may make more sense \n> for full nodes to simply stop functioning (with a user override) after the \n> cut-off date). In light of this, I do not yet recommend its adoption, but am \n> posting it for review and comments only.\n> Text: https://github.com/luke-jr/bips/blob/bip-hfprep/bip-hfprep.mediawiki\n> \n> 3) Third is an anti-replay softfork which can be used to prevent replay \n> attacks whether induced by a hardfork-related chain split, or even in ordinary \n> operation. It does this by using a new opcode (OP_CHECKBLOCKATHEIGHT) for the \n> Bitcoin scripting system that allows construction of transactions which are \n> valid only on specific blockchains.\n> Text: https://github.com/luke-jr/bips/blob/bip-noreplay/bip-noreplay.mediawiki\n> \n> Luke\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170127/e45ecabf/attachment.html>"
            },
            {
                "author": "t. khan",
                "date": "2017-01-27T18:54:26",
                "message_text_only": "Regarding #1, I agree with Johnson Lau and others who have responded since\nthen\u2014this proposal is not appropriate and should not be adopted for the\nfollowing reasons:\n\n1. Miners will view it as way too little, delivered way too late. And as\nsoon as you say 300kb blocks, you've lost them all.\n\n2. \"Spam\" - You're very fixated on this concept of spam transactions, but\nthe transactions that you deem as spam are legitimate, fee-paying\ntransactions. They're not a problem for miners. It's only a problem to you\nas you've arbitrarily decided some transactions are legit and some are not.\nIt's an imaginary problem and we should focus on designs that solve real\nproblems instead.\n\nAlso, even if you changed the max size to 300kb, transactions that you (and\nas far as I can tell, only you) consider spam will still be in there!\nThey'll just be paying a ridiculous fee along with everyone else.\n\n3. 17% per year growth rate - This is making the assumption that the\ncurrent 1MB limit is already at the upper limit supportable by the network.\nThis isn't even remotely true, and starting this rate at the current limit\nwould cause the system to lag far behind the actual capability of the\nnetwork for no reason.\n\n4. Nodes - Individuals have no incentive to run full nodes and we've\nalready passed the time where it makes any sense for them to do so.\nTherefore restricting the blockchain size in an attempt to keep individuals\nrunning nodes is futile at best and likely very damaging. Miners and\nbusinesses using Bitcoin do have an incentive to run nodes and over the\nyears we've seen a migration of nodes from weak hands (individuals) to\nstrong hands (businesses).\n\nOverall, this proposal would hamstring Bitcoin Core and would drive miners\ntowards Unlimited.\n\n- t.k.\n\nOn Thu, Jan 26, 2017 at 8:06 PM, Luke Dashjr via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n> I've put together three hardfork-related BIPs. This is parallel to the\n> ongoing\n> research into the MMHF/SHF WIP BIP, which might still be best long-term.\n>\n> 1) The first is a block size limit protocol change. It also addresses three\n> criticisms of segwit: 1) segwit increases the block size limit which is\n> already considered by many to be too large; 2) segwit treats pre-segwit\n> transactions \u201cunfairly\u201d by giving the witness discount only to segwit\n> transactions; and 3) that spam blocks can be larger than blocks mining\n> legitimate transactions. This proposal may (depending on activation date)\n> initially reduce the block size limit to a more sustainable size in the\n> short-\n> term, and gradually increase it up over the long-term to 31 MB; it will\n> also\n> extend the witness discount to non-segwit transactions. Should the initial\n> block size limit reduction prove to be too controversial, miners can simply\n> wait to activate it until closer to the point where it becomes acceptable\n> and/or increases the limit. However, since the BIP includes a hardfork, the\n> eventual block size increase needs community consensus before it can be\n> deployed. Proponents of block size increases should note that this BIP does\n> not interfere with another more aggressive block size increase hardfork in\n> the\n> meantime. I believe I can immediately recommend this for adoption; however,\n> peer and community review are welcome to suggest changes.\n> Text: https://github.com/luke-jr/bips/blob/bip-blksize/bip-\n> blksize.mediawiki\n> Code: https://github.com/bitcoin/bitcoin/compare/master...luke-\n> jr:bip-blksize\n> (consensus code changes only)\n>\n> 2) The second is a *preparatory* change, that should allow trivially\n> transforming certain classes of hardforks into softforks in the future. It\n> essentially says that full nodes should relax their rule enforcement, after\n> sufficient time that would virtually guarantee they have ceased to be\n> enforcing the full set of rules anyway. This allows these relaxed rules to\n> be\n> modified or removed in a softfork, provided the proposal to do so is\n> accepted\n> and implemented with enough advance notice. Attempting to implement this\n> has\n> proven more complicated than I originally expected, and it may make more\n> sense\n> for full nodes to simply stop functioning (with a user override) after the\n> cut-off date). In light of this, I do not yet recommend its adoption, but\n> am\n> posting it for review and comments only.\n> Text: https://github.com/luke-jr/bips/blob/bip-hfprep/bip-hfprep.mediawiki\n>\n> 3) Third is an anti-replay softfork which can be used to prevent replay\n> attacks whether induced by a hardfork-related chain split, or even in\n> ordinary\n> operation. It does this by using a new opcode (OP_CHECKBLOCKATHEIGHT) for\n> the\n> Bitcoin scripting system that allows construction of transactions which are\n> valid only on specific blockchains.\n> Text: https://github.com/luke-jr/bips/blob/bip-noreplay/bip-\n> noreplay.mediawiki\n>\n> Luke\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170127/58219b02/attachment-0001.html>"
            },
            {
                "author": "Andrew Johnson",
                "date": "2017-01-27T03:04:50",
                "message_text_only": "Comment on #1.  You're dropping the blocksize limit to 300KB and only\nreaching the limit that we have in place today 7 years later?  We're\nalready at capacity today, surely you're not serious with this proposal?\nWhen you promised code for a hard forking block size increase in the HK\nagreement I don't believe that a decrease first was made apparent.  While\nnot technically in violation of the letter of the agreement, I think this\nis a pretty obviously not in the spirit of it.\n\nOn Jan 26, 2017 7:07 PM, \"Luke Dashjr via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nI've put together three hardfork-related BIPs. This is parallel to the\nongoing\nresearch into the MMHF/SHF WIP BIP, which might still be best long-term.\n\n1) The first is a block size limit protocol change. It also addresses three\ncriticisms of segwit: 1) segwit increases the block size limit which is\nalready considered by many to be too large; 2) segwit treats pre-segwit\ntransactions \u201cunfairly\u201d by giving the witness discount only to segwit\ntransactions; and 3) that spam blocks can be larger than blocks mining\nlegitimate transactions. This proposal may (depending on activation date)\ninitially reduce the block size limit to a more sustainable size in the\nshort-\nterm, and gradually increase it up over the long-term to 31 MB; it will also\nextend the witness discount to non-segwit transactions. Should the initial\nblock size limit reduction prove to be too controversial, miners can simply\nwait to activate it until closer to the point where it becomes acceptable\nand/or increases the limit. However, since the BIP includes a hardfork, the\neventual block size increase needs community consensus before it can be\ndeployed. Proponents of block size increases should note that this BIP does\nnot interfere with another more aggressive block size increase hardfork in\nthe\nmeantime. I believe I can immediately recommend this for adoption; however,\npeer and community review are welcome to suggest changes.\nText: https://github.com/luke-jr/bips/blob/bip-blksize/bip-blksize.mediawiki\nCode: https://github.com/bitcoin/bitcoin/compare/master...luke-\njr:bip-blksize\n(consensus code changes only)\n\n2) The second is a *preparatory* change, that should allow trivially\ntransforming certain classes of hardforks into softforks in the future. It\nessentially says that full nodes should relax their rule enforcement, after\nsufficient time that would virtually guarantee they have ceased to be\nenforcing the full set of rules anyway. This allows these relaxed rules to\nbe\nmodified or removed in a softfork, provided the proposal to do so is\naccepted\nand implemented with enough advance notice. Attempting to implement this has\nproven more complicated than I originally expected, and it may make more\nsense\nfor full nodes to simply stop functioning (with a user override) after the\ncut-off date). In light of this, I do not yet recommend its adoption, but am\nposting it for review and comments only.\nText: https://github.com/luke-jr/bips/blob/bip-hfprep/bip-hfprep.mediawiki\n\n3) Third is an anti-replay softfork which can be used to prevent replay\nattacks whether induced by a hardfork-related chain split, or even in\nordinary\noperation. It does this by using a new opcode (OP_CHECKBLOCKATHEIGHT) for\nthe\nBitcoin scripting system that allows construction of transactions which are\nvalid only on specific blockchains.\nText: https://github.com/luke-jr/bips/blob/bip-noreplay/bip-\nnoreplay.mediawiki\n\nLuke\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170126/dffe0aa7/attachment-0001.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-01-27T04:14:16",
                "message_text_only": "On Friday, January 27, 2017 3:04:50 AM Andrew Johnson wrote:\n> Comment on #1.  You're dropping the blocksize limit to 300KB and only\n> reaching the limit that we have in place today 7 years later?\n\nThe limit only drops all the way to 300k if it activates before 2017 April. \nConsidering that this requires the consensus of a hardfork, followed by a \nrelease in software, and then actual activation by miners using BIP9, I think \nit's extremely unlikely to activate by then.\n\nBut more importantly: such a drop would probably be good for the network in \nthe long-term. As explained in the Rationale section, 300k is necessary to \nmaintain our *current* IBD (first-time node sync) costs even with \ntechnological improvements (which appear to be slowing lately).\n\n> We're already at capacity today, surely you're not serious with this\n> proposal?\n\nWe are only at capacity because the space is available below actual costs, \nand/or because efficient alternatives are not yet widely supported. A \nreduction of block size will likely squeeze out spam, and perhaps some \nunsustainable microtransaction use, but the volume which actually *benefits \nfrom* the blockchain's security should continue along fine. Furthermore, once \nLightning is widely implemented as well-tested, at least microtransactions are \nlikely to gain a huge improvement in efficiency, reducing legitimate usage of \nblock sizes well below 300k naturally - that is frankly when I first expect \nthis proposal to be seriously considered for activation (which is independent \nfrom the consensus to include support for it in nodes).\n\n> When you promised code for a hard forking block size increase in the HK\n> agreement I don't believe that a decrease first was made apparent.  While\n> not technically in violation of the letter of the agreement, I think this\n> is a pretty obviously not in the spirit of it.\n\nI did not mention the HK \"roundtable\", because this is indeed not in the \nspirit of what we set out to do, and do not wish this to be interpreted as \nsome kind of slap in the face of the honest participants of that discussion.\n\nThis proposal is, however, the best I am currently able to honestly recommend \nthat meets the hard criteria outlined at Hong Kong a year ago. (Continued work \non the MMHF/SHF concept may eventually deliver a better solution, but it is \nnot yet ready.)\n\nLuke"
            },
            {
                "author": "Andrew Johnson",
                "date": "2017-01-27T06:13:34",
                "message_text_only": "On Jan 26, 2017 10:15 PM, \"Luke Dashjr\" <luke at dashjr.org> wrote:\n\nOn Friday, January 27, 2017 3:04:50 AM Andrew Johnson wrote:\n> Comment on #1.  You're dropping the blocksize limit to 300KB and only\n> reaching the limit that we have in place today 7 years later?\n\nThe limit only drops all the way to 300k if it activates before 2017 April.\nConsidering that this requires the consensus of a hardfork, followed by a\nrelease in software, and then actual activation by miners using BIP9, I\nthink\nit's extremely unlikely to activate by then.\n\nBut more importantly: such a drop would probably be good for the network in\nthe long-term. As explained in the Rationale section, 300k is necessary to\nmaintain our *current* IBD (first-time node sync) costs even with\ntechnological improvements (which appear to be slowing lately).\n\n\nOther researchers have come to the conservative conclusion that we could\nhandle 4MB blocks today.  Imagine bitcoin had been invented in 1987 and had\na block size correspondent to the internet connections and hard drive sizes\nof the day.  Your proposal would have probably brought us from 1Kb(then\nreduced to 300 bytes) and up to a whopping 20Kb or so today.  Yet even you\nthink we can handle 15x that today.\n\nYou drastically underestimate the speed of technological progression, and\nseem to fancy yourself the central planner of bitcoin.  Isn't that one of\nthe things we're trying to get away from, centrally planned economics?\n\n\n> We're already at capacity today, surely you're not serious with this\n> proposal?\n\nWe are only at capacity because the space is available below actual costs,\nand/or because efficient alternatives are not yet widely supported. A\nreduction of block size will likely squeeze out spam, and perhaps some\nunsustainable microtransaction use, but the volume which actually *benefits\nfrom* the blockchain's security should continue along fine. Furthermore,\nonce\nLightning is widely implemented as well-tested, at least microtransactions\nare\nlikely to gain a huge improvement in efficiency, reducing legitimate usage\nof\nblock sizes well below 300k naturally - that is frankly when I first expect\nthis proposal to be seriously considered for activation (which is\nindependent\nfrom the consensus to include support for it in nodes).\n\n\nLegitimate usage is a transaction that pays the appropriate fee to be\nincluded.  The term legitimate transaction should be stricken from one's\nvocabulary when describing a censorship resistant system such as bitcoin.\n\n\n> When you promised code for a hard forking block size increase in the HK\n> agreement I don't believe that a decrease first was made apparent.  While\n> not technically in violation of the letter of the agreement, I think this\n> is a pretty obviously not in the spirit of it.\n\nI did not mention the HK \"roundtable\", because this is indeed not in the\nspirit of what we set out to do, and do not wish this to be interpreted as\nsome kind of slap in the face of the honest participants of that discussion.\n\n\nToo late for that, I suspect.\n\n\nThis proposal is, however, the best I am currently able to honestly\nrecommend\nthat meets the hard criteria outlined at Hong Kong a year ago. (Continued\nwork\non the MMHF/SHF concept may eventually deliver a better solution, but it is\nnot yet ready.)\n\nLuke\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170127/6822887f/attachment.html>"
            },
            {
                "author": "Daniele Pinna",
                "date": "2017-01-27T12:12:57",
                "message_text_only": "Your BIP implementation should stress the capacity to softfork the rate of\nblocksize increase if necessary. You briefly mention that:\n\n*If over time, this growth factor is beyond what the actual technology\noffers, the intention should be to soft fork a tighter limit.*\n\nHowever this can work both ways so that the rate can potentially be\nincreased also. I think just mentioning this will soothe a lot of future\ncritiques.\n\nDaniele\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*Message: 5Date: Fri, 27 Jan 2017 01:06:59 +0000From: Luke Dashjr\n<luke at dashjr.org\n<luke at dashjr.org>>To: bitcoin-dev at lists.linuxfoundation.org\n<bitcoin-dev at lists.linuxfoundation.org>Subject: [bitcoin-dev] Three\nhardfork-related BIPsMessage-ID: <201701270107.01092.luke at dashjr.org\n<201701270107.01092.luke at dashjr.org>>Content-Type: Text/Plain;\ncharset=\"utf-8\"I've put together three hardfork-related BIPs. This is\nparallel to the ongoingresearch into the MMHF/SHF WIP BIP, which might\nstill be best long-term.1) The first is a block size limit protocol change.\nIt also addresses threecriticisms of segwit: 1) segwit increases the block\nsize limit which isalready considered by many to be too large; 2) segwit\ntreats pre-segwittransactions ?unfairly? by giving the witness discount\nonly to segwittransactions; and 3) that spam blocks can be larger than\nblocks mininglegitimate transactions. This proposal may (depending on\nactivation date)initially reduce the block size limit to a more sustainable\nsize in the short-term, and gradually increase it up over the long-term to\n31 MB; it will alsoextend the witness discount to non-segwit transactions.\nShould the initialblock size limit reduction prove to be too controversial,\nminers can simplywait to activate it until closer to the point where it\nbecomes acceptableand/or increases the limit. However, since the BIP\nincludes a hardfork, theeventual block size increase needs community\nconsensus before it can bedeployed. Proponents of block size increases\nshould note that this BIP doesnot interfere with another more aggressive\nblock size increase hardfork in themeantime. I believe I can immediately\nrecommend this for adoption; however,peer and community review are welcome\nto suggest\nchanges.Text: https://github.com/luke-jr/bips/blob/bip-blksize/bip-blksize.mediawiki\n<https://github.com/luke-jr/bips/blob/bip-blksize/bip-blksize.mediawiki>Code:\nhttps://github.com/bitcoin/bitcoin/compare/master...luke-jr:bip-blksize\n<https://github.com/bitcoin/bitcoin/compare/master...luke-jr:bip-blksize>(consensus\ncode changes only)2) The second is a *preparatory* change, that should\nallow triviallytransforming certain classes of hardforks into softforks in\nthe future. Itessentially says that full nodes should relax their rule\nenforcement, aftersufficient time that would virtually guarantee they have\nceased to beenforcing the full set of rules anyway. This allows these\nrelaxed rules to bemodified or removed in a softfork, provided the proposal\nto do so is acceptedand implemented with enough advance notice. Attempting\nto implement this hasproven more complicated than I originally expected,\nand it may make more sensefor full nodes to simply stop functioning (with a\nuser override) after thecut-off date). In light of this, I do not yet\nrecommend its adoption, but amposting it for review and comments\nonly.Text: https://github.com/luke-jr/bips/blob/bip-hfprep/bip-hfprep.mediawiki\n<https://github.com/luke-jr/bips/blob/bip-hfprep/bip-hfprep.mediawiki>3)\nThird is an anti-replay softfork which can be used to prevent replayattacks\nwhether induced by a hardfork-related chain split, or even in\nordinaryoperation. It does this by using a new opcode\n(OP_CHECKBLOCKATHEIGHT) for theBitcoin scripting system that allows\nconstruction of transactions which arevalid only on specific\nblockchains.Text:\nhttps://github.com/luke-jr/bips/blob/bip-noreplay/bip-noreplay.mediawiki\n<https://github.com/luke-jr/bips/blob/bip-noreplay/bip-noreplay.mediawiki>Luke*\nDaniele Pinna, Ph.D\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170127/50920e07/attachment.html>"
            },
            {
                "author": "Russell O'Connor",
                "date": "2017-01-27T20:34:13",
                "message_text_only": "On Jan 27, 2017 03:03, \"Andrew Johnson via bitcoin-dev\" <bitcoin-dev at lists.\nlinuxfoundation.org> wrote:\n\nOther researchers have come to the conservative conclusion that we could\nhandle 4MB blocks today.\n\n\nI believe this is a mischaracterization of the research conclusions.  The\nactual conclusion was that the maximum value for the blocksize that the\nnetwork can safely handle (at that time) is some value that is\n(conservatively) no more than 4MB.  This is because the research only\nstudies one aspect of the effect of blocksize on the network at a time and\nthe true safe value is the minimum of all aspects.  For example, the 4MB\ndoesn't cover the aspect of quadratic hashing for large transactions in\nlarge blocks.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170127/b8d23e91/attachment.html>"
            },
            {
                "author": "Greg Sanders",
                "date": "2017-01-27T20:47:20",
                "message_text_only": "Note that the 4MB number comes from a single network metric.\n\nQuotes directly from the paper in question:\nhttp://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf\n\n>Our results hinge on the key metric of effective throughput in the overlay\nnetwork, which we define here as which blocks propagate within an average\nblock interval period the percentage of nodes to.\n...\n>Note that as we consider only a subset of possible metrics (due to\ndifficulty in accurately measuring others), our results on\nreparametrization may be viewed as upper bounds: additional metrics could\nreveal even stricter limits.\n\nIt says nothing about any mining centralization pressure, DoS attacks, etc.\nA single metric among many we have to contend with.\n\n\n\n\n\n\nOn Fri, Jan 27, 2017 at 3:34 PM, Russell O'Connor via bitcoin-dev <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\n>\n>\n> On Jan 27, 2017 03:03, \"Andrew Johnson via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org> wrote:\n>\n> Other researchers have come to the conservative conclusion that we could\n> handle 4MB blocks today.\n>\n>\n> I believe this is a mischaracterization of the research conclusions.  The\n> actual conclusion was that the maximum value for the blocksize that the\n> network can safely handle (at that time) is some value that is\n> (conservatively) no more than 4MB.  This is because the research only\n> studies one aspect of the effect of blocksize on the network at a time and\n> the true safe value is the minimum of all aspects.  For example, the 4MB\n> doesn't cover the aspect of quadratic hashing for large transactions in\n> large blocks.\n>\n> _______________________________________________\n> bitcoin-dev mailing list\n> bitcoin-dev at lists.linuxfoundation.org\n> https://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n>\n>\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170127/c4f1926d/attachment-0001.html>"
            },
            {
                "author": "Christian Decker",
                "date": "2017-01-27T21:28:10",
                "message_text_only": "On Fri, Jan 27, 2017 at 03:47:20PM -0500, Greg Sanders via bitcoin-dev wrote:\n> Note that the 4MB number comes from a single network metric.\n> \n> Quotes directly from the paper in question:\n> http://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf\n> \n> >Our results hinge on the key metric of effective throughput in the overlay\n> network, which we define here as which blocks propagate within an average\n> block interval period the percentage of nodes to.\n> ...\n> >Note that as we consider only a subset of possible metrics (due to\n> difficulty in accurately measuring others), our results on\n> reparametrization may be viewed as upper bounds: additional metrics could\n> reveal even stricter limits.\n> \n> It says nothing about any mining centralization pressure, DoS attacks, etc.\n> A single metric among many we have to contend with.\n>\n\nAs one of the authors of that paper and the source of the measurement\ndata I'd also like to point out that the 4MB number is indeed intended\nas an optimistic upper bound on todays network capacity.\n\nMore importantly it's not a black and white situation, where there is\na magic number beyond which Bad Things (TM) happen, it's a spectrum on\nwhich we can see a few threshold beyond which we _know_ Bad Things\ndefinitely happen. Miner centralization pressure is felt earlier."
            },
            {
                "author": "Andrew Johnson",
                "date": "2017-01-27T23:53:02",
                "message_text_only": "Thanks for replying, I'd be interested to see what you would come up with\ntoday using the same methodology, seeing as max single hard drive capacity\nhas roughly doubled, global average internet bandwidth has increased 31%\nfrom 4.8Mbps to 6.3Mbps(sourced from Akamai State of the Internet reports\n2014q4 and 2016q3), and we now have xThin and compact blocks to help\nsignificantly with block propagation time.  Not to mention the usual\nimprovements in CPUs(not that we're anywhere near a CPU bottleneck today\nanyway save for quadratic hashing when raising the blocksize, but I don't\nthink that anyone would seriously suggest an increase without addressing\nthat).\n\nI don't think that the 17% yearly increase is too far off base considering\ncurrent global trends(although I still don't particularly like the idea of\ncentrally planning the limit, especially not that far into the future), but\nthe 66% decrease first seems completely out of touch with reality.\n\nI'd also like to point out to Luke that Satoshi envisioned most full nodes\nrunning in data centers in the white paper, not every single user needs to\nrun a full node to use bitcoin.  Not to present this as an argument from\nauthority, but rather to remind us what the intention of the system was to\nbe(p2p cash, not a settlement layer only afforded by the wealthiest and\nlargest value transactions).  That a lot of people want to continue to move\nin that direction shouldn't be a surprise.\n\nOn Jan 27, 2017 3:28 PM, \"Christian Decker via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nOn Fri, Jan 27, 2017 at 03:47:20PM -0500, Greg Sanders via bitcoin-dev\nwrote:\n> Note that the 4MB number comes from a single network metric.\n>\n> Quotes directly from the paper in question:\n> http://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf\n>\n> >Our results hinge on the key metric of effective throughput in the\noverlay\n> network, which we define here as which blocks propagate within an average\n> block interval period the percentage of nodes to.\n> ...\n> >Note that as we consider only a subset of possible metrics (due to\n> difficulty in accurately measuring others), our results on\n> reparametrization may be viewed as upper bounds: additional metrics could\n> reveal even stricter limits.\n>\n> It says nothing about any mining centralization pressure, DoS attacks,\netc.\n> A single metric among many we have to contend with.\n>\n\nAs one of the authors of that paper and the source of the measurement\ndata I'd also like to point out that the 4MB number is indeed intended\nas an optimistic upper bound on todays network capacity.\n\nMore importantly it's not a black and white situation, where there is\na magic number beyond which Bad Things (TM) happen, it's a spectrum on\nwhich we can see a few threshold beyond which we _know_ Bad Things\ndefinitely happen. Miner centralization pressure is felt earlier.\n_______________________________________________\nbitcoin-dev mailing list\nbitcoin-dev at lists.linuxfoundation.org\nhttps://lists.linuxfoundation.org/mailman/listinfo/bitcoin-dev\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170127/7c98b4de/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-01-28T04:03:03",
                "message_text_only": "On Friday, January 27, 2017 11:53:02 PM Andrew Johnson via bitcoin-dev wrote:\n> I don't think that the 17% yearly increase is too far off base considering\n> current global trends(although I still don't particularly like the idea of\n> centrally planning the limit, especially not that far into the future), but\n> the 66% decrease first seems completely out of touch with reality.\n\nAssume as a premise (despite your apparent disagreement below) that for \nBitcoin to function, a supermajority of economic activity needs to be verified \nusing full nodes operated by the recipient. Evidence suggests that at this \ncurrent time, at best 10% of economic activity is in fact using a full node to \nverify the transaction. On this basis, it seems pretty clear that serious \naction must be taken to change the status quo, and so for efforts to do so \nwithout dropping the block size have proven ineffective.\n\n> I'd also like to point out to Luke that Satoshi envisioned most full nodes\n> running in data centers in the white paper, not every single user needs to\n> run a full node to use bitcoin.\n\nSatoshi envisioned a system where full nodes could publish proofs of invalid \nblocks that would be automatically verified by SPV nodes and used to ensure \neven they maintained the equivalent of full node security so long as they were \nnot isolated. But as a matter of fact, this vision has proven impossible, and \nthere is to date no viable theory on how it might be fixed. As a result, the \nonly way for nodes to have full-node-security is to actually be a true full \nnode, and therefore the plan of only having full nodes in datacenters is \nsimply not realistic without transforming Bitcoin into a centralised system.\n\n> That a lot of people want to continue to move in that direction shouldn't\n> be a surprise.\n\nI think it's likely safe to say that if this were a possibility, everyone \nwould want to continue to move in that direction. But as the facts stand, it \nsimply isn't possible.\n\nLuke"
            },
            {
                "author": "Natanael",
                "date": "2017-01-28T10:36:16",
                "message_text_only": "Den 28 jan. 2017 05:04 skrev \"Luke Dashjr via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org>:\n\nSatoshi envisioned a system where full nodes could publish proofs of invalid\nblocks that would be automatically verified by SPV nodes and used to ensure\neven they maintained the equivalent of full node security so long as they\nwere\nnot isolated. But as a matter of fact, this vision has proven impossible,\nand\nthere is to date no viable theory on how it might be fixed. As a result, the\nonly way for nodes to have full-node-security is to actually be a true full\nnode, and therefore the plan of only having full nodes in datacenters is\nsimply not realistic without transforming Bitcoin into a centralised system.\n\n\nBeside Zero-knowledge proofs, which is capable of proving much so more than\njust validity, there are multi types of fraud proofs that only rely on the\nformat of the blocks. Such as publishing the block header + the two\ncolliding transactions included in it (in the case of double spending), or\nif the syntax or logic is broken then you just publish that single\ntransaction.\n\nThere aren't all  that many cases where fraud proofs are unreasonably large\nfor a networked system like in Bitcoin. If Zero-knowledge proofs can be\napplied securely, then I can't think of any exceptions at all for when the\nproofs would be unmanageable. (Remember that full Zero-knowledge proofs can\nbe chained together!)\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170128/faab5c67/attachment.html>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-01-28T18:29:32",
                "message_text_only": "On 28 January 2017 02:36:16 GMT-08:00, Natanael via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>Den 28 jan. 2017 05:04 skrev \"Luke Dashjr via bitcoin-dev\" <\n>bitcoin-dev at lists.linuxfoundation.org>:\n>\n>Satoshi envisioned a system where full nodes could publish proofs of\n>invalid\n>blocks that would be automatically verified by SPV nodes and used to\n>ensure\n>even they maintained the equivalent of full node security so long as\n>they\n>were\n>not isolated. But as a matter of fact, this vision has proven\n>impossible,\n>and\n>there is to date no viable theory on how it might be fixed. As a\n>result, the\n>only way for nodes to have full-node-security is to actually be a true\n>full\n>node, and therefore the plan of only having full nodes in datacenters\n>is\n>simply not realistic without transforming Bitcoin into a centralised\n>system.\n>\n>\n>Beside Zero-knowledge proofs, which is capable of proving much so more\n>than\n>just validity, there are multi types of fraud proofs that only rely on\n>the\n>format of the blocks. Such as publishing the block header + the two\n>colliding transactions included in it (in the case of double spending),\n>or\n>if the syntax or logic is broken then you just publish that single\n>transaction.\n\nThat's a perfect example of why fraud proofs aren't as secure as expected: the miner who created such a block wouldn't even give you the data necessary to prove the fraud in the first place.\n\nWhat you actually need are validity challenges, where someone makes a challenge claiming that part of the block is invalid. A failure to meet the challenge with proof that the rules are followed is considered defacto evidence of fraud.\n\nBut validity challenges don't scale well and pose DoS attacks issues; it's far from clear that they can be implemented in a useful way. Even if validity challenges work, they also don't solve censorship: a world of nodes in large datacenters is a world where it's very easy to force the few Bitcoin nodes remaining to follow AML/KYC rules for instance, a risk we wouldn't be able to mitigate with a PoW change.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 500 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170128/03b7306f/attachment.sig>"
            },
            {
                "author": "Tom Harding",
                "date": "2017-01-29T19:15:38",
                "message_text_only": "On 1/28/2017 10:29 AM, Peter Todd via bitcoin-dev wrote:\n> a world of nodes in large datacenters is a world where it's very easy\n> to force the few Bitcoin nodes remaining to follow AML/KYC rules\n\nIf that's true, why haven't we already seen AML/KYC required of mining\npools?  That would be comparatively trivial."
            },
            {
                "author": "Eric Voskuil",
                "date": "2017-01-29T19:37:07",
                "message_text_only": "> On Jan 29, 2017, at 11:15 AM, Tom Harding via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n> \n>> On 1/28/2017 10:29 AM, Peter Todd via bitcoin-dev wrote:\n>> a world of nodes in large datacenters is a world where it's very easy\n>> to force the few Bitcoin nodes remaining to follow AML/KYC rules\n> \n> If that's true, why haven't we already seen AML/KYC required of mining\n> pools?  That would be comparatively trivial.\n\nIt is true, there is no question. The fact that an attack does not appear to have occurred does not mean that the vulnerability exists. It is as you say a trivial exploit, which means it will happen when the economic incentive is great enough. Analogous attacks on other points of centralization are already well underway.\n\ne"
            },
            {
                "author": "David Vorick",
                "date": "2017-01-29T19:39:46",
                "message_text_only": "On Jan 29, 2017 2:28 PM, \"Tom Harding via bitcoin-dev\" <\nbitcoin-dev at lists.linuxfoundation.org> wrote:\n\nIf that's true, why haven't we already seen AML/KYC required of mining\npools?  That would be comparatively trivial.\n\n\n\nSome regulators are already looking into it. Even at this point you'd\neither need multinational cooperation or you'd need China to decide that\n51% attacking a budding technology is a good thing to do, something that\nwould be sure to increase tensions across the world.\n\nBut there are two bigger reasons. The first is that regulators are used to\ndoing regulation at exchange points, regulating mining is new and\nunfamiliar and requires a decent understanding of blockchains. And the\nsecond is that Bitcoin is tiny potatoes at this point. To the best of my\nknowledge, organized crime outside of DNMs doesn't use Bitcoin. There's\nminimal reason to target it while it's so small.\n\nRegulated mining I believe is going to be a genuine risk as Bitcoin grows.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170129/1b9673b5/attachment.html>"
            },
            {
                "author": "Luke Dashjr",
                "date": "2017-01-28T19:43:48",
                "message_text_only": "On Saturday, January 28, 2017 10:36:16 AM Natanael wrote:\n> Den 28 jan. 2017 05:04 skrev \"Luke Dashjr via bitcoin-dev\" <\n> bitcoin-dev at lists.linuxfoundation.org>:\n> > Satoshi envisioned a system where full nodes could publish proofs of\n> > invalid blocks that would be automatically verified by SPV nodes and used\n> > to ensure even they maintained the equivalent of full node security so\n> > long as they were not isolated. But as a matter of fact, this vision has\n> > proven impossible, and there is to date no viable theory on how it might\n> > be fixed. As a result, the only way for nodes to have full-node-security\n> > is to actually be a true full node, and therefore the plan of only having\n> > full nodes in datacenters is simply not realistic without transforming\n> > Bitcoin into a centralised system.\n> \n> Beside Zero-knowledge proofs, which is capable of proving much so more than\n> just validity, there are multi types of fraud proofs that only rely on the\n> format of the blocks. Such as publishing the block header + the two\n> colliding transactions included in it (in the case of double spending), or\n> if the syntax or logic is broken then you just publish that single\n> transaction.\n\nWhy would someone malicious follow the format sufficiently to make those \nproofs possible?\n\n> There aren't all  that many cases where fraud proofs are unreasonably large\n> for a networked system like in Bitcoin. If Zero-knowledge proofs can be\n> applied securely, then I can't think of any exceptions at all for when the\n> proofs would be unmanageable. (Remember that full Zero-knowledge proofs can\n> be chained together!)\n\nZK proofs do to a large extent simplify the situation, but still fail in one \ncase (and one case is all an attacker needs, since he can choose how he \nattacks). Specifically, the attacker can create a block which is 100% well-\nformed and valid (or not - nobody will really ever know), and simply withhold \na single transaction in it, such that nobody ever has the complete block's \ndata. Full nodes will of course not accept such a block until they have the \ncomplete data to validate, but they similarly cannot prove it is invalid \nwithout the complete data, and any non-full nodes cannot prove there is data \nmissing without fetching and (to an extent) checking the entire block \nthemselves.\n\nLuke"
            },
            {
                "author": "Peter Todd",
                "date": "2017-01-28T21:54:00",
                "message_text_only": "On Sat, Jan 28, 2017 at 07:43:48PM +0000, Luke Dashjr via bitcoin-dev wrote:\n> On Saturday, January 28, 2017 10:36:16 AM Natanael wrote:\n> > There aren't all  that many cases where fraud proofs are unreasonably large\n> > for a networked system like in Bitcoin. If Zero-knowledge proofs can be\n> > applied securely, then I can't think of any exceptions at all for when the\n> > proofs would be unmanageable. (Remember that full Zero-knowledge proofs can\n> > be chained together!)\n> \n> ZK proofs do to a large extent simplify the situation, but still fail in one \n> case (and one case is all an attacker needs, since he can choose how he \n> attacks). Specifically, the attacker can create a block which is 100% well-\n> formed and valid (or not - nobody will really ever know), and simply withhold \n> a single transaction in it, such that nobody ever has the complete block's \n> data. Full nodes will of course not accept such a block until they have the \n> complete data to validate, but they similarly cannot prove it is invalid \n> without the complete data, and any non-full nodes cannot prove there is data \n> missing without fetching and (to an extent) checking the entire block \n> themselves.\n\nSo, in that particular type of case, the ZK proof may show that the block\nitself is valid and follows all the rules; there'd be no need to get the block\ndata to know that.\n\nThe issue here is other miners being able to mine. Exactly what happens here\ndepends on the exact construction of the ZK proofs, but at best the missing\ndata will mean that part of the UTXO state can no longer be updated by other\nminers, and thus they can't mine all transactions; at worst they'd be\ncompletely preventing from mining at all.\n\nThis is why part of the economic pressure that users exert on miners is\nsubverted by SPV/lite-clients: users that can transact without sufficient\nblockchain data to allow others to mine aren't exerting pressure on miners to\nallow other miners to mine - particularly new entrants to mining. In that\nrespect, ZK proofs are in fact quite harmful to the security of the system if\napplied naively.\n\nEqually, I'll point out that if ZK proofs can be made sufficiently powerful to\ndo all the above, genuinely scalable sharded systems like my own Treechains are\nfar easier to implement, changing the discussion entirely. Currently it is far\nfrom proven that ZK proofs can in fact accomplish this; I hear that Zcash will\nsoon have to upgrade their ZK-SNARK scheme due to advances in cryptographic\nanalysis that may result in a full system break in the near future. We really\ndon't want to be depending on that technology for Bitcoin's security until\nevents like that become much less common.\n\n-- \nhttps://petertodd.org 'peter'[:-1]@petertodd.org\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 455 bytes\nDesc: Digital signature\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170128/437f6ab5/attachment.sig>"
            },
            {
                "author": "Peter Todd",
                "date": "2017-01-28T18:22:25",
                "message_text_only": "On 27 January 2017 15:53:02 GMT-08:00, Andrew Johnson via bitcoin-dev <bitcoin-dev at lists.linuxfoundation.org> wrote:\n>I'd also like to point out to Luke that Satoshi envisioned most full\n>nodes\n>running in data centers in the white paper, not every single user needs\n>to\n>run a full node to use bitcoin.  Not to present this as an argument\n>from\n>authority, but rather to remind us what the intention of the system was\n>to\n>be(p2p cash, not a settlement layer only afforded by the wealthiest and\n>largest value transactions).  That a lot of people want to continue to\n>move\n>in that direction shouldn't be a surprise.\n\nSatoshi also thought that SPV clients would be able to use fraud proofs (called \"alerts\" in the white paper) to detect fraudulent behavior by miners, and thus not have to completely trust those nodes in those datacenters. Unfortunately it turns out that fraud proofs are both a very difficult engineering challenge to implement, and also offer much less security than once thought. In fact, as per Satoshi's vision, SPV clients don't currently exist; what's called SPV isn't what Satoshi was envisioning.\n\nOf course, this wouldn't be the first time that aspects of Satoshi's vision for Bitcoin turned out to be wrong: the white paper also refers to the \"longest chain\" rather than most-work chain, something that had to be fixed in what's technically a hardfork after Bitcoin's initial release.\n-------------- next part --------------\nA non-text attachment was scrubbed...\nName: signature.asc\nType: application/pgp-signature\nSize: 500 bytes\nDesc: not available\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170128/11181563/attachment-0001.sig>"
            }
        ],
        "thread_summary": {
            "title": "Three hardfork-related BIPs",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Eric Voskuil",
                "Natanael",
                "Andrew Johnson",
                "David Vorick",
                "t. khan",
                "Johnson Lau",
                "Daniele Pinna",
                "Russell O'Connor",
                "Peter Todd",
                "Christian Decker",
                "Luke Dashjr",
                "Tom Harding",
                "Greg Sanders"
            ],
            "messages_count": 20,
            "total_messages_chars_count": 50902
        }
    },
    {
        "title": "[bitcoin-dev] Consensus critical limits in Bitcoin protocol and proposed block resources limit accounting",
        "thread_messages": [
            {
                "author": "Johnson Lau",
                "date": "2017-01-27T20:36:03",
                "message_text_only": "There are many consensus critical limits scattered all over the Bitcoin protocol. The first part of this post is to analyse what the current limits are. These limits could be arranged into different categories:\n\n1. Script level limit. Some limits are restricted to scripts, including size (10000 bytes), nOpCount (201), stack plus alt-stack size (1000), and stack push size (520). If these limits are passed, they won\u2019t have any effects on the limits of the other levels.\n\n2. Output value limit: any single output value must be >=0 and <= 21 million bitcoin\n\n3. Transaction level limit: The only transaction level limit we have currently, is the total output value must be equal to or smaller than the total input value for non-coinbase tx.\n\n4. Block level limit: there are several block level limits:\na. The total output value of all txs must be equal to or smaller than the total input value with block reward.\nb. The serialised size including block header and transactions must not be over 1MB. (or 4,000,000 in terms of tx weight with segwit)\nc. The total nSigOpCount must not be over 20,000 (or 80,000 nSigOpCost with segwit)\n\nThere is an unavoidable layer violation in terms of the block level total output value. However, all the other limits are restricted to its level. Particularly, the counting of nSigOp does not require execution of scripts. BIP109 (now withdrawn) tried to change this by implementing a block level SigatureHash limit and SigOp limit by counting the accurate value through running the scripts.\n\nSo currently, we have 2 somewhat independent block resources limits: weight and SigOp. A valid block must not exceed any of these limits. However, for miners trying to maximise the fees under these limits, they need to solve a non-linear equation. It\u2019s even worse for wallets trying to estimate fees, as they have no idea what txs are miners trying to include. In reality, everyone just ignore SigOp for fee estimation, as the size/weight is almost always the dominant factor.\n\nIn order to not introduce further non-linearity with segwit, after examining different alternatives, we decided that the block weight limit should be a simple linear function:  3*base size + total size, which allows bigger block size and provides incentives to limit UTXO growth. With normal use, this allows up to 2MB of block size, and even more if multi-sig becomes more popular. A side effect is that allows a theoretical way to fill up the block to 4MB with mostly non-transaction data, but that\u2019d only happen if a miner decide to do it due to non-standardness. (and this is actually not too bad, as witness could be pruned in the future)\n\nSome also criticised that the weight accounting would make a \u201csimple 2MB hardfork\u201d more dangerous, as the theoretical limits will be 8MB which is too much. This is a complete straw man argument, as with a hardfork, one could introduce any rules at will, including revolutionising the calculation of block resources, as shown below.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nProposal: a new block resources limit accounting\n\nObjectives: \n1. linear fee estimation\n2. a single, unified, block level limit for everything we want to limit\n3. do not require expensive script evaluation\n\nAssumptions:\n1. the maximum base block size is about 1MB (for a hardfork with bigger block, it just needs to upscale the value)\n2. a hardfork is done (despite some of these could also be done with a softfork)\n\nVersion 1: without segwit\nThe tx weight is the maximum of the following values:\n\u2014 Serialised size in byte\n\u2014 accurate nSigOpCount * 50 (statical counting of SigOp in scriptSig, redeemScript, and previous scriptPubKey, but not the new scriptPubKey)\nThe block level limit is 1,000,000\n\nAlthough this looks similar to the existing approach, this actually makes the fee estimation a linear problem. Wallets may now calculate both values for a tx and take the maximum, and compare with other txs on the same basis. On the other hand, the total size and SigOpCount of a block may never go above the existing limits (1MB and 20000) no matter how the txs look like. (In some edge cases, the max block size might be smaller than 1MB, if the weight of some transactions is dominated by the SigOpCount)\n\nVersion 2: extending version 1 with segwit\nThe tx weight is the maximum of the following values:\n\u2014 Serialised size in byte * 2\n\u2014 Base size * 3 + total size\n\u2014 accurate SigOpCount * 50 (as a hardfork, segwit and non-segwit SigOp could be counted in the same way and no need to scale) \nThe block level limit is 4,000,000\n\nFor similar reasons the fee estimation is also a linear problem. An interesting difference between this and BIP141 is this will limit the total block size under 2MB, as 4,000,000 / 2 (the 2 as the scaling factor for the serialised size). If the witness inflation really happens (which I highly doubt as it\u2019s a miner initiated attack), we could introduce a similar limit just with a softfork.\n\nVersion 3: extending version 2 to limit UTXO growth:\nThe tx weight is the maximum of the following values:\n\u2014 Serialised size in byte * 2\n\u2014 Adjusted size = Base size * 3 + total size + (number of non-OP_RETURN outputs - number of inputs) * 4 * 41\n\u2014 accurate SigOpCount * 50\n\nI have explained the rationale for the adjusted size in an earlier post but just repeat here. \u201c4\u201d in the formula is the witness scale factor, and \u201c41\u201d is the minimum size of transaction input (32 hash + 4 index + 4 sequence + 1 for empty scriptSig). This requires everyone to pay a significant portion of the spending fee when they create a UTXO, so they pay less when it is spent. For transactions with 1:1 input and output ratios, the effect is cancelled out and won\u2019t actually affect the weight estimation. When spending becomes cheaper, even UTXOs with lower value might become economical to spend, which helps cleaning up the UTXO. Since UTXO is the most expensive aspect, I strongly believe that any block size increase proposal must somehow discourage further growth of the set.\n\nVersion 4: including a sighash limit\nThis is what I actually implemented in my experimental hardfork network: https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html <https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-January/013472.html>\nI\u2019m not repeating here, but it shows how further limits might be added on top of the old ones through a softfork. Basically, you just add more metrics, and always take to maximum one.\n-------------- next part --------------\nAn HTML attachment was scrubbed...\nURL: <http://lists.linuxfoundation.org/pipermail/bitcoin-dev/attachments/20170128/84d6df5e/attachment.html>"
            }
        ],
        "thread_summary": {
            "title": "Consensus critical limits in Bitcoin protocol and proposed block resources limit accounting",
            "categories": [
                "bitcoin-dev"
            ],
            "authors": [
                "Johnson Lau"
            ],
            "messages_count": 1,
            "total_messages_chars_count": 6613
        }
    }
]